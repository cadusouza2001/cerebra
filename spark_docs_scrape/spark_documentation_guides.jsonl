{"url": "https://spark.apache.org/docs/latest/index.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nApache Spark - A Unified engine for large-scale data analytics\nApache Spark is a unified analytics engine for large-scale data processing.\n                  It provides high-level APIs in Java, Scala, Python and R,\n                  and an optimized engine that supports general execution graphs.\n                  It also supports a rich set of higher-level tools including\nSpark SQL\nfor SQL and structured data processing,\npandas API on Spark\nfor pandas workloads,\nMLlib\nfor machine learning,\nGraphX\nfor graph processing,\n                   and\nStructured Streaming\nfor incremental computation and stream processing.\nDownloading\nGet Spark from the\ndownloads page\nof the project website. This documentation is for Spark version 4.0.0. Spark uses Hadoop’s client libraries for HDFS and YARN. Downloads are pre-packaged for a handful of popular Hadoop versions.\nUsers can also download a “Hadoop free” binary and run Spark with any Hadoop version\nby augmenting Spark’s classpath\n.\nScala and Java users can include Spark in their projects using its Maven coordinates and Python users can install Spark from PyPI.\nIf you’d like to build Spark from\nsource, visit\nBuilding Spark\n.\nSpark runs on both Windows and UNIX-like systems (e.g. Linux, Mac OS), and it should run on any platform that runs a supported version of Java. This should include JVMs on x86_64 and ARM64. It’s easy to run locally on one machine — all you need is to have\njava\ninstalled on your system\nPATH\n, or the\nJAVA_HOME\nenvironment variable pointing to a Java installation.\nSpark runs on Java 17/21, Scala 2.13, Python 3.9+, and R 3.5+ (Deprecated).\nWhen using the Scala API, it is necessary for applications to use the same version of Scala that Spark was compiled for. Since Spark 4.0.0, it’s Scala 2.13.\nRunning the Examples and Shell\nSpark comes with several sample programs. Python, Scala, Java, and R examples are in the\nexamples/src/main\ndirectory.\nTo run Spark interactively in a Python interpreter, use\nbin/pyspark\n:\n./bin/pyspark --master \"local[2]\"\nSample applications are provided in Python. For example:\n./bin/spark-submit examples/src/main/python/pi.py 10\nTo run one of the Scala or Java sample programs, use\nbin/run-example <class> [params]\nin the top-level Spark directory. (Behind the scenes, this\ninvokes the more general\nspark-submit\nscript\nfor\nlaunching applications). For example,\n./bin/run-example SparkPi 10\nYou can also run Spark interactively through a modified version of the Scala shell. This is a\ngreat way to learn the framework.\n./bin/spark-shell --master \"local[2]\"\nThe\n--master\noption specifies the\nmaster URL for a distributed cluster\n, or\nlocal\nto run\nlocally with one thread, or\nlocal[N]\nto run locally with N threads. You should start by using\nlocal\nfor testing. For a full list of options, run the Spark shell with the\n--help\noption.\nSince version 1.4, Spark has provided an\nR API\n(only the DataFrame APIs are included).\nTo run Spark interactively in an R interpreter, use\nbin/sparkR\n:\n./bin/sparkR --master \"local[2]\"\nExample applications are also provided in R. For example:\n./bin/spark-submit examples/src/main/r/dataframe.R\nRunning Spark Client Applications Anywhere with Spark Connect\nSpark Connect is a new client-server architecture introduced in Spark 3.4 that decouples Spark\nclient applications and allows remote connectivity to Spark clusters. The separation between\nclient and server allows Spark and its open ecosystem to be leveraged from anywhere, embedded\nin any application. In Spark 3.4, Spark Connect provides DataFrame API coverage for PySpark and\nDataFrame/Dataset API support in Scala.\nTo learn more about Spark Connect and how to use it, see\nSpark Connect Overview\n.\nLaunching on a Cluster\nThe Spark\ncluster mode overview\nexplains the key concepts in running on a cluster.\nSpark can run both by itself, or over several existing cluster managers. It currently provides several\noptions for deployment:\nStandalone Deploy Mode\n: simplest way to deploy Spark on a private cluster\nHadoop YARN\nKubernetes\nWhere to Go from Here\nProgramming Guides:\nQuick Start\n: a quick introduction to the Spark API; start here!\nRDD Programming Guide\n: overview of Spark basics - RDDs (core but old API), accumulators, and broadcast variables\nSpark SQL, Datasets, and DataFrames\n: processing structured data with relational queries (newer API than RDDs)\nStructured Streaming\n: processing structured data streams with relation queries (using Datasets and DataFrames, newer API than DStreams)\nSpark Streaming\n: processing data streams using DStreams (old API)\nMLlib\n: applying machine learning algorithms\nGraphX\n: processing graphs\nSparkR (Deprecated)\n: processing data with Spark in R\nPySpark\n: processing data with Spark in Python\nSpark SQL CLI\n: processing data with SQL on the command line\nAPI Docs:\nSpark Python API (Sphinx)\nSpark Scala API (Scaladoc)\nSpark Java API (Javadoc)\nSpark R API (Roxygen2)\nSpark SQL, Built-in Functions (MkDocs)\nDeployment Guides:\nCluster Overview\n: overview of concepts and components when running on a cluster\nSubmitting Applications\n: packaging and deploying applications\nDeployment modes:\nStandalone Deploy Mode\n: launch a standalone cluster quickly without a third-party cluster manager\nYARN\n: deploy Spark on top of Hadoop NextGen (YARN)\nKubernetes\n: deploy Spark apps on top of Kubernetes directly\nAmazon EC2\n: scripts that let you launch a cluster on EC2 in about 5 minutes\nSpark Kubernetes Operator\n:\nSparkApp\n: deploy Spark apps on top of Kubernetes via\noperator patterns\nSparkCluster\n: deploy Spark clusters on top of Kubernetes via\noperator patterns\nOther Documents:\nConfiguration\n: customize Spark via its configuration system\nMonitoring\n: track the behavior of your applications\nWeb UI\n: view useful information about your applications\nTuning Guide\n: best practices to optimize performance and memory use\nJob Scheduling\n: scheduling resources across and within Spark applications\nSecurity\n: Spark security support\nHardware Provisioning\n: recommendations for cluster hardware\nIntegration with other storage systems:\nCloud Infrastructures\nOpenStack Swift\nMigration Guide\n: migration guides for Spark components\nBuilding Spark\n: build Spark using the Maven system\nContributing to Spark\nThird Party Projects\n: related third party Spark projects\nExternal Resources:\nSpark Homepage\nSpark Community\nresources, including local meetups\nStackOverflow tag\napache-spark\nMailing Lists\n: ask questions about Spark here\nAMP Camps: a series of training camps at UC Berkeley that featured talks and\nexercises about Spark, Spark Streaming, Mesos, and more.\nVideos\n,\nare available online for free.\nCode Examples\n: more are also available in the\nexamples\nsubfolder of Spark (\nPython\n,\nScala\n,\nJava\n,\nR\n)"}
{"url": "https://spark.apache.org/docs/latest/quick-start.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nQuick Start\nInteractive Analysis with the Spark Shell\nBasics\nMore on Dataset Operations\nCaching\nSelf-Contained Applications\nWhere to Go from Here\nThis tutorial provides a quick introduction to using Spark. We will first introduce the API through Spark’s\ninteractive shell (in Python or Scala),\nthen show how to write applications in Java, Scala, and Python.\nTo follow along with this guide, first, download a packaged release of Spark from the\nSpark website\n. Since we won’t be using HDFS,\nyou can download a package for any version of Hadoop.\nNote that, before Spark 2.0, the main programming interface of Spark was the Resilient Distributed Dataset (RDD). After Spark 2.0, RDDs are replaced by Dataset, which is strongly-typed like an RDD, but with richer optimizations under the hood. The RDD interface is still supported, and you can get a more detailed reference at the\nRDD programming guide\n. However, we highly recommend you to switch to use Dataset, which has better performance than RDD. See the\nSQL programming guide\nto get more information about Dataset.\nInteractive Analysis with the Spark Shell\nBasics\nSpark’s shell provides a simple way to learn the API, as well as a powerful tool to analyze data interactively.\nIt is available in either Scala (which runs on the Java VM and is thus a good way to use existing Java libraries)\nor Python. Start it by running the following in the Spark directory:\n./bin/pyspark\nOr if PySpark is installed with pip in your current environment:\npyspark\nSpark’s primary abstraction is a distributed collection of items called a Dataset. Datasets can be created from Hadoop InputFormats (such as HDFS files) or by transforming other Datasets. Due to Python’s dynamic nature, we don’t need the Dataset to be strongly-typed in Python. As a result, all Datasets in Python are Dataset[Row], and we call it\nDataFrame\nto be consistent with the data frame concept in Pandas and R. Let’s make a new DataFrame from the text of the README file in the Spark source directory:\n>>>\ntextFile\n=\nspark\n.\nread\n.\ntext\n(\n\"\nREADME.md\n\"\n)\nYou can get values from DataFrame directly, by calling some actions, or transform the DataFrame to get a new one. For more details, please read the\nAPI doc\n.\n>>>\ntextFile\n.\ncount\n()\n# Number of rows in this DataFrame\n126\n>>>\ntextFile\n.\nfirst\n()\n# First row in this DataFrame\nRow\n(\nvalue\n=\nu\n'\n# Apache Spark\n'\n)\nNow let’s transform this DataFrame to a new one. We call\nfilter\nto return a new DataFrame with a subset of the lines in the file.\n>>>\nlinesWithSpark\n=\ntextFile\n.\nfilter\n(\ntextFile\n.\nvalue\n.\ncontains\n(\n\"\nSpark\n\"\n))\nWe can chain together transformations and actions:\n>>>\ntextFile\n.\nfilter\n(\ntextFile\n.\nvalue\n.\ncontains\n(\n\"\nSpark\n\"\n)).\ncount\n()\n# How many lines contain \"Spark\"?\n15\n./bin/spark-shell\nSpark’s primary abstraction is a distributed collection of items called a Dataset. Datasets can be created from Hadoop InputFormats (such as HDFS files) or by transforming other Datasets. Let’s make a new Dataset from the text of the README file in the Spark source directory:\nscala\n>\nval\ntextFile\n=\nspark\n.\nread\n.\ntextFile\n(\n\"README.md\"\n)\ntextFile\n:\norg.apache.spark.sql.Dataset\n[\nString\n]\n=\n[\nvalue:\nstring\n]\nYou can get values from Dataset directly, by calling some actions, or transform the Dataset to get a new one. For more details, please read the\nAPI doc\n.\nscala\n>\ntextFile\n.\ncount\n()\n// Number of items in this Dataset\nres0\n:\nLong\n=\n126\n// May be different from yours as README.md will change over time, similar to other outputs\nscala\n>\ntextFile\n.\nfirst\n()\n// First item in this Dataset\nres1\n:\nString\n=\n#\nApache\nSpark\nNow let’s transform this Dataset into a new one. We call\nfilter\nto return a new Dataset with a subset of the items in the file.\nscala\n>\nval\nlinesWithSpark\n=\ntextFile\n.\nfilter\n(\nline\n=>\nline\n.\ncontains\n(\n\"Spark\"\n))\nlinesWithSpark\n:\norg.apache.spark.sql.Dataset\n[\nString\n]\n=\n[\nvalue:\nstring\n]\nWe can chain together transformations and actions:\nscala\n>\ntextFile\n.\nfilter\n(\nline\n=>\nline\n.\ncontains\n(\n\"Spark\"\n)).\ncount\n()\n// How many lines contain \"Spark\"?\nres3\n:\nLong\n=\n15\nMore on Dataset Operations\nDataset actions and transformations can be used for more complex computations. Let’s say we want to find the line with the most words:\n>>>\nfrom\npyspark.sql\nimport\nfunctions\nas\nsf\n>>>\ntextFile\n.\nselect\n(\nsf\n.\nsize\n(\nsf\n.\nsplit\n(\ntextFile\n.\nvalue\n,\n\"\n\\s+\n\"\n)).\nname\n(\n\"\nnumWords\n\"\n)).\nagg\n(\nsf\n.\nmax\n(\nsf\n.\ncol\n(\n\"\nnumWords\n\"\n))).\ncollect\n()\n[\nRow\n(\nmax\n(\nnumWords\n)\n=\n15\n)]\nThis first maps a line to an integer value and aliases it as “numWords”, creating a new DataFrame.\nagg\nis called on that DataFrame to find the largest word count. The arguments to\nselect\nand\nagg\nare both\nColumn\n, we can use\ndf.colName\nto get a column from a DataFrame. We can also import pyspark.sql.functions, which provides a lot of convenient functions to build a new Column from an old one.\nOne common data flow pattern is MapReduce, as popularized by Hadoop. Spark can implement MapReduce flows easily:\n>>>\nwordCounts\n=\ntextFile\n.\nselect\n(\nsf\n.\nexplode\n(\nsf\n.\nsplit\n(\ntextFile\n.\nvalue\n,\n\"\n\\s+\n\"\n)).\nalias\n(\n\"\nword\n\"\n)).\ngroupBy\n(\n\"\nword\n\"\n).\ncount\n()\nHere, we use the\nexplode\nfunction in\nselect\n, to transform a Dataset of lines to a Dataset of words, and then combine\ngroupBy\nand\ncount\nto compute the per-word counts in the file as a DataFrame of 2 columns: “word” and “count”. To collect the word counts in our shell, we can call\ncollect\n:\n>>>\nwordCounts\n.\ncollect\n()\n[\nRow\n(\nword\n=\nu\n'\nonline\n'\n,\ncount\n=\n1\n),\nRow\n(\nword\n=\nu\n'\ngraphs\n'\n,\ncount\n=\n1\n),\n...]\nscala\n>\ntextFile\n.\nmap\n(\nline\n=>\nline\n.\nsplit\n(\n\" \"\n).\nsize\n).\nreduce\n((\na\n,\nb\n)\n=>\nif\n(\na\n>\nb\n)\na\nelse\nb\n)\nres4\n:\nInt\n=\n15\nThis first maps a line to an integer value, creating a new Dataset.\nreduce\nis called on that Dataset to find the largest word count. The arguments to\nmap\nand\nreduce\nare Scala function literals (closures), and can use any language feature or Scala/Java library. For example, we can easily call functions declared elsewhere. We’ll use\nMath.max()\nfunction to make this code easier to understand:\nscala\n>\nimport\njava.lang.Math\nimport\njava.lang.Math\nscala\n>\ntextFile\n.\nmap\n(\nline\n=>\nline\n.\nsplit\n(\n\" \"\n).\nsize\n).\nreduce\n((\na\n,\nb\n)\n=>\nMath\n.\nmax\n(\na\n,\nb\n))\nres5\n:\nInt\n=\n15\nOne common data flow pattern is MapReduce, as popularized by Hadoop. Spark can implement MapReduce flows easily:\nscala\n>\nval\nwordCounts\n=\ntextFile\n.\nflatMap\n(\nline\n=>\nline\n.\nsplit\n(\n\" \"\n)).\ngroupByKey\n(\nidentity\n).\ncount\n()\nwordCounts\n:\norg.apache.spark.sql.Dataset\n[(\nString\n,\nLong\n)]\n=\n[\nvalue:\nstring\n,\ncount\n(\n1\n)\n:\nbigint\n]\nHere, we call\nflatMap\nto transform a Dataset of lines to a Dataset of words, and then combine\ngroupByKey\nand\ncount\nto compute the per-word counts in the file as a Dataset of (String, Long) pairs. To collect the word counts in our shell, we can call\ncollect\n:\nscala\n>\nwordCounts\n.\ncollect\n()\nres6\n:\nArray\n[(\nString\n,\nInt\n)]\n=\nArray\n((\nmeans\n,\n1\n),\n(\nunder\n,\n2\n),\n(\nthis\n,\n3\n),\n(\nBecause\n,\n1\n),\n(\nPython\n,\n2\n),\n(\nagree\n,\n1\n),\n(\ncluster\n.,\n1\n),\n...)\nCaching\nSpark also supports pulling data sets into a cluster-wide in-memory cache. This is very useful when data is accessed repeatedly, such as when querying a small “hot” dataset or when running an iterative algorithm like PageRank. As a simple example, let’s mark our\nlinesWithSpark\ndataset to be cached:\n>>>\nlinesWithSpark\n.\ncache\n()\n>>>\nlinesWithSpark\n.\ncount\n()\n15\n>>>\nlinesWithSpark\n.\ncount\n()\n15\nIt may seem silly to use Spark to explore and cache a 100-line text file. The interesting part is\nthat these same functions can be used on very large data sets, even when they are striped across\ntens or hundreds of nodes. You can also do this interactively by connecting\nbin/pyspark\nto\na cluster, as described in the\nRDD programming guide\n.\nscala\n>\nlinesWithSpark\n.\ncache\n()\nres7\n:\nlinesWithSpark.\ntype\n=\n[\nvalue:\nstring\n]\nscala\n>\nlinesWithSpark\n.\ncount\n()\nres8\n:\nLong\n=\n15\nscala\n>\nlinesWithSpark\n.\ncount\n()\nres9\n:\nLong\n=\n15\nIt may seem silly to use Spark to explore and cache a 100-line text file. The interesting part is\nthat these same functions can be used on very large data sets, even when they are striped across\ntens or hundreds of nodes. You can also do this interactively by connecting\nbin/spark-shell\nto\na cluster, as described in the\nRDD programming guide\n.\nSelf-Contained Applications\nSuppose we wish to write a self-contained application using the Spark API. We will walk through a\nsimple application in Scala (with sbt), Java (with Maven), and Python (pip).\nNow we will show how to write an application using the Python API (PySpark).\nIf you are building a packaged PySpark application or library you can add it to your setup.py file as:\ninstall_requires\n=\n[\n'\npyspark==4.0.0\n'\n]\nAs an example, we’ll create a simple Spark application,\nSimpleApp.py\n:\n\"\"\"\nSimpleApp.py\n\"\"\"\nfrom\npyspark.sql\nimport\nSparkSession\nlogFile\n=\n\"\nYOUR_SPARK_HOME/README.md\n\"\n# Should be some file on your system\nspark\n=\nSparkSession\n.\nbuilder\n.\nappName\n(\n\"\nSimpleApp\n\"\n).\ngetOrCreate\n()\nlogData\n=\nspark\n.\nread\n.\ntext\n(\nlogFile\n).\ncache\n()\nnumAs\n=\nlogData\n.\nfilter\n(\nlogData\n.\nvalue\n.\ncontains\n(\n'\na\n'\n)).\ncount\n()\nnumBs\n=\nlogData\n.\nfilter\n(\nlogData\n.\nvalue\n.\ncontains\n(\n'\nb\n'\n)).\ncount\n()\nprint\n(\n\"\nLines with a: %i, lines with b: %i\n\"\n%\n(\nnumAs\n,\nnumBs\n))\nspark\n.\nstop\n()\nThis program just counts the number of lines containing ‘a’ and the number containing ‘b’ in a\ntext file.\nNote that you’ll need to replace YOUR_SPARK_HOME with the location where Spark is installed.\nAs with the Scala and Java examples, we use a SparkSession to create Datasets.\nFor applications that use custom classes or third-party libraries, we can also add code\ndependencies to\nspark-submit\nthrough its\n--py-files\nargument by packaging them into a\n.zip file (see\nspark-submit --help\nfor details).\nSimpleApp\nis simple enough that we do not need to specify any code dependencies.\nWe can run this application using the\nbin/spark-submit\nscript:\n# Use spark-submit to run your application\n$\nYOUR_SPARK_HOME/bin/spark-submit\n\\\n--master\n\"local[4]\"\n\\\nSimpleApp.py\n...\nLines with a: 46, Lines with b: 23\nIf you have PySpark pip installed into your environment (e.g.,\npip install pyspark\n), you can run your application with the regular Python interpreter or use the provided ‘spark-submit’ as you prefer.\n# Use the Python interpreter to run your application\n$\npython SimpleApp.py\n...\nLines with a: 46, Lines with b: 23\nWe’ll create a very simple Spark application in Scala–so simple, in fact, that it’s\nnamed\nSimpleApp.scala\n:\n/* SimpleApp.scala */\nimport\norg.apache.spark.sql.SparkSession\nobject\nSimpleApp\n{\ndef\nmain\n(\nargs\n:\nArray\n[\nString\n])\n:\nUnit\n=\n{\nval\nlogFile\n=\n\"YOUR_SPARK_HOME/README.md\"\n// Should be some file on your system\nval\nspark\n=\nSparkSession\n.\nbuilder\n.\nappName\n(\n\"Simple Application\"\n).\ngetOrCreate\n()\nval\nlogData\n=\nspark\n.\nread\n.\ntextFile\n(\nlogFile\n).\ncache\n()\nval\nnumAs\n=\nlogData\n.\nfilter\n(\nline\n=>\nline\n.\ncontains\n(\n\"a\"\n)).\ncount\n()\nval\nnumBs\n=\nlogData\n.\nfilter\n(\nline\n=>\nline\n.\ncontains\n(\n\"b\"\n)).\ncount\n()\nprintln\n(\ns\n\"Lines with a: $numAs, Lines with b: $numBs\"\n)\nspark\n.\nstop\n()\n}\n}\nNote that applications should define a\nmain()\nmethod instead of extending\nscala.App\n.\nSubclasses of\nscala.App\nmay not work correctly.\nThis program just counts the number of lines containing ‘a’ and the number containing ‘b’ in the\nSpark README. Note that you’ll need to replace YOUR_SPARK_HOME with the location where Spark is\ninstalled. Unlike the earlier examples with the Spark shell, which initializes its own SparkSession,\nwe initialize a SparkSession as part of the program.\nWe call\nSparkSession.builder\nto construct a\nSparkSession\n, then set the application name, and finally call\ngetOrCreate\nto get the\nSparkSession\ninstance.\nOur application depends on the Spark API, so we’ll also include an sbt configuration file,\nbuild.sbt\n, which explains that Spark is a dependency. This file also adds a repository that\nSpark depends on:\nname\n:=\n\"Simple Project\"\nversion\n:=\n\"1.0\"\nscalaVersion\n:=\n\"2.13.16\"\nlibraryDependencies\n+=\n\"org.apache.spark\"\n%%\n\"spark-sql\"\n%\n\"4.0.0\"\nFor sbt to work correctly, we’ll need to layout\nSimpleApp.scala\nand\nbuild.sbt\naccording to the typical directory structure. Once that is in place, we can create a JAR package\ncontaining the application’s code, then use the\nspark-submit\nscript to run our program.\n# Your directory layout should look like this\n$\nfind\n.\n.\n./build.sbt\n./src\n./src/main\n./src/main/scala\n./src/main/scala/SimpleApp.scala\n# Package a jar containing your application\n$\nsbt package\n...\n[\ninfo] Packaging\n{\n..\n}\n/\n{\n..\n}\n/target/scala-2.13/simple-project_2.13-1.0.jar\n# Use spark-submit to run your application\n$\nYOUR_SPARK_HOME/bin/spark-submit\n\\\n--class\n\"SimpleApp\"\n\\\n--master\n\"local[4]\"\n\\\ntarget/scala-2.13/simple-project_2.13-1.0.jar\n...\nLines with a: 46, Lines with b: 23\nThis example will use Maven to compile an application JAR, but any similar build system will work.\nWe’ll create a very simple Spark application,\nSimpleApp.java\n:\n/* SimpleApp.java */\nimport\norg.apache.spark.sql.SparkSession\n;\nimport\norg.apache.spark.sql.Dataset\n;\npublic\nclass\nSimpleApp\n{\npublic\nstatic\nvoid\nmain\n(\nString\n[]\nargs\n)\n{\nString\nlogFile\n=\n\"YOUR_SPARK_HOME/README.md\"\n;\n// Should be some file on your system\nSparkSession\nspark\n=\nSparkSession\n.\nbuilder\n().\nappName\n(\n\"Simple Application\"\n).\ngetOrCreate\n();\nDataset\n<\nString\n>\nlogData\n=\nspark\n.\nread\n().\ntextFile\n(\nlogFile\n).\ncache\n();\nlong\nnumAs\n=\nlogData\n.\nfilter\n(\ns\n->\ns\n.\ncontains\n(\n\"a\"\n)).\ncount\n();\nlong\nnumBs\n=\nlogData\n.\nfilter\n(\ns\n->\ns\n.\ncontains\n(\n\"b\"\n)).\ncount\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Lines with a: \"\n+\nnumAs\n+\n\", lines with b: \"\n+\nnumBs\n);\nspark\n.\nstop\n();\n}\n}\nThis program just counts the number of lines containing ‘a’ and the number containing ‘b’ in the\nSpark README. Note that you’ll need to replace YOUR_SPARK_HOME with the location where Spark is\ninstalled. Unlike the earlier examples with the Spark shell, which initializes its own SparkSession,\nwe initialize a SparkSession as part of the program.\nTo build the program, we also write a Maven\npom.xml\nfile that lists Spark as a dependency.\nNote that Spark artifacts are tagged with a Scala version.\n<project>\n<groupId>\nedu.berkeley\n</groupId>\n<artifactId>\nsimple-project\n</artifactId>\n<modelVersion>\n4.0.0\n</modelVersion>\n<name>\nSimple Project\n</name>\n<packaging>\njar\n</packaging>\n<version>\n1.0\n</version>\n<dependencies>\n<dependency>\n<!-- Spark dependency -->\n<groupId>\norg.apache.spark\n</groupId>\n<artifactId>\nspark-sql_2.13\n</artifactId>\n<version>\n4.0.0\n</version>\n<scope>\nprovided\n</scope>\n</dependency>\n</dependencies>\n</project>\nWe lay out these files according to the canonical Maven directory structure:\n$\nfind\n.\n./pom.xml\n./src\n./src/main\n./src/main/java\n./src/main/java/SimpleApp.java\nNow, we can package the application using Maven and execute it with\n./bin/spark-submit\n.\n# Package a JAR containing your application\n$\nmvn package\n...\n[\nINFO] Building jar:\n{\n..\n}\n/\n{\n..\n}\n/target/simple-project-1.0.jar\n# Use spark-submit to run your application\n$\nYOUR_SPARK_HOME/bin/spark-submit\n\\\n--class\n\"SimpleApp\"\n\\\n--master\n\"local[4]\"\n\\\ntarget/simple-project-1.0.jar\n...\nLines with a: 46, Lines with b: 23\nOther dependency management tools such as Conda and pip can be also used for custom classes or third-party libraries. See also\nPython Package Management\n.\nWhere to Go from Here\nCongratulations on running your first Spark application!\nFor an in-depth overview of the API, start with the\nRDD programming guide\nand the\nSQL programming guide\n, or see “Programming Guides” menu for other components.\nFor running applications on a cluster, head to the\ndeployment overview\n.\nFinally, Spark includes several samples in the\nexamples\ndirectory\n(\nPython\n,\nScala\n,\nJava\n,\nR\n).\nYou can run them as follows:\n# For Python examples, use spark-submit directly:\n./bin/spark-submit examples/src/main/python/pi.py\n# For Scala and Java, use run-example:\n./bin/run-example SparkPi\n# For R examples, use spark-submit directly:\n./bin/spark-submit examples/src/main/r/dataframe.R"}
{"url": "https://spark.apache.org/docs/latest/storage-openstack-swift.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nAccessing OpenStack Swift from Spark\nSpark’s support for Hadoop InputFormat allows it to process data in OpenStack Swift using the\nsame URI formats as in Hadoop. You can specify a path in Swift as input through a\nURI of the form\nswift://container.PROVIDER/path\n. You will also need to set your\nSwift security credentials, through\ncore-site.xml\nor via\nSparkContext.hadoopConfiguration\n.\nThe current Swift driver requires Swift to use the Keystone authentication method, or\nits Rackspace-specific predecessor.\nConfiguring Swift for Better Data Locality\nAlthough not mandatory, it is recommended to configure the proxy server of Swift with\nlist_endpoints\nto have better data locality. More information is\navailable here\n.\nDependencies\nThe Spark application should include\nhadoop-openstack\ndependency, which can\nbe done by including the\nhadoop-cloud\nmodule for the specific version of spark used.\nFor example, for Maven support, add the following to the\npom.xml\nfile:\n<dependencyManagement>\n...\n<dependency>\n<groupId>\norg.apache.spark\n</groupId>\n<artifactId>\nhadoop-cloud_2.13\n</artifactId>\n<version>\n${spark.version}\n</version>\n</dependency>\n...\n</dependencyManagement>\nConfiguration Parameters\nCreate\ncore-site.xml\nand place it inside Spark’s\nconf\ndirectory.\nThe main category of parameters that should be configured is the authentication parameters\nrequired by Keystone.\nThe following table contains a list of Keystone mandatory parameters.\nPROVIDER\ncan be\nany (alphanumeric) name.\nProperty Name\nMeaning\nRequired\nfs.swift.service.PROVIDER.auth.url\nKeystone Authentication URL\nMandatory\nfs.swift.service.PROVIDER.auth.endpoint.prefix\nKeystone endpoints prefix\nOptional\nfs.swift.service.PROVIDER.tenant\nTenant\nMandatory\nfs.swift.service.PROVIDER.username\nUsername\nMandatory\nfs.swift.service.PROVIDER.password\nPassword\nMandatory\nfs.swift.service.PROVIDER.http.port\nHTTP port\nMandatory\nfs.swift.service.PROVIDER.region\nKeystone region\nMandatory\nfs.swift.service.PROVIDER.public\nIndicates whether to use the public (off cloud) or private (in cloud; no transfer fees) endpoints\nMandatory\nFor example, assume\nPROVIDER=SparkTest\nand Keystone contains user\ntester\nwith password\ntesting\ndefined for tenant\ntest\n. Then\ncore-site.xml\nshould include:\n<configuration>\n<property>\n<name>\nfs.swift.service.SparkTest.auth.url\n</name>\n<value>\nhttp://127.0.0.1:5000/v2.0/tokens\n</value>\n</property>\n<property>\n<name>\nfs.swift.service.SparkTest.auth.endpoint.prefix\n</name>\n<value>\nendpoints\n</value>\n</property>\n<name>\nfs.swift.service.SparkTest.http.port\n</name>\n<value>\n8080\n</value>\n</property>\n<property>\n<name>\nfs.swift.service.SparkTest.region\n</name>\n<value>\nRegionOne\n</value>\n</property>\n<property>\n<name>\nfs.swift.service.SparkTest.public\n</name>\n<value>\ntrue\n</value>\n</property>\n<property>\n<name>\nfs.swift.service.SparkTest.tenant\n</name>\n<value>\ntest\n</value>\n</property>\n<property>\n<name>\nfs.swift.service.SparkTest.username\n</name>\n<value>\ntester\n</value>\n</property>\n<property>\n<name>\nfs.swift.service.SparkTest.password\n</name>\n<value>\ntesting\n</value>\n</property>\n</configuration>\nNotice that\nfs.swift.service.PROVIDER.tenant\n,\nfs.swift.service.PROVIDER.username\n,\nfs.swift.service.PROVIDER.password\ncontains sensitive information and keeping them in\ncore-site.xml\nis not always a good approach.\nWe suggest to keep those parameters in\ncore-site.xml\nfor testing purposes when running Spark\nvia\nspark-shell\n.\nFor job submissions they should be provided via\nsparkContext.hadoopConfiguration\n."}
{"url": "https://spark.apache.org/docs/latest/hardware-provisioning.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nHardware Provisioning\nA common question received by Spark developers is how to configure hardware for it. While the right\nhardware will depend on the situation, we make the following recommendations.\nStorage Systems\nBecause most Spark jobs will likely have to read input data from an external storage system (e.g.\nthe Hadoop File System, or HBase), it is important to place it\nas close to this system as\npossible\n. We recommend the following:\nIf at all possible, run Spark on the same nodes as HDFS. The simplest way is to set up a Spark\nstandalone mode cluster\non the same nodes, and configure Spark and\nHadoop’s memory and CPU usage to avoid interference (for Hadoop, the relevant options are\nmapred.child.java.opts\nfor the per-task memory and\nmapreduce.tasktracker.map.tasks.maximum\nand\nmapreduce.tasktracker.reduce.tasks.maximum\nfor number of tasks). Alternatively, you can run\nHadoop and Spark on a common cluster manager like\nHadoop YARN\n.\nIf this is not possible, run Spark on different nodes in the same local-area network as HDFS.\nFor low-latency data stores like HBase, it may be preferable to run computing jobs on different\nnodes than the storage system to avoid interference.\nLocal Disks\nWhile Spark can perform a lot of its computation in memory, it still uses local disks to store\ndata that doesn’t fit in RAM, as well as to preserve intermediate output between stages. We\nrecommend having\n4-8 disks\nper node, configured\nwithout\nRAID (just as separate mount points).\nIn Linux, mount the disks with the\nnoatime\noption\nto reduce unnecessary writes. In Spark,\nconfigure\nthe\nspark.local.dir\nvariable to be a comma-separated list of the local disks. If you are running HDFS, it’s fine to\nuse the same disks as HDFS.\nMemory\nIn general, Spark can run well with anywhere from\n8 GiB to hundreds of gigabytes\nof memory per\nmachine. In all cases, we recommend allocating only at most 75% of the memory for Spark; leave the\nrest for the operating system and buffer cache.\nHow much memory you will need will depend on your application. To determine how much your\napplication uses for a certain dataset size, load part of your dataset in a Spark RDD and use the\nStorage tab of Spark’s monitoring UI (\nhttp://<driver-node>:4040\n) to see its size in memory.\nNote that memory usage is greatly affected by storage level and serialization format – see\nthe\ntuning guide\nfor tips on how to reduce it.\nFinally, note that the Java VM does not always behave well with more than 200 GiB of RAM. If you\npurchase machines with more RAM than this, you can launch multiple executors in a single node. In\nSpark’s\nstandalone mode\n, a worker is responsible for launching multiple\nexecutors according to its available memory and cores, and each executor will be launched in a\nseparate Java VM.\nNetwork\nIn our experience, when the data is in memory, a lot of Spark applications are network-bound.\nUsing a\n10 Gigabit\nor higher network is the best way to make these applications faster.\nThis is especially true for “distributed reduce” applications such as group-bys, reduce-bys, and\nSQL joins. In any given application, you can see how much data Spark shuffles across the network\nfrom the application’s monitoring UI (\nhttp://<driver-node>:4040\n).\nCPU Cores\nSpark scales well to tens of CPU cores per machine because it performs minimal sharing between\nthreads. You should likely provision at least\n8-16 cores\nper machine. Depending on the CPU\ncost of your workload, you may also need more: once data is in memory, most applications are\neither CPU- or network-bound."}
{"url": "https://spark.apache.org/docs/latest/rdd-programming-guide.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nRDD Programming Guide\nOverview\nLinking with Spark\nInitializing Spark\nUsing the Shell\nResilient Distributed Datasets (RDDs)\nParallelized Collections\nExternal Datasets\nRDD Operations\nBasics\nPassing Functions to Spark\nUnderstanding closures\nExample\nLocal vs. cluster modes\nPrinting elements of an RDD\nWorking with Key-Value Pairs\nTransformations\nActions\nShuffle operations\nBackground\nPerformance Impact\nRDD Persistence\nWhich Storage Level to Choose?\nRemoving Data\nShared Variables\nBroadcast Variables\nAccumulators\nDeploying to a Cluster\nLaunching Spark jobs from Java / Scala\nUnit Testing\nWhere to Go from Here\nOverview\nAt a high level, every Spark application consists of a\ndriver program\nthat runs the user’s\nmain\nfunction and executes various\nparallel operations\non a cluster. The main abstraction Spark provides is a\nresilient distributed dataset\n(RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. RDDs are created by starting with a file in the Hadoop file system (or any other Hadoop-supported file system), or an existing Scala collection in the driver program, and transforming it. Users may also ask Spark to\npersist\nan RDD in memory, allowing it to be reused efficiently across parallel operations. Finally, RDDs automatically recover from node failures.\nA second abstraction in Spark is\nshared variables\nthat can be used in parallel operations. By default, when Spark runs a function in parallel as a set of tasks on different nodes, it ships a copy of each variable used in the function to each task. Sometimes, a variable needs to be shared across tasks, or between tasks and the driver program. Spark supports two types of shared variables:\nbroadcast variables\n, which can be used to cache a value in memory on all nodes, and\naccumulators\n, which are variables that are only “added” to, such as counters and sums.\nThis guide shows each of these features in each of Spark’s supported languages. It is easiest to follow\nalong with if you launch Spark’s interactive shell – either\nbin/spark-shell\nfor the Scala shell or\nbin/pyspark\nfor the Python one.\nLinking with Spark\nSpark 4.0.0 works with Python 3.9+. It can use the standard CPython interpreter,\nso C libraries like NumPy can be used. It also works with PyPy 7.3.6+.\nSpark applications in Python can either be run with the\nbin/spark-submit\nscript which includes Spark at runtime, or by including it in your setup.py as:\ninstall_requires\n=\n[\n'\npyspark==4.0.0\n'\n]\nTo run Spark applications in Python without pip installing PySpark, use the\nbin/spark-submit\nscript located in the Spark directory.\nThis script will load Spark’s Java/Scala libraries and allow you to submit applications to a cluster.\nYou can also use\nbin/pyspark\nto launch an interactive Python shell.\nIf you wish to access HDFS data, you need to use a build of PySpark linking\nto your version of HDFS.\nPrebuilt packages\nare also available on the Spark homepage\nfor common HDFS versions.\nFinally, you need to import some Spark classes into your program. Add the following line:\nfrom\npyspark\nimport\nSparkContext\n,\nSparkConf\nPySpark requires the same minor version of Python in both driver and workers. It uses the default python version in PATH,\nyou can specify which version of Python you want to use by\nPYSPARK_PYTHON\n, for example:\n$ PYSPARK_PYTHON\n=\npython3.8 bin/pyspark\n$ PYSPARK_PYTHON\n=\n/path-to-your-pypy/pypy bin/spark-submit examples/src/main/python/pi.py\nSpark 4.0.0 is built and distributed to work with Scala 2.13\nby default. (Spark can be built to work with other versions of Scala, too.) To write\napplications in Scala, you will need to use a compatible Scala version (e.g. 2.13.X).\nTo write a Spark application, you need to add a Maven dependency on Spark. Spark is available through Maven Central at:\ngroupId = org.apache.spark\nartifactId = spark-core_2.13\nversion = 4.0.0\nIn addition, if you wish to access an HDFS cluster, you need to add a dependency on\nhadoop-client\nfor your version of HDFS.\ngroupId = org.apache.hadoop\nartifactId = hadoop-client\nversion = <your-hdfs-version>\nFinally, you need to import some Spark classes into your program. Add the following lines:\nimport\norg.apache.spark.SparkContext\nimport\norg.apache.spark.SparkConf\n(Before Spark 1.3.0, you need to explicitly\nimport org.apache.spark.SparkContext._\nto enable essential implicit conversions.)\nSpark 4.0.0 supports\nlambda expressions\nfor concisely writing functions, otherwise you can use the classes in the\norg.apache.spark.api.java.function\npackage.\nNote that support for Java 7 was removed in Spark 2.2.0.\nTo write a Spark application in Java, you need to add a dependency on Spark. Spark is available through Maven Central at:\ngroupId = org.apache.spark\nartifactId = spark-core_2.13\nversion = 4.0.0\nIn addition, if you wish to access an HDFS cluster, you need to add a dependency on\nhadoop-client\nfor your version of HDFS.\ngroupId = org.apache.hadoop\nartifactId = hadoop-client\nversion = <your-hdfs-version>\nFinally, you need to import some Spark classes into your program. Add the following lines:\nimport\norg.apache.spark.api.java.JavaSparkContext\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.SparkConf\n;\nInitializing Spark\nThe first thing a Spark program must do is to create a\nSparkContext\nobject, which tells Spark\nhow to access a cluster. To create a\nSparkContext\nyou first need to build a\nSparkConf\nobject\nthat contains information about your application.\nconf\n=\nSparkConf\n().\nsetAppName\n(\nappName\n).\nsetMaster\n(\nmaster\n)\nsc\n=\nSparkContext\n(\nconf\n=\nconf\n)\nThe first thing a Spark program must do is to create a\nSparkContext\nobject, which tells Spark\nhow to access a cluster. To create a\nSparkContext\nyou first need to build a\nSparkConf\nobject\nthat contains information about your application.\nOnly one SparkContext should be active per JVM. You must\nstop()\nthe active SparkContext before creating a new one.\nval\nconf\n=\nnew\nSparkConf\n().\nsetAppName\n(\nappName\n).\nsetMaster\n(\nmaster\n)\nnew\nSparkContext\n(\nconf\n)\nThe first thing a Spark program must do is to create a\nJavaSparkContext\nobject, which tells Spark\nhow to access a cluster. To create a\nSparkContext\nyou first need to build a\nSparkConf\nobject\nthat contains information about your application.\nSparkConf\nconf\n=\nnew\nSparkConf\n().\nsetAppName\n(\nappName\n).\nsetMaster\n(\nmaster\n);\nJavaSparkContext\nsc\n=\nnew\nJavaSparkContext\n(\nconf\n);\nThe\nappName\nparameter is a name for your application to show on the cluster UI.\nmaster\nis a\nSpark or YARN cluster URL\n,\nor a special “local” string to run in local mode.\nIn practice, when running on a cluster, you will not want to hardcode\nmaster\nin the program,\nbut rather\nlaunch the application with\nspark-submit\nand\nreceive it there. However, for local testing and unit tests, you can pass “local” to run Spark\nin-process.\nUsing the Shell\nIn the PySpark shell, a special interpreter-aware SparkContext is already created for you, in the\nvariable called\nsc\n. Making your own SparkContext will not work. You can set which master the\ncontext connects to using the\n--master\nargument, and you can add Python .zip, .egg or .py files\nto the runtime path by passing a comma-separated list to\n--py-files\n. For third-party Python dependencies,\nsee\nPython Package Management\n. You can also add dependencies\n(e.g. Spark Packages) to your shell session by supplying a comma-separated list of Maven coordinates\nto the\n--packages\nargument. Any additional repositories where dependencies might exist (e.g. Sonatype)\ncan be passed to the\n--repositories\nargument. For example, to run\nbin/pyspark\non exactly four cores, use:\n$\n./bin/pyspark\n--master\n\"local[4]\"\nOr, to also add\ncode.py\nto the search path (in order to later be able to\nimport code\n), use:\n$\n./bin/pyspark\n--master\n\"local[4]\"\n--py-files\ncode.py\nFor a complete list of options, run\npyspark --help\n. Behind the scenes,\npyspark\ninvokes the more general\nspark-submit\nscript\n.\nIt is also possible to launch the PySpark shell in\nIPython\n, the\nenhanced Python interpreter. PySpark works with IPython 1.0.0 and later. To\nuse IPython, set the\nPYSPARK_DRIVER_PYTHON\nvariable to\nipython\nwhen running\nbin/pyspark\n:\n$ PYSPARK_DRIVER_PYTHON\n=\nipython ./bin/pyspark\nTo use the Jupyter notebook (previously known as the IPython notebook),\n$ PYSPARK_DRIVER_PYTHON\n=\njupyter\nPYSPARK_DRIVER_PYTHON_OPTS\n=\nnotebook ./bin/pyspark\nYou can customize the\nipython\nor\njupyter\ncommands by setting\nPYSPARK_DRIVER_PYTHON_OPTS\n.\nAfter the Jupyter Notebook server is launched, you can create a new notebook from\nthe “Files” tab. Inside the notebook, you can input the command\n%pylab inline\nas part of\nyour notebook before you start to try Spark from the Jupyter notebook.\nIn the Spark shell, a special interpreter-aware SparkContext is already created for you, in the\nvariable called\nsc\n. Making your own SparkContext will not work. You can set which master the\ncontext connects to using the\n--master\nargument, and you can add JARs to the classpath\nby passing a comma-separated list to the\n--jars\nargument. You can also add dependencies\n(e.g. Spark Packages) to your shell session by supplying a comma-separated list of Maven coordinates\nto the\n--packages\nargument. Any additional repositories where dependencies might exist (e.g. Sonatype)\ncan be passed to the\n--repositories\nargument. For example, to run\nbin/spark-shell\non exactly\nfour cores, use:\n$\n./bin/spark-shell\n--master\n\"local[4]\"\nOr, to also add\ncode.jar\nto its classpath, use:\n$\n./bin/spark-shell\n--master\n\"local[4]\"\n--jars\ncode.jar\nTo include a dependency using Maven coordinates:\n$\n./bin/spark-shell\n--master\n\"local[4]\"\n--packages\n\"org.example:example:0.1\"\nFor a complete list of options, run\nspark-shell --help\n. Behind the scenes,\nspark-shell\ninvokes the more general\nspark-submit\nscript\n.\nResilient Distributed Datasets (RDDs)\nSpark revolves around the concept of a\nresilient distributed dataset\n(RDD), which is a fault-tolerant collection of elements that can be operated on in parallel. There are two ways to create RDDs:\nparallelizing\nan existing collection in your driver program, or referencing a dataset in an external storage system, such as a\nshared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.\nParallelized Collections\nParallelized collections are created by calling\nSparkContext\n’s\nparallelize\nmethod on an existing iterable or collection in your driver program. The elements of the collection are copied to form a distributed dataset that can be operated on in parallel. For example, here is how to create a parallelized collection holding the numbers 1 to 5:\ndata\n=\n[\n1\n,\n2\n,\n3\n,\n4\n,\n5\n]\ndistData\n=\nsc\n.\nparallelize\n(\ndata\n)\nOnce created, the distributed dataset (\ndistData\n) can be operated on in parallel. For example, we can call\ndistData.reduce(lambda a, b: a + b)\nto add up the elements of the list.\nWe describe operations on distributed datasets later on.\nParallelized collections are created by calling\nSparkContext\n’s\nparallelize\nmethod on an existing collection in your driver program (a Scala\nSeq\n). The elements of the collection are copied to form a distributed dataset that can be operated on in parallel. For example, here is how to create a parallelized collection holding the numbers 1 to 5:\nval\ndata\n=\nArray\n(\n1\n,\n2\n,\n3\n,\n4\n,\n5\n)\nval\ndistData\n=\nsc\n.\nparallelize\n(\ndata\n)\nOnce created, the distributed dataset (\ndistData\n) can be operated on in parallel. For example, we might call\ndistData.reduce((a, b) => a + b)\nto add up the elements of the array. We describe operations on distributed datasets later on.\nParallelized collections are created by calling\nJavaSparkContext\n’s\nparallelize\nmethod on an existing\nCollection\nin your driver program. The elements of the collection are copied to form a distributed dataset that can be operated on in parallel. For example, here is how to create a parallelized collection holding the numbers 1 to 5:\nList\n<\nInteger\n>\ndata\n=\nArrays\n.\nasList\n(\n1\n,\n2\n,\n3\n,\n4\n,\n5\n);\nJavaRDD\n<\nInteger\n>\ndistData\n=\nsc\n.\nparallelize\n(\ndata\n);\nOnce created, the distributed dataset (\ndistData\n) can be operated on in parallel. For example, we might call\ndistData.reduce((a, b) -> a + b)\nto add up the elements of the list.\nWe describe operations on distributed datasets later on.\nOne important parameter for parallel collections is the number of\npartitions\nto cut the dataset into. Spark will run one task for each partition of the cluster. Typically you want 2-4 partitions for each CPU in your cluster. Normally, Spark tries to set the number of partitions automatically based on your cluster. However, you can also set it manually by passing it as a second parameter to\nparallelize\n(e.g.\nsc.parallelize(data, 10)\n). Note: some places in the code use the term slices (a synonym for partitions) to maintain backward compatibility.\nExternal Datasets\nPySpark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase,\nAmazon S3\n, etc. Spark supports text files,\nSequenceFiles\n, and any other Hadoop\nInputFormat\n.\nText file RDDs can be created using\nSparkContext\n’s\ntextFile\nmethod. This method takes a URI for the file (either a local path on the machine, or a\nhdfs://\n,\ns3a://\n, etc URI) and reads it as a collection of lines. Here is an example invocation:\n>>>\ndistFile\n=\nsc\n.\ntextFile\n(\n\"\ndata.txt\n\"\n)\nOnce created,\ndistFile\ncan be acted on by dataset operations. For example, we can add up the sizes of all the lines using the\nmap\nand\nreduce\noperations as follows:\ndistFile.map(lambda s: len(s)).reduce(lambda a, b: a + b)\n.\nSome notes on reading files with Spark:\nIf using a path on the local filesystem, the file must also be accessible at the same path on worker nodes. Either copy the file to all workers or use a network-mounted shared file system.\nAll of Spark’s file-based input methods, including\ntextFile\n, support running on directories, compressed files, and wildcards as well. For example, you can use\ntextFile(\"/my/directory\")\n,\ntextFile(\"/my/directory/*.txt\")\n, and\ntextFile(\"/my/directory/*.gz\")\n.\nThe\ntextFile\nmethod also takes an optional second argument for controlling the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Note that you cannot have fewer partitions than blocks.\nApart from text files, Spark’s Python API also supports several other data formats:\nSparkContext.wholeTextFiles\nlets you read a directory containing multiple small text files, and returns each of them as (filename, content) pairs. This is in contrast with\ntextFile\n, which would return one record per line in each file.\nRDD.saveAsPickleFile\nand\nSparkContext.pickleFile\nsupport saving an RDD in a simple format consisting of pickled Python objects. Batching is used on pickle serialization, with default batch size 10.\nSequenceFile and Hadoop Input/Output Formats\nNote\nthis feature is currently marked\nExperimental\nand is intended for advanced users. It may be replaced in future with read/write support based on Spark SQL, in which case Spark SQL is the preferred approach.\nWritable Support\nPySpark SequenceFile support loads an RDD of key-value pairs within Java, converts Writables to base Java types, and pickles the\nresulting Java objects using\npickle\n. When saving an RDD of key-value pairs to SequenceFile,\nPySpark does the reverse. It unpickles Python objects into Java objects and then converts them to Writables. The following\nWritables are automatically converted:\nWritable Type\nPython Type\nText\nstr\nIntWritable\nint\nFloatWritable\nfloat\nDoubleWritable\nfloat\nBooleanWritable\nbool\nBytesWritable\nbytearray\nNullWritable\nNone\nMapWritable\ndict\nArrays are not handled out-of-the-box. Users need to specify custom\nArrayWritable\nsubtypes when reading or writing. When writing,\nusers also need to specify custom converters that convert arrays to custom\nArrayWritable\nsubtypes. When reading, the default\nconverter will convert custom\nArrayWritable\nsubtypes to Java\nObject[]\n, which then get pickled to Python tuples. To get\nPython\narray.array\nfor arrays of primitive types, users need to specify custom converters.\nSaving and Loading SequenceFiles\nSimilarly to text files, SequenceFiles can be saved and loaded by specifying the path. The key and value\nclasses can be specified, but for standard Writables this is not required.\n>>>\nrdd\n=\nsc\n.\nparallelize\n(\nrange\n(\n1\n,\n4\n)).\nmap\n(\nlambda\nx\n:\n(\nx\n,\n\"\na\n\"\n*\nx\n))\n>>>\nrdd\n.\nsaveAsSequenceFile\n(\n\"\npath/to/file\n\"\n)\n>>>\nsorted\n(\nsc\n.\nsequenceFile\n(\n\"\npath/to/file\n\"\n).\ncollect\n())\n[(\n1\n,\nu\n'\na\n'\n),\n(\n2\n,\nu\n'\naa\n'\n),\n(\n3\n,\nu\n'\naaa\n'\n)]\nSaving and Loading Other Hadoop Input/Output Formats\nPySpark can also read any Hadoop InputFormat or write any Hadoop OutputFormat, for both ‘new’ and ‘old’ Hadoop MapReduce APIs.\nIf required, a Hadoop configuration can be passed in as a Python dict. Here is an example using the\nElasticsearch ESInputFormat:\n$\n.\n/\nbin\n/\npyspark\n--\njars\n/\npath\n/\nto\n/\nelasticsearch\n-\nhadoop\n.\njar\n>>>\nconf\n=\n{\n\"\nes.resource\n\"\n:\n\"\nindex/type\n\"\n}\n# assume Elasticsearch is running on localhost defaults\n>>>\nrdd\n=\nsc\n.\nnewAPIHadoopRDD\n(\n\"\norg.elasticsearch.hadoop.mr.EsInputFormat\n\"\n,\n\"\norg.apache.hadoop.io.NullWritable\n\"\n,\n\"\norg.elasticsearch.hadoop.mr.LinkedMapWritable\n\"\n,\nconf\n=\nconf\n)\n>>>\nrdd\n.\nfirst\n()\n# the result is a MapWritable that is converted to a Python dict\n(\nu\n'\nElasticsearch ID\n'\n,\n{\nu\n'\nfield1\n'\n:\nTrue\n,\nu\n'\nfield2\n'\n:\nu\n'\nSome Text\n'\n,\nu\n'\nfield3\n'\n:\n12345\n})\nNote that, if the InputFormat simply depends on a Hadoop configuration and/or input path, and\nthe key and value classes can easily be converted according to the above table,\nthen this approach should work well for such cases.\nIf you have custom serialized binary data (such as loading data from Cassandra / HBase), then you will first need to\ntransform that data on the Scala/Java side to something which can be handled by pickle’s pickler.\nA\nConverter\ntrait is provided\nfor this. Simply extend this trait and implement your transformation code in the\nconvert\nmethod. Remember to ensure that this class, along with any dependencies required to access your\nInputFormat\n, are packaged into your Spark job jar and included on the PySpark\nclasspath.\nSee the\nPython examples\nand\nthe\nConverter examples\nfor examples of using Cassandra / HBase\nInputFormat\nand\nOutputFormat\nwith custom converters.\nSpark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase,\nAmazon S3\n, etc. Spark supports text files,\nSequenceFiles\n, and any other Hadoop\nInputFormat\n.\nText file RDDs can be created using\nSparkContext\n’s\ntextFile\nmethod. This method takes a URI for the file (either a local path on the machine, or a\nhdfs://\n,\ns3a://\n, etc URI) and reads it as a collection of lines. Here is an example invocation:\nscala\n>\nval\ndistFile\n=\nsc\n.\ntextFile\n(\n\"data.txt\"\n)\ndistFile\n:\norg.apache.spark.rdd.RDD\n[\nString\n]\n=\ndata\n.\ntxt\nMapPartitionsRDD\n[\n10\n]\nat\ntextFile\nat\n<\nconsole\n>:\n26\nOnce created,\ndistFile\ncan be acted on by dataset operations. For example, we can add up the sizes of all the lines using the\nmap\nand\nreduce\noperations as follows:\ndistFile.map(s => s.length).reduce((a, b) => a + b)\n.\nSome notes on reading files with Spark:\nIf using a path on the local filesystem, the file must also be accessible at the same path on worker nodes. Either copy the file to all workers or use a network-mounted shared file system.\nAll of Spark’s file-based input methods, including\ntextFile\n, support running on directories, compressed files, and wildcards as well. For example, you can use\ntextFile(\"/my/directory\")\n,\ntextFile(\"/my/directory/*.txt\")\n, and\ntextFile(\"/my/directory/*.gz\")\n. When multiple files are read, the order of the partitions depends on the order the files are returned from the filesystem. It may or may not, for example, follow the lexicographic ordering of the files by path. Within a partition, elements are ordered according to their order in the underlying file.\nThe\ntextFile\nmethod also takes an optional second argument for controlling the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Note that you cannot have fewer partitions than blocks.\nApart from text files, Spark’s Scala API also supports several other data formats:\nSparkContext.wholeTextFiles\nlets you read a directory containing multiple small text files, and returns each of them as (filename, content) pairs. This is in contrast with\ntextFile\n, which would return one record per line in each file. Partitioning is determined by data locality which, in some cases, may result in too few partitions. For those cases,\nwholeTextFiles\nprovides an optional second argument for controlling the minimal number of partitions.\nFor\nSequenceFiles\n, use SparkContext’s\nsequenceFile[K, V]\nmethod where\nK\nand\nV\nare the types of key and values in the file. These should be subclasses of Hadoop’s\nWritable\ninterface, like\nIntWritable\nand\nText\n. In addition, Spark allows you to specify native types for a few common Writables; for example,\nsequenceFile[Int, String]\nwill automatically read IntWritables and Texts.\nFor other Hadoop InputFormats, you can use the\nSparkContext.hadoopRDD\nmethod, which takes an arbitrary\nJobConf\nand input format class, key class and value class. Set these the same way you would for a Hadoop job with your input source. You can also use\nSparkContext.newAPIHadoopRDD\nfor InputFormats based on the “new” MapReduce API (\norg.apache.hadoop.mapreduce\n).\nRDD.saveAsObjectFile\nand\nSparkContext.objectFile\nsupport saving an RDD in a simple format consisting of serialized Java objects. While this is not as efficient as specialized formats like Avro, it offers an easy way to save any RDD.\nSpark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase,\nAmazon S3\n, etc. Spark supports text files,\nSequenceFiles\n, and any other Hadoop\nInputFormat\n.\nText file RDDs can be created using\nSparkContext\n’s\ntextFile\nmethod. This method takes a URI for the file (either a local path on the machine, or a\nhdfs://\n,\ns3a://\n, etc URI) and reads it as a collection of lines. Here is an example invocation:\nJavaRDD\n<\nString\n>\ndistFile\n=\nsc\n.\ntextFile\n(\n\"data.txt\"\n);\nOnce created,\ndistFile\ncan be acted on by dataset operations. For example, we can add up the sizes of all the lines using the\nmap\nand\nreduce\noperations as follows:\ndistFile.map(s -> s.length()).reduce((a, b) -> a + b)\n.\nSome notes on reading files with Spark:\nIf using a path on the local filesystem, the file must also be accessible at the same path on worker nodes. Either copy the file to all workers or use a network-mounted shared file system.\nAll of Spark’s file-based input methods, including\ntextFile\n, support running on directories, compressed files, and wildcards as well. For example, you can use\ntextFile(\"/my/directory\")\n,\ntextFile(\"/my/directory/*.txt\")\n, and\ntextFile(\"/my/directory/*.gz\")\n.\nThe\ntextFile\nmethod also takes an optional second argument for controlling the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Note that you cannot have fewer partitions than blocks.\nApart from text files, Spark’s Java API also supports several other data formats:\nJavaSparkContext.wholeTextFiles\nlets you read a directory containing multiple small text files, and returns each of them as (filename, content) pairs. This is in contrast with\ntextFile\n, which would return one record per line in each file.\nFor\nSequenceFiles\n, use SparkContext’s\nsequenceFile[K, V]\nmethod where\nK\nand\nV\nare the types of key and values in the file. These should be subclasses of Hadoop’s\nWritable\ninterface, like\nIntWritable\nand\nText\n.\nFor other Hadoop InputFormats, you can use the\nJavaSparkContext.hadoopRDD\nmethod, which takes an arbitrary\nJobConf\nand input format class, key class and value class. Set these the same way you would for a Hadoop job with your input source. You can also use\nJavaSparkContext.newAPIHadoopRDD\nfor InputFormats based on the “new” MapReduce API (\norg.apache.hadoop.mapreduce\n).\nJavaRDD.saveAsObjectFile\nand\nJavaSparkContext.objectFile\nsupport saving an RDD in a simple format consisting of serialized Java objects. While this is not as efficient as specialized formats like Avro, it offers an easy way to save any RDD.\nRDD Operations\nRDDs support two types of operations:\ntransformations\n, which create a new dataset from an existing one, and\nactions\n, which return a value to the driver program after running a computation on the dataset. For example,\nmap\nis a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand,\nreduce\nis an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel\nreduceByKey\nthat returns a distributed dataset).\nAll transformations in Spark are\nlazy\n, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently. For example, we can realize that a dataset created through\nmap\nwill be used in a\nreduce\nand return only the result of the\nreduce\nto the driver, rather than the larger mapped dataset.\nBy default, each transformed RDD may be recomputed each time you run an action on it. However, you may also\npersist\nan RDD in memory using the\npersist\n(or\ncache\n) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes.\nBasics\nTo illustrate RDD basics, consider the simple program below:\nlines\n=\nsc\n.\ntextFile\n(\n\"\ndata.txt\n\"\n)\nlineLengths\n=\nlines\n.\nmap\n(\nlambda\ns\n:\nlen\n(\ns\n))\ntotalLength\n=\nlineLengths\n.\nreduce\n(\nlambda\na\n,\nb\n:\na\n+\nb\n)\nThe first line defines a base RDD from an external file. This dataset is not loaded in memory or\notherwise acted on:\nlines\nis merely a pointer to the file.\nThe second line defines\nlineLengths\nas the result of a\nmap\ntransformation. Again,\nlineLengths\nis\nnot\nimmediately computed, due to laziness.\nFinally, we run\nreduce\n, which is an action. At this point Spark breaks the computation into tasks\nto run on separate machines, and each machine runs both its part of the map and a local reduction,\nreturning only its answer to the driver program.\nIf we also wanted to use\nlineLengths\nagain later, we could add:\nlineLengths\n.\npersist\n()\nbefore the\nreduce\n, which would cause\nlineLengths\nto be saved in memory after the first time it is computed.\nTo illustrate RDD basics, consider the simple program below:\nval\nlines\n=\nsc\n.\ntextFile\n(\n\"data.txt\"\n)\nval\nlineLengths\n=\nlines\n.\nmap\n(\ns\n=>\ns\n.\nlength\n)\nval\ntotalLength\n=\nlineLengths\n.\nreduce\n((\na\n,\nb\n)\n=>\na\n+\nb\n)\nThe first line defines a base RDD from an external file. This dataset is not loaded in memory or\notherwise acted on:\nlines\nis merely a pointer to the file.\nThe second line defines\nlineLengths\nas the result of a\nmap\ntransformation. Again,\nlineLengths\nis\nnot\nimmediately computed, due to laziness.\nFinally, we run\nreduce\n, which is an action. At this point Spark breaks the computation into tasks\nto run on separate machines, and each machine runs both its part of the map and a local reduction,\nreturning only its answer to the driver program.\nIf we also wanted to use\nlineLengths\nagain later, we could add:\nlineLengths\n.\npersist\n()\nbefore the\nreduce\n, which would cause\nlineLengths\nto be saved in memory after the first time it is computed.\nTo illustrate RDD basics, consider the simple program below:\nJavaRDD\n<\nString\n>\nlines\n=\nsc\n.\ntextFile\n(\n\"data.txt\"\n);\nJavaRDD\n<\nInteger\n>\nlineLengths\n=\nlines\n.\nmap\n(\ns\n->\ns\n.\nlength\n());\nint\ntotalLength\n=\nlineLengths\n.\nreduce\n((\na\n,\nb\n)\n->\na\n+\nb\n);\nThe first line defines a base RDD from an external file. This dataset is not loaded in memory or\notherwise acted on:\nlines\nis merely a pointer to the file.\nThe second line defines\nlineLengths\nas the result of a\nmap\ntransformation. Again,\nlineLengths\nis\nnot\nimmediately computed, due to laziness.\nFinally, we run\nreduce\n, which is an action. At this point Spark breaks the computation into tasks\nto run on separate machines, and each machine runs both its part of the map and a local reduction,\nreturning only its answer to the driver program.\nIf we also wanted to use\nlineLengths\nagain later, we could add:\nlineLengths\n.\npersist\n(\nStorageLevel\n.\nMEMORY_ONLY\n());\nbefore the\nreduce\n, which would cause\nlineLengths\nto be saved in memory after the first time it is computed.\nPassing Functions to Spark\nSpark’s API relies heavily on passing functions in the driver program to run on the cluster.\nThere are three recommended ways to do this:\nLambda expressions\n,\nfor simple functions that can be written as an expression. (Lambdas do not support multi-statement\nfunctions or statements that do not return a value.)\nLocal\ndef\ns inside the function calling into Spark, for longer code.\nTop-level functions in a module.\nFor example, to pass a longer function than can be supported using a\nlambda\n, consider\nthe code below:\n\"\"\"\nMyScript.py\n\"\"\"\nif\n__name__\n==\n\"\n__main__\n\"\n:\ndef\nmyFunc\n(\ns\n):\nwords\n=\ns\n.\nsplit\n(\n\"\n\"\n)\nreturn\nlen\n(\nwords\n)\nsc\n=\nSparkContext\n(...)\nsc\n.\ntextFile\n(\n\"\nfile.txt\n\"\n).\nmap\n(\nmyFunc\n)\nNote that while it is also possible to pass a reference to a method in a class instance (as opposed to\na singleton object), this requires sending the object that contains that class along with the method.\nFor example, consider:\nclass\nMyClass\n(\nobject\n):\ndef\nfunc\n(\nself\n,\ns\n):\nreturn\ns\ndef\ndoStuff\n(\nself\n,\nrdd\n):\nreturn\nrdd\n.\nmap\n(\nself\n.\nfunc\n)\nHere, if we create a\nnew MyClass\nand call\ndoStuff\non it, the\nmap\ninside there references the\nfunc\nmethod\nof that\nMyClass\ninstance\n, so the whole object needs to be sent to the cluster.\nIn a similar way, accessing fields of the outer object will reference the whole object:\nclass\nMyClass\n(\nobject\n):\ndef\n__init__\n(\nself\n):\nself\n.\nfield\n=\n\"\nHello\n\"\ndef\ndoStuff\n(\nself\n,\nrdd\n):\nreturn\nrdd\n.\nmap\n(\nlambda\ns\n:\nself\n.\nfield\n+\ns\n)\nTo avoid this issue, the simplest way is to copy\nfield\ninto a local variable instead\nof accessing it externally:\ndef\ndoStuff\n(\nself\n,\nrdd\n):\nfield\n=\nself\n.\nfield\nreturn\nrdd\n.\nmap\n(\nlambda\ns\n:\nfield\n+\ns\n)\nSpark’s API relies heavily on passing functions in the driver program to run on the cluster.\nThere are two recommended ways to do this:\nAnonymous function syntax\n,\nwhich can be used for short pieces of code.\nStatic methods in a global singleton object. For example, you can define\nobject MyFunctions\nand then\npass\nMyFunctions.func1\n, as follows:\nobject\nMyFunctions\n{\ndef\nfunc1\n(\ns\n:\nString\n)\n:\nString\n=\n{\n...\n}\n}\nmyRdd\n.\nmap\n(\nMyFunctions\n.\nfunc1\n)\nNote that while it is also possible to pass a reference to a method in a class instance (as opposed to\na singleton object), this requires sending the object that contains that class along with the method.\nFor example, consider:\nclass\nMyClass\n{\ndef\nfunc1\n(\ns\n:\nString\n)\n:\nString\n=\n{\n...\n}\ndef\ndoStuff\n(\nrdd\n:\nRDD\n[\nString\n])\n:\nRDD\n[\nString\n]\n=\n{\nrdd\n.\nmap\n(\nfunc1\n)\n}\n}\nHere, if we create a new\nMyClass\ninstance and call\ndoStuff\non it, the\nmap\ninside there references the\nfunc1\nmethod\nof that\nMyClass\ninstance\n, so the whole object needs to be sent to the cluster. It is\nsimilar to writing\nrdd.map(x => this.func1(x))\n.\nIn a similar way, accessing fields of the outer object will reference the whole object:\nclass\nMyClass\n{\nval\nfield\n=\n\"Hello\"\ndef\ndoStuff\n(\nrdd\n:\nRDD\n[\nString\n])\n:\nRDD\n[\nString\n]\n=\n{\nrdd\n.\nmap\n(\nx\n=>\nfield\n+\nx\n)\n}\n}\nis equivalent to writing\nrdd.map(x => this.field + x)\n, which references all of\nthis\n. To avoid this\nissue, the simplest way is to copy\nfield\ninto a local variable instead of accessing it externally:\ndef\ndoStuff\n(\nrdd\n:\nRDD\n[\nString\n])\n:\nRDD\n[\nString\n]\n=\n{\nval\nfield_\n=\nthis\n.\nfield\nrdd\n.\nmap\n(\nx\n=>\nfield_\n+\nx\n)\n}\nSpark’s API relies heavily on passing functions in the driver program to run on the cluster.\nIn Java, functions are represented by classes implementing the interfaces in the\norg.apache.spark.api.java.function\npackage.\nThere are two ways to create such functions:\nImplement the Function interfaces in your own class, either as an anonymous inner class or a named one,\nand pass an instance of it to Spark.\nUse\nlambda expressions\nto concisely define an implementation.\nWhile much of this guide uses lambda syntax for conciseness, it is easy to use all the same APIs\nin long-form. For example, we could have written our code above as follows:\nJavaRDD\n<\nString\n>\nlines\n=\nsc\n.\ntextFile\n(\n\"data.txt\"\n);\nJavaRDD\n<\nInteger\n>\nlineLengths\n=\nlines\n.\nmap\n(\nnew\nFunction\n<\nString\n,\nInteger\n>()\n{\npublic\nInteger\ncall\n(\nString\ns\n)\n{\nreturn\ns\n.\nlength\n();\n}\n});\nint\ntotalLength\n=\nlineLengths\n.\nreduce\n(\nnew\nFunction2\n<\nInteger\n,\nInteger\n,\nInteger\n>()\n{\npublic\nInteger\ncall\n(\nInteger\na\n,\nInteger\nb\n)\n{\nreturn\na\n+\nb\n;\n}\n});\nOr, if writing the functions inline is unwieldy:\nclass\nGetLength\nimplements\nFunction\n<\nString\n,\nInteger\n>\n{\npublic\nInteger\ncall\n(\nString\ns\n)\n{\nreturn\ns\n.\nlength\n();\n}\n}\nclass\nSum\nimplements\nFunction2\n<\nInteger\n,\nInteger\n,\nInteger\n>\n{\npublic\nInteger\ncall\n(\nInteger\na\n,\nInteger\nb\n)\n{\nreturn\na\n+\nb\n;\n}\n}\nJavaRDD\n<\nString\n>\nlines\n=\nsc\n.\ntextFile\n(\n\"data.txt\"\n);\nJavaRDD\n<\nInteger\n>\nlineLengths\n=\nlines\n.\nmap\n(\nnew\nGetLength\n());\nint\ntotalLength\n=\nlineLengths\n.\nreduce\n(\nnew\nSum\n());\nNote that anonymous inner classes in Java can also access variables in the enclosing scope as long\nas they are marked\nfinal\n. Spark will ship copies of these variables to each worker node as it does\nfor other languages.\nUnderstanding closures\nOne of the harder things about Spark is understanding the scope and life cycle of variables and methods when executing code across a cluster. RDD operations that modify variables outside of their scope can be a frequent source of confusion. In the example below we’ll look at code that uses\nforeach()\nto increment a counter, but similar issues can occur for other operations as well.\nExample\nConsider the naive RDD element sum below, which may behave differently depending on whether execution is happening within the same JVM. A common example of this is when running Spark in\nlocal\nmode (\n--master = \"local[n]\"\n) versus deploying a Spark application to a cluster (e.g. via spark-submit to YARN):\ncounter\n=\n0\nrdd\n=\nsc\n.\nparallelize\n(\ndata\n)\n# Wrong: Don't do this!!\ndef\nincrement_counter\n(\nx\n):\nglobal\ncounter\ncounter\n+=\nx\nrdd\n.\nforeach\n(\nincrement_counter\n)\nprint\n(\n\"\nCounter value:\n\"\n,\ncounter\n)\nvar\ncounter\n=\n0\nvar\nrdd\n=\nsc\n.\nparallelize\n(\ndata\n)\n// Wrong: Don't do this!!\nrdd\n.\nforeach\n(\nx\n=>\ncounter\n+=\nx\n)\nprintln\n(\n\"Counter value: \"\n+\ncounter\n)\nint\ncounter\n=\n0\n;\nJavaRDD\n<\nInteger\n>\nrdd\n=\nsc\n.\nparallelize\n(\ndata\n);\n// Wrong: Don't do this!!\nrdd\n.\nforeach\n(\nx\n->\ncounter\n+=\nx\n);\nprintln\n(\n\"Counter value: \"\n+\ncounter\n);\nLocal vs. cluster modes\nThe behavior of the above code is undefined, and may not work as intended. To execute jobs, Spark breaks up the processing of RDD operations into tasks, each of which is executed by an executor. Prior to execution, Spark computes the task’s\nclosure\n. The closure is those variables and methods which must be visible for the executor to perform its computations on the RDD (in this case\nforeach()\n). This closure is serialized and sent to each executor.\nThe variables within the closure sent to each executor are now copies and thus, when\ncounter\nis referenced within the\nforeach\nfunction, it’s no longer the\ncounter\non the driver node. There is still a\ncounter\nin the memory of the driver node but this is no longer visible to the executors! The executors only see the copy from the serialized closure. Thus, the final value of\ncounter\nwill still be zero since all operations on\ncounter\nwere referencing the value within the serialized closure.\nIn local mode, in some circumstances, the\nforeach\nfunction will actually execute within the same JVM as the driver and will reference the same original\ncounter\n, and may actually update it.\nTo ensure well-defined behavior in these sorts of scenarios one should use an\nAccumulator\n. Accumulators in Spark are used specifically to provide a mechanism for safely updating a variable when execution is split up across worker nodes in a cluster. The Accumulators section of this guide discusses these in more detail.\nIn general, closures - constructs like loops or locally defined methods, should not be used to mutate some global state. Spark does not define or guarantee the behavior of mutations to objects referenced from outside of closures. Some code that does this may work in local mode, but that’s just by accident and such code will not behave as expected in distributed mode. Use an Accumulator instead if some global aggregation is needed.\nPrinting elements of an RDD\nAnother common idiom is attempting to print out the elements of an RDD using\nrdd.foreach(println)\nor\nrdd.map(println)\n. On a single machine, this will generate the expected output and print all the RDD’s elements. However, in\ncluster\nmode, the output to\nstdout\nbeing called by the executors is now writing to the executor’s\nstdout\ninstead, not the one on the driver, so\nstdout\non the driver won’t show these! To print all elements on the driver, one can use the\ncollect()\nmethod to first bring the RDD to the driver node thus:\nrdd.collect().foreach(println)\n. This can cause the driver to run out of memory, though, because\ncollect()\nfetches the entire RDD to a single machine; if you only need to print a few elements of the RDD, a safer approach is to use the\ntake()\n:\nrdd.take(100).foreach(println)\n.\nWorking with Key-Value Pairs\nWhile most Spark operations work on RDDs containing any type of objects, a few special operations are\nonly available on RDDs of key-value pairs.\nThe most common ones are distributed “shuffle” operations, such as grouping or aggregating the elements\nby a key.\nIn Python, these operations work on RDDs containing built-in Python tuples such as\n(1, 2)\n.\nSimply create such tuples and then call your desired operation.\nFor example, the following code uses the\nreduceByKey\noperation on key-value pairs to count how\nmany times each line of text occurs in a file:\nlines\n=\nsc\n.\ntextFile\n(\n\"\ndata.txt\n\"\n)\npairs\n=\nlines\n.\nmap\n(\nlambda\ns\n:\n(\ns\n,\n1\n))\ncounts\n=\npairs\n.\nreduceByKey\n(\nlambda\na\n,\nb\n:\na\n+\nb\n)\nWe could also use\ncounts.sortByKey()\n, for example, to sort the pairs alphabetically, and finally\ncounts.collect()\nto bring them back to the driver program as a list of objects.\nWhile most Spark operations work on RDDs containing any type of objects, a few special operations are\nonly available on RDDs of key-value pairs.\nThe most common ones are distributed “shuffle” operations, such as grouping or aggregating the elements\nby a key.\nIn Scala, these operations are automatically available on RDDs containing\nTuple2\nobjects\n(the built-in tuples in the language, created by simply writing\n(a, b)\n). The key-value pair operations are available in the\nPairRDDFunctions\nclass,\nwhich automatically wraps around an RDD of tuples.\nFor example, the following code uses the\nreduceByKey\noperation on key-value pairs to count how\nmany times each line of text occurs in a file:\nval\nlines\n=\nsc\n.\ntextFile\n(\n\"data.txt\"\n)\nval\npairs\n=\nlines\n.\nmap\n(\ns\n=>\n(\ns\n,\n1\n))\nval\ncounts\n=\npairs\n.\nreduceByKey\n((\na\n,\nb\n)\n=>\na\n+\nb\n)\nWe could also use\ncounts.sortByKey()\n, for example, to sort the pairs alphabetically, and finally\ncounts.collect()\nto bring them back to the driver program as an array of objects.\nNote:\nwhen using custom objects as the key in key-value pair operations, you must be sure that a\ncustom\nequals()\nmethod is accompanied with a matching\nhashCode()\nmethod.  For full details, see\nthe contract outlined in the\nObject.hashCode()\ndocumentation\n.\nWhile most Spark operations work on RDDs containing any type of objects, a few special operations are\nonly available on RDDs of key-value pairs.\nThe most common ones are distributed “shuffle” operations, such as grouping or aggregating the elements\nby a key.\nIn Java, key-value pairs are represented using the\nscala.Tuple2\nclass\nfrom the Scala standard library. You can simply call\nnew Tuple2(a, b)\nto create a tuple, and access\nits fields later with\ntuple._1()\nand\ntuple._2()\n.\nRDDs of key-value pairs are represented by the\nJavaPairRDD\nclass. You can construct\nJavaPairRDDs from JavaRDDs using special versions of the\nmap\noperations, like\nmapToPair\nand\nflatMapToPair\n. The JavaPairRDD will have both standard RDD functions and special\nkey-value ones.\nFor example, the following code uses the\nreduceByKey\noperation on key-value pairs to count how\nmany times each line of text occurs in a file:\nJavaRDD\n<\nString\n>\nlines\n=\nsc\n.\ntextFile\n(\n\"data.txt\"\n);\nJavaPairRDD\n<\nString\n,\nInteger\n>\npairs\n=\nlines\n.\nmapToPair\n(\ns\n->\nnew\nTuple2\n(\ns\n,\n1\n));\nJavaPairRDD\n<\nString\n,\nInteger\n>\ncounts\n=\npairs\n.\nreduceByKey\n((\na\n,\nb\n)\n->\na\n+\nb\n);\nWe could also use\ncounts.sortByKey()\n, for example, to sort the pairs alphabetically, and finally\ncounts.collect()\nto bring them back to the driver program as an array of objects.\nNote:\nwhen using custom objects as the key in key-value pair operations, you must be sure that a\ncustom\nequals()\nmethod is accompanied with a matching\nhashCode()\nmethod.  For full details, see\nthe contract outlined in the\nObject.hashCode()\ndocumentation\n.\nTransformations\nThe following table lists some of the common transformations supported by Spark. Refer to the\nRDD API doc\n(\nPython\n,\nScala\n,\nJava\n,\nR\n)\nand pair RDD functions doc\n(\nScala\n,\nJava\n)\nfor details.\nTransformation\nMeaning\nmap\n(\nfunc\n)\nReturn a new distributed dataset formed by passing each element of the source through a function\nfunc\n.\nfilter\n(\nfunc\n)\nReturn a new dataset formed by selecting those elements of the source on which\nfunc\nreturns true.\nflatMap\n(\nfunc\n)\nSimilar to map, but each input item can be mapped to 0 or more output items (so\nfunc\nshould return a Seq rather than a single item).\nmapPartitions\n(\nfunc\n)\nSimilar to map, but runs separately on each partition (block) of the RDD, so\nfunc\nmust be of type\n    Iterator<T> => Iterator<U> when running on an RDD of type T.\nmapPartitionsWithIndex\n(\nfunc\n)\nSimilar to mapPartitions, but also provides\nfunc\nwith an integer value representing the index of\n  the partition, so\nfunc\nmust be of type (Int, Iterator<T>) => Iterator<U> when running on an RDD of type T.\nsample\n(\nwithReplacement\n,\nfraction\n,\nseed\n)\nSample a fraction\nfraction\nof the data, with or without replacement, using a given random number generator seed.\nunion\n(\notherDataset\n)\nReturn a new dataset that contains the union of the elements in the source dataset and the argument.\nintersection\n(\notherDataset\n)\nReturn a new RDD that contains the intersection of elements in the source dataset and the argument.\ndistinct\n([\nnumPartitions\n]))\nReturn a new dataset that contains the distinct elements of the source dataset.\ngroupByKey\n([\nnumPartitions\n])\nWhen called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs.\nNote:\nIf you are grouping in order to perform an aggregation (such as a sum or\n      average) over each key, using\nreduceByKey\nor\naggregateByKey\nwill yield much better\n      performance.\nNote:\nBy default, the level of parallelism in the output depends on the number of partitions of the parent RDD.\n      You can pass an optional\nnumPartitions\nargument to set a different number of tasks.\nreduceByKey\n(\nfunc\n, [\nnumPartitions\n])\nWhen called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function\nfunc\n, which must be of type (V,V) => V. Like in\ngroupByKey\n, the number of reduce tasks is configurable through an optional second argument.\naggregateByKey\n(\nzeroValue\n)(\nseqOp\n,\ncombOp\n, [\nnumPartitions\n])\nWhen called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral \"zero\" value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in\ngroupByKey\n, the number of reduce tasks is configurable through an optional second argument.\nsortByKey\n([\nascending\n], [\nnumPartitions\n])\nWhen called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean\nascending\nargument.\njoin\n(\notherDataset\n, [\nnumPartitions\n])\nWhen called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key.\n    Outer joins are supported through\nleftOuterJoin\n,\nrightOuterJoin\n, and\nfullOuterJoin\n.\ncogroup\n(\notherDataset\n, [\nnumPartitions\n])\nWhen called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable<V>, Iterable<W>)) tuples. This operation is also called\ngroupWith\n.\ncartesian\n(\notherDataset\n)\nWhen called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements).\npipe\n(\ncommand\n,\n[envVars]\n)\nPipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the\n    process's stdin and lines output to its stdout are returned as an RDD of strings.\ncoalesce\n(\nnumPartitions\n)\nDecrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently\n    after filtering down a large dataset.\nrepartition\n(\nnumPartitions\n)\nReshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them.\n    This always shuffles all data over the network.\nrepartitionAndSortWithinPartitions\n(\npartitioner\n)\nRepartition the RDD according to the given partitioner and, within each resulting partition,\n  sort records by their keys. This is more efficient than calling\nrepartition\nand then sorting within\n  each partition because it can push the sorting down into the shuffle machinery.\nActions\nThe following table lists some of the common actions supported by Spark. Refer to the\nRDD API doc\n(\nPython\n,\nScala\n,\nJava\n,\nR\n)\nand pair RDD functions doc\n(\nScala\n,\nJava\n)\nfor details.\nAction\nMeaning\nreduce\n(\nfunc\n)\nAggregate the elements of the dataset using a function\nfunc\n(which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.\ncollect\n()\nReturn all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data.\ncount\n()\nReturn the number of elements in the dataset.\nfirst\n()\nReturn the first element of the dataset (similar to take(1)).\ntake\n(\nn\n)\nReturn an array with the first\nn\nelements of the dataset.\ntakeSample\n(\nwithReplacement\n,\nnum\n, [\nseed\n])\nReturn an array with a random sample of\nnum\nelements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed.\ntakeOrdered\n(\nn\n,\n[ordering]\n)\nReturn the first\nn\nelements of the RDD using either their natural order or a custom comparator.\nsaveAsTextFile\n(\npath\n)\nWrite the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file.\nsaveAsSequenceFile\n(\npath\n)\n(Java and Scala)\nWrite the elements of the dataset as a Hadoop SequenceFile in a given path in the local filesystem, HDFS or any other Hadoop-supported file system. This is available on RDDs of key-value pairs that implement Hadoop's Writable interface. In Scala, it is also\n   available on types that are implicitly convertible to Writable (Spark includes conversions for basic types like Int, Double, String, etc).\nsaveAsObjectFile\n(\npath\n)\n(Java and Scala)\nWrite the elements of the dataset in a simple format using Java serialization, which can then be loaded using\nSparkContext.objectFile()\n.\ncountByKey\n()\nOnly available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key.\nforeach\n(\nfunc\n)\nRun a function\nfunc\non each element of the dataset. This is usually done for side effects such as updating an\nAccumulator\nor interacting with external storage systems.\nNote\n: modifying variables other than Accumulators outside of the\nforeach()\nmay result in undefined behavior. See\nUnderstanding closures\nfor more details.\nThe Spark RDD API also exposes asynchronous versions of some actions, like\nforeachAsync\nfor\nforeach\n, which immediately return a\nFutureAction\nto the caller instead of blocking on completion of the action. This can be used to manage or wait for the asynchronous execution of the action.\nShuffle operations\nCertain operations within Spark trigger an event known as the shuffle. The shuffle is Spark’s\nmechanism for re-distributing data so that it’s grouped differently across partitions. This typically\ninvolves copying data across executors and machines, making the shuffle a complex and\ncostly operation.\nBackground\nTo understand what happens during the shuffle, we can consider the example of the\nreduceByKey\noperation. The\nreduceByKey\noperation generates a new RDD where all\nvalues for a single key are combined into a tuple - the key and the result of executing a reduce\nfunction against all values associated with that key. The challenge is that not all values for a\nsingle key necessarily reside on the same partition, or even the same machine, but they must be\nco-located to compute the result.\nIn Spark, data is generally not distributed across partitions to be in the necessary place for a\nspecific operation. During computations, a single task will operate on a single partition - thus, to\norganize all the data for a single\nreduceByKey\nreduce task to execute, Spark needs to perform an\nall-to-all operation. It must read from all partitions to find all the values for all keys,\nand then bring together values across partitions to compute the final result for each key -\nthis is called the\nshuffle\n.\nAlthough the set of elements in each partition of newly shuffled data will be deterministic, and so\nis the ordering of partitions themselves, the ordering of these elements is not. If one desires predictably\nordered data following shuffle then it’s possible to use:\nmapPartitions\nto sort each partition using, for example,\n.sorted\nrepartitionAndSortWithinPartitions\nto efficiently sort partitions while simultaneously repartitioning\nsortBy\nto make a globally ordered RDD\nOperations which can cause a shuffle include\nrepartition\noperations like\nrepartition\nand\ncoalesce\n,\n‘ByKey\noperations\n(except for counting) like\ngroupByKey\nand\nreduceByKey\n, and\njoin\noperations like\ncogroup\nand\njoin\n.\nPerformance Impact\nThe\nShuffle\nis an expensive operation since it involves disk I/O, data serialization, and\nnetwork I/O. To organize data for the shuffle, Spark generates sets of tasks -\nmap\ntasks to\norganize the data, and a set of\nreduce\ntasks to aggregate it. This nomenclature comes from\nMapReduce and does not directly relate to Spark’s\nmap\nand\nreduce\noperations.\nInternally, results from individual map tasks are kept in memory until they can’t fit. Then, these\nare sorted based on the target partition and written to a single file. On the reduce side, tasks\nread the relevant sorted blocks.\nCertain shuffle operations can consume significant amounts of heap memory since they employ\nin-memory data structures to organize records before or after transferring them. Specifically,\nreduceByKey\nand\naggregateByKey\ncreate these structures on the map side, and\n'ByKey\noperations\ngenerate these on the reduce side. When data does not fit in memory Spark will spill these tables\nto disk, incurring the additional overhead of disk I/O and increased garbage collection.\nShuffle also generates a large number of intermediate files on disk. As of Spark 1.3, these files\nare preserved until the corresponding RDDs are no longer used and are garbage collected.\nThis is done so the shuffle files don’t need to be re-created if the lineage is re-computed.\nGarbage collection may happen only after a long period of time, if the application retains references\nto these RDDs or if GC does not kick in frequently. This means that long-running Spark jobs may\nconsume a large amount of disk space. The temporary storage directory is specified by the\nspark.local.dir\nconfiguration parameter when configuring the Spark context.\nShuffle behavior can be tuned by adjusting a variety of configuration parameters. See the\n‘Shuffle Behavior’ section within the\nSpark Configuration Guide\n.\nRDD Persistence\nOne of the most important capabilities in Spark is\npersisting\n(or\ncaching\n) a dataset in memory\nacross operations. When you persist an RDD, each node stores any partitions of it that it computes in\nmemory and reuses them in other actions on that dataset (or datasets derived from it). This allows\nfuture actions to be much faster (often by more than 10x). Caching is a key tool for\niterative algorithms and fast interactive use.\nYou can mark an RDD to be persisted using the\npersist()\nor\ncache()\nmethods on it. The first time\nit is computed in an action, it will be kept in memory on the nodes. Spark’s cache is fault-tolerant –\nif any partition of an RDD is lost, it will automatically be recomputed using the transformations\nthat originally created it.\nIn addition, each persisted RDD can be stored using a different\nstorage level\n, allowing you, for example,\nto persist the dataset on disk, persist it in memory but as serialized Java objects (to save space),\nreplicate it across nodes.\nThese levels are set by passing a\nStorageLevel\nobject (\nPython\n,\nScala\n,\nJava\n)\nto\npersist()\n. The\ncache()\nmethod is a shorthand for using the default storage level,\nwhich is\nStorageLevel.MEMORY_ONLY\n(store deserialized objects in memory). The full set of\nstorage levels is:\nStorage Level\nMeaning\nMEMORY_ONLY\nStore RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will\n    not be cached and will be recomputed on the fly each time they're needed. This is the default level.\nMEMORY_AND_DISK\nStore RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the\n    partitions that don't fit on disk, and read them from there when they're needed.\nMEMORY_ONLY_SER\n(Java and Scala)\nStore RDD as\nserialized\nJava objects (one byte array per partition).\n    This is generally more space-efficient than deserialized objects, especially when using a\nfast serializer\n, but more CPU-intensive to read.\nMEMORY_AND_DISK_SER\n(Java and Scala)\nSimilar to MEMORY_ONLY_SER, but spill partitions that don't fit in memory to disk instead of\n    recomputing them on the fly each time they're needed.\nDISK_ONLY\nStore the RDD partitions only on disk.\nMEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.\nSame as the levels above, but replicate each partition on two cluster nodes.\nOFF_HEAP (experimental)\nSimilar to MEMORY_ONLY_SER, but store the data in\noff-heap memory\n. This requires off-heap memory to be enabled.\nNote:\nIn Python, stored objects will always be serialized with the\nPickle\nlibrary,\nso it does not matter whether you choose a serialized level. The available storage levels in Python include\nMEMORY_ONLY\n,\nMEMORY_ONLY_2\n,\nMEMORY_AND_DISK\n,\nMEMORY_AND_DISK_2\n,\nDISK_ONLY\n,\nDISK_ONLY_2\n, and\nDISK_ONLY_3\n.\nSpark also automatically persists some intermediate data in shuffle operations (e.g.\nreduceByKey\n), even without users calling\npersist\n. This is done to avoid recomputing the entire input if a node fails during the shuffle. We still recommend users call\npersist\non the resulting RDD if they plan to reuse it.\nWhich Storage Level to Choose?\nSpark’s storage levels are meant to provide different trade-offs between memory usage and CPU\nefficiency. We recommend going through the following process to select one:\nIf your RDDs fit comfortably with the default storage level (\nMEMORY_ONLY\n), leave them that way.\nThis is the most CPU-efficient option, allowing operations on the RDDs to run as fast as possible.\nIf not, try using\nMEMORY_ONLY_SER\nand\nselecting a fast serialization library\nto\nmake the objects much more space-efficient, but still reasonably fast to access. (Java and Scala)\nDon’t spill to disk unless the functions that computed your datasets are expensive, or they filter\na large amount of the data. Otherwise, recomputing a partition may be as fast as reading it from\ndisk.\nUse the replicated storage levels if you want fast fault recovery (e.g. if using Spark to serve\nrequests from a web application).\nAll\nthe storage levels provide full fault tolerance by\nrecomputing lost data, but the replicated ones let you continue running tasks on the RDD without\nwaiting to recompute a lost partition.\nRemoving Data\nSpark automatically monitors cache usage on each node and drops out old data partitions in a\nleast-recently-used (LRU) fashion. If you would like to manually remove an RDD instead of waiting for\nit to fall out of the cache, use the\nRDD.unpersist()\nmethod. Note that this method does not\nblock by default. To block until resources are freed, specify\nblocking=true\nwhen calling this method.\nShared Variables\nNormally, when a function passed to a Spark operation (such as\nmap\nor\nreduce\n) is executed on a\nremote cluster node, it works on separate copies of all the variables used in the function. These\nvariables are copied to each machine, and no updates to the variables on the remote machine are\npropagated back to the driver program. Supporting general, read-write shared variables across tasks\nwould be inefficient. However, Spark does provide two limited types of\nshared variables\nfor two\ncommon usage patterns: broadcast variables and accumulators.\nBroadcast Variables\nBroadcast variables allow the programmer to keep a read-only variable cached on each machine rather\nthan shipping a copy of it with tasks. They can be used, for example, to give every node a copy of a\nlarge input dataset in an efficient manner. Spark also attempts to distribute broadcast variables\nusing efficient broadcast algorithms to reduce communication cost.\nSpark actions are executed through a set of stages, separated by distributed “shuffle” operations.\nSpark automatically broadcasts the common data needed by tasks within each stage. The data\nbroadcasted this way is cached in serialized form and deserialized before running each task. This\nmeans that explicitly creating broadcast variables is only useful when tasks across multiple stages\nneed the same data or when caching the data in deserialized form is important.\nBroadcast variables are created from a variable\nv\nby calling\nSparkContext.broadcast(v)\n. The\nbroadcast variable is a wrapper around\nv\n, and its value can be accessed by calling the\nvalue\nmethod. The code below shows this:\n>>>\nbroadcastVar\n=\nsc\n.\nbroadcast\n([\n1\n,\n2\n,\n3\n])\n<\npyspark\n.\ncore\n.\nbroadcast\n.\nBroadcast\nobject\nat\n0x102789f10\n>\n>>>\nbroadcastVar\n.\nvalue\n[\n1\n,\n2\n,\n3\n]\nscala\n>\nval\nbroadcastVar\n=\nsc\n.\nbroadcast\n(\nArray\n(\n1\n,\n2\n,\n3\n))\nbroadcastVar\n:\norg.apache.spark.broadcast.Broadcast\n[\nArray\n[\nInt\n]]\n=\nBroadcast\n(\n0\n)\nscala\n>\nbroadcastVar\n.\nvalue\nres0\n:\nArray\n[\nInt\n]\n=\nArray\n(\n1\n,\n2\n,\n3\n)\nBroadcast\n<\nint\n[]>\nbroadcastVar\n=\nsc\n.\nbroadcast\n(\nnew\nint\n[]\n{\n1\n,\n2\n,\n3\n});\nbroadcastVar\n.\nvalue\n();\n// returns [1, 2, 3]\nAfter the broadcast variable is created, it should be used instead of the value\nv\nin any functions\nrun on the cluster so that\nv\nis not shipped to the nodes more than once. In addition, the object\nv\nshould not be modified after it is broadcast in order to ensure that all nodes get the same\nvalue of the broadcast variable (e.g. if the variable is shipped to a new node later).\nTo release the resources that the broadcast variable copied onto executors, call\n.unpersist()\n.\nIf the broadcast is used again afterwards, it will be re-broadcast. To permanently release all\nresources used by the broadcast variable, call\n.destroy()\n. The broadcast variable can’t be used\nafter that. Note that these methods do not block by default. To block until resources are freed,\nspecify\nblocking=true\nwhen calling them.\nAccumulators\nAccumulators are variables that are only “added” to through an associative and commutative operation and can\ntherefore be efficiently supported in parallel. They can be used to implement counters (as in\nMapReduce) or sums. Spark natively supports accumulators of numeric types, and programmers\ncan add support for new types.\nAs a user, you can create named or unnamed accumulators. As seen in the image below, a named accumulator (in this instance\ncounter\n) will display in the web UI for the stage that modifies that accumulator. Spark displays the value for each accumulator modified by a task in the “Tasks” table.\nTracking accumulators in the UI can be useful for understanding the progress of\nrunning stages (NOTE: this is not yet supported in Python).\nAn accumulator is created from an initial value\nv\nby calling\nSparkContext.accumulator(v)\n. Tasks\nrunning on a cluster can then add to it using the\nadd\nmethod or the\n+=\noperator. However, they cannot read its value.\nOnly the driver program can read the accumulator’s value, using its\nvalue\nmethod.\nThe code below shows an accumulator being used to add up the elements of an array:\n>>>\naccum\n=\nsc\n.\naccumulator\n(\n0\n)\n>>>\naccum\nAccumulator\n<\nid\n=\n0\n,\nvalue\n=\n0\n>\n>>>\nsc\n.\nparallelize\n([\n1\n,\n2\n,\n3\n,\n4\n]).\nforeach\n(\nlambda\nx\n:\naccum\n.\nadd\n(\nx\n))\n...\n10\n/\n09\n/\n29\n18\n:\n41\n:\n08\nINFO\nSparkContext\n:\nTasks\nfinished\nin\n0.317106\ns\n>>>\naccum\n.\nvalue\n10\nWhile this code used the built-in support for accumulators of type Int, programmers can also\ncreate their own types by subclassing\nAccumulatorParam\n.\nThe AccumulatorParam interface has two methods:\nzero\nfor providing a “zero value” for your data\ntype, and\naddInPlace\nfor adding two values together. For example, supposing we had a\nVector\nclass\nrepresenting mathematical vectors, we could write:\nclass\nVectorAccumulatorParam\n(\nAccumulatorParam\n):\ndef\nzero\n(\nself\n,\ninitialValue\n):\nreturn\nVector\n.\nzeros\n(\ninitialValue\n.\nsize\n)\ndef\naddInPlace\n(\nself\n,\nv1\n,\nv2\n):\nv1\n+=\nv2\nreturn\nv1\n# Then, create an Accumulator of this type:\nvecAccum\n=\nsc\n.\naccumulator\n(\nVector\n(...),\nVectorAccumulatorParam\n())\nA numeric accumulator can be created by calling\nSparkContext.longAccumulator()\nor\nSparkContext.doubleAccumulator()\nto accumulate values of type Long or Double, respectively. Tasks running on a cluster can then add to it using\nthe\nadd\nmethod.  However, they cannot read its value. Only the driver program can read the accumulator’s value,\nusing its\nvalue\nmethod.\nThe code below shows an accumulator being used to add up the elements of an array:\nscala\n>\nval\naccum\n=\nsc\n.\nlongAccumulator\n(\n\"My Accumulator\"\n)\naccum\n:\norg.apache.spark.util.LongAccumulator\n=\nLongAccumulator\n(\nid\n:\n0\n,\nname\n:\nSome\n(\nMy\nAccumulator\n),\nvalue\n:\n0\n)\nscala\n>\nsc\n.\nparallelize\n(\nArray\n(\n1\n,\n2\n,\n3\n,\n4\n)).\nforeach\n(\nx\n=>\naccum\n.\nadd\n(\nx\n))\n...\n10\n/\n09\n/\n29\n18\n:\n41\n:\n08\nINFO\nSparkContext:\nTasks\nfinished\nin\n0\n.\n317106\ns\nscala\n>\naccum\n.\nvalue\nres2\n:\nLong\n=\n10\nWhile this code used the built-in support for accumulators of type Long, programmers can also\ncreate their own types by subclassing\nAccumulatorV2\n.\nThe AccumulatorV2 abstract class has several methods which one has to override:\nreset\nfor resetting\nthe accumulator to zero,\nadd\nfor adding another value into the accumulator,\nmerge\nfor merging another same-type accumulator into this one. Other methods that must be overridden\nare contained in the\nAPI documentation\n. For example, supposing we had a\nMyVector\nclass\nrepresenting mathematical vectors, we could write:\nclass\nVectorAccumulatorV2\nextends\nAccumulatorV2\n[\nMyVector\n,\nMyVector\n]\n{\nprivate\nval\nmyVector\n:\nMyVector\n=\nMyVector\n.\ncreateZeroVector\ndef\nreset\n()\n:\nUnit\n=\n{\nmyVector\n.\nreset\n()\n}\ndef\nadd\n(\nv\n:\nMyVector\n)\n:\nUnit\n=\n{\nmyVector\n.\nadd\n(\nv\n)\n}\n...\n}\n// Then, create an Accumulator of this type:\nval\nmyVectorAcc\n=\nnew\nVectorAccumulatorV2\n// Then, register it into spark context:\nsc\n.\nregister\n(\nmyVectorAcc\n,\n\"MyVectorAcc1\"\n)\nNote that, when programmers define their own type of AccumulatorV2, the resulting type can be different than that of the elements added.\nA numeric accumulator can be created by calling\nSparkContext.longAccumulator()\nor\nSparkContext.doubleAccumulator()\nto accumulate values of type Long or Double, respectively. Tasks running on a cluster can then add to it using\nthe\nadd\nmethod.  However, they cannot read its value. Only the driver program can read the accumulator’s value,\nusing its\nvalue\nmethod.\nThe code below shows an accumulator being used to add up the elements of an array:\nLongAccumulator\naccum\n=\njsc\n.\nsc\n().\nlongAccumulator\n();\nsc\n.\nparallelize\n(\nArrays\n.\nasList\n(\n1\n,\n2\n,\n3\n,\n4\n)).\nforeach\n(\nx\n->\naccum\n.\nadd\n(\nx\n));\n// ...\n// 10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s\naccum\n.\nvalue\n();\n// returns 10\nWhile this code used the built-in support for accumulators of type Long, programmers can also\ncreate their own types by subclassing\nAccumulatorV2\n.\nThe AccumulatorV2 abstract class has several methods which one has to override:\nreset\nfor resetting\nthe accumulator to zero,\nadd\nfor adding another value into the accumulator,\nmerge\nfor merging another same-type accumulator into this one. Other methods that must be overridden\nare contained in the\nAPI documentation\n. For example, supposing we had a\nMyVector\nclass\nrepresenting mathematical vectors, we could write:\nclass\nVectorAccumulatorV2\nimplements\nAccumulatorV2\n<\nMyVector\n,\nMyVector\n>\n{\nprivate\nMyVector\nmyVector\n=\nMyVector\n.\ncreateZeroVector\n();\npublic\nvoid\nreset\n()\n{\nmyVector\n.\nreset\n();\n}\npublic\nvoid\nadd\n(\nMyVector\nv\n)\n{\nmyVector\n.\nadd\n(\nv\n);\n}\n...\n}\n// Then, create an Accumulator of this type:\nVectorAccumulatorV2\nmyVectorAcc\n=\nnew\nVectorAccumulatorV2\n();\n// Then, register it into spark context:\njsc\n.\nsc\n().\nregister\n(\nmyVectorAcc\n,\n\"MyVectorAcc1\"\n);\nNote that, when programmers define their own type of AccumulatorV2, the resulting type can be different than that of the elements added.\nWarning\n: When a Spark task finishes, Spark will try to merge the accumulated updates in this task to an accumulator.\nIf it fails, Spark will ignore the failure and still mark the task successful and continue to run other tasks. Hence,\na buggy accumulator will not impact a Spark job, but it may not get updated correctly although a Spark job is successful.\nFor accumulator updates performed inside\nactions only\n, Spark guarantees that each task’s update to the accumulator\nwill only be applied once, i.e. restarted tasks will not update the value. In transformations, users should be aware\nof that each task’s update may be applied more than once if tasks or job stages are re-executed.\nAccumulators do not change the lazy evaluation model of Spark. If they are being updated within an operation on an RDD, their value is only updated once that RDD is computed as part of an action. Consequently, accumulator updates are not guaranteed to be executed when made within a lazy transformation like\nmap()\n. The below code fragment demonstrates this property:\naccum\n=\nsc\n.\naccumulator\n(\n0\n)\ndef\ng\n(\nx\n):\naccum\n.\nadd\n(\nx\n)\nreturn\nf\n(\nx\n)\ndata\n.\nmap\n(\ng\n)\n# Here, accum is still 0 because no actions have caused the `map` to be computed.\nval\naccum\n=\nsc\n.\nlongAccumulator\ndata\n.\nmap\n{\nx\n=>\naccum\n.\nadd\n(\nx\n);\nx\n}\n// Here, accum is still 0 because no actions have caused the map operation to be computed.\nLongAccumulator\naccum\n=\njsc\n.\nsc\n().\nlongAccumulator\n();\ndata\n.\nmap\n(\nx\n->\n{\naccum\n.\nadd\n(\nx\n);\nreturn\nf\n(\nx\n);\n});\n// Here, accum is still 0 because no actions have caused the `map` to be computed.\nDeploying to a Cluster\nThe\napplication submission guide\ndescribes how to submit applications to a cluster.\nIn short, once you package your application into a JAR (for Java/Scala) or a set of\n.py\nor\n.zip\nfiles (for Python),\nthe\nbin/spark-submit\nscript lets you submit it to any supported cluster manager.\nLaunching Spark jobs from Java / Scala\nThe\norg.apache.spark.launcher\npackage provides classes for launching Spark jobs as child processes using a simple Java API.\nUnit Testing\nSpark is friendly to unit testing with any popular unit test framework.\nSimply create a\nSparkContext\nin your test with the master URL set to\nlocal\n, run your operations,\nand then call\nSparkContext.stop()\nto tear it down.\nMake sure you stop the context within a\nfinally\nblock or the test framework’s\ntearDown\nmethod,\nas Spark does not support two contexts running concurrently in the same program.\nWhere to Go from Here\nYou can see some\nexample Spark programs\non the Spark website.\nIn addition, Spark includes several samples in the\nexamples\ndirectory\n(\nPython\n,\nScala\n,\nJava\n,\nR\n).\nYou can run Java and Scala examples by passing the class name to Spark’s\nbin/run-example\nscript; for instance:\n./bin/run-example SparkPi\nFor Python examples, use\nspark-submit\ninstead:\n./bin/spark-submit examples/src/main/python/pi.py\nFor R examples, use\nspark-submit\ninstead:\n./bin/spark-submit examples/src/main/r/dataframe.R\nFor help on optimizing your programs, the\nconfiguration\nand\ntuning\nguides provide information on best practices. They are especially important for\nmaking sure that your data is stored in memory in an efficient format.\nFor help on deploying, the\ncluster mode overview\ndescribes the components involved\nin distributed operation and supported cluster managers.\nFinally, full API documentation is available in\nPython\n,\nScala\n,\nJava\nand\nR\n."}
{"url": "https://spark.apache.org/docs/latest/tuning.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nTuning Spark\nData Serialization\nMemory Tuning\nMemory Management Overview\nDetermining Memory Consumption\nTuning Data Structures\nSerialized RDD Storage\nGarbage Collection Tuning\nOther Considerations\nLevel of Parallelism\nParallel Listing on Input Paths\nMemory Usage of Reduce Tasks\nBroadcasting Large Variables\nData Locality\nSummary\nBecause of the in-memory nature of most Spark computations, Spark programs can be bottlenecked\nby any resource in the cluster: CPU, network bandwidth, or memory.\nMost often, if the data fits in memory, the bottleneck is network bandwidth, but sometimes, you\nalso need to do some tuning, such as\nstoring RDDs in serialized form\n, to\ndecrease memory usage.\nThis guide will cover two main topics: data serialization, which is crucial for good network\nperformance and can also reduce memory use, and memory tuning. We also sketch several smaller topics.\nData Serialization\nSerialization plays an important role in the performance of any distributed application.\nFormats that are slow to serialize objects into, or consume a large number of\nbytes, will greatly slow down the computation.\nOften, this will be the first thing you should tune to optimize a Spark application.\nSpark aims to strike a balance between convenience (allowing you to work with any Java type\nin your operations) and performance. It provides two serialization libraries:\nJava serialization\n:\nBy default, Spark serializes objects using Java’s\nObjectOutputStream\nframework, and can work\nwith any class you create that implements\njava.io.Serializable\n.\nYou can also control the performance of your serialization more closely by extending\njava.io.Externalizable\n.\nJava serialization is flexible but often quite slow, and leads to large\nserialized formats for many classes.\nKryo serialization\n: Spark can also use\nthe Kryo library (version 4) to serialize objects more quickly. Kryo is significantly\nfaster and more compact than Java serialization (often as much as 10x), but does not support all\nSerializable\ntypes and requires you to\nregister\nthe classes you’ll use in the program in advance\nfor best performance.\nYou can switch to using Kryo by initializing your job with a\nSparkConf\nand calling\nconf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n.\nThis setting configures the serializer used for not only shuffling data between worker\nnodes but also when serializing RDDs to disk.  The only reason Kryo is not the default is because of the custom\nregistration requirement, but we recommend trying it in any network-intensive application.\nSince Spark 2.0.0, we internally use Kryo serializer when shuffling RDDs with simple types, arrays of simple types, or string type.\nSpark automatically includes Kryo serializers for the many commonly-used core Scala classes covered\nin the AllScalaRegistrar from the\nTwitter chill\nlibrary.\nTo register your own custom classes with Kryo, use the\nregisterKryoClasses\nmethod.\nval\nconf\n=\nnew\nSparkConf\n().\nsetMaster\n(...).\nsetAppName\n(...)\nconf\n.\nregisterKryoClasses\n(\nArray\n(\nclassOf\n[\nMyClass1\n],\nclassOf\n[\nMyClass2\n]))\nval\nsc\n=\nnew\nSparkContext\n(\nconf\n)\nThe\nKryo documentation\ndescribes more advanced\nregistration options, such as adding custom serialization code.\nIf your objects are large, you may also need to increase the\nspark.kryoserializer.buffer\nconfig\n. This value needs to be large enough\nto hold the\nlargest\nobject you will serialize.\nFinally, if you don’t register your custom classes, Kryo will still work, but it will have to store\nthe full class name with each object, which is wasteful.\nMemory Tuning\nThere are three considerations in tuning memory usage: the\namount\nof memory used by your objects\n(you may want your entire dataset to fit in memory), the\ncost\nof accessing those objects, and the\noverhead of\ngarbage collection\n(if you have high turnover in terms of objects).\nBy default, Java objects are fast to access, but can easily consume a factor of 2-5x more space\nthan the “raw” data inside their fields. This is due to several reasons:\nEach distinct Java object has an “object header”, which is about 16 bytes and contains information\nsuch as a pointer to its class. For an object with very little data in it (say one\nInt\nfield), this\ncan be bigger than the data.\nJava\nString\ns have about 40 bytes of overhead over the raw string data (since they store it in an\narray of\nChar\ns and keep extra data such as the length), and store each character\nas\ntwo\nbytes due to\nString\n’s internal usage of UTF-16 encoding. Thus a 10-character string can\neasily consume 60 bytes.\nCommon collection classes, such as\nHashMap\nand\nLinkedList\n, use linked data structures, where\nthere is a “wrapper” object for each entry (e.g.\nMap.Entry\n). This object not only has a header,\nbut also pointers (typically 8 bytes each) to the next object in the list.\nCollections of primitive types often store them as “boxed” objects such as\njava.lang.Integer\n.\nThis section will start with an overview of memory management in Spark, then discuss specific\nstrategies the user can take to make more efficient use of memory in his/her application. In\nparticular, we will describe how to determine the memory usage of your objects, and how to\nimprove it – either by changing your data structures, or by storing data in a serialized\nformat. We will then cover tuning Spark’s cache size and the Java garbage collector.\nMemory Management Overview\nMemory usage in Spark largely falls under one of two categories: execution and storage.\nExecution memory refers to that used for computation in shuffles, joins, sorts and aggregations,\nwhile storage memory refers to that used for caching and propagating internal data across the\ncluster. In Spark, execution and storage share a unified region (M). When no execution memory is\nused, storage can acquire all the available memory and vice versa. Execution may evict storage\nif necessary, but only until total storage memory usage falls under a certain threshold (R).\nIn other words,\nR\ndescribes a subregion within\nM\nwhere cached blocks are never evicted.\nStorage may not evict execution due to complexities in implementation.\nThis design ensures several desirable properties. First, applications that do not use caching\ncan use the entire space for execution, obviating unnecessary disk spills. Second, applications\nthat do use caching can reserve a minimum storage space (R) where their data blocks are immune\nto being evicted. Lastly, this approach provides reasonable out-of-the-box performance for a\nvariety of workloads without requiring user expertise of how memory is divided internally.\nAlthough there are two relevant configurations, the typical user should not need to adjust them\nas the default values are applicable to most workloads:\nspark.memory.fraction\nexpresses the size of\nM\nas a fraction of the (JVM heap space - 300MiB)\n(default 0.6). The rest of the space (40%) is reserved for user data structures, internal\nmetadata in Spark, and safeguarding against OOM errors in the case of sparse and unusually\nlarge records.\nspark.memory.storageFraction\nexpresses the size of\nR\nas a fraction of\nM\n(default 0.5).\nR\nis the storage space within\nM\nwhere cached blocks immune to being evicted by execution.\nThe value of\nspark.memory.fraction\nshould be set in order to fit this amount of heap space\ncomfortably within the JVM’s old or “tenured” generation. See the discussion of advanced GC\ntuning below for details.\nDetermining Memory Consumption\nThe best way to size the amount of memory consumption a dataset will require is to create an RDD, put it\ninto cache, and look at the “Storage” page in the web UI. The page will tell you how much memory the RDD\nis occupying.\nTo estimate the memory consumption of a particular object, use\nSizeEstimator\n’s\nestimate\nmethod.\nThis is useful for experimenting with different data layouts to trim memory usage, as well as\ndetermining the amount of space a broadcast variable will occupy on each executor heap.\nTuning Data Structures\nThe first way to reduce memory consumption is to avoid the Java features that add overhead, such as\npointer-based data structures and wrapper objects. There are several ways to do this:\nDesign your data structures to prefer arrays of objects, and primitive types, instead of the\nstandard Java or Scala collection classes (e.g.\nHashMap\n). The\nfastutil\nlibrary provides convenient collection classes for primitive types that are compatible with the\nJava standard library.\nAvoid nested structures with a lot of small objects and pointers when possible.\nConsider using numeric IDs or enumeration objects instead of strings for keys.\nIf you have less than 32 GiB of RAM, set the JVM flag\n-XX:+UseCompressedOops\nto make pointers be\nfour bytes instead of eight. You can add these options in\nspark-env.sh\n.\nSerialized RDD Storage\nWhen your objects are still too large to efficiently store despite this tuning, a much simpler way\nto reduce memory usage is to store them in\nserialized\nform, using the serialized StorageLevels in\nthe\nRDD persistence API\n, such as\nMEMORY_ONLY_SER\n.\nSpark will then store each RDD partition as one large byte array.\nThe only downside of storing data in serialized form is slower access times, due to having to\ndeserialize each object on the fly.\nWe highly recommend\nusing Kryo\nif you want to cache data in serialized form, as\nit leads to much smaller sizes than Java serialization (and certainly than raw Java objects).\nGarbage Collection Tuning\nJVM garbage collection can be a problem when you have large “churn” in terms of the RDDs\nstored by your program. (It is usually not a problem in programs that just read an RDD once\nand then run many operations on it.) When Java needs to evict old objects to make room for new ones, it will\nneed to trace through all your Java objects and find the unused ones. The main point to remember here is\nthat\nthe cost of garbage collection is proportional to the number of Java objects\n, so using data\nstructures with fewer objects (e.g. an array of\nInt\ns instead of a\nLinkedList\n) greatly lowers\nthis cost. An even better method is to persist objects in serialized form, as described above: now\nthere will be only\none\nobject (a byte array) per RDD partition. Before trying other\ntechniques, the first thing to try if GC is a problem is to use\nserialized caching\n.\nGC can also be a problem due to interference between your tasks’ working memory (the\namount of space needed to run the task) and the RDDs cached on your nodes. We will discuss how to control\nthe space allocated to the RDD cache to mitigate this.\nMeasuring the Impact of GC\nThe first step in GC tuning is to collect statistics on how frequently garbage collection occurs and the amount of\ntime spent GC. This can be done by adding\n-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps\nto the Java options.  (See the\nconfiguration guide\nfor info on passing Java options to Spark jobs.)  Next time your Spark job is run, you will see messages printed in the worker’s logs\neach time a garbage collection occurs. Note these logs will be on your cluster’s worker nodes (in the\nstdout\nfiles in\ntheir work directories),\nnot\non your driver program.\nAdvanced GC Tuning\nTo further tune garbage collection, we first need to understand some basic information about memory management in the JVM:\nJava Heap space is divided into two regions Young and Old. The Young generation is meant to hold short-lived objects\nwhile the Old generation is intended for objects with longer lifetimes.\nThe Young generation is further divided into three regions [Eden, Survivor1, Survivor2].\nA simplified description of the garbage collection procedure: When Eden is full, a minor GC is run on Eden and objects\nthat are alive from Eden and Survivor1 are copied to Survivor2. The Survivor regions are swapped. If an object is old\nenough or Survivor2 is full, it is moved to Old. Finally, when Old is close to full, a full GC is invoked.\nThe goal of GC tuning in Spark is to ensure that only long-lived RDDs are stored in the Old generation and that\nthe Young generation is sufficiently sized to store short-lived objects. This will help avoid full GCs to collect\ntemporary objects created during task execution. Some steps which may be useful are:\nCheck if there are too many garbage collections by collecting GC stats. If a full GC is invoked multiple times\nbefore a task completes, it means that there isn’t enough memory available for executing tasks.\nIf there are too many minor collections but not many major GCs, allocating more memory for Eden would help. You\ncan set the size of the Eden to be an over-estimate of how much memory each task will need. If the size of Eden\nis determined to be\nE\n, then you can set the size of the Young generation using the option\n-Xmn=4/3*E\n. (The scaling\nup by 4/3 is to account for space used by survivor regions as well.)\nIn the GC stats that are printed, if the OldGen is close to being full, reduce the amount of\nmemory used for caching by lowering\nspark.memory.fraction\n; it is better to cache fewer\nobjects than to slow down task execution. Alternatively, consider decreasing the size of\nthe Young generation. This means lowering\n-Xmn\nif you’ve set it as above. If not, try changing the \nvalue of the JVM’s\nNewRatio\nparameter. Many JVMs default this to 2, meaning that the Old generation \noccupies 2/3 of the heap. It should be large enough such that this fraction exceeds\nspark.memory.fraction\n.\nSince 4.0.0, Spark uses JDK 17 by default, which also makes the G1GC garbage collector the default. Note that with\nlarge executor heap sizes, it may be important to increase the G1 region size with\n-XX:G1HeapRegionSize\n.\nAs an example, if your task is reading data from HDFS, the amount of memory used by the task can be estimated using\nthe size of the data block read from HDFS. Note that the size of a decompressed block is often 2 or 3 times the\nsize of the block. So if we wish to have 3 or 4 tasks’ worth of working space, and the HDFS block size is 128 MiB,\nwe can estimate the size of Eden to be\n4*3*128MiB\n.\nMonitor how the frequency and time taken by garbage collection changes with the new settings.\nOur experience suggests that the effect of GC tuning depends on your application and the amount of memory available.\nThere are\nmany more tuning options\ndescribed online,\nbut at a high level, managing how frequently full GC takes place can help in reducing the overhead.\nGC tuning flags for executors can be specified by setting\nspark.executor.defaultJavaOptions\nor\nspark.executor.extraJavaOptions\nin\na job’s configuration.\nOther Considerations\nLevel of Parallelism\nClusters will not be fully utilized unless you set the level of parallelism for each operation high\nenough. Spark automatically sets the number of “map” tasks to run on each file according to its size\n(though you can control it through optional parameters to\nSparkContext.textFile\n, etc), and for\ndistributed “reduce” operations, such as\ngroupByKey\nand\nreduceByKey\n, it uses the largest\nparent RDD’s number of partitions. You can pass the level of parallelism as a second argument\n(see the\nspark.PairRDDFunctions\ndocumentation),\nor set the config property\nspark.default.parallelism\nto change the default.\nIn general, we recommend 2-3 tasks per CPU core in your cluster.\nParallel Listing on Input Paths\nSometimes you may also need to increase directory listing parallelism when job input has large number of directories,\notherwise the process could take a very long time, especially when against object store like S3.\nIf your job works on RDD with Hadoop input formats (e.g., via\nSparkContext.sequenceFile\n), the parallelism is\ncontrolled via\nspark.hadoop.mapreduce.input.fileinputformat.list-status.num-threads\n(currently default is 1).\nFor Spark SQL with file-based data sources, you can tune\nspark.sql.sources.parallelPartitionDiscovery.threshold\nand\nspark.sql.sources.parallelPartitionDiscovery.parallelism\nto improve listing parallelism. Please\nrefer to\nSpark SQL performance tuning guide\nfor more details.\nMemory Usage of Reduce Tasks\nSometimes, you will get an OutOfMemoryError not because your RDDs don’t fit in memory, but because the\nworking set of one of your tasks, such as one of the reduce tasks in\ngroupByKey\n, was too large.\nSpark’s shuffle operations (\nsortByKey\n,\ngroupByKey\n,\nreduceByKey\n,\njoin\n, etc) build a hash table\nwithin each task to perform the grouping, which can often be large. The simplest fix here is to\nincrease the level of parallelism\n, so that each task’s input set is smaller. Spark can efficiently\nsupport tasks as short as 200 ms, because it reuses one executor JVM across many tasks and it has\na low task launching cost, so you can safely increase the level of parallelism to more than the\nnumber of cores in your clusters.\nBroadcasting Large Variables\nUsing the\nbroadcast functionality\navailable in\nSparkContext\ncan greatly reduce the size of each serialized task, and the cost\nof launching a job over a cluster. If your tasks use any large object from the driver program\ninside of them (e.g. a static lookup table), consider turning it into a broadcast variable.\nSpark prints the serialized size of each task on the master, so you can look at that to\ndecide whether your tasks are too large; in general, tasks larger than about 20 KiB are probably\nworth optimizing.\nData Locality\nData locality can have a major impact on the performance of Spark jobs.  If data and the code that\noperates on it are together, then computation tends to be fast.  But if code and data are separated,\none must move to the other.  Typically, it is faster to ship serialized code from place to place than\na chunk of data because code size is much smaller than data.  Spark builds its scheduling around\nthis general principle of data locality.\nData locality is how close data is to the code processing it.  There are several levels of\nlocality based on the data’s current location.  In order from closest to farthest:\nPROCESS_LOCAL\ndata is in the same JVM as the running code.  This is the best locality\npossible.\nNODE_LOCAL\ndata is on the same node.  Examples might be in HDFS on the same node, or in\nanother executor on the same node.  This is a little slower than\nPROCESS_LOCAL\nbecause the data\nhas to travel between processes.\nNO_PREF\ndata is accessed equally quickly from anywhere and has no locality preference.\nRACK_LOCAL\ndata is on the same rack of servers.  Data is on a different server on the same rack\nso needs to be sent over the network, typically through a single switch.\nANY\ndata is elsewhere on the network and not in the same rack.\nSpark prefers to schedule all tasks at the best locality level, but this is not always possible.  In\nsituations where there is no unprocessed data on any idle executor, Spark switches to lower locality\nlevels. There are two options: a) wait until a busy CPU frees up to start a task on data on the same\nserver, or b) immediately start a new task in a farther away place that requires moving data there.\nWhat Spark typically does is wait a bit in the hopes that a busy CPU frees up.  Once that timeout\nexpires, it starts moving the data from far away to the free CPU.  The wait timeout for fallback\nbetween each level can be configured individually or all together in one parameter; see the\nspark.locality\nparameters on the\nconfiguration page\nfor details.\nYou should increase these settings if your tasks are long and see poor locality, but the default\nusually works well.\nSummary\nThis has been a short guide to point out the main concerns you should know about when tuning a\nSpark application – most importantly, data serialization and memory tuning. For most programs,\nswitching to Kryo serialization and persisting data in serialized form will solve most common\nperformance issues. Feel free to ask on the\nSpark mailing list\nabout other tuning best practices."}
{"url": "https://spark.apache.org/docs/latest/ml-guide.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nMachine Learning Library (MLlib) Guide\nMLlib is Spark’s machine learning (ML) library.\nIts goal is to make practical machine learning scalable and easy.\nAt a high level, it provides tools such as:\nML Algorithms: common learning algorithms such as classification, regression, clustering, and collaborative filtering\nFeaturization: feature extraction, transformation, dimensionality reduction, and selection\nPipelines: tools for constructing, evaluating, and tuning ML Pipelines\nPersistence: saving and load algorithms, models, and Pipelines\nUtilities: linear algebra, statistics, data handling, etc.\nAnnouncement: DataFrame-based API is primary API\nThe MLlib RDD-based API is now in maintenance mode.\nAs of Spark 2.0, the\nRDD\n-based APIs in the\nspark.mllib\npackage have entered maintenance mode.\nThe primary Machine Learning API for Spark is now the\nDataFrame\n-based API in the\nspark.ml\npackage.\nWhat are the implications?\nMLlib will still support the RDD-based API in\nspark.mllib\nwith bug fixes.\nMLlib will not add new features to the RDD-based API.\nIn the Spark 2.x releases, MLlib will add features to the DataFrames-based API to reach feature parity with the RDD-based API.\nWhy is MLlib switching to the DataFrame-based API?\nDataFrames provide a more user-friendly API than RDDs.  The many benefits of DataFrames include Spark Datasources, SQL/DataFrame queries, Tungsten and Catalyst optimizations, and uniform APIs across languages.\nThe DataFrame-based API for MLlib provides a uniform API across ML algorithms and across multiple languages.\nDataFrames facilitate practical ML Pipelines, particularly feature transformations.  See the\nPipelines guide\nfor details.\nWhat is “Spark ML”?\n“Spark ML” is not an official name but occasionally used to refer to the MLlib DataFrame-based API.\nThis is majorly due to the\norg.apache.spark.ml\nScala package name used by the DataFrame-based API, \nand the “Spark ML Pipelines” term we used initially to emphasize the pipeline concept.\nIs MLlib deprecated?\nNo. MLlib includes both the RDD-based API and the DataFrame-based API.\nThe RDD-based API is now in maintenance mode.\nBut neither API is deprecated, nor MLlib as a whole.\nDependencies\nMLlib uses linear algebra packages\nBreeze\nand\ndev.ludovic.netlib\nfor optimised numerical processing\n1\n. Those packages may call native acceleration libraries such as\nIntel MKL\nor\nOpenBLAS\nif they are available as system libraries or in runtime library paths.\nHowever, native acceleration libraries can’t be distributed with Spark. See\nMLlib Linear Algebra Acceleration Guide\nfor how to enable accelerated linear algebra processing. If accelerated native libraries are not enabled, you will see a warning message like below and a pure JVM implementation will be used instead:\nWARNING: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\nTo use MLlib in Python, you will need\nNumPy\nversion 1.4 or newer.\nHighlights in 3.0\nThe list below highlights some of the new features and enhancements added to MLlib in the\n3.0\nrelease of Spark:\nMultiple columns support was added to\nBinarizer\n(\nSPARK-23578\n),\nStringIndexer\n(\nSPARK-11215\n),\nStopWordsRemover\n(\nSPARK-29808\n) and PySpark\nQuantileDiscretizer\n(\nSPARK-22796\n).\nTree-Based Feature Transformation was added\n(\nSPARK-13677\n).\nTwo new evaluators\nMultilabelClassificationEvaluator\n(\nSPARK-16692\n) and\nRankingEvaluator\n(\nSPARK-28045\n) were added.\nSample weights support was added in\nDecisionTreeClassifier/Regressor\n(\nSPARK-19591\n),\nRandomForestClassifier/Regressor\n(\nSPARK-9478\n),\nGBTClassifier/Regressor\n(\nSPARK-9612\n),\nMulticlassClassificationEvaluator\n(\nSPARK-24101\n),\nRegressionEvaluator\n(\nSPARK-24102\n),\nBinaryClassificationEvaluator\n(\nSPARK-24103\n),\nBisectingKMeans\n(\nSPARK-30351\n),\nKMeans\n(\nSPARK-29967\n) and\nGaussianMixture\n(\nSPARK-30102\n).\nR API for\nPowerIterationClustering\nwas added\n(\nSPARK-19827\n).\nAdded Spark ML listener for tracking ML pipeline status\n(\nSPARK-23674\n).\nFit with validation set was added to Gradient Boosted Trees in Python\n(\nSPARK-24333\n).\nRobustScaler\ntransformer was added\n(\nSPARK-28399\n).\nFactorization Machines\nclassifier and regressor were added\n(\nSPARK-29224\n).\nGaussian Naive Bayes Classifier (\nSPARK-16872\n) and Complement Naive Bayes Classifier (\nSPARK-29942\n) were added.\nML function parity between Scala and Python\n(\nSPARK-28958\n).\npredictRaw\nis made public in all the Classification models.\npredictProbability\nis made public in all the Classification models except\nLinearSVCModel\n(\nSPARK-30358\n).\nMigration Guide\nThe migration guide is now archived\non this page\n.\nTo learn more about the benefits and background of system optimised natives, you may wish to\nwatch Sam Halliday’s ScalaX talk on\nHigh Performance Linear Algebra in Scala\n.\n↩"}
{"url": "https://spark.apache.org/docs/latest/sql-programming-guide.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nSpark SQL, DataFrames and Datasets Guide\nSpark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided\nby Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally,\nSpark SQL uses this extra information to perform extra optimizations. There are several ways to\ninteract with Spark SQL including SQL and the Dataset API. When computing a result,\nthe same execution engine is used, independent of which API/language you are using to express the\ncomputation. This unification means that developers can easily switch back and forth between\ndifferent APIs based on which provides the most natural way to express a given transformation.\nAll of the examples on this page use sample data included in the Spark distribution and can be run in\nthe\nspark-shell\n,\npyspark\nshell, or\nsparkR\nshell.\nSQL\nOne use of Spark SQL is to execute SQL queries.\nSpark SQL can also be used to read data from an existing Hive installation. For more on how to\nconfigure this feature, please refer to the\nHive Tables\nsection. When running\nSQL from within another programming language the results will be returned as a\nDataset/DataFrame\n.\nYou can also interact with the SQL interface using the\ncommand-line\nor over\nJDBC/ODBC\n.\nDatasets and DataFrames\nA Dataset is a distributed collection of data.\nDataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong\ntyping, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized\nexecution engine. A Dataset can be\nconstructed\nfrom JVM objects and then\nmanipulated using functional transformations (\nmap\n,\nflatMap\n,\nfilter\n, etc.).\nThe Dataset API is available in\nScala\nand\nJava\n. Python does not have the support for the Dataset API. But due to Python’s dynamic nature,\nmany of the benefits of the Dataset API are already available (i.e. you can access the field of a row by name naturally\nrow.columnName\n). The case for R is similar.\nA DataFrame is a\nDataset\norganized into named columns. It is conceptually\nequivalent to a table in a relational database or a data frame in R/Python, but with richer\noptimizations under the hood. DataFrames can be constructed from a wide array of\nsources\nsuch\nas: structured data files, tables in Hive, external databases, or existing RDDs.\nThe DataFrame API is available in\nPython\n, Scala,\nJava and\nR\n.\nIn Scala and Java, a DataFrame is represented by a Dataset of\nRow\ns.\nIn\nthe Scala API\n,\nDataFrame\nis simply a type alias of\nDataset[Row]\n.\nWhile, in\nJava API\n, users need to use\nDataset<Row>\nto represent a\nDataFrame\n.\nThroughout this document, we will often refer to Scala/Java Datasets of\nRow\ns as DataFrames."}
{"url": "https://spark.apache.org/docs/latest/streaming/index.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nStructured Streaming Programming Guide\nOverview\nGetting Started\nAPIs on DataFrames and Datasets\nPerformance Tips\nAdditional Information\nStructured Streaming Programming Guide\nOverview\nStructured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. You can express your streaming computation the same way you would express a batch computation on static data. The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive. You can use the\nDataset/DataFrame API\nin Scala, Java, Python or R to express streaming aggregations, event-time windows, stream-to-batch joins, etc. The computation is executed on the same optimized Spark SQL engine. Finally, the system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs. In short,\nStructured Streaming provides fast, scalable, fault-tolerant, end-to-end exactly-once stream processing without the user having to reason about streaming.\nInternally, by default, Structured Streaming queries are processed using a\nmicro-batch processing\nengine, which processes data streams as a series of small batch jobs thereby achieving end-to-end latencies as low as 100 milliseconds and exactly-once fault-tolerance guarantees. However, since Spark 2.3, we have introduced a new low-latency processing mode called\nContinuous Processing\n, which can achieve end-to-end latencies as low as 1 millisecond with at-least-once guarantees. Without changing the Dataset/DataFrame operations in your queries, you will be able to choose the mode based on your application requirements.\nIn this guide, we are going to walk you through the programming model and the APIs. We are going to explain the concepts mostly using the default micro-batch processing model, and then\nlater\ndiscuss Continuous Processing model. First, let’s start with a simple example of a Structured Streaming query - a streaming word count."}
{"url": "https://spark.apache.org/docs/latest/graphx-programming-guide.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nGraphX Programming Guide\nOverview\nGetting Started\nThe Property Graph\nExample Property Graph\nGraph Operators\nSummary List of Operators\nProperty Operators\nStructural Operators\nJoin Operators\nNeighborhood Aggregation\nAggregate Messages (aggregateMessages)\nMap Reduce Triplets Transition Guide (Legacy)\nComputing Degree Information\nCollecting Neighbors\nCaching and Uncaching\nPregel API\nGraph Builders\nVertex and Edge RDDs\nVertexRDDs\nEdgeRDDs\nOptimized Representation\nGraph Algorithms\nPageRank\nConnected Components\nTriangle Counting\nExamples\nOverview\nGraphX is a new component in Spark for graphs and graph-parallel computation. At a high level,\nGraphX extends the Spark\nRDD\nby introducing a\nnew\nGraph\nabstraction: a directed multigraph with properties\nattached to each vertex and edge.  To support graph computation, GraphX exposes a set of fundamental\noperators (e.g.,\nsubgraph\n,\njoinVertices\n, and\naggregateMessages\n) as well as an optimized variant of the\nPregel\nAPI. In addition, GraphX includes a growing collection of graph\nalgorithms\nand\nbuilders\nto simplify graph analytics tasks.\nGetting Started\nTo get started you first need to import Spark and GraphX into your project, as follows:\nimport\norg.apache.spark._\nimport\norg.apache.spark.graphx._\n// To make some of the examples work we will also need RDD\nimport\norg.apache.spark.rdd.RDD\nIf you are not using the Spark shell you will also need a\nSparkContext\n.  To learn more about\ngetting started with Spark refer to the\nSpark Quick Start Guide\n.\nThe Property Graph\nThe\nproperty graph\nis a directed multigraph\nwith user defined objects attached to each vertex and edge.  A directed multigraph is a directed\ngraph with potentially multiple parallel edges sharing the same source and destination vertex.  The\nability to support parallel edges simplifies modeling scenarios where there can be multiple\nrelationships (e.g., co-worker and friend) between the same vertices.  Each vertex is keyed by a\nunique\n64-bit long identifier (\nVertexId\n).  GraphX does not impose any ordering constraints on\nthe vertex identifiers.  Similarly, edges have corresponding source and destination vertex\nidentifiers.\nThe property graph is parameterized over the vertex (\nVD\n) and edge (\nED\n) types.  These\nare the types of the objects associated with each vertex and edge respectively.\nGraphX optimizes the representation of vertex and edge types when they are primitive data types\n(e.g., int, double, etc…) reducing the in memory footprint by storing them in specialized\narrays.\nIn some cases it may be desirable to have vertices with different property types in the same graph.\nThis can be accomplished through inheritance.  For example to model users and products as a\nbipartite graph we might do the following:\nclass\nVertexProperty\n()\ncase\nclass\nUserProperty\n(\nval\nname\n:\nString\n)\nextends\nVertexProperty\ncase\nclass\nProductProperty\n(\nval\nname\n:\nString\n,\nval\nprice\n:\nDouble\n)\nextends\nVertexProperty\n// The graph might then have the type:\nvar\ngraph\n:\nGraph\n[\nVertexProperty\n,\nString\n]\n=\nnull\nLike RDDs, property graphs are immutable, distributed, and fault-tolerant.  Changes to the values or\nstructure of the graph are accomplished by producing a new graph with the desired changes.  Note\nthat substantial parts of the original graph (i.e., unaffected structure, attributes, and indices)\nare reused in the new graph reducing the cost of this inherently functional data structure.  The\ngraph is partitioned across the executors using a range of vertex partitioning heuristics.  As with\nRDDs, each partition of the graph can be recreated on a different machine in the event of a failure.\nLogically the property graph corresponds to a pair of typed collections (RDDs) encoding the\nproperties for each vertex and edge.  As a consequence, the graph class contains members to access\nthe vertices and edges of the graph:\nclass\nGraph\n[\nVD\n,\nED\n]\n{\nval\nvertices\n:\nVertexRDD\n[\nVD\n]\nval\nedges\n:\nEdgeRDD\n[\nED\n]\n}\nThe classes\nVertexRDD[VD]\nand\nEdgeRDD[ED]\nextend and are optimized versions of\nRDD[(VertexId,\nVD)]\nand\nRDD[Edge[ED]]\nrespectively.  Both\nVertexRDD[VD]\nand\nEdgeRDD[ED]\nprovide  additional\nfunctionality built around graph computation and leverage internal optimizations.  We discuss the\nVertexRDD\nVertexRDD\nand\nEdgeRDD\nEdgeRDD\nAPI in greater detail in the section on\nvertex and edge\nRDDs\nbut for now they can be thought of as simply RDDs of the form:\nRDD[(VertexId, VD)]\nand\nRDD[Edge[ED]]\n.\nExample Property Graph\nSuppose we want to construct a property graph consisting of the various collaborators on the GraphX\nproject. The vertex property might contain the username and occupation.  We could annotate edges\nwith a string describing the relationships between collaborators:\nThe resulting graph would have the type signature:\nval\nuserGraph\n:\nGraph\n[(\nString\n,\nString\n)\n,\nString\n]\nThere are numerous ways to construct a property graph from raw files, RDDs, and even synthetic\ngenerators and these are discussed in more detail in the section on\ngraph builders\n.  Probably the most general method is to use the\nGraph object\n.  For example the following\ncode constructs a graph from a collection of RDDs:\n// Assume the SparkContext has already been constructed\nval\nsc\n:\nSparkContext\n// Create an RDD for the vertices\nval\nusers\n:\nRDD\n[(\nVertexId\n,\n(\nString\n,\nString\n))]\n=\nsc\n.\nparallelize\n(\nSeq\n((\n3L\n,\n(\n\"rxin\"\n,\n\"student\"\n)),\n(\n7L\n,\n(\n\"jgonzal\"\n,\n\"postdoc\"\n)),\n(\n5L\n,\n(\n\"franklin\"\n,\n\"prof\"\n)),\n(\n2L\n,\n(\n\"istoica\"\n,\n\"prof\"\n))))\n// Create an RDD for edges\nval\nrelationships\n:\nRDD\n[\nEdge\n[\nString\n]]\n=\nsc\n.\nparallelize\n(\nSeq\n(\nEdge\n(\n3L\n,\n7L\n,\n\"collab\"\n),\nEdge\n(\n5L\n,\n3L\n,\n\"advisor\"\n),\nEdge\n(\n2L\n,\n5L\n,\n\"colleague\"\n),\nEdge\n(\n5L\n,\n7L\n,\n\"pi\"\n)))\n// Define a default user in case there are relationship with missing user\nval\ndefaultUser\n=\n(\n\"John Doe\"\n,\n\"Missing\"\n)\n// Build the initial Graph\nval\ngraph\n=\nGraph\n(\nusers\n,\nrelationships\n,\ndefaultUser\n)\nIn the above example we make use of the\nEdge\ncase class. Edges have a\nsrcId\nand a\ndstId\ncorresponding to the source and destination vertex identifiers. In addition, the\nEdge\nclass has an\nattr\nmember which stores the edge property.\nWe can deconstruct a graph into the respective vertex and edge views by using the\ngraph.vertices\nand\ngraph.edges\nmembers respectively.\nval\ngraph\n:\nGraph\n[(\nString\n,\nString\n)\n,\nString\n]\n// Constructed from above\n// Count all users which are postdocs\ngraph\n.\nvertices\n.\nfilter\n{\ncase\n(\nid\n,\n(\nname\n,\npos\n))\n=>\npos\n==\n\"postdoc\"\n}.\ncount\n// Count all the edges where src > dst\ngraph\n.\nedges\n.\nfilter\n(\ne\n=>\ne\n.\nsrcId\n>\ne\n.\ndstId\n).\ncount\nNote that\ngraph.vertices\nreturns an\nVertexRDD[(String, String)]\nwhich extends\nRDD[(VertexId, (String, String))]\nand so we use the scala\ncase\nexpression to deconstruct the\ntuple.  On the other hand,\ngraph.edges\nreturns an\nEdgeRDD\ncontaining\nEdge[String]\nobjects.\nWe could have also used the case class type constructor as in the following:\ngraph\n.\nedges\n.\nfilter\n{\ncase\nEdge\n(\nsrc\n,\ndst\n,\nprop\n)\n=>\nsrc\n>\ndst\n}.\ncount\nIn addition to the vertex and edge views of the property graph, GraphX also exposes a triplet view.\nThe triplet view logically joins the vertex and edge properties yielding an\nRDD[EdgeTriplet[VD, ED]]\ncontaining instances of the\nEdgeTriplet\nclass. This\njoin\ncan be expressed in the following SQL expression:\nSELECT\nsrc\n.\nid\n,\ndst\n.\nid\n,\nsrc\n.\nattr\n,\ne\n.\nattr\n,\ndst\n.\nattr\nFROM\nedges\nAS\ne\nLEFT\nJOIN\nvertices\nAS\nsrc\n,\nvertices\nAS\ndst\nON\ne\n.\nsrcId\n=\nsrc\n.\nId\nAND\ne\n.\ndstId\n=\ndst\n.\nId\nor graphically as:\nThe\nEdgeTriplet\nclass extends the\nEdge\nclass by adding the\nsrcAttr\nand\ndstAttr\nmembers which contain the source and destination properties respectively. We can use the\ntriplet view of a graph to render a collection of strings describing relationships between users.\nval\ngraph\n:\nGraph\n[(\nString\n,\nString\n)\n,\nString\n]\n// Constructed from above\n// Use the triplets view to create an RDD of facts.\nval\nfacts\n:\nRDD\n[\nString\n]\n=\ngraph\n.\ntriplets\n.\nmap\n(\ntriplet\n=>\ntriplet\n.\nsrcAttr\n.\n_1\n+\n\" is the \"\n+\ntriplet\n.\nattr\n+\n\" of \"\n+\ntriplet\n.\ndstAttr\n.\n_1\n)\nfacts\n.\ncollect\n.\nforeach\n(\nprintln\n(\n_\n))\nGraph Operators\nJust as RDDs have basic operations like\nmap\n,\nfilter\n, and\nreduceByKey\n, property graphs also\nhave a collection of basic operators that take user defined functions and produce new graphs with\ntransformed properties and structure.  The core operators that have optimized implementations are\ndefined in\nGraph\nand convenient operators that are expressed as a compositions of the\ncore operators are defined in\nGraphOps\n.  However, thanks to Scala implicits the\noperators in\nGraphOps\nare automatically available as members of\nGraph\n.  For example, we can\ncompute the in-degree of each vertex (defined in\nGraphOps\n) by the following:\nval\ngraph\n:\nGraph\n[(\nString\n,\nString\n)\n,\nString\n]\n// Use the implicit GraphOps.inDegrees operator\nval\ninDegrees\n:\nVertexRDD\n[\nInt\n]\n=\ngraph\n.\ninDegrees\nThe reason for differentiating between core graph operations and\nGraphOps\nis to be\nable to support different graph representations in the future.  Each graph representation must\nprovide implementations of the core operations and reuse many of the useful operations defined in\nGraphOps\n.\nSummary List of Operators\nThe following is a quick summary of the functionality defined in both\nGraph\nand\nGraphOps\nbut presented as members of Graph for simplicity. Note that some function\nsignatures have been simplified (e.g., default arguments and type constraints removed) and some more\nadvanced functionality has been removed so please consult the API docs for the official list of\noperations.\n/** Summary of the functionality in the property graph */\nclass\nGraph\n[\nVD\n,\nED\n]\n{\n// Information about the Graph ===================================================================\nval\nnumEdges\n:\nLong\nval\nnumVertices\n:\nLong\nval\ninDegrees\n:\nVertexRDD\n[\nInt\n]\nval\noutDegrees\n:\nVertexRDD\n[\nInt\n]\nval\ndegrees\n:\nVertexRDD\n[\nInt\n]\n// Views of the graph as collections =============================================================\nval\nvertices\n:\nVertexRDD\n[\nVD\n]\nval\nedges\n:\nEdgeRDD\n[\nED\n]\nval\ntriplets\n:\nRDD\n[\nEdgeTriplet\n[\nVD\n,\nED\n]]\n// Functions for caching graphs ==================================================================\ndef\npersist\n(\nnewLevel\n:\nStorageLevel\n=\nStorageLevel\n.\nMEMORY_ONLY\n)\n:\nGraph\n[\nVD\n,\nED\n]\ndef\ncache\n()\n:\nGraph\n[\nVD\n,\nED\n]\ndef\nunpersistVertices\n(\nblocking\n:\nBoolean\n=\nfalse\n)\n:\nGraph\n[\nVD\n,\nED\n]\n// Change the partitioning heuristic  ============================================================\ndef\npartitionBy\n(\npartitionStrategy\n:\nPartitionStrategy\n)\n:\nGraph\n[\nVD\n,\nED\n]\n// Transform vertex and edge attributes ==========================================================\ndef\nmapVertices\n[\nVD2\n](\nmap\n:\n(\nVertexId\n,\nVD\n)\n=>\nVD2\n)\n:\nGraph\n[\nVD2\n,\nED\n]\ndef\nmapEdges\n[\nED2\n](\nmap\n:\nEdge\n[\nED\n]\n=>\nED2\n)\n:\nGraph\n[\nVD\n,\nED2\n]\ndef\nmapEdges\n[\nED2\n](\nmap\n:\n(\nPartitionID\n,\nIterator\n[\nEdge\n[\nED\n]])\n=>\nIterator\n[\nED2\n])\n:\nGraph\n[\nVD\n,\nED2\n]\ndef\nmapTriplets\n[\nED2\n](\nmap\n:\nEdgeTriplet\n[\nVD\n,\nED\n]\n=>\nED2\n)\n:\nGraph\n[\nVD\n,\nED2\n]\ndef\nmapTriplets\n[\nED2\n](\nmap\n:\n(\nPartitionID\n,\nIterator\n[\nEdgeTriplet\n[\nVD\n,\nED\n]])\n=>\nIterator\n[\nED2\n])\n:\nGraph\n[\nVD\n,\nED2\n]\n// Modify the graph structure ====================================================================\ndef\nreverse\n:\nGraph\n[\nVD\n,\nED\n]\ndef\nsubgraph\n(\nepred\n:\nEdgeTriplet\n[\nVD\n,\nED\n]\n=>\nBoolean\n=\n(\nx\n=>\ntrue\n),\nvpred\n:\n(\nVertexId\n,\nVD\n)\n=>\nBoolean\n=\n((\nv\n,\nd\n)\n=>\ntrue\n))\n:\nGraph\n[\nVD\n,\nED\n]\ndef\nmask\n[\nVD2\n,\nED2\n](\nother\n:\nGraph\n[\nVD2\n,\nED2\n])\n:\nGraph\n[\nVD\n,\nED\n]\ndef\ngroupEdges\n(\nmerge\n:\n(\nED\n,\nED\n)\n=>\nED\n)\n:\nGraph\n[\nVD\n,\nED\n]\n// Join RDDs with the graph ======================================================================\ndef\njoinVertices\n[\nU\n](\ntable\n:\nRDD\n[(\nVertexId\n,\nU\n)])(\nmapFunc\n:\n(\nVertexId\n,\nVD\n,\nU\n)\n=>\nVD\n)\n:\nGraph\n[\nVD\n,\nED\n]\ndef\nouterJoinVertices\n[\nU\n,\nVD2\n](\nother\n:\nRDD\n[(\nVertexId\n,\nU\n)])\n(\nmapFunc\n:\n(\nVertexId\n,\nVD\n,\nOption\n[\nU\n])\n=>\nVD2\n)\n:\nGraph\n[\nVD2\n,\nED\n]\n// Aggregate information about adjacent triplets =================================================\ndef\ncollectNeighborIds\n(\nedgeDirection\n:\nEdgeDirection\n)\n:\nVertexRDD\n[\nArray\n[\nVertexId\n]]\ndef\ncollectNeighbors\n(\nedgeDirection\n:\nEdgeDirection\n)\n:\nVertexRDD\n[\nArray\n[(\nVertexId\n,\nVD\n)]]\ndef\naggregateMessages\n[\nMsg:\nClassTag\n](\nsendMsg\n:\nEdgeContext\n[\nVD\n,\nED\n,\nMsg\n]\n=>\nUnit\n,\nmergeMsg\n:\n(\nMsg\n,\nMsg\n)\n=>\nMsg\n,\ntripletFields\n:\nTripletFields\n=\nTripletFields\n.\nAll\n)\n:\nVertexRDD\n[\nA\n]\n// Iterative graph-parallel computation ==========================================================\ndef\npregel\n[\nA\n](\ninitialMsg\n:\nA\n,\nmaxIterations\n:\nInt\n,\nactiveDirection\n:\nEdgeDirection\n)(\nvprog\n:\n(\nVertexId\n,\nVD\n,\nA\n)\n=>\nVD\n,\nsendMsg\n:\nEdgeTriplet\n[\nVD\n,\nED\n]\n=>\nIterator\n[(\nVertexId\n,\nA\n)],\nmergeMsg\n:\n(\nA\n,\nA\n)\n=>\nA\n)\n:\nGraph\n[\nVD\n,\nED\n]\n// Basic graph algorithms ========================================================================\ndef\npageRank\n(\ntol\n:\nDouble\n,\nresetProb\n:\nDouble\n=\n0.15\n)\n:\nGraph\n[\nDouble\n,\nDouble\n]\ndef\nconnectedComponents\n()\n:\nGraph\n[\nVertexId\n,\nED\n]\ndef\ntriangleCount\n()\n:\nGraph\n[\nInt\n,\nED\n]\ndef\nstronglyConnectedComponents\n(\nnumIter\n:\nInt\n)\n:\nGraph\n[\nVertexId\n,\nED\n]\n}\nProperty Operators\nLike the RDD\nmap\noperator, the property graph contains the following:\nclass\nGraph\n[\nVD\n,\nED\n]\n{\ndef\nmapVertices\n[\nVD2\n](\nmap\n:\n(\nVertexId\n,\nVD\n)\n=>\nVD2\n)\n:\nGraph\n[\nVD2\n,\nED\n]\ndef\nmapEdges\n[\nED2\n](\nmap\n:\nEdge\n[\nED\n]\n=>\nED2\n)\n:\nGraph\n[\nVD\n,\nED2\n]\ndef\nmapTriplets\n[\nED2\n](\nmap\n:\nEdgeTriplet\n[\nVD\n,\nED\n]\n=>\nED2\n)\n:\nGraph\n[\nVD\n,\nED2\n]\n}\nEach of these operators yields a new graph with the vertex or edge properties modified by the user\ndefined\nmap\nfunction.\nNote that in each case the graph structure is unaffected. This is a key feature of these operators\nwhich allows the resulting graph to reuse the structural indices of the original graph. The\nfollowing snippets are logically equivalent, but the first one does not preserve the structural\nindices and would not benefit from the GraphX system optimizations:\nval\nnewVertices\n=\ngraph\n.\nvertices\n.\nmap\n{\ncase\n(\nid\n,\nattr\n)\n=>\n(\nid\n,\nmapUdf\n(\nid\n,\nattr\n))\n}\nval\nnewGraph\n=\nGraph\n(\nnewVertices\n,\ngraph\n.\nedges\n)\nInstead, use\nmapVertices\nto preserve the indices:\nval\nnewGraph\n=\ngraph\n.\nmapVertices\n((\nid\n,\nattr\n)\n=>\nmapUdf\n(\nid\n,\nattr\n))\nThese operators are often used to initialize the graph for a particular computation or project away\nunnecessary properties.  For example, given a graph with the out degrees as the vertex properties\n(we describe how to construct such a graph later), we initialize it for PageRank:\n// Given a graph where the vertex property is the out degree\nval\ninputGraph\n:\nGraph\n[\nInt\n,\nString\n]\n=\ngraph\n.\nouterJoinVertices\n(\ngraph\n.\noutDegrees\n)((\nvid\n,\n_\n,\ndegOpt\n)\n=>\ndegOpt\n.\ngetOrElse\n(\n0\n))\n// Construct a graph where each edge contains the weight\n// and each vertex is the initial PageRank\nval\noutputGraph\n:\nGraph\n[\nDouble\n,\nDouble\n]\n=\ninputGraph\n.\nmapTriplets\n(\ntriplet\n=>\n1.0\n/\ntriplet\n.\nsrcAttr\n).\nmapVertices\n((\nid\n,\n_\n)\n=>\n1.0\n)\nStructural Operators\nCurrently GraphX supports only a simple set of commonly used structural operators and we expect to\nadd more in the future.  The following is a list of the basic structural operators.\nclass\nGraph\n[\nVD\n,\nED\n]\n{\ndef\nreverse\n:\nGraph\n[\nVD\n,\nED\n]\ndef\nsubgraph\n(\nepred\n:\nEdgeTriplet\n[\nVD\n,\nED\n]\n=>\nBoolean\n,\nvpred\n:\n(\nVertexId\n,\nVD\n)\n=>\nBoolean\n)\n:\nGraph\n[\nVD\n,\nED\n]\ndef\nmask\n[\nVD2\n,\nED2\n](\nother\n:\nGraph\n[\nVD2\n,\nED2\n])\n:\nGraph\n[\nVD\n,\nED\n]\ndef\ngroupEdges\n(\nmerge\n:\n(\nED\n,\nED\n)\n=>\nED\n)\n:\nGraph\n[\nVD\n,\nED\n]\n}\nThe\nreverse\noperator returns a new graph with all the edge directions reversed.\nThis can be useful when, for example, trying to compute the inverse PageRank.  Because the reverse\noperation does not modify vertex or edge properties or change the number of edges, it can be\nimplemented efficiently without data movement or duplication.\nThe\nsubgraph\noperator takes vertex and edge predicates and returns the graph\ncontaining only the vertices that satisfy the vertex predicate (evaluate to true) and edges that\nsatisfy the edge predicate\nand connect vertices that satisfy the vertex predicate\n.  The\nsubgraph\noperator can be used in number of situations to restrict the graph to the vertices and edges of\ninterest or eliminate broken links. For example in the following code we remove broken links:\n// Create an RDD for the vertices\nval\nusers\n:\nRDD\n[(\nVertexId\n,\n(\nString\n,\nString\n))]\n=\nsc\n.\nparallelize\n(\nSeq\n((\n3L\n,\n(\n\"rxin\"\n,\n\"student\"\n)),\n(\n7L\n,\n(\n\"jgonzal\"\n,\n\"postdoc\"\n)),\n(\n5L\n,\n(\n\"franklin\"\n,\n\"prof\"\n)),\n(\n2L\n,\n(\n\"istoica\"\n,\n\"prof\"\n)),\n(\n4L\n,\n(\n\"peter\"\n,\n\"student\"\n))))\n// Create an RDD for edges\nval\nrelationships\n:\nRDD\n[\nEdge\n[\nString\n]]\n=\nsc\n.\nparallelize\n(\nSeq\n(\nEdge\n(\n3L\n,\n7L\n,\n\"collab\"\n),\nEdge\n(\n5L\n,\n3L\n,\n\"advisor\"\n),\nEdge\n(\n2L\n,\n5L\n,\n\"colleague\"\n),\nEdge\n(\n5L\n,\n7L\n,\n\"pi\"\n),\nEdge\n(\n4L\n,\n0L\n,\n\"student\"\n),\nEdge\n(\n5L\n,\n0L\n,\n\"colleague\"\n)))\n// Define a default user in case there are relationship with missing user\nval\ndefaultUser\n=\n(\n\"John Doe\"\n,\n\"Missing\"\n)\n// Build the initial Graph\nval\ngraph\n=\nGraph\n(\nusers\n,\nrelationships\n,\ndefaultUser\n)\n// Notice that there is a user 0 (for which we have no information) connected to users\n// 4 (peter) and 5 (franklin).\ngraph\n.\ntriplets\n.\nmap\n(\ntriplet\n=>\ntriplet\n.\nsrcAttr\n.\n_1\n+\n\" is the \"\n+\ntriplet\n.\nattr\n+\n\" of \"\n+\ntriplet\n.\ndstAttr\n.\n_1\n).\ncollect\n.\nforeach\n(\nprintln\n(\n_\n))\n// Remove missing vertices as well as the edges to connected to them\nval\nvalidGraph\n=\ngraph\n.\nsubgraph\n(\nvpred\n=\n(\nid\n,\nattr\n)\n=>\nattr\n.\n_2\n!=\n\"Missing\"\n)\n// The valid subgraph will disconnect users 4 and 5 by removing user 0\nvalidGraph\n.\nvertices\n.\ncollect\n.\nforeach\n(\nprintln\n(\n_\n))\nvalidGraph\n.\ntriplets\n.\nmap\n(\ntriplet\n=>\ntriplet\n.\nsrcAttr\n.\n_1\n+\n\" is the \"\n+\ntriplet\n.\nattr\n+\n\" of \"\n+\ntriplet\n.\ndstAttr\n.\n_1\n).\ncollect\n.\nforeach\n(\nprintln\n(\n_\n))\nNote in the above example only the vertex predicate is provided.  The\nsubgraph\noperator defaults\nto\ntrue\nif the vertex or edge predicates are not provided.\nThe\nmask\noperator constructs a subgraph by returning a graph that contains the\nvertices and edges that are also found in the input graph.  This can be used in conjunction with the\nsubgraph\noperator to restrict a graph based on the properties in another related graph.  For\nexample, we might run connected components using the graph with missing vertices and then restrict\nthe answer to the valid subgraph.\n// Run Connected Components\nval\nccGraph\n=\ngraph\n.\nconnectedComponents\n()\n// No longer contains missing field\n// Remove missing vertices as well as the edges to connected to them\nval\nvalidGraph\n=\ngraph\n.\nsubgraph\n(\nvpred\n=\n(\nid\n,\nattr\n)\n=>\nattr\n.\n_2\n!=\n\"Missing\"\n)\n// Restrict the answer to the valid subgraph\nval\nvalidCCGraph\n=\nccGraph\n.\nmask\n(\nvalidGraph\n)\nThe\ngroupEdges\noperator merges parallel edges (i.e., duplicate edges between\npairs of vertices) in the multigraph.  In many numerical applications, parallel edges can be\nadded\n(their weights combined) into a single edge thereby reducing the size of the graph.\nJoin Operators\nIn many cases it is necessary to join data from external collections (RDDs) with graphs.  For\nexample, we might have extra user properties that we want to merge with an existing graph or we\nmight want to pull vertex properties from one graph into another.  These tasks can be accomplished\nusing the\njoin\noperators. Below we list the key join operators:\nclass\nGraph\n[\nVD\n,\nED\n]\n{\ndef\njoinVertices\n[\nU\n](\ntable\n:\nRDD\n[(\nVertexId\n,\nU\n)])(\nmap\n:\n(\nVertexId\n,\nVD\n,\nU\n)\n=>\nVD\n)\n:\nGraph\n[\nVD\n,\nED\n]\ndef\nouterJoinVertices\n[\nU\n,\nVD2\n](\ntable\n:\nRDD\n[(\nVertexId\n,\nU\n)])(\nmap\n:\n(\nVertexId\n,\nVD\n,\nOption\n[\nU\n])\n=>\nVD2\n)\n:\nGraph\n[\nVD2\n,\nED\n]\n}\nThe\njoinVertices\noperator joins the vertices with the input RDD and\nreturns a new graph with the vertex properties obtained by applying the user defined\nmap\nfunction\nto the result of the joined vertices.  Vertices without a matching value in the RDD retain their\noriginal value.\nNote that if the RDD contains more than one value for a given vertex only one will be used.  It\nis therefore recommended that the input RDD be made unique using the following which will\nalso\npre-index\nthe resulting values to substantially accelerate the subsequent join.\nval\nnonUniqueCosts\n:\nRDD\n[(\nVertexId\n,\nDouble\n)]\nval\nuniqueCosts\n:\nVertexRDD\n[\nDouble\n]\n=\ngraph\n.\nvertices\n.\naggregateUsingIndex\n(\nnonUnique\n,\n(\na\n,\nb\n)\n=>\na\n+\nb\n)\nval\njoinedGraph\n=\ngraph\n.\njoinVertices\n(\nuniqueCosts\n)(\n(\nid\n,\noldCost\n,\nextraCost\n)\n=>\noldCost\n+\nextraCost\n)\nThe more general\nouterJoinVertices\nbehaves similarly to\njoinVertices\nexcept that the user defined\nmap\nfunction is applied to all vertices and can change the vertex\nproperty type.  Because not all vertices may have a matching value in the input RDD the\nmap\nfunction takes an\nOption\ntype.  For example, we can set up a graph for PageRank by initializing\nvertex properties with their\noutDegree\n.\nval\noutDegrees\n:\nVertexRDD\n[\nInt\n]\n=\ngraph\n.\noutDegrees\nval\ndegreeGraph\n=\ngraph\n.\nouterJoinVertices\n(\noutDegrees\n)\n{\n(\nid\n,\noldAttr\n,\noutDegOpt\n)\n=>\noutDegOpt\nmatch\n{\ncase\nSome\n(\noutDeg\n)\n=>\noutDeg\ncase\nNone\n=>\n0\n// No outDegree means zero outDegree\n}\n}\nYou may have noticed the multiple parameter lists (e.g.,\nf(a)(b)\n) curried function pattern used\nin the above examples.  While we could have equally written\nf(a)(b)\nas\nf(a,b)\nthis would mean\nthat type inference on\nb\nwould not depend on\na\n.  As a consequence, the user would need to\nprovide type annotation for the user defined function:\nval\njoinedGraph\n=\ngraph\n.\njoinVertices\n(\nuniqueCosts\n,\n(\nid\n:\nVertexId\n,\noldCost\n:\nDouble\n,\nextraCost\n:\nDouble\n)\n=>\noldCost\n+\nextraCost\n)\nNeighborhood Aggregation\nA key step in many graph analytics tasks is aggregating information about the neighborhood of each\nvertex.\nFor example, we might want to know the number of followers each user has or the average age of\nthe followers of each user.  Many iterative graph algorithms (e.g., PageRank, Shortest Path, and\nconnected components) repeatedly aggregate properties of neighboring vertices (e.g., current\nPageRank Value, shortest path to the source, and smallest reachable vertex id).\nTo improve performance the primary aggregation operator changed from\ngraph.mapReduceTriplets\nto the new\ngraph.AggregateMessages\n.  While the changes in the API are\nrelatively small, we provide a transition guide below.\nAggregate Messages (aggregateMessages)\nThe core aggregation operation in GraphX is\naggregateMessages\n.\nThis operator applies a user defined\nsendMsg\nfunction to each\nedge triplet\nin the graph\nand then uses the\nmergeMsg\nfunction to aggregate those messages at their destination vertex.\nclass\nGraph\n[\nVD\n,\nED\n]\n{\ndef\naggregateMessages\n[\nMsg:\nClassTag\n](\nsendMsg\n:\nEdgeContext\n[\nVD\n,\nED\n,\nMsg\n]\n=>\nUnit\n,\nmergeMsg\n:\n(\nMsg\n,\nMsg\n)\n=>\nMsg\n,\ntripletFields\n:\nTripletFields\n=\nTripletFields\n.\nAll\n)\n:\nVertexRDD\n[\nMsg\n]\n}\nThe user defined\nsendMsg\nfunction takes an\nEdgeContext\n, which exposes the\nsource and destination attributes along with the edge attribute and functions\n(\nsendToSrc\n, and\nsendToDst\n) to send\nmessages to the source and destination attributes.  Think of\nsendMsg\nas the\nmap\nfunction in map-reduce.\nThe user defined\nmergeMsg\nfunction takes two messages destined to the same vertex and\nyields a single message.  Think of\nmergeMsg\nas the\nreduce\nfunction in map-reduce.\nThe\naggregateMessages\noperator returns a\nVertexRDD[Msg]\ncontaining the aggregate message (of type\nMsg\n) destined to each vertex.  Vertices that did not\nreceive a message are not included in the returned\nVertexRDD\nVertexRDD\n.\nIn addition,\naggregateMessages\ntakes an optional\ntripletsFields\nwhich indicates what data is accessed in the\nEdgeContext\n(i.e., the source vertex attribute but not the destination vertex attribute).\nThe possible options for the\ntripletsFields\nare defined in\nTripletFields\nand\nthe default value is\nTripletFields.All\nwhich indicates that the user\ndefined\nsendMsg\nfunction may access any of the fields in the\nEdgeContext\n.\nThe\ntripletFields\nargument can be used to notify GraphX that only part of the\nEdgeContext\nwill be needed allowing GraphX to select an optimized join strategy.\nFor example if we are computing the average age of the followers of each user we would only require\nthe source field and so we would use\nTripletFields.Src\nto indicate that we\nonly require the source field\nIn earlier versions of GraphX we used byte code inspection to infer the\nTripletFields\nhowever we have found that bytecode inspection to be\nslightly unreliable and instead opted for more explicit user control.\nIn the following example we use the\naggregateMessages\noperator to\ncompute the average age of the more senior followers of each user.\nimport\norg.apache.spark.graphx.\n{\nGraph\n,\nVertexRDD\n}\nimport\norg.apache.spark.graphx.util.GraphGenerators\n// Create a graph with \"age\" as the vertex property.\n// Here we use a random graph for simplicity.\nval\ngraph\n:\nGraph\n[\nDouble\n,\nInt\n]\n=\nGraphGenerators\n.\nlogNormalGraph\n(\nsc\n,\nnumVertices\n=\n100\n).\nmapVertices\n(\n(\nid\n,\n_\n)\n=>\nid\n.\ntoDouble\n)\n// Compute the number of older followers and their total age\nval\nolderFollowers\n:\nVertexRDD\n[(\nInt\n,\nDouble\n)]\n=\ngraph\n.\naggregateMessages\n[(\nInt\n,\nDouble\n)](\ntriplet\n=>\n{\n// Map Function\nif\n(\ntriplet\n.\nsrcAttr\n>\ntriplet\n.\ndstAttr\n)\n{\n// Send message to destination vertex containing counter and age\ntriplet\n.\nsendToDst\n((\n1\n,\ntriplet\n.\nsrcAttr\n))\n}\n},\n// Add counter and age\n(\na\n,\nb\n)\n=>\n(\na\n.\n_1\n+\nb\n.\n_1\n,\na\n.\n_2\n+\nb\n.\n_2\n)\n// Reduce Function\n)\n// Divide total age by number of older followers to get average age of older followers\nval\navgAgeOfOlderFollowers\n:\nVertexRDD\n[\nDouble\n]\n=\nolderFollowers\n.\nmapValues\n(\n(\nid\n,\nvalue\n)\n=>\nvalue\nmatch\n{\ncase\n(\ncount\n,\ntotalAge\n)\n=>\ntotalAge\n/\ncount\n}\n)\n// Display the results\navgAgeOfOlderFollowers\n.\ncollect\n().\nforeach\n(\nprintln\n(\n_\n))\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\" in the Spark repo.\nThe\naggregateMessages\noperation performs optimally when the messages (and the sums of\nmessages) are constant sized (e.g., floats and addition instead of lists and concatenation).\nMap Reduce Triplets Transition Guide (Legacy)\nIn earlier versions of GraphX neighborhood aggregation was accomplished using the\nmapReduceTriplets\noperator:\nclass\nGraph\n[\nVD\n,\nED\n]\n{\ndef\nmapReduceTriplets\n[\nMsg\n](\nmap\n:\nEdgeTriplet\n[\nVD\n,\nED\n]\n=>\nIterator\n[(\nVertexId\n,\nMsg\n)],\nreduce\n:\n(\nMsg\n,\nMsg\n)\n=>\nMsg\n)\n:\nVertexRDD\n[\nMsg\n]\n}\nThe\nmapReduceTriplets\noperator takes a user defined map function which\nis applied to each triplet and can yield\nmessages\nwhich are aggregated using the user defined\nreduce\nfunction.\nHowever, we found the user of the returned iterator to be expensive and it inhibited our ability to\napply additional optimizations (e.g., local vertex renumbering).\nIn\naggregateMessages\nwe introduced the EdgeContext which exposes the\ntriplet fields and also functions to explicitly send messages to the source and destination vertex.\nFurthermore we removed bytecode inspection and instead require the user to indicate what fields\nin the triplet are actually required.\nThe following code block using\nmapReduceTriplets\n:\nval\ngraph\n:\nGraph\n[\nInt\n,\nFloat\n]\n=\n...\ndef\nmsgFun\n(\ntriplet\n:\nTriplet\n[\nInt\n,\nFloat\n])\n:\nIterator\n[(\nInt\n,\nString\n)]\n=\n{\nIterator\n((\ntriplet\n.\ndstId\n,\n\"Hi\"\n))\n}\ndef\nreduceFun\n(\na\n:\nString\n,\nb\n:\nString\n)\n:\nString\n=\na\n+\n\" \"\n+\nb\nval\nresult\n=\ngraph\n.\nmapReduceTriplets\n[\nString\n](\nmsgFun\n,\nreduceFun\n)\ncan be rewritten using\naggregateMessages\nas:\nval\ngraph\n:\nGraph\n[\nInt\n,\nFloat\n]\n=\n...\ndef\nmsgFun\n(\ntriplet\n:\nEdgeContext\n[\nInt\n,\nFloat\n,\nString\n])\n{\ntriplet\n.\nsendToDst\n(\n\"Hi\"\n)\n}\ndef\nreduceFun\n(\na\n:\nString\n,\nb\n:\nString\n)\n:\nString\n=\na\n+\n\" \"\n+\nb\nval\nresult\n=\ngraph\n.\naggregateMessages\n[\nString\n](\nmsgFun\n,\nreduceFun\n)\nComputing Degree Information\nA common aggregation task is computing the degree of each vertex: the number of edges adjacent to\neach vertex.  In the context of directed graphs it is often necessary to know the in-degree, \nout-degree, and the total degree of each vertex.  The\nGraphOps\nclass contains a\ncollection of operators to compute the degrees of each vertex.  For example in the following we\ncompute the max in, out, and total degrees:\n// Define a reduce operation to compute the highest degree vertex\ndef\nmax\n(\na\n:\n(\nVertexId\n,\nInt\n),\nb\n:\n(\nVertexId\n,\nInt\n))\n:\n(\nVertexId\n,\nInt\n)\n=\n{\nif\n(\na\n.\n_2\n>\nb\n.\n_2\n)\na\nelse\nb\n}\n// Compute the max degrees\nval\nmaxInDegree\n:\n(\nVertexId\n,\nInt\n)\n=\ngraph\n.\ninDegrees\n.\nreduce\n(\nmax\n)\nval\nmaxOutDegree\n:\n(\nVertexId\n,\nInt\n)\n=\ngraph\n.\noutDegrees\n.\nreduce\n(\nmax\n)\nval\nmaxDegrees\n:\n(\nVertexId\n,\nInt\n)\n=\ngraph\n.\ndegrees\n.\nreduce\n(\nmax\n)\nCollecting Neighbors\nIn some cases it may be easier to express computation by collecting neighboring vertices and their\nattributes at each vertex. This can be easily accomplished using the\ncollectNeighborIds\nand the\ncollectNeighbors\noperators.\nclass\nGraphOps\n[\nVD\n,\nED\n]\n{\ndef\ncollectNeighborIds\n(\nedgeDirection\n:\nEdgeDirection\n)\n:\nVertexRDD\n[\nArray\n[\nVertexId\n]]\ndef\ncollectNeighbors\n(\nedgeDirection\n:\nEdgeDirection\n)\n:\nVertexRDD\n[\nArray\n[(\nVertexId\n,\nVD\n)]\n]\n}\nThese operators can be quite costly as they duplicate information and require\nsubstantial communication.  If possible try expressing the same computation using the\naggregateMessages\noperator directly.\nCaching and Uncaching\nIn Spark, RDDs are not persisted in memory by default. To avoid recomputation, they must be explicitly cached when using them multiple times (see the\nSpark Programming Guide\n). Graphs in GraphX behave the same way.\nWhen using a graph multiple times, make sure to call\nGraph.cache()\non it first.\nIn iterative computations,\nuncaching\nmay also be necessary for best performance. By default, cached RDDs and graphs will remain in memory until memory pressure forces them to be evicted in LRU order. For iterative computation, intermediate results from previous iterations will fill up the cache. Though they will eventually be evicted, the unnecessary data stored in memory will slow down garbage collection. It would be more efficient to uncache intermediate results as soon as they are no longer necessary. This involves materializing (caching and forcing) a graph or RDD every iteration, uncaching all other datasets, and only using the materialized dataset in future iterations. However, because graphs are composed of multiple RDDs, it can be difficult to unpersist them correctly.\nFor iterative computation we recommend using the Pregel API, which correctly unpersists intermediate results.\nPregel API\nGraphs are inherently recursive data structures as properties of vertices depend on properties of\ntheir neighbors which in turn depend on properties of\ntheir\nneighbors.  As a\nconsequence many important graph algorithms iteratively recompute the properties of each vertex\nuntil a fixed-point condition is reached.  A range of graph-parallel abstractions have been proposed\nto express these iterative algorithms.  GraphX exposes a variant of the Pregel API.\nAt a high level the Pregel operator in GraphX is a bulk-synchronous parallel messaging abstraction\nconstrained to the topology of the graph\n.  The Pregel operator executes in a series of super steps\nin which vertices receive the\nsum\nof their inbound messages from the previous super step, compute\na new value for the vertex property, and then send messages to neighboring vertices in the next\nsuper step.  Unlike Pregel, messages are computed in parallel as a\nfunction of the edge triplet and the message computation has access to both the source and\ndestination vertex attributes.  Vertices that do not receive a message are skipped within a super\nstep.  The Pregel operator terminates iteration and returns the final graph when there are no\nmessages remaining.\nNote, unlike more standard Pregel implementations, vertices in GraphX can only send messages to\nneighboring vertices and the message construction is done in parallel using a user defined\nmessaging function.  These constraints allow additional optimization within GraphX.\nThe following is the type signature of the\nPregel operator\nas well as a\nsketch\nof its implementation (note: to avoid stackOverflowError due to long lineage chains, pregel support periodically\ncheckpoint graph and messages by setting “spark.graphx.pregel.checkpointInterval” to a positive number,\nsay 10. And set checkpoint directory as well using SparkContext.setCheckpointDir(directory: String)):\nclass\nGraphOps\n[\nVD\n,\nED\n]\n{\ndef\npregel\n[\nA\n]\n(\ninitialMsg\n:\nA\n,\nmaxIter\n:\nInt\n=\nInt\n.\nMaxValue\n,\nactiveDir\n:\nEdgeDirection\n=\nEdgeDirection\n.\nOut\n)\n(\nvprog\n:\n(\nVertexId\n,\nVD\n,\nA\n)\n=>\nVD\n,\nsendMsg\n:\nEdgeTriplet\n[\nVD\n,\nED\n]\n=>\nIterator\n[(\nVertexId\n,\nA\n)],\nmergeMsg\n:\n(\nA\n,\nA\n)\n=>\nA\n)\n:\nGraph\n[\nVD\n,\nED\n]\n=\n{\n// Receive the initial message at each vertex\nvar\ng\n=\nmapVertices\n(\n(\nvid\n,\nvdata\n)\n=>\nvprog\n(\nvid\n,\nvdata\n,\ninitialMsg\n)\n).\ncache\n()\n// compute the messages\nvar\nmessages\n=\nGraphXUtils\n.\nmapReduceTriplets\n(\ng\n,\nsendMsg\n,\nmergeMsg\n)\nvar\nactiveMessages\n=\nmessages\n.\ncount\n()\n// Loop until no messages remain or maxIterations is achieved\nvar\ni\n=\n0\nwhile\n(\nactiveMessages\n>\n0\n&&\ni\n<\nmaxIterations\n)\n{\n// Receive the messages and update the vertices.\ng\n=\ng\n.\njoinVertices\n(\nmessages\n)(\nvprog\n).\ncache\n()\nval\noldMessages\n=\nmessages\n// Send new messages, skipping edges where neither side received a message. We must cache\n// messages so it can be materialized on the next line, allowing us to uncache the previous\n// iteration.\nmessages\n=\nGraphXUtils\n.\nmapReduceTriplets\n(\ng\n,\nsendMsg\n,\nmergeMsg\n,\nSome\n((\noldMessages\n,\nactiveDirection\n))).\ncache\n()\nactiveMessages\n=\nmessages\n.\ncount\n()\ni\n+=\n1\n}\ng\n}\n}\nNotice that Pregel takes two argument lists (i.e.,\ngraph.pregel(list1)(list2)\n).  The first\nargument list contains configuration parameters including the initial message, the maximum number of\niterations, and the edge direction in which to send messages (by default along out edges).  The\nsecond argument list contains the user defined functions for receiving messages (the vertex program\nvprog\n), computing messages (\nsendMsg\n), and combining messages\nmergeMsg\n.\nWe can use the Pregel operator to express computation such as single source\nshortest path in the following example.\nimport\norg.apache.spark.graphx.\n{\nGraph\n,\nVertexId\n}\nimport\norg.apache.spark.graphx.util.GraphGenerators\n// A graph with edge attributes containing distances\nval\ngraph\n:\nGraph\n[\nLong\n,\nDouble\n]\n=\nGraphGenerators\n.\nlogNormalGraph\n(\nsc\n,\nnumVertices\n=\n100\n).\nmapEdges\n(\ne\n=>\ne\n.\nattr\n.\ntoDouble\n)\nval\nsourceId\n:\nVertexId\n=\n42\n// The ultimate source\n// Initialize the graph such that all vertices except the root have distance infinity.\nval\ninitialGraph\n=\ngraph\n.\nmapVertices\n((\nid\n,\n_\n)\n=>\nif\n(\nid\n==\nsourceId\n)\n0.0\nelse\nDouble\n.\nPositiveInfinity\n)\nval\nsssp\n=\ninitialGraph\n.\npregel\n(\nDouble\n.\nPositiveInfinity\n)(\n(\nid\n,\ndist\n,\nnewDist\n)\n=>\nmath\n.\nmin\n(\ndist\n,\nnewDist\n),\n// Vertex Program\ntriplet\n=>\n{\n// Send Message\nif\n(\ntriplet\n.\nsrcAttr\n+\ntriplet\n.\nattr\n<\ntriplet\n.\ndstAttr\n)\n{\nIterator\n((\ntriplet\n.\ndstId\n,\ntriplet\n.\nsrcAttr\n+\ntriplet\n.\nattr\n))\n}\nelse\n{\nIterator\n.\nempty\n}\n},\n(\na\n,\nb\n)\n=>\nmath\n.\nmin\n(\na\n,\nb\n)\n// Merge Message\n)\nprintln\n(\nsssp\n.\nvertices\n.\ncollect\n().\nmkString\n(\n\"\\n\"\n))\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\" in the Spark repo.\nGraph Builders\nGraphX provides several ways of building a graph from a collection of vertices and edges in an RDD or on disk. None of the graph builders repartitions the graph’s edges by default; instead, edges are left in their default partitions (such as their original blocks in HDFS).\nGraph.groupEdges\nrequires the graph to be repartitioned because it assumes identical edges will be colocated on the same partition, so you must call\nGraph.partitionBy\nbefore calling\ngroupEdges\n.\nobject\nGraphLoader\n{\ndef\nedgeListFile\n(\nsc\n:\nSparkContext\n,\npath\n:\nString\n,\ncanonicalOrientation\n:\nBoolean\n=\nfalse\n,\nminEdgePartitions\n:\nInt\n=\n1\n)\n:\nGraph\n[\nInt\n,\nInt\n]\n}\nGraphLoader.edgeListFile\nprovides a way to load a graph from a list of edges on disk. It parses an adjacency list of (source vertex ID, destination vertex ID) pairs of the following form, skipping comment lines that begin with\n#\n:\n# This is a comment\n2 1\n4 1\n1 2\nIt creates a\nGraph\nfrom the specified edges, automatically creating any vertices mentioned by edges. All vertex and edge attributes default to 1. The\ncanonicalOrientation\nargument allows reorienting edges in the positive direction (\nsrcId < dstId\n), which is required by the\nconnected components\nalgorithm. The\nminEdgePartitions\nargument specifies the minimum number of edge partitions to generate; there may be more edge partitions than specified if, for example, the HDFS file has more blocks.\nobject\nGraph\n{\ndef\napply\n[\nVD\n,\nED\n](\nvertices\n:\nRDD\n[(\nVertexId\n,\nVD\n)],\nedges\n:\nRDD\n[\nEdge\n[\nED\n]],\ndefaultVertexAttr\n:\nVD\n=\nnull\n)\n:\nGraph\n[\nVD\n,\nED\n]\ndef\nfromEdges\n[\nVD\n,\nED\n](\nedges\n:\nRDD\n[\nEdge\n[\nED\n]],\ndefaultValue\n:\nVD\n)\n:\nGraph\n[\nVD\n,\nED\n]\ndef\nfromEdgeTuples\n[\nVD\n](\nrawEdges\n:\nRDD\n[(\nVertexId\n,\nVertexId\n)],\ndefaultValue\n:\nVD\n,\nuniqueEdges\n:\nOption\n[\nPartitionStrategy\n]\n=\nNone\n)\n:\nGraph\n[\nVD\n,\nInt\n]\n}\nGraph.apply\nallows creating a graph from RDDs of vertices and edges. Duplicate vertices are picked arbitrarily and vertices found in the edge RDD but not the vertex RDD are assigned the default attribute.\nGraph.fromEdges\nallows creating a graph from only an RDD of edges, automatically creating any vertices mentioned by edges and assigning them the default value.\nGraph.fromEdgeTuples\nallows creating a graph from only an RDD of edge tuples, assigning the edges the value 1, and automatically creating any vertices mentioned by edges and assigning them the default value. It also supports deduplicating the edges; to deduplicate, pass\nSome\nof a\nPartitionStrategy\nas the\nuniqueEdges\nparameter (for example,\nuniqueEdges = Some(PartitionStrategy.RandomVertexCut)\n). A partition strategy is necessary to colocate identical edges on the same partition so they can be deduplicated.\nVertex and Edge RDDs\nGraphX exposes\nRDD\nviews of the vertices and edges stored within the graph.  However, because\nGraphX maintains the vertices and edges in optimized data structures and these data structures\nprovide additional functionality, the vertices and edges are returned as\nVertexRDD\nVertexRDD\nand\nEdgeRDD\nEdgeRDD\nrespectively.  In this section we review some of the additional useful functionality in these types.\nNote that this is just an incomplete list, please refer to the API docs for the official list of operations.\nVertexRDDs\nThe\nVertexRDD[A]\nextends\nRDD[(VertexId, A)]\nand adds the additional constraint that each\nVertexId\noccurs only\nonce\n.  Moreover,\nVertexRDD[A]\nrepresents a\nset\nof vertices each with an\nattribute of type\nA\n.  Internally, this is achieved by storing the vertex attributes in a reusable\nhash-map data-structure.  As a consequence if two\nVertexRDD\ns are derived from the same base\nVertexRDD\nVertexRDD\n(e.g., by\nfilter\nor\nmapValues\n) they can be joined in constant time without hash\nevaluations. To leverage this indexed data structure, the\nVertexRDD\nVertexRDD\nexposes the following\nadditional functionality:\nclass\nVertexRDD\n[\nVD\n]\nextends\nRDD\n[(\nVertexId\n,\nVD\n)]\n{\n// Filter the vertex set but preserves the internal index\ndef\nfilter\n(\npred\n:\nTuple2\n[\nVertexId\n,\nVD\n]\n=>\nBoolean\n)\n:\nVertexRDD\n[\nVD\n]\n// Transform the values without changing the ids (preserves the internal index)\ndef\nmapValues\n[\nVD2\n](\nmap\n:\nVD\n=>\nVD2\n)\n:\nVertexRDD\n[\nVD2\n]\ndef\nmapValues\n[\nVD2\n](\nmap\n:\n(\nVertexId\n,\nVD\n)\n=>\nVD2\n)\n:\nVertexRDD\n[\nVD2\n]\n// Show only vertices unique to this set based on their VertexId's\ndef\nminus\n(\nother\n:\nRDD\n[(\nVertexId\n,\nVD\n)])\n// Remove vertices from this set that appear in the other set\ndef\ndiff\n(\nother\n:\nVertexRDD\n[\nVD\n])\n:\nVertexRDD\n[\nVD\n]\n// Join operators that take advantage of the internal indexing to accelerate joins (substantially)\ndef\nleftJoin\n[\nVD2\n,\nVD3\n](\nother\n:\nRDD\n[(\nVertexId\n,\nVD2\n)])(\nf\n:\n(\nVertexId\n,\nVD\n,\nOption\n[\nVD2\n])\n=>\nVD3\n)\n:\nVertexRDD\n[\nVD3\n]\ndef\ninnerJoin\n[\nU\n,\nVD2\n](\nother\n:\nRDD\n[(\nVertexId\n,\nU\n)])(\nf\n:\n(\nVertexId\n,\nVD\n,\nU\n)\n=>\nVD2\n)\n:\nVertexRDD\n[\nVD2\n]\n// Use the index on this RDD to accelerate a `reduceByKey` operation on the input RDD.\ndef\naggregateUsingIndex\n[\nVD2\n](\nother\n:\nRDD\n[(\nVertexId\n,\nVD2\n)],\nreduceFunc\n:\n(\nVD2\n,\nVD2\n)\n=>\nVD2\n)\n:\nVertexRDD\n[\nVD2\n]\n}\nNotice, for example,  how the\nfilter\noperator returns a\nVertexRDD\nVertexRDD\n.  Filter is actually\nimplemented using a\nBitSet\nthereby reusing the index and preserving the ability to do fast joins\nwith other\nVertexRDD\ns.  Likewise, the\nmapValues\noperators do not allow the\nmap\nfunction to\nchange the\nVertexId\nthereby enabling the same\nHashMap\ndata structures to be reused.  Both the\nleftJoin\nand\ninnerJoin\nare able to identify when joining two\nVertexRDD\ns derived from the same\nHashMap\nand implement the join by linear scan rather than costly point lookups.\nThe\naggregateUsingIndex\noperator is useful for efficient construction of a new\nVertexRDD\nVertexRDD\nfrom an\nRDD[(VertexId, A)]\n.  Conceptually, if I have constructed a\nVertexRDD[B]\nover a set of vertices,\nwhich is a super-set\nof the vertices in some\nRDD[(VertexId, A)]\nthen I can reuse the index to\nboth aggregate and then subsequently index the\nRDD[(VertexId, A)]\n.  For example:\nval\nsetA\n:\nVertexRDD\n[\nInt\n]\n=\nVertexRDD\n(\nsc\n.\nparallelize\n(\n0L\nuntil\n100L\n).\nmap\n(\nid\n=>\n(\nid\n,\n1\n)))\nval\nrddB\n:\nRDD\n[(\nVertexId\n,\nDouble\n)]\n=\nsc\n.\nparallelize\n(\n0L\nuntil\n100L\n).\nflatMap\n(\nid\n=>\nList\n((\nid\n,\n1.0\n),\n(\nid\n,\n2.0\n)))\n// There should be 200 entries in rddB\nrddB\n.\ncount\nval\nsetB\n:\nVertexRDD\n[\nDouble\n]\n=\nsetA\n.\naggregateUsingIndex\n(\nrddB\n,\n_\n+\n_\n)\n// There should be 100 entries in setB\nsetB\n.\ncount\n// Joining A and B should now be fast!\nval\nsetC\n:\nVertexRDD\n[\nDouble\n]\n=\nsetA\n.\ninnerJoin\n(\nsetB\n)((\nid\n,\na\n,\nb\n)\n=>\na\n+\nb\n)\nEdgeRDDs\nThe\nEdgeRDD[ED]\n, which extends\nRDD[Edge[ED]]\norganizes the edges in blocks partitioned using one\nof the various partitioning strategies defined in\nPartitionStrategy\n.  Within\neach partition, edge attributes and adjacency structure, are stored separately enabling maximum\nreuse when changing attribute values.\nThe three additional functions exposed by the\nEdgeRDD\nEdgeRDD\nare:\n// Transform the edge attributes while preserving the structure\ndef\nmapValues\n[\nED2\n](\nf\n:\nEdge\n[\nED\n]\n=>\nED2\n)\n:\nEdgeRDD\n[\nED2\n]\n// Reverse the edges reusing both attributes and structure\ndef\nreverse\n:\nEdgeRDD\n[\nED\n]\n// Join two `EdgeRDD`s partitioned using the same partitioning strategy.\ndef\ninnerJoin\n[\nED2\n,\nED3\n](\nother\n:\nEdgeRDD\n[\nED2\n])(\nf\n:\n(\nVertexId\n,\nVertexId\n,\nED\n,\nED2\n)\n=>\nED3\n)\n:\nEdgeRDD\n[\nED3\n]\nIn most applications we have found that operations on the\nEdgeRDD\nEdgeRDD\nare accomplished through the\ngraph operators or rely on operations defined in the base\nRDD\nclass.\nOptimized Representation\nWhile a detailed description of the optimizations used in the GraphX representation of distributed\ngraphs is beyond the scope of this guide, some high-level understanding may aid in the design of\nscalable algorithms as well as optimal use of the API.  GraphX adopts a vertex-cut approach to\ndistributed graph partitioning:\nRather than splitting graphs along edges, GraphX partitions the graph along vertices which can\nreduce both the communication and storage overhead.  Logically, this corresponds to assigning edges\nto machines and allowing vertices to span multiple machines.  The exact method of assigning edges\ndepends on the\nPartitionStrategy\nand there are several tradeoffs to the\nvarious heuristics.  Users can choose between different strategies by repartitioning the graph with\nthe\nGraph.partitionBy\noperator.  The default partitioning strategy is to use\nthe initial partitioning of the edges as provided on graph construction.  However, users can easily\nswitch to 2D-partitioning or other heuristics included in GraphX.\nOnce the edges have been partitioned the key challenge to efficient graph-parallel computation is\nefficiently joining vertex attributes with the edges.  Because real-world graphs typically have more\nedges than vertices, we move vertex attributes to the edges.  Because not all partitions will\ncontain edges adjacent to all vertices we internally maintain a routing table which identifies where\nto broadcast vertices when implementing the join required for operations like\ntriplets\nand\naggregateMessages\n.\nGraph Algorithms\nGraphX includes a set of graph algorithms to simplify analytics tasks. The algorithms are contained in the\norg.apache.spark.graphx.lib\npackage and can be accessed directly as methods on\nGraph\nvia\nGraphOps\n. This section describes the algorithms and how they are used.\nPageRank\nPageRank measures the importance of each vertex in a graph, assuming an edge from\nu\nto\nv\nrepresents an endorsement of\nv\n’s importance by\nu\n. For example, if a Twitter user is followed by many others, the user will be ranked highly.\nGraphX comes with static and dynamic implementations of PageRank as methods on the\nPageRank\nobject\n. Static PageRank runs for a fixed number of iterations, while dynamic PageRank runs until the ranks converge (i.e., stop changing by more than a specified tolerance).\nGraphOps\nallows calling these algorithms directly as methods on\nGraph\n.\nGraphX also includes an example social network dataset that we can run PageRank on. A set of users is given in\ndata/graphx/users.txt\n, and a set of relationships between users is given in\ndata/graphx/followers.txt\n. We compute the PageRank of each user as follows:\nimport\norg.apache.spark.graphx.GraphLoader\n// Load the edges as a graph\nval\ngraph\n=\nGraphLoader\n.\nedgeListFile\n(\nsc\n,\n\"data/graphx/followers.txt\"\n)\n// Run PageRank\nval\nranks\n=\ngraph\n.\npageRank\n(\n0.0001\n).\nvertices\n// Join the ranks with the usernames\nval\nusers\n=\nsc\n.\ntextFile\n(\n\"data/graphx/users.txt\"\n).\nmap\n{\nline\n=>\nval\nfields\n=\nline\n.\nsplit\n(\n\",\"\n)\n(\nfields\n(\n0\n).\ntoLong\n,\nfields\n(\n1\n))\n}\nval\nranksByUsername\n=\nusers\n.\njoin\n(\nranks\n).\nmap\n{\ncase\n(\nid\n,\n(\nusername\n,\nrank\n))\n=>\n(\nusername\n,\nrank\n)\n}\n// Print the result\nprintln\n(\nranksByUsername\n.\ncollect\n().\nmkString\n(\n\"\\n\"\n))\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\" in the Spark repo.\nConnected Components\nThe connected components algorithm labels each connected component of the graph with the ID of its lowest-numbered vertex. For example, in a social network, connected components can approximate clusters. GraphX contains an implementation of the algorithm in the\nConnectedComponents\nobject\n, and we compute the connected components of the example social network dataset from the\nPageRank section\nas follows:\nimport\norg.apache.spark.graphx.GraphLoader\n// Load the graph as in the PageRank example\nval\ngraph\n=\nGraphLoader\n.\nedgeListFile\n(\nsc\n,\n\"data/graphx/followers.txt\"\n)\n// Find the connected components\nval\ncc\n=\ngraph\n.\nconnectedComponents\n().\nvertices\n// Join the connected components with the usernames\nval\nusers\n=\nsc\n.\ntextFile\n(\n\"data/graphx/users.txt\"\n).\nmap\n{\nline\n=>\nval\nfields\n=\nline\n.\nsplit\n(\n\",\"\n)\n(\nfields\n(\n0\n).\ntoLong\n,\nfields\n(\n1\n))\n}\nval\nccByUsername\n=\nusers\n.\njoin\n(\ncc\n).\nmap\n{\ncase\n(\nid\n,\n(\nusername\n,\ncc\n))\n=>\n(\nusername\n,\ncc\n)\n}\n// Print the result\nprintln\n(\nccByUsername\n.\ncollect\n().\nmkString\n(\n\"\\n\"\n))\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\" in the Spark repo.\nTriangle Counting\nA vertex is part of a triangle when it has two adjacent vertices with an edge between them. GraphX implements a triangle counting algorithm in the\nTriangleCount\nobject\nthat determines the number of triangles passing through each vertex, providing a measure of clustering. We compute the triangle count of the social network dataset from the\nPageRank section\n.\nNote that\nTriangleCount\nrequires the edges to be in canonical orientation (\nsrcId < dstId\n) and the graph to be partitioned using\nGraph.partitionBy\n.\nimport\norg.apache.spark.graphx.\n{\nGraphLoader\n,\nPartitionStrategy\n}\n// Load the edges in canonical order and partition the graph for triangle count\nval\ngraph\n=\nGraphLoader\n.\nedgeListFile\n(\nsc\n,\n\"data/graphx/followers.txt\"\n,\ntrue\n)\n.\npartitionBy\n(\nPartitionStrategy\n.\nRandomVertexCut\n)\n// Find the triangle count for each vertex\nval\ntriCounts\n=\ngraph\n.\ntriangleCount\n().\nvertices\n// Join the triangle counts with the usernames\nval\nusers\n=\nsc\n.\ntextFile\n(\n\"data/graphx/users.txt\"\n).\nmap\n{\nline\n=>\nval\nfields\n=\nline\n.\nsplit\n(\n\",\"\n)\n(\nfields\n(\n0\n).\ntoLong\n,\nfields\n(\n1\n))\n}\nval\ntriCountByUsername\n=\nusers\n.\njoin\n(\ntriCounts\n).\nmap\n{\ncase\n(\nid\n,\n(\nusername\n,\ntc\n))\n=>\n(\nusername\n,\ntc\n)\n}\n// Print the result\nprintln\n(\ntriCountByUsername\n.\ncollect\n().\nmkString\n(\n\"\\n\"\n))\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\" in the Spark repo.\nExamples\nSuppose I want to build a graph from some text files, restrict the graph\nto important relationships and users, run page-rank on the subgraph, and\nthen finally return attributes associated with the top users.  I can do\nall of this in just a few lines with GraphX:\nimport\norg.apache.spark.graphx.GraphLoader\n// Load my user data and parse into tuples of user id and attribute list\nval\nusers\n=\n(\nsc\n.\ntextFile\n(\n\"data/graphx/users.txt\"\n)\n.\nmap\n(\nline\n=>\nline\n.\nsplit\n(\n\",\"\n)).\nmap\n(\nparts\n=>\n(\nparts\n.\nhead\n.\ntoLong\n,\nparts\n.\ntail\n)\n))\n// Parse the edge data which is already in userId -> userId format\nval\nfollowerGraph\n=\nGraphLoader\n.\nedgeListFile\n(\nsc\n,\n\"data/graphx/followers.txt\"\n)\n// Attach the user attributes\nval\ngraph\n=\nfollowerGraph\n.\nouterJoinVertices\n(\nusers\n)\n{\ncase\n(\nuid\n,\ndeg\n,\nSome\n(\nattrList\n))\n=>\nattrList\n// Some users may not have attributes so we set them as empty\ncase\n(\nuid\n,\ndeg\n,\nNone\n)\n=>\nArray\n.\nempty\n[\nString\n]\n}\n// Restrict the graph to users with usernames and names\nval\nsubgraph\n=\ngraph\n.\nsubgraph\n(\nvpred\n=\n(\nvid\n,\nattr\n)\n=>\nattr\n.\nsize\n==\n2\n)\n// Compute the PageRank\nval\npagerankGraph\n=\nsubgraph\n.\npageRank\n(\n0.001\n)\n// Get the attributes of the top pagerank users\nval\nuserInfoWithPageRank\n=\nsubgraph\n.\nouterJoinVertices\n(\npagerankGraph\n.\nvertices\n)\n{\ncase\n(\nuid\n,\nattrList\n,\nSome\n(\npr\n))\n=>\n(\npr\n,\nattrList\n.\ntoList\n)\ncase\n(\nuid\n,\nattrList\n,\nNone\n)\n=>\n(\n0.0\n,\nattrList\n.\ntoList\n)\n}\nprintln\n(\nuserInfoWithPageRank\n.\nvertices\n.\ntop\n(\n5\n)(\nOrdering\n.\nby\n(\n_\n.\n_2\n.\n_1\n)).\nmkString\n(\n\"\\n\"\n))\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\" in the Spark repo."}
{"url": "https://spark.apache.org/docs/latest/sparkr.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSparkR (R on Spark)\nOverview\nSparkDataFrame\nStarting Up: SparkSession\nStarting Up from RStudio\nCreating SparkDataFrames\nFrom local data frames\nFrom Data Sources\nFrom Hive tables\nSparkDataFrame Operations\nSelecting rows, columns\nGrouping, Aggregation\nOperating on Columns\nApplying User-Defined Function\nRun a given function on a large dataset using\ndapply\nor\ndapplyCollect\ndapply\ndapplyCollect\nRun a given function on a large dataset grouping by input column(s) and using\ngapply\nor\ngapplyCollect\ngapply\ngapplyCollect\nRun local R functions distributed using\nspark.lapply\nspark.lapply\nEager execution\nRunning SQL Queries from SparkR\nMachine Learning\nAlgorithms\nClassification\nRegression\nTree\nClustering\nCollaborative Filtering\nFrequent Pattern Mining\nStatistics\nModel persistence\nData type mapping between R and Spark\nStructured Streaming\nApache Arrow in SparkR\nEnsure Arrow Installed\nEnabling for Conversion to/from R DataFrame,\ndapply\nand\ngapply\nSupported SQL Types\nR Function Name Conflicts\nMigration Guide\nSparkR is deprecated from Apache Spark 4.0.0 and will be removed in a future version.\nOverview\nSparkR is an R package that provides a light-weight frontend to use Apache Spark from R.\nIn Spark 4.0.0, SparkR provides a distributed data frame implementation that\nsupports operations like selection, filtering, aggregation etc. (similar to R data frames,\ndplyr\n) but on large datasets. SparkR also supports distributed\nmachine learning using MLlib.\nSparkDataFrame\nA SparkDataFrame is a distributed collection of data organized into named columns. It is conceptually\nequivalent to a table in a relational database or a data frame in R, but with richer\noptimizations under the hood. SparkDataFrames can be constructed from a wide array of sources such as:\nstructured data files, tables in Hive, external databases, or existing local R data frames.\nAll of the examples on this page use sample data included in R or the Spark distribution and can be run using the\n./bin/sparkR\nshell.\nStarting Up: SparkSession\nThe entry point into SparkR is the\nSparkSession\nwhich connects your R program to a Spark cluster.\nYou can create a\nSparkSession\nusing\nsparkR.session\nand pass in options such as the application name, any spark packages depended on, etc. Further, you can also work with SparkDataFrames via\nSparkSession\n. If you are working from the\nsparkR\nshell, the\nSparkSession\nshould already be created for you, and you would not need to call\nsparkR.session\n.\nsparkR.session\n()\nStarting Up from RStudio\nYou can also start SparkR from RStudio. You can connect your R program to a Spark cluster from\nRStudio, R shell, Rscript or other R IDEs. To start, make sure SPARK_HOME is set in environment\n(you can check\nSys.getenv\n),\nload the SparkR package, and call\nsparkR.session\nas below. It will check for the Spark installation, and, if not found, it will be downloaded and cached automatically. Alternatively, you can also run\ninstall.spark\nmanually.\nIn addition to calling\nsparkR.session\n,\n you could also specify certain Spark driver properties. Normally these\nApplication properties\nand\nRuntime Environment\ncannot be set programmatically, as the\ndriver JVM process would have been started, in this case SparkR takes care of this for you. To set\nthem, pass them as you would other configuration properties in the\nsparkConfig\nargument to\nsparkR.session()\n.\nif\n(\nnchar\n(\nSys.getenv\n(\n\"SPARK_HOME\"\n))\n<\n1\n)\n{\nSys.setenv\n(\nSPARK_HOME\n=\n\"/home/spark\"\n)\n}\nlibrary\n(\nSparkR\n,\nlib.loc\n=\nc\n(\nfile.path\n(\nSys.getenv\n(\n\"SPARK_HOME\"\n),\n\"R\"\n,\n\"lib\"\n)))\nsparkR.session\n(\nmaster\n=\n\"local[*]\"\n,\nsparkConfig\n=\nlist\n(\nspark.driver.memory\n=\n\"2g\"\n))\nThe following Spark driver properties can be set in\nsparkConfig\nwith\nsparkR.session\nfrom RStudio:\nProperty Name\nProperty group\nspark-submit\nequivalent\nspark.master\nApplication Properties\n--master\nspark.kerberos.keytab\nApplication Properties\n--keytab\nspark.kerberos.principal\nApplication Properties\n--principal\nspark.driver.memory\nApplication Properties\n--driver-memory\nspark.driver.extraClassPath\nRuntime Environment\n--driver-class-path\nspark.driver.extraJavaOptions\nRuntime Environment\n--driver-java-options\nspark.driver.extraLibraryPath\nRuntime Environment\n--driver-library-path\nCreating SparkDataFrames\nWith a\nSparkSession\n, applications can create\nSparkDataFrame\ns from a local R data frame, from a\nHive table\n, or from other\ndata sources\n.\nFrom local data frames\nThe simplest way to create a data frame is to convert a local R data frame into a SparkDataFrame. Specifically, we can use\nas.DataFrame\nor\ncreateDataFrame\nand pass in the local R data frame to create a SparkDataFrame. As an example, the following creates a\nSparkDataFrame\nbased using the\nfaithful\ndataset from R.\ndf\n<-\nas.DataFrame\n(\nfaithful\n)\n# Displays the first part of the SparkDataFrame\nhead\n(\ndf\n)\n##  eruptions waiting\n##1     3.600      79\n##2     1.800      54\n##3     3.333      74\nFrom Data Sources\nSparkR supports operating on a variety of data sources through the\nSparkDataFrame\ninterface. This section describes the general methods for loading and saving data using Data Sources. You can check the Spark SQL programming guide for more\nspecific options\nthat are available for the built-in data sources.\nThe general method for creating SparkDataFrames from data sources is\nread.df\n. This method takes in the path for the file to load and the type of data source, and the currently active SparkSession will be used automatically.\nSparkR supports reading JSON, CSV and Parquet files natively, and through packages available from sources like\nThird Party Projects\n, you can find data source connectors for popular file formats like Avro. These packages can either be added by\nspecifying\n--packages\nwith\nspark-submit\nor\nsparkR\ncommands, or if initializing SparkSession with\nsparkPackages\nparameter when in an interactive R shell or from RStudio.\nsparkR.session\n(\nsparkPackages\n=\n\"org.apache.spark:spark-avro_2.13:4.0.0\"\n)\nWe can see how to use data sources using an example JSON input file. Note that the file that is used here is\nnot\na typical JSON file. Each line in the file must contain a separate, self-contained valid JSON object. For more information, please see\nJSON Lines text format, also called newline-delimited JSON\n. As a consequence, a regular multi-line JSON file will most often fail.\npeople\n<-\nread.df\n(\n\"./examples/src/main/resources/people.json\"\n,\n\"json\"\n)\nhead\n(\npeople\n)\n##  age    name\n##1  NA Michael\n##2  30    Andy\n##3  19  Justin\n# SparkR automatically infers the schema from the JSON file\nprintSchema\n(\npeople\n)\n# root\n#  |-- age: long (nullable = true)\n#  |-- name: string (nullable = true)\n# Similarly, multiple files can be read with read.json\npeople\n<-\nread.json\n(\nc\n(\n\"./examples/src/main/resources/people.json\"\n,\n\"./examples/src/main/resources/people2.json\"\n))\nThe data sources API natively supports CSV formatted input files. For more information please refer to SparkR\nread.df\nAPI documentation.\ndf\n<-\nread.df\n(\ncsvPath\n,\n\"csv\"\n,\nheader\n=\n\"true\"\n,\ninferSchema\n=\n\"true\"\n,\nna.strings\n=\n\"NA\"\n)\nThe data sources API can also be used to save out SparkDataFrames into multiple file formats. For example, we can save the SparkDataFrame from the previous example\nto a Parquet file using\nwrite.df\n.\nwrite.df\n(\npeople\n,\npath\n=\n\"people.parquet\"\n,\nsource\n=\n\"parquet\"\n,\nmode\n=\n\"overwrite\"\n)\nFrom Hive tables\nYou can also create SparkDataFrames from Hive tables. To do this we will need to create a SparkSession with Hive support which can access tables in the Hive MetaStore. Note that Spark should have been built with\nHive support\nand more details can be found in the\nSQL programming guide\n. In SparkR, by default it will attempt to create a SparkSession with Hive support enabled (\nenableHiveSupport = TRUE\n).\nsparkR.session\n()\nsql\n(\n\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\"\n)\nsql\n(\n\"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src\"\n)\n# Queries can be expressed in HiveQL.\nresults\n<-\nsql\n(\n\"FROM src SELECT key, value\"\n)\n# results is now a SparkDataFrame\nhead\n(\nresults\n)\n##  key   value\n## 1 238 val_238\n## 2  86  val_86\n## 3 311 val_311\nSparkDataFrame Operations\nSparkDataFrames support a number of functions to do structured data processing.\nHere we include some basic examples and a complete list can be found in the\nAPI\ndocs:\nSelecting rows, columns\n# Create the SparkDataFrame\ndf\n<-\nas.DataFrame\n(\nfaithful\n)\n# Get basic information about the SparkDataFrame\ndf\n## SparkDataFrame[eruptions:double, waiting:double]\n# Select only the \"eruptions\" column\nhead\n(\nselect\n(\ndf\n,\ndf\n$\neruptions\n))\n##  eruptions\n##1     3.600\n##2     1.800\n##3     3.333\n# You can also pass in column name as strings\nhead\n(\nselect\n(\ndf\n,\n\"eruptions\"\n))\n# Filter the SparkDataFrame to only retain rows with wait times shorter than 50 mins\nhead\n(\nfilter\n(\ndf\n,\ndf\n$\nwaiting\n<\n50\n))\n##  eruptions waiting\n##1     1.750      47\n##2     1.750      47\n##3     1.867      48\nGrouping, Aggregation\nSparkR data frames support a number of commonly used functions to aggregate data after grouping. For example, we can compute a histogram of the\nwaiting\ntime in the\nfaithful\ndataset as shown below\n# We use the `n` operator to count the number of times each waiting time appears\nhead\n(\nsummarize\n(\ngroupBy\n(\ndf\n,\ndf\n$\nwaiting\n),\ncount\n=\nn\n(\ndf\n$\nwaiting\n)))\n##  waiting count\n##1      70     4\n##2      67     1\n##3      69     2\n# We can also sort the output from the aggregation to get the most common waiting times\nwaiting_counts\n<-\nsummarize\n(\ngroupBy\n(\ndf\n,\ndf\n$\nwaiting\n),\ncount\n=\nn\n(\ndf\n$\nwaiting\n))\nhead\n(\narrange\n(\nwaiting_counts\n,\ndesc\n(\nwaiting_counts\n$\ncount\n)))\n##   waiting count\n##1      78    15\n##2      83    14\n##3      81    13\nIn addition to standard aggregations, SparkR supports\nOLAP cube\noperators\ncube\n:\nhead\n(\nagg\n(\ncube\n(\ndf\n,\n\"cyl\"\n,\n\"disp\"\n,\n\"gear\"\n),\navg\n(\ndf\n$\nmpg\n)))\n##  cyl  disp gear avg(mpg)\n##1  NA 140.8    4     22.8\n##2   4  75.7    4     30.4\n##3   8 400.0    3     19.2\n##4   8 318.0    3     15.5\n##5  NA 351.0   NA     15.8\n##6  NA 275.8   NA     16.3\nand\nrollup\n:\nhead\n(\nagg\n(\nrollup\n(\ndf\n,\n\"cyl\"\n,\n\"disp\"\n,\n\"gear\"\n),\navg\n(\ndf\n$\nmpg\n)))\n##  cyl  disp gear avg(mpg)\n##1   4  75.7    4     30.4\n##2   8 400.0    3     19.2\n##3   8 318.0    3     15.5\n##4   4  78.7   NA     32.4\n##5   8 304.0    3     15.2\n##6   4  79.0   NA     27.3\nOperating on Columns\nSparkR also provides a number of functions that can be directly applied to columns for data processing and during aggregation. The example below shows the use of basic arithmetic functions.\n# Convert waiting time from hours to seconds.\n# Note that we can assign this to a new column in the same SparkDataFrame\ndf\n$\nwaiting_secs\n<-\ndf\n$\nwaiting\n*\n60\nhead\n(\ndf\n)\n##  eruptions waiting waiting_secs\n##1     3.600      79         4740\n##2     1.800      54         3240\n##3     3.333      74         4440\nApplying User-Defined Function\nIn SparkR, we support several kinds of User-Defined Functions:\nRun a given function on a large dataset using\ndapply\nor\ndapplyCollect\ndapply\nApply a function to each partition of a\nSparkDataFrame\n. The function to be applied to each partition of the\nSparkDataFrame\nand should have only one parameter, to which a\ndata.frame\ncorresponds to each partition will be passed. The output of function should be a\ndata.frame\n. Schema specifies the row format of the resulting a\nSparkDataFrame\n. It must match to\ndata types\nof returned value.\n# Convert waiting time from hours to seconds.\n# Note that we can apply UDF to DataFrame.\nschema\n<-\nstructType\n(\nstructField\n(\n\"eruptions\"\n,\n\"double\"\n),\nstructField\n(\n\"waiting\"\n,\n\"double\"\n),\nstructField\n(\n\"waiting_secs\"\n,\n\"double\"\n))\ndf1\n<-\ndapply\n(\ndf\n,\nfunction\n(\nx\n)\n{\nx\n<-\ncbind\n(\nx\n,\nx\n$\nwaiting\n*\n60\n)\n},\nschema\n)\nhead\n(\ncollect\n(\ndf1\n))\n##  eruptions waiting waiting_secs\n##1     3.600      79         4740\n##2     1.800      54         3240\n##3     3.333      74         4440\n##4     2.283      62         3720\n##5     4.533      85         5100\n##6     2.883      55         3300\ndapplyCollect\nLike\ndapply\n, apply a function to each partition of a\nSparkDataFrame\nand collect the result back. The output of function\nshould be a\ndata.frame\n. But, Schema is not required to be passed. Note that\ndapplyCollect\ncan fail if the output of UDF run on all the partition cannot be pulled to the driver and fit in driver memory.\n# Convert waiting time from hours to seconds.\n# Note that we can apply UDF to DataFrame and return a R's data.frame\nldf\n<-\ndapplyCollect\n(\ndf\n,\nfunction\n(\nx\n)\n{\nx\n<-\ncbind\n(\nx\n,\n\"waiting_secs\"\n=\nx\n$\nwaiting\n*\n60\n)\n})\nhead\n(\nldf\n,\n3\n)\n##  eruptions waiting waiting_secs\n##1     3.600      79         4740\n##2     1.800      54         3240\n##3     3.333      74         4440\nRun a given function on a large dataset grouping by input column(s) and using\ngapply\nor\ngapplyCollect\ngapply\nApply a function to each group of a\nSparkDataFrame\n. The function is to be applied to each group of the\nSparkDataFrame\nand should have only two parameters: grouping key and R\ndata.frame\ncorresponding to\nthat key. The groups are chosen from\nSparkDataFrame\ns column(s).\nThe output of function should be a\ndata.frame\n. Schema specifies the row format of the resulting\nSparkDataFrame\n. It must represent R function’s output schema on the basis of Spark\ndata types\n. The column names of the returned\ndata.frame\nare set by user.\n# Determine six waiting times with the largest eruption time in minutes.\nschema\n<-\nstructType\n(\nstructField\n(\n\"waiting\"\n,\n\"double\"\n),\nstructField\n(\n\"max_eruption\"\n,\n\"double\"\n))\nresult\n<-\ngapply\n(\ndf\n,\n\"waiting\"\n,\nfunction\n(\nkey\n,\nx\n)\n{\ny\n<-\ndata.frame\n(\nkey\n,\nmax\n(\nx\n$\neruptions\n))\n},\nschema\n)\nhead\n(\ncollect\n(\narrange\n(\nresult\n,\n\"max_eruption\"\n,\ndecreasing\n=\nTRUE\n)))\n##    waiting   max_eruption\n##1      64       5.100\n##2      69       5.067\n##3      71       5.033\n##4      87       5.000\n##5      63       4.933\n##6      89       4.900\ngapplyCollect\nLike\ngapply\n, applies a function to each partition of a\nSparkDataFrame\nand collect the result back to R data.frame. The output of the function should be a\ndata.frame\n. But, the schema is not required to be passed. Note that\ngapplyCollect\ncan fail if the output of UDF run on all the partition cannot be pulled to the driver and fit in driver memory.\n# Determine six waiting times with the largest eruption time in minutes.\nresult\n<-\ngapplyCollect\n(\ndf\n,\n\"waiting\"\n,\nfunction\n(\nkey\n,\nx\n)\n{\ny\n<-\ndata.frame\n(\nkey\n,\nmax\n(\nx\n$\neruptions\n))\ncolnames\n(\ny\n)\n<-\nc\n(\n\"waiting\"\n,\n\"max_eruption\"\n)\ny\n})\nhead\n(\nresult\n[\norder\n(\nresult\n$\nmax_eruption\n,\ndecreasing\n=\nTRUE\n),\n])\n##    waiting   max_eruption\n##1      64       5.100\n##2      69       5.067\n##3      71       5.033\n##4      87       5.000\n##5      63       4.933\n##6      89       4.900\nRun local R functions distributed using\nspark.lapply\nspark.lapply\nSimilar to\nlapply\nin native R,\nspark.lapply\nruns a function over a list of elements and distributes the computations with Spark.\nApplies a function in a manner that is similar to\ndoParallel\nor\nlapply\nto elements of a list. The results of all the computations\nshould fit in a single machine. If that is not the case they can do something like\ndf <- createDataFrame(list)\nand then use\ndapply\n# Perform distributed training of multiple models with spark.lapply. Here, we pass\n# a read-only list of arguments which specifies family the generalized linear model should be.\nfamilies\n<-\nc\n(\n\"gaussian\"\n,\n\"poisson\"\n)\ntrain\n<-\nfunction\n(\nfamily\n)\n{\nmodel\n<-\nglm\n(\nSepal.Length\n~\nSepal.Width\n+\nSpecies\n,\niris\n,\nfamily\n=\nfamily\n)\nsummary\n(\nmodel\n)\n}\n# Return a list of model's summaries\nmodel.summaries\n<-\nspark.lapply\n(\nfamilies\n,\ntrain\n)\n# Print the summary of each model\nprint\n(\nmodel.summaries\n)\nEager execution\nIf eager execution is enabled, the data will be returned to R client immediately when the\nSparkDataFrame\nis created. By default, eager execution is not enabled and can be enabled by setting the configuration property\nspark.sql.repl.eagerEval.enabled\nto\ntrue\nwhen the\nSparkSession\nis started up.\nMaximum number of rows and maximum number of characters per column of data to display can be controlled by\nspark.sql.repl.eagerEval.maxNumRows\nand\nspark.sql.repl.eagerEval.truncate\nconfiguration properties, respectively. These properties are only effective when eager execution is enabled. If these properties are not set explicitly, by default, data up to 20 rows and up to 20 characters per column will be showed.\n# Start up spark session with eager execution enabled\nsparkR.session\n(\nmaster\n=\n\"local[*]\"\n,\nsparkConfig\n=\nlist\n(\nspark.sql.repl.eagerEval.enabled\n=\n\"true\"\n,\nspark.sql.repl.eagerEval.maxNumRows\n=\nas.integer\n(\n10\n)))\n# Create a grouped and sorted SparkDataFrame\ndf\n<-\ncreateDataFrame\n(\nfaithful\n)\ndf2\n<-\narrange\n(\nsummarize\n(\ngroupBy\n(\ndf\n,\ndf\n$\nwaiting\n),\ncount\n=\nn\n(\ndf\n$\nwaiting\n)),\n\"waiting\"\n)\n# Similar to R data.frame, displays the data returned, instead of SparkDataFrame class string\ndf2\n##+-------+-----+\n##|waiting|count|\n##+-------+-----+\n##|   43.0|    1|\n##|   45.0|    3|\n##|   46.0|    5|\n##|   47.0|    4|\n##|   48.0|    3|\n##|   49.0|    5|\n##|   50.0|    5|\n##|   51.0|    6|\n##|   52.0|    5|\n##|   53.0|    7|\n##+-------+-----+\n##only showing top 10 rows\nNote that to enable eager execution in\nsparkR\nshell, add\nspark.sql.repl.eagerEval.enabled=true\nconfiguration property to the\n--conf\noption.\nRunning SQL Queries from SparkR\nA SparkDataFrame can also be registered as a temporary view in Spark SQL and that allows you to run SQL queries over its data.\nThe\nsql\nfunction enables applications to run SQL queries programmatically and returns the result as a\nSparkDataFrame\n.\n# Load a JSON file\npeople\n<-\nread.df\n(\n\"./examples/src/main/resources/people.json\"\n,\n\"json\"\n)\n# Register this SparkDataFrame as a temporary view.\ncreateOrReplaceTempView\n(\npeople\n,\n\"people\"\n)\n# SQL statements can be run by using the sql method\nteenagers\n<-\nsql\n(\n\"SELECT name FROM people WHERE age >= 13 AND age <= 19\"\n)\nhead\n(\nteenagers\n)\n##    name\n##1 Justin\nMachine Learning\nAlgorithms\nSparkR supports the following machine learning algorithms currently:\nClassification\nspark.logit\n:\nLogistic Regression\nspark.mlp\n:\nMultilayer Perceptron (MLP)\nspark.naiveBayes\n:\nNaive Bayes\nspark.svmLinear\n:\nLinear Support Vector Machine\nspark.fmClassifier\n:\nFactorization Machines classifier\nRegression\nspark.survreg\n:\nAccelerated Failure Time (AFT) Survival  Model\nspark.glm\nor\nglm\n:\nGeneralized Linear Model (GLM)\nspark.isoreg\n:\nIsotonic Regression\nspark.lm\n:\nLinear Regression\nspark.fmRegressor\n:\nFactorization Machines regressor\nTree\nspark.decisionTree\n:\nDecision Tree for\nRegression\nand\nClassification\nspark.gbt\n:\nGradient Boosted Trees for\nRegression\nand\nClassification\nspark.randomForest\n:\nRandom Forest for\nRegression\nand\nClassification\nClustering\nspark.bisectingKmeans\n:\nBisecting k-means\nspark.gaussianMixture\n:\nGaussian Mixture Model (GMM)\nspark.kmeans\n:\nK-Means\nspark.lda\n:\nLatent Dirichlet Allocation (LDA)\nspark.powerIterationClustering (PIC)\n:\nPower Iteration Clustering (PIC)\nCollaborative Filtering\nspark.als\n:\nAlternating Least Squares (ALS)\nFrequent Pattern Mining\nspark.fpGrowth\n:\nFP-growth\nspark.prefixSpan\n:\nPrefixSpan\nStatistics\nspark.kstest\n:\nKolmogorov-Smirnov Test\nUnder the hood, SparkR uses MLlib to train the model. Please refer to the corresponding section of MLlib user guide for example code.\nUsers can call\nsummary\nto print a summary of the fitted model,\npredict\nto make predictions on new data, and\nwrite.ml\n/\nread.ml\nto save/load fitted models.\nSparkR supports a subset of the available R formula operators for model fitting, including ‘~’, ‘.’, ‘:’, ‘+’, and ‘-‘.\nModel persistence\nThe following example shows how to save/load a MLlib model by SparkR.\ntraining\n<-\nread.df\n(\n\"data/mllib/sample_multiclass_classification_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\n# Fit a generalized linear model of family \"gaussian\" with spark.glm\ndf_list\n<-\nrandomSplit\n(\ntraining\n,\nc\n(\n7\n,\n3\n),\n2\n)\ngaussianDF\n<-\ndf_list\n[[\n1\n]]\ngaussianTestDF\n<-\ndf_list\n[[\n2\n]]\ngaussianGLM\n<-\nspark.glm\n(\ngaussianDF\n,\nlabel\n~\nfeatures\n,\nfamily\n=\n\"gaussian\"\n)\n# Save and then load a fitted MLlib model\nmodelPath\n<-\ntempfile\n(\npattern\n=\n\"ml\"\n,\nfileext\n=\n\".tmp\"\n)\nwrite.ml\n(\ngaussianGLM\n,\nmodelPath\n)\ngaussianGLM2\n<-\nread.ml\n(\nmodelPath\n)\n# Check model summary\nsummary\n(\ngaussianGLM2\n)\n# Check model prediction\ngaussianPredictions\n<-\npredict\n(\ngaussianGLM2\n,\ngaussianTestDF\n)\nhead\n(\ngaussianPredictions\n)\nunlink\n(\nmodelPath\n)\nFind full example code at \"examples/src/main/r/ml/ml.R\" in the Spark repo.\nData type mapping between R and Spark\nR\nSpark\nbyte\nbyte\ninteger\ninteger\nfloat\nfloat\ndouble\ndouble\nnumeric\ndouble\ncharacter\nstring\nstring\nstring\nbinary\nbinary\nraw\nbinary\nlogical\nboolean\nPOSIXct\ntimestamp\nPOSIXlt\ntimestamp\nDate\ndate\narray\narray\nlist\narray\nenv\nmap\nStructured Streaming\nSparkR supports the Structured Streaming API. Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. For more information see the R API on the\nStructured Streaming Programming Guide\n.\nApache Arrow in SparkR\nApache Arrow is an in-memory columnar data format that is used in Spark to efficiently transfer data between JVM and R processes. See also PySpark optimization done,\nPySpark Usage Guide for Pandas with Apache Arrow\n. This guide targets to explain how to use Arrow optimization in SparkR with some key points.\nEnsure Arrow Installed\nArrow R library is available on CRAN and it can be installed as below.\nRscript\n-e\n'install.packages(\"arrow\", repos=\"https://cloud.r-project.org/\")'\nPlease refer\nthe official documentation of Apache Arrow\nfor more details.\nNote that you must ensure that Arrow R package is installed and available on all cluster nodes.\nThe current supported minimum version is 1.0.0; however, this might change between the minor releases since Arrow optimization in SparkR is experimental.\nEnabling for Conversion to/from R DataFrame,\ndapply\nand\ngapply\nArrow optimization is available when converting a Spark DataFrame to an R DataFrame using the call\ncollect(spark_df)\n,\nwhen creating a Spark DataFrame from an R DataFrame with\ncreateDataFrame(r_df)\n, when applying an R native function to each partition\nvia\ndapply(...)\nand when applying an R native function to grouped data via\ngapply(...)\n.\nTo use Arrow when executing these, users need to set the Spark configuration ‘spark.sql.execution.arrow.sparkr.enabled’\nto ‘true’ first. This is disabled by default.\nWhether the optimization is enabled or not, SparkR produces the same results. In addition, the conversion\nbetween Spark DataFrame and R DataFrame falls back automatically to non-Arrow optimization implementation\nwhen the optimization fails for any reasons before the actual computation.\n# Start up spark session with Arrow optimization enabled\nsparkR.session\n(\nmaster\n=\n\"local[*]\"\n,\nsparkConfig\n=\nlist\n(\nspark.sql.execution.arrow.sparkr.enabled\n=\n\"true\"\n))\n# Converts Spark DataFrame from an R DataFrame\nspark_df\n<-\ncreateDataFrame\n(\nmtcars\n)\n# Converts Spark DataFrame to an R DataFrame\ncollect\n(\nspark_df\n)\n# Apply an R native function to each partition.\ncollect\n(\ndapply\n(\nspark_df\n,\nfunction\n(\nrdf\n)\n{\ndata.frame\n(\nrdf\n$\ngear\n+\n1\n)\n},\nstructType\n(\n\"gear double\"\n)))\n# Apply an R native function to grouped data.\ncollect\n(\ngapply\n(\nspark_df\n,\n\"gear\"\n,\nfunction\n(\nkey\n,\ngroup\n)\n{\ndata.frame\n(\ngear\n=\nkey\n[[\n1\n]],\ndisp\n=\nmean\n(\ngroup\n$\ndisp\n)\n>\ngroup\n$\ndisp\n)\n},\nstructType\n(\n\"gear double, disp boolean\"\n)))\nNote that even with Arrow,\ncollect(spark_df)\nresults in the collection of all records in the DataFrame to\nthe driver program and should be done on a small subset of the data. In addition, the specified output schema\nin\ngapply(...)\nand\ndapply(...)\nshould be matched to the R DataFrame’s returned by the given function.\nSupported SQL Types\nCurrently, all Spark SQL data types are supported by Arrow-based conversion except\nFloatType\n,\nBinaryType\n,\nArrayType\n,\nStructType\nand\nMapType\n.\nR Function Name Conflicts\nWhen loading and attaching a new package in R, it is possible to have a name\nconflict\n, where a\nfunction is masking another function.\nThe following functions are masked by the SparkR package:\nMasked function\nHow to Access\ncov\nin\npackage:stats\nstats::cov(x, y = NULL, use = \"everything\",\n           method = c(\"pearson\", \"kendall\", \"spearman\"))\nfilter\nin\npackage:stats\nstats::filter(x, filter, method = c(\"convolution\", \"recursive\"),\n              sides = 2, circular = FALSE, init)\nsample\nin\npackage:base\nbase::sample(x, size, replace = FALSE, prob = NULL)\nSince part of SparkR is modeled on the\ndplyr\npackage, certain functions in SparkR share the same names with those in\ndplyr\n. Depending on the load order of the two packages, some functions from the package loaded first are masked by those in the package loaded after. In such case, prefix such calls with the package name, for instance,\nSparkR::cume_dist(x)\nor\ndplyr::cume_dist(x)\n.\nYou can inspect the search path in R with\nsearch()\nMigration Guide\nThe migration guide is now archived\non this page\n."}
{"url": "https://spark.apache.org/docs/latest/submitting-applications.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSubmitting Applications\nThe\nspark-submit\nscript in Spark’s\nbin\ndirectory is used to launch applications on a cluster.\nIt can use all of Spark’s supported\ncluster managers\nthrough a uniform interface so you don’t have to configure your application especially for each one.\nBundling Your Application’s Dependencies\nIf your code depends on other projects, you will need to package them alongside\nyour application in order to distribute the code to a Spark cluster. To do this,\ncreate an assembly jar (or “uber” jar) containing your code and its dependencies. Both\nsbt\nand\nMaven\nhave assembly plugins. When creating assembly jars, list Spark and Hadoop\nas\nprovided\ndependencies; these need not be bundled since they are provided by\nthe cluster manager at runtime. Once you have an assembled jar you can call the\nbin/spark-submit\nscript as shown here while passing your jar.\nFor Python, you can use the\n--py-files\nargument of\nspark-submit\nto add\n.py\n,\n.zip\nor\n.egg\nfiles to be distributed with your application. If you depend on multiple Python files we recommend\npackaging them into a\n.zip\nor\n.egg\n. For third-party Python dependencies,\nsee\nPython Package Management\n.\nLaunching Applications with spark-submit\nOnce a user application is bundled, it can be launched using the\nbin/spark-submit\nscript.\nThis script takes care of setting up the classpath with Spark and its\ndependencies, and can support different cluster managers and deploy modes that Spark supports:\n./bin/spark-submit\n\\\n--class\n<main-class>\n\\\n--master\n<master-url>\n\\\n--deploy-mode\n<deploy-mode>\n\\\n--conf\n<key>\n=\n<value>\n\\\n...\n# other options\n<application-jar>\n\\\n[\napplication-arguments]\nSome of the commonly used options are:\n--class\n: The entry point for your application (e.g.\norg.apache.spark.examples.SparkPi\n)\n--master\n: The\nmaster URL\nfor the cluster (e.g.\nspark://23.195.26.187:7077\n)\n--deploy-mode\n: Whether to deploy your driver on the worker nodes (\ncluster\n) or locally as an external client (\nclient\n) (default:\nclient\n)\n†\n--conf\n: Arbitrary Spark configuration property in key=value format. For values that contain spaces wrap “key=value” in quotes (as shown). Multiple configurations should be passed as separate arguments. (e.g.\n--conf <key>=<value> --conf <key2>=<value2>\n)\napplication-jar\n: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an\nhdfs://\npath or a\nfile://\npath that is present on all nodes.\napplication-arguments\n: Arguments passed to the main method of your main class, if any\n†\nA common deployment strategy is to submit your application from a gateway machine\nthat is\nphysically co-located with your worker machines (e.g. Master node in a standalone EC2 cluster).\nIn this setup,\nclient\nmode is appropriate. In\nclient\nmode, the driver is launched directly\nwithin the\nspark-submit\nprocess which acts as a\nclient\nto the cluster. The input and\noutput of the application is attached to the console. Thus, this mode is especially suitable\nfor applications that involve the REPL (e.g. Spark shell).\nAlternatively, if your application is submitted from a machine far from the worker machines (e.g.\nlocally on your laptop), it is common to use\ncluster\nmode to minimize network latency between\nthe drivers and the executors. Currently, the standalone mode does not support cluster mode for Python\napplications.\nFor Python applications, simply pass a\n.py\nfile in the place of\n<application-jar>\n,\nand add Python\n.zip\n,\n.egg\nor\n.py\nfiles to the search path with\n--py-files\n.\nThere are a few options available that are specific to the\ncluster manager\nthat is being used.\nFor example, with a\nSpark standalone cluster\nwith\ncluster\ndeploy mode,\nyou can also specify\n--supervise\nto make sure that the driver is automatically restarted if it\nfails with a non-zero exit code. To enumerate all such options available to\nspark-submit\n,\nrun it with\n--help\n. Here are a few examples of common options:\n# Run application locally on 8 cores\n./bin/spark-submit\n\\\n--class\norg.apache.spark.examples.SparkPi\n\\\n--master\n\"local[8]\"\n\\\n/path/to/examples.jar\n\\\n100\n# Run on a Spark standalone cluster in client deploy mode\n./bin/spark-submit\n\\\n--class\norg.apache.spark.examples.SparkPi\n\\\n--master\nspark://207.184.161.138:7077\n\\\n--executor-memory\n20G\n\\\n--total-executor-cores\n100\n\\\n/path/to/examples.jar\n\\\n1000\n# Run on a Spark standalone cluster in cluster deploy mode with supervise\n./bin/spark-submit\n\\\n--class\norg.apache.spark.examples.SparkPi\n\\\n--master\nspark://207.184.161.138:7077\n\\\n--deploy-mode\ncluster\n\\\n--supervise\n\\\n--executor-memory\n20G\n\\\n--total-executor-cores\n100\n\\\n/path/to/examples.jar\n\\\n1000\n# Run on a YARN cluster in cluster deploy mode\nexport\nHADOOP_CONF_DIR\n=\nXXX\n./bin/spark-submit\n\\\n--class\norg.apache.spark.examples.SparkPi\n\\\n--master\nyarn\n\\\n--deploy-mode\ncluster\n\\\n--executor-memory\n20G\n\\\n--num-executors\n50\n\\\n/path/to/examples.jar\n\\\n1000\n# Run a Python application on a Spark standalone cluster\n./bin/spark-submit\n\\\n--master\nspark://207.184.161.138:7077\n\\\nexamples/src/main/python/pi.py\n\\\n1000\n# Run on a Kubernetes cluster in cluster deploy mode\n./bin/spark-submit\n\\\n--class\norg.apache.spark.examples.SparkPi\n\\\n--master\nk8s://xx.yy.zz.ww:443\n\\\n--deploy-mode\ncluster\n\\\n--executor-memory\n20G\n\\\n--num-executors\n50\n\\\nhttp://path/to/examples.jar\n\\\n1000\nMaster URLs\nThe master URL passed to Spark can be in one of the following formats:\nMaster URL\nMeaning\nlocal\nRun Spark locally with one worker thread (i.e. no parallelism at all).\nlocal[K]\nRun Spark locally with K worker threads (ideally, set this to the number of cores on your machine).\nlocal[K,F]\nRun Spark locally with K worker threads and F maxFailures (see\nspark.task.maxFailures\nfor an explanation of this variable).\nlocal[*]\nRun Spark locally with as many worker threads as logical cores on your machine.\nlocal[*,F]\nRun Spark locally with as many worker threads as logical cores on your machine and F maxFailures.\nlocal-cluster[N,C,M]\nLocal-cluster mode is only for unit tests. It emulates a distributed cluster in a single JVM with N number of workers, C cores per worker and M MiB of memory per worker.\nspark://HOST:PORT\nConnect to the given\nSpark standalone\n        cluster\nmaster. The port must be whichever one your master is configured to use, which is 7077 by default.\nspark://HOST1:PORT1,HOST2:PORT2\nConnect to the given\nSpark standalone\n        cluster with standby masters with Zookeeper\n. The list must have all the master hosts in the high availability cluster set up with Zookeeper. The port must be whichever each master is configured to use, which is 7077 by default.\nyarn\nConnect to a\nYARN\ncluster in\nclient\nor\ncluster\nmode depending on the value of\n--deploy-mode\n.\n        The cluster location will be found based on the\nHADOOP_CONF_DIR\nor\nYARN_CONF_DIR\nvariable.\nk8s://HOST:PORT\nConnect to a\nKubernetes\ncluster in\nclient\nor\ncluster\nmode depending on the value of\n--deploy-mode\n.\n        The\nHOST\nand\nPORT\nrefer to the\nKubernetes API Server\n.\n        It connects using TLS by default. In order to force it to use an unsecured connection, you can use\nk8s://http://HOST:PORT\n.\nLoading Configuration from a File\nThe\nspark-submit\nscript can load default\nSpark configuration values\nfrom a\nproperties file and pass them on to your application. The file can be specified via the\n--properties-file\nparameter. When this is not specified, by default Spark will read options from\nconf/spark-defaults.conf\nin the\nSPARK_HOME\ndirectory.\nAn additional flag\n--load-spark-defaults\ncan be used to tell Spark to load configurations from\nconf/spark-defaults.conf\neven when a property file is provided via\n--properties-file\n. This is useful, for instance, when users\nwant to put system-wide default settings in the former while user/cluster specific settings in the latter.\nLoading default Spark configurations this way can obviate the need for certain flags to\nspark-submit\n. For instance, if the\nspark.master\nproperty is set, you can safely omit the\n--master\nflag from\nspark-submit\n. In general, configuration values explicitly set on a\nSparkConf\ntake the highest precedence, then flags passed to\nspark-submit\n, then values in the\ndefaults file.\nIf you are ever unclear where configuration options are coming from, you can print out fine-grained\ndebugging information by running\nspark-submit\nwith the\n--verbose\noption.\nAdvanced Dependency Management\nWhen using\nspark-submit\n, the application jar along with any jars included with the\n--jars\noption\nwill be automatically transferred to the cluster. URLs supplied after\n--jars\nmust be separated by commas. That list is included in the driver and executor classpaths. Directory expansion does not work with\n--jars\n.\nSpark uses the following URL scheme to allow different strategies for disseminating jars:\nfile:\n- Absolute paths and\nfile:/\nURIs are served by the driver’s HTTP file server, and\nevery executor pulls the file from the driver HTTP server.\nhdfs:\n,\nhttp:\n,\nhttps:\n,\nftp:\n- these pull down files and JARs from the URI as expected\nlocal:\n- a URI starting with local:/ is expected to exist as a local file on each worker node.  This\nmeans that no network IO will be incurred, and works well for large files/JARs that are pushed to each worker,\nor shared via NFS, GlusterFS, etc.\nNote that JARs and files are copied to the working directory for each SparkContext on the executor nodes.\nThis can use up a significant amount of space over time and will need to be cleaned up. With YARN, cleanup\nis handled automatically, and with Spark standalone, automatic cleanup can be configured with the\nspark.worker.cleanup.appDataTtl\nproperty.\nUsers may also include any other dependencies by supplying a comma-delimited list of Maven coordinates\nwith\n--packages\n. All transitive dependencies will be handled when using this command. Additional\nrepositories (or resolvers in SBT) can be added in a comma-delimited fashion with the flag\n--repositories\n.\n(Note that credentials for password-protected repositories can be supplied in some cases in the repository URI,\nsuch as in\nhttps://user:password@host/...\n. Be careful when supplying credentials this way.)\nThese commands can be used with\npyspark\n,\nspark-shell\n, and\nspark-submit\nto include Spark Packages.\nFor Python, the equivalent\n--py-files\noption can be used to distribute\n.egg\n,\n.zip\nand\n.py\nlibraries\nto executors.\nMore Information\nOnce you have deployed your application, the\ncluster mode overview\ndescribes\nthe components involved in distributed execution, and how to monitor and debug applications."}
{"url": "https://spark.apache.org/docs/latest/streaming-programming-guide.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark Streaming Programming Guide\nNote\nOverview\nA Quick Example\nBasic Concepts\nLinking\nInitializing StreamingContext\nDiscretized Streams (DStreams)\nInput DStreams and Receivers\nTransformations on DStreams\nOutput Operations on DStreams\nDataFrame and SQL Operations\nMLlib Operations\nCaching / Persistence\nCheckpointing\nAccumulators, Broadcast Variables, and Checkpoints\nDeploying Applications\nMonitoring Applications\nPerformance Tuning\nReducing the Batch Processing Times\nSetting the Right Batch Interval\nMemory Tuning\nFault-tolerance Semantics\nWhere to Go from Here\nNote\nSpark Streaming is the previous generation of Spark’s streaming engine. There are no longer\nupdates to Spark Streaming and it’s a legacy project. There is a newer and easier to use\nstreaming engine in Spark called Structured Streaming. You should use Spark Structured Streaming\nfor your streaming applications and pipelines. See\nStructured Streaming Programming Guide\n.\nOverview\nSpark Streaming is an extension of the core Spark API that enables scalable, high-throughput,\nfault-tolerant stream processing of live data streams. Data can be ingested from many sources\nlike Kafka, Kinesis, or TCP sockets, and can be processed using complex\nalgorithms expressed with high-level functions like\nmap\n,\nreduce\n,\njoin\nand\nwindow\n.\nFinally, processed data can be pushed out to filesystems, databases,\nand live dashboards. In fact, you can apply Spark’s\nmachine learning\nand\ngraph processing\nalgorithms on data streams.\nInternally, it works as follows. Spark Streaming receives live input data streams and divides\nthe data into batches, which are then processed by the Spark engine to generate the final\nstream of results in batches.\nSpark Streaming provides a high-level abstraction called\ndiscretized stream\nor\nDStream\n,\nwhich represents a continuous stream of data. DStreams can be created either from input data\nstreams from sources such as Kafka, and Kinesis, or by applying high-level\noperations on other DStreams. Internally, a DStream is represented as a sequence of\nRDDs\n.\nThis guide shows you how to start writing Spark Streaming programs with DStreams. You can\nwrite Spark Streaming programs in Scala, Java or Python (introduced in Spark 1.2),\nall of which are presented in this guide.\nYou will find tabs throughout this guide that let you choose between code snippets of\ndifferent languages.\nNote:\nThere are a few APIs that are either different or not available in Python. Throughout this guide, you will find the tag\nPython API\nhighlighting these differences.\nA Quick Example\nBefore we go into the details of how to write your own Spark Streaming program,\nlet’s take a quick look at what a simple Spark Streaming program looks like. Let’s say we want to\ncount the number of words in text data received from a data server listening on a TCP\nsocket. All you need to\ndo is as follows.\nFirst, we import\nStreamingContext\n, which is the main entry point for all streaming functionality. We create a local StreamingContext with two execution threads, and batch interval of 1 second.\nfrom\npyspark\nimport\nSparkContext\nfrom\npyspark.streaming\nimport\nStreamingContext\n# Create a local StreamingContext with two working thread and batch interval of 1 second\nsc\n=\nSparkContext\n(\n\"\nlocal[2]\n\"\n,\n\"\nNetworkWordCount\n\"\n)\nssc\n=\nStreamingContext\n(\nsc\n,\n1\n)\nUsing this context, we can create a DStream that represents streaming data from a TCP\nsource, specified as hostname (e.g.\nlocalhost\n) and port (e.g.\n9999\n).\n# Create a DStream that will connect to hostname:port, like localhost:9999\nlines\n=\nssc\n.\nsocketTextStream\n(\n\"\nlocalhost\n\"\n,\n9999\n)\nThis\nlines\nDStream represents the stream of data that will be received from the data\nserver. Each record in this DStream is a line of text. Next, we want to split the lines by\nspace into words.\n# Split each line into words\nwords\n=\nlines\n.\nflatMap\n(\nlambda\nline\n:\nline\n.\nsplit\n(\n\"\n\"\n))\nflatMap\nis a one-to-many DStream operation that creates a new DStream by\ngenerating multiple new records from each record in the source DStream. In this case,\neach line will be split into multiple words and the stream of words is represented as the\nwords\nDStream.  Next, we want to count these words.\n# Count each word in each batch\npairs\n=\nwords\n.\nmap\n(\nlambda\nword\n:\n(\nword\n,\n1\n))\nwordCounts\n=\npairs\n.\nreduceByKey\n(\nlambda\nx\n,\ny\n:\nx\n+\ny\n)\n# Print the first ten elements of each RDD generated in this DStream to the console\nwordCounts\n.\npprint\n()\nThe\nwords\nDStream is further mapped (one-to-one transformation) to a DStream of\n(word,\n1)\npairs, which is then reduced to get the frequency of words in each batch of data.\nFinally,\nwordCounts.pprint()\nwill print a few of the counts generated every second.\nNote that when these lines are executed, Spark Streaming only sets up the computation it\nwill perform when it is started, and no real processing has started yet. To start the processing\nafter all the transformations have been setup, we finally call\nssc\n.\nstart\n()\n# Start the computation\nssc\n.\nawaitTermination\n()\n# Wait for the computation to terminate\nThe complete code can be found in the Spark Streaming example\nNetworkWordCount\n.\nFirst, we import the names of the Spark Streaming classes and some implicit\nconversions from StreamingContext into our environment in order to add useful methods to\nother classes we need (like DStream).\nStreamingContext\nis the\nmain entry point for all streaming functionality. We create a local StreamingContext with two execution threads,  and a batch interval of 1 second.\nimport\norg.apache.spark._\nimport\norg.apache.spark.streaming._\nimport\norg.apache.spark.streaming.StreamingContext._\n// not necessary since Spark 1.3\n// Create a local StreamingContext with two working thread and batch interval of 1 second.\n// The master requires 2 cores to prevent a starvation scenario.\nval\nconf\n=\nnew\nSparkConf\n().\nsetMaster\n(\n\"local[2]\"\n).\nsetAppName\n(\n\"NetworkWordCount\"\n)\nval\nssc\n=\nnew\nStreamingContext\n(\nconf\n,\nSeconds\n(\n1\n))\nUsing this context, we can create a DStream that represents streaming data from a TCP\nsource, specified as hostname (e.g.\nlocalhost\n) and port (e.g.\n9999\n).\n// Create a DStream that will connect to hostname:port, like localhost:9999\nval\nlines\n=\nssc\n.\nsocketTextStream\n(\n\"localhost\"\n,\n9999\n)\nThis\nlines\nDStream represents the stream of data that will be received from the data\nserver. Each record in this DStream is a line of text. Next, we want to split the lines by\nspace characters into words.\n// Split each line into words\nval\nwords\n=\nlines\n.\nflatMap\n(\n_\n.\nsplit\n(\n\" \"\n))\nflatMap\nis a one-to-many DStream operation that creates a new DStream by\ngenerating multiple new records from each record in the source DStream. In this case,\neach line will be split into multiple words and the stream of words is represented as the\nwords\nDStream.  Next, we want to count these words.\nimport\norg.apache.spark.streaming.StreamingContext._\n// not necessary since Spark 1.3\n// Count each word in each batch\nval\npairs\n=\nwords\n.\nmap\n(\nword\n=>\n(\nword\n,\n1\n))\nval\nwordCounts\n=\npairs\n.\nreduceByKey\n(\n_\n+\n_\n)\n// Print the first ten elements of each RDD generated in this DStream to the console\nwordCounts\n.\nprint\n()\nThe\nwords\nDStream is further mapped (one-to-one transformation) to a DStream of\n(word,\n1)\npairs, which is then reduced to get the frequency of words in each batch of data.\nFinally,\nwordCounts.print()\nwill print a few of the counts generated every second.\nNote that when these lines are executed, Spark Streaming only sets up the computation it\nwill perform when it is started, and no real processing has started yet. To start the processing\nafter all the transformations have been setup, we finally call\nssc\n.\nstart\n()\n// Start the computation\nssc\n.\nawaitTermination\n()\n// Wait for the computation to terminate\nThe complete code can be found in the Spark Streaming example\nNetworkWordCount\n.\nFirst, we create a\nJavaStreamingContext\nobject,\nwhich is the main entry point for all streaming\nfunctionality. We create a local StreamingContext with two execution threads, and a batch interval of 1 second.\nimport\norg.apache.spark.*\n;\nimport\norg.apache.spark.api.java.function.*\n;\nimport\norg.apache.spark.streaming.*\n;\nimport\norg.apache.spark.streaming.api.java.*\n;\nimport\nscala.Tuple2\n;\n// Create a local StreamingContext with two working thread and batch interval of 1 second\nSparkConf\nconf\n=\nnew\nSparkConf\n().\nsetMaster\n(\n\"local[2]\"\n).\nsetAppName\n(\n\"NetworkWordCount\"\n);\nJavaStreamingContext\njssc\n=\nnew\nJavaStreamingContext\n(\nconf\n,\nDurations\n.\nseconds\n(\n1\n));\nUsing this context, we can create a DStream that represents streaming data from a TCP\nsource, specified as hostname (e.g.\nlocalhost\n) and port (e.g.\n9999\n).\n// Create a DStream that will connect to hostname:port, like localhost:9999\nJavaReceiverInputDStream\n<\nString\n>\nlines\n=\njssc\n.\nsocketTextStream\n(\n\"localhost\"\n,\n9999\n);\nThis\nlines\nDStream represents the stream of data that will be received from the data\nserver. Each record in this stream is a line of text. Then, we want to split the lines by\nspace into words.\n// Split each line into words\nJavaDStream\n<\nString\n>\nwords\n=\nlines\n.\nflatMap\n(\nx\n->\nArrays\n.\nasList\n(\nx\n.\nsplit\n(\n\" \"\n)).\niterator\n());\nflatMap\nis a DStream operation that creates a new DStream by\ngenerating multiple new records from each record in the source DStream. In this case,\neach line will be split into multiple words and the stream of words is represented as the\nwords\nDStream. Note that we defined the transformation using a\nFlatMapFunction\nobject.\nAs we will discover along the way, there are a number of such convenience classes in the Java API\nthat help defines DStream transformations.\nNext, we want to count these words.\n// Count each word in each batch\nJavaPairDStream\n<\nString\n,\nInteger\n>\npairs\n=\nwords\n.\nmapToPair\n(\ns\n->\nnew\nTuple2\n<>(\ns\n,\n1\n));\nJavaPairDStream\n<\nString\n,\nInteger\n>\nwordCounts\n=\npairs\n.\nreduceByKey\n((\ni1\n,\ni2\n)\n->\ni1\n+\ni2\n);\n// Print the first ten elements of each RDD generated in this DStream to the console\nwordCounts\n.\nprint\n();\nThe\nwords\nDStream is further mapped (one-to-one transformation) to a DStream of\n(word,\n1)\npairs, using a\nPairFunction\nobject. Then, it is reduced to get the frequency of words in each batch of data,\nusing a\nFunction2\nobject.\nFinally,\nwordCounts.print()\nwill print a few of the counts generated every second.\nNote that when these lines are executed, Spark Streaming only sets up the computation it\nwill perform after it is started, and no real processing has started yet. To start the processing\nafter all the transformations have been setup, we finally call\nstart\nmethod.\njssc\n.\nstart\n();\n// Start the computation\njssc\n.\nawaitTermination\n();\n// Wait for the computation to terminate\nThe complete code can be found in the Spark Streaming example\nJavaNetworkWordCount\n.\nIf you have already\ndownloaded\nand\nbuilt\nSpark,\nyou can run this example as follows. You will first need to run Netcat\n(a small utility found in most Unix-like systems) as a data server by using\n$\nnc\n-lk\n9999\nThen, in a different terminal, you can start the example by using\n$\n./bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999\n$\n./bin/run-example streaming.NetworkWordCount localhost 9999\n$\n./bin/run-example streaming.JavaNetworkWordCount localhost 9999\nThen, any lines typed in the terminal running the netcat server will be counted and printed on\nscreen every second. It will look something like the following.\n# TERMINAL 1:\n# Running Netcat\n$\nnc\n-lk\n9999\n\nhello world\n\n\n\n...\n# TERMINAL 2: RUNNING network_wordcount.py\n$\n./bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999\n...\n-------------------------------------------\nTime: 2014-10-14 15:25:21\n-------------------------------------------\n(\nhello,1\n)\n(\nworld,1\n)\n...\n# TERMINAL 2: RUNNING NetworkWordCount\n$\n./bin/run-example streaming.NetworkWordCount localhost 9999\n...\n-------------------------------------------\nTime: 1357008430000 ms\n-------------------------------------------\n(\nhello,1\n)\n(\nworld,1\n)\n...\n# TERMINAL 2: RUNNING JavaNetworkWordCount\n$\n./bin/run-example streaming.JavaNetworkWordCount localhost 9999\n...\n-------------------------------------------\nTime: 1357008430000 ms\n-------------------------------------------\n(\nhello,1\n)\n(\nworld,1\n)\n...\nBasic Concepts\nNext, we move beyond the simple example and elaborate on the basics of Spark Streaming.\nLinking\nSimilar to Spark, Spark Streaming is available through Maven Central. To write your own Spark Streaming program, you will have to add the following dependency to your SBT or Maven project.\n<dependency>\n    <groupId>org.apache.spark</groupId>\n    <artifactId>spark-streaming_2.13</artifactId>\n    <version>4.0.0</version>\n    <scope>provided</scope>\n</dependency>\nlibraryDependencies += \"org.apache.spark\" % \"spark-streaming_2.13\" % \"4.0.0\" % \"provided\"\nFor ingesting data from sources like Kafka and Kinesis that are not present in the Spark\nStreaming core\n API, you will have to add the corresponding\nartifact\nspark-streaming-xyz_2.13\nto the dependencies. For example,\nsome of the common ones are as follows.\nSource\nArtifact\nKafka\nspark-streaming-kafka-0-10_2.13\nKinesis\nspark-streaming-kinesis-asl_2.13 [Amazon Software License]\nFor an up-to-date list, please refer to the\nMaven repository\nfor the full list of supported sources and artifacts.\nInitializing StreamingContext\nTo initialize a Spark Streaming program, a\nStreamingContext\nobject has to be created which is the main entry point of all Spark Streaming functionality.\nA\nStreamingContext\nobject can be created from a\nSparkContext\nobject.\nfrom\npyspark\nimport\nSparkContext\nfrom\npyspark.streaming\nimport\nStreamingContext\nsc\n=\nSparkContext\n(\nmaster\n,\nappName\n)\nssc\n=\nStreamingContext\n(\nsc\n,\n1\n)\nThe\nappName\nparameter is a name for your application to show on the cluster UI.\nmaster\nis a\nSpark or YARN cluster URL\n,\nor a special\n“local[*]”\nstring to run in local mode. In practice, when running on a cluster,\nyou will not want to hardcode\nmaster\nin the program,\nbut rather\nlaunch the application with\nspark-submit\nand\nreceive it there. However, for local testing and unit tests, you can pass “local[*]” to run Spark Streaming\nin-process (detects the number of cores in the local system).\nThe batch interval must be set based on the latency requirements of your application\nand available cluster resources. See the\nPerformance Tuning\nsection for more details.\nA\nStreamingContext\nobject can be created from a\nSparkConf\nobject.\nimport\norg.apache.spark._\nimport\norg.apache.spark.streaming._\nval\nconf\n=\nnew\nSparkConf\n().\nsetAppName\n(\nappName\n).\nsetMaster\n(\nmaster\n)\nval\nssc\n=\nnew\nStreamingContext\n(\nconf\n,\nSeconds\n(\n1\n))\nThe\nappName\nparameter is a name for your application to show on the cluster UI.\nmaster\nis a\nSpark, Kubernetes or YARN cluster URL\n,\nor a special\n“local[*]”\nstring to run in local mode. In practice, when running on a cluster,\nyou will not want to hardcode\nmaster\nin the program,\nbut rather\nlaunch the application with\nspark-submit\nand\nreceive it there. However, for local testing and unit tests, you can pass “local[*]” to run Spark Streaming\nin-process (detects the number of cores in the local system). Note that this internally creates a\nSparkContext\n(starting point of all Spark functionality) which can be accessed as\nssc.sparkContext\n.\nThe batch interval must be set based on the latency requirements of your application\nand available cluster resources. See the\nPerformance Tuning\nsection for more details.\nA\nStreamingContext\nobject can also be created from an existing\nSparkContext\nobject.\nimport\norg.apache.spark.streaming._\nval\nsc\n=\n...\n// existing SparkContext\nval\nssc\n=\nnew\nStreamingContext\n(\nsc\n,\nSeconds\n(\n1\n))\nA\nJavaStreamingContext\nobject can be created from a\nSparkConf\nobject.\nimport\norg.apache.spark.*\n;\nimport\norg.apache.spark.streaming.api.java.*\n;\nSparkConf\nconf\n=\nnew\nSparkConf\n().\nsetAppName\n(\nappName\n).\nsetMaster\n(\nmaster\n);\nJavaStreamingContext\nssc\n=\nnew\nJavaStreamingContext\n(\nconf\n,\nnew\nDuration\n(\n1000\n));\nThe\nappName\nparameter is a name for your application to show on the cluster UI.\nmaster\nis a\nSpark or YARN cluster URL\n,\nor a special\n“local[*]”\nstring to run in local mode. In practice, when running on a cluster,\nyou will not want to hardcode\nmaster\nin the program,\nbut rather\nlaunch the application with\nspark-submit\nand\nreceive it there. However, for local testing and unit tests, you can pass “local[*]” to run Spark Streaming\nin-process. Note that this internally creates a\nJavaSparkContext\n(starting point of all Spark functionality) which can be accessed as\nssc.sparkContext\n.\nThe batch interval must be set based on the latency requirements of your application\nand available cluster resources. See the\nPerformance Tuning\nsection for more details.\nA\nJavaStreamingContext\nobject can also be created from an existing\nJavaSparkContext\n.\nimport\norg.apache.spark.streaming.api.java.*\n;\nJavaSparkContext\nsc\n=\n...\n//existing JavaSparkContext\nJavaStreamingContext\nssc\n=\nnew\nJavaStreamingContext\n(\nsc\n,\nDurations\n.\nseconds\n(\n1\n));\nAfter a context is defined, you have to do the following.\nDefine the input sources by creating input DStreams.\nDefine the streaming computations by applying transformation and output operations to DStreams.\nStart receiving data and processing it using\nstreamingContext.start()\n.\nWait for the processing to be stopped (manually or due to any error) using\nstreamingContext.awaitTermination()\n.\nThe processing can be manually stopped using\nstreamingContext.stop()\n.\nPoints to remember:\nOnce a context has been started, no new streaming computations can be set up or added to it.\nOnce a context has been stopped, it cannot be restarted.\nOnly one StreamingContext can be active in a JVM at the same time.\nstop() on StreamingContext also stops the SparkContext. To stop only the StreamingContext, set the optional parameter of\nstop()\ncalled\nstopSparkContext\nto false.\nA SparkContext can be re-used to create multiple StreamingContexts, as long as the previous StreamingContext is stopped (without stopping the SparkContext) before the next StreamingContext is created.\nDiscretized Streams (DStreams)\nDiscretized Stream\nor\nDStream\nis the basic abstraction provided by Spark Streaming.\nIt represents a continuous stream of data, either the input data stream received from source,\nor the processed data stream generated by transforming the input stream. Internally,\na DStream is represented by a continuous series of RDDs, which is Spark’s abstraction of an immutable,\ndistributed dataset (see\nSpark Programming Guide\nfor more details). Each RDD in a DStream contains data from a certain interval,\nas shown in the following figure.\nAny operation applied on a DStream translates to operations on the underlying RDDs. For example,\nin the\nearlier example\nof converting a stream of lines to words,\nthe\nflatMap\noperation is applied on each RDD in the\nlines\nDStream to generate the RDDs of the\nwords\nDStream. This is shown in the following figure.\nThese underlying RDD transformations are computed by the Spark engine. The DStream operations\nhide most of these details and provide the developer with a higher-level API for convenience.\nThese operations are discussed in detail in later sections.\nInput DStreams and Receivers\nInput DStreams are DStreams representing the stream of input data received from streaming\nsources. In the\nquick example\n,\nlines\nwas an input DStream as it represented\nthe stream of data received from the netcat server. Every input DStream\n(except file stream, discussed later in this section) is associated with a\nReceiver\n(\nScala doc\n,\nJava doc\n) object which receives the\ndata from a source and stores it in Spark’s memory for processing.\nSpark Streaming provides two categories of built-in streaming sources.\nBasic sources\n: Sources directly available in the StreamingContext API.\nExamples: file systems, and socket connections.\nAdvanced sources\n: Sources like Kafka, Kinesis, etc. are available through\nextra utility classes. These require linking against extra dependencies as discussed in the\nlinking\nsection.\nWe are going to discuss some of the sources present in each category later in this section.\nNote that, if you want to receive multiple streams of data in parallel in your streaming\napplication, you can create multiple input DStreams (discussed\nfurther in the\nPerformance Tuning\nsection). This will\ncreate multiple receivers which will simultaneously receive multiple data streams. But note that a\nSpark worker/executor is a long-running task, hence it occupies one of the cores allocated to the\nSpark Streaming application. Therefore, it is important to remember that a Spark Streaming application\nneeds to be allocated enough cores (or threads, if running locally) to process the received data,\nas well as to run the receiver(s).\nPoints to remember\nWhen running a Spark Streaming program locally, do not use “local” or “local[1]” as the master URL.\nEither of these means that only one thread will be used for running tasks locally. If you are using\nan input DStream based on a receiver (e.g. sockets, Kafka, etc.), then the single thread will\nbe used to run the receiver, leaving no thread for processing the received data. Hence, when\nrunning locally, always use “local[\nn\n]” as the master URL, where\nn\n> number of receivers to run\n(see\nSpark Properties\nfor information on how to set\nthe master).\nExtending the logic to running on a cluster, the number of cores allocated to the Spark Streaming\napplication must be more than the number of receivers. Otherwise the system will receive data, but\nnot be able to process it.\nBasic Sources\nWe have already taken a look at the\nssc.socketTextStream(...)\nin the\nquick example\nwhich creates a DStream from text\ndata received over a TCP socket connection. Besides sockets, the StreamingContext API provides\nmethods for creating DStreams from files as input sources.\nFile Streams\nFor reading data from files on any file system compatible with the HDFS API (that is, HDFS, S3, NFS, etc.), a DStream can be created as\nvia\nStreamingContext.fileStream[KeyClass, ValueClass, InputFormatClass]\n.\nFile streams do not require running a receiver so there is no need to allocate any cores for receiving file data.\nFor simple text files, the easiest method is\nStreamingContext.textFileStream(dataDirectory)\n.\nfileStream\nis not available in the Python API; only\ntextFileStream\nis available.\nstreamingContext\n.\ntextFileStream\n(\ndataDirectory\n)\nstreamingContext\n.\nfileStream\n[\nKeyClass\n,\nValueClass\n,\nInputFormatClass\n](\ndataDirectory\n)\nFor text files\nstreamingContext\n.\ntextFileStream\n(\ndataDirectory\n)\nstreamingContext\n.\nfileStream\n<\nKeyClass\n,\nValueClass\n,\nInputFormatClass\n>(\ndataDirectory\n);\nFor text files\nstreamingContext\n.\ntextFileStream\n(\ndataDirectory\n);\nHow Directories are Monitored\nSpark Streaming will monitor the directory\ndataDirectory\nand process any files created in that directory.\nA simple directory can be monitored, such as\n\"hdfs://namenode:8040/logs/\"\n.\nAll files directly under such a path will be processed as they are discovered.\nA\nPOSIX glob pattern\ncan be supplied, such as\n\"hdfs://namenode:8040/logs/2017/*\"\n.\nHere, the DStream will consist of all files in the directories\nmatching the pattern.\nThat is: it is a pattern of directories, not of files in directories.\nAll files must be in the same data format.\nA file is considered part of a time period based on its modification time,\nnot its creation time.\nOnce processed, changes to a file within the current window will not cause the file to be reread.\nThat is:\nupdates are ignored\n.\nThe more files under a directory, the longer it will take to\nscan for changes — even if no files have been modified.\nIf a wildcard is used to identify directories, such as\n\"hdfs://namenode:8040/logs/2016-*\"\n,\nrenaming an entire directory to match the path will add the directory to the list of\nmonitored directories. Only the files in the directory whose modification time is\nwithin the current window will be included in the stream.\nCalling\nFileSystem.setTimes()\nto fix the timestamp is a way to have the file picked up in a later window, even if its contents have not changed.\nUsing Object Stores as a source of data\n“Full” Filesystems such as HDFS tend to set the modification time on their files as soon\nas the output stream is created.\nWhen a file is opened, even before data has been completely written,\nit may be included in the\nDStream\n- after which updates to the file within the same window\nwill be ignored. That is: changes may be missed, and data omitted from the stream.\nTo guarantee that changes are picked up in a window, write the file\nto an unmonitored directory, then, immediately after the output stream is closed,\nrename it into the destination directory.\nProvided the renamed file appears in the scanned destination directory during the window\nof its creation, the new data will be picked up.\nIn contrast, Object Stores such as Amazon S3 and Azure Storage usually have slow rename operations, as the\ndata is actually copied.\nFurthermore, a renamed object may have the time of the\nrename()\noperation as its modification time, so\nmay not be considered part of the window which the original create time implied they were.\nCareful testing is needed against the target object store to verify that the timestamp behavior\nof the store is consistent with that expected by Spark Streaming. It may be\nthat writing directly into a destination directory is the appropriate strategy for\nstreaming data via the chosen object store.\nFor more details on this topic, consult the\nHadoop Filesystem Specification\n.\nStreams based on Custom Receivers\nDStreams can be created with data streams received through custom receivers. See the\nCustom Receiver\n  Guide\nfor more details.\nQueue of RDDs as a Stream\nFor testing a Spark Streaming application with test data, one can also create a DStream based on a queue of RDDs, using\nstreamingContext.queueStream(queueOfRDDs)\n. Each RDD pushed into the queue will be treated as a batch of data in the DStream, and processed like a stream.\nFor more details on streams from sockets and files, see the API documentations of the relevant functions in\nStreamingContext\nfor Python,\nStreamingContext\nfor Scala,\nand\nJavaStreamingContext\nfor Java.\nAdvanced Sources\nPython API\nAs of Spark 4.0.0,\nout of these sources, Kafka and Kinesis are available in the Python API.\nThis category of sources requires interfacing with external non-Spark libraries, some of them with\ncomplex dependencies (e.g., Kafka). Hence, to minimize issues related to version conflicts\nof dependencies, the functionality to create DStreams from these sources has been moved to separate\nlibraries that can be\nlinked\nto explicitly when necessary.\nNote that these advanced sources are not available in the Spark shell, hence applications based on\nthese advanced sources cannot be tested in the shell. If you really want to use them in the Spark\nshell you will have to download the corresponding Maven artifact’s JAR along with its dependencies\nand add it to the classpath.\nSome of these advanced sources are as follows.\nKafka:\nSpark Streaming 4.0.0 is compatible with Kafka broker versions 0.10 or higher. See the\nKafka Integration Guide\nfor more details.\nKinesis:\nSpark Streaming 4.0.0 is compatible with Kinesis Client Library 1.2.1. See the\nKinesis Integration Guide\nfor more details.\nCustom Sources\nPython API\nThis is not yet supported in Python.\nInput DStreams can also be created out of custom data sources. All you have to do is implement a\nuser-defined\nreceiver\n(see next section to understand what that is) that can receive data from\nthe custom sources and push it into Spark. See the\nCustom Receiver\nGuide\nfor details.\nReceiver Reliability\nThere can be two kinds of data sources based on their\nreliability\n. Sources\n(like Kafka) allow the transferred data to be acknowledged. If the system receiving\ndata from these\nreliable\nsources acknowledges the received data correctly, it can be ensured\nthat no data will be lost due to any kind of failure. This leads to two kinds of receivers:\nReliable Receiver\n- A\nreliable receiver\ncorrectly sends acknowledgment to a reliable\n  source when the data has been received and stored in Spark with replication.\nUnreliable Receiver\n- An\nunreliable receiver\ndoes\nnot\nsend acknowledgment to a source. This can be used for sources that do not support acknowledgment, or even for reliable sources when one does not want or need to go into the complexity of acknowledgment.\nThe details of how to write a reliable receiver are discussed in the\nCustom Receiver Guide\n.\nTransformations on DStreams\nSimilar to that of RDDs, transformations allow the data from the input DStream to be modified.\nDStreams support many of the transformations available on normal Spark RDD’s.\nSome of the common ones are as follows.\nTransformation\nMeaning\nmap\n(\nfunc\n)\nReturn a new DStream by passing each element of the source DStream through a\n  function\nfunc\n.\nflatMap\n(\nfunc\n)\nSimilar to map, but each input item can be mapped to 0 or more output items.\nfilter\n(\nfunc\n)\nReturn a new DStream by selecting only the records of the source DStream on which\nfunc\nreturns true.\nrepartition\n(\nnumPartitions\n)\nChanges the level of parallelism in this DStream by creating more or fewer partitions.\nunion\n(\notherStream\n)\nReturn a new DStream that contains the union of the elements in the source DStream and\notherDStream\n.\ncount\n()\nReturn a new DStream of single-element RDDs by counting the number of elements in each RDD\n   of the source DStream.\nreduce\n(\nfunc\n)\nReturn a new DStream of single-element RDDs by aggregating the elements in each RDD of the\n  source DStream using a function\nfunc\n(which takes two arguments and returns one).\n  The function should be associative and commutative so that it can be computed in parallel.\ncountByValue\n()\nWhen called on a DStream of elements of type K, return a new DStream of (K, Long) pairs\n  where the value of each key is its frequency in each RDD of the source DStream.\nreduceByKey\n(\nfunc\n, [\nnumTasks\n])\nWhen called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the\n  values for each key are aggregated using the given reduce function.\nNote:\nBy default,\n  this uses Spark's default number of parallel tasks (2 for local mode, and in cluster mode the number\n  is determined by the config property\nspark.default.parallelism\n) to do the grouping.\n  You can pass an optional\nnumTasks\nargument to set a different number of tasks.\njoin\n(\notherStream\n, [\nnumTasks\n])\nWhen called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W))\n  pairs with all pairs of elements for each key.\ncogroup\n(\notherStream\n, [\nnumTasks\n])\nWhen called on a DStream of (K, V) and (K, W) pairs, return a new DStream of\n  (K, Seq[V], Seq[W]) tuples.\ntransform\n(\nfunc\n)\nReturn a new DStream by applying a RDD-to-RDD function to every RDD of the source DStream.\n  This can be used to do arbitrary RDD operations on the DStream.\nupdateStateByKey\n(\nfunc\n)\nReturn a new \"state\" DStream where the state for each key is updated by applying the\n  given function on the previous state of the key and the new values for the key. This can be\n  used to maintain arbitrary state data for each key.\nA few of these transformations are worth discussing in more detail.\nUpdateStateByKey Operation\nThe\nupdateStateByKey\noperation allows you to maintain arbitrary state while continuously updating\nit with new information. To use this, you will have to do two steps.\nDefine the state - The state can be an arbitrary data type.\nDefine the state update function - Specify with a function how to update the state using the\nprevious state and the new values from an input stream.\nIn every batch, Spark will apply the state  update function for all existing keys, regardless of whether they have new data in a batch or not. If the update function returns\nNone\nthen the key-value pair will be eliminated.\nLet’s illustrate this with an example. Say you want to maintain a running count of each word\nseen in a text data stream. Here, the running count is the state and it is an integer. We\ndefine the update function as:\ndef\nupdateFunction\n(\nnewValues\n,\nrunningCount\n):\nif\nrunningCount\nis\nNone\n:\nrunningCount\n=\n0\nreturn\nsum\n(\nnewValues\n,\nrunningCount\n)\n# add the new values with the previous running count to get the new count\nThis is applied on a DStream containing words (say, the\npairs\nDStream containing\n(word,\n1)\npairs in the\nearlier example\n).\nrunningCounts\n=\npairs\n.\nupdateStateByKey\n(\nupdateFunction\n)\nThe update function will be called for each word, with\nnewValues\nhaving a sequence of 1’s (from\nthe\n(word, 1)\npairs) and the\nrunningCount\nhaving the previous count. For the complete\nPython code, take a look at the example\nstateful_network_wordcount.py\n.\ndef\nupdateFunction\n(\nnewValues\n:\nSeq\n[\nInt\n],\nrunningCount\n:\nOption\n[\nInt\n])\n:\nOption\n[\nInt\n]\n=\n{\nval\nnewCount\n=\n...\n// add the new values with the previous running count to get the new count\nSome\n(\nnewCount\n)\n}\nThis is applied on a DStream containing words (say, the\npairs\nDStream containing\n(word,\n1)\npairs in the\nearlier example\n).\nval\nrunningCounts\n=\npairs\n.\nupdateStateByKey\n[\nInt\n](\nupdateFunction\n_\n)\nThe update function will be called for each word, with\nnewValues\nhaving a sequence of 1’s (from\nthe\n(word, 1)\npairs) and the\nrunningCount\nhaving the previous count.\nFunction2\n<\nList\n<\nInteger\n>,\nOptional\n<\nInteger\n>,\nOptional\n<\nInteger\n>>\nupdateFunction\n=\n(\nvalues\n,\nstate\n)\n->\n{\nInteger\nnewSum\n=\n...\n// add the new values with the previous running count to get the new count\nreturn\nOptional\n.\nof\n(\nnewSum\n);\n};\nThis is applied on a DStream containing words (say, the\npairs\nDStream containing\n(word,\n1)\npairs in the\nquick example\n).\nJavaPairDStream\n<\nString\n,\nInteger\n>\nrunningCounts\n=\npairs\n.\nupdateStateByKey\n(\nupdateFunction\n);\nThe update function will be called for each word, with\nnewValues\nhaving a sequence of 1’s (from\nthe\n(word, 1)\npairs) and the\nrunningCount\nhaving the previous count. For the complete\nJava code, take a look at the example\nJavaStatefulNetworkWordCount.java\n.\nNote that using\nupdateStateByKey\nrequires the checkpoint directory to be configured, which is\ndiscussed in detail in the\ncheckpointing\nsection.\nTransform Operation\nThe\ntransform\noperation (along with its variations like\ntransformWith\n) allows\narbitrary RDD-to-RDD functions to be applied on a DStream. It can be used to apply any RDD\noperation that is not exposed in the DStream API.\nFor example, the functionality of joining every batch in a data stream\nwith another dataset is not directly exposed in the DStream API. However,\nyou can easily use\ntransform\nto do this. This enables very powerful possibilities. For example,\none can do real-time data cleaning by joining the input data stream with precomputed\nspam information (maybe generated with Spark as well) and then filtering based on it.\nspamInfoRDD\n=\nsc\n.\npickleFile\n(...)\n# RDD containing spam information\n# join data stream with spam information to do data cleaning\ncleanedDStream\n=\nwordCounts\n.\ntransform\n(\nlambda\nrdd\n:\nrdd\n.\njoin\n(\nspamInfoRDD\n).\nfilter\n(...))\nval\nspamInfoRDD\n=\nssc\n.\nsparkContext\n.\nnewAPIHadoopRDD\n(...)\n// RDD containing spam information\nval\ncleanedDStream\n=\nwordCounts\n.\ntransform\n{\nrdd\n=>\nrdd\n.\njoin\n(\nspamInfoRDD\n).\nfilter\n(...)\n// join data stream with spam information to do data cleaning\n...\n}\nimport\norg.apache.spark.streaming.api.java.*\n;\n// RDD containing spam information\nJavaPairRDD\n<\nString\n,\nDouble\n>\nspamInfoRDD\n=\njssc\n.\nsparkContext\n().\nnewAPIHadoopRDD\n(...);\nJavaPairDStream\n<\nString\n,\nInteger\n>\ncleanedDStream\n=\nwordCounts\n.\ntransform\n(\nrdd\n->\n{\nrdd\n.\njoin\n(\nspamInfoRDD\n).\nfilter\n(...);\n// join data stream with spam information to do data cleaning\n...\n});\nNote that the supplied function gets called in every batch interval. This allows you to do\ntime-varying RDD operations, that is, RDD operations, number of partitions, broadcast variables,\netc. can be changed between batches.\nWindow Operations\nSpark Streaming also provides\nwindowed computations\n, which allow you to apply\ntransformations over a sliding window of data. The following figure illustrates this sliding\nwindow.\nAs shown in the figure, every time the window\nslides\nover a source DStream,\nthe source RDDs that fall within the window are combined and operated upon to produce the\nRDDs of the windowed DStream. In this specific case, the operation is applied over the last 3 time\nunits of data, and slides by 2 time units. This shows that any window operation needs to\nspecify two parameters.\nwindow length\n- The duration of the window (3 in the figure).\nsliding interval\n- The interval at which the window operation is performed (2 in\n the figure).\nThese two parameters must be multiples of the batch interval of the source DStream (1 in the\nfigure).\nLet’s illustrate the window operations with an example. Say, you want to extend the\nearlier example\nby generating word counts over the last 30 seconds of data,\nevery 10 seconds. To do this, we have to apply the\nreduceByKey\noperation on the\npairs\nDStream of\n(word, 1)\npairs over the last 30 seconds of data. This is done using the\noperation\nreduceByKeyAndWindow\n.\n# Reduce last 30 seconds of data, every 10 seconds\nwindowedWordCounts\n=\npairs\n.\nreduceByKeyAndWindow\n(\nlambda\nx\n,\ny\n:\nx\n+\ny\n,\nlambda\nx\n,\ny\n:\nx\n-\ny\n,\n30\n,\n10\n)\n// Reduce last 30 seconds of data, every 10 seconds\nval\nwindowedWordCounts\n=\npairs\n.\nreduceByKeyAndWindow\n((\na\n:\nInt\n,\nb\n:\nInt\n)\n=>\n(\na\n+\nb\n),\nSeconds\n(\n30\n),\nSeconds\n(\n10\n))\n// Reduce last 30 seconds of data, every 10 seconds\nJavaPairDStream\n<\nString\n,\nInteger\n>\nwindowedWordCounts\n=\npairs\n.\nreduceByKeyAndWindow\n((\ni1\n,\ni2\n)\n->\ni1\n+\ni2\n,\nDurations\n.\nseconds\n(\n30\n),\nDurations\n.\nseconds\n(\n10\n));\nSome of the common window operations are as follows. All of these operations take the\nsaid two parameters -\nwindowLength\nand\nslideInterval\n.\nTransformation\nMeaning\nwindow\n(\nwindowLength\n,\nslideInterval\n)\nReturn a new DStream which is computed based on windowed batches of the source DStream.\ncountByWindow\n(\nwindowLength\n,\nslideInterval\n)\nReturn a sliding window count of elements in the stream.\nreduceByWindow\n(\nfunc\n,\nwindowLength\n,\nslideInterval\n)\nReturn a new single-element stream, created by aggregating elements in the stream over a\n  sliding interval using\nfunc\n. The function should be associative and commutative so that it can be computed\n  correctly in parallel.\nreduceByKeyAndWindow\n(\nfunc\n,\nwindowLength\n,\nslideInterval\n,\n  [\nnumTasks\n])\nWhen called on a DStream of (K, V) pairs, returns a new DStream of (K, V)\n  pairs where the values for each key are aggregated using the given reduce function\nfunc\nover batches in a sliding window.\nNote:\nBy default, this uses Spark's default number of\n  parallel tasks (2 for local mode, and in cluster mode the number is determined by the config\n  property\nspark.default.parallelism\n) to do the grouping. You can pass an optional\nnumTasks\nargument to set a different number of tasks.\nreduceByKeyAndWindow\n(\nfunc\n,\ninvFunc\n,\nwindowLength\n,\nslideInterval\n, [\nnumTasks\n])\nA more efficient version of the above\nreduceByKeyAndWindow()\nwhere the reduce\n  value of each window is calculated incrementally using the reduce values of the previous window.\n  This is done by reducing the new data that enters the sliding window, and “inverse reducing” the\n  old data that leaves the window. An example would be that of “adding” and “subtracting” counts\n  of keys as the window slides. However, it is applicable only to “invertible reduce functions”,\n  that is, those reduce functions which have a corresponding “inverse reduce” function (taken as\n  parameter\ninvFunc\n). Like in\nreduceByKeyAndWindow\n, the number of reduce tasks\n  is configurable through an optional argument. Note that\ncheckpointing\nmust be\n  enabled for using this operation.\ncountByValueAndWindow\n(\nwindowLength\n,\nslideInterval\n, [\nnumTasks\n])\nWhen called on a DStream of (K, V) pairs, returns a new DStream of (K, Long) pairs where the\n  value of each key is its frequency within a sliding window. Like in\nreduceByKeyAndWindow\n, the number of reduce tasks is configurable through an\n  optional argument.\nJoin Operations\nFinally, it’s worth highlighting how easily you can perform different kinds of joins in Spark Streaming.\nStream-stream joins\nStreams can be very easily joined with other streams.\nstream1\n=\n...\nstream2\n=\n...\njoinedStream\n=\nstream1\n.\njoin\n(\nstream2\n)\nval\nstream1\n:\nDStream\n[\nString\n,\nString\n]\n=\n...\nval\nstream2\n:\nDStream\n[\nString\n,\nString\n]\n=\n...\nval\njoinedStream\n=\nstream1\n.\njoin\n(\nstream2\n)\nJavaPairDStream\n<\nString\n,\nString\n>\nstream1\n=\n...\nJavaPairDStream\n<\nString\n,\nString\n>\nstream2\n=\n...\nJavaPairDStream\n<\nString\n,\nTuple2\n<\nString\n,\nString\n>>\njoinedStream\n=\nstream1\n.\njoin\n(\nstream2\n);\nHere, in each batch interval, the RDD generated by\nstream1\nwill be joined with the RDD generated by\nstream2\n. You can also do\nleftOuterJoin\n,\nrightOuterJoin\n,\nfullOuterJoin\n. Furthermore, it is often very useful to do joins over windows of the streams. That is pretty easy as well.\nwindowedStream1\n=\nstream1\n.\nwindow\n(\n20\n)\nwindowedStream2\n=\nstream2\n.\nwindow\n(\n60\n)\njoinedStream\n=\nwindowedStream1\n.\njoin\n(\nwindowedStream2\n)\nval\nwindowedStream1\n=\nstream1\n.\nwindow\n(\nSeconds\n(\n20\n))\nval\nwindowedStream2\n=\nstream2\n.\nwindow\n(\nMinutes\n(\n1\n))\nval\njoinedStream\n=\nwindowedStream1\n.\njoin\n(\nwindowedStream2\n)\nJavaPairDStream\n<\nString\n,\nString\n>\nwindowedStream1\n=\nstream1\n.\nwindow\n(\nDurations\n.\nseconds\n(\n20\n));\nJavaPairDStream\n<\nString\n,\nString\n>\nwindowedStream2\n=\nstream2\n.\nwindow\n(\nDurations\n.\nminutes\n(\n1\n));\nJavaPairDStream\n<\nString\n,\nTuple2\n<\nString\n,\nString\n>>\njoinedStream\n=\nwindowedStream1\n.\njoin\n(\nwindowedStream2\n);\nStream-dataset joins\nThis has already been shown earlier while explain\nDStream.transform\noperation. Here is yet another example of joining a windowed stream with a dataset.\ndataset\n=\n...\n# some RDD\nwindowedStream\n=\nstream\n.\nwindow\n(\n20\n)\njoinedStream\n=\nwindowedStream\n.\ntransform\n(\nlambda\nrdd\n:\nrdd\n.\njoin\n(\ndataset\n))\nval\ndataset\n:\nRDD\n[\nString\n,\nString\n]\n=\n...\nval\nwindowedStream\n=\nstream\n.\nwindow\n(\nSeconds\n(\n20\n))...\nval\njoinedStream\n=\nwindowedStream\n.\ntransform\n{\nrdd\n=>\nrdd\n.\njoin\n(\ndataset\n)\n}\nJavaPairRDD\n<\nString\n,\nString\n>\ndataset\n=\n...\nJavaPairDStream\n<\nString\n,\nString\n>\nwindowedStream\n=\nstream\n.\nwindow\n(\nDurations\n.\nseconds\n(\n20\n));\nJavaPairDStream\n<\nString\n,\nString\n>\njoinedStream\n=\nwindowedStream\n.\ntransform\n(\nrdd\n->\nrdd\n.\njoin\n(\ndataset\n));\nIn fact, you can also dynamically change the dataset you want to join against. The function provided to\ntransform\nis evaluated every batch interval and therefore will use the current dataset that\ndataset\nreference points to.\nThe complete list of DStream transformations is available in the API documentation. For the Python API,\nsee\nDStream\n.\nFor the Scala API, see\nDStream\nand\nPairDStreamFunctions\n.\nFor the Java API, see\nJavaDStream\nand\nJavaPairDStream\n.\nOutput Operations on DStreams\nOutput operations allow DStream’s data to be pushed out to external systems like a database or a file system.\nSince the output operations actually allow the transformed data to be consumed by external systems,\nthey trigger the actual execution of all the DStream transformations (similar to actions for RDDs).\nCurrently, the following output operations are defined:\nOutput Operation\nMeaning\nprint\n()\nPrints the first ten elements of every batch of data in a DStream on the driver node running\n  the streaming application. This is useful for development and debugging.\nPython API\nThis is called\npprint()\nin the Python API.\nsaveAsTextFiles\n(\nprefix\n, [\nsuffix\n])\nSave this DStream's contents as text files. The file name at each batch interval is\n  generated based on\nprefix\nand\nsuffix\n:\n\"prefix-TIME_IN_MS[.suffix]\"\n.\nsaveAsObjectFiles\n(\nprefix\n, [\nsuffix\n])\nSave this DStream's contents as\nSequenceFiles\nof serialized Java objects. The file\n  name at each batch interval is generated based on\nprefix\nand\nsuffix\n:\n\"prefix-TIME_IN_MS[.suffix]\"\n.\nPython API\nThis is not available in\n  the Python API.\nsaveAsHadoopFiles\n(\nprefix\n, [\nsuffix\n])\nSave this DStream's contents as Hadoop files. The file name at each batch interval is\n  generated based on\nprefix\nand\nsuffix\n:\n\"prefix-TIME_IN_MS[.suffix]\"\n.\nPython API\nThis is not available in\n  the Python API.\nforeachRDD\n(\nfunc\n)\nThe most generic output operator that applies a function,\nfunc\n, to each RDD generated from\n  the stream. This function should push the data in each RDD to an external system, such as saving the RDD to\n  files, or writing it over the network to a database. Note that the function\nfunc\nis executed\n  in the driver process running the streaming application, and will usually have RDD actions in it\n  that will force the computation of the streaming RDDs.\nDesign Patterns for using foreachRDD\ndstream.foreachRDD\nis a powerful primitive that allows data to be sent out to external systems.\nHowever, it is important to understand how to use this primitive correctly and efficiently.\nSome of the common mistakes to avoid are as follows.\nOften writing data to external systems requires creating a connection object\n(e.g. TCP connection to a remote server) and using it to send data to a remote system.\nFor this purpose, a developer may inadvertently try creating a connection object at\nthe Spark driver, and then try to use it in a Spark worker to save records in the RDDs.\nFor example (in Scala),\ndef\nsendRecord\n(\nrdd\n):\nconnection\n=\ncreateNewConnection\n()\n# executed at the driver\nrdd\n.\nforeach\n(\nlambda\nrecord\n:\nconnection\n.\nsend\n(\nrecord\n))\nconnection\n.\nclose\n()\ndstream\n.\nforeachRDD\n(\nsendRecord\n)\ndstream\n.\nforeachRDD\n{\nrdd\n=>\nval\nconnection\n=\ncreateNewConnection\n()\n// executed at the driver\nrdd\n.\nforeach\n{\nrecord\n=>\nconnection\n.\nsend\n(\nrecord\n)\n// executed at the worker\n}\n}\ndstream\n.\nforeachRDD\n(\nrdd\n->\n{\nConnection\nconnection\n=\ncreateNewConnection\n();\n// executed at the driver\nrdd\n.\nforeach\n(\nrecord\n->\n{\nconnection\n.\nsend\n(\nrecord\n);\n// executed at the worker\n});\n});\nThis is incorrect as this requires the connection object to be serialized and sent from the\ndriver to the worker. Such connection objects are rarely transferable across machines. This\nerror may manifest as serialization errors (connection object not serializable), initialization\nerrors (connection object needs to be initialized at the workers), etc. The correct solution is\nto create the connection object at the worker.\nHowever, this can lead to another common mistake - creating a new connection for every record.\nFor example,\ndef\nsendRecord\n(\nrecord\n):\nconnection\n=\ncreateNewConnection\n()\nconnection\n.\nsend\n(\nrecord\n)\nconnection\n.\nclose\n()\ndstream\n.\nforeachRDD\n(\nlambda\nrdd\n:\nrdd\n.\nforeach\n(\nsendRecord\n))\ndstream\n.\nforeachRDD\n{\nrdd\n=>\nrdd\n.\nforeach\n{\nrecord\n=>\nval\nconnection\n=\ncreateNewConnection\n()\nconnection\n.\nsend\n(\nrecord\n)\nconnection\n.\nclose\n()\n}\n}\ndstream\n.\nforeachRDD\n(\nrdd\n->\n{\nrdd\n.\nforeach\n(\nrecord\n->\n{\nConnection\nconnection\n=\ncreateNewConnection\n();\nconnection\n.\nsend\n(\nrecord\n);\nconnection\n.\nclose\n();\n});\n});\nTypically, creating a connection object has time and resource overheads. Therefore, creating and\ndestroying a connection object for each record can incur unnecessarily high overheads and can\nsignificantly reduce the overall throughput of the system. A better solution is to use\nrdd.foreachPartition\n- create a single connection object and send all the records in a RDD\npartition using that connection.\ndef\nsendPartition\n(\niter\n):\nconnection\n=\ncreateNewConnection\n()\nfor\nrecord\nin\niter\n:\nconnection\n.\nsend\n(\nrecord\n)\nconnection\n.\nclose\n()\ndstream\n.\nforeachRDD\n(\nlambda\nrdd\n:\nrdd\n.\nforeachPartition\n(\nsendPartition\n))\ndstream\n.\nforeachRDD\n{\nrdd\n=>\nrdd\n.\nforeachPartition\n{\npartitionOfRecords\n=>\nval\nconnection\n=\ncreateNewConnection\n()\npartitionOfRecords\n.\nforeach\n(\nrecord\n=>\nconnection\n.\nsend\n(\nrecord\n))\nconnection\n.\nclose\n()\n}\n}\ndstream\n.\nforeachRDD\n(\nrdd\n->\n{\nrdd\n.\nforeachPartition\n(\npartitionOfRecords\n->\n{\nConnection\nconnection\n=\ncreateNewConnection\n();\nwhile\n(\npartitionOfRecords\n.\nhasNext\n())\n{\nconnection\n.\nsend\n(\npartitionOfRecords\n.\nnext\n());\n}\nconnection\n.\nclose\n();\n});\n});\nThis amortizes the connection creation overheads over many records.\nFinally, this can be further optimized by reusing connection objects across multiple RDDs/batches.\nOne can maintain a static pool of connection objects than can be reused as\nRDDs of multiple batches are pushed to the external system, thus further reducing the overheads.\ndef\nsendPartition\n(\niter\n):\n# ConnectionPool is a static, lazily initialized pool of connections\nconnection\n=\nConnectionPool\n.\ngetConnection\n()\nfor\nrecord\nin\niter\n:\nconnection\n.\nsend\n(\nrecord\n)\n# return to the pool for future reuse\nConnectionPool\n.\nreturnConnection\n(\nconnection\n)\ndstream\n.\nforeachRDD\n(\nlambda\nrdd\n:\nrdd\n.\nforeachPartition\n(\nsendPartition\n))\ndstream\n.\nforeachRDD\n{\nrdd\n=>\nrdd\n.\nforeachPartition\n{\npartitionOfRecords\n=>\n// ConnectionPool is a static, lazily initialized pool of connections\nval\nconnection\n=\nConnectionPool\n.\ngetConnection\n()\npartitionOfRecords\n.\nforeach\n(\nrecord\n=>\nconnection\n.\nsend\n(\nrecord\n))\nConnectionPool\n.\nreturnConnection\n(\nconnection\n)\n// return to the pool for future reuse\n}\n}\ndstream\n.\nforeachRDD\n(\nrdd\n->\n{\nrdd\n.\nforeachPartition\n(\npartitionOfRecords\n->\n{\n// ConnectionPool is a static, lazily initialized pool of connections\nConnection\nconnection\n=\nConnectionPool\n.\ngetConnection\n();\nwhile\n(\npartitionOfRecords\n.\nhasNext\n())\n{\nconnection\n.\nsend\n(\npartitionOfRecords\n.\nnext\n());\n}\nConnectionPool\n.\nreturnConnection\n(\nconnection\n);\n// return to the pool for future reuse\n});\n});\nNote that the connections in the pool should be lazily created on demand and timed out if not used for a while. This achieves the most efficient sending of data to external systems.\nOther points to remember:\nDStreams are executed lazily by the output operations, just like RDDs are lazily executed by RDD actions. Specifically, RDD actions inside the DStream output operations force the processing of the received data. Hence, if your application does not have any output operation, or has output operations like\ndstream.foreachRDD()\nwithout any RDD action inside them, then nothing will get executed. The system will simply receive the data and discard it.\nBy default, output operations are executed one-at-a-time. And they are executed in the order they are defined in the application.\nDataFrame and SQL Operations\nYou can easily use\nDataFrames and SQL\noperations on streaming data. You have to create a SparkSession using the SparkContext that the StreamingContext is using. Furthermore, this has to be done such that it can be restarted on driver failures. This is done by creating a lazily instantiated singleton instance of SparkSession. This is shown in the following example. It modifies the earlier\nword count example\nto generate word counts using DataFrames and SQL. Each RDD is converted to a DataFrame, registered as a temporary table and then queried using SQL.\n# Lazily instantiated global instance of SparkSession\ndef\ngetSparkSessionInstance\n(\nsparkConf\n):\nif\n(\n\"\nsparkSessionSingletonInstance\n\"\nnot\nin\nglobals\n()):\nglobals\n()[\n\"\nsparkSessionSingletonInstance\n\"\n]\n=\nSparkSession\n\\\n.\nbuilder\n\\\n.\nconfig\n(\nconf\n=\nsparkConf\n)\n\\\n.\ngetOrCreate\n()\nreturn\nglobals\n()[\n\"\nsparkSessionSingletonInstance\n\"\n]\n...\n# DataFrame operations inside your streaming program\nwords\n=\n...\n# DStream of strings\ndef\nprocess\n(\ntime\n,\nrdd\n):\nprint\n(\n\"\n========= %s =========\n\"\n%\nstr\n(\ntime\n))\ntry\n:\n# Get the singleton instance of SparkSession\nspark\n=\ngetSparkSessionInstance\n(\nrdd\n.\ncontext\n.\ngetConf\n())\n# Convert RDD[String] to RDD[Row] to DataFrame\nrowRdd\n=\nrdd\n.\nmap\n(\nlambda\nw\n:\nRow\n(\nword\n=\nw\n))\nwordsDataFrame\n=\nspark\n.\ncreateDataFrame\n(\nrowRdd\n)\n# Creates a temporary view using the DataFrame\nwordsDataFrame\n.\ncreateOrReplaceTempView\n(\n\"\nwords\n\"\n)\n# Do word count on table using SQL and print it\nwordCountsDataFrame\n=\nspark\n.\nsql\n(\n\"\nselect word, count(*) as total from words group by word\n\"\n)\nwordCountsDataFrame\n.\nshow\n()\nexcept\n:\npass\nwords\n.\nforeachRDD\n(\nprocess\n)\nSee the full\nsource code\n.\n/** DataFrame operations inside your streaming program */\nval\nwords\n:\nDStream\n[\nString\n]\n=\n...\nwords\n.\nforeachRDD\n{\nrdd\n=>\n// Get the singleton instance of SparkSession\nval\nspark\n=\nSparkSession\n.\nbuilder\n.\nconfig\n(\nrdd\n.\nsparkContext\n.\ngetConf\n).\ngetOrCreate\n()\nimport\nspark.implicits._\n// Convert RDD[String] to DataFrame\nval\nwordsDataFrame\n=\nrdd\n.\ntoDF\n(\n\"word\"\n)\n// Create a temporary view\nwordsDataFrame\n.\ncreateOrReplaceTempView\n(\n\"words\"\n)\n// Do word count on DataFrame using SQL and print it\nval\nwordCountsDataFrame\n=\nspark\n.\nsql\n(\n\"select word, count(*) as total from words group by word\"\n)\nwordCountsDataFrame\n.\nshow\n()\n}\nSee the full\nsource code\n.\n/** Java Bean class for converting RDD to DataFrame */\npublic\nclass\nJavaRow\nimplements\njava\n.\nio\n.\nSerializable\n{\nprivate\nString\nword\n;\npublic\nString\ngetWord\n()\n{\nreturn\nword\n;\n}\npublic\nvoid\nsetWord\n(\nString\nword\n)\n{\nthis\n.\nword\n=\nword\n;\n}\n}\n...\n/** DataFrame operations inside your streaming program */\nJavaDStream\n<\nString\n>\nwords\n=\n...\nwords\n.\nforeachRDD\n((\nrdd\n,\ntime\n)\n->\n{\n// Get the singleton instance of SparkSession\nSparkSession\nspark\n=\nSparkSession\n.\nbuilder\n().\nconfig\n(\nrdd\n.\nsparkContext\n().\ngetConf\n()).\ngetOrCreate\n();\n// Convert RDD[String] to RDD[case class] to DataFrame\nJavaRDD\n<\nJavaRow\n>\nrowRDD\n=\nrdd\n.\nmap\n(\nword\n->\n{\nJavaRow\nrecord\n=\nnew\nJavaRow\n();\nrecord\n.\nsetWord\n(\nword\n);\nreturn\nrecord\n;\n});\nDataFrame\nwordsDataFrame\n=\nspark\n.\ncreateDataFrame\n(\nrowRDD\n,\nJavaRow\n.\nclass\n);\n// Creates a temporary view using the DataFrame\nwordsDataFrame\n.\ncreateOrReplaceTempView\n(\n\"words\"\n);\n// Do word count on table using SQL and print it\nDataFrame\nwordCountsDataFrame\n=\nspark\n.\nsql\n(\n\"select word, count(*) as total from words group by word\"\n);\nwordCountsDataFrame\n.\nshow\n();\n});\nSee the full\nsource code\n.\nYou can also run SQL queries on tables defined on streaming data from a different thread (that is, asynchronous to the running StreamingContext). Just make sure that you set the StreamingContext to remember a sufficient amount of streaming data such that the query can run. Otherwise the StreamingContext, which is unaware of any of the asynchronous SQL queries, will delete off old streaming data before the query can complete. For example, if you want to query the last batch, but your query can take 5 minutes to run, then call\nstreamingContext.remember(Minutes(5))\n(in Scala, or equivalent in other languages).\nSee the\nDataFrames and SQL\nguide to learn more about DataFrames.\nMLlib Operations\nYou can also easily use machine learning algorithms provided by\nMLlib\n. First of all, there are streaming machine learning algorithms (e.g.\nStreaming Linear Regression\n,\nStreaming KMeans\n, etc.) which can simultaneously learn from the streaming data as well as apply the model on the streaming data. Beyond these, for a much larger class of machine learning algorithms, you can learn a learning model offline (i.e. using historical data) and then apply the model online on streaming data. See the\nMLlib\nguide for more details.\nCaching / Persistence\nSimilar to RDDs, DStreams also allow developers to persist the stream’s data in memory. That is,\nusing the\npersist()\nmethod on a DStream will automatically persist every RDD of that DStream in\nmemory. This is useful if the data in the DStream will be computed multiple times (e.g., multiple\noperations on the same data). For window-based operations like\nreduceByWindow\nand\nreduceByKeyAndWindow\nand state-based operations like\nupdateStateByKey\n, this is implicitly true.\nHence, DStreams generated by window-based operations are automatically persisted in memory, without\nthe developer calling\npersist()\n.\nFor input streams that receive data over the network (such as, Kafka, sockets, etc.), the\ndefault persistence level is set to replicate the data to two nodes for fault-tolerance.\nNote that, unlike RDDs, the default persistence level of DStreams keeps the data serialized in\nmemory. This is further discussed in the\nPerformance Tuning\nsection. More\ninformation on different persistence levels can be found in the\nSpark Programming Guide\n.\nCheckpointing\nA streaming application must operate 24/7 and hence must be resilient to failures unrelated\nto the application logic (e.g., system failures, JVM crashes, etc.). For this to be possible,\nSpark Streaming needs to\ncheckpoint\nenough information to a fault-\ntolerant storage system such that it can recover from failures. There are two types of data\nthat are checkpointed.\nMetadata checkpointing\n- Saving of the information defining the streaming computation to\nfault-tolerant storage like HDFS. This is used to recover from failure of the node running the\ndriver of the streaming application (discussed in detail later). Metadata includes:\nConfiguration\n- The configuration that was used to create the streaming application.\nDStream operations\n- The set of DStream operations that define the streaming application.\nIncomplete batches\n- Batches whose jobs are queued but have not completed yet.\nData checkpointing\n- Saving of the generated RDDs to reliable storage. This is necessary\nin some\nstateful\ntransformations that combine data across multiple batches. In such\ntransformations, the generated RDDs depend on RDDs of previous batches, which causes the length\nof the dependency chain to keep increasing with time. To avoid such unbounded increases in recovery\n time (proportional to dependency chain), intermediate RDDs of stateful transformations are periodically\ncheckpointed\nto reliable storage (e.g. HDFS) to cut off the dependency chains.\nTo summarize, metadata checkpointing is primarily needed for recovery from driver failures,\nwhereas data or RDD checkpointing is necessary even for basic functioning if stateful\ntransformations are used.\nWhen to enable Checkpointing\nCheckpointing must be enabled for applications with any of the following requirements:\nUsage of stateful transformations\n- If either\nupdateStateByKey\nor\nreduceByKeyAndWindow\n(with\ninverse function) is used in the application, then the checkpoint directory must be provided to\nallow for periodic RDD checkpointing.\nRecovering from failures of the driver running the application\n- Metadata checkpoints are used\n to recover with progress information.\nNote that simple streaming applications without the aforementioned stateful transformations can be\nrun without enabling checkpointing. The recovery from driver failures will also be partial in\nthat case (some received but unprocessed data may be lost). This is often acceptable and many run\nSpark Streaming applications in this way. Support for non-Hadoop environments is expected\nto improve in the future.\nHow to configure Checkpointing\nCheckpointing can be enabled by setting a directory in a fault-tolerant,\nreliable file system (e.g., HDFS, S3, etc.) to which the checkpoint information will be saved.\nThis is done by using\nstreamingContext.checkpoint(checkpointDirectory)\n. This will allow you to\nuse the aforementioned stateful transformations. Additionally,\nif you want to make the application recover from driver failures, you should rewrite your\nstreaming application to have the following behavior.\nWhen the program is being started for the first time, it will create a new StreamingContext,\nset up all the streams and then call start().\nWhen the program is being restarted after failure, it will re-create a StreamingContext\nfrom the checkpoint data in the checkpoint directory.\nThis behavior is made simple by using\nStreamingContext.getOrCreate\n. This is used as follows.\n# Function to create and setup a new StreamingContext\ndef\nfunctionToCreateContext\n():\nsc\n=\nSparkContext\n(...)\n# new context\nssc\n=\nStreamingContext\n(...)\nlines\n=\nssc\n.\nsocketTextStream\n(...)\n# create DStreams\n...\nssc\n.\ncheckpoint\n(\ncheckpointDirectory\n)\n# set checkpoint directory\nreturn\nssc\n# Get StreamingContext from checkpoint data or create a new one\ncontext\n=\nStreamingContext\n.\ngetOrCreate\n(\ncheckpointDirectory\n,\nfunctionToCreateContext\n)\n# Do additional setup on context that needs to be done,\n# irrespective of whether it is being started or restarted\ncontext\n.\n...\n# Start the context\ncontext\n.\nstart\n()\ncontext\n.\nawaitTermination\n()\nIf the\ncheckpointDirectory\nexists, then the context will be recreated from the checkpoint data.\nIf the directory does not exist (i.e., running for the first time),\nthen the function\nfunctionToCreateContext\nwill be called to create a new\ncontext and set up the DStreams. See the Python example\nrecoverable_network_wordcount.py\n.\nThis example appends the word counts of network data into a file.\nYou can also explicitly create a\nStreamingContext\nfrom the checkpoint data and start the\n computation by using\nStreamingContext.getOrCreate(checkpointDirectory, None)\n.\nThis behavior is made simple by using\nStreamingContext.getOrCreate\n. This is used as follows.\n// Function to create and setup a new StreamingContext\ndef\nfunctionToCreateContext\n()\n:\nStreamingContext\n=\n{\nval\nssc\n=\nnew\nStreamingContext\n(...)\n// new context\nval\nlines\n=\nssc\n.\nsocketTextStream\n(...)\n// create DStreams\n...\nssc\n.\ncheckpoint\n(\ncheckpointDirectory\n)\n// set checkpoint directory\nssc\n}\n// Get StreamingContext from checkpoint data or create a new one\nval\ncontext\n=\nStreamingContext\n.\ngetOrCreate\n(\ncheckpointDirectory\n,\nfunctionToCreateContext\n_\n)\n// Do additional setup on context that needs to be done,\n// irrespective of whether it is being started or restarted\ncontext\n.\n...\n// Start the context\ncontext\n.\nstart\n()\ncontext\n.\nawaitTermination\n()\nIf the\ncheckpointDirectory\nexists, then the context will be recreated from the checkpoint data.\nIf the directory does not exist (i.e., running for the first time),\nthen the function\nfunctionToCreateContext\nwill be called to create a new\ncontext and set up the DStreams. See the Scala example\nRecoverableNetworkWordCount\n.\nThis example appends the word counts of network data into a file.\nThis behavior is made simple by using\nJavaStreamingContext.getOrCreate\n. This is used as follows.\n// Create a factory object that can create and setup a new JavaStreamingContext\nJavaStreamingContextFactory\ncontextFactory\n=\nnew\nJavaStreamingContextFactory\n()\n{\n@Override\npublic\nJavaStreamingContext\ncreate\n()\n{\nJavaStreamingContext\njssc\n=\nnew\nJavaStreamingContext\n(...);\n// new context\nJavaDStream\n<\nString\n>\nlines\n=\njssc\n.\nsocketTextStream\n(...);\n// create DStreams\n...\njssc\n.\ncheckpoint\n(\ncheckpointDirectory\n);\n// set checkpoint directory\nreturn\njssc\n;\n}\n};\n// Get JavaStreamingContext from checkpoint data or create a new one\nJavaStreamingContext\ncontext\n=\nJavaStreamingContext\n.\ngetOrCreate\n(\ncheckpointDirectory\n,\ncontextFactory\n);\n// Do additional setup on context that needs to be done,\n// irrespective of whether it is being started or restarted\ncontext\n.\n...\n// Start the context\ncontext\n.\nstart\n();\ncontext\n.\nawaitTermination\n();\nIf the\ncheckpointDirectory\nexists, then the context will be recreated from the checkpoint data.\nIf the directory does not exist (i.e., running for the first time),\nthen the function\ncontextFactory\nwill be called to create a new\ncontext and set up the DStreams. See the Java example\nJavaRecoverableNetworkWordCount\n.\nThis example appends the word counts of network data into a file.\nIn addition to using\ngetOrCreate\none also needs to ensure that the driver process gets\nrestarted automatically on failure. This can only be done by the deployment infrastructure that is\nused to run the application. This is further discussed in the\nDeployment\nsection.\nNote that checkpointing of RDDs incurs the cost of saving to reliable storage.\nThis may cause an increase in the processing time of those batches where RDDs get checkpointed.\nHence, the interval of\ncheckpointing needs to be set carefully. At small batch sizes (say 1 second), checkpointing every\nbatch may significantly reduce operation throughput. Conversely, checkpointing too infrequently\ncauses the lineage and task sizes to grow, which may have detrimental effects. For stateful\ntransformations that require RDD checkpointing, the default interval is a multiple of the\nbatch interval that is at least 10 seconds. It can be set by using\ndstream.checkpoint(checkpointInterval)\n. Typically, a checkpoint interval of 5 - 10 sliding intervals of a DStream is a good setting to try.\nAccumulators, Broadcast Variables, and Checkpoints\nAccumulators\nand\nBroadcast variables\ncannot be recovered from checkpoint in Spark Streaming. If you enable checkpointing and use\nAccumulators\nor\nBroadcast variables\nas well, you’ll have to create lazily instantiated singleton instances for\nAccumulators\nand\nBroadcast variables\nso that they can be re-instantiated after the driver restarts on failure.\nThis is shown in the following example.\ndef\ngetWordExcludeList\n(\nsparkContext\n):\nif\n(\n\"\nwordExcludeList\n\"\nnot\nin\nglobals\n()):\nglobals\n()[\n\"\nwordExcludeList\n\"\n]\n=\nsparkContext\n.\nbroadcast\n([\n\"\na\n\"\n,\n\"\nb\n\"\n,\n\"\nc\n\"\n])\nreturn\nglobals\n()[\n\"\nwordExcludeList\n\"\n]\ndef\ngetDroppedWordsCounter\n(\nsparkContext\n):\nif\n(\n\"\ndroppedWordsCounter\n\"\nnot\nin\nglobals\n()):\nglobals\n()[\n\"\ndroppedWordsCounter\n\"\n]\n=\nsparkContext\n.\naccumulator\n(\n0\n)\nreturn\nglobals\n()[\n\"\ndroppedWordsCounter\n\"\n]\ndef\necho\n(\ntime\n,\nrdd\n):\n# Get or register the excludeList Broadcast\nexcludeList\n=\ngetWordExcludeList\n(\nrdd\n.\ncontext\n)\n# Get or register the droppedWordsCounter Accumulator\ndroppedWordsCounter\n=\ngetDroppedWordsCounter\n(\nrdd\n.\ncontext\n)\n# Use excludeList to drop words and use droppedWordsCounter to count them\ndef\nfilterFunc\n(\nwordCount\n):\nif\nwordCount\n[\n0\n]\nin\nexcludeList\n.\nvalue\n:\ndroppedWordsCounter\n.\nadd\n(\nwordCount\n[\n1\n])\nFalse\nelse\n:\nTrue\ncounts\n=\n\"\nCounts at time %s %s\n\"\n%\n(\ntime\n,\nrdd\n.\nfilter\n(\nfilterFunc\n).\ncollect\n())\nwordCounts\n.\nforeachRDD\n(\necho\n)\nSee the full\nsource code\n.\nobject\nWordExcludeList\n{\n@volatile\nprivate\nvar\ninstance\n:\nBroadcast\n[\nSeq\n[\nString\n]]\n=\nnull\ndef\ngetInstance\n(\nsc\n:\nSparkContext\n)\n:\nBroadcast\n[\nSeq\n[\nString\n]]\n=\n{\nif\n(\ninstance\n==\nnull\n)\n{\nsynchronized\n{\nif\n(\ninstance\n==\nnull\n)\n{\nval\nwordExcludeList\n=\nSeq\n(\n\"a\"\n,\n\"b\"\n,\n\"c\"\n)\ninstance\n=\nsc\n.\nbroadcast\n(\nwordExcludeList\n)\n}\n}\n}\ninstance\n}\n}\nobject\nDroppedWordsCounter\n{\n@volatile\nprivate\nvar\ninstance\n:\nLongAccumulator\n=\nnull\ndef\ngetInstance\n(\nsc\n:\nSparkContext\n)\n:\nLongAccumulator\n=\n{\nif\n(\ninstance\n==\nnull\n)\n{\nsynchronized\n{\nif\n(\ninstance\n==\nnull\n)\n{\ninstance\n=\nsc\n.\nlongAccumulator\n(\n\"DroppedWordsCounter\"\n)\n}\n}\n}\ninstance\n}\n}\nwordCounts\n.\nforeachRDD\n{\n(\nrdd\n:\nRDD\n[(\nString\n,\nInt\n)],\ntime\n:\nTime\n)\n=>\n// Get or register the excludeList Broadcast\nval\nexcludeList\n=\nWordExcludeList\n.\ngetInstance\n(\nrdd\n.\nsparkContext\n)\n// Get or register the droppedWordsCounter Accumulator\nval\ndroppedWordsCounter\n=\nDroppedWordsCounter\n.\ngetInstance\n(\nrdd\n.\nsparkContext\n)\n// Use excludeList to drop words and use droppedWordsCounter to count them\nval\ncounts\n=\nrdd\n.\nfilter\n{\ncase\n(\nword\n,\ncount\n)\n=>\nif\n(\nexcludeList\n.\nvalue\n.\ncontains\n(\nword\n))\n{\ndroppedWordsCounter\n.\nadd\n(\ncount\n)\nfalse\n}\nelse\n{\ntrue\n}\n}.\ncollect\n().\nmkString\n(\n\"[\"\n,\n\", \"\n,\n\"]\"\n)\nval\noutput\n=\n\"Counts at time \"\n+\ntime\n+\n\" \"\n+\ncounts\n})\nSee the full\nsource code\n.\nclass\nJavaWordExcludeList\n{\nprivate\nstatic\nvolatile\nBroadcast\n<\nList\n<\nString\n>>\ninstance\n=\nnull\n;\npublic\nstatic\nBroadcast\n<\nList\n<\nString\n>>\ngetInstance\n(\nJavaSparkContext\njsc\n)\n{\nif\n(\ninstance\n==\nnull\n)\n{\nsynchronized\n(\nJavaWordExcludeList\n.\nclass\n)\n{\nif\n(\ninstance\n==\nnull\n)\n{\nList\n<\nString\n>\nwordExcludeList\n=\nArrays\n.\nasList\n(\n\"a\"\n,\n\"b\"\n,\n\"c\"\n);\ninstance\n=\njsc\n.\nbroadcast\n(\nwordExcludeList\n);\n}\n}\n}\nreturn\ninstance\n;\n}\n}\nclass\nJavaDroppedWordsCounter\n{\nprivate\nstatic\nvolatile\nLongAccumulator\ninstance\n=\nnull\n;\npublic\nstatic\nLongAccumulator\ngetInstance\n(\nJavaSparkContext\njsc\n)\n{\nif\n(\ninstance\n==\nnull\n)\n{\nsynchronized\n(\nJavaDroppedWordsCounter\n.\nclass\n)\n{\nif\n(\ninstance\n==\nnull\n)\n{\ninstance\n=\njsc\n.\nsc\n().\nlongAccumulator\n(\n\"DroppedWordsCounter\"\n);\n}\n}\n}\nreturn\ninstance\n;\n}\n}\nwordCounts\n.\nforeachRDD\n((\nrdd\n,\ntime\n)\n->\n{\n// Get or register the excludeList Broadcast\nBroadcast\n<\nList\n<\nString\n>>\nexcludeList\n=\nJavaWordExcludeList\n.\ngetInstance\n(\nnew\nJavaSparkContext\n(\nrdd\n.\ncontext\n()));\n// Get or register the droppedWordsCounter Accumulator\nLongAccumulator\ndroppedWordsCounter\n=\nJavaDroppedWordsCounter\n.\ngetInstance\n(\nnew\nJavaSparkContext\n(\nrdd\n.\ncontext\n()));\n// Use excludeList to drop words and use droppedWordsCounter to count them\nString\ncounts\n=\nrdd\n.\nfilter\n(\nwordCount\n->\n{\nif\n(\nexcludeList\n.\nvalue\n().\ncontains\n(\nwordCount\n.\n_1\n()))\n{\ndroppedWordsCounter\n.\nadd\n(\nwordCount\n.\n_2\n());\nreturn\nfalse\n;\n}\nelse\n{\nreturn\ntrue\n;\n}\n}).\ncollect\n().\ntoString\n();\nString\noutput\n=\n\"Counts at time \"\n+\ntime\n+\n\" \"\n+\ncounts\n;\n}\nSee the full\nsource code\n.\nDeploying Applications\nThis section discusses the steps to deploy a Spark Streaming application.\nRequirements\nTo run Spark Streaming applications, you need to have the following.\nCluster with a cluster manager\n- This is the general requirement of any Spark application,\nand discussed in detail in the\ndeployment guide\n.\nPackage the application JAR\n- You have to compile your streaming application into a JAR.\nIf you are using\nspark-submit\nto start the\napplication, then you will not need to provide Spark and Spark Streaming in the JAR. However,\nif your application uses\nadvanced sources\n(e.g. Kafka),\nthen you will have to package the extra artifact they link to, along with their dependencies,\nin the JAR that is used to deploy the application. For example, an application using\nKafkaUtils\nwill have to include\nspark-streaming-kafka-0-10_2.13\nand all its\ntransitive dependencies in the application JAR.\nConfiguring sufficient memory for the executors\n- Since the received data must be stored in\nmemory, the executors must be configured with sufficient memory to hold the received data. Note\nthat if you are doing 10 minute window operations, the system has to keep at least last 10 minutes\nof data in memory. So the memory requirements for the application depends on the operations\nused in it.\nConfiguring checkpointing\n- If the stream application requires it, then a directory in the\nHadoop API compatible fault-tolerant storage (e.g. HDFS, S3, etc.) must be configured as the\ncheckpoint directory and the streaming application written in a way that checkpoint\ninformation can be used for failure recovery. See the\ncheckpointing\nsection\nfor more details.\nConfiguring automatic restart of the application driver\n- To automatically recover from a\ndriver failure, the deployment infrastructure that is\nused to run the streaming application must monitor the driver process and relaunch the driver\nif it fails. Different\ncluster managers\nhave different tools to achieve this.\nSpark Standalone\n- A Spark application driver can be submitted to run within the Spark\nStandalone cluster (see\ncluster deploy mode\n), that is, the\napplication driver itself runs on one of the worker nodes. Furthermore, the\nStandalone cluster manager can be instructed to\nsupervise\nthe driver,\nand relaunch it if the driver fails either due to non-zero exit code,\nor due to failure of the node running the driver. See\ncluster mode\nand\nsupervise\nin the\nSpark Standalone guide\nfor more details.\nYARN\n- Yarn supports a similar mechanism for automatically restarting an application.\nPlease refer to YARN documentation for more details.\nConfiguring write-ahead logs\n- Since Spark 1.2,\nwe have introduced\nwrite-ahead logs\nfor achieving strong\nfault-tolerance guarantees. If enabled,  all the data received from a receiver gets written into\na write-ahead log in the configuration checkpoint directory. This prevents data loss on driver\nrecovery, thus ensuring zero data loss (discussed in detail in the\nFault-tolerance Semantics\nsection). This can be enabled by setting\nthe\nconfiguration parameter\nspark.streaming.receiver.writeAheadLog.enable\nto\ntrue\n. However, these stronger semantics may\ncome at the cost of the receiving throughput of individual receivers. This can be corrected by\nrunning\nmore receivers in parallel\nto increase aggregate throughput. Additionally, it is recommended that the replication of the\nreceived data within Spark be disabled when the write-ahead log is enabled as the log is already\nstored in a replicated storage system. This can be done by setting the storage level for the\ninput stream to\nStorageLevel.MEMORY_AND_DISK_SER\n. While using S3 (or any file system that\ndoes not support flushing) for\nwrite-ahead logs\n, please remember to enable\nspark.streaming.driver.writeAheadLog.closeFileAfterWrite\nand\nspark.streaming.receiver.writeAheadLog.closeFileAfterWrite\n. See\nSpark Streaming Configuration\nfor more details.\nNote that Spark will not encrypt data written to the write-ahead log when I/O encryption is\nenabled. If encryption of the write-ahead log data is desired, it should be stored in a file\nsystem that supports encryption natively.\nSetting the max receiving rate\n- If the cluster resources are not large enough for the streaming\napplication to process data as fast as it is being received, the receivers can be rate limited\nby setting a maximum rate limit in terms of records / sec.\nSee the\nconfiguration parameters\nspark.streaming.receiver.maxRate\nfor receivers and\nspark.streaming.kafka.maxRatePerPartition\nfor Direct Kafka approach. In Spark 1.5, we have introduced a feature called\nbackpressure\nthat\neliminates the need to set this rate limit, as Spark Streaming automatically figures out the\nrate limits and dynamically adjusts them if the processing conditions change. This backpressure\ncan be enabled by setting the\nconfiguration parameter\nspark.streaming.backpressure.enabled\nto\ntrue\n.\nUpgrading Application Code\nIf a running Spark Streaming application needs to be upgraded with new\napplication code, then there are two possible mechanisms.\nThe upgraded Spark Streaming application is started and run in parallel to the existing application.\nOnce the new one (receiving the same data as the old one) has been warmed up and is ready\nfor prime time, the old one can be brought down. Note that this can be done for data sources that support\nsending the data to two destinations (i.e., the earlier and upgraded applications).\nThe existing application is shutdown gracefully (see\nStreamingContext.stop(...)\nor\nJavaStreamingContext.stop(...)\nfor graceful shutdown options) which ensure data that has been received is completely\nprocessed before shutdown. Then the\nupgraded application can be started, which will start processing from the same point where the earlier\napplication left off. Note that this can be done only with input sources that support source-side buffering\n(like Kafka) as data needs to be buffered while the previous application was down and\nthe upgraded application is not yet up. And restarting from earlier checkpoint\ninformation of pre-upgrade code cannot be done. The checkpoint information essentially\ncontains serialized Scala/Java/Python objects and trying to deserialize objects with new,\nmodified classes may lead to errors. In this case, either start the upgraded app with a different\ncheckpoint directory, or delete the previous checkpoint directory.\nMonitoring Applications\nBeyond Spark’s\nmonitoring capabilities\n, there are additional capabilities\nspecific to Spark Streaming. When a StreamingContext is used, the\nSpark web UI\nshows\nan additional\nStreaming\ntab which shows statistics about running receivers (whether\nreceivers are active, number of records received, receiver error, etc.)\nand completed batches (batch processing times, queueing delays, etc.). This can be used to\nmonitor the progress of the streaming application.\nThe following two metrics in web UI are particularly important:\nProcessing Time\n- The time to process each batch of data.\nScheduling Delay\n- the time a batch waits in a queue for the processing of previous batches\nto finish.\nIf the batch processing time is consistently more than the batch interval and/or the queueing\ndelay keeps increasing, then it indicates that the system is\nnot able to process the batches as fast they are being generated and is falling behind.\nIn that case, consider\nreducing\nthe batch processing time.\nThe progress of a Spark Streaming program can also be monitored using the\nStreamingListener\ninterface,\nwhich allows you to get receiver status and processing times. Note that this is a developer API\nand it is likely to be improved upon (i.e., more information reported) in the future.\nPerformance Tuning\nGetting the best performance out of a Spark Streaming application on a cluster requires a bit of\ntuning. This section explains a number of the parameters and configurations that can be tuned to\nimprove the performance of your application. At a high level, you need to consider two things:\nReducing the processing time of each batch of data by efficiently using cluster resources.\nSetting the right batch size such that the batches of data can be processed as fast as they\nare received (that is, data processing keeps up with the data ingestion).\nReducing the Batch Processing Times\nThere are a number of optimizations that can be done in Spark to minimize the processing time of\neach batch. These have been discussed in detail in the\nTuning Guide\n. This section\nhighlights some of the most important ones.\nLevel of Parallelism in Data Receiving\nReceiving data over the network (like Kafka, socket, etc.) requires the data to be deserialized\nand stored in Spark. If the data receiving becomes a bottleneck in the system, then consider\nparallelizing the data receiving. Note that each input DStream\ncreates a single receiver (running on a worker machine) that receives a single stream of data.\nReceiving multiple data streams can therefore be achieved by creating multiple input DStreams\nand configuring them to receive different partitions of the data stream from the source(s).\nFor example, a single Kafka input DStream receiving two topics of data can be split into two\nKafka input streams, each receiving only one topic. This would run two receivers,\nallowing data to be received in parallel, thus increasing overall throughput. These multiple\nDStreams can be unioned together to create a single DStream. Then the transformations that were\nbeing applied on a single input DStream can be applied on the unified stream. This is done as follows.\nnumStreams\n=\n5\nkafkaStreams\n=\n[\nKafkaUtils\n.\ncreateStream\n(...)\nfor\n_\nin\nrange\n(\nnumStreams\n)]\nunifiedStream\n=\nstreamingContext\n.\nunion\n(\n*\nkafkaStreams\n)\nunifiedStream\n.\npprint\n()\nval\nnumStreams\n=\n5\nval\nkafkaStreams\n=\n(\n1\nto\nnumStreams\n).\nmap\n{\ni\n=>\nKafkaUtils\n.\ncreateStream\n(...)\n}\nval\nunifiedStream\n=\nstreamingContext\n.\nunion\n(\nkafkaStreams\n)\nunifiedStream\n.\nprint\n()\nint\nnumStreams\n=\n5\n;\nList\n<\nJavaPairDStream\n<\nString\n,\nString\n>>\nkafkaStreams\n=\nnew\nArrayList\n<>(\nnumStreams\n);\nfor\n(\nint\ni\n=\n0\n;\ni\n<\nnumStreams\n;\ni\n++)\n{\nkafkaStreams\n.\nadd\n(\nKafkaUtils\n.\ncreateStream\n(...));\n}\nJavaPairDStream\n<\nString\n,\nString\n>\nunifiedStream\n=\nstreamingContext\n.\nunion\n(\nkafkaStreams\n.\nget\n(\n0\n),\nkafkaStreams\n.\nsubList\n(\n1\n,\nkafkaStreams\n.\nsize\n()));\nunifiedStream\n.\nprint\n();\nAnother parameter that should be considered is the receiver’s block interval,\nwhich is determined by the\nconfiguration parameter\nspark.streaming.blockInterval\n. For most receivers, the received data is coalesced together into\nblocks of data before storing inside Spark’s memory. The number of blocks in each batch\ndetermines the number of tasks that will be used to process\nthe received data in a map-like transformation. The number of tasks per receiver per batch will be\napproximately (batch interval / block interval). For example, a block interval of 200 ms will\ncreate 10 tasks per 2 second batches. If the number of tasks is too low (that is, less than the number\nof cores per machine), then it will be inefficient as all available cores will not be used to\nprocess the data. To increase the number of tasks for a given batch interval, reduce the\nblock interval. However, the recommended minimum value of block interval is about 50 ms,\nbelow which the task launching overheads may be a problem.\nAn alternative to receiving data with multiple input streams / receivers is to explicitly repartition\nthe input data stream (using\ninputStream.repartition(<number of partitions>)\n).\nThis distributes the received batches of data across the specified number of machines in the cluster\nbefore further processing.\nFor direct stream, please refer to\nSpark Streaming + Kafka Integration Guide\nLevel of Parallelism in Data Processing\nCluster resources can be under-utilized if the number of parallel tasks used in any stage of the\ncomputation is not high enough. For example, for distributed reduce operations like\nreduceByKey\nand\nreduceByKeyAndWindow\n, the default number of parallel tasks is controlled by\nthe\nspark.default.parallelism\nconfiguration property\n. You\ncan pass the level of parallelism as an argument (see\nPairDStreamFunctions\ndocumentation), or set the\nspark.default.parallelism\nconfiguration property\nto change the default.\nData Serialization\nThe overheads of data serialization can be reduced by tuning the serialization formats. In the case of streaming, there are two types of data that are being serialized.\nInput data\n: By default, the input data received through Receivers is stored in the executors’ memory with\nStorageLevel.MEMORY_AND_DISK_SER_2\n. That is, the data is serialized into bytes to reduce GC overheads, and replicated for tolerating executor failures. Also, the data is kept first in memory, and spilled over to disk only if the memory is insufficient to hold all of the input data necessary for the streaming computation. This serialization obviously has overheads – the receiver must deserialize the received data and re-serialize it using Spark’s serialization format.\nPersisted RDDs generated by Streaming Operations\n: RDDs generated by streaming computations may be persisted in memory. For example, window operations persist data in memory as they would be processed multiple times. However, unlike the Spark Core default of\nStorageLevel.MEMORY_ONLY\n, persisted RDDs generated by streaming computations are persisted with\nStorageLevel.MEMORY_ONLY_SER\n(i.e. serialized) by default to minimize GC overheads.\nIn both cases, using Kryo serialization can reduce both CPU and memory overheads. See the\nSpark Tuning Guide\nfor more details. For Kryo, consider registering custom classes, and disabling object reference tracking (see Kryo-related configurations in the\nConfiguration Guide\n).\nIn specific cases where the amount of data that needs to be retained for the streaming application is not large, it may be feasible to persist data (both types) as deserialized objects without incurring excessive GC overheads. For example, if you are using batch intervals of a few seconds and no window operations, then you can try disabling serialization in persisted data by explicitly setting the storage level accordingly. This would reduce the CPU overheads due to serialization, potentially improving performance without too much GC overheads.\nSetting the Right Batch Interval\nFor a Spark Streaming application running on a cluster to be stable, the system should be able to\nprocess data as fast as it is being received. In other words, batches of data should be processed\nas fast as they are being generated. Whether this is true for an application can be found by\nmonitoring\nthe processing times in the streaming web UI, where the batch\nprocessing time should be less than the batch interval.\nDepending on the nature of the streaming\ncomputation, the batch interval used may have significant impact on the data rates that can be\nsustained by the application on a fixed set of cluster resources. For example, let us\nconsider the earlier WordCountNetwork example. For a particular data rate, the system may be able\nto keep up with reporting word counts every 2 seconds (i.e., batch interval of 2 seconds), but not\nevery 500 milliseconds. So the batch interval needs to be set such that the expected data rate in\nproduction can be sustained.\nA good approach to figure out the right batch size for your application is to test it with a\nconservative batch interval (say, 5-10 seconds) and a low data rate. To verify whether the system\nis able to keep up with the data rate, you can check the value of the end-to-end delay experienced\nby each processed batch (either look for “Total delay” in Spark driver log4j logs, or use the\nStreamingListener\ninterface).\nIf the delay is maintained to be comparable to the batch size, then the system is stable. Otherwise,\nif the delay is continuously increasing, it means that the system is unable to keep up and it is\ntherefore unstable. Once you have an idea of a stable configuration, you can try increasing the\ndata rate and/or reducing the batch size. Note that a momentary increase in the delay due to\ntemporary data rate increases may be fine as long as the delay reduces back to a low value\n(i.e., less than batch size).\nMemory Tuning\nTuning the memory usage and GC behavior of Spark applications has been discussed in great detail\nin the\nTuning Guide\n. It is strongly recommended that you read that. In this section, we discuss a few tuning parameters specifically in the context of Spark Streaming applications.\nThe amount of cluster memory required by a Spark Streaming application depends heavily on the type of transformations used. For example, if you want to use a window operation on the last 10 minutes of data, then your cluster should have sufficient memory to hold 10 minutes worth of data in memory. Or if you want to use\nupdateStateByKey\nwith a large number of keys, then the necessary memory  will be high. On the contrary, if you want to do a simple map-filter-store operation, then the necessary memory will be low.\nIn general, since the data received through receivers is stored with StorageLevel.MEMORY_AND_DISK_SER_2, the data that does not fit in memory will spill over to the disk. This may reduce the performance of the streaming application, and hence it is advised to provide sufficient memory as required by your streaming application. Its best to try and see the memory usage on a small scale and estimate accordingly.\nAnother aspect of memory tuning is garbage collection. For a streaming application that requires low latency, it is undesirable to have large pauses caused by JVM Garbage Collection.\nThere are a few parameters that can help you tune the memory usage and GC overheads:\nPersistence Level of DStreams\n: As mentioned earlier in the\nData Serialization\nsection, the input data and RDDs are by default persisted as serialized bytes. This reduces both the memory usage and GC overheads, compared to deserialized persistence. Enabling Kryo serialization further reduces serialized sizes and memory usage. Further reduction in memory usage can be achieved with compression (see the Spark configuration\nspark.rdd.compress\n), at the cost of CPU time.\nClearing old data\n: By default, all input data and persisted RDDs generated by DStream transformations are automatically cleared. Spark Streaming decides when to clear the data based on the transformations that are used. For example, if you are using a window operation of 10 minutes, then Spark Streaming will keep around the last 10 minutes of data, and actively throw away older data.\nData can be retained for a longer duration (e.g. interactively querying older data) by setting\nstreamingContext.remember\n.\nOther tips\n: To further reduce GC overheads, here are some more tips to try.\nPersist RDDs using the\nOFF_HEAP\nstorage level. See more detail in the\nSpark Programming Guide\n.\nUse more executors with smaller heap sizes. This will reduce the GC pressure within each JVM heap.\nImportant points to remember:\nA DStream is associated with a single receiver. For attaining read parallelism multiple receivers i.e. multiple DStreams need to be created. A receiver is run within an executor. It occupies one core. Ensure that there are enough cores for processing after receiver slots are booked i.e.\nspark.cores.max\nshould take the receiver slots into account. The receivers are allocated to executors in a round robin fashion.\nWhen data is received from a stream source, the receiver creates blocks of data.  A new block of data is generated every blockInterval milliseconds. N blocks of data are created during the batchInterval where N = batchInterval/blockInterval. These blocks are distributed by the BlockManager of the current executor to the block managers of other executors. After that, the Network Input Tracker running on the driver is informed about the block locations for further processing.\nAn RDD is created on the driver for the blocks created during the batchInterval. The blocks generated during the batchInterval are partitions of the RDD. Each partition is a task in spark. blockInterval== batchinterval would mean that a single partition is created and probably it is processed locally.\nThe map tasks on the blocks are processed in the executors (one that received the block, and another where the block was replicated) that has the blocks irrespective of block interval, unless non-local scheduling kicks in.\nHaving a bigger blockinterval means bigger blocks. A high value of\nspark.locality.wait\nincreases the chance of processing a block on the local node. A balance needs to be found out between these two parameters to ensure that the bigger blocks are processed locally.\nInstead of relying on batchInterval and blockInterval, you can define the number of partitions by calling\ninputDstream.repartition(n)\n. This reshuffles the data in RDD randomly to create n number of partitions. Yes, for greater parallelism. Though comes at the cost of a shuffle. An RDD’s processing is scheduled by the driver’s jobscheduler as a job. At a given point of time only one job is active. So, if one job is executing the other jobs are queued.\nIf you have two dstreams there will be two RDDs formed and there will be two jobs created which will be scheduled one after the another. To avoid this, you can union two dstreams. This will ensure that a single unionRDD is formed for the two RDDs of the dstreams. This unionRDD is then considered as a single job. However, the partitioning of the RDDs is not impacted.\nIf the batch processing time is more than batchinterval then obviously the receiver’s memory will start filling up and will end up in throwing exceptions (most probably BlockNotFoundException). Currently, there is  no way to pause the receiver. Using SparkConf configuration\nspark.streaming.receiver.maxRate\n, rate of receiver can be limited.\nFault-tolerance Semantics\nIn this section, we will discuss the behavior of Spark Streaming applications in the event\nof failures.\nBackground\nTo understand the semantics provided by Spark Streaming, let us remember the basic fault-tolerance semantics of Spark’s RDDs.\nAn RDD is an immutable, deterministically re-computable, distributed dataset. Each RDD\nremembers the lineage of deterministic operations that were used on a fault-tolerant input\ndataset to create it.\nIf any partition of an RDD is lost due to a worker node failure, then that partition can be\nre-computed from the original fault-tolerant dataset using the lineage of operations.\nAssuming that all of the RDD transformations are deterministic, the data in the final transformed\nRDD will always be the same irrespective of failures in the Spark cluster.\nSpark operates on data in fault-tolerant file systems like HDFS or S3. Hence,\nall of the RDDs generated from the fault-tolerant data are also fault-tolerant. However, this is not\nthe case for Spark Streaming as the data in most cases is received over the network (except when\nfileStream\nis used). To achieve the same fault-tolerance properties for all of the generated RDDs,\nthe received data is replicated among multiple Spark executors in worker nodes in the cluster\n(default replication factor is 2). This leads to two kinds of data in the\nsystem that need to be recovered in the event of failures:\nData received and replicated\n- This data survives failure of a single worker node as a copy\n  of it exists on one of the other nodes.\nData received but buffered for replication\n- Since this is not replicated,\nthe only way to recover this data is to get it again from the source.\nFurthermore, there are two kinds of failures that we should be concerned about:\nFailure of a Worker Node\n- Any of the worker nodes running executors can fail,\nand all in-memory data on those nodes will be lost. If any receivers were running on failed\nnodes, then their buffered data will be lost.\nFailure of the Driver Node\n- If the driver node running the Spark Streaming application\nfails, then obviously the SparkContext is lost, and all executors with their in-memory\ndata are lost.\nWith this basic knowledge, let us understand the fault-tolerance semantics of Spark Streaming.\nDefinitions\nThe semantics of streaming systems are often captured in terms of how many times each record can be processed by the system. There are three types of guarantees that a system can provide under all possible operating conditions (despite failures, etc.)\nAt most once\n: Each record will be either processed once or not processed at all.\nAt least once\n: Each record will be processed one or more times. This is stronger than\nat-most once\nas it ensures that no data will be lost. But there may be duplicates.\nExactly once\n: Each record will be processed exactly once - no data will be lost and no data will be processed multiple times. This is obviously the strongest guarantee of the three.\nBasic Semantics\nIn any stream processing system, broadly speaking, there are three steps in processing the data.\nReceiving the data\n: The data is received from sources using Receivers or otherwise.\nTransforming the data\n: The received data is transformed using DStream and RDD transformations.\nPushing out the data\n: The final transformed data is pushed out to external systems like file systems, databases, dashboards, etc.\nIf a streaming application has to achieve end-to-end exactly-once guarantees, then each step has to provide an exactly-once guarantee. That is, each record must be received exactly once, transformed exactly once, and pushed to downstream systems exactly once. Let’s understand the semantics of these steps in the context of Spark Streaming.\nReceiving the data\n: Different input sources provide different guarantees. This is discussed in detail in the next subsection.\nTransforming the data\n: All data that has been received will be processed\nexactly once\n, thanks to the guarantees that RDDs provide. Even if there are failures, as long as the received input data is accessible, the final transformed RDDs will always have the same contents.\nPushing out the data\n: Output operations by default ensure\nat-least once\nsemantics because it depends on the type of output operation (idempotent, or not) and the semantics of the downstream system (supports transactions or not). But users can implement their own transaction mechanisms to achieve\nexactly-once\nsemantics. This is discussed in more details later in the section.\nSemantics of Received Data\nDifferent input sources provide different guarantees, ranging from\nat-least once\nto\nexactly once\n. Read for more details.\nWith Files\nIf all of the input data is already present in a fault-tolerant file system like\nHDFS, Spark Streaming can always recover from any failure and process all of the data. This gives\nexactly-once\nsemantics, meaning all of the data will be processed exactly once no matter what fails.\nWith Receiver-based Sources\nFor input sources based on receivers, the fault-tolerance semantics depend on both the failure\nscenario and the type of receiver.\nAs we discussed\nearlier\n, there are two types of receivers:\nReliable Receiver\n- These receivers acknowledge reliable sources only after ensuring that\n  the received data has been replicated. If such a receiver fails, the source will not receive\n  acknowledgment for the buffered (unreplicated) data. Therefore, if the receiver is\n  restarted, the source will resend the data, and no data will be lost due to the failure.\nUnreliable Receiver\n- Such receivers do\nnot\nsend acknowledgment and therefore\ncan\nlose\n  data when they fail due to worker or driver failures.\nDepending on what type of receivers are used we achieve the following semantics.\nIf a worker node fails, then there is no data loss with reliable receivers. With unreliable\nreceivers, data received but not replicated can get lost. If the driver node fails,\nthen besides these losses, all of the past data that was received and replicated in memory will be\nlost. This will affect the results of the stateful transformations.\nTo avoid this loss of past received data, Spark 1.2 introduced\nwrite\nahead logs\nwhich save the received data to fault-tolerant storage. With the\nwrite-ahead logs\nenabled\nand reliable receivers, there is zero data loss. In terms of semantics, it provides an at-least once guarantee.\nThe following table summarizes the semantics under failures:\nDeployment Scenario\nWorker Failure\nDriver Failure\nSpark 1.1 or earlier,\nOR\nSpark 1.2 or later without write-ahead logs\nBuffered data lost with unreliable receivers\nZero data loss with reliable receivers\nAt-least once semantics\nBuffered data lost with unreliable receivers\nPast data lost with all receivers\nUndefined semantics\nSpark 1.2 or later with write-ahead logs\nZero data loss with reliable receivers\nAt-least once semantics\nZero data loss with reliable receivers and files\nAt-least once semantics\nWith Kafka Direct API\nIn Spark 1.3, we have introduced a new Kafka Direct API, which can ensure that all the Kafka data is received by Spark Streaming exactly once. Along with this, if you implement exactly-once output operation, you can achieve end-to-end exactly-once guarantees. This approach is further discussed in the\nKafka Integration Guide\n.\nSemantics of output operations\nOutput operations (like\nforeachRDD\n) have\nat-least once\nsemantics, that is,\nthe transformed data may get written to an external entity more than once in\nthe event of a worker failure. While this is acceptable for saving to file systems using the\nsaveAs***Files\noperations (as the file will simply get overwritten with the same data),\nadditional effort may be necessary to achieve exactly-once semantics. There are two approaches.\nIdempotent updates\n: Multiple attempts always write the same data. For example,\nsaveAs***Files\nalways writes the same data to the generated files.\nTransactional updates\n: All updates are made transactionally so that updates are made exactly once atomically. One way to do this would be the following.\nUse the batch time (available in\nforeachRDD\n) and the partition index of the RDD to create an identifier. This identifier uniquely identifies a blob data in the streaming application.\nUpdate external system with this blob transactionally (that is, exactly once, atomically) using the identifier. That is, if the identifier is not already committed, commit the partition data and the identifier atomically. Else, if this was already committed, skip the update.\ndstream.foreachRDD { (rdd, time) =>\n  rdd.foreachPartition { partitionIterator =>\n    val partitionId = TaskContext.get.partitionId()\n    val uniqueId = generateUniqueId(time.milliseconds, partitionId)\n    // use this uniqueId to transactionally commit the data in partitionIterator\n  }\n}\nWhere to Go from Here\nAdditional guides\nKafka Integration Guide\nKinesis Integration Guide\nCustom Receiver Guide\nThird-party DStream data sources can be found in\nThird Party Projects\nAPI documentation\nPython docs\nStreamingContext\nand\nDStream\nScala docs\nStreamingContext\nand\nDStream\nKafkaUtils\n,\nKinesisUtils\nJava docs\nJavaStreamingContext\n,\nJavaDStream\nand\nJavaPairDStream\nKafkaUtils\n,\nKinesisUtils\nMore examples in\nPython\nand\nScala\nand\nJava\nPaper\nand\nvideo\ndescribing Spark Streaming."}
{"url": "https://spark.apache.org/docs/latest/sql-distributed-sql-engine-spark-sql-cli.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nPerformance Tuning\nDistributed SQL Engine\nRunning the Thrift JDBC/ODBC server\nRunning the Spark SQL CLI\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nSpark SQL CLI\nSpark SQL Command Line Options\nThe hiverc File\nPath interpretation\nSupported comment types\nSpark SQL CLI Interactive Shell Commands\nExamples\nThe Spark SQL CLI is a convenient interactive command tool to run the Hive metastore service and execute SQL\nqueries input from the command line. Note that the Spark SQL CLI cannot talk to the Thrift JDBC server.\nTo start the Spark SQL CLI, run the following in the Spark directory:\n./bin/spark-sql\nConfiguration of Hive is done by placing your\nhive-site.xml\n,\ncore-site.xml\nand\nhdfs-site.xml\nfiles in\nconf/\n.\nSpark SQL Command Line Options\nYou may run\n./bin/spark-sql --help\nfor a complete list of all available options.\nCLI options:\n -d,--define <key=value>          Variable substitution to apply to Hive\n                                  commands. e.g. -d A=B or --define A=B\n    --database <databasename>     Specify the database to use\n -e <quoted-query-string>         SQL from command line\n -f <filename>                    SQL from files\n -H,--help                        Print help information\n    --hiveconf <property=value>   Use value for given property\n    --hivevar <key=value>         Variable substitution to apply to Hive\n                                  commands. e.g. --hivevar A=B\n -i <filename>                    Initialization SQL file\n -S,--silent                      Silent mode in interactive shell\n -v,--verbose                     Verbose mode (echo executed SQL to the\n                                  console)\nThe hiverc File\nWhen invoked without the\n-i\n, the Spark SQL CLI will attempt to load\n$HIVE_HOME/bin/.hiverc\nand\n$HOME/.hiverc\nas initialization files.\nPath interpretation\nSpark SQL CLI supports running SQL from initialization script file(\n-i\n) or normal SQL file(\n-f\n), If path url don’t have a scheme component, the path will be handled as local file.\nFor example:\n/path/to/spark-sql-cli.sql\nequals to\nfile:///path/to/spark-sql-cli.sql\n. User also can use Hadoop supported filesystems such as\ns3://<mys3bucket>/path/to/spark-sql-cli.sql\nor\nhdfs://<namenode>:<port>/path/to/spark-sql-cli.sql\n.\nSupported comment types\nComment\nExample\nsimple comment\n-- This is a simple comment.\nSELECT 1;\nbracketed comment\n/* This is a bracketed comment. */\nSELECT 1;\nnested bracketed comment\n/*  This is a /* nested bracketed comment*/ .*/\nSELECT 1;\nSpark SQL CLI Interactive Shell Commands\nWhen\n./bin/spark-sql\nis run without either the\n-e\nor\n-f\noption, it enters interactive shell mode.\nUse\n;\n(semicolon) to terminate commands. Notice:\nThe CLI use\n;\nto terminate commands only when it’s at the end of line, and it’s not escaped by\n\\\\;\n.\n;\nis the only way to terminate commands. If the user types\nSELECT 1\nand presses enter, the console will just wait for input.\nIf the user types multiple commands in one line like\nSELECT 1; SELECT 2;\n, the commands\nSELECT 1\nand\nSELECT 2\nwill be executed separately.\nIf\n;\nappears within a SQL statement (not the end of the line), then it has no special meanings:\n-- This is a ; comment\nSELECT\n';'\nas\na\n;\nThis is just a comment line followed by a SQL query which returns a string literal.\n/* This is a comment contains ;\n*/\nSELECT\n1\n;\nHowever, if ‘;’ is the end of the line, it terminates the SQL statement. The example above will be terminated into\n/* This is a comment contains\nand\n*/ SELECT 1\n, Spark will submit these two commands separated and throw parser error (\nunclosed bracketed comment\nand\nSyntax error at or near '*/'\n).\nCommand\nDescription\nquit\nor\nexit\nExits the interactive shell.\n!<command>\nExecutes a shell command from the Spark SQL CLI shell.\ndfs <HDFS dfs command>\nExecutes a HDFS\ndfs command\nfrom the Spark SQL CLI shell.\n<query string>\nExecutes a Spark SQL query and prints results to standard output.\nsource <filepath>\nExecutes a script file inside the CLI.\nExamples\nExample of running a query from the command line:\n./bin/spark-sql -e 'SELECT COL FROM TBL'\nExample of setting Hive configuration variables:\n./bin/spark-sql -e 'SELECT COL FROM TBL' --hiveconf hive.exec.scratchdir=/home/my/hive_scratch\nExample of setting Hive configuration variables and using it in the SQL query:\n./bin/spark-sql -e 'SELECT ${hiveconf:aaa}' --hiveconf aaa=bbb --hiveconf hive.exec.scratchdir=/home/my/hive_scratch\nspark-sql> SELECT ${aaa};\nbbb\nExample of setting Hive variables substitution:\n./bin/spark-sql --hivevar aaa=bbb --define ccc=ddd\nspark-sql> SELECT ${aaa}, ${ccc};\nbbb ddd\nExample of dumping data out from a query into a file using silent mode:\n./bin/spark-sql -S -e 'SELECT COL FROM TBL' > result.txt\nExample of running a script non-interactively:\n./bin/spark-sql -f /path/to/spark-sql-script.sql\nExample of running an initialization script before entering interactive mode:\n./bin/spark-sql -i /path/to/spark-sql-init.sql\nExample of entering interactive mode:\n./bin/spark-sql\nspark-sql> SELECT 1;\n1\nspark-sql> -- This is a simple comment.\nspark-sql> SELECT 1;\n1\nExample of entering interactive mode with escape\n;\nin comment:\n./bin/spark-sql\nspark-sql>/* This is a comment contains \\\\;\n         > It won't be terminated by \\\\; */\n         > SELECT 1;\n1"}
{"url": "https://spark.apache.org/docs/latest/running-on-yarn.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nRunning Spark on YARN\nSecurity\nLaunching Spark on YARN\nAdding Other JARs\nPreparations\nConfiguration\nDebugging your Application\nSpark Properties\nAvailable patterns for SHS custom executor log URL\nResource Allocation and Configuration Overview\nStage Level Scheduling Overview\nImportant notes\nKerberos\nYARN-specific Kerberos Configuration\nTroubleshooting Kerberos\nConfiguring the External Shuffle Service\nLaunching your application with Apache Oozie\nUsing the Spark History Server to replace the Spark Web UI\nRunning multiple versions of the Spark Shuffle Service\nConfiguring different JDKs for Spark Applications\nSupport for running on\nYARN (Hadoop\nNextGen)\nwas added to Spark in version 0.6.0, and improved in subsequent releases.\nSecurity\nSecurity features like authentication are not enabled by default. When deploying a cluster that is open to the internet\nor an untrusted network, it’s important to secure access to the cluster to prevent unauthorized applications\nfrom running on the cluster.\nPlease see\nSpark Security\nand the specific security sections in this doc before running Spark.\nLaunching Spark on YARN\nApache Hadoop does not support Java 17 as of 3.4.1, while Apache Spark requires at least Java 17 since 4.0.0, so a different JDK should be configured for Spark applications.\nPlease refer to\nConfiguring different JDKs for Spark Applications\nfor details.\nEnsure that\nHADOOP_CONF_DIR\nor\nYARN_CONF_DIR\npoints to the directory which contains the (client side) configuration files for the Hadoop cluster.\nThese configs are used to write to HDFS and connect to the YARN ResourceManager. The\nconfiguration contained in this directory will be distributed to the YARN cluster so that all\ncontainers used by the application use the same configuration. If the configuration references\nJava system properties or environment variables not managed by YARN, they should also be set in the\nSpark application’s configuration (driver, executors, and the AM when running in client mode).\nThere are two deploy modes that can be used to launch Spark applications on YARN. In\ncluster\nmode, the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can go away after initiating the application. In\nclient\nmode, the driver runs in the client process, and the application master is only used for requesting resources from YARN.\nUnlike other cluster managers supported by Spark in which the master’s address is specified in the\n--master\nparameter, in YARN mode the ResourceManager’s address is picked up from the Hadoop configuration.\nThus, the\n--master\nparameter is\nyarn\n.\nTo launch a Spark application in\ncluster\nmode:\n$ ./bin/spark-submit --class path.to.your.Class --master yarn --deploy-mode cluster [options] <app jar> [app options]\nFor example:\n$ ./bin/spark-submit --class org.apache.spark.examples.SparkPi \\\n    --master yarn \\\n    --deploy-mode cluster \\\n    --driver-memory 4g \\\n    --executor-memory 2g \\\n    --executor-cores 1 \\\n    --queue thequeue \\\n    examples/jars/spark-examples*.jar \\\n    10\nThe above starts a YARN client program which starts the default Application Master. Then SparkPi will be run as a child thread of Application Master. The client will periodically poll the Application Master for status updates and display them in the console. The client will exit once your application has finished running.  Refer to the\nDebugging your Application\nsection below for how to see driver and executor logs.\nTo launch a Spark application in\nclient\nmode, do the same, but replace\ncluster\nwith\nclient\n. The following shows how you can run\nspark-shell\nin\nclient\nmode:\n$ ./bin/spark-shell --master yarn --deploy-mode client\nAdding Other JARs\nIn\ncluster\nmode, the driver runs on a different machine than the client, so\nSparkContext.addJar\nwon’t work out of the box with files that are local to the client. To make files on the client available to\nSparkContext.addJar\n, include them with the\n--jars\noption in the launch command.\n$ ./bin/spark-submit --class my.main.Class \\\n    --master yarn \\\n    --deploy-mode cluster \\\n    --jars my-other-jar.jar,my-other-other-jar.jar \\\n    my-main-jar.jar \\\n    app_arg1 app_arg2\nPreparations\nRunning Spark on YARN requires a binary distribution of Spark which is built with YARN support.\nBinary distributions can be downloaded from the\ndownloads page\nof the project website.\nThere are two variants of Spark binary distributions you can download. One is pre-built with a certain\nversion of Apache Hadoop; this Spark distribution contains built-in Hadoop runtime, so we call it\nwith-hadoop\nSpark\ndistribution. The other one is pre-built with user-provided Hadoop; since this Spark distribution\ndoesn’t contain a built-in Hadoop runtime, it’s smaller, but users have to provide a Hadoop installation separately.\nWe call this variant\nno-hadoop\nSpark distribution. For\nwith-hadoop\nSpark distribution, since\nit contains a built-in Hadoop runtime already, by default, when a job is submitted to Hadoop Yarn cluster, to prevent jar conflict, it will not\npopulate Yarn’s classpath into Spark. To override this behavior, you can set\nspark.yarn.populateHadoopClasspath=true\n.\nFor\nno-hadoop\nSpark distribution, Spark will populate Yarn’s classpath by default in order to get Hadoop runtime. For\nwith-hadoop\nSpark distribution,\nif your application depends on certain library that is only available in the cluster, you can try to populate the Yarn classpath by setting\nthe property mentioned above. If you run into jar conflict issue by doing so, you will need to turn it off and include this library\nin your application jar.\nTo build Spark yourself, refer to\nBuilding Spark\n.\nTo make Spark runtime jars accessible from YARN side, you can specify\nspark.yarn.archive\nor\nspark.yarn.jars\n. For details please refer to\nSpark Properties\n. If neither\nspark.yarn.archive\nnor\nspark.yarn.jars\nis specified, Spark will create a zip file with all jars under\n$SPARK_HOME/jars\nand upload it to the distributed cache.\nConfiguration\nMost of the configs are the same for Spark on YARN as for other deployment modes. See the\nconfiguration page\nfor more information on those.  These are configs that are specific to Spark on YARN.\nDebugging your Application\nIn YARN terminology, executors and application masters run inside “containers”. YARN has two modes for handling container logs after an application has completed. If log aggregation is turned on (with the\nyarn.log-aggregation-enable\nconfig), container logs are copied to HDFS and deleted on the local machine. These logs can be viewed from anywhere on the cluster with the\nyarn logs\ncommand.\nyarn logs -applicationId <app ID>\nwill print out the contents of all log files from all containers from the given application. You can also view the container log files directly in HDFS using the HDFS shell or API. The directory where they are located can be found by looking at your YARN configs (\nyarn.nodemanager.remote-app-log-dir\nand\nyarn.nodemanager.remote-app-log-dir-suffix\n). The logs are also available on the Spark Web UI under the Executors Tab. You need to have both the Spark history server and the MapReduce history server running and configure\nyarn.log.server.url\nin\nyarn-site.xml\nproperly. The log URL on the Spark history server UI will redirect you to the MapReduce history server to show the aggregated logs.\nWhen log aggregation isn’t turned on, logs are retained locally on each machine under\nYARN_APP_LOGS_DIR\n, which is usually configured to\n/tmp/logs\nor\n$HADOOP_HOME/logs/userlogs\ndepending on the Hadoop version and installation. Viewing logs for a container requires going to the host that contains them and looking in this directory.  Subdirectories organize log files by application ID and container ID. The logs are also available on the Spark Web UI under the Executors Tab and doesn’t require running the MapReduce history server.\nTo review per-container launch environment, increase\nyarn.nodemanager.delete.debug-delay-sec\nto a\nlarge value (e.g.\n36000\n), and then access the application cache through\nyarn.nodemanager.local-dirs\non the nodes on which containers are launched. This directory contains the launch script, JARs, and\nall environment variables used for launching each container. This process is useful for debugging\nclasspath problems in particular. (Note that enabling this requires admin privileges on cluster\nsettings and a restart of all node managers. Thus, this is not applicable to hosted clusters).\nTo use a custom log4j2 configuration for the application master or executors, here are the options:\nupload a custom\nlog4j2.properties\nusing\nspark-submit\n, by adding it to the\n--files\nlist of files\nto be uploaded with the application.\nadd\n-Dlog4j.configurationFile=<location of configuration file>\nto\nspark.driver.extraJavaOptions\n(for the driver) or\nspark.executor.extraJavaOptions\n(for executors). Note that if using a file,\nthe\nfile:\nprotocol should be explicitly provided, and the file needs to exist locally on all\nthe nodes.\nupdate the\n$SPARK_CONF_DIR/log4j2.properties\nfile and it will be automatically uploaded along\nwith the other configurations. Note that other 2 options has higher priority than this option if\nmultiple options are specified.\nNote that for the first option, both executors and the application master will share the same\nlog4j configuration, which may cause issues when they run on the same node (e.g. trying to write\nto the same log file).\nIf you need a reference to the proper location to put log files in the YARN so that YARN can properly display and aggregate them, use\nspark.yarn.app.container.log.dir\nin your\nlog4j2.properties\n. For example,\nappender.file_appender.fileName=${sys:spark.yarn.app.container.log.dir}/spark.log\n. For streaming applications, configuring\nRollingFileAppender\nand setting file location to YARN’s log directory will avoid disk overflow caused by large log files, and logs can be accessed using YARN’s log utility.\nTo use a custom metrics.properties for the application master and executors, update the\n$SPARK_CONF_DIR/metrics.properties\nfile. It will automatically be uploaded with other configurations, so you don’t need to specify it manually with\n--files\n.\nSpark Properties\nProperty Name\nDefault\nMeaning\nSince Version\nspark.yarn.am.memory\n512m\nAmount of memory to use for the YARN Application Master in client mode, in the same format as JVM memory strings (e.g.\n512m\n,\n2g\n).\n    In cluster mode, use\nspark.driver.memory\ninstead.\nUse lower-case suffixes, e.g.\nk\n,\nm\n,\ng\n,\nt\n, and\np\n, for kibi-, mebi-, gibi-, tebi-, and pebibytes, respectively.\n1.3.0\nspark.yarn.am.resource.{resource-type}.amount\n(none)\nAmount of resource to use for the YARN Application Master in client mode.\n    In cluster mode, use\nspark.yarn.driver.resource.<resource-type>.amount\ninstead.\n    Please note that this feature can be used only with YARN 3.0+\n    For reference, see\nYARN Resource Model documentation\nExample:\n    To request GPU resources from YARN, use:\nspark.yarn.am.resource.yarn.io/gpu.amount\n3.0.0\nspark.yarn.applicationType\nSPARK\nDefines more specific application types, e.g.\nSPARK\n,\nSPARK-SQL\n,\nSPARK-STREAMING\n,\nSPARK-MLLIB\nand\nSPARK-GRAPH\n. Please be careful not to exceed 20 characters.\n3.1.0\nspark.yarn.driver.resource.{resource-type}.amount\n(none)\nAmount of resource to use for the YARN Application Master in cluster mode.\n    Please note that this feature can be used only with YARN 3.0+\n    For reference, see\nYARN Resource Model documentation\nExample:\n    To request GPU resources from YARN, use:\nspark.yarn.driver.resource.yarn.io/gpu.amount\n3.0.0\nspark.yarn.executor.resource.{resource-type}.amount\n(none)\nAmount of resource to use per executor process.\n    Please note that this feature can be used only with YARN 3.0+\n    For reference, see\nYARN Resource Model documentation\nExample:\n    To request GPU resources from YARN, use:\nspark.yarn.executor.resource.yarn.io/gpu.amount\n3.0.0\nspark.yarn.resourceGpuDeviceName\nyarn.io/gpu\nSpecify the mapping of the Spark resource type of\ngpu\nto the YARN resource\n    representing a GPU. By default YARN uses\nyarn.io/gpu\nbut if YARN has been\n    configured with a custom resource type, this allows remapping it.\n    Applies when using the\nspark.{driver/executor}.resource.gpu.*\nconfigs.\n3.2.1\nspark.yarn.resourceFpgaDeviceName\nyarn.io/fpga\nSpecify the mapping of the Spark resource type of\nfpga\nto the YARN resource\n    representing a FPGA. By default YARN uses\nyarn.io/fpga\nbut if YARN has been\n    configured with a custom resource type, this allows remapping it.\n    Applies when using the\nspark.{driver/executor}.resource.fpga.*\nconfigs.\n3.2.1\nspark.yarn.am.cores\n1\nNumber of cores to use for the YARN Application Master in client mode.\n    In cluster mode, use\nspark.driver.cores\ninstead.\n1.3.0\nspark.yarn.am.waitTime\n100s\nOnly used in\ncluster\nmode. Time for the YARN Application Master to wait for the\n    SparkContext to be initialized.\n1.3.0\nspark.yarn.submit.file.replication\nThe default HDFS replication (usually\n3\n)\nHDFS replication level for the files uploaded into HDFS for the application. These include things like the Spark jar, the app jar, and any distributed cache files/archives.\n0.8.1\nspark.yarn.stagingDir\nCurrent user's home directory in the filesystem\nStaging directory used while submitting applications.\n2.0.0\nspark.yarn.preserve.staging.files\nfalse\nSet to\ntrue\nto preserve the staged files (Spark jar, app jar, distributed cache files) at the end of the job rather than delete them.\n1.1.0\nspark.yarn.scheduler.heartbeat.interval-ms\n3000\nThe interval in ms in which the Spark application master heartbeats into the YARN ResourceManager.\n    The value is capped at half the value of YARN's configuration for the expiry interval, i.e.\nyarn.am.liveness-monitor.expiry-interval-ms\n.\n0.8.1\nspark.yarn.scheduler.initial-allocation.interval\n200ms\nThe initial interval in which the Spark application master eagerly heartbeats to the YARN ResourceManager\n    when there are pending container allocation requests. It should be no larger than\nspark.yarn.scheduler.heartbeat.interval-ms\n. The allocation interval will doubled on\n    successive eager heartbeats if pending containers still exist, until\nspark.yarn.scheduler.heartbeat.interval-ms\nis reached.\n1.4.0\nspark.yarn.historyServer.address\n(none)\nThe address of the Spark history server, e.g.\nhost.com:18080\n. The address should not contain a scheme (\nhttp://\n). Defaults to not being set since the history server is an optional service. This address is given to the YARN ResourceManager when the Spark application finishes to link the application from the ResourceManager UI to the Spark history server UI.\n    For this property, YARN properties can be used as variables, and these are substituted by Spark at runtime. For example, if the Spark history server runs on the same node as the YARN ResourceManager, it can be set to\n${hadoopconf-yarn.resourcemanager.hostname}:18080\n.\n1.0.0\nspark.yarn.dist.archives\n(none)\nComma separated list of archives to be extracted into the working directory of each executor.\n1.0.0\nspark.yarn.dist.files\n(none)\nComma-separated list of files to be placed in the working directory of each executor.\n1.0.0\nspark.yarn.dist.jars\n(none)\nComma-separated list of jars to be placed in the working directory of each executor.\n2.0.0\nspark.yarn.dist.forceDownloadSchemes\n(none)\nComma-separated list of schemes for which resources will be downloaded to the local disk prior to\n    being added to YARN's distributed cache. For use in cases where the YARN service does not\n    support schemes that are supported by Spark, like http, https and ftp, or jars required to be in the\n    local YARN client's classpath. Wildcard '*' is denoted to download resources for all the schemes.\n2.3.0\nspark.executor.instances\n2\nThe number of executors for static allocation. With\nspark.dynamicAllocation.enabled\n, the initial set of executors will be at least this large.\n1.0.0\nspark.yarn.am.memoryOverhead\nAM memory * 0.10, with minimum of 384\nSame as\nspark.driver.memoryOverhead\n, but for the YARN Application Master in client mode.\n1.3.0\nspark.yarn.queue\ndefault\nThe name of the YARN queue to which the application is submitted.\n1.0.0\nspark.yarn.jars\n(none)\nList of libraries containing Spark code to distribute to YARN containers.\n    By default, Spark on YARN will use Spark jars installed locally, but the Spark jars can also be\n    in a world-readable location on HDFS. This allows YARN to cache it on nodes so that it doesn't\n    need to be distributed each time an application runs. To point to jars on HDFS, for example,\n    set this configuration to\nhdfs:///some/path\n. Globs are allowed.\n2.0.0\nspark.yarn.archive\n(none)\nAn archive containing needed Spark jars for distribution to the YARN cache. If set, this\n    configuration replaces\nspark.yarn.jars\nand the archive is used in all the\n    application's containers. The archive should contain jar files in its root directory.\n    Like with the previous option, the archive can also be hosted on HDFS to speed up file\n    distribution.\n2.0.0\nspark.yarn.appMasterEnv.[EnvironmentVariableName]\n(none)\nAdd the environment variable specified by\nEnvironmentVariableName\nto the\n     Application Master process launched on YARN. The user can specify multiple of\n     these and to set multiple environment variables. In\ncluster\nmode this controls\n     the environment of the Spark driver and in\nclient\nmode it only controls\n     the environment of the executor launcher.\n1.1.0\nspark.yarn.containerLauncherMaxThreads\n25\nThe maximum number of threads to use in the YARN Application Master for launching executor containers.\n1.2.0\nspark.yarn.am.extraJavaOptions\n(none)\nA string of extra JVM options to pass to the YARN Application Master in client mode.\n  In cluster mode, use\nspark.driver.extraJavaOptions\ninstead. Note that it is illegal\n  to set maximum heap size (-Xmx) settings with this option. Maximum heap size settings can be set\n  with\nspark.yarn.am.memory\n1.3.0\nspark.yarn.am.extraLibraryPath\n(none)\nSet a special library path to use when launching the YARN Application Master in client mode.\n1.4.0\nspark.yarn.populateHadoopClasspath\nFor\nwith-hadoop\nSpark distribution, this is set to false;\n    for\nno-hadoop\ndistribution, this is set to true.\nWhether to populate Hadoop classpath from\nyarn.application.classpath\nand\nmapreduce.application.classpath\nNote that if this is set to\nfalse\n,\n    it requires a\nwith-Hadoop\nSpark distribution that bundles Hadoop runtime or\n    user has to provide a Hadoop installation separately.\n2.4.6\nspark.yarn.maxAppAttempts\nyarn.resourcemanager.am.max-attempts\nin YARN\nThe maximum number of attempts that will be made to submit the application.\n  It should be no larger than the global number of max attempts in the YARN configuration.\n1.3.0\nspark.yarn.am.attemptFailuresValidityInterval\n(none)\nDefines the validity interval for AM failure tracking.\n  If the AM has been running for at least the defined interval, the AM failure count will be reset.\n  This feature is not enabled if not configured.\n1.6.0\nspark.yarn.am.clientModeTreatDisconnectAsFailed\nfalse\nTreat yarn-client unclean disconnects as failures. In yarn-client mode, normally the application will always finish\n  with a final status of SUCCESS because in some cases, it is not possible to know if the Application was terminated\n  intentionally by the user or if there was a real error. This config changes that behavior such that if the Application\n  Master disconnects from the driver uncleanly (ie without the proper shutdown handshake) the application will\n  terminate with a final status of FAILED. This will allow the caller to decide if it was truly a failure. Note that if\n  this config is set and the user just terminate the client application badly it may show a status of FAILED when it wasn't really FAILED.\n3.3.0\nspark.yarn.am.clientModeExitOnError\nfalse\nIn yarn-client mode, when this is true, if driver got application report with final status of KILLED or FAILED,\n  driver will stop corresponding SparkContext and exit program with code 1.\n  Note, if this is true and called from another application, it will terminate the parent application as well.\n3.3.0\nspark.yarn.am.tokenConfRegex\n(none)\nThe value of this config is a regex expression used to grep a list of config entries from the job's configuration file (e.g., hdfs-site.xml)\n    and send to RM, which uses them when renewing delegation tokens. A typical use case of this feature is to support delegation\n    tokens in an environment where a YARN cluster needs to talk to multiple downstream HDFS clusters, where the YARN RM may not have configs\n    (e.g., dfs.nameservices, dfs.ha.namenodes.*, dfs.namenode.rpc-address.*) to connect to these clusters.\n    In this scenario, Spark users can specify the config value to be\n^dfs.nameservices\\$|^dfs.namenode.rpc-address.*\\$|^dfs.ha.namenodes.*\\$\nto parse\n    these HDFS configs from the job's local configuration files. This config is very similar to\nmapreduce.job.send-token-conf\n. Please check YARN-5910 for more details.\n3.3.0\nspark.yarn.submit.waitAppCompletion\ntrue\nIn YARN cluster mode, controls whether the client waits to exit until the application completes.\n  If set to\ntrue\n, the client process will stay alive reporting the application's status.\n  Otherwise, the client process will exit after submission.\n1.4.0\nspark.yarn.am.nodeLabelExpression\n(none)\nA YARN node label expression that restricts the set of nodes AM will be scheduled on.\n  Only versions of YARN greater than or equal to 2.6 support node label expressions, so when\n  running against earlier versions, this property will be ignored.\n1.6.0\nspark.yarn.executor.nodeLabelExpression\n(none)\nA YARN node label expression that restricts the set of nodes executors will be scheduled on.\n  Only versions of YARN greater than or equal to 2.6 support node label expressions, so when\n  running against earlier versions, this property will be ignored.\n1.4.0\nspark.yarn.tags\n(none)\nComma-separated list of strings to pass through as YARN application tags appearing\n  in YARN ApplicationReports, which can be used for filtering when querying YARN apps.\n1.5.0\nspark.yarn.priority\n(none)\nApplication priority for YARN to define pending applications ordering policy, those with higher\n  integer value have a better opportunity to be activated. Currently, YARN only supports application\n  priority when using FIFO ordering policy.\n3.0.0\nspark.yarn.config.gatewayPath\n(none)\nA path that is valid on the gateway host (the host where a Spark application is started) but may\n  differ for paths for the same resource in other nodes in the cluster. Coupled with\nspark.yarn.config.replacementPath\n, this is used to support clusters with\n  heterogeneous configurations, so that Spark can correctly launch remote processes.\nThe replacement path normally will contain a reference to some environment variable exported by\n  YARN (and, thus, visible to Spark containers).\nFor example, if the gateway node has Hadoop libraries installed on\n/disk1/hadoop\n, and\n  the location of the Hadoop install is exported by YARN as the\nHADOOP_HOME\nenvironment variable, setting this value to\n/disk1/hadoop\nand the replacement path to\n$HADOOP_HOME\nwill make sure that paths used to launch remote processes properly\n  reference the local YARN configuration.\n1.5.0\nspark.yarn.config.replacementPath\n(none)\nSee\nspark.yarn.config.gatewayPath\n.\n1.5.0\nspark.yarn.rolledLog.includePattern\n(none)\nJava Regex to filter the log files which match the defined include pattern\n  and those log files will be aggregated in a rolling fashion.\n  This will be used with YARN's rolling log aggregation, to enable this feature in YARN side\nyarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds\nshould be\n  configured in yarn-site.xml. The Spark log4j appender needs be changed to use\n  FileAppender or another appender that can handle the files being removed while it is running. Based\n  on the file name configured in the log4j configuration (like spark.log), the user should set the\n  regex (spark*) to include all the log files that need to be aggregated.\n2.0.0\nspark.yarn.rolledLog.excludePattern\n(none)\nJava Regex to filter the log files which match the defined exclude pattern\n  and those log files will not be aggregated in a rolling fashion. If the log file\n  name matches both the include and the exclude pattern, this file will be excluded eventually.\n2.0.0\nspark.yarn.executor.launch.excludeOnFailure.enabled\nfalse\nFlag to enable exclusion of nodes having YARN resource allocation problems.\n  The error limit for excluding can be configured by\nspark.excludeOnFailure.application.maxFailedExecutorsPerNode\n.\n2.4.0\nspark.yarn.exclude.nodes\n(none)\nComma-separated list of YARN node names which are excluded from resource allocation.\n3.0.0\nspark.yarn.metrics.namespace\n(none)\nThe root namespace for AM metrics reporting.\n  If it is not set then the YARN application ID is used.\n2.4.0\nspark.yarn.report.interval\n1s\nInterval between reports of the current Spark job status in cluster mode.\n0.9.0\nspark.yarn.report.loggingFrequency\n30\nMaximum number of application reports processed until the next application status\n    is logged. If there is a change of state, the application status will be logged regardless\n    of the number of application reports processed.\n3.5.0\nspark.yarn.clientLaunchMonitorInterval\n1s\nInterval between requests for status the client mode AM when starting the app.\n2.3.0\nspark.yarn.includeDriverLogsLink\nfalse\nIn cluster mode, whether the client application report includes links to the driver\n    container's logs. This requires polling the ResourceManager's REST API, so it\n    places some additional load on the RM.\n3.1.0\nspark.yarn.unmanagedAM.enabled\nfalse\nIn client mode, whether to launch the Application Master service as part of the client\n    using unmanaged am.\n3.0.0\nspark.yarn.shuffle.server.recovery.disabled\nfalse\nSet to true for applications that have higher security requirements and prefer that their\n    secret is not saved in the db. The shuffle data of such applications will not be recovered after\n    the External Shuffle Service restarts.\n3.5.0\nAvailable patterns for SHS custom executor log URL\nPattern\nMeaning\n{{HTTP_SCHEME}}\nhttp://\nor\nhttps://\naccording to YARN HTTP policy. (Configured via\nyarn.http.policy\n)\n{{NM_HOST}}\nThe \"host\" of node where container was run.\n{{NM_PORT}}\nThe \"port\" of node manager where container was run.\n{{NM_HTTP_PORT}}\nThe \"port\" of node manager's http server where container was run.\n{{NM_HTTP_ADDRESS}}\nHttp URI of the node on which the container is allocated.\n{{CLUSTER_ID}}\nThe cluster ID of Resource Manager. (Configured via\nyarn.resourcemanager.cluster-id\n)\n{{CONTAINER_ID}}\nThe ID of container.\n{{USER}}\nSPARK_USER\non system environment.\n{{FILE_NAME}}\nstdout\n,\nstderr\n.\nFor example, suppose you would like to point log url link to Job History Server directly instead of let NodeManager http server redirects it, you can configure\nspark.history.custom.executor.log.url\nas below:\n{{HTTP_SCHEME}}<JHS_HOST>:<JHS_PORT>/jobhistory/logs/{{NM_HOST}}:{{NM_PORT}}/{{CONTAINER_ID}}/{{CONTAINER_ID}}/{{USER}}/{{FILE_NAME}}?start=-4096\nNOTE: you need to replace\n<JHS_HOST>\nand\n<JHS_PORT>\nwith actual value.\nResource Allocation and Configuration Overview\nPlease make sure to have read the Custom Resource Scheduling and Configuration Overview section on the\nconfiguration page\n. This section only talks about the YARN specific aspects of resource scheduling.\nYARN needs to be configured to support any resources the user wants to use with Spark. Resource scheduling on YARN was added in YARN 3.1.0. See the YARN documentation for more information on configuring resources and properly setting up isolation. Ideally the resources are setup isolated so that an executor can only see the resources it was allocated. If you do not have isolation enabled, the user is responsible for creating a discovery script that ensures the resource is not shared between executors.\nYARN supports user defined resource types but has built in types for GPU (\nyarn.io/gpu\n) and FPGA (\nyarn.io/fpga\n). For that reason, if you are using either of those resources, Spark can translate your request for spark resources into YARN resources and you only have to specify the\nspark.{driver/executor}.resource.\nconfigs. Note, if you are using a custom resource type for GPUs or FPGAs with YARN you can change the Spark mapping using\nspark.yarn.resourceGpuDeviceName\nand\nspark.yarn.resourceFpgaDeviceName\n.\n If you are using a resource other than FPGA or GPU, the user is responsible for specifying the configs for both YARN (\nspark.yarn.{driver/executor}.resource.\n) and Spark (\nspark.{driver/executor}.resource.\n).\nFor example, the user wants to request 2 GPUs for each executor. The user can just specify\nspark.executor.resource.gpu.amount=2\nand Spark will handle requesting\nyarn.io/gpu\nresource type from YARN.\nIf the user has a user defined YARN resource, lets call it\nacceleratorX\nthen the user must specify\nspark.yarn.executor.resource.acceleratorX.amount=2\nand\nspark.executor.resource.acceleratorX.amount=2\n.\nYARN does not tell Spark the addresses of the resources allocated to each container. For that reason, the user must specify a discovery script that gets run by the executor on startup to discover what resources are available to that executor. You can find an example scripts in\nexamples/src/main/scripts/getGpusResources.sh\n. The script must have execute permissions set and the user should setup permissions to not allow malicious users to modify it. The script should write to STDOUT a JSON string in the format of the ResourceInformation class. This has the resource name and an array of resource addresses available to just that executor.\nStage Level Scheduling Overview\nStage level scheduling is supported on YARN:\nWhen dynamic allocation is disabled: It allows users to specify different task resource requirements at the stage level and will use the same executors requested at startup.\nWhen dynamic allocation is enabled: It allows users to specify task and executor resource requirements at the stage level and will request the extra executors.\nOne thing to note that is YARN specific is that each ResourceProfile requires a different container priority on YARN. The mapping is simply the ResourceProfile id becomes the priority, on YARN lower numbers are higher priority. This means that profiles created earlier will have a higher priority in YARN. Normally this won’t matter as Spark finishes one stage before starting another one, the only case this might have an affect is in a job server type scenario, so its something to keep in mind.\nNote there is a difference in the way custom resources are handled between the base default profile and custom ResourceProfiles. To allow for the user to request YARN containers with extra resources without Spark scheduling on them, the user can specify resources via the\nspark.yarn.executor.resource.\nconfig. Those configs are only used in the base default profile though and do not get propagated into any other custom ResourceProfiles. This is because there would be no way to remove them if you wanted a stage to not have them. This results in your default profile getting custom resources defined in\nspark.yarn.executor.resource.\nplus spark defined resources of GPU or FPGA. Spark converts GPU and FPGA resources into the YARN built in types\nyarn.io/gpu\n) and\nyarn.io/fpga\n, but does not know the mapping of any other resources. Any other Spark custom resources are not propagated to YARN for the default profile. So if you want Spark to schedule based off a custom resource and have it requested from YARN, you must specify it in both YARN (\nspark.yarn.{driver/executor}.resource.\n) and Spark (\nspark.{driver/executor}.resource.\n) configs. Leave the Spark config off if you only want YARN containers with the extra resources but Spark not to schedule using them. Now for custom ResourceProfiles, it doesn’t currently have a way to only specify YARN resources without Spark scheduling off of them. This means for custom ResourceProfiles we propagate all the resources defined in the ResourceProfile to YARN. We still convert GPU and FPGA to the YARN build in types as well. This requires that the name of any custom resources you specify match what they are defined as in YARN.\nImportant notes\nWhether core requests are honored in scheduling decisions depends on which scheduler is in use and how it is configured.\nIn\ncluster\nmode, the local directories used by the Spark executors and the Spark driver will be the local directories configured for YARN (Hadoop YARN config\nyarn.nodemanager.local-dirs\n). If the user specifies\nspark.local.dir\n, it will be ignored. In\nclient\nmode, the Spark executors will use the local directories configured for YARN while the Spark driver will use those defined in\nspark.local.dir\n. This is because the Spark driver does not run on the YARN cluster in\nclient\nmode, only the Spark executors do.\nThe\n--files\nand\n--archives\noptions support specifying file names with the # similar to Hadoop. For example, you can specify:\n--files localtest.txt#appSees.txt\nand this will upload the file you have locally named\nlocaltest.txt\ninto HDFS but this will be linked to by the name\nappSees.txt\n, and your application should use the name as\nappSees.txt\nto reference it when running on YARN.\nThe\n--jars\noption allows the\nSparkContext.addJar\nfunction to work if you are using it with local files and running in\ncluster\nmode. It does not need to be used if you are using it with HDFS, HTTP, HTTPS, or FTP files.\nKerberos\nStandard Kerberos support in Spark is covered in the\nSecurity\npage.\nIn YARN mode, when accessing Hadoop file systems, aside from the default file system in the hadoop\nconfiguration, Spark will also automatically obtain delegation tokens for the service hosting the\nstaging directory of the Spark application.\nYARN-specific Kerberos Configuration\nProperty Name\nDefault\nMeaning\nSince Version\nspark.kerberos.keytab\n(none)\nThe full path to the file that contains the keytab for the principal specified above. This keytab\n  will be copied to the node running the YARN Application Master via the YARN Distributed Cache, and\n  will be used for renewing the login tickets and the delegation tokens periodically. Equivalent to\n  the\n--keytab\ncommand line argument.\n(Works also with the \"local\" master.)\n3.0.0\nspark.kerberos.principal\n(none)\nPrincipal to be used to login to KDC, while running on secure clusters. Equivalent to the\n--principal\ncommand line argument.\n(Works also with the \"local\" master.)\n3.0.0\nspark.yarn.kerberos.relogin.period\n1m\nHow often to check whether the kerberos TGT should be renewed. This should be set to a value\n  that is shorter than the TGT renewal period (or the TGT lifetime if TGT renewal is not enabled).\n  The default value should be enough for most deployments.\n2.3.0\nspark.yarn.kerberos.renewal.excludeHadoopFileSystems\n(none)\nA comma-separated list of Hadoop filesystems for whose hosts will be excluded from delegation\n    token renewal at resource scheduler. For example,\nspark.yarn.kerberos.renewal.excludeHadoopFileSystems=hdfs://nn1.com:8032,\n    hdfs://nn2.com:8032\n. This is known to work under YARN for now, so YARN Resource Manager won't renew tokens for the application.\n    Note that as resource scheduler does not renew token, so any application running longer than the original token expiration that tries\n    to use that token will likely fail.\n3.2.0\nTroubleshooting Kerberos\nDebugging Hadoop/Kerberos problems can be “difficult”. One useful technique is to\nenable extra logging of Kerberos operations in Hadoop by setting the\nHADOOP_JAAS_DEBUG\nenvironment variable.\nexport\nHADOOP_JAAS_DEBUG\n=\ntrue\nThe JDK classes can be configured to enable extra logging of their Kerberos and\nSPNEGO/REST authentication via the system properties\nsun.security.krb5.debug\nand\nsun.security.spnego.debug=true\n-Dsun.security.krb5.debug=true -Dsun.security.spnego.debug=true\nAll these options can be enabled in the Application Master:\nspark.yarn.appMasterEnv.HADOOP_JAAS_DEBUG true\nspark.yarn.am.extraJavaOptions -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug=true\nFinally, if the log level for\norg.apache.spark.deploy.yarn.Client\nis set to\nDEBUG\n, the log\nwill include a list of all tokens obtained, and their expiry details\nConfiguring the External Shuffle Service\nTo start the Spark Shuffle Service on each\nNodeManager\nin your YARN cluster, follow these\ninstructions:\nBuild Spark with the\nYARN profile\n. Skip this step if you are using a\npre-packaged distribution.\nLocate the\nspark-<version>-yarn-shuffle.jar\n. This should be under\n$SPARK_HOME/common/network-yarn/target/scala-<version>\nif you are building Spark yourself, and under\nyarn\nif you are using a distribution.\nAdd this jar to the classpath of all\nNodeManager\ns in your cluster.\nIn the\nyarn-site.xml\non each node, add\nspark_shuffle\nto\nyarn.nodemanager.aux-services\n,\nthen set\nyarn.nodemanager.aux-services.spark_shuffle.class\nto\norg.apache.spark.network.yarn.YarnShuffleService\n.\nIncrease\nNodeManager's\nheap size by setting\nYARN_HEAPSIZE\n(1000 by default) in\netc/hadoop/yarn-env.sh\nto avoid garbage collection issues during shuffle.\nRestart all\nNodeManager\ns in your cluster.\nThe following extra configuration options are available when the shuffle service is running on YARN:\nProperty Name\nDefault\nMeaning\nSince Version\nspark.yarn.shuffle.stopOnFailure\nfalse\nWhether to stop the NodeManager when there's a failure in the Spark Shuffle Service's\n    initialization. This prevents application failures caused by running containers on\n    NodeManagers where the Spark Shuffle Service is not running.\n2.1.0\nspark.yarn.shuffle.service.metrics.namespace\nsparkShuffleService\nThe namespace to use when emitting shuffle service metrics into Hadoop metrics2 system of the\n    NodeManager.\n3.2.0\nspark.yarn.shuffle.service.logs.namespace\n(not set)\nA namespace which will be appended to the class name when forming the logger name to use for\n    emitting logs from the YARN shuffle service, like\norg.apache.spark.network.yarn.YarnShuffleService.logsNamespaceValue\n. Since some logging frameworks\n    may expect the logger name to look like a class name, it's generally recommended to provide a value which\n    would be a valid Java package or class name and not include spaces.\n3.3.0\nspark.shuffle.service.db.backend\nROCKSDB\nWhen work-preserving restart is enabled in YARN, this is used to specify the disk-base store used\n    in shuffle service state store, supports `ROCKSDB` and `LEVELDB` (deprecated) with `ROCKSDB` as default value.\n    The original data store in `RocksDB/LevelDB` will not be automatically converted to another kind\n    of storage now. The original data store will be retained and the new type data store will be\n    created when switching storage types.\n3.4.0\nPlease note that the instructions above assume that the default shuffle service name,\nspark_shuffle\n, has been used. It is possible to use any name here, but the values used in the\nYARN NodeManager configurations must match the value of\nspark.shuffle.service.name\nin the\nSpark application.\nThe shuffle service will, by default, take all of its configurations from the Hadoop Configuration\nused by the NodeManager (e.g.\nyarn-site.xml\n). However, it is also possible to configure the\nshuffle service independently using a file named\nspark-shuffle-site.xml\nwhich should be placed\nonto the classpath of the shuffle service (which is, by default, shared with the classpath of the\nNodeManager). The shuffle service will treat this as a standard Hadoop Configuration resource and\noverlay it on top of the NodeManager’s configuration.\nLaunching your application with Apache Oozie\nApache Oozie can launch Spark applications as part of a workflow.\nIn a secure cluster, the launched application will need the relevant tokens to access the cluster’s\nservices. If Spark is launched with a keytab, this is automatic.\nHowever, if Spark is to be launched without a keytab, the responsibility for setting up security\nmust be handed over to Oozie.\nThe details of configuring Oozie for secure clusters and obtaining\ncredentials for a job can be found on the\nOozie web site\nin the “Authentication” section of the specific release’s documentation.\nFor Spark applications, the Oozie workflow must be set up for Oozie to request all tokens which\nthe application needs, including:\nThe YARN resource manager.\nThe local Hadoop filesystem.\nAny remote Hadoop filesystems used as a source or destination of I/O.\nHive —if used.\nHBase —if used.\nThe YARN timeline server, if the application interacts with this.\nTo avoid Spark attempting —and then failing— to obtain Hive, HBase and remote HDFS tokens,\nthe Spark configuration must be set to disable token collection for the services.\nThe Spark configuration must include the lines:\nspark.security.credentials.hive.enabled   false\nspark.security.credentials.hbase.enabled  false\nThe configuration option\nspark.kerberos.access.hadoopFileSystems\nmust be unset.\nUsing the Spark History Server to replace the Spark Web UI\nIt is possible to use the Spark History Server application page as the tracking URL for running\napplications when the application UI is disabled. This may be desirable on secure clusters, or to\nreduce the memory usage of the Spark driver. To set up tracking through the Spark History Server,\ndo the following:\nOn the application side, set\nspark.yarn.historyServer.allowTracking=true\nin Spark’s\nconfiguration. This will tell Spark to use the history server’s URL as the tracking URL if\nthe application’s UI is disabled.\nOn the Spark History Server, add\norg.apache.spark.deploy.yarn.YarnProxyRedirectFilter\nto the list of filters in the\nspark.ui.filters\nconfiguration.\nBe aware that the history server information may not be up-to-date with the application’s state.\nRunning multiple versions of the Spark Shuffle Service\nPlease note that this section only applies when running on YARN versions >= 2.9.0.\nIn some cases it may be desirable to run multiple instances of the Spark Shuffle Service which are\nusing different versions of Spark. This can be helpful, for example, when running a YARN cluster\nwith a mixed workload of applications running multiple Spark versions, since a given version of\nthe shuffle service is not always compatible with other versions of Spark. YARN versions since 2.9.0\nsupport the ability to run shuffle services within an isolated classloader\n(see\nYARN-4577\n), meaning multiple Spark versions\ncan coexist within a single NodeManager. The\nyarn.nodemanager.aux-services.<service-name>.classpath\nand, starting from YARN 2.10.2/3.1.1/3.2.0,\nyarn.nodemanager.aux-services.<service-name>.remote-classpath\noptions can be used to configure\nthis. Note that YARN 3.3.0/3.3.1 have an issue which requires setting\nyarn.nodemanager.aux-services.<service-name>.system-classes\nas a workaround. See\nYARN-11053\nfor details. In addition to setting\nup separate classpaths, it’s necessary to ensure the two versions advertise to different ports.\nThis can be achieved using the\nspark-shuffle-site.xml\nfile described above. For example, you may\nhave configuration like:\nyarn.nodemanager.aux-services\n=\nspark_shuffle_x,spark_shuffle_y\nyarn.nodemanager.aux-services.spark_shuffle_x.classpath\n=\n/path/to/spark-x-path/fat.jar:/path/to/spark-x-config\nyarn.nodemanager.aux-services.spark_shuffle_y.classpath\n=\n/path/to/spark-y-path/fat.jar:/path/to/spark-y-config\nOr\nyarn.nodemanager.aux-services\n=\nspark_shuffle_x,spark_shuffle_y\nyarn.nodemanager.aux-services.spark_shuffle_x.classpath\n=\n/path/to/spark-x-path/*:/path/to/spark-x-config\nyarn.nodemanager.aux-services.spark_shuffle_y.classpath\n=\n/path/to/spark-y-path/*:/path/to/spark-y-config\nThe two\nspark-*-config\ndirectories each contain one file,\nspark-shuffle-site.xml\n. These are XML\nfiles in the\nHadoop Configuration format\nwhich each contain a few configurations to adjust the port number and metrics name prefix used:\n<configuration>\n<property>\n<name>\nspark.shuffle.service.port\n</name>\n<value>\n7001\n</value>\n</property>\n<property>\n<name>\nspark.yarn.shuffle.service.metrics.namespace\n</name>\n<value>\nsparkShuffleServiceX\n</value>\n</property>\n</configuration>\nThe values should both be different for the two different services.\nThen, in the configuration of the Spark applications, one should be configured with:\nspark.shuffle.service.name\n=\nspark_shuffle_x\nspark.shuffle.service.port\n=\n7001\nand one should be configured with:\nspark.shuffle.service.name\n=\nspark_shuffle_y\nspark.shuffle.service.port\n=\n<other value>\nConfiguring different JDKs for Spark Applications\nIn some cases it may be desirable to use a different JDK from YARN node manager to run Spark applications,\nthis can be achieved by setting the\nJAVA_HOME\nenvironment variable for YARN containers and the\nspark-submit\nprocess.\nNote that, Spark assumes that all JVM processes runs in one application use the same version of JDK, otherwise,\nyou may encounter JDK serialization issues.\nTo configure a Spark application to use a JDK which has been pre-installed on all nodes at\n/opt/openjdk-17\n:\n$ export JAVA_HOME=/opt/openjdk-17\n$ ./bin/spark-submit --class path.to.your.Class \\\n    --master yarn \\\n    --conf spark.yarn.appMasterEnv.JAVA_HOME=/opt/openjdk-17 \\\n    --conf spark.executorEnv.JAVA_HOME=/opt/openjdk-17 \\\n    <app jar> [app options]\nOptionally, the user may want to avoid installing a different JDK on the YARN cluster nodes, in such a case,\nit’s also possible to distribute the JDK using YARN’s Distributed Cache. For example, to use Java 21 to run\na Spark application, prepare a JDK 21 tarball\nopenjdk-21.tar.gz\nand untar it to\n/opt\non the local node,\nthen submit a Spark application:\n$ export JAVA_HOME=/opt/openjdk-21\n$ ./bin/spark-submit --class path.to.your.Class \\\n    --master yarn \\\n    --archives path/to/openjdk-21.tar.gz \\\n    --conf spark.yarn.appMasterEnv.JAVA_HOME=./openjdk-21.tar.gz/openjdk-21 \\\n    --conf spark.executorEnv.JAVA_HOME=./openjdk-21.tar.gz/openjdk-21 \\\n    <app jar> [app options]"}
{"url": "https://spark.apache.org/docs/latest/spark-standalone.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark Standalone Mode\nSecurity\nInstalling Spark Standalone to a Cluster\nStarting a Cluster Manually\nCluster Launch Scripts\nResource Allocation and Configuration Overview\nConnecting an Application to the Cluster\nClient Properties\nLaunching Spark Applications\nSpark Protocol\nREST API\nResource Scheduling\nExecutors Scheduling\nStage Level Scheduling Overview\nCaveats\nMonitoring and Logging\nRunning Alongside Hadoop\nConfiguring Ports for Network Security\nHigh Availability\nStandby Masters with ZooKeeper\nSingle-Node Recovery with Local File System\nIn addition to running on the YARN cluster manager, Spark also provides a simple standalone deploy mode. You can launch a standalone cluster either manually, by starting a master and workers by hand, or use our provided\nlaunch scripts\n. It is also possible to run these daemons on a single machine for testing.\nSecurity\nSecurity features like authentication are not enabled by default. When deploying a cluster that is open to the internet\nor an untrusted network, it’s important to secure access to the cluster to prevent unauthorized applications\nfrom running on the cluster.\nPlease see\nSpark Security\nand the specific security sections in this doc before running Spark.\nInstalling Spark Standalone to a Cluster\nTo install Spark Standalone mode, you simply place a compiled version of Spark on each node on the cluster. You can obtain pre-built versions of Spark with each release or\nbuild it yourself\n.\nStarting a Cluster Manually\nYou can start a standalone master server by executing:\n./sbin/start-master.sh\nOnce started, the master will print out a\nspark://HOST:PORT\nURL for itself, which you can use to connect workers to it,\nor pass as the “master” argument to\nSparkContext\n. You can also find this URL on\nthe master’s web UI, which is\nhttp://localhost:8080\nby default.\nSimilarly, you can start one or more workers and connect them to the master via:\n./sbin/start-worker.sh <master-spark-URL>\nOnce you have started a worker, look at the master’s web UI (\nhttp://localhost:8080\nby default).\nYou should see the new node listed there, along with its number of CPUs and memory (minus one gigabyte left for the OS).\nFinally, the following configuration options can be passed to the master and worker:\nArgument\nMeaning\n-h HOST\n,\n--host HOST\nHostname to listen on\n-p PORT\n,\n--port PORT\nPort for service to listen on (default: 7077 for master, random for worker)\n--webui-port PORT\nPort for web UI (default: 8080 for master, 8081 for worker)\n-c CORES\n,\n--cores CORES\nTotal CPU cores to allow Spark applications to use on the machine (default: all available); only on worker\n-m MEM\n,\n--memory MEM\nTotal amount of memory to allow Spark applications to use on the machine, in a format like 1000M or 2G (default: your machine's total RAM minus 1 GiB); only on worker\n-d DIR\n,\n--work-dir DIR\nDirectory to use for scratch space and job output logs (default: SPARK_HOME/work); only on worker\n--properties-file FILE\nPath to a custom Spark properties file to load (default: conf/spark-defaults.conf)\nCluster Launch Scripts\nTo launch a Spark standalone cluster with the launch scripts, you should create a file called conf/workers in your Spark directory,\nwhich must contain the hostnames of all the machines where you intend to start Spark workers, one per line.\nIf conf/workers does not exist, the launch scripts defaults to a single machine (localhost), which is useful for testing.\nNote, the master machine accesses each of the worker machines via ssh. By default, ssh is run in parallel and requires password-less (using a private key) access to be setup.\nIf you do not have a password-less setup, you can set the environment variable SPARK_SSH_FOREGROUND and serially provide a password for each worker.\nOnce you’ve set up this file, you can launch or stop your cluster with the following shell scripts, based on Hadoop’s deploy scripts, and available in\nSPARK_HOME/sbin\n:\nsbin/start-master.sh\n- Starts a master instance on the machine the script is executed on.\nsbin/start-workers.sh\n- Starts a worker instance on each machine specified in the\nconf/workers\nfile.\nsbin/start-worker.sh\n- Starts a worker instance on the machine the script is executed on.\nsbin/start-connect-server.sh\n- Starts a Spark Connect server on the machine the script is executed on.\nsbin/start-all.sh\n- Starts both a master and a number of workers as described above.\nsbin/stop-master.sh\n- Stops the master that was started via the\nsbin/start-master.sh\nscript.\nsbin/stop-worker.sh\n- Stops all worker instances on the machine the script is executed on.\nsbin/stop-workers.sh\n- Stops all worker instances on the machines specified in the\nconf/workers\nfile.\nsbin/stop-connect-server.sh\n- Stops all Spark Connect server instances on the machine the script is executed on.\nsbin/stop-all.sh\n- Stops both the master and the workers as described above.\nNote that these scripts must be executed on the machine you want to run the Spark master on, not your local machine.\nYou can optionally configure the cluster further by setting environment variables in\nconf/spark-env.sh\n. Create this file by starting with the\nconf/spark-env.sh.template\n, and\ncopy it to all your worker machines\nfor the settings to take effect. The following settings are available:\nEnvironment Variable\nMeaning\nSPARK_MASTER_HOST\nBind the master to a specific hostname or IP address, for example a public one.\nSPARK_MASTER_PORT\nStart the master on a different port (default: 7077).\nSPARK_MASTER_WEBUI_PORT\nPort for the master web UI (default: 8080).\nSPARK_MASTER_OPTS\nConfiguration properties that apply only to the master in the form \"-Dx=y\" (default: none). See below for a list of possible options.\nSPARK_LOCAL_DIRS\nDirectory to use for \"scratch\" space in Spark, including map output files and RDDs that get\n    stored on disk. This should be on a fast, local disk in your system. It can also be a\n    comma-separated list of multiple directories on different disks.\nSPARK_LOG_DIR\nWhere log files are stored. (default: SPARK_HOME/logs).\nSPARK_LOG_MAX_FILES\nThe maximum number of log files (default: 5).\nSPARK_PID_DIR\nWhere pid files are stored. (default: /tmp).\nSPARK_WORKER_CORES\nTotal number of cores to allow Spark applications to use on the machine (default: all available cores).\nSPARK_WORKER_MEMORY\nTotal amount of memory to allow Spark applications to use on the machine, e.g.\n1000m\n,\n2g\n(default: total memory minus 1 GiB); note that each application's\nindividual\nmemory is configured using its\nspark.executor.memory\nproperty.\nSPARK_WORKER_PORT\nStart the Spark worker on a specific port (default: random).\nSPARK_WORKER_WEBUI_PORT\nPort for the worker web UI (default: 8081).\nSPARK_WORKER_DIR\nDirectory to run applications in, which will include both logs and scratch space (default: SPARK_HOME/work).\nSPARK_WORKER_OPTS\nConfiguration properties that apply only to the worker in the form \"-Dx=y\" (default: none). See below for a list of possible options.\nSPARK_DAEMON_MEMORY\nMemory to allocate to the Spark master and worker daemons themselves (default: 1g).\nSPARK_DAEMON_JAVA_OPTS\nJVM options for the Spark master and worker daemons themselves in the form \"-Dx=y\" (default: none).\nSPARK_DAEMON_CLASSPATH\nClasspath for the Spark master and worker daemons themselves (default: none).\nSPARK_PUBLIC_DNS\nThe public DNS name of the Spark master and workers (default: none).\nNote:\nThe launch scripts do not currently support Windows. To run a Spark cluster on Windows, start the master and workers by hand.\nSPARK_MASTER_OPTS supports the following system properties:\nProperty Name\nDefault\nMeaning\nSince Version\nspark.master.ui.port\n8080\nSpecifies the port number of the Master Web UI endpoint.\n1.1.0\nspark.master.ui.title\n(None)\nSpecifies the title of the Master UI page. If unset,\nSpark Master at 'master url'\nis used by default.\n4.0.0\nspark.master.ui.decommission.allow.mode\nLOCAL\nSpecifies the behavior of the Master Web UI's /workers/kill endpoint. Possible choices\n    are:\nLOCAL\nmeans allow this endpoint from IP's that are local to the machine running\n    the Master,\nDENY\nmeans to completely disable this endpoint,\nALLOW\nmeans to allow\n    calling this endpoint from any IP.\n3.1.0\nspark.master.ui.historyServerUrl\n(None)\nThe URL where Spark history server is running. Please note that this assumes\n    that all Spark jobs share the same event log location where the history server accesses.\n4.0.0\nspark.master.rest.enabled\nfalse\nWhether to use the Master REST API endpoint or not.\n1.3.0\nspark.master.rest.host\n(None)\nSpecifies the host of the Master REST API endpoint.\n4.0.0\nspark.master.rest.port\n6066\nSpecifies the port number of the Master REST API endpoint.\n1.3.0\nspark.master.rest.filters\n(None)\nComma separated list of filter class names to apply to the Master REST API.\n4.0.0\nspark.master.useAppNameAsAppId.enabled\nfalse\n(Experimental) If true, Spark master uses the user-provided appName for appId.\n4.0.0\nspark.deploy.retainedApplications\n200\nThe maximum number of completed applications to display. Older applications will be dropped from the UI to maintain this limit.\n0.8.0\nspark.deploy.retainedDrivers\n200\nThe maximum number of completed drivers to display. Older drivers will be dropped from the UI to maintain this limit.\n1.1.0\nspark.deploy.spreadOutDrivers\ntrue\nWhether the standalone cluster manager should spread drivers out across nodes or try\n    to consolidate them onto as few nodes as possible. Spreading out is usually better for\n    data locality in HDFS, but consolidating is more efficient for compute-intensive workloads.\n4.0.0\nspark.deploy.spreadOutApps\ntrue\nWhether the standalone cluster manager should spread applications out across nodes or try\n    to consolidate them onto as few nodes as possible. Spreading out is usually better for\n    data locality in HDFS, but consolidating is more efficient for compute-intensive workloads.\n0.6.1\nspark.deploy.defaultCores\nInt.MaxValue\nDefault number of cores to give to applications in Spark's standalone mode if they don't\n    set\nspark.cores.max\n. If not set, applications always get all available\n    cores unless they configure\nspark.cores.max\nthemselves.\n    Set this lower on a shared cluster to prevent users from grabbing\n    the whole cluster by default.\n0.9.0\nspark.deploy.maxExecutorRetries\n10\nLimit on the maximum number of back-to-back executor failures that can occur before the\n    standalone cluster manager removes a faulty application. An application will never be removed\n    if it has any running executors. If an application experiences more than\nspark.deploy.maxExecutorRetries\nfailures in a row, no executors\n    successfully start running in between those failures, and the application has no running\n    executors then the standalone cluster manager will remove the application and mark it as failed.\n    To disable this automatic removal, set\nspark.deploy.maxExecutorRetries\nto\n-1\n.\n1.6.3\nspark.deploy.maxDrivers\nInt.MaxValue\nThe maximum number of running drivers.\n4.0.0\nspark.deploy.appNumberModulo\n(None)\nThe modulo for app number. By default, the next of\napp-yyyyMMddHHmmss-9999\nis\napp-yyyyMMddHHmmss-10000\n. If we have 10000 as modulo, it will be\napp-yyyyMMddHHmmss-0000\n.\n    In most cases, the prefix\napp-yyyyMMddHHmmss\nis increased already during creating 10000 applications.\n4.0.0\nspark.deploy.driverIdPattern\ndriver-%s-%04d\nThe pattern for driver ID generation based on Java\nString.format\nmethod.\n    The default value is\ndriver-%s-%04d\nwhich represents the existing driver id string, e.g.,\ndriver-20231031224459-0019\n. Please be careful to generate unique IDs.\n4.0.0\nspark.deploy.appIdPattern\napp-%s-%04d\nThe pattern for app ID generation based on Java\nString.format\nmethod.\n    The default value is\napp-%s-%04d\nwhich represents the existing app id string, e.g.,\napp-20231031224509-0008\n. Please be careful to generate unique IDs.\n4.0.0\nspark.worker.timeout\n60\nNumber of seconds after which the standalone deploy master considers a worker lost if it\n    receives no heartbeats.\n0.6.2\nspark.dead.worker.persistence\n15\nNumber of iterations to keep the deae worker information in UI. By default, the dead worker is visible for (15 + 1) *\nspark.worker.timeout\nsince its last heartbeat.\n0.8.0\nspark.worker.resource.{name}.amount\n(none)\nAmount of a particular resource to use on the worker.\n3.0.0\nspark.worker.resource.{name}.discoveryScript\n(none)\nPath to resource discovery script, which is used to find a particular resource while worker starting up.\n    And the output of the script should be formatted like the\nResourceInformation\nclass.\n3.0.0\nspark.worker.resourcesFile\n(none)\nPath to resources file which is used to find various resources while worker starting up.\n    The content of resources file should be formatted like\n[{\"id\":{\"componentName\": \"spark.worker\", \"resourceName\":\"gpu\"}, \"addresses\":[\"0\",\"1\",\"2\"]}]\n.\n    If a particular resource is not found in the resources file, the discovery script would be used to\n    find that resource. If the discovery script also does not find the resources, the worker will fail\n    to start up.\n3.0.0\nSPARK_WORKER_OPTS supports the following system properties:\nProperty Name\nDefault\nMeaning\nSince Version\nspark.worker.initialRegistrationRetries\n6\nThe number of retries to reconnect in short intervals (between 5 and 15 seconds).\n4.0.0\nspark.worker.maxRegistrationRetries\n16\nThe max number of retries to reconnect.\n    After\nspark.worker.initialRegistrationRetries\nattempts, the interval is between\n    30 and 90 seconds.\n4.0.0\nspark.worker.cleanup.enabled\ntrue\nEnable periodic cleanup of worker / application directories.  Note that this only affects standalone\n    mode, as YARN works differently. Only the directories of stopped applications are cleaned up.\n    This should be enabled if\nspark.shuffle.service.db.enabled\nis \"true\"\n1.0.0\nspark.worker.cleanup.interval\n1800 (30 minutes)\nControls the interval, in seconds, at which the worker cleans up old application work dirs\n    on the local machine.\n1.0.0\nspark.worker.cleanup.appDataTtl\n604800 (7 days, 7 * 24 * 3600)\nThe number of seconds to retain application work directories on each worker.  This is a Time To Live\n    and should depend on the amount of available disk space you have.  Application logs and jars are\n    downloaded to each application work dir.  Over time, the work dirs can quickly fill up disk space,\n    especially if you run jobs very frequently.\n1.0.0\nspark.shuffle.service.db.enabled\ntrue\nStore External Shuffle service state on local disk so that when the external shuffle service is restarted, it will\n    automatically reload info on current executors.  This only affects standalone mode (yarn always has this behavior\n    enabled).  You should also enable\nspark.worker.cleanup.enabled\n, to ensure that the state\n    eventually gets cleaned up.  This config may be removed in the future.\n3.0.0\nspark.shuffle.service.db.backend\nROCKSDB\nWhen\nspark.shuffle.service.db.enabled\nis true, user can use this to specify the kind of disk-based\n    store used in shuffle service state store. This supports\nROCKSDB\nand\nLEVELDB\n(deprecated) now and\nROCKSDB\nas default value.\n    The original data store in\nRocksDB/LevelDB\nwill not be automatically convert to another kind of storage now.\n3.4.0\nspark.storage.cleanupFilesAfterExecutorExit\ntrue\nEnable cleanup non-shuffle files(such as temp. shuffle blocks, cached RDD/broadcast blocks,\n    spill files, etc) of worker directories following executor exits. Note that this doesn't\n    overlap with\nspark.worker.cleanup.enabled\n, as this enables cleanup of non-shuffle files in\n    local directories of a dead executor, while\nspark.worker.cleanup.enabled\nenables cleanup of\n    all files/subdirectories of a stopped and timeout application.\n    This only affects Standalone mode, support of other cluster managers can be added in the future.\n2.4.0\nspark.worker.ui.compressedLogFileLengthCacheSize\n100\nFor compressed log files, the uncompressed file can only be computed by uncompressing the files.\n    Spark caches the uncompressed file size of compressed log files. This property controls the cache\n    size.\n2.0.2\nspark.worker.idPattern\nworker-%s-%s-%d\nThe pattern for worker ID generation based on Java\nString.format\nmethod.\n    The default value is\nworker-%s-%s-%d\nwhich represents the existing worker id string, e.g.,\nworker-20231109183042-[fe80::1%lo0]-39729\n. Please be careful to generate unique IDs\n4.0.0\nResource Allocation and Configuration Overview\nPlease make sure to have read the Custom Resource Scheduling and Configuration Overview section on the\nconfiguration page\n. This section only talks about the Spark Standalone specific aspects of resource scheduling.\nSpark Standalone has 2 parts, the first is configuring the resources for the Worker, the second is the resource allocation for a specific application.\nThe user must configure the Workers to have a set of resources available so that it can assign them out to Executors. The\nspark.worker.resource.{resourceName}.amount\nis used to control the amount of each resource the worker has allocated. The user must also specify either\nspark.worker.resourcesFile\nor\nspark.worker.resource.{resourceName}.discoveryScript\nto specify how the Worker discovers the resources its assigned. See the descriptions above for each of those to see which method works best for your setup.\nThe second part is running an application on Spark Standalone. The only special case from the standard Spark resource configs is when you are running the Driver in client mode. For a Driver in client mode, the user can specify the resources it uses via\nspark.driver.resourcesFile\nor\nspark.driver.resource.{resourceName}.discoveryScript\n. If the Driver is running on the same host as other Drivers, please make sure the resources file or discovery script only returns resources that do not conflict with other Drivers running on the same node.\nNote, the user does not need to specify a discovery script when submitting an application as the Worker will start each Executor with the resources it allocates to it.\nConnecting an Application to the Cluster\nTo run an application on the Spark cluster, simply pass the\nspark://IP:PORT\nURL of the master as to the\nSparkContext\nconstructor\n.\nTo run an interactive Spark shell against the cluster, run the following command:\n./bin/spark-shell --master spark://IP:PORT\nYou can also pass an option\n--total-executor-cores <numCores>\nto control the number of cores that spark-shell uses on the cluster.\nClient Properties\nSpark applications supports the following configuration properties specific to standalone mode:\nProperty Name\nDefault Value\nMeaning\nSince Version\nspark.standalone.submit.waitAppCompletion\nfalse\nIn standalone cluster mode, controls whether the client waits to exit until the application completes.\n  If set to\ntrue\n, the client process will stay alive polling the driver's status.\n  Otherwise, the client process will exit after submission.\n3.1.0\nLaunching Spark Applications\nSpark Protocol\nThe\nspark-submit\nscript\nprovides the most straightforward way to\nsubmit a compiled Spark application to the cluster. For standalone clusters, Spark currently\nsupports two deploy modes. In\nclient\nmode, the driver is launched in the same process as the\nclient that submits the application. In\ncluster\nmode, however, the driver is launched from one\nof the Worker processes inside the cluster, and the client process exits as soon as it fulfills\nits responsibility of submitting the application without waiting for the application to finish.\nIf your application is launched through Spark submit, then the application jar is automatically\ndistributed to all worker nodes. For any additional jars that your application depends on, you\nshould specify them through the\n--jars\nflag using comma as a delimiter (e.g.\n--jars jar1,jar2\n).\nTo control the application’s configuration or execution environment, see\nSpark Configuration\n.\nAdditionally, standalone\ncluster\nmode supports restarting your application automatically if it\nexited with non-zero exit code. To use this feature, you may pass in the\n--supervise\nflag to\nspark-submit\nwhen launching your application. Then, if you wish to kill an application that is\nfailing repeatedly, you may do so through:\n./bin/spark-class org.apache.spark.deploy.Client kill <master url> <driver ID>\nYou can find the driver ID through the standalone Master web UI at\nhttp://<master url>:8080\n.\nREST API\nIf\nspark.master.rest.enabled\nis enabled, Spark master provides additional REST API\nvia\nhttp://[host:port]/[version]/submissions/[action]\nwhere\nhost\nis the master host, and\nport\nis the port number specified by\nspark.master.rest.port\n(default: 6066), and\nversion\nis a protocol version,\nv1\nas of today, and\naction\nis one of the following supported actions.\nCommand\nHTTP METHOD\nDescription\nSince Version\ncreate\nPOST\nCreate a Spark driver via\ncluster\nmode. Since 4.0.0, Spark master supports server-side\n      variable replacements for the values of Spark properties and environment variables.\n1.3.0\nkill\nPOST\nKill a single Spark driver.\n1.3.0\nkillall\nPOST\nKill all running Spark drivers.\n4.0.0\nstatus\nGET\nCheck the status of a Spark job.\n1.3.0\nclear\nPOST\nClear the completed drivers and applications.\n4.0.0\nThe following is a\ncurl\nCLI command example with the\npi.py\nand REST API.\n$\ncurl\n-XPOST\nhttp://IP:PORT/v1/submissions/create\n\\\n--header\n\"Content-Type:application/json;charset=UTF-8\"\n\\\n--data\n'{\n  \"appResource\": \"\",\n  \"sparkProperties\": {\n    \"spark.master\": \"spark://master:7077\",\n    \"spark.app.name\": \"Spark Pi\",\n    \"spark.driver.memory\": \"1g\",\n    \"spark.driver.cores\": \"1\",\n    \"spark.jars\": \"\"\n  },\n  \"clientSparkVersion\": \"\",\n  \"mainClass\": \"org.apache.spark.deploy.SparkSubmit\",\n  \"environmentVariables\": { },\n  \"action\": \"CreateSubmissionRequest\",\n  \"appArgs\": [ \"/opt/spark/examples/src/main/python/pi.py\", \"10\" ]\n}'\nThe following is the response from the REST API for the above\ncreate\nrequest.\n{\n\"action\"\n:\n\"CreateSubmissionResponse\"\n,\n\"message\"\n:\n\"Driver successfully submitted as driver-20231124153531-0000\"\n,\n\"serverSparkVersion\"\n:\n\"4.0.0\"\n,\n\"submissionId\"\n:\n\"driver-20231124153531-0000\"\n,\n\"success\"\n:\ntrue\n}\nWhen Spark master requires HTTP\nAuthorization\nheader via\nspark.master.rest.filters=org.apache.spark.ui.JWSFilter\nand\nspark.org.apache.spark.ui.JWSFilter.param.secretKey=BASE64URL-ENCODED-KEY\nconfigurations,\ncurl\nCLI command can provide the required header like the following.\n$\ncurl\n-XPOST\nhttp://IP:PORT/v1/submissions/create\n\\\n--header\n\"Authorization: Bearer USER-PROVIDED-WEB-TOEN-SIGNED-BY-THE-SAME-SHARED-KEY\"\n...\nFor\nsparkProperties\nand\nenvironmentVariables\n, users can use place\nholders for server-side environment variables like the following.\n...\n\"sparkProperties\"\n:\n{\n\"spark.hadoop.fs.s3a.endpoint\"\n:\n\"{{AWS_ENDPOINT_URL}}\"\n,\n\"spark.hadoop.fs.s3a.endpoint.region\"\n:\n\"{{AWS_REGION}}\"\n}\n,\n\"environmentVariables\"\n:\n{\n\"AWS_CA_BUNDLE\"\n:\n\"{{AWS_CA_BUNDLE}}\"\n}\n,\n...\nResource Scheduling\nThe standalone cluster mode currently only supports a simple FIFO scheduler across applications.\nHowever, to allow multiple concurrent users, you can control the maximum number of resources each\napplication will use.\nBy default, it will acquire\nall\ncores in the cluster, which only makes sense if you just run one\napplication at a time. You can cap the number of cores by setting\nspark.cores.max\nin your\nSparkConf\n. For example:\nval\nconf\n=\nnew\nSparkConf\n()\n.\nsetMaster\n(...)\n.\nsetAppName\n(...)\n.\nset\n(\n\"spark.cores.max\"\n,\n\"10\"\n)\nval\nsc\n=\nnew\nSparkContext\n(\nconf\n)\nIn addition, you can configure\nspark.deploy.defaultCores\non the cluster master process to change the\ndefault for applications that don’t set\nspark.cores.max\nto something less than infinite.\nDo this by adding the following to\nconf/spark-env.sh\n:\nexport\nSPARK_MASTER_OPTS\n=\n\"-Dspark.deploy.defaultCores=<value>\"\nThis is useful on shared clusters where users might not have configured a maximum number of cores\nindividually.\nExecutors Scheduling\nThe number of cores assigned to each executor is configurable. When\nspark.executor.cores\nis\nexplicitly set, multiple executors from the same application may be launched on the same worker\nif the worker has enough cores and memory. Otherwise, each executor grabs all the cores available\non the worker by default, in which case only one executor per application may be launched on each\nworker during one single schedule iteration.\nStage Level Scheduling Overview\nStage level scheduling is supported on Standalone:\nWhen dynamic allocation is disabled: It allows users to specify different task resource requirements at the stage level and will use the same executors requested at startup.\nWhen dynamic allocation is enabled: Currently, when the Master allocates executors for one application, it will schedule based on the order of the ResourceProfile ids for multiple ResourceProfiles. The ResourceProfile with smaller id will be scheduled firstly. Normally this won’t matter as Spark finishes one stage before starting another one, the only case this might have an affect is in a job server type scenario, so its something to keep in mind. For scheduling, we will only take executor memory and executor cores from built-in executor resources and all other custom resources from a ResourceProfile, other built-in executor resources such as offHeap and memoryOverhead won’t take any effect. The base default profile will be created based on the spark configs when you submit an application. Executor memory and executor cores from the base default profile can be propagated to custom ResourceProfiles, but all other custom resources can not be propagated.\nCaveats\nAs mentioned in\nDynamic Resource Allocation\n, if cores for each executor is not explicitly specified with dynamic allocation enabled, spark will possibly acquire much more executors than expected. So you are recommended to explicitly set executor cores for each resource profile when using stage level scheduling.\nMonitoring and Logging\nSpark’s standalone mode offers a web-based user interface to monitor the cluster. The master and each worker has its own web UI that shows cluster and job statistics. By default, you can access the web UI for the master at port 8080. The port can be changed either in the configuration file or via command-line options.\nIn addition, detailed log output for each job is also written to the work directory of each worker node (\nSPARK_HOME/work\nby default). You will see two files for each job,\nstdout\nand\nstderr\n, with all output it wrote to its console.\nRunning Alongside Hadoop\nYou can run Spark alongside your existing Hadoop cluster by just launching it as a separate service on the same machines. To access Hadoop data from Spark, just use an hdfs:// URL (typically\nhdfs://<namenode>:9000/path\n, but you can find the right URL on your Hadoop Namenode’s web UI). Alternatively, you can set up a separate cluster for Spark, and still have it access HDFS over the network; this will be slower than disk-local access, but may not be a concern if you are still running in the same local area network (e.g. you place a few Spark machines on each rack that you have Hadoop on).\nConfiguring Ports for Network Security\nGenerally speaking, a Spark cluster and its services are not deployed on the public internet.\nThey are generally private services, and should only be accessible within the network of the\norganization that deploys Spark. Access to the hosts and ports used by Spark services should\nbe limited to origin hosts that need to access the services.\nThis is particularly important for clusters using the standalone resource manager, as they do\nnot support fine-grained access control in a way that other resource managers do.\nFor a complete list of ports to configure, see the\nsecurity page\n.\nHigh Availability\nBy default, standalone scheduling clusters are resilient to Worker failures (insofar as Spark itself is resilient to losing work by moving it to other workers). However, the scheduler uses a Master to make scheduling decisions, and this (by default) creates a single point of failure: if the Master crashes, no new applications can be created. In order to circumvent this, we have two high availability schemes, detailed below.\nStandby Masters with ZooKeeper\nOverview\nUtilizing ZooKeeper to provide leader election and some state storage, you can launch multiple Masters in your cluster connected to the same ZooKeeper instance. One will be elected “leader” and the others will remain in standby mode. If the current leader dies, another Master will be elected, recover the old Master’s state, and then resume scheduling. The entire recovery process (from the time the first leader goes down) should take between 1 and 2 minutes. Note that this delay only affects scheduling\nnew\napplications – applications that were already running during Master failover are unaffected.\nLearn more about getting started with ZooKeeper\nhere\n.\nConfiguration\nIn order to enable this recovery mode, you can set\nSPARK_DAEMON_JAVA_OPTS\nin spark-env by configuring\nspark.deploy.recoveryMode\nand related\nspark.deploy.zookeeper.*\nconfigurations.\nPossible gotcha: If you have multiple Masters in your cluster but fail to correctly configure the Masters to use ZooKeeper, the Masters will fail to discover each other and think they’re all leaders. This will not lead to a healthy cluster state (as all Masters will schedule independently).\nDetails\nAfter you have a ZooKeeper cluster set up, enabling high availability is straightforward. Simply start multiple Master processes on different nodes with the same ZooKeeper configuration (ZooKeeper URL and directory). Masters can be added and removed at any time.\nIn order to schedule new applications or add Workers to the cluster, they need to know the IP address of the current leader. This can be accomplished by simply passing in a list of Masters where you used to pass in a single one. For example, you might start your SparkContext pointing to\nspark://host1:port1,host2:port2\n. This would cause your SparkContext to try registering with both Masters – if\nhost1\ngoes down, this configuration would still be correct as we’d find the new leader,\nhost2\n.\nThere’s an important distinction to be made between “registering with a Master” and normal operation. When starting up, an application or Worker needs to be able to find and register with the current lead Master. Once it successfully registers, though, it is “in the system” (i.e., stored in ZooKeeper). If failover occurs, the new leader will contact all previously registered applications and Workers to inform them of the change in leadership, so they need not even have known of the existence of the new Master at startup.\nDue to this property, new Masters can be created at any time, and the only thing you need to worry about is that\nnew\napplications and Workers can find it to register with in case it becomes the leader. Once registered, you’re taken care of.\nSingle-Node Recovery with Local File System\nOverview\nZooKeeper is the best way to go for production-level high availability, but if you just want to be able to restart the Master if it goes down, FILESYSTEM mode can take care of it. When applications and Workers register, they have enough state written to the provided directory so that they can be recovered upon a restart of the Master process.\nConfiguration\nIn order to enable this recovery mode, you can set SPARK_DAEMON_JAVA_OPTS in spark-env using this configuration:\nSystem property\nDefault Value\nMeaning\nSince Version\nspark.deploy.recoveryMode\nNONE\nThe recovery mode setting to recover submitted Spark jobs with cluster mode when it failed and relaunches. Set to\n      FILESYSTEM to enable file-system-based single-node recovery mode,\n      ROCKSDB to enable RocksDB-based single-node recovery mode,\n      ZOOKEEPER to use Zookeeper-based recovery mode, and\n      CUSTOM to provide a customer provider class via additional `spark.deploy.recoveryMode.factory` configuration.\n      NONE is the default value which disables this recovery mode.\n0.8.1\nspark.deploy.recoveryDirectory\n\"\"\nThe directory in which Spark will store recovery state, accessible from the Master's perspective.\n      Note that the directory should be clearly manually if\nspark.deploy.recoveryMode\nor\nspark.deploy.recoveryCompressionCodec\nis changed.\n0.8.1\nspark.deploy.recoveryCompressionCodec\n(none)\nA compression codec for persistence engines. none (default), lz4, lzf, snappy, and zstd. Currently, only FILESYSTEM mode supports this configuration.\n4.0.0\nspark.deploy.recoveryTimeout\n(none)\nThe timeout for recovery process. The default value is the same with\nspark.worker.timeout\n.\n4.0.0\nspark.deploy.recoveryMode.factory\n\"\"\nA class to implement\nStandaloneRecoveryModeFactory\ninterface\n1.2.0\nspark.deploy.zookeeper.url\nNone\nWhen\nspark.deploy.recoveryMode\nis set to ZOOKEEPER, this configuration is used to set the zookeeper URL to connect to.\n0.8.1\nspark.deploy.zookeeper.dir\nNone\nWhen\nspark.deploy.recoveryMode\nis set to ZOOKEEPER, this configuration is used to set the zookeeper directory to store recovery state.\n0.8.1\nDetails\nThis solution can be used in tandem with a process monitor/manager like\nmonit\n, or just to enable manual recovery via restart.\nWhile filesystem recovery seems straightforwardly better than not doing any recovery at all, this mode may be suboptimal for certain development or experimental purposes. In particular, killing a master via stop-master.sh does not clean up its recovery state, so whenever you start a new Master, it will enter recovery mode. This could increase the startup time by up to 1 minute if it needs to wait for all previously-registered Workers/clients to timeout.\nWhile it’s not officially supported, you could mount an NFS directory as the recovery directory. If the original Master node dies completely, you could then start a Master on a different node, which would correctly recover all previously registered Workers/applications (equivalent to ZooKeeper recovery). Future applications will have to be able to find the new Master, however, in order to register."}
{"url": "https://spark.apache.org/docs/latest/sql-performance-tuning.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nPerformance Tuning\nCaching Data\nTuning Partitions\nLeveraging Statistics\nOptimizing the Join Strategy\nAdaptive Query Execution\nStorage Partition Join\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nPerformance Tuning\nSpark offers many techniques for tuning the performance of DataFrame or SQL workloads. Those techniques, broadly speaking, include caching data, altering how datasets are partitioned, selecting the optimal join strategy, and providing the optimizer with additional information it can use to build more efficient execution plans.\nCaching Data\nTuning Partitions\nCoalesce Hints\nLeveraging Statistics\nOptimizing the Join Strategy\nAutomatically Broadcasting Joins\nJoin Strategy Hints\nAdaptive Query Execution\nCoalescing Post Shuffle Partitions\nSplitting skewed shuffle partitions\nConverting sort-merge join to broadcast join\nConverting sort-merge join to shuffled hash join\nOptimizing Skew Join\nAdvanced Customization\nStorage Partition Join\nCaching Data\nSpark SQL can cache tables using an in-memory columnar format by calling\nspark.catalog.cacheTable(\"tableName\")\nor\ndataFrame.cache()\n.\nThen Spark SQL will scan only required columns and will automatically tune compression to minimize\nmemory usage and GC pressure. You can call\nspark.catalog.uncacheTable(\"tableName\")\nor\ndataFrame.unpersist()\nto remove the table from memory.\nConfiguration of in-memory caching can be done via\nspark.conf.set\nor by running\nSET key=value\ncommands using SQL.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.inMemoryColumnarStorage.compressed\ntrue\nWhen set to true, Spark SQL will automatically select a compression codec for each column based\n    on statistics of the data.\n1.0.1\nspark.sql.inMemoryColumnarStorage.batchSize\n10000\nControls the size of batches for columnar caching. Larger batch sizes can improve memory utilization\n    and compression, but risk OOMs when caching data.\n1.1.1\nTuning Partitions\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.files.maxPartitionBytes\n134217728 (128 MB)\nThe maximum number of bytes to pack into a single partition when reading files.\n      This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.\n2.0.0\nspark.sql.files.openCostInBytes\n4194304 (4 MB)\nThe estimated cost to open a file, measured by the number of bytes that could be scanned in the same\n      time. This is used when putting multiple files into a partition. It is better to over-estimate,\n      then the partitions with small files will be faster than partitions with bigger files (which is\n      scheduled first). This configuration is effective only when using file-based sources such as Parquet,\n      JSON and ORC.\n2.0.0\nspark.sql.files.minPartitionNum\nDefault Parallelism\nThe suggested (not guaranteed) minimum number of split file partitions. If not set, the default\n      value is `spark.sql.leafNodeDefaultParallelism`. This configuration is effective only when using file-based\n      sources such as Parquet, JSON and ORC.\n3.1.0\nspark.sql.files.maxPartitionNum\nNone\nThe suggested (not guaranteed) maximum number of split file partitions. If it is set,\n      Spark will rescale each partition to make the number of partitions is close to this\n      value if the initial number of partitions exceeds this value. This configuration is\n      effective only when using file-based sources such as Parquet, JSON and ORC.\n3.5.0\nspark.sql.shuffle.partitions\n200\nConfigures the number of partitions to use when shuffling data for joins or aggregations.\n1.1.0\nspark.sql.sources.parallelPartitionDiscovery.threshold\n32\nConfigures the threshold to enable parallel listing for job input paths. If the number of\n      input paths is larger than this threshold, Spark will list the files by using Spark distributed job.\n      Otherwise, it will fallback to sequential listing. This configuration is only effective when\n      using file-based data sources such as Parquet, ORC and JSON.\n1.5.0\nspark.sql.sources.parallelPartitionDiscovery.parallelism\n10000\nConfigures the maximum listing parallelism for job input paths. In case the number of input\n      paths is larger than this value, it will be throttled down to use this value. This configuration is only effective when using file-based data sources such as Parquet, ORC\n      and JSON.\n2.1.1\nCoalesce Hints\nCoalesce hints allow Spark SQL users to control the number of output files just like\ncoalesce\n,\nrepartition\nand\nrepartitionByRange\nin the Dataset API, they can be used for performance\ntuning and reducing the number of output files. The “COALESCE” hint only has a partition number as a\nparameter. The “REPARTITION” hint has a partition number, columns, or both/neither of them as parameters.\nThe “REPARTITION_BY_RANGE” hint must have column names and a partition number is optional. The “REBALANCE”\nhint has an initial partition number, columns, or both/neither of them as parameters.\nSELECT\n/*+ COALESCE(3) */\n*\nFROM\nt\n;\nSELECT\n/*+ REPARTITION(3) */\n*\nFROM\nt\n;\nSELECT\n/*+ REPARTITION(c) */\n*\nFROM\nt\n;\nSELECT\n/*+ REPARTITION(3, c) */\n*\nFROM\nt\n;\nSELECT\n/*+ REPARTITION */\n*\nFROM\nt\n;\nSELECT\n/*+ REPARTITION_BY_RANGE(c) */\n*\nFROM\nt\n;\nSELECT\n/*+ REPARTITION_BY_RANGE(3, c) */\n*\nFROM\nt\n;\nSELECT\n/*+ REBALANCE */\n*\nFROM\nt\n;\nSELECT\n/*+ REBALANCE(3) */\n*\nFROM\nt\n;\nSELECT\n/*+ REBALANCE(c) */\n*\nFROM\nt\n;\nSELECT\n/*+ REBALANCE(3, c) */\n*\nFROM\nt\n;\nFor more details please refer to the documentation of\nPartitioning Hints\n.\nLeveraging Statistics\nApache Spark’s ability to choose the best execution plan among many possible options is determined in part by its estimates of how many rows will be output by every node in the execution plan (read, filter, join, etc.). Those estimates in turn are based on statistics that are made available to Spark in one of several ways:\nData source\n: Statistics that Spark reads directly from the underlying data source, like the counts and min/max values in the metadata of Parquet files. These statistics are maintained by the underlying data source.\nCatalog\n: Statistics that Spark reads from the catalog, like the Hive Metastore. These statistics are collected or updated whenever you run\nANALYZE TABLE\n.\nRuntime\n: Statistics that Spark computes itself as a query is running. This is part of the\nadaptive query execution framework\n.\nMissing or inaccurate statistics will hinder Spark’s ability to select an optimal plan, and may lead to poor query performance. It’s helpful then to inspect the statistics available to Spark and the estimates it makes during query planning and execution.\nData object statistics\n: You can inspect the statistics on a table or column with\nDESCRIBE EXTENDED\n.\nQuery plan estimates\n: You can inspect Spark’s cost estimates in the optimized query plan via\nEXPLAIN COST\nor\nDataFrame.explain(mode=\"cost\")\n.\nRuntime statistics\n: You can inspect these statistics in the\nSQL UI\nunder the “Details” section as a query is running. Look for\nStatistics(..., isRuntime=true)\nin the plan.\nOptimizing the Join Strategy\nAutomatically Broadcasting Joins\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.autoBroadcastJoinThreshold\n10485760 (10 MB)\nConfigures the maximum size in bytes for a table that will be broadcast to all worker nodes when\n      performing a join. By setting this value to -1, broadcasting can be disabled.\n1.1.0\nspark.sql.broadcastTimeout\n300\nTimeout in seconds for the broadcast wait time in broadcast joins\n1.3.0\nJoin Strategy Hints\nThe join strategy hints, namely\nBROADCAST\n,\nMERGE\n,\nSHUFFLE_HASH\nand\nSHUFFLE_REPLICATE_NL\n,\ninstruct Spark to use the hinted strategy on each specified relation when joining them with another\nrelation. For example, when the\nBROADCAST\nhint is used on table ‘t1’, broadcast join (either\nbroadcast hash join or broadcast nested loop join depending on whether there is any equi-join key)\nwith ‘t1’ as the build side will be prioritized by Spark even if the size of table ‘t1’ suggested\nby the statistics is above the configuration\nspark.sql.autoBroadcastJoinThreshold\n.\nWhen different join strategy hints are specified on both sides of a join, Spark prioritizes the\nBROADCAST\nhint over the\nMERGE\nhint over the\nSHUFFLE_HASH\nhint over the\nSHUFFLE_REPLICATE_NL\nhint. When both sides are specified with the\nBROADCAST\nhint or the\nSHUFFLE_HASH\nhint, Spark will\npick the build side based on the join type and the sizes of the relations.\nNote that there is no guarantee that Spark will choose the join strategy specified in the hint since\na specific strategy may not support all join types.\nspark\n.\ntable\n(\n\"\nsrc\n\"\n).\njoin\n(\nspark\n.\ntable\n(\n\"\nrecords\n\"\n).\nhint\n(\n\"\nbroadcast\n\"\n),\n\"\nkey\n\"\n).\nshow\n()\nspark\n.\ntable\n(\n\"src\"\n).\njoin\n(\nspark\n.\ntable\n(\n\"records\"\n).\nhint\n(\n\"broadcast\"\n),\n\"key\"\n).\nshow\n()\nspark\n.\ntable\n(\n\"src\"\n).\njoin\n(\nspark\n.\ntable\n(\n\"records\"\n).\nhint\n(\n\"broadcast\"\n),\n\"key\"\n).\nshow\n();\nsrc\n<-\nsql\n(\n\"SELECT * FROM src\"\n)\nrecords\n<-\nsql\n(\n\"SELECT * FROM records\"\n)\nhead\n(\njoin\n(\nsrc\n,\nhint\n(\nrecords\n,\n\"broadcast\"\n),\nsrc\n$\nkey\n==\nrecords\n$\nkey\n))\n-- We accept BROADCAST, BROADCASTJOIN and MAPJOIN for broadcast hint\nSELECT\n/*+ BROADCAST(r) */\n*\nFROM\nrecords\nr\nJOIN\nsrc\ns\nON\nr\n.\nkey\n=\ns\n.\nkey\nFor more details please refer to the documentation of\nJoin Hints\n.\nAdaptive Query Execution\nAdaptive Query Execution (AQE) is an optimization technique in Spark SQL that makes use of the runtime statistics to choose the most efficient query execution plan, which is enabled by default since Apache Spark 3.2.0. Spark SQL can turn on and off AQE by\nspark.sql.adaptive.enabled\nas an umbrella configuration.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.adaptive.enabled\ntrue\nWhen true, enable adaptive query execution, which re-optimizes the query plan in the middle of query execution, based on accurate runtime statistics.\n1.6.0\nCoalescing Post Shuffle Partitions\nThis feature coalesces the post shuffle partitions based on the map output statistics when both\nspark.sql.adaptive.enabled\nand\nspark.sql.adaptive.coalescePartitions.enabled\nconfigurations are true. This feature simplifies the tuning of shuffle partition number when running queries. You do not need to set a proper shuffle partition number to fit your dataset. Spark can pick the proper shuffle partition number at runtime once you set a large enough initial number of shuffle partitions via\nspark.sql.adaptive.coalescePartitions.initialPartitionNum\nconfiguration.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.adaptive.coalescePartitions.enabled\ntrue\nWhen true and\nspark.sql.adaptive.enabled\nis true, Spark will coalesce contiguous shuffle partitions according to the target size (specified by\nspark.sql.adaptive.advisoryPartitionSizeInBytes\n), to avoid too many small tasks.\n3.0.0\nspark.sql.adaptive.coalescePartitions.parallelismFirst\ntrue\nWhen true, Spark ignores the target size specified by\nspark.sql.adaptive.advisoryPartitionSizeInBytes\n(default 64MB) when coalescing contiguous shuffle partitions, and only respect the minimum partition size specified by\nspark.sql.adaptive.coalescePartitions.minPartitionSize\n(default 1MB), to maximize the parallelism. This is to avoid performance regressions when enabling adaptive query execution. It's recommended to set this config to false on a busy cluster to make resource utilization more efficient (not many small tasks).\n3.2.0\nspark.sql.adaptive.coalescePartitions.minPartitionSize\n1MB\nThe minimum size of shuffle partitions after coalescing. This is useful when the target size is ignored during partition coalescing, which is the default case.\n3.2.0\nspark.sql.adaptive.coalescePartitions.initialPartitionNum\n(none)\nThe initial number of shuffle partitions before coalescing. If not set, it equals to\nspark.sql.shuffle.partitions\n. This configuration only has an effect when\nspark.sql.adaptive.enabled\nand\nspark.sql.adaptive.coalescePartitions.enabled\nare both enabled.\n3.0.0\nspark.sql.adaptive.advisoryPartitionSizeInBytes\n64 MB\nThe advisory size in bytes of the shuffle partition during adaptive optimization (when\nspark.sql.adaptive.enabled\nis true). It takes effect when Spark coalesces small shuffle partitions or splits skewed shuffle partition.\n3.0.0\nSplitting skewed shuffle partitions\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled\ntrue\nWhen true and\nspark.sql.adaptive.enabled\nis true, Spark will optimize the skewed shuffle partitions in RebalancePartitions and split them to smaller ones according to the target size (specified by\nspark.sql.adaptive.advisoryPartitionSizeInBytes\n), to avoid data skew.\n3.2.0\nspark.sql.adaptive.rebalancePartitionsSmallPartitionFactor\n0.2\nA partition will be merged during splitting if its size is small than this factor multiply\nspark.sql.adaptive.advisoryPartitionSizeInBytes\n.\n3.3.0\nConverting sort-merge join to broadcast join\nAQE converts sort-merge join to broadcast hash join when the runtime statistics of any join side are smaller than the adaptive broadcast hash join threshold. This is not as efficient as planning a broadcast hash join in the first place, but it’s better than continuing the sort-merge join, as we can avoid sorting both join sides and read shuffle files locally to save network traffic (provided\nspark.sql.adaptive.localShuffleReader.enabled\nis true).\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.adaptive.autoBroadcastJoinThreshold\n(none)\nConfigures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1, broadcasting can be disabled. The default value is the same as\nspark.sql.autoBroadcastJoinThreshold\n. Note that, this config is used only in adaptive framework.\n3.2.0\nspark.sql.adaptive.localShuffleReader.enabled\ntrue\nWhen true and\nspark.sql.adaptive.enabled\nis true, Spark tries to use local shuffle reader to read the shuffle data when the shuffle partitioning is not needed, for example, after converting sort-merge join to broadcast-hash join.\n3.0.0\nConverting sort-merge join to shuffled hash join\nAQE converts sort-merge join to shuffled hash join when all post shuffle partitions are smaller than the threshold configured in\nspark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold\n.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold\n0\nConfigures the maximum size in bytes per partition that can be allowed to build local hash map. If this value is not smaller than\nspark.sql.adaptive.advisoryPartitionSizeInBytes\nand all the partition sizes are not larger than this config, join selection prefers to use shuffled hash join instead of sort merge join regardless of the value of\nspark.sql.join.preferSortMergeJoin\n.\n3.2.0\nOptimizing Skew Join\nData skew can severely downgrade the performance of join queries. This feature dynamically handles skew in sort-merge join by splitting (and replicating if needed) skewed tasks into roughly evenly sized tasks. It takes effect when both\nspark.sql.adaptive.enabled\nand\nspark.sql.adaptive.skewJoin.enabled\nconfigurations are enabled.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.adaptive.skewJoin.enabled\ntrue\nWhen true and\nspark.sql.adaptive.enabled\nis true, Spark dynamically handles skew in sort-merge join by splitting (and replicating if needed) skewed partitions.\n3.0.0\nspark.sql.adaptive.skewJoin.skewedPartitionFactor\n5.0\nA partition is considered as skewed if its size is larger than this factor multiplying the median partition size and also larger than\nspark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\n.\n3.0.0\nspark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\n256MB\nA partition is considered as skewed if its size in bytes is larger than this threshold and also larger than\nspark.sql.adaptive.skewJoin.skewedPartitionFactor\nmultiplying the median partition size. Ideally, this config should be set larger than\nspark.sql.adaptive.advisoryPartitionSizeInBytes\n.\n3.0.0\nspark.sql.adaptive.forceOptimizeSkewedJoin\nfalse\nWhen true, force enable OptimizeSkewedJoin, which is an adaptive rule to optimize skewed joins to avoid straggler tasks, even if it introduces extra shuffle.\n3.3.0\nAdvanced Customization\nYou can control the details of how AQE works by providing your own cost evaluator class or by excluding AQE optimizer rules.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.adaptive.optimizer.excludedRules\n(none)\nConfigures a list of rules to be disabled in the adaptive optimizer, in which the rules are specified by their rule names and separated by comma. The optimizer will log the rules that have indeed been excluded.\n3.1.0\nspark.sql.adaptive.customCostEvaluatorClass\n(none)\nThe custom cost evaluator class to be used for adaptive execution. If not being set, Spark will use its own\nSimpleCostEvaluator\nby default.\n3.2.0\nStorage Partition Join\nStorage Partition Join (SPJ) is an optimization technique in Spark SQL that makes use the existing storage layout to avoid the shuffle phase.\nThis is a generalization of the concept of Bucket Joins, which is only applicable for\nbucketed\ntables, to tables partitioned by functions registered in FunctionCatalog. Storage Partition Joins are currently supported for compatible V2 DataSources.\nThe following SQL properties enable Storage Partition Join in different join queries with various optimizations.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.sources.v2.bucketing.enabled\nfalse\nWhen true, try to eliminate shuffle by using the partitioning reported by a compatible V2 data source.\n3.3.0\nspark.sql.sources.v2.bucketing.pushPartValues.enabled\ntrue\nWhen enabled, try to eliminate shuffle if one side of the join has missing partition values from the other side. This config requires\nspark.sql.sources.v2.bucketing.enabled\nto be true.\n3.4.0\nspark.sql.requireAllClusterKeysForCoPartition\ntrue\nWhen true, require the join or MERGE keys to be same and in the same order as the partition keys to eliminate shuffle. Hence, set to\nfalse\nin this situation to eliminate shuffle.\n3.4.0\nspark.sql.sources.v2.bucketing.partiallyClusteredDistribution.enabled\nfalse\nWhen true, and when the join is not a full outer join, enable skew optimizations to handle partitions with large amounts of data when avoiding shuffle. One side will be chosen as the big table based on table statistics, and the splits on this side will be partially-clustered. The splits of the other side will be grouped and replicated to match. This config requires both\nspark.sql.sources.v2.bucketing.enabled\nand\nspark.sql.sources.v2.bucketing.pushPartValues.enabled\nto be true.\n3.4.0\nspark.sql.sources.v2.bucketing.allowJoinKeysSubsetOfPartitionKeys.enabled\nfalse\nWhen enabled, try to avoid shuffle if join or MERGE condition does not include all partition columns. This config requires both\nspark.sql.sources.v2.bucketing.enabled\nand\nspark.sql.sources.v2.bucketing.pushPartValues.enabled\nto be true, and\nspark.sql.requireAllClusterKeysForCoPartition\nto be false.\n4.0.0\nspark.sql.sources.v2.bucketing.allowCompatibleTransforms.enabled\nfalse\nWhen enabled, try to avoid shuffle if partition transforms are compatible but not identical. This config requires both\nspark.sql.sources.v2.bucketing.enabled\nand\nspark.sql.sources.v2.bucketing.pushPartValues.enabled\nto be true.\n4.0.0\nspark.sql.sources.v2.bucketing.shuffle.enabled\nfalse\nWhen enabled, try to avoid shuffle on one side of the join, by recognizing the partitioning reported by a V2 data source on the other side.\n4.0.0\nIf Storage Partition Join is performed, the query plan will not contain Exchange nodes prior to the join.\nThe following example uses Iceberg (\nhttps://iceberg.apache.org/docs/latest/spark-getting-started/\n), a Spark V2 DataSource that supports Storage Partition Join.\nCREATE\nTABLE\nprod\n.\ndb\n.\ntarget\n(\nid\nINT\n,\nsalary\nINT\n,\ndep\nSTRING\n)\nUSING\niceberg\nPARTITIONED\nBY\n(\ndep\n,\nbucket\n(\n8\n,\nid\n))\nCREATE\nTABLE\nprod\n.\ndb\n.\nsource\n(\nid\nINT\n,\nsalary\nINT\n,\ndep\nSTRING\n)\nUSING\niceberg\nPARTITIONED\nBY\n(\ndep\n,\nbucket\n(\n8\n,\nid\n))\nEXPLAIN\nSELECT\n*\nFROM\ntarget\nt\nINNER\nJOIN\nsource\ns\nON\nt\n.\ndep\n=\ns\n.\ndep\nAND\nt\n.\nid\n=\ns\n.\nid\n-- Plan without Storage Partition Join\n==\nPhysical\nPlan\n==\n*\nProject\n(\n12\n)\n+-\n*\nSortMergeJoin\nInner\n(\n11\n)\n:\n-\n*\nSort\n(\n5\n)\n:\n+-\nExchange\n(\n4\n)\n//\nDATA\nSHUFFLE\n:\n+-\n*\nFilter\n(\n3\n)\n:\n+-\n*\nColumnarToRow\n(\n2\n)\n:\n+-\nBatchScan\n(\n1\n)\n+-\n*\nSort\n(\n10\n)\n+-\nExchange\n(\n9\n)\n//\nDATA\nSHUFFLE\n+-\n*\nFilter\n(\n8\n)\n+-\n*\nColumnarToRow\n(\n7\n)\n+-\nBatchScan\n(\n6\n)\nSET\n'spark.sql.sources.v2.bucketing.enabled'\n'true'\nSET\n'spark.sql.iceberg.planning.preserve-data-grouping'\n'true'\nSET\n'spark.sql.sources.v2.bucketing.pushPartValues.enabled'\n'true'\nSET\n'spark.sql.requireAllClusterKeysForCoPartition'\n'false'\nSET\n'spark.sql.sources.v2.bucketing.partiallyClusteredDistribution.enabled'\n'true'\n-- Plan with Storage Partition Join\n==\nPhysical\nPlan\n==\n*\nProject\n(\n10\n)\n+-\n*\nSortMergeJoin\nInner\n(\n9\n)\n:\n-\n*\nSort\n(\n4\n)\n:\n+-\n*\nFilter\n(\n3\n)\n:\n+-\n*\nColumnarToRow\n(\n2\n)\n:\n+-\nBatchScan\n(\n1\n)\n+-\n*\nSort\n(\n8\n)\n+-\n*\nFilter\n(\n7\n)\n+-\n*\nColumnarToRow\n(\n6\n)\n+-\nBatchScan\n(\n5\n)"}
{"url": "https://spark.apache.org/docs/latest/monitoring.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMonitoring and Instrumentation\nWeb Interfaces\nViewing After the Fact\nEnvironment Variables\nApplying compaction on rolling event log files\nSpark History Server Configuration Options\nREST API\nExecutor Task Metrics\nExecutor Metrics\nAPI Versioning Policy\nMetrics\nList of available metrics providers\nComponent instance = Driver\nComponent instance = Executor\nSource = JVM Source\nComponent instance = applicationMaster\nComponent instance = master\nComponent instance = ApplicationSource\nComponent instance = worker\nComponent instance = shuffleService\nAdvanced Instrumentation\nThere are several ways to monitor Spark applications: web UIs, metrics, and external instrumentation.\nWeb Interfaces\nEvery SparkContext launches a\nWeb UI\n, by default on port 4040, that\ndisplays useful information about the application. This includes:\nA list of scheduler stages and tasks\nA summary of RDD sizes and memory usage\nEnvironmental information.\nInformation about the running executors\nYou can access this interface by simply opening\nhttp://<driver-node>:4040\nin a web browser.\nIf multiple SparkContexts are running on the same host, they will bind to successive ports\nbeginning with 4040 (4041, 4042, etc).\nNote that this information is only available for the duration of the application by default.\nTo view the web UI after the fact, set\nspark.eventLog.enabled\nto true before starting the\napplication. This configures Spark to log Spark events that encode the information displayed\nin the UI to persisted storage.\nViewing After the Fact\nIt is still possible to construct the UI of an application through Spark’s history server,\nprovided that the application’s event logs exist.\nYou can start the history server by executing:\n./sbin/start-history-server.sh\nThis creates a web interface at\nhttp://<server-url>:18080\nby default, listing incomplete\nand completed applications and attempts.\nWhen using the file-system provider class (see\nspark.history.provider\nbelow), the base logging\ndirectory must be supplied in the\nspark.history.fs.logDirectory\nconfiguration option,\nand should contain sub-directories that each represents an application’s event logs.\nThe spark jobs themselves must be configured to log events, and to log them to the same shared,\nwritable directory. For example, if the server was configured with a log directory of\nhdfs://namenode/shared/spark-logs\n, then the client-side options would be:\nspark.eventLog.enabled true\nspark.eventLog.dir hdfs://namenode/shared/spark-logs\nThe history server can be configured as follows:\nEnvironment Variables\nEnvironment Variable\nMeaning\nSPARK_DAEMON_MEMORY\nMemory to allocate to the history server (default: 1g).\nSPARK_DAEMON_JAVA_OPTS\nJVM options for the history server (default: none).\nSPARK_DAEMON_CLASSPATH\nClasspath for the history server (default: none).\nSPARK_PUBLIC_DNS\nThe public address for the history server. If this is not set, links to application history\n      may use the internal address of the server, resulting in broken links (default: none).\nSPARK_HISTORY_OPTS\nspark.history.*\nconfiguration options for the history server (default: none).\nApplying compaction on rolling event log files\nA long-running application (e.g. streaming) can bring a huge single event log file which may cost a lot to maintain and\nalso requires a bunch of resource to replay per each update in Spark History Server.\nEnabling\nspark.eventLog.rolling.enabled\nand\nspark.eventLog.rolling.maxFileSize\nwould\nlet you have rolling event log files instead of single huge event log file which may help some scenarios on its own,\nbut it still doesn’t help you reducing the overall size of logs.\nSpark History Server can apply compaction on the rolling event log files to reduce the overall size of\nlogs, via setting the configuration\nspark.history.fs.eventLog.rolling.maxFilesToRetain\non the\nSpark History Server.\nDetails will be described below, but please note in prior that compaction is LOSSY operation.\nCompaction will discard some events which will be no longer seen on UI - you may want to check which events will be discarded\nbefore enabling the option.\nWhen the compaction happens, the History Server lists all the available event log files for the application, and considers\nthe event log files having less index than the file with smallest index which will be retained as target of compaction.\nFor example, if the application A has 5 event log files and\nspark.history.fs.eventLog.rolling.maxFilesToRetain\nis set to 2, then first 3 log files will be selected to be compacted.\nOnce it selects the target, it analyzes them to figure out which events can be excluded, and rewrites them\ninto one compact file with discarding events which are decided to exclude.\nThe compaction tries to exclude the events which point to the outdated data. As of now, below describes the candidates of events to be excluded:\nEvents for the job which is finished, and related stage/tasks events\nEvents for the executor which is terminated\nEvents for the SQL execution which is finished, and related job/stage/tasks events\nOnce rewriting is done, original log files will be deleted, via best-effort manner. The History Server may not be able to delete\nthe original log files, but it will not affect the operation of the History Server.\nPlease note that Spark History Server may not compact the old event log files if figures out not a lot of space\nwould be reduced during compaction. For streaming query we normally expect compaction\nwill run as each micro-batch will trigger one or more jobs which will be finished shortly, but compaction won’t run\nin many cases for batch query.\nPlease also note that this is a new feature introduced in Spark 3.0, and may not be completely stable. Under some circumstances,\nthe compaction may exclude more events than you expect, leading some UI issues on History Server for the application.\nUse it with caution.\nSpark History Server Configuration Options\nSecurity options for the Spark History Server are covered more detail in the\nSecurity\npage.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.history.provider\norg.apache.spark.deploy.history.FsHistoryProvider\nName of the class implementing the application history backend. Currently there is only\n    one implementation, provided by Spark, which looks for application logs stored in the\n    file system.\n1.1.0\nspark.history.fs.logDirectory\nfile:/tmp/spark-events\nFor the filesystem history provider, the URL to the directory containing application event\n    logs to load. This can be a local\nfile://\npath,\n    an HDFS path\nhdfs://namenode/shared/spark-logs\nor that of an alternative filesystem supported by the Hadoop APIs.\n1.1.0\nspark.history.fs.update.interval\n10s\nThe period at which the filesystem history provider checks for new or\n      updated logs in the log directory. A shorter interval detects new applications faster,\n      at the expense of more server load re-reading updated applications.\n      As soon as an update has completed, listings of the completed and incomplete applications\n      will reflect the changes.\n1.4.0\nspark.history.retainedApplications\n50\nThe number of applications to retain UI data for in the cache. If this cap is exceeded, then\n      the oldest applications will be removed from the cache. If an application is not in the cache,\n      it will have to be loaded from disk if it is accessed from the UI.\n1.0.0\nspark.history.ui.maxApplications\nInt.MaxValue\nThe number of applications to display on the history summary page. Application UIs are still\n      available by accessing their URLs directly even if they are not displayed on the history summary page.\n2.0.1\nspark.history.ui.port\n18080\nThe port to which the web interface of the history server binds.\n1.0.0\nspark.history.kerberos.enabled\nfalse\nIndicates whether the history server should use kerberos to login. This is required\n      if the history server is accessing HDFS files on a secure Hadoop cluster.\n1.0.1\nspark.history.kerberos.principal\n(none)\nWhen\nspark.history.kerberos.enabled=true\n, specifies kerberos principal name for the History Server.\n1.0.1\nspark.history.kerberos.keytab\n(none)\nWhen\nspark.history.kerberos.enabled=true\n, specifies location of the kerberos keytab file for the History Server.\n1.0.1\nspark.history.fs.cleaner.enabled\nfalse\nSpecifies whether the History Server should periodically clean up event logs from storage.\n1.4.0\nspark.history.fs.cleaner.interval\n1d\nWhen\nspark.history.fs.cleaner.enabled=true\n, specifies how often the filesystem job history cleaner checks for files to delete.\n      Files are deleted if at least one of two conditions holds.\n      First, they're deleted if they're older than\nspark.history.fs.cleaner.maxAge\n.\n      They are also deleted if the number of files is more than\nspark.history.fs.cleaner.maxNum\n, Spark tries to clean up the completed attempts\n      from the applications based on the order of their oldest attempt time.\n1.4.0\nspark.history.fs.cleaner.maxAge\n7d\nWhen\nspark.history.fs.cleaner.enabled=true\n, job history files older than this will be deleted when the filesystem history cleaner runs.\n1.4.0\nspark.history.fs.cleaner.maxNum\nInt.MaxValue\nWhen\nspark.history.fs.cleaner.enabled=true\n, specifies the maximum number of files in the event log directory.\n      Spark tries to clean up the completed attempt logs to maintain the log directory under this limit.\n      This should be smaller than the underlying file system limit like\n      `dfs.namenode.fs-limits.max-directory-items` in HDFS.\n3.0.0\nspark.history.fs.endEventReparseChunkSize\n1m\nHow many bytes to parse at the end of log files looking for the end event.\n      This is used to speed up generation of application listings by skipping unnecessary\n      parts of event log files. It can be disabled by setting this config to 0.\n2.4.0\nspark.history.fs.inProgressOptimization.enabled\ntrue\nEnable optimized handling of in-progress logs. This option may leave finished\n      applications that fail to rename their event logs listed as in-progress.\n2.4.0\nspark.history.fs.driverlog.cleaner.enabled\nspark.history.fs.cleaner.enabled\nSpecifies whether the History Server should periodically clean up driver logs from storage.\n3.0.0\nspark.history.fs.driverlog.cleaner.interval\nspark.history.fs.cleaner.interval\nWhen\nspark.history.fs.driverlog.cleaner.enabled=true\n, specifies how often the filesystem driver log cleaner checks for files to delete.\n      Files are only deleted if they are older than\nspark.history.fs.driverlog.cleaner.maxAge\n3.0.0\nspark.history.fs.driverlog.cleaner.maxAge\nspark.history.fs.cleaner.maxAge\nWhen\nspark.history.fs.driverlog.cleaner.enabled=true\n, driver log files older than this will be deleted when the driver log cleaner runs.\n3.0.0\nspark.history.fs.numReplayThreads\n25% of available cores\nNumber of threads that will be used by history server to process event logs.\n2.0.0\nspark.history.store.maxDiskUsage\n10g\nMaximum disk usage for the local directory where the cache application history information\n      are stored.\n2.3.0\nspark.history.store.path\n(none)\nLocal directory where to cache application history data. If set, the history\n        server will store application data on disk instead of keeping it in memory. The data\n        written to disk will be re-used in the event of a history server restart.\n2.3.0\nspark.history.store.serializer\nJSON\nSerializer for writing/reading in-memory UI objects to/from disk-based KV Store; JSON or PROTOBUF.\n        JSON serializer is the only choice before Spark 3.4.0, thus it is the default value.\n        PROTOBUF serializer is fast and compact, compared to the JSON serializer.\n3.4.0\nspark.history.custom.executor.log.url\n(none)\nSpecifies custom spark executor log URL for supporting external log service instead of using cluster\n        managers' application log URLs in the history server. Spark will support some path variables via patterns\n        which can vary on cluster manager. Please check the documentation for your cluster manager to\n        see which patterns are supported, if any. This configuration has no effect on a live application, it only\n        affects the history server.\nFor now, only YARN mode supports this configuration\n3.0.0\nspark.history.custom.executor.log.url.applyIncompleteApplication\ntrue\nSpecifies whether to apply custom spark executor log URL to incomplete applications as well.\n        If executor logs for running applications should be provided as origin log URLs, set this to `false`.\n        Please note that incomplete applications may include applications which didn't shutdown gracefully.\n        Even this is set to `true`, this configuration has no effect on a live application, it only affects the history server.\n3.0.0\nspark.history.fs.eventLog.rolling.maxFilesToRetain\nInt.MaxValue\nThe maximum number of event log files which will be retained as non-compacted. By default,\n      all event log files will be retained. The lowest value is 1 for technical reason.\nPlease read the section of \"Applying compaction of old event log files\" for more details.\n3.0.0\nspark.history.store.hybridStore.enabled\nfalse\nWhether to use HybridStore as the store when parsing event logs. HybridStore will first write data\n      to an in-memory store and having a background thread that dumps data to a disk store after the writing\n      to in-memory store is completed.\n3.1.0\nspark.history.store.hybridStore.maxMemoryUsage\n2g\nMaximum memory space that can be used to create HybridStore. The HybridStore co-uses the heap memory,\n      so the heap memory should be increased through the memory option for SHS if the HybridStore is enabled.\n3.1.0\nspark.history.store.hybridStore.diskBackend\nROCKSDB\nSpecifies a disk-based store used in hybrid store; ROCKSDB or LEVELDB (deprecated).\n3.3.0\nspark.history.fs.update.batchSize\nInt.MaxValue\nSpecifies the batch size for updating new eventlog files.\n      This controls each scan process to be completed within a reasonable time, and such\n      prevent the initial scan from running too long and blocking new eventlog files to\n      be scanned in time in large environments.\n3.4.0\nNote that in all of these UIs, the tables are sortable by clicking their headers,\nmaking it easy to identify slow tasks, data skew, etc.\nNote\nThe history server displays both completed and incomplete Spark jobs. If an application makes\nmultiple attempts after failures, the failed attempts will be displayed, as well as any ongoing\nincomplete attempt or the final successful attempt.\nIncomplete applications are only updated intermittently. The time between updates is defined\nby the interval between checks for changed files (\nspark.history.fs.update.interval\n).\nOn larger clusters, the update interval may be set to large values.\nThe way to view a running application is actually to view its own web UI.\nApplications which exited without registering themselves as completed will be listed\nas incomplete —even though they are no longer running. This can happen if an application\ncrashes.\nOne way to signal the completion of a Spark job is to stop the Spark Context\nexplicitly (\nsc.stop()\n), or in Python using the\nwith SparkContext() as sc:\nconstruct\nto handle the Spark Context setup and tear down.\nREST API\nIn addition to viewing the metrics in the UI, they are also available as JSON.  This gives developers\nan easy way to create new visualizations and monitoring tools for Spark.  The JSON is available for\nboth running applications, and in the history server.  The endpoints are mounted at\n/api/v1\n.  For example,\nfor the history server, they would typically be accessible at\nhttp://<server-url>:18080/api/v1\n, and\nfor a running application, at\nhttp://localhost:4040/api/v1\n.\nIn the API, an application is referenced by its application ID,\n[app-id]\n.\nWhen running on YARN, each application may have multiple attempts, but there are attempt IDs\nonly for applications in cluster mode, not applications in client mode. Applications in YARN cluster mode\ncan be identified by their\n[attempt-id]\n. In the API listed below, when running in YARN cluster mode,\n[app-id]\nwill actually be\n[base-app-id]/[attempt-id]\n, where\n[base-app-id]\nis the YARN application ID.\nEndpoint\nMeaning\n/applications\nA list of all applications.\n?status=[completed|running]\nlist only applications in the chosen state.\n?minDate=[date]\nearliest start date/time to list.\n?maxDate=[date]\nlatest start date/time to list.\n?minEndDate=[date]\nearliest end date/time to list.\n?maxEndDate=[date]\nlatest end date/time to list.\n?limit=[limit]\nlimits the number of applications listed.\nExamples:\n?minDate=2015-02-10\n?minDate=2015-02-03T16:42:40.000GMT\n?maxDate=2015-02-11T20:41:30.000GMT\n?minEndDate=2015-02-12\n?minEndDate=2015-02-12T09:15:10.000GMT\n?maxEndDate=2015-02-14T16:30:45.000GMT\n?limit=10\n/applications/[app-id]/jobs\nA list of all jobs for a given application.\n?status=[running|succeeded|failed|unknown]\nlist only jobs in the specific state.\n/applications/[app-id]/jobs/[job-id]\nDetails for the given job.\n/applications/[app-id]/stages\nA list of all stages for a given application.\n?status=[active|complete|pending|failed]\nlist only stages in the given state.\n?details=true\nlists all stages with the task data.\n?taskStatus=[RUNNING|SUCCESS|FAILED|KILLED|PENDING]\nlists only those tasks with the specified task status. Query parameter taskStatus takes effect only when\ndetails=true\n. This also supports multiple\ntaskStatus\nsuch as\n?details=true&taskStatus=SUCCESS&taskStatus=FAILED\nwhich will return all tasks matching any of specified task status.\n?withSummaries=true\nlists stages with task metrics distribution and executor metrics distribution.\n?quantiles=0.0,0.25,0.5,0.75,1.0\nsummarize the metrics with the given quantiles. Query parameter quantiles takes effect only when\nwithSummaries=true\n. Default value is\n0.0,0.25,0.5,0.75,1.0\n.\n/applications/[app-id]/stages/[stage-id]\nA list of all attempts for the given stage.\n?details=true\nlists all attempts with the task data for the given stage.\n?taskStatus=[RUNNING|SUCCESS|FAILED|KILLED|PENDING]\nlists only those tasks with the specified task status. Query parameter taskStatus takes effect only when\ndetails=true\n. This also supports multiple\ntaskStatus\nsuch as\n?details=true&taskStatus=SUCCESS&taskStatus=FAILED\nwhich will return all tasks matching any of specified task status.\n?withSummaries=true\nlists task metrics distribution and executor metrics distribution of each attempt.\n?quantiles=0.0,0.25,0.5,0.75,1.0\nsummarize the metrics with the given quantiles. Query parameter quantiles takes effect only when\nwithSummaries=true\n. Default value is\n0.0,0.25,0.5,0.75,1.0\n.\nExample:\n?details=true\n?details=true&taskStatus=RUNNING\n?withSummaries=true\n?details=true&withSummaries=true&quantiles=0.01,0.5,0.99\n/applications/[app-id]/stages/[stage-id]/[stage-attempt-id]\nDetails for the given stage attempt.\n?details=true\nlists all task data for the given stage attempt.\n?taskStatus=[RUNNING|SUCCESS|FAILED|KILLED|PENDING]\nlists only those tasks with the specified task status. Query parameter taskStatus takes effect only when\ndetails=true\n. This also supports multiple\ntaskStatus\nsuch as\n?details=true&taskStatus=SUCCESS&taskStatus=FAILED\nwhich will return all tasks matching any of specified task status.\n?withSummaries=true\nlists task metrics distribution and executor metrics distribution for the given stage attempt.\n?quantiles=0.0,0.25,0.5,0.75,1.0\nsummarize the metrics with the given quantiles. Query parameter quantiles takes effect only when\nwithSummaries=true\n. Default value is\n0.0,0.25,0.5,0.75,1.0\n.\nExample:\n?details=true\n?details=true&taskStatus=RUNNING\n?withSummaries=true\n?details=true&withSummaries=true&quantiles=0.01,0.5,0.99\n/applications/[app-id]/stages/[stage-id]/[stage-attempt-id]/taskSummary\nSummary metrics of all tasks in the given stage attempt.\n?quantiles\nsummarize the metrics with the given quantiles.\nExample:\n?quantiles=0.01,0.5,0.99\n/applications/[app-id]/stages/[stage-id]/[stage-attempt-id]/taskList\nA list of all tasks for the given stage attempt.\n?offset=[offset]&length=[len]\nlist tasks in the given range.\n?sortBy=[runtime|-runtime]\nsort the tasks.\n?status=[running|success|killed|failed|unknown]\nlist only tasks in the state.\nExample:\n?offset=10&length=50&sortBy=runtime&status=running\n/applications/[app-id]/executors\nA list of all active executors for the given application.\n/applications/[app-id]/executors/[executor-id]/threads\nStack traces of all the threads running within the given active executor.\n      Not available via the history server.\n/applications/[app-id]/allexecutors\nA list of all(active and dead) executors for the given application.\n/applications/[app-id]/storage/rdd\nA list of stored RDDs for the given application.\n/applications/[app-id]/storage/rdd/[rdd-id]\nDetails for the storage status of a given RDD.\n/applications/[base-app-id]/logs\nDownload the event logs for all attempts of the given application as files within\n    a zip file.\n/applications/[base-app-id]/[attempt-id]/logs\nDownload the event logs for a specific application attempt as a zip file.\n/applications/[app-id]/streaming/statistics\nStatistics for the streaming context.\n/applications/[app-id]/streaming/receivers\nA list of all streaming receivers.\n/applications/[app-id]/streaming/receivers/[stream-id]\nDetails of the given receiver.\n/applications/[app-id]/streaming/batches\nA list of all retained batches.\n/applications/[app-id]/streaming/batches/[batch-id]\nDetails of the given batch.\n/applications/[app-id]/streaming/batches/[batch-id]/operations\nA list of all output operations of the given batch.\n/applications/[app-id]/streaming/batches/[batch-id]/operations/[outputOp-id]\nDetails of the given operation and given batch.\n/applications/[app-id]/sql\nA list of all queries for a given application.\n?details=[true (default) | false]\nlists/hides details of Spark plan nodes.\n?planDescription=[true (default) | false]\nenables/disables Physical\nplanDescription\non demand when Physical Plan size is high.\n?offset=[offset]&length=[len]\nlists queries in the given range.\n/applications/[app-id]/sql/[execution-id]\nDetails for the given query.\n?details=[true (default) | false]\nlists/hides metric details in addition to given query details.\n?planDescription=[true (default) | false]\nenables/disables Physical\nplanDescription\non demand for the given query when Physical Plan size is high.\n/applications/[app-id]/environment\nEnvironment details of the given application.\n/version\nGet the current spark version.\nThe number of jobs and stages which can be retrieved is constrained by the same retention\nmechanism of the standalone Spark UI;\n\"spark.ui.retainedJobs\"\ndefines the threshold\nvalue triggering garbage collection on jobs, and\nspark.ui.retainedStages\nthat for stages.\nNote that the garbage collection takes place on playback: it is possible to retrieve\nmore entries by increasing these values and restarting the history server.\nExecutor Task Metrics\nThe REST API exposes the values of the Task Metrics collected by Spark executors with the granularity\nof task execution. The metrics can be used for performance troubleshooting and workload characterization.\nA list of the available metrics, with a short description:\nSpark Executor Task Metric name\nShort description\nexecutorRunTime\nElapsed time the executor spent running this task. This includes time fetching shuffle data.\n    The value is expressed in milliseconds.\nexecutorCpuTime\nCPU time the executor spent running this task. This includes time fetching shuffle data.\n    The value is expressed in nanoseconds.\nexecutorDeserializeTime\nElapsed time spent to deserialize this task. The value is expressed in milliseconds.\nexecutorDeserializeCpuTime\nCPU time taken on the executor to deserialize this task. The value is expressed\n    in nanoseconds.\nresultSize\nThe number of bytes this task transmitted back to the driver as the TaskResult.\njvmGCTime\nElapsed time the JVM spent in garbage collection while executing this task.\n    The value is expressed in milliseconds.\nConcurrentGCCount\nThis metric returns the total number of collections that have occurred.\n        It only applies when the Java Garbage collector is G1 Concurrent GC.\nConcurrentGCTime\nThis metric returns the approximate accumulated collection elapsed time in milliseconds.\n        It only applies when the Java Garbage collector is G1 Concurrent GC.\nresultSerializationTime\nElapsed time spent serializing the task result. The value is expressed in milliseconds.\nmemoryBytesSpilled\nThe number of in-memory bytes spilled by this task.\ndiskBytesSpilled\nThe number of on-disk bytes spilled by this task.\npeakExecutionMemory\nPeak memory used by internal data structures created during shuffles, aggregations and\n        joins. The value of this accumulator should be approximately the sum of the peak sizes\n        across all such data structures created in this task. For SQL jobs, this only tracks all\n         unsafe operators and ExternalSort.\ninputMetrics.*\nMetrics related to reading data from\norg.apache.spark.rdd.HadoopRDD\nor from persisted data.\n.bytesRead\nTotal number of bytes read.\n.recordsRead\nTotal number of records read.\noutputMetrics.*\nMetrics related to writing data externally (e.g. to a distributed filesystem),\n    defined only in tasks with output.\n.bytesWritten\nTotal number of bytes written\n.recordsWritten\nTotal number of records written\nshuffleReadMetrics.*\nMetrics related to shuffle read operations.\n.recordsRead\nNumber of records read in shuffle operations\n.remoteBlocksFetched\nNumber of remote blocks fetched in shuffle operations\n.localBlocksFetched\nNumber of local (as opposed to read from a remote executor) blocks fetched\n    in shuffle operations\n.totalBlocksFetched\nNumber of blocks fetched in shuffle operations (both local and remote)\n.remoteBytesRead\nNumber of remote bytes read in shuffle operations\n.localBytesRead\nNumber of bytes read in shuffle operations from local disk (as opposed to\n    read from a remote executor)\n.totalBytesRead\nNumber of bytes read in shuffle operations (both local and remote)\n.remoteBytesReadToDisk\nNumber of remote bytes read to disk in shuffle operations.\n    Large blocks are fetched to disk in shuffle read operations, as opposed to\n    being read into memory, which is the default behavior.\n.fetchWaitTime\nTime the task spent waiting for remote shuffle blocks.\n        This only includes the time blocking on shuffle input data.\n        For instance if block B is being fetched while the task is still not finished\n        processing block A, it is not considered to be blocking on block B.\n        The value is expressed in milliseconds.\nshuffleWriteMetrics.*\nMetrics related to operations writing shuffle data.\n.bytesWritten\nNumber of bytes written in shuffle operations\n.recordsWritten\nNumber of records written in shuffle operations\n.writeTime\nTime spent blocking on writes to disk or buffer cache. The value is expressed\n     in nanoseconds.\nExecutor Metrics\nExecutor-level metrics are sent from each executor to the driver as part of the Heartbeat to describe the performance metrics of Executor itself like JVM heap memory, GC information.\nExecutor metric values and their measured memory peak values per executor are exposed via the REST API in JSON format and in Prometheus format.\nThe JSON end point is exposed at:\n/applications/[app-id]/executors\n, and the Prometheus endpoint at:\n/metrics/executors/prometheus\n.\nIn addition, aggregated per-stage peak values of the executor memory metrics are written to the event log if\nspark.eventLog.logStageExecutorMetrics\nis true.\nExecutor memory metrics are also exposed via the Spark metrics system based on the\nDropwizard metrics library\n.\nA list of the available metrics, with a short description:\nExecutor Level Metric name\nShort description\nrddBlocks\nRDD blocks in the block manager of this executor.\nmemoryUsed\nStorage memory used by this executor.\ndiskUsed\nDisk space used for RDD storage by this executor.\ntotalCores\nNumber of cores available in this executor.\nmaxTasks\nMaximum number of tasks that can run concurrently in this executor.\nactiveTasks\nNumber of tasks currently executing.\nfailedTasks\nNumber of tasks that have failed in this executor.\ncompletedTasks\nNumber of tasks that have completed in this executor.\ntotalTasks\nTotal number of tasks (running, failed and completed) in this executor.\ntotalDuration\nElapsed time the JVM spent executing tasks in this executor.\n    The value is expressed in milliseconds.\ntotalGCTime\nElapsed time the JVM spent in garbage collection summed in this executor.\n    The value is expressed in milliseconds.\ntotalInputBytes\nTotal input bytes summed in this executor.\ntotalShuffleRead\nTotal shuffle read bytes summed in this executor.\ntotalShuffleWrite\nTotal shuffle write bytes summed in this executor.\nmaxMemory\nTotal amount of memory available for storage, in bytes.\nmemoryMetrics.*\nCurrent value of memory metrics:\n.usedOnHeapStorageMemory\nUsed on heap memory currently for storage, in bytes.\n.usedOffHeapStorageMemory\nUsed off heap memory currently for storage, in bytes.\n.totalOnHeapStorageMemory\nTotal available on heap memory for storage, in bytes. This amount can vary over time,  on the MemoryManager implementation.\n.totalOffHeapStorageMemory\nTotal available off heap memory for storage, in bytes. This amount can vary over time, depending on the MemoryManager implementation.\npeakMemoryMetrics.*\nPeak value of memory (and GC) metrics:\n.JVMHeapMemory\nPeak memory usage of the heap that is used for object allocation.\n    The heap consists of one or more memory pools. The used and committed size of the returned memory usage is the sum of those values of all heap memory pools whereas the init and max size of the returned memory usage represents the setting of the heap memory which may not be the sum of those of all heap memory pools.\n    The amount of used memory in the returned memory usage is the amount of memory occupied by both live objects and garbage objects that have not been collected, if any.\n.JVMOffHeapMemory\nPeak memory usage of non-heap memory that is used by the Java virtual machine. The non-heap memory consists of one or more memory pools. The used and committed size of the returned memory usage is the sum of those values of all non-heap memory pools whereas the init and max size of the returned memory usage represents the setting of the non-heap memory which may not be the sum of those of all non-heap memory pools.\n.OnHeapExecutionMemory\nPeak on heap execution memory in use, in bytes.\n.OffHeapExecutionMemory\nPeak off heap execution memory in use, in bytes.\n.OnHeapStorageMemory\nPeak on heap storage memory in use, in bytes.\n.OffHeapStorageMemory\nPeak off heap storage memory in use, in bytes.\n.OnHeapUnifiedMemory\nPeak on heap memory (execution and storage).\n.OffHeapUnifiedMemory\nPeak off heap memory (execution and storage).\n.DirectPoolMemory\nPeak memory that the JVM is using for direct buffer pool (\njava.lang.management.BufferPoolMXBean\n)\n.MappedPoolMemory\nPeak memory that the JVM is using for mapped buffer pool (\njava.lang.management.BufferPoolMXBean\n)\n.ProcessTreeJVMVMemory\nVirtual memory size in bytes. Enabled if spark.executor.processTreeMetrics.enabled is true.\n.ProcessTreeJVMRSSMemory\nResident Set Size: number of pages the process has\n      in real memory.  This is just the pages which count\n      toward text, data, or stack space.  This does not\n      include pages which have not been demand-loaded in,\n      or which are swapped out. Enabled if spark.executor.processTreeMetrics.enabled is true.\n.ProcessTreePythonVMemory\nVirtual memory size for Python in bytes. Enabled if spark.executor.processTreeMetrics.enabled is true.\n.ProcessTreePythonRSSMemory\nResident Set Size for Python. Enabled if spark.executor.processTreeMetrics.enabled is true.\n.ProcessTreeOtherVMemory\nVirtual memory size for other kind of process in bytes. Enabled if spark.executor.processTreeMetrics.enabled is true.\n.ProcessTreeOtherRSSMemory\nResident Set Size for other kind of process. Enabled if spark.executor.processTreeMetrics.enabled is true.\n.MinorGCCount\nTotal minor GC count. For example, the garbage collector is one of     Copy, PS Scavenge, ParNew, G1 Young Generation and so on.\n.MinorGCTime\nElapsed total minor GC time.\n    The value is expressed in milliseconds.\n.MajorGCCount\nTotal major GC count. For example, the garbage collector is one of     MarkSweepCompact, PS MarkSweep, ConcurrentMarkSweep, G1 Old Generation and so on.\n.MajorGCTime\nElapsed total major GC time.\n    The value is expressed in milliseconds.\nThe computation of RSS and Vmem are based on\nproc(5)\nAPI Versioning Policy\nThese endpoints have been strongly versioned to make it easier to develop applications on top.\n In particular, Spark guarantees:\nEndpoints will never be removed from one version\nIndividual fields will never be removed for any given endpoint\nNew endpoints may be added\nNew fields may be added to existing endpoints\nNew versions of the api may be added in the future as a separate endpoint (e.g.,\napi/v2\n).  New versions are\nnot\nrequired to be backwards compatible.\nApi versions may be dropped, but only after at least one minor release of co-existing with a new api version.\nNote that even when examining the UI of running applications, the\napplications/[app-id]\nportion is\nstill required, though there is only one application available.  E.g. to see the list of jobs for the\nrunning app, you would go to\nhttp://localhost:4040/api/v1/applications/[app-id]/jobs\n.  This is to\nkeep the paths consistent in both modes.\nMetrics\nSpark has a configurable metrics system based on the\nDropwizard Metrics Library\n.\nThis allows users to report Spark metrics to a variety of sinks including HTTP, JMX, and CSV\nfiles. The metrics are generated by sources embedded in the Spark code base. They\nprovide instrumentation for specific activities and Spark components.\nThe metrics system is configured via a configuration file that Spark expects to be present\nat\n$SPARK_HOME/conf/metrics.properties\n. A custom file location can be specified via the\nspark.metrics.conf\nconfiguration property\n.\nInstead of using the configuration file, a set of configuration parameters with prefix\nspark.metrics.conf.\ncan be used.\nBy default, the root namespace used for driver or executor metrics is\nthe value of\nspark.app.id\n. However, often times, users want to be able to track the metrics\nacross apps for driver and executors, which is hard to do with application ID\n(i.e.\nspark.app.id\n) since it changes with every invocation of the app. For such use cases,\na custom namespace can be specified for metrics reporting using\nspark.metrics.namespace\nconfiguration property.\nIf, say, users wanted to set the metrics namespace to the name of the application, they\ncan set the\nspark.metrics.namespace\nproperty to a value like\n${spark.app.name}\n. This value is\nthen expanded appropriately by Spark and is used as the root namespace of the metrics system.\nNon-driver and executor metrics are never prefixed with\nspark.app.id\n, nor does the\nspark.metrics.namespace\nproperty have any such affect on such metrics.\nSpark’s metrics are decoupled into different\ninstances\ncorresponding to Spark components. Within each instance, you can configure a\nset of sinks to which metrics are reported. The following instances are currently supported:\nmaster\n: The Spark standalone master process.\napplications\n: A component within the master which reports on various applications.\nworker\n: A Spark standalone worker process.\nexecutor\n: A Spark executor.\ndriver\n: The Spark driver process (the process in which your SparkContext is created).\nshuffleService\n: The Spark shuffle service.\napplicationMaster\n: The Spark ApplicationMaster when running on YARN.\nEach instance can report to zero or more\nsinks\n. Sinks are contained in the\norg.apache.spark.metrics.sink\npackage:\nConsoleSink\n: Logs metrics information to the console.\nCSVSink\n: Exports metrics data to CSV files at regular intervals.\nJmxSink\n: Registers metrics for viewing in a JMX console.\nMetricsServlet\n: Adds a servlet within the existing Spark UI to serve metrics data as JSON data.\nPrometheusServlet\n: (Experimental) Adds a servlet within the existing Spark UI to serve metrics data in Prometheus format.\nGraphiteSink\n: Sends metrics to a Graphite node.\nSlf4jSink\n: Sends metrics to slf4j as log entries.\nStatsdSink\n: Sends metrics to a StatsD node.\nThe Prometheus Servlet mirrors the JSON data exposed by the\nMetrics Servlet\nand the REST API, but in a time-series format. The following are the equivalent Prometheus Servlet endpoints.\nComponent\nPort\nJSON End Point\nPrometheus End Point\nMaster\n8080\n/metrics/master/json/\n/metrics/master/prometheus/\nMaster\n8080\n/metrics/applications/json/\n/metrics/applications/prometheus/\nWorker\n8081\n/metrics/json/\n/metrics/prometheus/\nDriver\n4040\n/metrics/json/\n/metrics/prometheus/\nDriver\n4040\n/api/v1/applications/{id}/executors/\n/metrics/executors/prometheus/\nSpark also supports a Ganglia sink which is not included in the default build due to\nlicensing restrictions:\nGangliaSink\n: Sends metrics to a Ganglia node or multicast group.\nTo install the\nGangliaSink\nyou’ll need to perform a custom build of Spark.\nNote that\nby embedding this library you will include\nLGPL\n-licensed\ncode in your Spark package\n. For sbt users, set the\nSPARK_GANGLIA_LGPL\nenvironment variable before building. For Maven users, enable\nthe\n-Pspark-ganglia-lgpl\nprofile. In addition to modifying the cluster’s Spark build\nuser applications will need to link to the\nspark-ganglia-lgpl\nartifact.\nThe syntax of the metrics configuration file and the parameters available for each sink are defined\nin an example configuration file,\n$SPARK_HOME/conf/metrics.properties.template\n.\nWhen using Spark configuration parameters instead of the metrics configuration file, the relevant\nparameter names are composed by the prefix\nspark.metrics.conf.\nfollowed by the configuration\ndetails, i.e. the parameters take the following form:\nspark.metrics.conf.[instance|*].sink.[sink_name].[parameter_name]\n.\nThis example shows a list of Spark configuration parameters for a Graphite sink:\n\"spark.metrics.conf.*.sink.graphite.class\"=\"org.apache.spark.metrics.sink.GraphiteSink\"\n\"spark.metrics.conf.*.sink.graphite.host\"=\"graphiteEndPoint_hostName>\"\n\"spark.metrics.conf.*.sink.graphite.port\"=<graphite_listening_port>\n\"spark.metrics.conf.*.sink.graphite.period\"=10\n\"spark.metrics.conf.*.sink.graphite.unit\"=seconds\n\"spark.metrics.conf.*.sink.graphite.prefix\"=\"optional_prefix\"\n\"spark.metrics.conf.*.sink.graphite.regex\"=\"optional_regex_to_send_matching_metrics\"\nDefault values of the Spark metrics configuration are as follows:\n\"*.sink.servlet.class\" = \"org.apache.spark.metrics.sink.MetricsServlet\"\n\"*.sink.servlet.path\" = \"/metrics/json\"\n\"master.sink.servlet.path\" = \"/metrics/master/json\"\n\"applications.sink.servlet.path\" = \"/metrics/applications/json\"\nAdditional sources can be configured using the metrics configuration file or the configuration\nparameter\nspark.metrics.conf.[component_name].source.jvm.class=[source_name]\n. At present the\nJVM source is the only available optional source. For example the following configuration parameter\nactivates the JVM source:\n\"spark.metrics.conf.*.source.jvm.class\"=\"org.apache.spark.metrics.source.JvmSource\"\nList of available metrics providers\nMetrics used by Spark are of multiple types: gauge, counter, histogram, meter and timer,\nsee\nDropwizard library documentation for details\n.\nThe following list of components and metrics reports the name and some details about the available metrics,\ngrouped per component instance and source namespace.\nThe most common time of metrics used in Spark instrumentation are gauges and counters.\nCounters can be recognized as they have the\n.count\nsuffix. Timers, meters and histograms are annotated\nin the list, the rest of the list elements are metrics of type gauge.\nThe large majority of metrics are active as soon as their parent component instance is configured,\nsome metrics require also to be enabled via an additional configuration parameter, the details are\nreported in the list.\nComponent instance = Driver\nThis is the component with the largest amount of instrumented metrics\nnamespace=BlockManager\ndisk.diskSpaceUsed_MB\nmemory.maxMem_MB\nmemory.maxOffHeapMem_MB\nmemory.maxOnHeapMem_MB\nmemory.memUsed_MB\nmemory.offHeapMemUsed_MB\nmemory.onHeapMemUsed_MB\nmemory.remainingMem_MB\nmemory.remainingOffHeapMem_MB\nmemory.remainingOnHeapMem_MB\nnamespace=HiveExternalCatalog\nnote:\nthese metrics are conditional to a configuration parameter:\nspark.metrics.staticSources.enabled\n(default is true)\nfileCacheHits.count\nfilesDiscovered.count\nhiveClientCalls.count\nparallelListingJobCount.count\npartitionsFetched.count\nnamespace=CodeGenerator\nnote:\nthese metrics are conditional to a configuration parameter:\nspark.metrics.staticSources.enabled\n(default is true)\ncompilationTime (histogram)\ngeneratedClassSize (histogram)\ngeneratedMethodSize (histogram)\nsourceCodeSize (histogram)\nnamespace=DAGScheduler\njob.activeJobs\njob.allJobs\nmessageProcessingTime (timer)\nstage.failedStages\nstage.runningStages\nstage.waitingStages\nnamespace=LiveListenerBus\nlistenerProcessingTime.org.apache.spark.HeartbeatReceiver (timer)\nlistenerProcessingTime.org.apache.spark.scheduler.EventLoggingListener (timer)\nlistenerProcessingTime.org.apache.spark.status.AppStatusListener (timer)\nnumEventsPosted.count\nqueue.appStatus.listenerProcessingTime (timer)\nqueue.appStatus.numDroppedEvents.count\nqueue.appStatus.size\nqueue.eventLog.listenerProcessingTime (timer)\nqueue.eventLog.numDroppedEvents.count\nqueue.eventLog.size\nqueue.executorManagement.listenerProcessingTime (timer)\nnamespace=appStatus (all metrics of type=counter)\nnote:\nIntroduced in Spark 3.0. Conditional to a configuration parameter:\nspark.metrics.appStatusSource.enabled\n(default is true)\nstages.failedStages.count\nstages.skippedStages.count\nstages.completedStages.count\ntasks.blackListedExecutors.count // deprecated use excludedExecutors instead\ntasks.excludedExecutors.count\ntasks.completedTasks.count\ntasks.failedTasks.count\ntasks.killedTasks.count\ntasks.skippedTasks.count\ntasks.unblackListedExecutors.count // deprecated use unexcludedExecutors instead\ntasks.unexcludedExecutors.count\njobs.succeededJobs\njobs.failedJobs\njobDuration\nnamespace=AccumulatorSource\nnote:\nUser-configurable sources to attach accumulators to metric system\nDoubleAccumulatorSource\nLongAccumulatorSource\nnamespace=spark.streaming\nnote:\nThis applies to Spark Structured Streaming only. Conditional to a configuration\nparameter:\nspark.sql.streaming.metricsEnabled=true\n(default is false)\neventTime-watermark\ninputRate-total\nlatency\nprocessingRate-total\nstates-rowsTotal\nstates-usedBytes\nnamespace=JVMCPU\njvmCpuTime\nnamespace=executor\nnote:\nThese metrics are available in the driver in local mode only.\nA full list of available metrics in this\nnamespace can be found in the corresponding entry for the Executor component instance.\nnamespace=ExecutorMetrics\nnote:\nthese metrics are conditional to a configuration parameter:\nspark.metrics.executorMetricsSource.enabled\n(default is true)\nThis source contains memory-related metrics. A full list of available metrics in this\nnamespace can be found in the corresponding entry for the Executor component instance.\nnamespace=ExecutorAllocationManager\nnote:\nthese metrics are only emitted when using dynamic allocation. Conditional to a configuration\nparameter\nspark.dynamicAllocation.enabled\n(default is false)\nexecutors.numberExecutorsToAdd\nexecutors.numberExecutorsPendingToRemove\nexecutors.numberAllExecutors\nexecutors.numberTargetExecutors\nexecutors.numberMaxNeededExecutors\nexecutors.numberDecommissioningExecutors\nexecutors.numberExecutorsGracefullyDecommissioned.count\nexecutors.numberExecutorsDecommissionUnfinished.count\nexecutors.numberExecutorsExitedUnexpectedly.count\nexecutors.numberExecutorsKilledByDriver.count\nnamespace=plugin.<Plugin Class Name>\nOptional namespace(s). Metrics in this namespace are defined by user-supplied code, and\nconfigured using the Spark plugin API. See “Advanced Instrumentation” below for how to load\ncustom plugins into Spark.\nComponent instance = Executor\nThese metrics are exposed by Spark executors.\nnamespace=executor (metrics are of type counter or gauge)\nnotes:\nspark.executor.metrics.fileSystemSchemes\n(default:\nfile,hdfs\n) determines the exposed file system metrics.\nbytesRead.count\nbytesWritten.count\ncpuTime.count\ndeserializeCpuTime.count\ndeserializeTime.count\ndiskBytesSpilled.count\nfilesystem.file.largeRead_ops\nfilesystem.file.read_bytes\nfilesystem.file.read_ops\nfilesystem.file.write_bytes\nfilesystem.file.write_ops\nfilesystem.hdfs.largeRead_ops\nfilesystem.hdfs.read_bytes\nfilesystem.hdfs.read_ops\nfilesystem.hdfs.write_bytes\nfilesystem.hdfs.write_ops\njvmGCTime.count\nmemoryBytesSpilled.count\nrecordsRead.count\nrecordsWritten.count\nresultSerializationTime.count\nresultSize.count\nrunTime.count\nshuffleBytesWritten.count\nshuffleFetchWaitTime.count\nshuffleLocalBlocksFetched.count\nshuffleLocalBytesRead.count\nshuffleRecordsRead.count\nshuffleRecordsWritten.count\nshuffleRemoteBlocksFetched.count\nshuffleRemoteBytesRead.count\nshuffleRemoteBytesReadToDisk.count\nshuffleTotalBytesRead.count\nshuffleWriteTime.count\nMetrics related to push-based shuffle:\nshuffleCorruptMergedBlockChunks\nshuffleMergedFetchFallbackCount\nshuffleMergedRemoteBlocksFetched\nshuffleMergedLocalBlocksFetched\nshuffleMergedRemoteChunksFetched\nshuffleMergedLocalChunksFetched\nshuffleMergedRemoteBytesRead\nshuffleMergedLocalBytesRead\nshuffleRemoteReqsDuration\nshuffleMergedRemoteReqsDuration\nsucceededTasks.count\nthreadpool.activeTasks\nthreadpool.completeTasks\nthreadpool.currentPool_size\nthreadpool.maxPool_size\nthreadpool.startedTasks\nnamespace=ExecutorMetrics\nnotes:\nThese metrics are conditional to a configuration parameter:\nspark.metrics.executorMetricsSource.enabled\n(default value is true)\nExecutorMetrics are updated as part of heartbeat processes scheduled\n for the executors and for the driver at regular intervals:\nspark.executor.heartbeatInterval\n(default value is 10 seconds)\nAn optional faster polling mechanism is available for executor memory metrics,\n it can be activated by setting a polling interval (in milliseconds) using the configuration parameter\nspark.executor.metrics.pollingInterval\nJVMHeapMemory\nJVMOffHeapMemory\nOnHeapExecutionMemory\nOnHeapStorageMemory\nOnHeapUnifiedMemory\nOffHeapExecutionMemory\nOffHeapStorageMemory\nOffHeapUnifiedMemory\nDirectPoolMemory\nMappedPoolMemory\nMinorGCCount\nMinorGCTime\nMajorGCCount\nMajorGCTime\n“ProcessTree*” metric counters:\nProcessTreeJVMVMemory\nProcessTreeJVMRSSMemory\nProcessTreePythonVMemory\nProcessTreePythonRSSMemory\nProcessTreeOtherVMemory\nProcessTreeOtherRSSMemory\nnote:\n“ProcessTree\n” metrics are collected only under certain conditions.\nThe conditions are the logical AND of the following:\n/proc\nfilesystem exists,\nspark.executor.processTreeMetrics.enabled=true\n.\n“ProcessTree\n” metrics report 0 when those conditions are not met.\nnamespace=JVMCPU\njvmCpuTime\nnamespace=NettyBlockTransfer\nshuffle-client.usedDirectMemory\nshuffle-client.usedHeapMemory\nshuffle-server.usedDirectMemory\nshuffle-server.usedHeapMemory\nnamespace=HiveExternalCatalog\nnote:\nthese metrics are conditional to a configuration parameter:\nspark.metrics.staticSources.enabled\n(default is true)\nfileCacheHits.count\nfilesDiscovered.count\nhiveClientCalls.count\nparallelListingJobCount.count\npartitionsFetched.count\nnamespace=CodeGenerator\nnote:\nthese metrics are conditional to a configuration parameter:\nspark.metrics.staticSources.enabled\n(default is true)\ncompilationTime (histogram)\ngeneratedClassSize (histogram)\ngeneratedMethodSize (histogram)\nsourceCodeSize (histogram)\nnamespace=plugin.<Plugin Class Name>\nOptional namespace(s). Metrics in this namespace are defined by user-supplied code, and\nconfigured using the Spark plugin API. See “Advanced Instrumentation” below for how to load\ncustom plugins into Spark.\nSource = JVM Source\nNotes:\nActivate this source by setting the relevant\nmetrics.properties\nfile entry or the\n  configuration parameter:\nspark.metrics.conf.*.source.jvm.class=org.apache.spark.metrics.source.JvmSource\nThese metrics are conditional to a configuration parameter:\nspark.metrics.staticSources.enabled\n(default is true)\nThis source is available for driver and executor instances and is also available for other instances.\nThis source provides information on JVM metrics using the\nDropwizard/Codahale Metric Sets for JVM instrumentation\nand in particular the metric sets BufferPoolMetricSet, GarbageCollectorMetricSet and MemoryUsageGaugeSet.\nComponent instance = applicationMaster\nNote: applies when running on YARN\nnumContainersPendingAllocate\nnumExecutorsFailed\nnumExecutorsRunning\nnumLocalityAwareTasks\nnumReleasedContainers\nComponent instance = master\nNote: applies when running in Spark standalone as master\nworkers\naliveWorkers\napps\nwaitingApps\nComponent instance = ApplicationSource\nNote: applies when running in Spark standalone as master\nstatus\nruntime_ms\ncores\nComponent instance = worker\nNote: applies when running in Spark standalone as worker\nexecutors\ncoresUsed\nmemUsed_MB\ncoresFree\nmemFree_MB\nComponent instance = shuffleService\nNote: applies to the shuffle service\nblockTransferRate (meter) - rate of blocks being transferred\nblockTransferMessageRate (meter) - rate of block transfer messages,\ni.e. if batch fetches are enabled, this represents number of batches rather than number of blocks\nblockTransferRateBytes (meter)\nblockTransferAvgSize_1min (gauge - 1-minute moving average)\nnumActiveConnections.count\nnumRegisteredConnections.count\nnumCaughtExceptions.count\nopenBlockRequestLatencyMillis (timer)\nregisterExecutorRequestLatencyMillis (timer)\nfetchMergedBlocksMetaLatencyMillis (timer)\nfinalizeShuffleMergeLatencyMillis (timer)\nregisteredExecutorsSize\nshuffle-server.usedDirectMemory\nshuffle-server.usedHeapMemory\nnote:\nthe metrics below apply when the server side configuration\nspark.shuffle.push.server.mergedShuffleFileManagerImpl\nis set to\norg.apache.spark.network.shuffle.MergedShuffleFileManager\nfor Push-Based Shuffle\nblockBytesWritten - size of the pushed block data written to file in bytes\nblockAppendCollisions - number of shuffle push blocks collided in shuffle services\nas another block for the same reduce partition were being written\nlateBlockPushes - number of shuffle push blocks that are received in shuffle service\nafter the specific shuffle merge has been finalized\ndeferredBlocks - number of the current deferred block parts buffered in memory\ndeferredBlockBytes - size of the current deferred block parts buffered in memory\nstaleBlockPushes - number of stale shuffle block push requests\nignoredBlockBytes - size of the pushed block data that was transferred to ESS, but ignored.\nThe pushed block data are considered as ignored when: 1. it was received after the shuffle\nwas finalized; 2. when a push request is for a duplicate block; 3. ESS was unable to write the block.\nAdvanced Instrumentation\nSeveral external tools can be used to help profile the performance of Spark jobs:\nCluster-wide monitoring tools, such as\nGanglia\n, can provide\ninsight into overall cluster utilization and resource bottlenecks. For instance, a Ganglia\ndashboard can quickly reveal whether a particular workload is disk bound, network bound, or\nCPU bound.\nOS profiling tools such as\ndstat\n,\niostat\n, and\niotop\ncan provide fine-grained profiling on individual nodes.\nJVM utilities such as\njstack\nfor providing stack traces,\njmap\nfor creating heap-dumps,\njstat\nfor reporting time-series statistics and\njconsole\nfor visually exploring various JVM\nproperties are useful for those comfortable with JVM internals.\nSpark also provides a plugin API so that custom instrumentation code can be added to Spark\napplications. There are two configuration keys available for loading plugins into Spark:\nspark.plugins\nspark.plugins.defaultList\nBoth take a comma-separated list of class names that implement the\norg.apache.spark.api.plugin.SparkPlugin\ninterface. The two names exist so that it’s\npossible for one list to be placed in the Spark default config file, allowing users to\neasily add other plugins from the command line without overwriting the config file’s list. Duplicate\nplugins are ignored."}
{"url": "https://spark.apache.org/docs/latest/ml-statistics.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nBasic Statistics\n\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\wv}{\\mathbf{w}}\n\\newcommand{\\av}{\\mathbf{\\alpha}}\n\\newcommand{\\bv}{\\mathbf{b}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\id}{\\mathbf{I}}\n\\newcommand{\\ind}{\\mathbf{1}}\n\\newcommand{\\0}{\\mathbf{0}}\n\\newcommand{\\unit}{\\mathbf{e}}\n\\newcommand{\\one}{\\mathbf{1}}\n\\newcommand{\\zero}{\\mathbf{0}}\n\\]\nTable of Contents\nCorrelation\nHypothesis testing\nChiSquareTest\nSummarizer\nCorrelation\nCalculating the correlation between two series of data is a common operation in Statistics. In\nspark.ml\nwe provide the flexibility to calculate pairwise correlations among many series. The supported\ncorrelation methods are currently Pearson’s and Spearman’s correlation.\nCorrelation\ncomputes the correlation matrix for the input Dataset of Vectors using the specified method.\nThe output will be a DataFrame that contains the correlation matrix of the column of vectors.\nfrom\npyspark.ml.linalg\nimport\nVectors\nfrom\npyspark.ml.stat\nimport\nCorrelation\ndata\n=\n[(\nVectors\n.\nsparse\n(\n4\n,\n[(\n0\n,\n1.0\n),\n(\n3\n,\n-\n2.0\n)]),),\n(\nVectors\n.\ndense\n([\n4.0\n,\n5.0\n,\n0.0\n,\n3.0\n]),),\n(\nVectors\n.\ndense\n([\n6.0\n,\n7.0\n,\n0.0\n,\n8.0\n]),),\n(\nVectors\n.\nsparse\n(\n4\n,\n[(\n0\n,\n9.0\n),\n(\n3\n,\n1.0\n)]),)]\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\n[\n\"\nfeatures\n\"\n])\nr1\n=\nCorrelation\n.\ncorr\n(\ndf\n,\n\"\nfeatures\n\"\n).\nhead\n()\nprint\n(\n\"\nPearson correlation matrix:\n\\n\n\"\n+\nstr\n(\nr1\n[\n0\n]))\nr2\n=\nCorrelation\n.\ncorr\n(\ndf\n,\n\"\nfeatures\n\"\n,\n\"\nspearman\n\"\n).\nhead\n()\nprint\n(\n\"\nSpearman correlation matrix:\n\\n\n\"\n+\nstr\n(\nr2\n[\n0\n]))\nFind full example code at \"examples/src/main/python/ml/correlation_example.py\" in the Spark repo.\nCorrelation\ncomputes the correlation matrix for the input Dataset of Vectors using the specified method.\nThe output will be a DataFrame that contains the correlation matrix of the column of vectors.\nimport\norg.apache.spark.ml.linalg.\n{\nMatrix\n,\nVectors\n}\nimport\norg.apache.spark.ml.stat.Correlation\nimport\norg.apache.spark.sql.Row\nval\ndata\n=\nSeq\n(\nVectors\n.\nsparse\n(\n4\n,\nSeq\n((\n0\n,\n1.0\n),\n(\n3\n,\n-\n2.0\n))),\nVectors\n.\ndense\n(\n4.0\n,\n5.0\n,\n0.0\n,\n3.0\n),\nVectors\n.\ndense\n(\n6.0\n,\n7.0\n,\n0.0\n,\n8.0\n),\nVectors\n.\nsparse\n(\n4\n,\nSeq\n((\n0\n,\n9.0\n),\n(\n3\n,\n1.0\n)))\n)\nval\ndf\n=\ndata\n.\nmap\n(\nTuple1\n.\napply\n).\ntoDF\n(\n\"features\"\n)\nval\nRow\n(\ncoeff1\n:\nMatrix\n)\n=\nCorrelation\n.\ncorr\n(\ndf\n,\n\"features\"\n).\nhead\n()\nprintln\n(\ns\n\"Pearson correlation matrix:\\n $coeff1\"\n)\nval\nRow\n(\ncoeff2\n:\nMatrix\n)\n=\nCorrelation\n.\ncorr\n(\ndf\n,\n\"features\"\n,\n\"spearman\"\n).\nhead\n()\nprintln\n(\ns\n\"Spearman correlation matrix:\\n $coeff2\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\" in the Spark repo.\nCorrelation\ncomputes the correlation matrix for the input Dataset of Vectors using the specified method.\nThe output will be a DataFrame that contains the correlation matrix of the column of vectors.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.ml.stat.Correlation\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.*\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\nVectors\n.\nsparse\n(\n4\n,\nnew\nint\n[]{\n0\n,\n3\n},\nnew\ndouble\n[]{\n1.0\n,\n-\n2.0\n})),\nRowFactory\n.\ncreate\n(\nVectors\n.\ndense\n(\n4.0\n,\n5.0\n,\n0.0\n,\n3.0\n)),\nRowFactory\n.\ncreate\n(\nVectors\n.\ndense\n(\n6.0\n,\n7.0\n,\n0.0\n,\n8.0\n)),\nRowFactory\n.\ncreate\n(\nVectors\n.\nsparse\n(\n4\n,\nnew\nint\n[]{\n0\n,\n3\n},\nnew\ndouble\n[]{\n9.0\n,\n1.0\n}))\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"features\"\n,\nnew\nVectorUDT\n(),\nfalse\n,\nMetadata\n.\nempty\n()),\n});\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nRow\nr1\n=\nCorrelation\n.\ncorr\n(\ndf\n,\n\"features\"\n).\nhead\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Pearson correlation matrix:\\n\"\n+\nr1\n.\nget\n(\n0\n).\ntoString\n());\nRow\nr2\n=\nCorrelation\n.\ncorr\n(\ndf\n,\n\"features\"\n,\n\"spearman\"\n).\nhead\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Spearman correlation matrix:\\n\"\n+\nr2\n.\nget\n(\n0\n).\ntoString\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\" in the Spark repo.\nHypothesis testing\nHypothesis testing is a powerful tool in statistics to determine whether a result is statistically\nsignificant, whether this result occurred by chance or not.\nspark.ml\ncurrently supports Pearson’s\nChi-squared ( $\\chi^2$) tests for independence.\nChiSquareTest\nChiSquareTest\nconducts Pearson’s independence test for every feature against the label.\nFor each feature, the (feature, label) pairs are converted into a contingency matrix for which\nthe Chi-squared statistic is computed. All label and feature values must be categorical.\nRefer to the\nChiSquareTest\nPython docs\nfor details on the API.\nfrom\npyspark.ml.linalg\nimport\nVectors\nfrom\npyspark.ml.stat\nimport\nChiSquareTest\ndata\n=\n[(\n0.0\n,\nVectors\n.\ndense\n(\n0.5\n,\n10.0\n)),\n(\n0.0\n,\nVectors\n.\ndense\n(\n1.5\n,\n20.0\n)),\n(\n1.0\n,\nVectors\n.\ndense\n(\n1.5\n,\n30.0\n)),\n(\n0.0\n,\nVectors\n.\ndense\n(\n3.5\n,\n30.0\n)),\n(\n0.0\n,\nVectors\n.\ndense\n(\n3.5\n,\n40.0\n)),\n(\n1.0\n,\nVectors\n.\ndense\n(\n3.5\n,\n40.0\n))]\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\n[\n\"\nlabel\n\"\n,\n\"\nfeatures\n\"\n])\nr\n=\nChiSquareTest\n.\ntest\n(\ndf\n,\n\"\nfeatures\n\"\n,\n\"\nlabel\n\"\n).\nhead\n()\nprint\n(\n\"\npValues:\n\"\n+\nstr\n(\nr\n.\npValues\n))\nprint\n(\n\"\ndegreesOfFreedom:\n\"\n+\nstr\n(\nr\n.\ndegreesOfFreedom\n))\nprint\n(\n\"\nstatistics:\n\"\n+\nstr\n(\nr\n.\nstatistics\n))\nFind full example code at \"examples/src/main/python/ml/chi_square_test_example.py\" in the Spark repo.\nRefer to the\nChiSquareTest\nScala docs\nfor details on the API.\nimport\norg.apache.spark.ml.linalg.\n{\nVector\n,\nVectors\n}\nimport\norg.apache.spark.ml.stat.ChiSquareTest\nval\ndata\n=\nSeq\n(\n(\n0.0\n,\nVectors\n.\ndense\n(\n0.5\n,\n10.0\n)),\n(\n0.0\n,\nVectors\n.\ndense\n(\n1.5\n,\n20.0\n)),\n(\n1.0\n,\nVectors\n.\ndense\n(\n1.5\n,\n30.0\n)),\n(\n0.0\n,\nVectors\n.\ndense\n(\n3.5\n,\n30.0\n)),\n(\n0.0\n,\nVectors\n.\ndense\n(\n3.5\n,\n40.0\n)),\n(\n1.0\n,\nVectors\n.\ndense\n(\n3.5\n,\n40.0\n))\n)\nval\ndf\n=\ndata\n.\ntoDF\n(\n\"label\"\n,\n\"features\"\n)\nval\nchi\n=\nChiSquareTest\n.\ntest\n(\ndf\n,\n\"features\"\n,\n\"label\"\n).\nhead\n()\nprintln\n(\ns\n\"pValues = ${chi.getAs[Vector](0)}\"\n)\nprintln\n(\ns\n\"degreesOfFreedom ${chi.getSeq[Int](1).mkString(\"\n[\n\"\n,\n\"\n,\n\"\n,\n\"\n]\n\")}\"\n)\nprintln\n(\ns\n\"statistics ${chi.getAs[Vector](2)}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\" in the Spark repo.\nRefer to the\nChiSquareTest\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.ml.stat.ChiSquareTest\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.*\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n0.0\n,\nVectors\n.\ndense\n(\n0.5\n,\n10.0\n)),\nRowFactory\n.\ncreate\n(\n0.0\n,\nVectors\n.\ndense\n(\n1.5\n,\n20.0\n)),\nRowFactory\n.\ncreate\n(\n1.0\n,\nVectors\n.\ndense\n(\n1.5\n,\n30.0\n)),\nRowFactory\n.\ncreate\n(\n0.0\n,\nVectors\n.\ndense\n(\n3.5\n,\n30.0\n)),\nRowFactory\n.\ncreate\n(\n0.0\n,\nVectors\n.\ndense\n(\n3.5\n,\n40.0\n)),\nRowFactory\n.\ncreate\n(\n1.0\n,\nVectors\n.\ndense\n(\n3.5\n,\n40.0\n))\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"label\"\n,\nDataTypes\n.\nDoubleType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"features\"\n,\nnew\nVectorUDT\n(),\nfalse\n,\nMetadata\n.\nempty\n()),\n});\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nRow\nr\n=\nChiSquareTest\n.\ntest\n(\ndf\n,\n\"features\"\n,\n\"label\"\n).\nhead\n();\nSystem\n.\nout\n.\nprintln\n(\n\"pValues: \"\n+\nr\n.\nget\n(\n0\n).\ntoString\n());\nSystem\n.\nout\n.\nprintln\n(\n\"degreesOfFreedom: \"\n+\nr\n.\ngetList\n(\n1\n).\ntoString\n());\nSystem\n.\nout\n.\nprintln\n(\n\"statistics: \"\n+\nr\n.\nget\n(\n2\n).\ntoString\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\" in the Spark repo.\nSummarizer\nWe provide vector column summary statistics for\nDataframe\nthrough\nSummarizer\n.\nAvailable metrics are the column-wise max, min, mean, sum, variance, std, and number of nonzeros,\nas well as the total count.\nRefer to the\nSummarizer\nPython docs\nfor details on the API.\nfrom\npyspark.ml.stat\nimport\nSummarizer\nfrom\npyspark.sql\nimport\nRow\nfrom\npyspark.ml.linalg\nimport\nVectors\ndf\n=\nsc\n.\nparallelize\n([\nRow\n(\nweight\n=\n1.0\n,\nfeatures\n=\nVectors\n.\ndense\n(\n1.0\n,\n1.0\n,\n1.0\n)),\nRow\n(\nweight\n=\n0.0\n,\nfeatures\n=\nVectors\n.\ndense\n(\n1.0\n,\n2.0\n,\n3.0\n))]).\ntoDF\n()\n# create summarizer for multiple metrics \"mean\" and \"count\"\nsummarizer\n=\nSummarizer\n.\nmetrics\n(\n\"\nmean\n\"\n,\n\"\ncount\n\"\n)\n# compute statistics for multiple metrics with weight\ndf\n.\nselect\n(\nsummarizer\n.\nsummary\n(\ndf\n.\nfeatures\n,\ndf\n.\nweight\n)).\nshow\n(\ntruncate\n=\nFalse\n)\n# compute statistics for multiple metrics without weight\ndf\n.\nselect\n(\nsummarizer\n.\nsummary\n(\ndf\n.\nfeatures\n)).\nshow\n(\ntruncate\n=\nFalse\n)\n# compute statistics for single metric \"mean\" with weight\ndf\n.\nselect\n(\nSummarizer\n.\nmean\n(\ndf\n.\nfeatures\n,\ndf\n.\nweight\n)).\nshow\n(\ntruncate\n=\nFalse\n)\n# compute statistics for single metric \"mean\" without weight\ndf\n.\nselect\n(\nSummarizer\n.\nmean\n(\ndf\n.\nfeatures\n)).\nshow\n(\ntruncate\n=\nFalse\n)\nFind full example code at \"examples/src/main/python/ml/summarizer_example.py\" in the Spark repo.\nThe following example demonstrates using\nSummarizer\nto compute the mean and variance for a vector column of the input dataframe, with and without a weight column.\nimport\norg.apache.spark.ml.linalg.\n{\nVector\n,\nVectors\n}\nimport\norg.apache.spark.ml.stat.Summarizer\nval\ndata\n=\nSeq\n(\n(\nVectors\n.\ndense\n(\n2.0\n,\n3.0\n,\n5.0\n),\n1.0\n),\n(\nVectors\n.\ndense\n(\n4.0\n,\n6.0\n,\n7.0\n),\n2.0\n)\n)\nval\ndf\n=\ndata\n.\ntoDF\n(\n\"features\"\n,\n\"weight\"\n)\nval\n(\nmeanVal\n,\nvarianceVal\n)\n=\ndf\n.\nselect\n(\nmetrics\n(\n\"mean\"\n,\n\"variance\"\n)\n.\nsummary\n(\n$\n\"features\"\n,\n$\n\"weight\"\n).\nas\n(\n\"summary\"\n))\n.\nselect\n(\n\"summary.mean\"\n,\n\"summary.variance\"\n)\n.\nas\n[(\nVector\n,\nVector\n)].\nfirst\n()\nprintln\n(\ns\n\"with weight: mean = ${meanVal}, variance = ${varianceVal}\"\n)\nval\n(\nmeanVal2\n,\nvarianceVal2\n)\n=\ndf\n.\nselect\n(\nmean\n(\n$\n\"features\"\n),\nvariance\n(\n$\n\"features\"\n))\n.\nas\n[(\nVector\n,\nVector\n)].\nfirst\n()\nprintln\n(\ns\n\"without weight: mean = ${meanVal2}, sum = ${varianceVal2}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/SummarizerExample.scala\" in the Spark repo.\nThe following example demonstrates using\nSummarizer\nto compute the mean and variance for a vector column of the input dataframe, with and without a weight column.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.linalg.Vector\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.ml.stat.Summarizer\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\nVectors\n.\ndense\n(\n2.0\n,\n3.0\n,\n5.0\n),\n1.0\n),\nRowFactory\n.\ncreate\n(\nVectors\n.\ndense\n(\n4.0\n,\n6.0\n,\n7.0\n),\n2.0\n)\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"features\"\n,\nnew\nVectorUDT\n(),\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"weight\"\n,\nDataTypes\n.\nDoubleType\n,\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nRow\nresult1\n=\ndf\n.\nselect\n(\nSummarizer\n.\nmetrics\n(\n\"mean\"\n,\n\"variance\"\n)\n.\nsummary\n(\nnew\nColumn\n(\n\"features\"\n),\nnew\nColumn\n(\n\"weight\"\n)).\nas\n(\n\"summary\"\n))\n.\nselect\n(\n\"summary.mean\"\n,\n\"summary.variance\"\n).\nfirst\n();\nSystem\n.\nout\n.\nprintln\n(\n\"with weight: mean = \"\n+\nresult1\n.<\nVector\n>\ngetAs\n(\n0\n).\ntoString\n()\n+\n\", variance = \"\n+\nresult1\n.<\nVector\n>\ngetAs\n(\n1\n).\ntoString\n());\nRow\nresult2\n=\ndf\n.\nselect\n(\nSummarizer\n.\nmean\n(\nnew\nColumn\n(\n\"features\"\n)),\nSummarizer\n.\nvariance\n(\nnew\nColumn\n(\n\"features\"\n))\n).\nfirst\n();\nSystem\n.\nout\n.\nprintln\n(\n\"without weight: mean = \"\n+\nresult2\n.<\nVector\n>\ngetAs\n(\n0\n).\ntoString\n()\n+\n\", variance = \"\n+\nresult2\n.<\nVector\n>\ngetAs\n(\n1\n).\ntoString\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaSummarizerExample.java\" in the Spark repo."}
{"url": "https://spark.apache.org/docs/latest/running-on-kubernetes.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nRunning Spark on Kubernetes\nSecurity\nUser Identity\nVolume Mounts\nPrerequisites\nHow it works\nSubmitting Applications to Kubernetes\nDocker Images\nCluster Mode\nClient Mode\nClient Mode Networking\nClient Mode Executor Pod Garbage Collection\nAuthentication Parameters\nIPv4 and IPv6\nDependency Management\nSecret Management\nPod Template\nUsing Kubernetes Volumes\nPVC-oriented executor pod allocation\nLocal Storage\nUsing RAM for local storage\nIntrospection and Debugging\nAccessing Logs\nAccessing Driver UI\nDebugging\nKubernetes Features\nConfiguration File\nContexts\nNamespaces\nRBAC\nSpark Application Management\nFuture Work\nConfiguration\nSpark Properties\nPod template properties\nPod Metadata\nPod Spec\nContainer spec\nResource Allocation and Configuration Overview\nResource Level Scheduling Overview\nPriority Scheduling\nCustomized Kubernetes Schedulers for Spark on Kubernetes\nUsing Volcano as Customized Scheduler for Spark on Kubernetes\nPrerequisites\nBuild\nUsage\nVolcano Feature Step\nVolcano PodGroup Template\nUsing Apache YuniKorn as Customized Scheduler for Spark on Kubernetes\nPrerequisites\nGet started\nStage Level Scheduling Overview\nSpark can run on clusters managed by\nKubernetes\n. This feature makes use of native\nKubernetes scheduler that has been added to Spark.\nSecurity\nSecurity features like authentication are not enabled by default. When deploying a cluster that is open to the internet\nor an untrusted network, it’s important to secure access to the cluster to prevent unauthorized applications\nfrom running on the cluster.\nPlease see\nSpark Security\nand the specific security sections in this doc before running Spark.\nUser Identity\nImages built from the project provided Dockerfiles contain a default\nUSER\ndirective with a default UID of\n185\n.  This means that the resulting images will be running the Spark processes as this UID inside the container. Security conscious deployments should consider providing custom images with\nUSER\ndirectives specifying their desired unprivileged UID and GID.  The resulting UID should include the root group in its supplementary groups in order to be able to run the Spark executables.  Users building their own images with the provided\ndocker-image-tool.sh\nscript can use the\n-u <uid>\noption to specify the desired UID.\nAlternatively the\nPod Template\nfeature can be used to add a\nSecurity Context\nwith a\nrunAsUser\nto the pods that Spark submits.  This can be used to override the\nUSER\ndirectives in the images themselves.  Please bear in mind that this requires cooperation from your users and as such may not be a suitable solution for shared environments.  Cluster administrators should use\nPod Security Policies\nif they wish to limit the users that pods may run as.\nVolume Mounts\nAs described later in this document under\nUsing Kubernetes Volumes\nSpark on K8S provides configuration options that allow for mounting certain volume types into the driver and executor pods.  In particular it allows for\nhostPath\nvolumes which as described in the Kubernetes documentation have known security vulnerabilities.\nCluster administrators should use\nPod Security Policies\nto limit the ability to mount\nhostPath\nvolumes appropriately for their environments.\nPrerequisites\nA running Kubernetes cluster at version >= 1.30 with access configured to it using\nkubectl\n.  If you do not already have a working Kubernetes cluster,\nyou may set up a test cluster on your local machine using\nminikube\n.\nWe recommend using the latest release of minikube with the DNS addon enabled.\nBe aware that the default minikube configuration is not enough for running Spark applications.\nWe recommend 3 CPUs and 4g of memory to be able to start a simple Spark application with a single\nexecutor.\nCheck\nkubernetes-client library\n’s version of your Spark environment, and its compatibility with your Kubernetes cluster’s version.\nYou must have appropriate permissions to list, create, edit and delete\npods\nin your cluster. You can verify that you can list these resources\nby running\nkubectl auth can-i <list|create|edit|delete> pods\n.\nThe service account credentials used by the driver pods must be allowed to create pods, services and configmaps.\nYou must have\nKubernetes DNS\nconfigured in your cluster.\nHow it works\nspark-submit\ncan be directly used to submit a Spark application to a Kubernetes cluster.\nThe submission mechanism works as follows:\nSpark creates a Spark driver running within a\nKubernetes pod\n.\nThe driver creates executors which are also running within Kubernetes pods and connects to them, and executes application code.\nWhen the application completes, the executor pods terminate and are cleaned up, but the driver pod persists\nlogs and remains in “completed” state in the Kubernetes API until it’s eventually garbage collected or manually cleaned up.\nNote that in the completed state, the driver pod does\nnot\nuse any computational or memory resources.\nThe driver and executor pod scheduling is handled by Kubernetes. Communication to the Kubernetes API is done via fabric8. It is possible to schedule the\ndriver and executor pods on a subset of available nodes through a\nnode selector\nusing the configuration property for it. It will be possible to use more advanced\nscheduling hints like\nnode/pod affinities\nin a future release.\nSubmitting Applications to Kubernetes\nDocker Images\nKubernetes requires users to supply images that can be deployed into containers within pods. The images are built to\nbe run in a container runtime environment that Kubernetes supports. Docker is a container runtime environment that is\nfrequently used with Kubernetes. Spark (starting with version 2.3) ships with a Dockerfile that can be used for this\npurpose, or customized to match an individual application’s needs. It can be found in the\nkubernetes/dockerfiles/\ndirectory.\nSpark also ships with a\nbin/docker-image-tool.sh\nscript that can be used to build and publish the Docker images to\nuse with the Kubernetes backend.\nExample usage is:\n$\n./bin/docker-image-tool.sh\n-r\n<repo>\n-t\nmy-tag build\n$\n./bin/docker-image-tool.sh\n-r\n<repo>\n-t\nmy-tag push\nThis will build using the projects provided default\nDockerfiles\n. To see more options available for customising the behaviour of this tool, including providing custom\nDockerfiles\n, please run with the\n-h\nflag.\nBy default\nbin/docker-image-tool.sh\nbuilds docker image for running JVM jobs. You need to opt-in to build additional\nlanguage binding docker images.\nExample usage is\n# To build additional PySpark docker image\n$\n./bin/docker-image-tool.sh\n-r\n<repo>\n-t\nmy-tag\n-p\n./kubernetes/dockerfiles/spark/bindings/python/Dockerfile build\n# To build additional SparkR docker image\n$\n./bin/docker-image-tool.sh\n-r\n<repo>\n-t\nmy-tag\n-R\n./kubernetes/dockerfiles/spark/bindings/R/Dockerfile build\nYou can also use the\nApache Spark Docker images\n(such as\napache/spark:<version>\n) directly.\nCluster Mode\nTo launch Spark Pi in cluster mode,\n$\n./bin/spark-submit\n\\\n--master\nk8s://https://<k8s-apiserver-host>:<k8s-apiserver-port>\n\\\n--deploy-mode\ncluster\n\\\n--name\nspark-pi\n\\\n--class\norg.apache.spark.examples.SparkPi\n\\\n--conf\nspark.executor.instances\n=\n5\n\\\n--conf\nspark.kubernetes.container.image\n=\n<spark-image>\n\\\nlocal\n:///path/to/examples.jar\nThe Spark master, specified either via passing the\n--master\ncommand line argument to\nspark-submit\nor by setting\nspark.master\nin the application’s configuration, must be a URL with the format\nk8s://<api_server_host>:<k8s-apiserver-port>\n. The port must always be specified, even if it’s the HTTPS port 443. Prefixing the\nmaster string with\nk8s://\nwill cause the Spark application to launch on the Kubernetes cluster, with the API server\nbeing contacted at\napi_server_url\n. If no HTTP protocol is specified in the URL, it defaults to\nhttps\n. For example,\nsetting the master to\nk8s://example.com:443\nis equivalent to setting it to\nk8s://https://example.com:443\n, but to\nconnect without TLS on a different port, the master would be set to\nk8s://http://example.com:8080\n.\nIn Kubernetes mode, the Spark application name that is specified by\nspark.app.name\nor the\n--name\nargument to\nspark-submit\nis used by default to name the Kubernetes resources created like drivers and executors. So, application names\nmust consist of lower case alphanumeric characters,\n-\n, and\n.\nand must start and end with an alphanumeric character.\nIf you have a Kubernetes cluster setup, one way to discover the apiserver URL is by executing\nkubectl cluster-info\n.\n$\nkubectl cluster-info\nKubernetes master is running at http://127.0.0.1:6443\nIn the above example, the specific Kubernetes cluster can be used with\nspark-submit\nby specifying\n--master k8s://http://127.0.0.1:6443\nas an argument to spark-submit. Additionally, it is also possible to use the\nauthenticating proxy,\nkubectl proxy\nto communicate to the Kubernetes API.\nThe local proxy can be started by:\n$\nkubectl proxy\nIf the local proxy is running at localhost:8001,\n--master k8s://http://127.0.0.1:8001\ncan be used as the argument to\nspark-submit. Finally, notice that in the above example we specify a jar with a specific URI with a scheme of\nlocal://\n.\nThis URI is the location of the example jar that is already in the Docker image.\nClient Mode\nStarting with Spark 2.4.0, it is possible to run Spark applications on Kubernetes in client mode. When your application\nruns in client mode, the driver can run inside a pod or on a physical host. When running an application in client mode,\nit is recommended to account for the following factors:\nClient Mode Networking\nSpark executors must be able to connect to the Spark driver over a hostname and a port that is routable from the Spark\nexecutors. The specific network configuration that will be required for Spark to work in client mode will vary per\nsetup. If you run your driver inside a Kubernetes pod, you can use a\nheadless service\nto allow your\ndriver pod to be routable from the executors by a stable hostname. When deploying your headless service, ensure that\nthe service’s label selector will only match the driver pod and no other pods; it is recommended to assign your driver\npod a sufficiently unique label and to use that label in the label selector of the headless service. Specify the driver’s\nhostname via\nspark.driver.host\nand your spark driver’s port to\nspark.driver.port\n.\nClient Mode Executor Pod Garbage Collection\nIf you run your Spark driver in a pod, it is highly recommended to set\nspark.kubernetes.driver.pod.name\nto the name of that pod.\nWhen this property is set, the Spark scheduler will deploy the executor pods with an\nOwnerReference\n, which in turn will\nensure that once the driver pod is deleted from the cluster, all of the application’s executor pods will also be deleted.\nThe driver will look for a pod with the given name in the namespace specified by\nspark.kubernetes.namespace\n, and\nan OwnerReference pointing to that pod will be added to each executor pod’s OwnerReferences list. Be careful to avoid\nsetting the OwnerReference to a pod that is not actually that driver pod, or else the executors may be terminated\nprematurely when the wrong pod is deleted.\nIf your application is not running inside a pod, or if\nspark.kubernetes.driver.pod.name\nis not set when your application is\nactually running in a pod, keep in mind that the executor pods may not be properly deleted from the cluster when the\napplication exits. The Spark scheduler attempts to delete these pods, but if the network request to the API server fails\nfor any reason, these pods will remain in the cluster. The executor processes should exit when they cannot reach the\ndriver, so the executor pods should not consume compute resources (cpu and memory) in the cluster after your application\nexits.\nYou may use\nspark.kubernetes.executor.podNamePrefix\nto fully control the executor pod names.\nWhen this property is set, it’s highly recommended to make it unique across all jobs in the same namespace.\nAuthentication Parameters\nUse the exact prefix\nspark.kubernetes.authenticate\nfor Kubernetes authentication parameters in client mode.\nIPv4 and IPv6\nStarting with 3.4.0, Spark supports additionally IPv6-only environment via\nIPv4/IPv6 dual-stack network\nfeature which enables the allocation of both IPv4 and IPv6 addresses to Pods and Services.\nAccording to the K8s cluster capability,\nspark.kubernetes.driver.service.ipFamilyPolicy\nand\nspark.kubernetes.driver.service.ipFamilies\ncan be one of\nSingleStack\n,\nPreferDualStack\n,\nand\nRequireDualStack\nand one of\nIPv4\n,\nIPv6\n,\nIPv4,IPv6\n, and\nIPv6,IPv4\nrespectively.\nBy default, Spark uses\nspark.kubernetes.driver.service.ipFamilyPolicy=SingleStack\nand\nspark.kubernetes.driver.service.ipFamilies=IPv4\n.\nTo use only\nIPv6\n, you can submit your jobs with the following.\n...\n--conf\nspark.kubernetes.driver.service.ipFamilies\n=\nIPv6\n\\\nIn\nDualStack\nenvironment, you may need\njava.net.preferIPv6Addresses=true\nfor JVM\nand\nSPARK_PREFER_IPV6=true\nfor Python additionally to use\nIPv6\n.\nDependency Management\nIf your application’s dependencies are all hosted in remote locations like HDFS or HTTP servers, they may be referred to\nby their appropriate remote URIs. Also, application dependencies can be pre-mounted into custom-built Docker images.\nThose dependencies can be added to the classpath by referencing them with\nlocal://\nURIs and/or setting the\nSPARK_EXTRA_CLASSPATH\nenvironment variable in your Dockerfiles. The\nlocal://\nscheme is also required when referring to\ndependencies in custom-built Docker images in\nspark-submit\n. We support dependencies from the submission\nclient’s local file system using the\nfile://\nscheme or without a scheme (using a full path), where the destination should be a Hadoop compatible filesystem.\nA typical example of this using S3 is via passing the following options:\n...\n--packages org.apache.hadoop:hadoop-aws:3.4.1\n--conf spark.kubernetes.file.upload.path=s3a://<s3-bucket>/path\n--conf spark.hadoop.fs.s3a.access.key=...\n--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem\n--conf spark.hadoop.fs.s3a.fast.upload=true\n--conf spark.hadoop.fs.s3a.secret.key=....\n--conf spark.driver.extraJavaOptions=-Divy.cache.dir=/tmp -Divy.home=/tmp\nfile:///full/path/to/app.jar\nThe app jar file will be uploaded to the S3 and then when the driver is launched it will be downloaded\nto the driver pod and will be added to its classpath. Spark will generate a subdir under the upload path with a random name\nto avoid conflicts with spark apps running in parallel. User could manage the subdirs created according to his needs.\nThe client scheme is supported for the application jar, and dependencies specified by properties\nspark.jars\n,\nspark.files\nand\nspark.archives\n.\nImportant: all client-side dependencies will be uploaded to the given path with a flat directory structure so\nfile names must be unique otherwise files will be overwritten. Also make sure in the derived k8s image default ivy dir\nhas the required access rights or modify the settings as above. The latter is also important if you use\n--packages\nin\ncluster mode.\nSecret Management\nKubernetes\nSecrets\ncan be used to provide credentials for a\nSpark application to access secured services. To mount a user-specified secret into the driver container, users can use\nthe configuration property of the form\nspark.kubernetes.driver.secrets.[SecretName]=<mount path>\n. Similarly, the\nconfiguration property of the form\nspark.kubernetes.executor.secrets.[SecretName]=<mount path>\ncan be used to mount a\nuser-specified secret into the executor containers. Note that it is assumed that the secret to be mounted is in the same\nnamespace as that of the driver and executor pods. For example, to mount a secret named\nspark-secret\nonto the path\n/etc/secrets\nin both the driver and executor containers, add the following options to the\nspark-submit\ncommand:\n--conf spark.kubernetes.driver.secrets.spark-secret=/etc/secrets\n--conf spark.kubernetes.executor.secrets.spark-secret=/etc/secrets\nTo use a secret through an environment variable use the following options to the\nspark-submit\ncommand:\n--conf spark.kubernetes.driver.secretKeyRef.ENV_NAME=name:key\n--conf spark.kubernetes.executor.secretKeyRef.ENV_NAME=name:key\nPod Template\nKubernetes allows defining pods from\ntemplate files\n.\nSpark users can similarly use template files to define the driver or executor pod configurations that Spark configurations do not support.\nTo do so, specify the spark properties\nspark.kubernetes.driver.podTemplateFile\nand\nspark.kubernetes.executor.podTemplateFile\nto point to files accessible to the\nspark-submit\nprocess.\n--conf spark.kubernetes.driver.podTemplateFile=s3a://bucket/driver.yml\n--conf spark.kubernetes.executor.podTemplateFile=s3a://bucket/executor.yml\nTo allow the driver pod access the executor pod template\nfile, the file will be automatically mounted onto a volume in the driver pod when it’s created.\nSpark does not do any validation after unmarshalling these template files and relies on the Kubernetes API server for validation.\nIt is important to note that Spark is opinionated about certain pod configurations so there are values in the\npod template that will always be overwritten by Spark. Therefore, users of this feature should note that specifying\nthe pod template file only lets Spark start with a template pod instead of an empty pod during the pod-building process.\nFor details, see the\nfull list\nof pod template values that will be overwritten by spark.\nPod template files can also define multiple containers. In such cases, you can use the spark properties\nspark.kubernetes.driver.podTemplateContainerName\nand\nspark.kubernetes.executor.podTemplateContainerName\nto indicate which container should be used as a basis for the driver or executor.\nIf not specified, or if the container name is not valid, Spark will assume that the first container in the list\nwill be the driver or executor container.\nUsing Kubernetes Volumes\nUsers can mount the following types of Kubernetes\nvolumes\ninto the driver and executor pods:\nhostPath\n: mounts a file or directory from the host node’s filesystem into a pod.\nemptyDir\n: an initially empty volume created when a pod is assigned to a node.\nnfs\n: mounts an existing NFS(Network File System) into a pod.\npersistentVolumeClaim\n: mounts a\nPersistentVolume\ninto a pod.\nNB:\nPlease see the\nSecurity\nsection of this document for security issues related to volume mounts.\nTo mount a volume of any of the types above into the driver pod, use the following configuration property:\n--conf spark.kubernetes.driver.volumes.[VolumeType].[VolumeName].mount.path=<mount path>\n--conf spark.kubernetes.driver.volumes.[VolumeType].[VolumeName].mount.readOnly=<true|false>\n--conf spark.kubernetes.driver.volumes.[VolumeType].[VolumeName].mount.subPath=<mount subPath>\nSpecifically,\nVolumeType\ncan be one of the following values:\nhostPath\n,\nemptyDir\n,\nnfs\nand\npersistentVolumeClaim\n.\nVolumeName\nis the name you want to use for the volume under the\nvolumes\nfield in the pod specification.\nEach supported type of volumes may have some specific configuration options, which can be specified using configuration properties of the following form:\nspark.kubernetes.driver.volumes.[VolumeType].[VolumeName].options.[OptionName]=<value>\nFor example, the server and path of a\nnfs\nwith volume name\nimages\ncan be specified using the following properties:\nspark.kubernetes.driver.volumes.nfs.images.options.server=example.com\nspark.kubernetes.driver.volumes.nfs.images.options.path=/data\nAnd, the claim name of a\npersistentVolumeClaim\nwith volume name\ncheckpointpvc\ncan be specified using the following property:\nspark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.options.claimName=check-point-pvc-claim\nThe configuration properties for mounting volumes into the executor pods use prefix\nspark.kubernetes.executor.\ninstead of\nspark.kubernetes.driver.\n.\nFor example, you can mount a dynamically-created persistent volume claim per executor by using\nOnDemand\nas a claim name and\nstorageClass\nand\nsizeLimit\noptions like the following. This is useful in case of\nDynamic Allocation\n.\nspark.kubernetes.executor.volumes.persistentVolumeClaim.data.options.claimName=OnDemand\nspark.kubernetes.executor.volumes.persistentVolumeClaim.data.options.storageClass=gp\nspark.kubernetes.executor.volumes.persistentVolumeClaim.data.options.sizeLimit=500Gi\nspark.kubernetes.executor.volumes.persistentVolumeClaim.data.mount.path=/data\nspark.kubernetes.executor.volumes.persistentVolumeClaim.data.mount.readOnly=false\nFor a complete list of available options for each supported type of volumes, please refer to the\nSpark Properties\nsection below.\nPVC-oriented executor pod allocation\nSince disks are one of the important resource types, Spark driver provides a fine-grained control\nvia a set of configurations. For example, by default, on-demand PVCs are owned by executors and\nthe lifecycle of PVCs are tightly coupled with its owner executors.\nHowever, on-demand PVCs can be owned by driver and reused by another executors during the Spark job’s\nlifetime with the following options. This reduces the overhead of PVC creation and deletion.\nspark.kubernetes.driver.ownPersistentVolumeClaim=true\nspark.kubernetes.driver.reusePersistentVolumeClaim=true\nIn addition, since Spark 3.4, Spark driver is able to do PVC-oriented executor allocation which means\nSpark counts the total number of created PVCs which the job can have, and holds on a new executor creation\nif the driver owns the maximum number of PVCs. This helps the transition of the existing PVC from one executor\nto another executor.\nspark.kubernetes.driver.waitToReusePersistentVolumeClaim=true\nLocal Storage\nSpark supports using volumes to spill data during shuffles and other operations. To use a volume as local storage, the volume’s name should starts with\nspark-local-dir-\n, for example:\n--conf spark.kubernetes.driver.volumes.[VolumeType].spark-local-dir-[VolumeName].mount.path=<mount path>\n--conf spark.kubernetes.driver.volumes.[VolumeType].spark-local-dir-[VolumeName].mount.readOnly=false\nSpecifically, you can use persistent volume claims if the jobs require large shuffle and sorting operations in executors.\nspark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.claimName=OnDemand\nspark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.storageClass=gp\nspark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.sizeLimit=500Gi\nspark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.mount.path=/data\nspark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.mount.readOnly=false\nTo enable shuffle data recovery feature via the built-in\nKubernetesLocalDiskShuffleDataIO\nplugin, we need to have the following. You may want to enable\nspark.kubernetes.driver.waitToReusePersistentVolumeClaim\nadditionally.\nspark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.mount.path=/data/spark-x/executor-x\nspark.shuffle.sort.io.plugin.class=org.apache.spark.shuffle.KubernetesLocalDiskShuffleDataIO\nIf no volume is set as local storage, Spark uses temporary scratch space to spill data to disk during shuffles and other operations. When using Kubernetes as the resource manager the pods will be created with an\nemptyDir\nvolume mounted for each directory listed in\nspark.local.dir\nor the environment variable\nSPARK_LOCAL_DIRS\n.  If no directories are explicitly specified then a default directory is created and configured appropriately.\nemptyDir\nvolumes use the ephemeral storage feature of Kubernetes and do not persist beyond the life of the pod.\nUsing RAM for local storage\nemptyDir\nvolumes use the nodes backing storage for ephemeral storage by default, this behaviour may not be appropriate for some compute environments.  For example if you have diskless nodes with remote storage mounted over a network, having lots of executors doing IO to this remote storage may actually degrade performance.\nIn this case it may be desirable to set\nspark.kubernetes.local.dirs.tmpfs=true\nin your configuration which will cause the\nemptyDir\nvolumes to be configured as\ntmpfs\ni.e. RAM backed volumes.  When configured like this Spark’s local storage usage will count towards your pods memory usage therefore you may wish to increase your memory requests by increasing the value of\nspark.{driver,executor}.memoryOverheadFactor\nas appropriate.\nIntrospection and Debugging\nThese are the different ways in which you can investigate a running/completed Spark application, monitor progress, and\ntake actions.\nAccessing Logs\nLogs can be accessed using the Kubernetes API and the\nkubectl\nCLI. When a Spark application is running, it’s possible\nto stream logs from the application using:\n$\nkubectl\n-n\n=\n<namespace> logs\n-f\n<driver-pod-name>\nThe same logs can also be accessed through the\nKubernetes dashboard\nif installed on\nthe cluster.\nWhen there exists a log collection system, you can expose it at Spark Driver\nExecutors\ntab UI. For example,\nspark.ui.custom.executor.log.url='https://log-server/log?appId=&execId='\nYou can add additional custom variables to this url template, populated with the values of existing executor environment variables like\nspark.executorEnv.SPARK_EXECUTOR_ATTRIBUTE_YOUR_VAR='$(EXISTING_EXECUTOR_ENV_VAR)'\nspark.ui.custom.executor.log.url='https://log-server/log?appId=&execId=&your_var='\nAccessing Driver UI\nThe UI associated with any application can be accessed locally using\nkubectl port-forward\n.\n$\nkubectl port-forward <driver-pod-name> 4040:4040\nThen, the Spark driver UI can be accessed on\nhttp://localhost:4040\n.\nSince Apache Spark 4.0.0, Driver UI provides a way to see driver logs via a new configuration.\nspark.driver.log.localDir=/tmp\nThen, the Spark driver UI can be accessed on\nhttp://localhost:4040/logs/\n.\nOptionally, the layout of log is configured by the following.\nspark.driver.log.layout=\"%m%n%ex\"\nDebugging\nThere may be several kinds of failures. If the Kubernetes API server rejects the request made from spark-submit, or the\nconnection is refused for a different reason, the submission logic should indicate the error encountered. However, if there\nare errors during the running of the application, often, the best way to investigate may be through the Kubernetes CLI.\nTo get some basic information about the scheduling decisions made around the driver pod, you can run:\n$\nkubectl describe pod <spark-driver-pod>\nIf the pod has encountered a runtime error, the status can be probed further using:\n$\nkubectl logs <spark-driver-pod>\nStatus and logs of failed executor pods can be checked in similar ways. Finally, deleting the driver pod will clean up the entire spark\napplication, including all executors, associated service, etc. The driver pod can be thought of as the Kubernetes representation of\nthe Spark application.\nKubernetes Features\nConfiguration File\nYour Kubernetes config file typically lives under\n.kube/config\nin your home directory or in a location specified by the\nKUBECONFIG\nenvironment variable.  Spark on Kubernetes will attempt to use this file to do an initial auto-configuration of the Kubernetes client used to interact with the Kubernetes cluster.  A variety of Spark configuration properties are provided that allow further customising the client configuration e.g. using an alternative authentication method.\nContexts\nKubernetes configuration files can contain multiple contexts that allow for switching between different clusters and/or user identities.  By default Spark on Kubernetes will use your current context (which can be checked by running\nkubectl config current-context\n) when doing the initial auto-configuration of the Kubernetes client.\nIn order to use an alternative context users can specify the desired context via the Spark configuration property\nspark.kubernetes.context\ne.g.\nspark.kubernetes.context=minikube\n.\nNamespaces\nKubernetes has the concept of\nnamespaces\n.\nNamespaces are ways to divide cluster resources between multiple users (via resource quota). Spark on Kubernetes can\nuse namespaces to launch Spark applications. This can be made use of through the\nspark.kubernetes.namespace\nconfiguration.\nKubernetes allows using\nResourceQuota\nto set limits on\nresources, number of objects, etc on individual namespaces. Namespaces and ResourceQuota can be used in combination by\nadministrator to control sharing and resource allocation in a Kubernetes cluster running Spark applications.\nRBAC\nIn Kubernetes clusters with\nRBAC\nenabled, users can configure\nKubernetes RBAC roles and service accounts used by the various Spark on Kubernetes components to access the Kubernetes\nAPI server.\nThe Spark driver pod uses a Kubernetes service account to access the Kubernetes API server to create and watch executor\npods. The service account used by the driver pod must have the appropriate permission for the driver to be able to do\nits work. Specifically, at minimum, the service account must be granted a\nRole\nor\nClusterRole\nthat allows driver\npods to create pods and services. By default, the driver pod is automatically assigned the\ndefault\nservice account in\nthe namespace specified by\nspark.kubernetes.namespace\n, if no service account is specified when the pod gets created.\nDepending on the version and setup of Kubernetes deployed, this\ndefault\nservice account may or may not have the role\nthat allows driver pods to create pods and services under the default Kubernetes\nRBAC\npolicies. Sometimes users may need to specify a custom\nservice account that has the right role granted. Spark on Kubernetes supports specifying a custom service account to\nbe used by the driver pod through the configuration property\nspark.kubernetes.authenticate.driver.serviceAccountName=<service account name>\n. For example, to make the driver pod\nuse the\nspark\nservice account, a user simply adds the following option to the\nspark-submit\ncommand:\n--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark\nTo create a custom service account, a user can use the\nkubectl create serviceaccount\ncommand. For example, the\nfollowing command creates a service account named\nspark\n:\n$\nkubectl create serviceaccount spark\nTo grant a service account a\nRole\nor\nClusterRole\n, a\nRoleBinding\nor\nClusterRoleBinding\nis needed. To create\na\nRoleBinding\nor\nClusterRoleBinding\n, a user can use the\nkubectl create rolebinding\n(or\nclusterrolebinding\nfor\nClusterRoleBinding\n) command. For example, the following command creates an\nedit\nClusterRole\nin the\ndefault\nnamespace and grants it to the\nspark\nservice account created above:\n$\nkubectl create clusterrolebinding spark-role\n--clusterrole\n=\nedit\n--serviceaccount\n=\ndefault:spark\n--namespace\n=\ndefault\nNote that a\nRole\ncan only be used to grant access to resources (like pods) within a single namespace, whereas a\nClusterRole\ncan be used to grant access to cluster-scoped resources (like nodes) as well as namespaced resources\n(like pods) across all namespaces. For Spark on Kubernetes, since the driver always creates executor pods in the\nsame namespace, a\nRole\nis sufficient, although users may use a\nClusterRole\ninstead. For more information on\nRBAC authorization and how to configure Kubernetes service accounts for pods, please refer to\nUsing RBAC Authorization\nand\nConfigure Service Accounts for Pods\n.\nSpark Application Management\nKubernetes provides simple application management via the spark-submit CLI tool in cluster mode.\nUsers can kill a job by providing the submission ID that is printed when submitting their job.\nThe submission ID follows the format\nnamespace:driver-pod-name\n.\nIf user omits the namespace then the namespace set in current k8s context is used.\nFor example if user has set a specific namespace as follows\nkubectl config set-context minikube --namespace=spark\nthen the\nspark\nnamespace will be used by default. On the other hand, if there is no namespace added to the specific context\nthen all namespaces will be considered by default. That means operations will affect all Spark applications matching the given submission ID regardless of namespace.\nMoreover, spark-submit for application management uses the same backend code that is used for submitting the driver, so the same properties\nlike\nspark.kubernetes.context\netc., can be re-used.\nFor example:\n$\nspark-submit\n--kill\nspark:spark-pi-1547948636094-driver\n--master\nk8s://https://192.168.2.8:8443\nUsers also can list the application status by using the\n--status\nflag:\n$\nspark-submit\n--status\nspark:spark-pi-1547948636094-driver\n--master\nk8s://https://192.168.2.8:8443\nBoth operations support glob patterns. For example user can run:\n$\nspark-submit\n--kill\nspark:spark-pi\n*\n--master\nk8s://https://192.168.2.8:8443\nThe above will kill all application with the specific prefix.\nUser can specify the grace period for pod termination via the\nspark.kubernetes.appKillPodDeletionGracePeriod\nproperty,\nusing\n--conf\nas means to provide it (default value for all K8s pods is\n30 secs\n).\nFuture Work\nThere are several Spark on Kubernetes features that are currently being worked on or planned to be worked on. Those features are expected to eventually make it into future versions of the spark-kubernetes integration.\nSome of these include:\nExternal Shuffle Service\nJob Queues and Resource Management\nConfiguration\nSee the\nconfiguration page\nfor information on Spark configurations.  The following configurations are specific to Spark on Kubernetes.\nSpark Properties\nProperty Name\nDefault\nMeaning\nSince Version\nspark.kubernetes.context\n(none)\nThe context from the user Kubernetes configuration file used for the initial\n    auto-configuration of the Kubernetes client library.  When not specified then\n    the users current context is used.\nNB:\nMany of the\n    auto-configured settings can be overridden by the use of other Spark\n    configuration properties e.g.\nspark.kubernetes.namespace\n.\n3.0.0\nspark.kubernetes.driver.master\nhttps://kubernetes.default.svc\nThe internal Kubernetes master (API server) address to be used for driver to request executors or\n    'local[*]' for driver-pod-only mode.\n3.0.0\nspark.kubernetes.namespace\ndefault\nThe namespace that will be used for running the driver and executor pods.\n2.3.0\nspark.kubernetes.container.image\n(none)\nContainer image to use for the Spark application.\n    This is usually of the form\nexample.com/repo/spark:v1.0.0\n.\n    This configuration is required and must be provided by the user, unless explicit\n    images are provided for each different container type.\n2.3.0\nspark.kubernetes.driver.container.image\n(value of spark.kubernetes.container.image)\nCustom container image to use for the driver.\n2.3.0\nspark.kubernetes.executor.container.image\n(value of spark.kubernetes.container.image)\nCustom container image to use for executors.\n2.3.0\nspark.kubernetes.container.image.pullPolicy\nIfNotPresent\nContainer image pull policy used when pulling images within Kubernetes.\n    Valid values are\nAlways\n,\nNever\n, and\nIfNotPresent\n.\n2.3.0\nspark.kubernetes.container.image.pullSecrets\nComma separated list of Kubernetes secrets used to pull images from private image registries.\n2.4.0\nspark.kubernetes.allocation.batch.size\n10\nNumber of pods to launch at once in each round of executor pod allocation.\n2.3.0\nspark.kubernetes.allocation.batch.delay\n1s\nTime to wait between each round of executor pod allocation. Specifying values less than 1 second may lead to\n    excessive CPU usage on the spark driver.\n2.3.0\nspark.kubernetes.jars.avoidDownloadSchemes\n(none)\nComma-separated list of schemes for which jars will NOT be downloaded to the \n    driver local disk prior to be distributed to executors, only for kubernetes deployment. \n    For use in cases when the jars are big and executor counts are high, \n    concurrent download causes network saturation and timeouts. \n    Wildcard '*' is denoted to not downloading jars for any the schemes.\n4.0.0\nspark.kubernetes.authenticate.submission.caCertFile\n(none)\nPath to the CA cert file for connecting to the Kubernetes API server over TLS when starting the driver. This file\n    must be located on the submitting machine's disk. Specify this as a path as opposed to a URI (i.e. do not provide\n    a scheme). In client mode, use\nspark.kubernetes.authenticate.caCertFile\ninstead.\n2.3.0\nspark.kubernetes.authenticate.submission.clientKeyFile\n(none)\nPath to the client key file for authenticating against the Kubernetes API server when starting the driver. This file\n    must be located on the submitting machine's disk. Specify this as a path as opposed to a URI (i.e. do not provide\n    a scheme). In client mode, use\nspark.kubernetes.authenticate.clientKeyFile\ninstead.\n2.3.0\nspark.kubernetes.authenticate.submission.clientCertFile\n(none)\nPath to the client cert file for authenticating against the Kubernetes API server when starting the driver. This\n    file must be located on the submitting machine's disk. Specify this as a path as opposed to a URI (i.e. do not\n    provide a scheme). In client mode, use\nspark.kubernetes.authenticate.clientCertFile\ninstead.\n2.3.0\nspark.kubernetes.authenticate.submission.oauthToken\n(none)\nOAuth token to use when authenticating against the Kubernetes API server when starting the driver. Note\n    that unlike the other authentication options, this is expected to be the exact string value of the token to use for\n    the authentication. In client mode, use\nspark.kubernetes.authenticate.oauthToken\ninstead.\n2.3.0\nspark.kubernetes.authenticate.submission.oauthTokenFile\n(none)\nPath to the OAuth token file containing the token to use when authenticating against the Kubernetes API server when starting the driver.\n    This file must be located on the submitting machine's disk. Specify this as a path as opposed to a URI (i.e. do not\n    provide a scheme). In client mode, use\nspark.kubernetes.authenticate.oauthTokenFile\ninstead.\n2.3.0\nspark.kubernetes.authenticate.driver.caCertFile\n(none)\nPath to the CA cert file for connecting to the Kubernetes API server over TLS from the driver pod when requesting\n    executors. This file must be located on the submitting machine's disk, and will be uploaded to the driver pod.\n    Specify this as a path as opposed to a URI (i.e. do not provide a scheme). In client mode, use\nspark.kubernetes.authenticate.caCertFile\ninstead.\n2.3.0\nspark.kubernetes.authenticate.driver.clientKeyFile\n(none)\nPath to the client key file for authenticating against the Kubernetes API server from the driver pod when requesting\n    executors. This file must be located on the submitting machine's disk, and will be uploaded to the driver pod as\n    a Kubernetes secret. Specify this as a path as opposed to a URI (i.e. do not provide a scheme).\n    In client mode, use\nspark.kubernetes.authenticate.clientKeyFile\ninstead.\n2.3.0\nspark.kubernetes.authenticate.driver.clientCertFile\n(none)\nPath to the client cert file for authenticating against the Kubernetes API server from the driver pod when\n    requesting executors. This file must be located on the submitting machine's disk, and will be uploaded to the\n    driver pod as a Kubernetes secret. Specify this as a path as opposed to a URI (i.e. do not provide a scheme).\n    In client mode, use\nspark.kubernetes.authenticate.clientCertFile\ninstead.\n2.3.0\nspark.kubernetes.authenticate.driver.oauthToken\n(none)\nOAuth token to use when authenticating against the Kubernetes API server from the driver pod when\n    requesting executors. Note that unlike the other authentication options, this must be the exact string value of\n    the token to use for the authentication. This token value is uploaded to the driver pod as a Kubernetes secret.\n    In client mode, use\nspark.kubernetes.authenticate.oauthToken\ninstead.\n2.3.0\nspark.kubernetes.authenticate.driver.oauthTokenFile\n(none)\nPath to the OAuth token file containing the token to use when authenticating against the Kubernetes API server from the driver pod when\n    requesting executors. Note that unlike the other authentication options, this file must contain the exact string value of\n    the token to use for the authentication. This token value is uploaded to the driver pod as a secret. In client mode, use\nspark.kubernetes.authenticate.oauthTokenFile\ninstead.\n2.3.0\nspark.kubernetes.authenticate.driver.mounted.caCertFile\n(none)\nPath to the CA cert file for connecting to the Kubernetes API server over TLS from the driver pod when requesting\n    executors. This path must be accessible from the driver pod.\n    Specify this as a path as opposed to a URI (i.e. do not provide a scheme). In client mode, use\nspark.kubernetes.authenticate.caCertFile\ninstead.\n2.3.0\nspark.kubernetes.authenticate.driver.mounted.clientKeyFile\n(none)\nPath to the client key file for authenticating against the Kubernetes API server from the driver pod when requesting\n    executors. This path must be accessible from the driver pod.\n    Specify this as a path as opposed to a URI (i.e. do not provide a scheme). In client mode, use\nspark.kubernetes.authenticate.clientKeyFile\ninstead.\n2.3.0\nspark.kubernetes.authenticate.driver.mounted.clientCertFile\n(none)\nPath to the client cert file for authenticating against the Kubernetes API server from the driver pod when\n    requesting executors. This path must be accessible from the driver pod.\n    Specify this as a path as opposed to a URI (i.e. do not provide a scheme). In client mode, use\nspark.kubernetes.authenticate.clientCertFile\ninstead.\n2.3.0\nspark.kubernetes.authenticate.driver.mounted.oauthTokenFile\n(none)\nPath to the file containing the OAuth token to use when authenticating against the Kubernetes API server from the driver pod when\n    requesting executors. This path must be accessible from the driver pod.\n    Note that unlike the other authentication options, this file must contain the exact string value of the token to use\n    for the authentication. In client mode, use\nspark.kubernetes.authenticate.oauthTokenFile\ninstead.\n2.3.0\nspark.kubernetes.authenticate.driver.serviceAccountName\ndefault\nService account that is used when running the driver pod. The driver pod uses this service account when requesting\n    executor pods from the API server. Note that this cannot be specified alongside a CA cert file, client key file,\n    client cert file, and/or OAuth token. In client mode, use\nspark.kubernetes.authenticate.serviceAccountName\ninstead.\n2.3.0\nspark.kubernetes.authenticate.executor.serviceAccountName\n(value of spark.kubernetes.authenticate.driver.serviceAccountName)\nService account that is used when running the executor pod.\n    If this parameter is not setup, the fallback logic will use the driver's service account.\n3.1.0\nspark.kubernetes.authenticate.caCertFile\n(none)\nIn client mode, path to the CA cert file for connecting to the Kubernetes API server over TLS when\n    requesting executors. Specify this as a path as opposed to a URI (i.e. do not provide a scheme).\n2.4.0\nspark.kubernetes.authenticate.clientKeyFile\n(none)\nIn client mode, path to the client key file for authenticating against the Kubernetes API server\n    when requesting executors. Specify this as a path as opposed to a URI (i.e. do not provide a scheme).\n2.4.0\nspark.kubernetes.authenticate.clientCertFile\n(none)\nIn client mode, path to the client cert file for authenticating against the Kubernetes API server\n    when requesting executors. Specify this as a path as opposed to a URI (i.e. do not provide a scheme).\n2.4.0\nspark.kubernetes.authenticate.oauthToken\n(none)\nIn client mode, the OAuth token to use when authenticating against the Kubernetes API server when\n    requesting executors. Note that unlike the other authentication options, this must be the exact string value of\n    the token to use for the authentication.\n2.4.0\nspark.kubernetes.authenticate.oauthTokenFile\n(none)\nIn client mode, path to the file containing the OAuth token to use when authenticating against the Kubernetes API\n    server when requesting executors.\n2.4.0\nspark.kubernetes.driver.label.[LabelName]\n(none)\nAdd the label specified by\nLabelName\nto the driver pod.\n    For example,\nspark.kubernetes.driver.label.something=true\n.\n    Note that Spark also adds its own labels to the driver pod\n    for bookkeeping purposes.\n2.3.0\nspark.kubernetes.driver.annotation.[AnnotationName]\n(none)\nAdd the Kubernetes\nannotation\nspecified by\nAnnotationName\nto the driver pod.\n    For example,\nspark.kubernetes.driver.annotation.something=true\n.\n2.3.0\nspark.kubernetes.driver.service.label.[LabelName]\n(none)\nAdd the Kubernetes\nlabel\nspecified by\nLabelName\nto the driver service.\n    For example,\nspark.kubernetes.driver.service.label.something=true\n.\n    Note that Spark also adds its own labels to the driver service\n    for bookkeeping purposes.\n3.4.0\nspark.kubernetes.driver.service.annotation.[AnnotationName]\n(none)\nAdd the Kubernetes\nannotation\nspecified by\nAnnotationName\nto the driver service.\n    For example,\nspark.kubernetes.driver.service.annotation.something=true\n.\n3.0.0\nspark.kubernetes.executor.label.[LabelName]\n(none)\nAdd the label specified by\nLabelName\nto the executor pods.\n    For example,\nspark.kubernetes.executor.label.something=true\n.\n    Note that Spark also adds its own labels to the executor pod\n    for bookkeeping purposes.\n2.3.0\nspark.kubernetes.executor.annotation.[AnnotationName]\n(none)\nAdd the Kubernetes\nannotation\nspecified by\nAnnotationName\nto the executor pods.\n    For example,\nspark.kubernetes.executor.annotation.something=true\n.\n2.3.0\nspark.kubernetes.driver.pod.name\n(none)\nName of the driver pod. In cluster mode, if this is not set, the driver pod name is set to \"spark.app.name\"\n    suffixed by the current timestamp to avoid name conflicts. In client mode, if your application is running\n    inside a pod, it is highly recommended to set this to the name of the pod your driver is running in. Setting this\n    value in client mode allows the driver to become the owner of its executor pods, which in turn allows the executor\n    pods to be garbage collected by the cluster.\n2.3.0\nspark.kubernetes.executor.podNamePrefix\n(none)\nPrefix to use in front of the executor pod names. It must conform the rules defined by the Kubernetes\nDNS Label Names\n.\n    The prefix will be used to generate executor pod names in the form of\n\\$podNamePrefix-exec-\\$id\n, where the `id` is\n    a positive int value, so the length of the `podNamePrefix` needs to be less than or equal to 47(= 63 - 10 - 6).\n2.3.0\nspark.kubernetes.submission.waitAppCompletion\ntrue\nIn cluster mode, whether to wait for the application to finish before exiting the launcher process.  When changed to\n    false, the launcher has a \"fire-and-forget\" behavior when launching the Spark job.\n2.3.0\nspark.kubernetes.report.interval\n1s\nInterval between reports of the current Spark job status in cluster mode.\n2.3.0\nspark.kubernetes.executor.apiPollingInterval\n30s\nInterval between polls against the Kubernetes API server to inspect the state of executors.\n2.4.0\nspark.kubernetes.driver.request.cores\n(none)\nSpecify the cpu request for the driver pod. Values conform to the Kubernetes\nconvention\n.\n    Example values include 0.1, 500m, 1.5, 5, etc., with the definition of cpu units documented in\nCPU units\n.\n    This takes precedence over\nspark.driver.cores\nfor specifying the driver pod cpu request if set.\n3.0.0\nspark.kubernetes.driver.limit.cores\n(none)\nSpecify a hard cpu\nlimit\nfor the driver pod.\n2.3.0\nspark.kubernetes.executor.request.cores\n(none)\nSpecify the cpu request for each executor pod. Values conform to the Kubernetes\nconvention\n.\n    Example values include 0.1, 500m, 1.5, 5, etc., with the definition of cpu units documented in\nCPU units\n.\n    This is distinct from\nspark.executor.cores\n: it is only used and takes precedence over\nspark.executor.cores\nfor specifying the executor pod cpu request if set. Task\n    parallelism, e.g., number of tasks an executor can run concurrently is not affected by this.\n2.4.0\nspark.kubernetes.executor.limit.cores\n(none)\nSpecify a hard cpu\nlimit\nfor each executor pod launched for the Spark Application.\n2.3.0\nspark.kubernetes.node.selector.[labelKey]\n(none)\nAdds to the node selector of the driver pod and executor pods, with key\nlabelKey\nand the value as the\n    configuration's value. For example, setting\nspark.kubernetes.node.selector.identifier\nto\nmyIdentifier\nwill result in the driver pod and executors having a node selector with key\nidentifier\nand value\nmyIdentifier\n. Multiple node selector keys can be added by setting multiple configurations with this prefix.\n2.3.0\nspark.kubernetes.driver.node.selector.[labelKey]\n(none)\nAdds to the driver node selector of the driver pod, with key\nlabelKey\nand the value as the\n    configuration's value. For example, setting\nspark.kubernetes.driver.node.selector.identifier\nto\nmyIdentifier\nwill result in the driver pod having a node selector with key\nidentifier\nand value\nmyIdentifier\n. Multiple driver node selector keys can be added by setting multiple configurations with this prefix.\n3.3.0\nspark.kubernetes.executor.node.selector.[labelKey]\n(none)\nAdds to the executor node selector of the executor pods, with key\nlabelKey\nand the value as the\n    configuration's value. For example, setting\nspark.kubernetes.executor.node.selector.identifier\nto\nmyIdentifier\nwill result in the executors having a node selector with key\nidentifier\nand value\nmyIdentifier\n. Multiple executor node selector keys can be added by setting multiple configurations with this prefix.\n3.3.0\nspark.kubernetes.driverEnv.[EnvironmentVariableName]\n(none)\nAdd the environment variable specified by\nEnvironmentVariableName\nto\n    the Driver process. The user can specify multiple of these to set multiple environment variables.\n2.3.0\nspark.kubernetes.driver.secrets.[SecretName]\n(none)\nAdd the\nKubernetes Secret\nnamed\nSecretName\nto the driver pod on the path specified in the value. For example,\nspark.kubernetes.driver.secrets.spark-secret=/etc/secrets\n.\n2.3.0\nspark.kubernetes.executor.secrets.[SecretName]\n(none)\nAdd the\nKubernetes Secret\nnamed\nSecretName\nto the executor pod on the path specified in the value. For example,\nspark.kubernetes.executor.secrets.spark-secret=/etc/secrets\n.\n2.3.0\nspark.kubernetes.driver.secretKeyRef.[EnvName]\n(none)\nAdd as an environment variable to the driver container with name EnvName (case sensitive), the value referenced by key\nkey\nin the data of the referenced\nKubernetes Secret\n. For example,\nspark.kubernetes.driver.secretKeyRef.ENV_VAR=spark-secret:key\n.\n2.4.0\nspark.kubernetes.executor.secretKeyRef.[EnvName]\n(none)\nAdd as an environment variable to the executor container with name EnvName (case sensitive), the value referenced by key\nkey\nin the data of the referenced\nKubernetes Secret\n. For example,\nspark.kubernetes.executor.secrets.ENV_VAR=spark-secret:key\n.\n2.4.0\nspark.kubernetes.driver.volumes.[VolumeType].[VolumeName].mount.path\n(none)\nAdd the\nKubernetes Volume\nnamed\nVolumeName\nof the\nVolumeType\ntype to the driver pod on the path specified in the value. For example,\nspark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.path=/checkpoint\n.\n2.4.0\nspark.kubernetes.driver.volumes.[VolumeType].[VolumeName].mount.subPath\n(none)\nSpecifies a\nsubpath\nto be mounted from the volume into the driver pod.\nspark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.subPath=checkpoint\n.\n3.0.0\nspark.kubernetes.driver.volumes.[VolumeType].[VolumeName].mount.readOnly\n(none)\nSpecify if the mounted volume is read only or not. For example,\nspark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.readOnly=false\n.\n2.4.0\nspark.kubernetes.driver.volumes.[VolumeType].[VolumeName].options.[OptionName]\n(none)\nConfigure\nKubernetes Volume\noptions passed to the Kubernetes with\nOptionName\nas key having specified value, must conform with Kubernetes option format. For example,\nspark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.options.claimName=spark-pvc-claim\n.\n2.4.0\nspark.kubernetes.driver.volumes.[VolumeType].[VolumeName].label.[LabelName]\n(none)\nConfigure\nKubernetes Volume\nlabels passed to the Kubernetes with\nLabelName\nas key having specified value, must conform with Kubernetes label format. For example,\nspark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.label.foo=bar\n.\n4.0.0\nspark.kubernetes.driver.volumes.[VolumeType].[VolumeName].annotation.[AnnotationName]\n(none)\nConfigure\nKubernetes Volume\nannotations passed to the Kubernetes with\nAnnotationName\nas key having specified value, must conform with Kubernetes annotations format. For example,\nspark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.annotation.foo=bar\n.\n4.0.0\nspark.kubernetes.executor.volumes.[VolumeType].[VolumeName].mount.path\n(none)\nAdd the\nKubernetes Volume\nnamed\nVolumeName\nof the\nVolumeType\ntype to the executor pod on the path specified in the value. For example,\nspark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.path=/checkpoint\n.\n2.4.0\nspark.kubernetes.executor.volumes.[VolumeType].[VolumeName].mount.subPath\n(none)\nSpecifies a\nsubpath\nto be mounted from the volume into the executor pod.\nspark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.subPath=checkpoint\n.\n3.0.0\nspark.kubernetes.executor.volumes.[VolumeType].[VolumeName].mount.readOnly\nfalse\nSpecify if the mounted volume is read only or not. For example,\nspark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.readOnly=false\n.\n2.4.0\nspark.kubernetes.executor.volumes.[VolumeType].[VolumeName].options.[OptionName]\n(none)\nConfigure\nKubernetes Volume\noptions passed to the Kubernetes with\nOptionName\nas key having specified value. For example,\nspark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.options.claimName=spark-pvc-claim\n.\n2.4.0\nspark.kubernetes.executor.volumes.[VolumeType].[VolumeName].label.[LabelName]\n(none)\nConfigure\nKubernetes Volume\nlabels passed to the Kubernetes with\nLabelName\nas key having specified value, must conform with Kubernetes label format. For example,\nspark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.label.foo=bar\n.\n4.0.0\nspark.kubernetes.executor.volumes.[VolumeType].[VolumeName].annotation.[AnnotationName]\n(none)\nConfigure\nKubernetes Volume\nannotations passed to the Kubernetes with\nAnnotationName\nas key having specified value, must conform with Kubernetes annotations format. For example,\nspark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.annotation.foo=bar\n.\n4.0.0\nspark.kubernetes.local.dirs.tmpfs\nfalse\nConfigure the\nemptyDir\nvolumes used to back\nSPARK_LOCAL_DIRS\nwithin the Spark driver and executor pods to use\ntmpfs\nbacking i.e. RAM.  See\nLocal Storage\nearlier on this page\n   for more discussion of this.\n3.0.0\nspark.kubernetes.memoryOverheadFactor\n0.1\nThis sets the Memory Overhead Factor that will allocate memory to non-JVM memory, which includes off-heap memory allocations, non-JVM tasks, various systems processes, and\ntmpfs\n-based local directories when\nspark.kubernetes.local.dirs.tmpfs\nis\ntrue\n. For JVM-based jobs this value will default to 0.10 and 0.40 for non-JVM jobs.\n    This is done as non-JVM tasks need more non-JVM heap space and such tasks commonly fail with \"Memory Overhead Exceeded\" errors. This preempts this error with a higher default.\n    This will be overridden by the value set by\nspark.driver.memoryOverheadFactor\nand\nspark.executor.memoryOverheadFactor\nexplicitly.\n2.4.0\nspark.kubernetes.pyspark.pythonVersion\n\"3\"\nThis sets the major Python version of the docker image used to run the driver and executor containers.\n   It can be only \"3\". This configuration was deprecated from Spark 3.1.0, and is effectively no-op.\n   Users should set 'spark.pyspark.python' and 'spark.pyspark.driver.python' configurations or\n   'PYSPARK_PYTHON' and 'PYSPARK_DRIVER_PYTHON' environment variables.\n2.4.0\nspark.kubernetes.kerberos.krb5.path\n(none)\nSpecify the local location of the krb5.conf file to be mounted on the driver and executors for Kerberos interaction.\n   It is important to note that the KDC defined needs to be visible from inside the containers.\n3.0.0\nspark.kubernetes.kerberos.krb5.configMapName\n(none)\nSpecify the name of the ConfigMap, containing the krb5.conf file, to be mounted on the driver and executors\n   for Kerberos interaction. The KDC defined needs to be visible from inside the containers. The ConfigMap must also\n   be in the same namespace of the driver and executor pods.\n3.0.0\nspark.kubernetes.hadoop.configMapName\n(none)\nSpecify the name of the ConfigMap, containing the HADOOP_CONF_DIR files, to be mounted on the driver\n    and executors for custom Hadoop configuration.\n3.0.0\nspark.kubernetes.kerberos.tokenSecret.name\n(none)\nSpecify the name of the secret where your existing delegation tokens are stored. This removes the need for the job user\n    to provide any kerberos credentials for launching a job.\n3.0.0\nspark.kubernetes.kerberos.tokenSecret.itemKey\n(none)\nSpecify the item key of the data where your existing delegation tokens are stored. This removes the need for the job user\n    to provide any kerberos credentials for launching a job.\n3.0.0\nspark.kubernetes.driver.podTemplateFile\n(none)\nSpecify the local file that contains the driver\npod template\n. For example\nspark.kubernetes.driver.podTemplateFile=/path/to/driver-pod-template.yaml\n3.0.0\nspark.kubernetes.driver.podTemplateContainerName\n(none)\nSpecify the container name to be used as a basis for the driver in the given\npod template\n.\n   For example\nspark.kubernetes.driver.podTemplateContainerName=spark-driver\n3.0.0\nspark.kubernetes.executor.podTemplateFile\n(none)\nSpecify the local file that contains the executor\npod template\n. For example\nspark.kubernetes.executor.podTemplateFile=/path/to/executor-pod-template.yaml\n3.0.0\nspark.kubernetes.executor.podTemplateContainerName\n(none)\nSpecify the container name to be used as a basis for the executor in the given\npod template\n.\n   For example\nspark.kubernetes.executor.podTemplateContainerName=spark-executor\n3.0.0\nspark.kubernetes.executor.deleteOnTermination\ntrue\nSpecify whether executor pods should be deleted in case of failure or normal termination.\n3.0.0\nspark.kubernetes.executor.checkAllContainers\ntrue\nSpecify whether executor pods should be check all containers (including sidecars) or only the executor container when determining the pod status.\n3.1.0\nspark.kubernetes.submission.connectionTimeout\n10000\nConnection timeout in milliseconds for the kubernetes client to use for starting the driver.\n3.0.0\nspark.kubernetes.submission.requestTimeout\n10000\nRequest timeout in milliseconds for the kubernetes client to use for starting the driver.\n3.0.0\nspark.kubernetes.trust.certificates\nfalse\nIf set to true then client can submit to kubernetes cluster only with token.\n3.2.0\nspark.kubernetes.driver.connectionTimeout\n10000\nConnection timeout in milliseconds for the kubernetes client in driver to use when requesting executors.\n3.0.0\nspark.kubernetes.driver.requestTimeout\n10000\nRequest timeout in milliseconds for the kubernetes client in driver to use when requesting executors.\n3.0.0\nspark.kubernetes.appKillPodDeletionGracePeriod\n(none)\nSpecify the grace period in seconds when deleting a Spark application using spark-submit.\n3.0.0\nspark.kubernetes.dynamicAllocation.deleteGracePeriod\n5s\nHow long to wait for executors to shut down gracefully before a forceful kill.\n3.0.0\nspark.kubernetes.file.upload.path\n(none)\nPath to store files at the spark submit side in cluster mode. For example:\nspark.kubernetes.file.upload.path=s3a://<s3-bucket>/path\nFile should specified as\nfile://path/to/file\nor absolute path.\n3.0.0\nspark.kubernetes.executor.decommissionLabel\n(none)\nLabel to be applied to pods which are exiting or being decommissioned. Intended for use\n    with pod disruption budgets, deletion costs, and similar.\n3.3.0\nspark.kubernetes.executor.decommissionLabelValue\n(none)\nValue to be applied with the label when\nspark.kubernetes.executor.decommissionLabel\nis enabled.\n3.3.0\nspark.kubernetes.executor.scheduler.name\n(none)\nSpecify the scheduler name for each executor pod.\n3.0.0\nspark.kubernetes.driver.scheduler.name\n(none)\nSpecify the scheduler name for driver pod.\n3.3.0\nspark.kubernetes.scheduler.name\n(none)\nSpecify the scheduler name for driver and executor pods. If `spark.kubernetes.driver.scheduler.name` or\n    `spark.kubernetes.executor.scheduler.name` is set, will override this.\n3.3.0\nspark.kubernetes.configMap.maxSize\n1048576\nMax size limit for a config map.\n    This is configurable as per\nlimit\non k8s server end.\n3.1.0\nspark.kubernetes.executor.missingPodDetectDelta\n30s\nWhen a registered executor's POD is missing from the Kubernetes API server's polled\n    list of PODs then this delta time is taken as the accepted time difference between the\n    registration time and the time of the polling. After this time the POD is considered\n    missing from the cluster and the executor will be removed.\n3.1.1\nspark.kubernetes.decommission.script\n/opt/decom.sh\nThe location of the script to use for graceful decommissioning.\n3.2.0\nspark.kubernetes.driver.service.deleteOnTermination\ntrue\nIf true, driver service will be deleted on Spark application termination. If false, it will be cleaned up when the driver pod is deletion.\n3.2.0\nspark.kubernetes.driver.service.ipFamilyPolicy\nSingleStack\nK8s IP Family Policy for Driver Service. Valid values are\nSingleStack\n,\nPreferDualStack\n, and\nRequireDualStack\n.\n3.4.0\nspark.kubernetes.driver.service.ipFamilies\nIPv4\nA list of IP families for K8s Driver Service. Valid values are\nIPv4\nand\nIPv6\n.\n3.4.0\nspark.kubernetes.driver.ownPersistentVolumeClaim\ntrue\nIf true, driver pod becomes the owner of on-demand persistent volume claims instead of the executor pods\n3.2.0\nspark.kubernetes.driver.reusePersistentVolumeClaim\ntrue\nIf true, driver pod tries to reuse driver-owned on-demand persistent volume claims\n    of the deleted executor pods if exists. This can be useful to reduce executor pod\n    creation delay by skipping persistent volume creations. Note that a pod in\n    `Terminating` pod status is not a deleted pod by definition and its resources\n    including persistent volume claims are not reusable yet. Spark will create new\n    persistent volume claims when there exists no reusable one. In other words, the total\n    number of persistent volume claims can be larger than the number of running executors\n    sometimes. This config requires\nspark.kubernetes.driver.ownPersistentVolumeClaim=true.\n3.2.0\nspark.kubernetes.driver.waitToReusePersistentVolumeClaim\nfalse\nIf true, driver pod counts the number of created on-demand persistent volume claims\n    and wait if the number is greater than or equal to the total number of volumes which\n    the Spark job is able to have. This config requires both\nspark.kubernetes.driver.ownPersistentVolumeClaim=true\nand\nspark.kubernetes.driver.reusePersistentVolumeClaim=true.\n3.4.0\nspark.kubernetes.executor.disableConfigMap\nfalse\nIf true, disable ConfigMap creation for executors.\n3.2.0\nspark.kubernetes.driver.pod.featureSteps\n(none)\nClass names of an extra driver pod feature step implementing\n    `KubernetesFeatureConfigStep`. This is a developer API. Comma separated.\n    Runs after all of Spark internal feature steps. Since 3.3.0, your driver feature step\n    can implement `KubernetesDriverCustomFeatureConfigStep` where the driver config\n    is also available.\n3.2.0\nspark.kubernetes.executor.pod.featureSteps\n(none)\nClass names of an extra executor pod feature step implementing\n    `KubernetesFeatureConfigStep`. This is a developer API. Comma separated.\n    Runs after all of Spark internal feature steps. Since 3.3.0, your executor feature step\n    can implement `KubernetesExecutorCustomFeatureConfigStep` where the executor config\n    is also available.\n3.2.0\nspark.kubernetes.allocation.maxPendingPods\nInt.MaxValue\nMaximum number of pending PODs allowed during executor allocation for this\n    application. Those newly requested executors which are unknown by Kubernetes yet are\n    also counted into this limit as they will change into pending PODs by time.\n    This limit is independent from the resource profiles as it limits the sum of all\n    allocation for all the used resource profiles.\n3.2.0\nspark.kubernetes.allocation.pods.allocator\ndirect\nAllocator to use for pods. Possible values are\ndirect\n(the default)\n    and\nstatefulset\n, or a full class name of a class implementing `AbstractPodsAllocator`.\n    Future version may add Job or replicaset. This is a developer API and may change\n    or be removed at anytime.\n3.3.0\nspark.kubernetes.allocation.executor.timeout\n600s\nTime to wait before a newly created executor POD request, which does not reached\n    the POD pending state yet, considered timedout and will be deleted.\n3.1.0\nspark.kubernetes.allocation.driver.readinessTimeout\n1s\nTime to wait for driver pod to get ready before creating executor pods. This wait\n    only happens on application start. If timeout happens, executor pods will still be\n    created.\n3.1.3\nspark.kubernetes.executor.enablePollingWithResourceVersion\nfalse\nIf true, `resourceVersion` is set with `0` during invoking pod listing APIs\n    in order to allow API Server-side caching. This should be used carefully.\n3.3.0\nspark.kubernetes.executor.eventProcessingInterval\n1s\nInterval between successive inspection of executor events sent from the Kubernetes API.\n2.4.0\nspark.kubernetes.executor.rollInterval\n0s\nInterval between executor roll operations. It's disabled by default with `0s`.\n3.3.0\nspark.kubernetes.executor.minTasksPerExecutorBeforeRolling\n0\nThe minimum number of tasks per executor before rolling.\n    Spark will not roll executors whose total number of tasks is smaller\n    than this configuration. The default value is zero.\n3.3.0\nspark.kubernetes.executor.rollPolicy\nOUTLIER\nExecutor roll policy: Valid values are ID, ADD_TIME, TOTAL_GC_TIME,\n    TOTAL_DURATION, FAILED_TASKS, and OUTLIER (default).\n    When executor roll happens, Spark uses this policy to choose\n    an executor and decommission it. The built-in policies are based on executor summary\n    and newly started executors are protected by spark.kubernetes.executor.minTasksPerExecutorBeforeRolling.\n    ID policy chooses an executor with the smallest executor ID.\n    ADD_TIME policy chooses an executor with the smallest add-time.\n    TOTAL_GC_TIME policy chooses an executor with the biggest total task GC time.\n    TOTAL_DURATION policy chooses an executor with the biggest total task time.\n    AVERAGE_DURATION policy chooses an executor with the biggest average task time.\n    FAILED_TASKS policy chooses an executor with the most number of failed tasks.\n    OUTLIER policy chooses an executor with outstanding statistics which is bigger than\n    at least two standard deviation from the mean in average task time,\n    total task time, total task GC time, and the number of failed tasks if exists.\n    If there is no outlier, it works like TOTAL_DURATION policy.\n3.3.0\nPod template properties\nSee the below table for the full list of pod specifications that will be overwritten by spark.\nPod Metadata\nPod metadata key\nModified value\nDescription\nname\nValue of\nspark.kubernetes.driver.pod.name\nThe driver pod name will be overwritten with either the configured or default value of\nspark.kubernetes.driver.pod.name\n. The executor pod names will be unaffected.\nnamespace\nValue of\nspark.kubernetes.namespace\nSpark makes strong assumptions about the driver and executor namespaces. Both driver and executor namespaces will\n    be replaced by either the configured or default spark conf value.\nlabels\nAdds the labels from\nspark.kubernetes.{driver,executor}.label.*\nSpark will add additional labels specified by the spark configuration.\nannotations\nAdds the annotations from\nspark.kubernetes.{driver,executor}.annotation.*\nSpark will add additional annotations specified by the spark configuration.\nPod Spec\nPod spec key\nModified value\nDescription\nimagePullSecrets\nAdds image pull secrets from\nspark.kubernetes.container.image.pullSecrets\nAdditional pull secrets will be added from the spark configuration to both executor pods.\nnodeSelector\nAdds node selectors from\nspark.kubernetes.node.selector.*\nAdditional node selectors will be added from the spark configuration to both executor pods.\nrestartPolicy\n\"never\"\nSpark assumes that both drivers and executors never restart.\nserviceAccount\nValue of\nspark.kubernetes.authenticate.driver.serviceAccountName\nSpark will override\nserviceAccount\nwith the value of the spark configuration for only\n    driver pods, and only if the spark configuration is specified. Executor pods will remain unaffected.\nserviceAccountName\nValue of\nspark.kubernetes.authenticate.driver.serviceAccountName\nSpark will override\nserviceAccountName\nwith the value of the spark configuration for only\n    driver pods, and only if the spark configuration is specified. Executor pods will remain unaffected.\nvolumes\nAdds volumes from\nspark.kubernetes.{driver,executor}.volumes.[VolumeType].[VolumeName].mount.path\nSpark will add volumes as specified by the spark conf, as well as additional volumes necessary for passing\n    spark conf and pod template files.\nContainer spec\nThe following affect the driver and executor containers. All other containers in the pod spec will be unaffected.\nContainer spec key\nModified value\nDescription\nenv\nAdds env variables from\nspark.kubernetes.driverEnv.[EnvironmentVariableName]\nSpark will add driver env variables from\nspark.kubernetes.driverEnv.[EnvironmentVariableName]\n, and\n    executor env variables from\nspark.executorEnv.[EnvironmentVariableName]\n.\nimage\nValue of\nspark.kubernetes.{driver,executor}.container.image\nThe image will be defined by the spark configurations.\nimagePullPolicy\nValue of\nspark.kubernetes.container.image.pullPolicy\nSpark will override the pull policy for both driver and executors.\nname\nSee description\nThe container name will be assigned by spark (\"spark-kubernetes-driver\" for the driver container, and\n    \"spark-kubernetes-executor\" for each executor container) if not defined by the pod template. If the container is defined by the\n    template, the template's name will be used.\nresources\nSee description\nThe cpu limits are set by\nspark.kubernetes.{driver,executor}.limit.cores\n. The cpu is set by\nspark.{driver,executor}.cores\n. The memory request and limit are set by summing the values of\nspark.{driver,executor}.memory\nand\nspark.{driver,executor}.memoryOverhead\n.\n    Other resource limits are set by\nspark.{driver,executor}.resources.{resourceName}.*\nconfigs.\nvolumeMounts\nAdd volumes from\nspark.kubernetes.driver.volumes.[VolumeType].[VolumeName].mount.{path,readOnly}\nSpark will add volumes as specified by the spark conf, as well as additional volumes necessary for passing\n    spark conf and pod template files.\nResource Allocation and Configuration Overview\nPlease make sure to have read the Custom Resource Scheduling and Configuration Overview section on the\nconfiguration page\n. This section only talks about the Kubernetes specific aspects of resource scheduling.\nThe user is responsible to properly configuring the Kubernetes cluster to have the resources available and ideally isolate each resource per container so that a resource is not shared between multiple containers. If the resource is not isolated the user is responsible for writing a discovery script so that the resource is not shared between containers. See the Kubernetes documentation for specifics on configuring Kubernetes with\ncustom resources\n.\nSpark automatically handles translating the Spark configs\nspark.{driver/executor}.resource.{resourceType}\ninto the kubernetes configs as long as the Kubernetes resource type follows the Kubernetes device plugin format of\nvendor-domain/resourcetype\n. The user must specify the vendor using the\nspark.{driver/executor}.resource.{resourceType}.vendor\nconfig. The user does not need to explicitly add anything if you are using Pod templates. For reference and an example, you can see the Kubernetes documentation for scheduling\nGPUs\n. Spark only supports setting the resource limits.\nKubernetes does not tell Spark the addresses of the resources allocated to each container. For that reason, the user must specify a discovery script that gets run by the executor on startup to discover what resources are available to that executor. You can find an example scripts in\nexamples/src/main/scripts/getGpusResources.sh\n. The script must have execute permissions set and the user should setup permissions to not allow malicious users to modify it. The script should write to STDOUT a JSON string in the format of the ResourceInformation class. This has the resource name and an array of resource addresses available to just that executor.\nResource Level Scheduling Overview\nThere are several resource level scheduling features supported by Spark on Kubernetes.\nPriority Scheduling\nKubernetes supports\nPod priority\nby default.\nSpark on Kubernetes allows defining the priority of jobs by\nPod template\n. The user can specify the\npriorityClassName\nin driver or executor Pod template\nspec\nsection. Below is an example to show how to specify it:\napiVersion\n:\nv1\nKind\n:\nPod\nmetadata\n:\nlabels\n:\ntemplate-label-key\n:\ndriver-template-label-value\nspec\n:\n# Specify the priority in here\npriorityClassName\n:\nsystem-node-critical\ncontainers\n:\n-\nname\n:\ntest-driver-container\nimage\n:\nwill-be-overwritten\nCustomized Kubernetes Schedulers for Spark on Kubernetes\nSpark allows users to specify a custom Kubernetes schedulers.\nSpecify a scheduler name.\nUsers can specify a custom scheduler using\nspark.kubernetes.scheduler.name\nor\nspark.kubernetes.{driver/executor}.scheduler.name\nconfiguration.\nSpecify scheduler related configurations.\nTo configure the custom scheduler the user can use\nPod templates\n, add labels (\nspark.kubernetes.{driver,executor}.label.*\n), annotations (\nspark.kubernetes.{driver/executor}.annotation.*\n) or scheduler specific configurations (such as\nspark.kubernetes.scheduler.volcano.podGroupTemplateFile\n).\nSpecify scheduler feature step.\nUsers may also consider to use\nspark.kubernetes.{driver/executor}.pod.featureSteps\nto support more complex requirements, including but not limited to:\nCreate additional Kubernetes custom resources for driver/executor scheduling.\nSet scheduler hints according to configuration or existing Pod info dynamically.\nUsing Volcano as Customized Scheduler for Spark on Kubernetes\nPrerequisites\nSpark on Kubernetes with\nVolcano\nas a custom scheduler is supported since Spark v3.3.0 and Volcano v1.7.0. Below is an example to install Volcano 1.7.0:\nkubectl apply\n-f\nhttps://raw.githubusercontent.com/volcano-sh/volcano/v1.7.0/installer/volcano-development.yaml\nBuild\nTo create a Spark distribution along with Volcano support like those distributed by the Spark\nDownloads page\n, also see more in\n“Building Spark”\n:\n./dev/make-distribution.sh\n--name\ncustom-spark\n--pip\n--r\n--tgz\n-Psparkr\n-Phive\n-Phive-thriftserver\n-Pkubernetes\n-Pvolcano\nUsage\nSpark on Kubernetes allows using Volcano as a custom scheduler. Users can use Volcano to\nsupport more advanced resource scheduling: queue scheduling, resource reservation, priority scheduling, and more.\nTo use Volcano as a custom scheduler the user needs to specify the following configuration options:\n# Specify volcano scheduler and PodGroup template\n--conf\nspark.kubernetes.scheduler.name\n=\nvolcano\n--conf\nspark.kubernetes.scheduler.volcano.podGroupTemplateFile\n=\n/path/to/podgroup-template.yaml\n# Specify driver/executor VolcanoFeatureStep\n--conf\nspark.kubernetes.driver.pod.featureSteps\n=\norg.apache.spark.deploy.k8s.features.VolcanoFeatureStep\n--conf\nspark.kubernetes.executor.pod.featureSteps\n=\norg.apache.spark.deploy.k8s.features.VolcanoFeatureStep\nVolcano Feature Step\nVolcano feature steps help users to create a Volcano PodGroup and set driver/executor pod annotation to link with this\nPodGroup\n.\nNote that currently only driver/job level PodGroup is supported in Volcano Feature Step.\nVolcano PodGroup Template\nVolcano defines PodGroup spec using\nCRD yaml\n.\nSimilar to\nPod template\n, Spark users can use Volcano PodGroup Template to define the PodGroup spec configurations.\nTo do so, specify the Spark property\nspark.kubernetes.scheduler.volcano.podGroupTemplateFile\nto point to files accessible to the\nspark-submit\nprocess.\nBelow is an example of PodGroup template:\napiVersion\n:\nscheduling.volcano.sh/v1beta1\nkind\n:\nPodGroup\nspec\n:\n# Specify minMember to 1 to make a driver pod\nminMember\n:\n1\n# Specify minResources to support resource reservation (the driver pod resource and executors pod resource should be considered)\n# It is useful for ensource the available resources meet the minimum requirements of the Spark job and avoiding the\n# situation where drivers are scheduled, and then they are unable to schedule sufficient executors to progress.\nminResources\n:\ncpu\n:\n\"\n2\"\nmemory\n:\n\"\n3Gi\"\n# Specify the priority, help users to specify job priority in the queue during scheduling.\npriorityClassName\n:\nsystem-node-critical\n# Specify the queue, indicates the resource queue which the job should be submitted to\nqueue\n:\ndefault\nUsing Apache YuniKorn as Customized Scheduler for Spark on Kubernetes\nApache YuniKorn\nis a resource scheduler for Kubernetes that provides advanced batch scheduling\ncapabilities, such as job queuing, resource fairness, min/max queue capacity and flexible job ordering policies.\nFor available Apache YuniKorn features, please refer to\ncore features\n.\nPrerequisites\nInstall Apache YuniKorn:\nhelm repo add yunikorn https://apache.github.io/yunikorn-release\nhelm repo update\nhelm\ninstall\nyunikorn yunikorn/yunikorn\n--namespace\nyunikorn\n--version\n1.6.3\n--create-namespace\n--set\nembedAdmissionController\n=\nfalse\nThe above steps will install YuniKorn v1.6.3 on an existing Kubernetes cluster.\nGet started\nSubmit Spark jobs with the following extra options:\n--conf\nspark.kubernetes.scheduler.name\n=\nyunikorn\n--conf\nspark.kubernetes.driver.label.queue\n=\nroot.default\n--conf\nspark.kubernetes.executor.label.queue\n=\nroot.default\n--conf\nspark.kubernetes.driver.annotation.yunikorn.apache.org/app-id\n={{\nAPP_ID\n}}\n--conf\nspark.kubernetes.executor.annotation.yunikorn.apache.org/app-id\n={{\nAPP_ID\n}}\nNote that {{APP_ID}} is the built-in variable that will be substituted with Spark job ID automatically.\nWith the above configuration, the job will be scheduled by YuniKorn scheduler instead of the default Kubernetes scheduler.\nStage Level Scheduling Overview\nStage level scheduling is supported on Kubernetes:\nWhen dynamic allocation is disabled: It allows users to specify different task resource requirements at the stage level and will use the same executors requested at startup.\nWhen dynamic allocation is enabled: It allows users to specify task and executor resource requirements at the stage level and will request the extra executors. This also requires\nspark.dynamicAllocation.shuffleTracking.enabled\nto be enabled since Kubernetes doesn’t support an external shuffle service at this time. The order in which containers for different profiles is requested from Kubernetes is not guaranteed. Note that since dynamic allocation on Kubernetes requires the shuffle tracking feature, this means that executors from previous stages that used a different ResourceProfile may not idle timeout due to having shuffle data on them. This could result in using more cluster resources and in the worst case if there are no remaining resources on the Kubernetes cluster then Spark could potentially hang. You may consider looking at config\nspark.dynamicAllocation.shuffleTracking.timeout\nto set a timeout, but that could result in data having to be recomputed if the shuffle data is really needed.\nNote, there is a difference in the way pod template resources are handled between the base default profile and custom ResourceProfiles. Any resources specified in the pod template file will only be used with the base default profile. If you create custom ResourceProfiles be sure to include all necessary resources there since the resources from the template file will not be propagated to custom ResourceProfiles."}
{"url": "https://spark.apache.org/docs/latest/security.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark Security\nSpark Security: Things You Need To Know\nSpark RPC (Communication protocol between Spark processes)\nAuthentication\nYARN\nKubernetes\nNetwork Encryption\nSSL Encryption (Preferred)\nAES-based Encryption (Legacy)\nLocal Storage Encryption\nWeb UI\nAuthentication and Authorization\nSpark History Server ACLs\nSSL Configuration\nPreparing the key stores\nYARN mode\nStandalone mode\nHTTP Security Headers\nConfiguring Ports for Network Security\nStandalone mode only\nAll cluster managers\nKerberos\nLong-Running Applications\nUsing a Keytab\nUsing a ticket cache\nSecure Interaction with Kubernetes\nEvent Logging\nPersisting driver logs in client mode\nSpark Security: Things You Need To Know\nSecurity features like authentication are not enabled by default. When deploying a cluster that is open to the internet\nor an untrusted network, it’s important to secure access to the cluster to prevent unauthorized applications\nfrom running on the cluster.\nSpark supports multiple deployments types and each one supports different levels of security. Not\nall deployment types will be secure in all environments and none are secure by default. Be\nsure to evaluate your environment, what Spark supports, and take the appropriate measure to secure\nyour Spark deployment.\nThere are many different types of security concerns. Spark does not necessarily protect against\nall things. Listed below are some of the things Spark supports. Also check the deployment\ndocumentation for the type of deployment you are using for deployment specific settings. Anything\nnot documented, Spark does not support.\nSpark RPC (Communication protocol between Spark processes)\nAuthentication\nSpark currently supports authentication for RPC channels using a shared secret. Authentication can\nbe turned on by setting the\nspark.authenticate\nconfiguration parameter.\nThe exact mechanism used to generate and distribute the shared secret is deployment-specific. Unless\nspecified below, the secret must be defined by setting the\nspark.authenticate.secret\nconfig\noption. The same secret is shared by all Spark applications and daemons in that case, which limits\nthe security of these deployments, especially on multi-tenant clusters.\nThe REST Submission Server supports HTTP\nAuthorization\nheader with\na cryptographically signed JSON Web Token via\nJWSFilter\n.\nTo enable authorization, Spark Master should have\nspark.master.rest.filters=org.apache.spark.ui.JWSFilter\nand\nspark.org.apache.spark.ui.JWSFilter.param.secretKey=BASE64URL-ENCODED-KEY\nconfigurations, and\nclient should provide HTTP\nAuthorization\nheader which contains JSON Web Token signed by\nthe shared secret key.\nYARN\nFor Spark on\nYARN\n, Spark will automatically handle generating and\ndistributing the shared secret. Each application will use a unique shared secret. In\nthe case of YARN, this feature relies on YARN RPC encryption being enabled for the distribution of\nsecrets to be secure.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.yarn.shuffle.server.recovery.disabled\nfalse\nSet to true for applications that have higher security requirements and prefer that their\n    secret is not saved in the db. The shuffle data of such applications will not be recovered after\n    the External Shuffle Service restarts.\n3.5.0\nKubernetes\nOn Kubernetes, Spark will also automatically generate an authentication secret unique to each\napplication. The secret is propagated to executor pods using environment variables. This means\nthat any user that can list pods in the namespace where the Spark application is running can\nalso see their authentication secret. Access control rules should be properly set up by the\nKubernetes admin to ensure that Spark authentication is secure.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.authenticate\nfalse\nWhether Spark authenticates its internal connections.\n1.0.0\nspark.authenticate.secret\nNone\nThe secret key used authentication. See above for when this configuration should be set.\n1.0.0\nAlternatively, one can mount authentication secrets using files and Kubernetes secrets that\nthe user mounts into their pods.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.authenticate.secret.file\nNone\nPath pointing to the secret key to use for securing connections. Ensure that the\n    contents of the file have been securely generated. This file is loaded on both the driver\n    and the executors unless other settings override this (see below).\n3.0.0\nspark.authenticate.secret.driver.file\nThe value of\nspark.authenticate.secret.file\nWhen specified, overrides the location that the Spark driver reads to load the secret.\n    Useful when in client mode, when the location of the secret file may differ in the pod versus\n    the node the driver is running in. When this is specified,\nspark.authenticate.secret.executor.file\nmust be specified so that the driver\n    and the executors can both use files to load the secret key. Ensure that the contents of the file\n    on the driver is identical to the contents of the file on the executors.\n3.0.0\nspark.authenticate.secret.executor.file\nThe value of\nspark.authenticate.secret.file\nWhen specified, overrides the location that the Spark executors read to load the secret.\n    Useful in client mode, when the location of the secret file may differ in the pod versus\n    the node the driver is running in. When this is specified,\nspark.authenticate.secret.driver.file\nmust be specified so that the driver\n    and the executors can both use files to load the secret key. Ensure that the contents of the file\n    on the driver is identical to the contents of the file on the executors.\n3.0.0\nNote that when using files, Spark will not mount these files into the containers for you. It is up\nyou to ensure that the secret files are deployed securely into your containers and that the driver’s\nsecret file agrees with the executors’ secret file.\nNetwork Encryption\nSpark supports two mutually exclusive forms of encryption for RPC connections:\nThe\npreferred method\nuses TLS (aka SSL) encryption via Netty’s support for SSL. Enabling SSL\nrequires keys and certificates to be properly configured. SSL is standardized and considered more\nsecure.\nThe legacy method is an AES-based encryption mechanism relying on a shared secret. This requires\nRPC authentication to also be enabled. This method uses a bespoke protocol and it is recommended\nto use SSL instead.\nOne may prefer to use the SSL based encryption in scenarios where compliance mandates the usage\nof specific protocols; or to leverage the security of a more standard encryption library. However,\nthe AES based encryption is simpler to configure and may be preferred if the only requirement\nis that data be encrypted in transit.\nIf both options are enabled in the configuration, the SSL based RPC encryption takes precedence\nand the AES based encryption will not be used (and a warning message will be emitted).\nSSL Encryption (Preferred)\nSpark supports SSL based encryption for RPC connections. Please refer to the SSL Configuration\nsection below to understand how to configure it. The SSL settings are mostly similar across the UI\nand RPC, however there are a few additional settings which are specific to the RPC implementation.\nThe RPC implementation uses Netty under the hood (while the UI uses Jetty), which supports a\ndifferent set of options.\nUnlike the other SSL settings for the UI, the RPC SSL is\nnot\nautomatically enabled if\nspark.ssl.enabled\nis set. It must be explicitly enabled, to ensure a safe migration path for users\nupgrading Spark versions.\nAES-based Encryption (Legacy)\nSpark supports AES-based encryption for RPC connections. For encryption to be enabled, RPC\nauthentication must also be enabled and properly configured. AES encryption uses the\nApache Commons Crypto\nlibrary, and Spark’s\nconfiguration system allows access to that library’s configuration for advanced users.\nThis legacy protocol has two mutually incompatible versions. Version 1 omits applying key derivation function\n(KDF) to the key exchange protocol’s output, while version 2 applies a KDF to ensure that the derived session\nkey is uniformly distributed. Version 1 is default for backward compatibility. It is\nrecommended to use version 2\nfor better security properties. The version can be configured by setting\nspark.network.crypto.authEngineVersion\nto\n1 or 2 respectively.\nThere is also support for SASL-based encryption, although it should be considered deprecated. It\nis still required when talking to shuffle services from Spark versions older than 2.2.0.\nThe following table describes the different options available for configuring this feature.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.network.crypto.enabled\nfalse\nEnable AES-based RPC encryption, including the new authentication protocol added in 2.2.0.\n2.2.0\nspark.network.crypto.cipher\nAES/CTR/NoPadding\nCipher mode to use. Defaults \"AES/CTR/NoPadding\" for backward compatibility, which is not authenticated. \n    Recommended to use \"AES/GCM/NoPadding\", which is an authenticated encryption mode.\n4.0.0, 3.5.2, 3.4.4\nspark.network.crypto.authEngineVersion\n1\nVersion of AES-based RPC encryption to use. Valid versions are 1 or 2. Version 2 is recommended.\n4.0.0\nspark.network.crypto.config.*\nNone\nConfiguration values for the commons-crypto library, such as which cipher implementations to\n    use. The config name should be the name of commons-crypto configuration without the\ncommons.crypto\nprefix.\n2.2.0\nspark.network.crypto.saslFallback\ntrue\nWhether to fall back to SASL authentication if authentication fails using Spark's internal\n    mechanism. This is useful when the application is connecting to old shuffle services that\n    do not support the internal Spark authentication protocol. On the shuffle service side,\n    disabling this feature will block older clients from authenticating.\n2.2.0\nspark.authenticate.enableSaslEncryption\nfalse\nEnable SASL-based encrypted communication.\n2.2.0\nspark.network.sasl.serverAlwaysEncrypt\nfalse\nDisable unencrypted connections for ports using SASL authentication. This will deny connections\n    from clients that have authentication enabled, but do not request SASL-based encryption.\n1.4.0\nLocal Storage Encryption\nSpark supports encrypting temporary data written to local disks. This covers shuffle files, shuffle\nspills and data blocks stored on disk (for both caching and broadcast variables). It does not cover\nencrypting output data generated by applications with APIs such as\nsaveAsHadoopFile\nor\nsaveAsTable\n. It also may not cover temporary files created explicitly by the user.\nThe following settings cover enabling encryption for data written to disk:\nProperty Name\nDefault\nMeaning\nSince Version\nspark.io.encryption.enabled\nfalse\nEnable local disk I/O encryption. Currently supported by all modes. It's strongly\n    recommended that RPC encryption be enabled when using this feature.\n2.1.0\nspark.io.encryption.keySizeBits\n128\nIO encryption key size in bits. Supported values are 128, 192 and 256.\n2.1.0\nspark.io.encryption.keygen.algorithm\nHmacSHA1\nThe algorithm to use when generating the IO encryption key. The supported algorithms are\n    described in the KeyGenerator section of the Java Cryptography Architecture Standard Algorithm\n    Name Documentation.\n2.1.0\nspark.io.encryption.commons.config.*\nNone\nConfiguration values for the commons-crypto library, such as which cipher implementations to\n    use. The config name should be the name of commons-crypto configuration without the\ncommons.crypto\nprefix.\n2.1.0\nWeb UI\nAuthentication and Authorization\nEnabling authentication for the Web UIs is done using\njakarta servlet filters\n.\nYou will need a filter that implements the authentication method you want to deploy. Spark does not\nprovide any built-in authentication filters.\nSpark also supports access control to the UI when an authentication filter is present. Each\napplication can be configured with its own separate access control lists (ACLs). Spark\ndifferentiates between “view” permissions (who is allowed to see the application’s UI), and “modify”\npermissions (who can do things like kill jobs in a running application).\nACLs can be configured for either users or groups. Configuration entries accept comma-separated\nlists as input, meaning multiple users or groups can be given the desired privileges. This can be\nused if you run on a shared cluster and have a set of administrators or developers who need to\nmonitor applications they may not have started themselves. A wildcard (\n*\n) added to specific ACL\nmeans that all users will have the respective privilege. By default, only the user submitting the\napplication is added to the ACLs.\nGroup membership is established by using a configurable group mapping provider. The mapper is\nconfigured using the\nspark.user.groups.mapping\nconfig option, described in the table\nbelow.\nThe following options control the authentication of Web UIs:\nProperty Name\nDefault\nMeaning\nSince Version\nspark.ui.allowFramingFrom\nSAMEORIGIN\nAllow framing for a specific named URI via\nX-Frame-Options\n. By default, allow only from the same origin.\n1.6.0\nspark.ui.filters\nNone\nSpark supports HTTP\nAuthorization\nheader with a cryptographically signed\n    JSON Web Token via\norg.apache.spark.ui.JWSFilter\n.\nSee the\nSpark UI\nconfiguration for how to configure\n    filters.\n1.0.0\nspark.acls.enable\nfalse\nWhether UI ACLs should be enabled. If enabled, this checks to see if the user has access\n    permissions to view or modify the application. Note this requires the user to be authenticated,\n    so if no authentication filter is installed, this option does not do anything.\n1.1.0\nspark.admin.acls\nNone\nComma-separated list of users that have view and modify access to the Spark application.\n1.1.0\nspark.admin.acls.groups\nNone\nComma-separated list of groups that have view and modify access to the Spark application.\n2.0.0\nspark.modify.acls\nNone\nComma-separated list of users that have modify access to the Spark application.\n1.1.0\nspark.modify.acls.groups\nNone\nComma-separated list of groups that have modify access to the Spark application.\n2.0.0\nspark.ui.view.acls\nNone\nComma-separated list of users that have view access to the Spark application.\n1.0.0\nspark.ui.view.acls.groups\nNone\nComma-separated list of groups that have view access to the Spark application.\n2.0.0\nspark.user.groups.mapping\norg.apache.spark.security.ShellBasedGroupsMappingProvider\nThe list of groups for a user is determined by a group mapping service defined by the trait\norg.apache.spark.security.GroupMappingServiceProvider\n, which can be configured by\n    this property.\nBy default, a Unix shell-based implementation is used, which collects this information\n    from the host OS.\nNote:\nThis implementation supports only Unix/Linux-based environments.\n    Windows environment is currently\nnot\nsupported. However, a new platform/protocol can\n    be supported by implementing the trait mentioned above.\n2.0.0\nOn YARN, the view and modify ACLs are provided to the YARN service when submitting applications, and\ncontrol who has the respective privileges via YARN interfaces.\nSpark History Server ACLs\nAuthentication for the SHS Web UI is enabled the same way as for regular applications, using\nservlet filters.\nTo enable authorization in the SHS, a few extra options are used:\nProperty Name\nDefault\nMeaning\nSince Version\nspark.history.ui.acls.enable\nfalse\nSpecifies whether ACLs should be checked to authorize users viewing the applications in\n    the history server. If enabled, access control checks are performed regardless of what the\n    individual applications had set for\nspark.ui.acls.enable\n. The application owner\n    will always have authorization to view their own application and any users specified via\nspark.ui.view.acls\nand groups specified via\nspark.ui.view.acls.groups\nwhen the application was run will also have authorization to view that application.\n    If disabled, no access control checks are made for any application UIs available through\n    the history server.\n1.0.1\nspark.history.ui.admin.acls\nNone\nComma separated list of users that have view access to all the Spark applications in history\n    server.\n2.1.1\nspark.history.ui.admin.acls.groups\nNone\nComma separated list of groups that have view access to all the Spark applications in history\n    server.\n2.1.1\nThe SHS uses the same options to configure the group mapping provider as regular applications.\nIn this case, the group mapping provider will apply to all UIs server by the SHS, and individual\napplication configurations will be ignored.\nSSL Configuration\nConfiguration for SSL is organized hierarchically. The user can configure the default SSL settings\nwhich will be used for all the supported communication protocols unless they are overwritten by\nprotocol-specific settings. This way the user can easily provide the common settings for all the\nprotocols without disabling the ability to configure each one individually. Note that all settings \nare inherited this way,\nexcept\nfor\nspark.ssl.rpc.enabled\nwhich must be explicitly set.\nThe following table describes the SSL configuration namespaces:\nConfig Namespace\nComponent\nspark.ssl\nThe default SSL configuration. These values will apply to all namespaces below, unless\n      explicitly overridden at the namespace level.\nspark.ssl.ui\nSpark application Web UI\nspark.ssl.standalone\nStandalone Master / Worker Web UI\nspark.ssl.historyServer\nHistory Server Web UI\nspark.ssl.rpc\nSpark RPC communication\nThe full breakdown of available SSL options can be found below. The\n${ns}\nplaceholder should be\nreplaced with one of the above namespaces.\nProperty Name\nDefault\nMeaning\nSupported Namespaces\n${ns}.enabled\nfalse\nEnables SSL. When enabled,\n${ns}.ssl.protocol\nis required.\nui,standalone,historyServer,rpc\n${ns}.port\nNone\nThe port where the SSL service will listen on.\nThe port must be defined within a specific namespace configuration. The default\n      namespace is ignored when reading this configuration.\nWhen not set, the SSL port will be derived from the non-SSL port for the\n      same service. A value of \"0\" will make the service bind to an ephemeral port.\nui,standalone,historyServer\n${ns}.enabledAlgorithms\nNone\nA comma-separated list of ciphers. The specified ciphers must be supported by JVM.\nThe reference list of protocols can be found in the \"JSSE Cipher Suite Names\" section\n      of the Java security guide. The list for Java 17 can be found at\nthis\npage.\nNote: If not set, the default cipher suite for the JRE will be used.\nui,standalone,historyServer,rpc\n${ns}.keyPassword\nNone\nThe password to the private key in the key store.\nui,standalone,historyServer,rpc\n${ns}.keyStore\nNone\nPath to the key store file. The path can be absolute or relative to the directory in which the\n      process is started.\nui,standalone,historyServer,rpc\n${ns}.keyStorePassword\nNone\nPassword to the key store.\nui,standalone,historyServer,rpc\n${ns}.keyStoreType\nJKS\nThe type of the key store.\nui,standalone,historyServer\n${ns}.protocol\nNone\nTLS protocol to use. The protocol must be supported by JVM.\nThe reference list of protocols can be found in the \"Additional JSSE Standard Names\"\n      section of the Java security guide. For Java 17, the list can be found at\nthis\npage.\nui,standalone,historyServer,rpc\n${ns}.needClientAuth\nfalse\nWhether to require client authentication.\nui,standalone,historyServer\n${ns}.trustStore\nNone\nPath to the trust store file. The path can be absolute or relative to the directory in which\n      the process is started.\nui,standalone,historyServer,rpc\n${ns}.trustStorePassword\nNone\nPassword for the trust store.\nui,standalone,historyServer,rpc\n${ns}.trustStoreType\nJKS\nThe type of the trust store.\nui,standalone,historyServer\n${ns}.openSSLEnabled\nfalse\nWhether to use OpenSSL for cryptographic operations instead of the JDK SSL provider.\n      This setting requires the `certChain` and `privateKey` settings to be set.\n      This takes precedence over the `keyStore` and `trustStore` settings if both are specified.\n      If the OpenSSL library is not available at runtime, we will fall back to the JDK provider.\nrpc\n${ns}.privateKey\nNone\nPath to the private key file in PEM format. The path can be absolute or relative to the \n      directory in which the process is started. \n      This setting is required when using the OpenSSL implementation.\nrpc\n${ns}.privateKeyPassword\nNone\nThe password to the above private key file in PEM format.\nrpc\n${ns}.certChain\nNone\nPath to the certificate chain file in PEM format. The path can be absolute or relative to the \n      directory in which the process is started. \n      This setting is required when using the OpenSSL implementation.\nrpc\n${ns}.trustStoreReloadingEnabled\nfalse\nWhether the trust store should be reloaded periodically.\n      This setting is mostly only useful in standalone deployments, not k8s or yarn deployments.\nrpc\n${ns}.trustStoreReloadIntervalMs\n10000\nThe interval at which the trust store should be reloaded (in milliseconds).\n      This setting is mostly only useful in standalone deployments, not k8s or yarn deployments.\nrpc\nSpark also supports retrieving\n${ns}.keyPassword\n,\n${ns}.keyStorePassword\nand\n${ns}.trustStorePassword\nfrom\nHadoop Credential Providers\n.\nUser could store password into credential file and make it accessible by different components, like:\nhadoop credential create spark.ssl.keyPassword -value password \\\n    -provider jceks://hdfs@nn1.example.com:9001/user/backup/ssl.jceks\nTo configure the location of the credential provider, set the\nhadoop.security.credential.provider.path\nconfig option in the Hadoop configuration used by Spark, like:\n<property>\n    <name>hadoop.security.credential.provider.path</name>\n    <value>jceks://hdfs@nn1.example.com:9001/user/backup/ssl.jceks</value>\n  </property>\nOr via SparkConf “spark.hadoop.hadoop.security.credential.provider.path=jceks://hdfs@nn1.example.com:9001/user/backup/ssl.jceks”.\nPreparing the key stores\nKey stores can be generated by\nkeytool\nprogram. The reference documentation for this tool for\nJava 17 is\nhere\n.\nThe most basic steps to configure the key stores and the trust store for a Spark Standalone\ndeployment mode is as follows:\nGenerate a key pair for each node\nExport the public key of the key pair to a file on each node\nImport all exported public keys into a single trust store\nDistribute the trust store to the cluster nodes\nYARN mode\nTo provide a local trust store or key store file to drivers running in cluster mode, they can be\ndistributed with the application using the\n--files\ncommand line argument (or the equivalent\nspark.files\nconfiguration). The files will be placed on the driver’s working directory, so the TLS\nconfiguration should just reference the file name with no absolute path.\nDistributing local key stores this way may require the files to be staged in HDFS (or other similar\ndistributed file system used by the cluster), so it’s recommended that the underlying file system be\nconfigured with security in mind (e.g. by enabling authentication and wire encryption).\nStandalone mode\nThe user needs to provide key stores and configuration options for master and workers. They have to\nbe set by attaching appropriate Java system properties in\nSPARK_MASTER_OPTS\nand in\nSPARK_WORKER_OPTS\nenvironment variables, or just in\nSPARK_DAEMON_JAVA_OPTS\n.\nThe user may allow the executors to use the SSL settings inherited from the worker process. That\ncan be accomplished by setting\nspark.ssl.useNodeLocalConf\nto\ntrue\n. In that case, the settings\nprovided by the user on the client side are not used.\nHTTP Security Headers\nApache Spark can be configured to include HTTP headers to aid in preventing Cross Site Scripting\n(XSS), Cross-Frame Scripting (XFS), MIME-Sniffing, and also to enforce HTTP Strict Transport\nSecurity.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.ui.xXssProtection\n1; mode=block\nValue for HTTP X-XSS-Protection response header. You can choose appropriate value\n    from below:\n0\n(Disables XSS filtering)\n1\n(Enables XSS filtering. If a cross-site scripting attack is detected,\n        the browser will sanitize the page.)\n1; mode=block\n(Enables XSS filtering. The browser will prevent rendering\n        of the page if an attack is detected.)\n2.3.0\nspark.ui.xContentTypeOptions.enabled\ntrue\nWhen enabled, X-Content-Type-Options HTTP response header will be set to \"nosniff\".\n2.3.0\nspark.ui.strictTransportSecurity\nNone\nValue for HTTP Strict Transport Security (HSTS) Response Header. You can choose appropriate\n    value from below and set\nexpire-time\naccordingly. This option is only used when\n    SSL/TLS is enabled.\nmax-age=<expire-time>\nmax-age=<expire-time>; includeSubDomains\nmax-age=<expire-time>; preload\n2.3.0\nConfiguring Ports for Network Security\nGenerally speaking, a Spark cluster and its services are not deployed on the public internet.\nThey are generally private services, and should only be accessible within the network of the\norganization that deploys Spark. Access to the hosts and ports used by Spark services should\nbe limited to origin hosts that need to access the services.\nHowever, like the REST Submission port, Spark also supports HTTP\nAuthorization\nheader\nwith a cryptographically signed JSON Web Token (JWT) for all UI ports.\nTo use it, a user needs to configure\nspark.ui.filters=org.apache.spark.ui.JWSFilter\nand\nspark.org.apache.spark.ui.JWSFilter.param.secretKey=BASE64URL-ENCODED-KEY\n.\nBelow are the primary ports that Spark uses for its communication and how to\nconfigure those ports.\nStandalone mode only\nFrom\nTo\nDefault Port\nPurpose\nConfiguration\n    Setting\nNotes\nBrowser\nStandalone Master\n8080\nWeb UI\nspark.master.ui.port /\nSPARK_MASTER_WEBUI_PORT\nJetty-based. Standalone mode only.\nBrowser\nStandalone Worker\n8081\nWeb UI\nspark.worker.ui.port /\nSPARK_WORKER_WEBUI_PORT\nJetty-based. Standalone mode only.\nDriver /\nStandalone Worker\nStandalone Master\n7077\nSubmit job to cluster /\nJoin cluster\nSPARK_MASTER_PORT\nSet to \"0\" to choose a port randomly. Standalone mode only.\nExternal Service\nStandalone Master\n6066\nSubmit job to cluster via REST API\nspark.master.rest.port\nUse\nspark.master.rest.enabled\nto enable/disable this service. Standalone mode only.\nStandalone Master\nStandalone Worker\n(random)\nSchedule executors\nSPARK_WORKER_PORT\nSet to \"0\" to choose a port randomly. Standalone mode only.\nAll cluster managers\nFrom\nTo\nDefault Port\nPurpose\nConfiguration\n    Setting\nNotes\nBrowser\nApplication\n4040\nWeb UI\nspark.ui.port\nJetty-based\nBrowser\nHistory Server\n18080\nWeb UI\nspark.history.ui.port\nJetty-based\nExecutor /\nStandalone Master\nDriver\n(random)\nConnect to application /\nNotify executor state changes\nspark.driver.port\nSet to \"0\" to choose a port randomly.\nExecutor / Driver\nExecutor / Driver\n(random)\nBlock Manager port\nspark.blockManager.port\nRaw socket via ServerSocketChannel\nKerberos\nSpark supports submitting applications in environments that use Kerberos for authentication.\nIn most cases, Spark relies on the credentials of the current logged in user when authenticating\nto Kerberos-aware services. Such credentials can be obtained by logging in to the configured KDC\nwith tools like\nkinit\n.\nWhen talking to Hadoop-based services, Spark needs to obtain delegation tokens so that non-local\nprocesses can authenticate. Spark ships with support for HDFS and other Hadoop file systems, Hive\nand HBase.\nWhen using a Hadoop filesystem (such HDFS or WebHDFS), Spark will acquire the relevant tokens\nfor the service hosting the user’s home directory.\nAn HBase token will be obtained if HBase is in the application’s classpath, and the HBase\nconfiguration has Kerberos authentication turned (\nhbase.security.authentication=kerberos\n).\nSimilarly, a Hive token will be obtained if Hive is in the classpath, and the configuration includes\nURIs for remote metastore services (\nhive.metastore.uris\nis not empty).\nIf an application needs to interact with other secure Hadoop filesystems, their URIs need to be\nexplicitly provided to Spark at launch time. This is done by listing them in the\nspark.kerberos.access.hadoopFileSystems\nproperty, described in the configuration section below.\nSpark also supports custom delegation token providers using the Java Services\nmechanism (see\njava.util.ServiceLoader\n). Implementations of\norg.apache.spark.security.HadoopDelegationTokenProvider\ncan be made available to Spark\nby listing their names in the corresponding file in the jar’s\nMETA-INF/services\ndirectory.\nDelegation token support is currently only supported in YARN and Kubernetes mode. Consult the\ndeployment-specific page for more information.\nThe following options provides finer-grained control for this feature:\nProperty Name\nDefault\nMeaning\nSince Version\nspark.security.credentials.${service}.enabled\ntrue\nControls whether to obtain credentials for services when security is enabled.\n    By default, credentials for all supported services are retrieved when those services are\n    configured, but it's possible to disable that behavior if it somehow conflicts with the\n    application being run.\n2.3.0\nspark.kerberos.access.hadoopFileSystems\n(none)\nA comma-separated list of secure Hadoop filesystems your Spark application is going to access. For\n    example,\nspark.kerberos.access.hadoopFileSystems=hdfs://nn1.com:8032,hdfs://nn2.com:8032,\n    webhdfs://nn3.com:50070\n. The Spark application must have access to the filesystems listed\n    and Kerberos must be properly configured to be able to access them (either in the same realm\n    or in a trusted realm). Spark acquires security tokens for each of the filesystems so that\n    the Spark application can access those remote Hadoop filesystems.\n3.0.0\nUsers can exclude Kerberos delegation token renewal at resource scheduler. Currently it is only supported\non YARN. The configuration is covered in the\nRunning Spark on YARN\npage.\nLong-Running Applications\nLong-running applications may run into issues if their run time exceeds the maximum delegation\ntoken lifetime configured in services it needs to access.\nThis feature is not available everywhere. In particular, it’s only implemented\non YARN and Kubernetes (both client and cluster modes).\nSpark supports automatically creating new tokens for these applications. There are two ways to\nenable this functionality.\nUsing a Keytab\nBy providing Spark with a principal and keytab (e.g. using\nspark-submit\nwith\n--principal\nand\n--keytab\nparameters), the application will maintain a valid Kerberos login that can be\nused to retrieve delegation tokens indefinitely.\nNote that when using a keytab in cluster mode, it will be copied over to the machine running the\nSpark driver. In the case of YARN, this means using HDFS as a staging area for the keytab, so it’s\nstrongly recommended that both YARN and HDFS be secured with encryption, at least.\nUsing a ticket cache\nBy setting\nspark.kerberos.renewal.credentials\nto\nccache\nin Spark’s configuration, the local\nKerberos ticket cache will be used for authentication. Spark will keep the ticket renewed during its\nrenewable life, but after it expires a new ticket needs to be acquired (e.g. by running\nkinit\n).\nIt’s up to the user to maintain an updated ticket cache that Spark can use.\nThe location of the ticket cache can be customized by setting the\nKRB5CCNAME\nenvironment\nvariable.\nSecure Interaction with Kubernetes\nWhen talking to Hadoop-based services behind Kerberos, it was noted that Spark needs to obtain delegation tokens\nso that non-local processes can authenticate. These delegation tokens in Kubernetes are stored in Secrets that are\nshared by the Driver and its Executors. As such, there are three ways of submitting a Kerberos job:\nIn all cases you must define the environment variable:\nHADOOP_CONF_DIR\nor\nspark.kubernetes.hadoop.configMapName.\nIt also important to note that the KDC needs to be visible from inside the containers.\nIf a user wishes to use a remote HADOOP_CONF directory, that contains the Hadoop configuration files, this could be\nachieved by setting\nspark.kubernetes.hadoop.configMapName\nto a pre-existing ConfigMap.\nSubmitting with a $kinit that stores a TGT in the Local Ticket Cache:\n/usr/bin/kinit\n-kt\n<keytab_file> <username>/<krb5 realm>\n/opt/spark/bin/spark-submit\n\\\n--deploy-mode\ncluster\n\\\n--class\norg.apache.spark.examples.HdfsTest\n\\\n--master\nk8s://<KUBERNETES_MASTER_ENDPOINT>\n\\\n--conf\nspark.executor.instances\n=\n1\n\\\n--conf\nspark.app.name\n=\nspark-hdfs\n\\\n--conf\nspark.kubernetes.container.image\n=\nspark:latest\n\\\n--conf\nspark.kubernetes.kerberos.krb5.path\n=\n/etc/krb5.conf\n\\\nlocal\n:///opt/spark/examples/jars/spark-examples_<VERSION>.jar\n\\\n<HDFS_FILE_LOCATION>\nSubmitting with a local Keytab and Principal\n/opt/spark/bin/spark-submit\n\\\n--deploy-mode\ncluster\n\\\n--class\norg.apache.spark.examples.HdfsTest\n\\\n--master\nk8s://<KUBERNETES_MASTER_ENDPOINT>\n\\\n--conf\nspark.executor.instances\n=\n1\n\\\n--conf\nspark.app.name\n=\nspark-hdfs\n\\\n--conf\nspark.kubernetes.container.image\n=\nspark:latest\n\\\n--conf\nspark.kerberos.keytab\n=\n<KEYTAB_FILE>\n\\\n--conf\nspark.kerberos.principal\n=\n<PRINCIPAL>\n\\\n--conf\nspark.kubernetes.kerberos.krb5.path\n=\n/etc/krb5.conf\n\\\nlocal\n:///opt/spark/examples/jars/spark-examples_<VERSION>.jar\n\\\n<HDFS_FILE_LOCATION>\nSubmitting with pre-populated secrets, that contain the Delegation Token, already existing within the namespace\n/opt/spark/bin/spark-submit\n\\\n--deploy-mode\ncluster\n\\\n--class\norg.apache.spark.examples.HdfsTest\n\\\n--master\nk8s://<KUBERNETES_MASTER_ENDPOINT>\n\\\n--conf\nspark.executor.instances\n=\n1\n\\\n--conf\nspark.app.name\n=\nspark-hdfs\n\\\n--conf\nspark.kubernetes.container.image\n=\nspark:latest\n\\\n--conf\nspark.kubernetes.kerberos.tokenSecret.name\n=\n<SECRET_TOKEN_NAME>\n\\\n--conf\nspark.kubernetes.kerberos.tokenSecret.itemKey\n=\n<SECRET_ITEM_KEY>\n\\\n--conf\nspark.kubernetes.kerberos.krb5.path\n=\n/etc/krb5.conf\n\\\nlocal\n:///opt/spark/examples/jars/spark-examples_<VERSION>.jar\n\\\n<HDFS_FILE_LOCATION>\n3b. Submitting like in (3) however specifying a pre-created krb5 ConfigMap and pre-created\nHADOOP_CONF_DIR\nConfigMap\n/opt/spark/bin/spark-submit\n\\\n--deploy-mode\ncluster\n\\\n--class\norg.apache.spark.examples.HdfsTest\n\\\n--master\nk8s://<KUBERNETES_MASTER_ENDPOINT>\n\\\n--conf\nspark.executor.instances\n=\n1\n\\\n--conf\nspark.app.name\n=\nspark-hdfs\n\\\n--conf\nspark.kubernetes.container.image\n=\nspark:latest\n\\\n--conf\nspark.kubernetes.kerberos.tokenSecret.name\n=\n<SECRET_TOKEN_NAME>\n\\\n--conf\nspark.kubernetes.kerberos.tokenSecret.itemKey\n=\n<SECRET_ITEM_KEY>\n\\\n--conf\nspark.kubernetes.hadoop.configMapName\n=\n<HCONF_CONFIG_MAP_NAME>\n\\\n--conf\nspark.kubernetes.kerberos.krb5.configMapName\n=\n<KRB_CONFIG_MAP_NAME>\n\\\nlocal\n:///opt/spark/examples/jars/spark-examples_<VERSION>.jar\n\\\n<HDFS_FILE_LOCATION>\nEvent Logging\nIf your applications are using event logging, the directory where the event logs go\n(\nspark.eventLog.dir\n) should be manually created with proper permissions. To secure the log files,\nthe directory permissions should be set to\ndrwxrwxrwxt\n. The owner and group of the directory\nshould correspond to the super user who is running the Spark History Server.\nThis will allow all users to write to the directory but will prevent unprivileged users from\nreading, removing or renaming a file unless they own it. The event log files will be created by\nSpark with permissions such that only the user and group have read and write access.\nPersisting driver logs in client mode\nIf your applications persist driver logs in client mode by enabling\nspark.driver.log.persistToDfs.enabled\n,\nthe directory where the driver logs go (\nspark.driver.log.dfsDir\n) should be manually created with proper\npermissions. To secure the log files, the directory permissions should be set to\ndrwxrwxrwxt\n. The owner\nand group of the directory should correspond to the super user who is running the Spark History Server.\nThis will allow all users to write to the directory but will prevent unprivileged users from\nreading, removing or renaming a file unless they own it. The driver log files will be created by\nSpark with permissions such that only the user and group have read and write access."}
{"url": "https://spark.apache.org/docs/latest/cluster-overview.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nCluster Mode Overview\nThis document gives a short overview of how Spark runs on clusters, to make it easier to understand\nthe components involved. Read through the\napplication submission guide\nto learn about launching applications on a cluster.\nComponents\nSpark applications run as independent sets of processes on a cluster, coordinated by the\nSparkContext\nobject in your main program (called the\ndriver program\n).\nSpecifically, to run on a cluster, the SparkContext can connect to several types of\ncluster managers\n(either Spark’s own standalone cluster manager, YARN or Kubernetes), which allocate resources across\napplications. Once connected, Spark acquires\nexecutors\non nodes in the cluster, which are\nprocesses that run computations and store data for your application.\nNext, it sends your application code (defined by JAR or Python files passed to SparkContext) to\nthe executors. Finally, SparkContext sends\ntasks\nto the executors to run.\nThere are several useful things to note about this architecture:\nEach application gets its own executor processes, which stay up for the duration of the whole\napplication and run tasks in multiple threads. This has the benefit of isolating applications\nfrom each other, on both the scheduling side (each driver schedules its own tasks) and executor\nside (tasks from different applications run in different JVMs). However, it also means that\ndata cannot be shared across different Spark applications (instances of SparkContext) without\nwriting it to an external storage system.\nSpark is agnostic to the underlying cluster manager. As long as it can acquire executor\nprocesses, and these communicate with each other, it is relatively easy to run it even on a\ncluster manager that also supports other applications (e.g. YARN/Kubernetes).\nThe driver program must listen for and accept incoming connections from its executors throughout\nits lifetime (e.g., see\nspark.driver.port in the network config\nsection\n). As such, the driver program must be network\naddressable from the worker nodes.\nBecause the driver schedules tasks on the cluster, it should be run close to the worker\nnodes, preferably on the same local area network. If you’d like to send requests to the\ncluster remotely, it’s better to open an RPC to the driver and have it submit operations\nfrom nearby than to run a driver far away from the worker nodes.\nCluster Manager Types\nThe system currently supports several cluster managers:\nStandalone\n– a simple cluster manager included with Spark that makes it\neasy to set up a cluster.\nHadoop YARN\n– the resource manager in Hadoop 3.\nKubernetes\n– an open-source system for automating deployment, scaling,\nand management of containerized applications.\nSubmitting Applications\nApplications can be submitted to a cluster of any type using the\nspark-submit\nscript.\nThe\napplication submission guide\ndescribes how to do this.\nMonitoring\nEach driver program has a web UI, typically on port 4040, that displays information about running\ntasks, executors, and storage usage. Simply go to\nhttp://<driver-node>:4040\nin a web browser to\naccess this UI. The\nmonitoring guide\nalso describes other monitoring options.\nJob Scheduling\nSpark gives control over resource allocation both\nacross\napplications (at the level of the cluster\nmanager) and\nwithin\napplications (if multiple computations are happening on the same SparkContext).\nThe\njob scheduling overview\ndescribes this in more detail.\nGlossary\nThe following table summarizes terms you’ll see used to refer to cluster concepts:\nTerm\nMeaning\nApplication\nUser program built on Spark. Consists of a\ndriver program\nand\nexecutors\non the cluster.\nApplication jar\nA jar containing the user's Spark application. In some cases users will want to create\n        an \"uber jar\" containing their application along with its dependencies. The user's jar\n        should never include Hadoop or Spark libraries, however, these will be added at runtime.\nDriver program\nThe process running the main() function of the application and creating the SparkContext\nCluster manager\nAn external service for acquiring resources on the cluster (e.g. standalone manager, YARN, Kubernetes)\nDeploy mode\nDistinguishes where the driver process runs. In \"cluster\" mode, the framework launches\n        the driver inside of the cluster. In \"client\" mode, the submitter launches the driver\n        outside of the cluster.\nWorker node\nAny node that can run application code in the cluster\nExecutor\nA process launched for an application on a worker node, that runs tasks and keeps data in memory\n        or disk storage across them. Each application has its own executors.\nTask\nA unit of work that will be sent to one executor\nJob\nA parallel computation consisting of multiple tasks that gets spawned in response to a Spark action\n        (e.g.\nsave\n,\ncollect\n); you'll see this term used in the driver's logs.\nStage\nEach job gets divided into smaller sets of tasks called\nstages\nthat depend on each other\n        (similar to the map and reduce stages in MapReduce); you'll see this term used in the driver's logs."}
{"url": "https://spark.apache.org/docs/latest/configuration.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark Configuration\nSpark Properties\nDynamically Loading Spark Properties\nViewing Spark Properties\nAvailable Properties\nApplication Properties\nRuntime Environment\nShuffle Behavior\nSpark UI\nCompression and Serialization\nMemory Management\nExecution Behavior\nExecutor Metrics\nNetworking\nScheduling\nBarrier Execution Mode\nDynamic Allocation\nThread Configurations\nSpark Connect\nServer Configuration\nSecurity\nSpark SQL\nRuntime SQL Configuration\nStatic SQL Configuration\nSpark Streaming\nSparkR (deprecated)\nGraphX\nCluster Managers\nYARN\nKubernetes\nStandalone Mode\nEnvironment Variables\nConfiguring Logging\nPlain Text Logging\nStructured Logging\nQuerying Structured Logs with Spark SQL\nOverriding configuration directory\nInheriting Hadoop Cluster Configuration\nCustom Hadoop/Hive Configuration\nCustom Resource Scheduling and Configuration Overview\nStage Level Scheduling Overview\nPush-based shuffle overview\nExternal Shuffle service(server) side configuration options\nClient side configuration options\nSpark provides three locations to configure the system:\nSpark properties\ncontrol most application parameters and can be set by using\na\nSparkConf\nobject, or through Java\nsystem properties.\nEnvironment variables\ncan be used to set per-machine settings, such as\nthe IP address, through the\nconf/spark-env.sh\nscript on each node.\nLogging\ncan be configured through\nlog4j2.properties\n.\nSpark Properties\nSpark properties control most application settings and are configured separately for each\napplication. These properties can be set directly on a\nSparkConf\npassed to your\nSparkContext\n.\nSparkConf\nallows you to configure some of the common properties\n(e.g. master URL and application name), as well as arbitrary key-value pairs through the\nset()\nmethod. For example, we could initialize an application with two threads as follows:\nNote that we run with local[2], meaning two threads - which represents “minimal” parallelism,\nwhich can help detect bugs that only exist when we run in a distributed context.\nval\nconf\n=\nnew\nSparkConf\n()\n.\nsetMaster\n(\n\"local[2]\"\n)\n.\nsetAppName\n(\n\"CountingSheep\"\n)\nval\nsc\n=\nnew\nSparkContext\n(\nconf\n)\nNote that we can have more than 1 thread in local mode, and in cases like Spark Streaming, we may\nactually require more than 1 thread to prevent any sort of starvation issues.\nProperties that specify some time duration should be configured with a unit of time.\nThe following format is accepted:\n25ms (milliseconds)\n5s (seconds)\n10m or 10min (minutes)\n3h (hours)\n5d (days)\n1y (years)\nProperties that specify a byte size should be configured with a unit of size.\nThe following format is accepted:\n1b (bytes)\n1k or 1kb (kibibytes = 1024 bytes)\n1m or 1mb (mebibytes = 1024 kibibytes)\n1g or 1gb (gibibytes = 1024 mebibytes)\n1t or 1tb (tebibytes = 1024 gibibytes)\n1p or 1pb (pebibytes = 1024 tebibytes)\nWhile numbers without units are generally interpreted as bytes, a few are interpreted as KiB or MiB.\nSee documentation of individual configuration properties. Specifying units is desirable where\npossible.\nDynamically Loading Spark Properties\nIn some cases, you may want to avoid hard-coding certain configurations in a\nSparkConf\n. For\ninstance, if you’d like to run the same application with different masters or different\namounts of memory. Spark allows you to simply create an empty conf:\nval\nsc\n=\nnew\nSparkContext\n(\nnew\nSparkConf\n())\nThen, you can supply configuration values at runtime:\n./bin/spark-submit\n\\\n--name\n\"My app\"\n\\\n--master\n\"local[4]\"\n\\\n--conf\nspark.eventLog.enabled\n=\nfalse\n\\\n--conf\n\"spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps\"\n\\\nmyApp.jar\nThe Spark shell and\nspark-submit\ntool support two ways to load configurations dynamically. The first is command line options,\nsuch as\n--master\n, as shown above.\nspark-submit\ncan accept any Spark property using the\n--conf/-c\nflag, but uses special flags for properties that play a part in launching the Spark application.\nRunning\n./bin/spark-submit --help\nwill show the entire list of these options.\nWhen configurations are specified via the\n--conf/-c\nflags,\nbin/spark-submit\nwill also read\nconfiguration options from\nconf/spark-defaults.conf\n, in which each line consists of a key and\na value separated by whitespace. For example:\nspark.master            spark://5.6.7.8:7077\nspark.executor.memory   4g\nspark.eventLog.enabled  true\nspark.serializer        org.apache.spark.serializer.KryoSerializer\nIn addition, a property file with Spark configurations can be passed to\nbin/spark-submit\nvia\n--properties-file\nparameter. When this is set, Spark will no longer load configurations from\nconf/spark-defaults.conf\nunless another parameter\n--load-spark-defaults\nis provided.\nAny values specified as flags or in the properties file will be passed on to the application\nand merged with those specified through SparkConf. Properties set directly on the SparkConf\ntake the highest precedence, then those through\n--conf\nflags or\n--properties-file\npassed to\nspark-submit\nor\nspark-shell\n, then options in the\nspark-defaults.conf\nfile. A few\nconfiguration keys have been renamed since earlier versions of Spark; in such cases, the older\nkey names are still accepted, but take lower precedence than any instance of the newer key.\nSpark properties mainly can be divided into two kinds: one is related to deploy, like\n“spark.driver.memory”, “spark.executor.instances”, this kind of properties may not be affected when\nsetting programmatically through\nSparkConf\nin runtime, or the behavior is depending on which\ncluster manager and deploy mode you choose, so it would be suggested to set through configuration\nfile or\nspark-submit\ncommand line options; another is mainly related to Spark runtime control,\nlike “spark.task.maxFailures”, this kind of properties can be set in either way.\nViewing Spark Properties\nThe application web UI at\nhttp://<driver>:4040\nlists Spark properties in the “Environment” tab.\nThis is a useful place to check to make sure that your properties have been set correctly. Note\nthat only values explicitly specified through\nspark-defaults.conf\n,\nSparkConf\n, or the command\nline will appear. For all other configuration properties, you can assume the default value is used.\nAvailable Properties\nMost of the properties that control internal settings have reasonable default values. Some\nof the most common options to set are:\nApplication Properties\nProperty Name\nDefault\nMeaning\nSince Version\nspark.app.name\n(none)\nThe name of your application. This will appear in the UI and in log data.\n0.9.0\nspark.driver.cores\n1\nNumber of cores to use for the driver process, only in cluster mode.\n1.3.0\nspark.driver.maxResultSize\n1g\nLimit of total size of serialized results of all partitions for each Spark action (e.g.\n    collect) in bytes. Should be at least 1M, or 0 for unlimited. Jobs will be aborted if the total\n    size is above this limit.\n    Having a high limit may cause out-of-memory errors in driver (depends on spark.driver.memory\n    and memory overhead of objects in JVM). Setting a proper limit can protect the driver from\n    out-of-memory errors.\n1.2.0\nspark.driver.memory\n1g\nAmount of memory to use for the driver process, i.e. where SparkContext is initialized, in the\n    same format as JVM memory strings with a size unit suffix (\"k\", \"m\", \"g\" or \"t\")\n    (e.g.\n512m\n,\n2g\n).\nNote:\nIn client mode, this config must not be set through the\nSparkConf\ndirectly in your application, because the driver JVM has already started at that point.\n    Instead, please set this through the\n--driver-memory\ncommand line option\n    or in your default properties file.\n1.1.1\nspark.driver.memoryOverhead\ndriverMemory *\nspark.driver.memoryOverheadFactor\n, with minimum of\nspark.driver.minMemoryOverhead\nAmount of non-heap memory to be allocated per driver process in cluster mode, in MiB unless\n    otherwise specified. This is memory that accounts for things like VM overheads, interned strings,\n    other native overheads, etc. This tends to grow with the container size (typically 6-10%).\n    This option is currently supported on YARN and Kubernetes.\nNote:\nNon-heap memory includes off-heap memory\n    (when\nspark.memory.offHeap.enabled=true\n) and memory used by other driver processes\n    (e.g. python process that goes with a PySpark driver) and memory used by other non-driver\n    processes running in the same container. The maximum memory size of container to running\n    driver is determined by the sum of\nspark.driver.memoryOverhead\nand\nspark.driver.memory\n.\n2.3.0\nspark.driver.minMemoryOverhead\n384m\nThe minimum amount of non-heap memory to be allocated per driver process in cluster mode, in MiB unless otherwise specified, if\nspark.driver.memoryOverhead\nis not defined.\n    This option is currently supported on YARN and Kubernetes.\n4.0.0\nspark.driver.memoryOverheadFactor\n0.10\nFraction of driver memory to be allocated as additional non-heap memory per driver process in cluster mode.\n    This is memory that accounts for things like VM overheads, interned strings,\n    other native overheads, etc. This tends to grow with the container size.\n    This value defaults to 0.10 except for Kubernetes non-JVM jobs, which defaults to\n    0.40. This is done as non-JVM tasks need more non-JVM heap space and such tasks\n    commonly fail with \"Memory Overhead Exceeded\" errors. This preempts this error\n    with a higher default.\n    This value is ignored if\nspark.driver.memoryOverhead\nis set directly.\n3.3.0\nspark.driver.resource.{resourceName}.amount\n0\nAmount of a particular resource type to use on the driver.\n    If this is used, you must also specify the\nspark.driver.resource.{resourceName}.discoveryScript\nfor the driver to find the resource on startup.\n3.0.0\nspark.driver.resource.{resourceName}.discoveryScript\nNone\nA script for the driver to run to discover a particular resource type. This should\n    write to STDOUT a JSON string in the format of the ResourceInformation class. This has a\n    name and an array of addresses. For a client-submitted driver, discovery script must assign\n    different resource addresses to this driver comparing to other drivers on the same host.\n3.0.0\nspark.driver.resource.{resourceName}.vendor\nNone\nVendor of the resources to use for the driver. This option is currently\n    only supported on Kubernetes and is actually both the vendor and domain following\n    the Kubernetes device plugin naming convention. (e.g. For GPUs on Kubernetes\n    this config would be set to nvidia.com or amd.com)\n3.0.0\nspark.resources.discoveryPlugin\norg.apache.spark.resource.ResourceDiscoveryScriptPlugin\nComma-separated list of class names implementing\n    org.apache.spark.api.resource.ResourceDiscoveryPlugin to load into the application.\n    This is for advanced users to replace the resource discovery class with a\n    custom implementation. Spark will try each class specified until one of them\n    returns the resource information for that resource. It tries the discovery\n    script last if none of the plugins return information for that resource.\n3.0.0\nspark.executor.memory\n1g\nAmount of memory to use per executor process, in the same format as JVM memory strings with\n    a size unit suffix (\"k\", \"m\", \"g\" or \"t\") (e.g.\n512m\n,\n2g\n).\n0.7.0\nspark.executor.pyspark.memory\nNot set\nThe amount of memory to be allocated to PySpark in each executor, in MiB\n    unless otherwise specified.  If set, PySpark memory for an executor will be\n    limited to this amount. If not set, Spark will not limit Python's memory use\n    and it is up to the application to avoid exceeding the overhead memory space\n    shared with other non-JVM processes. When PySpark is run in YARN or Kubernetes, this memory\n    is added to executor resource requests.\nNote:\nThis feature is dependent on Python's\nresource\nmodule; therefore, the behaviors and\n    limitations are inherited. For instance, Windows does not support resource limiting and actual\n    resource is not limited on MacOS.\n2.4.0\nspark.executor.memoryOverhead\nexecutorMemory *\nspark.executor.memoryOverheadFactor\n, with minimum of\nspark.executor.minMemoryOverhead\nAmount of additional memory to be allocated per executor process, in MiB unless otherwise specified.\n    This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc.\n    This tends to grow with the executor size (typically 6-10%). This option is currently supported on YARN and Kubernetes.\nNote:\nAdditional memory includes PySpark executor memory\n    (when\nspark.executor.pyspark.memory\nis not configured) and memory used by other\n    non-executor processes running in the same container. The maximum memory size of container to\n    running executor is determined by the sum of\nspark.executor.memoryOverhead\n,\nspark.executor.memory\n,\nspark.memory.offHeap.size\nand\nspark.executor.pyspark.memory\n.\n2.3.0\nspark.executor.minMemoryOverhead\n384m\nThe minimum amount of non-heap memory to be allocated per executor process, in MiB unless otherwise specified, if\nspark.executor.memoryOverhead\nis not defined.\n    This option is currently supported on YARN and Kubernetes.\n4.0.0\nspark.executor.memoryOverheadFactor\n0.10\nFraction of executor memory to be allocated as additional non-heap memory per executor process.\n    This is memory that accounts for things like VM overheads, interned strings,\n    other native overheads, etc. This tends to grow with the container size.\n    This value defaults to 0.10 except for Kubernetes non-JVM jobs, which defaults to\n    0.40. This is done as non-JVM tasks need more non-JVM heap space and such tasks\n    commonly fail with \"Memory Overhead Exceeded\" errors. This preempts this error\n    with a higher default.\n    This value is ignored if\nspark.executor.memoryOverhead\nis set directly.\n3.3.0\nspark.executor.resource.{resourceName}.amount\n0\nAmount of a particular resource type to use per executor process.\n    If this is used, you must also specify the\nspark.executor.resource.{resourceName}.discoveryScript\nfor the executor to find the resource on startup.\n3.0.0\nspark.executor.resource.{resourceName}.discoveryScript\nNone\nA script for the executor to run to discover a particular resource type. This should\n    write to STDOUT a JSON string in the format of the ResourceInformation class. This has a\n    name and an array of addresses.\n3.0.0\nspark.executor.resource.{resourceName}.vendor\nNone\nVendor of the resources to use for the executors. This option is currently\n    only supported on Kubernetes and is actually both the vendor and domain following\n    the Kubernetes device plugin naming convention. (e.g. For GPUs on Kubernetes\n    this config would be set to nvidia.com or amd.com)\n3.0.0\nspark.extraListeners\n(none)\nA comma-separated list of classes that implement\nSparkListener\n; when initializing\n    SparkContext, instances of these classes will be created and registered with Spark's listener\n    bus.  If a class has a single-argument constructor that accepts a SparkConf, that constructor\n    will be called; otherwise, a zero-argument constructor will be called. If no valid constructor\n    can be found, the SparkContext creation will fail with an exception.\n1.3.0\nspark.local.dir\n/tmp\nDirectory to use for \"scratch\" space in Spark, including map output files and RDDs that get\n    stored on disk. This should be on a fast, local disk in your system. It can also be a\n    comma-separated list of multiple directories on different disks.\nNote:\nThis will be overridden by SPARK_LOCAL_DIRS (Standalone) or\n    LOCAL_DIRS (YARN) environment variables set by the cluster manager.\n0.5.0\nspark.logConf\nfalse\nLogs the effective SparkConf as INFO when a SparkContext is started.\n0.9.0\nspark.master\n(none)\nThe cluster manager to connect to. See the list of\nallowed master URL's\n.\n0.9.0\nspark.submit.deployMode\nclient\nThe deploy mode of Spark driver program, either \"client\" or \"cluster\",\n    Which means to launch driver program locally (\"client\")\n    or remotely (\"cluster\") on one of the nodes inside the cluster.\n1.5.0\nspark.log.callerContext\n(none)\nApplication information that will be written into Yarn RM log/HDFS audit log when running on Yarn/HDFS.\n    Its length depends on the Hadoop configuration\nhadoop.caller.context.max.size\n. It should be concise,\n    and typically can have up to 50 characters.\n2.2.0\nspark.log.level\n(none)\nWhen set, overrides any user-defined log settings as if calling\nSparkContext.setLogLevel()\nat Spark startup. Valid log levels include: \"ALL\", \"DEBUG\", \"ERROR\", \"FATAL\", \"INFO\", \"OFF\", \"TRACE\", \"WARN\".\n3.5.0\nspark.driver.supervise\nfalse\nIf true, restarts the driver automatically if it fails with a non-zero exit status.\n    Only has effect in Spark standalone mode.\n1.3.0\nspark.driver.timeout\n0min\nA timeout for Spark driver in minutes. 0 means infinite. For the positive time value,\n    terminate the driver with the exit code 124 if it runs after timeout duration. To use,\n    it's required to set\nspark.plugins\nwith\norg.apache.spark.deploy.DriverTimeoutPlugin\n.\n4.0.0\nspark.driver.log.localDir\n(none)\nSpecifies a local directory to write driver logs and enable Driver Log UI Tab.\n4.0.0\nspark.driver.log.dfsDir\n(none)\nBase directory in which Spark driver logs are synced, if\nspark.driver.log.persistToDfs.enabled\nis true. Within this base directory, each application logs the driver logs to an application specific file.\n    Users may want to set this to a unified location like an HDFS directory so driver log files can be persisted\n    for later usage. This directory should allow any Spark user to read/write files and the Spark History Server\n    user to delete files. Additionally, older logs from this directory are cleaned by the\nSpark History Server\nif\nspark.history.fs.driverlog.cleaner.enabled\nis true and, if they are older than max age configured\n    by setting\nspark.history.fs.driverlog.cleaner.maxAge\n.\n3.0.0\nspark.driver.log.persistToDfs.enabled\nfalse\nIf true, spark application running in client mode will write driver logs to a persistent storage, configured\n    in\nspark.driver.log.dfsDir\n. If\nspark.driver.log.dfsDir\nis not configured, driver logs\n    will not be persisted. Additionally, enable the cleaner by setting\nspark.history.fs.driverlog.cleaner.enabled\nto true in\nSpark History Server\n.\n3.0.0\nspark.driver.log.layout\n%d{yy/MM/dd HH:mm:ss.SSS} %t %p %c{1}: %m%n%ex\nThe layout for the driver logs that are synced to\nspark.driver.log.localDir\nand\nspark.driver.log.dfsDir\n. If this is not configured,\n    it uses the layout for the first appender defined in log4j2.properties. If that is also not configured, driver logs\n    use the default layout.\n3.0.0\nspark.driver.log.allowErasureCoding\nfalse\nWhether to allow driver logs to use erasure coding.  On HDFS, erasure coded files will not\n    update as quickly as regular replicated files, so they make take longer to reflect changes\n    written by the application. Note that even if this is true, Spark will still not force the\n    file to use erasure coding, it will simply use file system defaults.\n3.0.0\nspark.decommission.enabled\nfalse\nWhen decommission enabled, Spark will try its best to shut down the executor gracefully.\n    Spark will try to migrate all the RDD blocks (controlled by\nspark.storage.decommission.rddBlocks.enabled\n)\n    and shuffle blocks (controlled by\nspark.storage.decommission.shuffleBlocks.enabled\n) from the decommissioning\n    executor to a remote executor when\nspark.storage.decommission.enabled\nis enabled.\n    With decommission enabled, Spark will also decommission an executor instead of killing when\nspark.dynamicAllocation.enabled\nenabled.\n3.1.0\nspark.executor.decommission.killInterval\n(none)\nDuration after which a decommissioned executor will be killed forcefully by an outside (e.g. non-spark) service.\n3.1.0\nspark.executor.decommission.forceKillTimeout\n(none)\nDuration after which a Spark will force a decommissioning executor to exit.\n    This should be set to a high value in most situations as low values will prevent block migrations from having enough time to complete.\n3.2.0\nspark.executor.decommission.signal\nPWR\nThe signal that used to trigger the executor to start decommission.\n3.2.0\nspark.executor.maxNumFailures\nnumExecutors * 2, with minimum of 3\nThe maximum number of executor failures before failing the application.\n    This configuration only takes effect on YARN and Kubernetes.\n3.5.0\nspark.executor.failuresValidityInterval\n(none)\nInterval after which executor failures will be considered independent and\n    not accumulate towards the attempt count.\n    This configuration only takes effect on YARN and Kubernetes.\n3.5.0\nApart from these, the following properties are also available, and may be useful in some situations:\nRuntime Environment\nProperty Name\nDefault\nMeaning\nSince Version\nspark.driver.extraClassPath\n(none)\nExtra classpath entries to prepend to the classpath of the driver.\nNote:\nIn client mode, this config must not be set through the\nSparkConf\ndirectly in your application, because the driver JVM has already started at that point.\n    Instead, please set this through the\n--driver-class-path\ncommand line option or in\n    your default properties file.\n1.0.0\nspark.driver.defaultJavaOptions\n(none)\nA string of default JVM options to prepend to\nspark.driver.extraJavaOptions\n.\n    This is intended to be set by administrators.\n\n    For instance, GC settings or other logging.\n    Note that it is illegal to set maximum heap size (-Xmx) settings with this option. Maximum heap\n    size settings can be set with\nspark.driver.memory\nin the cluster mode and through\n    the\n--driver-memory\ncommand line option in the client mode.\nNote:\nIn client mode, this config must not be set through the\nSparkConf\ndirectly in your application, because the driver JVM has already started at that point.\n    Instead, please set this through the\n--driver-java-options\ncommand line option or in\n    your default properties file.\n3.0.0\nspark.driver.extraJavaOptions\n(none)\nA string of extra JVM options to pass to the driver. This is intended to be set by users.\n\n    For instance, GC settings or other logging.\n    Note that it is illegal to set maximum heap size (-Xmx) settings with this option. Maximum heap\n    size settings can be set with\nspark.driver.memory\nin the cluster mode and through\n    the\n--driver-memory\ncommand line option in the client mode.\nNote:\nIn client mode, this config must not be set through the\nSparkConf\ndirectly in your application, because the driver JVM has already started at that point.\n    Instead, please set this through the\n--driver-java-options\ncommand line option or in\n    your default properties file.\nspark.driver.defaultJavaOptions\nwill be prepended to this configuration.\n1.0.0\nspark.driver.extraLibraryPath\n(none)\nSet a special library path to use when launching the driver JVM.\nNote:\nIn client mode, this config must not be set through the\nSparkConf\ndirectly in your application, because the driver JVM has already started at that point.\n    Instead, please set this through the\n--driver-library-path\ncommand line option or in\n    your default properties file.\n1.0.0\nspark.driver.userClassPathFirst\nfalse\n(Experimental) Whether to give user-added jars precedence over Spark's own jars when loading\n    classes in the driver. This feature can be used to mitigate conflicts between Spark's\n    dependencies and user dependencies. It is currently an experimental feature.\n\n    This is used in cluster mode only.\n1.3.0\nspark.executor.extraClassPath\n(none)\nExtra classpath entries to prepend to the classpath of executors. This exists primarily for\n    backwards-compatibility with older versions of Spark. Users typically should not need to set\n    this option.\n1.0.0\nspark.executor.defaultJavaOptions\n(none)\nA string of default JVM options to prepend to\nspark.executor.extraJavaOptions\n.\n    This is intended to be set by administrators.\n\n    For instance, GC settings or other logging.\n    Note that it is illegal to set Spark properties or maximum heap size (-Xmx) settings with this\n    option. Spark properties should be set using a SparkConf object or the spark-defaults.conf file\n    used with the spark-submit script. Maximum heap size settings can be set with spark.executor.memory.\n\n    The following symbols, if present will be interpolated:  will be replaced by\n    application ID and  will be replaced by executor ID. For example, to enable\n    verbose gc logging to a file named for the executor ID of the app in /tmp, pass a 'value' of:\n-verbose:gc -Xloggc:/tmp/-.gc\n3.0.0\nspark.executor.extraJavaOptions\n(none)\nA string of extra JVM options to pass to executors. This is intended to be set by users.\n\n    For instance, GC settings or other logging.\n    Note that it is illegal to set Spark properties or maximum heap size (-Xmx) settings with this\n    option. Spark properties should be set using a SparkConf object or the spark-defaults.conf file\n    used with the spark-submit script. Maximum heap size settings can be set with spark.executor.memory.\n\n    The following symbols, if present will be interpolated:  will be replaced by\n    application ID and  will be replaced by executor ID. For example, to enable\n    verbose gc logging to a file named for the executor ID of the app in /tmp, pass a 'value' of:\n-verbose:gc -Xloggc:/tmp/-.gc\nspark.executor.defaultJavaOptions\nwill be prepended to this configuration.\n1.0.0\nspark.executor.extraLibraryPath\n(none)\nSet a special library path to use when launching executor JVM's.\n1.0.0\nspark.executor.logs.rolling.maxRetainedFiles\n-1\nSets the number of latest rolling log files that are going to be retained by the system.\n    Older log files will be deleted. Disabled by default.\n1.1.0\nspark.executor.logs.rolling.enableCompression\nfalse\nEnable executor log compression. If it is enabled, the rolled executor logs will be compressed.\n    Disabled by default.\n2.0.2\nspark.executor.logs.rolling.maxSize\n1024 * 1024\nSet the max size of the file in bytes by which the executor logs will be rolled over.\n    Rolling is disabled by default. See\nspark.executor.logs.rolling.maxRetainedFiles\nfor automatic cleaning of old logs.\n1.4.0\nspark.executor.logs.rolling.strategy\n\"\" (disabled)\nSet the strategy of rolling of executor logs. By default it is disabled. It can\n    be set to \"time\" (time-based rolling) or \"size\" (size-based rolling) or \"\" (disabled). For \"time\",\n    use\nspark.executor.logs.rolling.time.interval\nto set the rolling interval.\n    For \"size\", use\nspark.executor.logs.rolling.maxSize\nto set\n    the maximum file size for rolling.\n1.1.0\nspark.executor.logs.rolling.time.interval\ndaily\nSet the time interval by which the executor logs will be rolled over.\n    Rolling is disabled by default. Valid values are\ndaily\n,\nhourly\n,\nminutely\nor\n    any interval in seconds. See\nspark.executor.logs.rolling.maxRetainedFiles\nfor automatic cleaning of old logs.\n1.1.0\nspark.executor.userClassPathFirst\nfalse\n(Experimental) Same functionality as\nspark.driver.userClassPathFirst\n, but\n    applied to executor instances.\n1.3.0\nspark.executorEnv.[EnvironmentVariableName]\n(none)\nAdd the environment variable specified by\nEnvironmentVariableName\nto the Executor\n    process. The user can specify multiple of these to set multiple environment variables.\n0.9.0\nspark.redaction.regex\n(?i)secret|password|token|access[.]?key\nRegex to decide which Spark configuration properties and environment variables in driver and\n    executor environments contain sensitive information. When this regex matches a property key or\n    value, the value is redacted from the environment UI and various logs like YARN and event logs.\n2.1.2\nspark.redaction.string.regex\n(none)\nRegex to decide which parts of strings produced by Spark contain sensitive information.\n    When this regex matches a string part, that string part is replaced by a dummy value.\n    This is currently used to redact the output of SQL explain commands.\n2.2.0\nspark.python.profile\nfalse\nEnable profiling in Python worker, the profile result will show up by\nsc.show_profiles()\n,\n    or it will be displayed before the driver exits. It also can be dumped into disk by\nsc.dump_profiles(path)\n. If some of the profile results had been displayed manually,\n    they will not be displayed automatically before driver exiting.\n\n    By default the\npyspark.profiler.BasicProfiler\nwill be used, but this can be overridden by\n    passing a profiler class in as a parameter to the\nSparkContext\nconstructor.\n1.2.0\nspark.python.profile.dump\n(none)\nThe directory which is used to dump the profile result before driver exiting.\n    The results will be dumped as separated file for each RDD. They can be loaded\n    by\npstats.Stats()\n. If this is specified, the profile result will not be displayed\n    automatically.\n1.2.0\nspark.python.worker.memory\n512m\nAmount of memory to use per python worker process during aggregation, in the same\n    format as JVM memory strings with a size unit suffix (\"k\", \"m\", \"g\" or \"t\")\n    (e.g.\n512m\n,\n2g\n).\n    If the memory used during aggregation goes above this amount, it will spill the data into disks.\n1.1.0\nspark.python.worker.reuse\ntrue\nReuse Python worker or not. If yes, it will use a fixed number of Python workers,\n    does not need to fork() a Python process for every task. It will be very useful\n    if there is a large broadcast, then the broadcast will not need to be transferred\n    from JVM to Python worker for every task.\n1.2.0\nspark.files\nComma-separated list of files to be placed in the working directory of each executor. Globs are allowed.\n1.0.0\nspark.submit.pyFiles\nComma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps. Globs are allowed.\n1.0.1\nspark.jars\nComma-separated list of jars to include on the driver and executor classpaths. Globs are allowed.\n0.9.0\nspark.jars.packages\nComma-separated list of Maven coordinates of jars to include on the driver and executor\n    classpaths. The coordinates should be groupId:artifactId:version. If\nspark.jars.ivySettings\nis given artifacts will be resolved according to the configuration in the file, otherwise artifacts\n    will be searched for in the local maven repo, then maven central and finally any additional remote\n    repositories given by the command-line option\n--repositories\n. For more details, see\nAdvanced Dependency Management\n.\n1.5.0\nspark.jars.excludes\nComma-separated list of groupId:artifactId, to exclude while resolving the dependencies\n    provided in\nspark.jars.packages\nto avoid dependency conflicts.\n1.5.0\nspark.jars.ivy\nPath to specify the Ivy user directory, used for the local Ivy cache and package files from\nspark.jars.packages\n. This will override the Ivy property\nivy.default.ivy.user.dir\nwhich defaults to ~/.ivy2.\n1.3.0\nspark.jars.ivySettings\nPath to an Ivy settings file to customize resolution of jars specified using\nspark.jars.packages\ninstead of the built-in defaults, such as maven central. Additional repositories given by the command-line\n    option\n--repositories\nor\nspark.jars.repositories\nwill also be included.\n    Useful for allowing Spark to resolve artifacts from behind a firewall e.g. via an in-house\n    artifact server like Artifactory. Details on the settings file format can be\n    found at\nSettings Files\n.\n    Only paths with\nfile://\nscheme are supported. Paths without a scheme are assumed to have\n    a\nfile://\nscheme.\nWhen running in YARN cluster mode, this file will also be localized to the remote driver for dependency\n    resolution within\nSparkContext#addJar\n2.2.0\nspark.jars.repositories\nComma-separated list of additional remote repositories to search for the maven coordinates\n    given with\n--packages\nor\nspark.jars.packages\n.\n2.3.0\nspark.archives\nComma-separated list of archives to be extracted into the working directory of each executor.\n    .jar, .tar.gz, .tgz and .zip are supported. You can specify the directory name to unpack via\n    adding\n#\nafter the file name to unpack, for example,\nfile.zip#directory\n.\n    This configuration is experimental.\n3.1.0\nspark.pyspark.driver.python\nPython binary executable to use for PySpark in driver.\n    (default is\nspark.pyspark.python\n)\n2.1.0\nspark.pyspark.python\nPython binary executable to use for PySpark in both driver and executors.\n2.1.0\nShuffle Behavior\nProperty Name\nDefault\nMeaning\nSince Version\nspark.reducer.maxSizeInFlight\n48m\nMaximum size of map outputs to fetch simultaneously from each reduce task, in MiB unless\n    otherwise specified. Since each output requires us to create a buffer to receive it, this\n    represents a fixed memory overhead per reduce task, so keep it small unless you have a\n    large amount of memory.\n1.4.0\nspark.reducer.maxReqsInFlight\nInt.MaxValue\nThis configuration limits the number of remote requests to fetch blocks at any given point.\n    When the number of hosts in the cluster increase, it might lead to very large number\n    of inbound connections to one or more nodes, causing the workers to fail under load.\n    By allowing it to limit the number of fetch requests, this scenario can be mitigated.\n2.0.0\nspark.reducer.maxBlocksInFlightPerAddress\nInt.MaxValue\nThis configuration limits the number of remote blocks being fetched per reduce task from a\n    given host port. When a large number of blocks are being requested from a given address in a\n    single fetch or simultaneously, this could crash the serving executor or Node Manager. This\n    is especially useful to reduce the load on the Node Manager when external shuffle is enabled.\n    You can mitigate this issue by setting it to a lower value.\n2.2.1\nspark.shuffle.compress\ntrue\nWhether to compress map output files. Generally a good idea. Compression will use\nspark.io.compression.codec\n.\n0.6.0\nspark.shuffle.file.buffer\n32k\nSize of the in-memory buffer for each shuffle file output stream, in KiB unless otherwise\n    specified. These buffers reduce the number of disk seeks and system calls made in creating\n    intermediate shuffle files.\n1.4.0\nspark.shuffle.file.merge.buffer\n32k\nSize of the in-memory buffer for each shuffle file input stream, in KiB unless otherwise\n    specified. These buffers use off-heap buffers and are related to the number of files in\n    the shuffle file. Too large buffers should be avoided.\n4.0.0\nspark.shuffle.unsafe.file.output.buffer\n32k\nDeprecated since Spark 4.0, please use\nspark.shuffle.localDisk.file.output.buffer\n.\n2.3.0\nspark.shuffle.localDisk.file.output.buffer\n32k\nThe file system for this buffer size after each partition is written in all local disk shuffle writers.\n    In KiB unless otherwise specified.\n4.0.0\nspark.shuffle.spill.diskWriteBufferSize\n1024 * 1024\nThe buffer size, in bytes, to use when writing the sorted records to an on-disk file.\n2.3.0\nspark.shuffle.io.maxRetries\n3\n(Netty only) Fetches that fail due to IO-related exceptions are automatically retried if this is\n    set to a non-zero value. This retry logic helps stabilize large shuffles in the face of long GC\n    pauses or transient network connectivity issues.\n1.2.0\nspark.shuffle.io.numConnectionsPerPeer\n1\n(Netty only) Connections between hosts are reused in order to reduce connection buildup for\n    large clusters. For clusters with many hard disks and few hosts, this may result in insufficient\n    concurrency to saturate all disks, and so users may consider increasing this value.\n1.2.1\nspark.shuffle.io.preferDirectBufs\ntrue\n(Netty only) Off-heap buffers are used to reduce garbage collection during shuffle and cache\n    block transfer. For environments where off-heap memory is tightly limited, users may wish to\n    turn this off to force all allocations from Netty to be on-heap.\n1.2.0\nspark.shuffle.io.retryWait\n5s\n(Netty only) How long to wait between retries of fetches. The maximum delay caused by retrying\n    is 15 seconds by default, calculated as\nmaxRetries * retryWait\n.\n1.2.1\nspark.shuffle.io.backLog\n-1\nLength of the accept queue for the shuffle service. For large applications, this value may\n    need to be increased, so that incoming connections are not dropped if the service cannot keep\n    up with a large number of connections arriving in a short period of time. This needs to\n    be configured wherever the shuffle service itself is running, which may be outside of the\n    application (see\nspark.shuffle.service.enabled\noption below). If set below 1,\n    will fallback to OS default defined by Netty's\nio.netty.util.NetUtil#SOMAXCONN\n.\n1.1.1\nspark.shuffle.io.connectionTimeout\nvalue of\nspark.network.timeout\nTimeout for the established connections between shuffle servers and clients to be marked\n    as idled and closed if there are still outstanding fetch requests but no traffic no the channel\n    for at least\nconnectionTimeout\n.\n1.2.0\nspark.shuffle.io.connectionCreationTimeout\nvalue of\nspark.shuffle.io.connectionTimeout\nTimeout for establishing a connection between the shuffle servers and clients.\n3.2.0\nspark.shuffle.service.enabled\nfalse\nEnables the external shuffle service. This service preserves the shuffle files written by\n    executors e.g. so that executors can be safely removed, or so that shuffle fetches can continue in\n    the event of executor failure. The external shuffle service must be set up in order to enable it. See\ndynamic allocation\n    configuration and setup documentation\nfor more information.\n1.2.0\nspark.shuffle.service.port\n7337\nPort on which the external shuffle service will run.\n1.2.0\nspark.shuffle.service.name\nspark_shuffle\nThe configured name of the Spark shuffle service the client should communicate with.\n    This must match the name used to configure the Shuffle within the YARN NodeManager configuration\n    (\nyarn.nodemanager.aux-services\n). Only takes effect\n    when\nspark.shuffle.service.enabled\nis set to true.\n3.2.0\nspark.shuffle.service.index.cache.size\n100m\nCache entries limited to the specified memory footprint, in bytes unless otherwise specified.\n2.3.0\nspark.shuffle.service.removeShuffle\ntrue\nWhether to use the ExternalShuffleService for deleting shuffle blocks for\n    deallocated executors when the shuffle is no longer needed. Without this enabled,\n    shuffle data on executors that are deallocated will remain on disk until the\n    application ends.\n3.3.0\nspark.shuffle.maxChunksBeingTransferred\nLong.MAX_VALUE\nThe max number of chunks allowed to be transferred at the same time on shuffle service.\n    Note that new incoming connections will be closed when the max number is hit. The client will\n    retry according to the shuffle retry configs (see\nspark.shuffle.io.maxRetries\nand\nspark.shuffle.io.retryWait\n), if those limits are reached the task will fail with\n    fetch failure.\n2.3.0\nspark.shuffle.sort.bypassMergeThreshold\n200\n(Advanced) In the sort-based shuffle manager, avoid merge-sorting data if there is no\n    map-side aggregation and there are at most this many reduce partitions.\n1.1.1\nspark.shuffle.sort.io.plugin.class\norg.apache.spark.shuffle.sort.io.LocalDiskShuffleDataIO\nName of the class to use for shuffle IO.\n3.0.0\nspark.shuffle.spill.compress\ntrue\nWhether to compress data spilled during shuffles. Compression will use\nspark.io.compression.codec\n.\n0.9.0\nspark.shuffle.accurateBlockThreshold\n100 * 1024 * 1024\nThreshold in bytes above which the size of shuffle blocks in HighlyCompressedMapStatus is\n    accurately recorded. This helps to prevent OOM by avoiding underestimating shuffle\n    block size when fetch shuffle blocks.\n2.2.1\nspark.shuffle.accurateBlockSkewedFactor\n-1.0\nA shuffle block is considered as skewed and will be accurately recorded in\nHighlyCompressedMapStatus\nif its size is larger than this factor multiplying\n    the median shuffle block size or\nspark.shuffle.accurateBlockThreshold\n. It is\n    recommended to set this parameter to be the same as\nspark.sql.adaptive.skewJoin.skewedPartitionFactor\n. Set to -1.0 to disable this\n    feature by default.\n3.3.0\nspark.shuffle.registration.timeout\n5000\nTimeout in milliseconds for registration to the external shuffle service.\n2.3.0\nspark.shuffle.registration.maxAttempts\n3\nWhen we fail to register to the external shuffle service, we will retry for maxAttempts times.\n2.3.0\nspark.shuffle.reduceLocality.enabled\ntrue\nWhether to compute locality preferences for reduce tasks.\n1.5.0\nspark.shuffle.mapOutput.minSizeForBroadcast\n512k\nThe size at which we use Broadcast to send the map output statuses to the executors.\n2.0.0\nspark.shuffle.detectCorrupt\ntrue\nWhether to detect any corruption in fetched blocks.\n2.2.0\nspark.shuffle.detectCorrupt.useExtraMemory\nfalse\nIf enabled, part of a compressed/encrypted stream will be de-compressed/de-crypted by using extra memory\n    to detect early corruption. Any IOException thrown will cause the task to be retried once\n    and if it fails again with same exception, then FetchFailedException will be thrown to retry previous stage.\n3.0.0\nspark.shuffle.useOldFetchProtocol\nfalse\nWhether to use the old protocol while doing the shuffle block fetching. It is only enabled while we need the\n    compatibility in the scenario of new Spark version job fetching shuffle blocks from old version external shuffle service.\n3.0.0\nspark.shuffle.readHostLocalDisk\ntrue\nIf enabled (and\nspark.shuffle.useOldFetchProtocol\nis disabled, shuffle blocks requested from those block managers\n    which are running on the same host are read from the disk directly instead of being fetched as remote blocks over the network.\n3.0.0\nspark.files.io.connectionTimeout\nvalue of\nspark.network.timeout\nTimeout for the established connections for fetching files in Spark RPC environments to be marked\n    as idled and closed if there are still outstanding files being downloaded but no traffic no the channel\n    for at least\nconnectionTimeout\n.\n1.6.0\nspark.files.io.connectionCreationTimeout\nvalue of\nspark.files.io.connectionTimeout\nTimeout for establishing a connection for fetching files in Spark RPC environments.\n3.2.0\nspark.shuffle.checksum.enabled\ntrue\nWhether to calculate the checksum of shuffle data. If enabled, Spark will calculate the checksum values for each partition\n    data within the map output file and store the values in a checksum file on the disk. When there's shuffle data corruption\n    detected, Spark will try to diagnose the cause (e.g., network issue, disk issue, etc.) of the corruption by using the checksum file.\n3.2.0\nspark.shuffle.checksum.algorithm\nADLER32\nThe algorithm is used to calculate the shuffle checksum. Currently, it only supports built-in algorithms of JDK, e.g., ADLER32, CRC32 and CRC32C.\n3.2.0\nspark.shuffle.service.fetch.rdd.enabled\nfalse\nWhether to use the ExternalShuffleService for fetching disk persisted RDD blocks.\n    In case of dynamic allocation if this feature is enabled executors having only disk\n    persisted blocks are considered idle after\nspark.dynamicAllocation.executorIdleTimeout\nand will be released accordingly.\n3.0.0\nspark.shuffle.service.db.enabled\ntrue\nWhether to use db in ExternalShuffleService. Note that this only affects standalone mode.\n3.0.0\nspark.shuffle.service.db.backend\nROCKSDB\nSpecifies a disk-based store used in shuffle service local db. Setting as ROCKSDB or LEVELDB (deprecated).\n3.4.0\nSpark UI\nProperty Name\nDefault\nMeaning\nSince Version\nspark.eventLog.logBlockUpdates.enabled\nfalse\nWhether to log events for every block update, if\nspark.eventLog.enabled\nis true.\n    *Warning*: This will increase the size of the event log considerably.\n2.3.0\nspark.eventLog.longForm.enabled\nfalse\nIf true, use the long form of call sites in the event log. Otherwise use the short form.\n2.4.0\nspark.eventLog.compress\ntrue\nWhether to compress logged events, if\nspark.eventLog.enabled\nis true.\n1.0.0\nspark.eventLog.compression.codec\nzstd\nThe codec to compress logged events. By default, Spark provides four codecs:\nlz4\n,\nlzf\n,\nsnappy\n, and\nzstd\n.\n    You can also use fully qualified class names to specify the codec, e.g.\norg.apache.spark.io.LZ4CompressionCodec\n,\norg.apache.spark.io.LZFCompressionCodec\n,\norg.apache.spark.io.SnappyCompressionCodec\n,\n    and\norg.apache.spark.io.ZStdCompressionCodec\n.\n3.0.0\nspark.eventLog.erasureCoding.enabled\nfalse\nWhether to allow event logs to use erasure coding, or turn erasure coding off, regardless of\n    filesystem defaults.  On HDFS, erasure coded files will not update as quickly as regular\n    replicated files, so the application updates will take longer to appear in the History Server.\n    Note that even if this is true, Spark will still not force the file to use erasure coding, it\n    will simply use filesystem defaults.\n3.0.0\nspark.eventLog.dir\nfile:///tmp/spark-events\nBase directory in which Spark events are logged, if\nspark.eventLog.enabled\nis true.\n    Within this base directory, Spark creates a sub-directory for each application, and logs the\n    events specific to the application in this directory. Users may want to set this to\n    a unified location like an HDFS directory so history files can be read by the history server.\n1.0.0\nspark.eventLog.enabled\nfalse\nWhether to log Spark events, useful for reconstructing the Web UI after the application has\n    finished.\n1.0.0\nspark.eventLog.overwrite\nfalse\nWhether to overwrite any existing files.\n1.0.0\nspark.eventLog.buffer.kb\n100k\nBuffer size to use when writing to output streams, in KiB unless otherwise specified.\n1.0.0\nspark.eventLog.rolling.enabled\nfalse\nWhether rolling over event log files is enabled. If set to true, it cuts down each event\n    log file to the configured size.\n3.0.0\nspark.eventLog.rolling.maxFileSize\n128m\nWhen\nspark.eventLog.rolling.enabled=true\n, specifies the max size of event log file before it's rolled over.\n3.0.0\nspark.ui.dagGraph.retainedRootRDDs\nInt.MaxValue\nHow many DAG graph nodes the Spark UI and status APIs remember before garbage collecting.\n2.1.0\nspark.ui.groupSQLSubExecutionEnabled\ntrue\nWhether to group sub executions together in SQL UI when they belong to the same root execution\n3.4.0\nspark.ui.enabled\ntrue\nWhether to run the web UI for the Spark application.\n1.1.1\nspark.ui.store.path\nNone\nLocal directory where to cache application information for live UI.\n    By default this is not set, meaning all application information will be kept in memory.\n3.4.0\nspark.ui.killEnabled\ntrue\nAllows jobs and stages to be killed from the web UI.\n1.0.0\nspark.ui.threadDumpsEnabled\ntrue\nWhether to show a link for executor thread dumps in Stages and Executor pages.\n1.2.0\nspark.ui.threadDump.flamegraphEnabled\ntrue\nWhether to render the Flamegraph for executor thread dumps.\n4.0.0\nspark.ui.heapHistogramEnabled\ntrue\nWhether to show a link for executor heap histogram in Executor page.\n3.5.0\nspark.ui.liveUpdate.period\n100ms\nHow often to update live entities. -1 means \"never update\" when replaying applications,\n    meaning only the last write will happen. For live applications, this avoids a few\n    operations that we can live without when rapidly processing incoming task events.\n2.3.0\nspark.ui.liveUpdate.minFlushPeriod\n1s\nMinimum time elapsed before stale UI data is flushed. This avoids UI staleness when incoming\n    task events are not fired frequently.\n2.4.2\nspark.ui.port\n4040\nPort for your application's dashboard, which shows memory and workload data.\n0.7.0\nspark.ui.retainedJobs\n1000\nHow many jobs the Spark UI and status APIs remember before garbage collecting.\n    This is a target maximum, and fewer elements may be retained in some circumstances.\n1.2.0\nspark.ui.retainedStages\n1000\nHow many stages the Spark UI and status APIs remember before garbage collecting.\n    This is a target maximum, and fewer elements may be retained in some circumstances.\n0.9.0\nspark.ui.retainedTasks\n100000\nHow many tasks in one stage the Spark UI and status APIs remember before garbage collecting.\n    This is a target maximum, and fewer elements may be retained in some circumstances.\n2.0.1\nspark.ui.reverseProxy\nfalse\nEnable running Spark Master as reverse proxy for worker and application UIs. In this mode, Spark master will reverse proxy the worker and application UIs to enable access without requiring direct access to their hosts. Use it with caution, as worker and application UI will not be accessible directly, you will only be able to access them through spark master/proxy public URL. This setting affects all the workers and application UIs running in the cluster and must be set on all the workers, drivers and masters.\n2.1.0\nspark.ui.reverseProxyUrl\nIf the Spark UI should be served through another front-end reverse proxy, this is the URL\n    for accessing the Spark master UI through that reverse proxy.\n    This is useful when running proxy for authentication e.g. an OAuth proxy. The URL may contain\n    a path prefix, like\nhttp://mydomain.com/path/to/spark/\n, allowing you to serve the\n    UI for multiple Spark clusters and other web applications through the same virtual host and\n    port.\n    Normally, this should be an absolute URL including scheme (http/https), host and port.\n    It is possible to specify a relative URL starting with \"/\" here. In this case, all URLs\n    generated by the Spark UI and Spark REST APIs will be server-relative links -- this will still\n    work, as the entire Spark UI is served through the same host and port.\nThe setting affects link generation in the Spark UI, but the front-end reverse proxy\n    is responsible for\nstripping a path prefix before forwarding the request,\nrewriting redirects which point directly to the Spark master,\nredirecting access from\nhttp://mydomain.com/path/to/spark\nto\nhttp://mydomain.com/path/to/spark/\n(trailing slash after path prefix); otherwise\n      relative links on the master page do not work correctly.\nThis setting affects all the workers and application UIs running in the cluster and must be set\n    identically on all the workers, drivers and masters. In is only effective when\nspark.ui.reverseProxy\nis turned on. This setting is not needed when the Spark\n    master web UI is directly reachable.\nNote that the value of the setting can't contain the keyword\nproxy\nor\nhistory\nafter split by \"/\". Spark UI relies on both keywords for getting REST API endpoints from URIs.\n2.1.0\nspark.ui.proxyRedirectUri\nWhere to address redirects when Spark is running behind a proxy. This will make Spark\n    modify redirect responses so they point to the proxy server, instead of the Spark UI's own\n    address. This should be only the address of the server, without any prefix paths for the\n    application; the prefix should be set either by the proxy server itself (by adding the\nX-Forwarded-Context\nrequest header), or by setting the proxy base in the Spark\n    app's configuration.\n3.0.0\nspark.ui.showConsoleProgress\nfalse\nShow the progress bar in the console. The progress bar shows the progress of stages\n    that run for longer than 500ms. If multiple stages run at the same time, multiple\n    progress bars will be displayed on the same line.\nNote:\nIn shell environment, the default value of spark.ui.showConsoleProgress is true.\n1.2.1\nspark.ui.consoleProgress.update.interval\n200\nAn interval in milliseconds to update the progress bar in the console.\n2.1.0\nspark.ui.custom.executor.log.url\n(none)\nSpecifies custom spark executor log URL for supporting external log service instead of using cluster\n    managers' application log URLs in Spark UI. Spark will support some path variables via patterns\n    which can vary on cluster manager. Please check the documentation for your cluster manager to\n    see which patterns are supported, if any.\nPlease note that this configuration also replaces original log urls in event log,\n    which will be also effective when accessing the application on history server. The new log urls must be\n    permanent, otherwise you might have dead link for executor log urls.\nFor now, only YARN and K8s cluster manager supports this configuration\n3.0.0\nspark.ui.prometheus.enabled\ntrue\nExpose executor metrics at /metrics/executors/prometheus at driver web page.\n3.0.0\nspark.worker.ui.retainedExecutors\n1000\nHow many finished executors the Spark UI and status APIs remember before garbage collecting.\n1.5.0\nspark.worker.ui.retainedDrivers\n1000\nHow many finished drivers the Spark UI and status APIs remember before garbage collecting.\n1.5.0\nspark.sql.ui.retainedExecutions\n1000\nHow many finished executions the Spark UI and status APIs remember before garbage collecting.\n1.5.0\nspark.streaming.ui.retainedBatches\n1000\nHow many finished batches the Spark UI and status APIs remember before garbage collecting.\n1.0.0\nspark.ui.retainedDeadExecutors\n100\nHow many dead executors the Spark UI and status APIs remember before garbage collecting.\n2.0.0\nspark.ui.filters\nNone\nComma separated list of filter class names to apply to the Spark Web UI. The filter should be a\n    standard\njavax servlet Filter\n.\nFilter parameters can also be specified in the configuration, by setting config entries\n    of the form\nspark.<class name of filter>.param.<param name>=<value>\nFor example:\nspark.ui.filters=com.test.filter1\nspark.com.test.filter1.param.name1=foo\nspark.com.test.filter1.param.name2=bar\n1.0.0\nspark.ui.requestHeaderSize\n8k\nThe maximum allowed size for a HTTP request header, in bytes unless otherwise specified.\n    This setting applies for the Spark History Server too.\n2.2.3\nspark.ui.timelineEnabled\ntrue\nWhether to display event timeline data on UI pages.\n3.4.0\nspark.ui.timeline.executors.maximum\n250\nThe maximum number of executors shown in the event timeline.\n3.2.0\nspark.ui.timeline.jobs.maximum\n500\nThe maximum number of jobs shown in the event timeline.\n3.2.0\nspark.ui.timeline.stages.maximum\n500\nThe maximum number of stages shown in the event timeline.\n3.2.0\nspark.ui.timeline.tasks.maximum\n1000\nThe maximum number of tasks shown in the event timeline.\n1.4.0\nspark.appStatusStore.diskStoreDir\nNone\nLocal directory where to store diagnostic information of SQL executions. This configuration is only for live UI.\n3.4.0\nCompression and Serialization\nProperty Name\nDefault\nMeaning\nSince Version\nspark.broadcast.compress\ntrue\nWhether to compress broadcast variables before sending them. Generally a good idea.\n    Compression will use\nspark.io.compression.codec\n.\n0.6.0\nspark.checkpoint.dir\n(none)\nSet the default directory for checkpointing. It can be overwritten by\n    SparkContext.setCheckpointDir.\n4.0.0\nspark.checkpoint.compress\nfalse\nWhether to compress RDD checkpoints. Generally a good idea.\n    Compression will use\nspark.io.compression.codec\n.\n2.2.0\nspark.io.compression.codec\nlz4\nThe codec used to compress internal data such as RDD partitions, event log, broadcast variables\n    and shuffle outputs. By default, Spark provides four codecs:\nlz4\n,\nlzf\n,\nsnappy\n, and\nzstd\n. You can also use fully qualified class names to specify the codec,\n    e.g.\norg.apache.spark.io.LZ4CompressionCodec\n,\norg.apache.spark.io.LZFCompressionCodec\n,\norg.apache.spark.io.SnappyCompressionCodec\n,\n    and\norg.apache.spark.io.ZStdCompressionCodec\n.\n0.8.0\nspark.io.compression.lz4.blockSize\n32k\nBlock size used in LZ4 compression, in the case when LZ4 compression codec\n    is used. Lowering this block size will also lower shuffle memory usage when LZ4 is used.\n    Default unit is bytes, unless otherwise specified. This configuration only applies to\nspark.io.compression.codec\n.\n1.4.0\nspark.io.compression.snappy.blockSize\n32k\nBlock size in Snappy compression, in the case when Snappy compression codec is used.\n    Lowering this block size will also lower shuffle memory usage when Snappy is used.\n    Default unit is bytes, unless otherwise specified. This configuration only applies\n    to\nspark.io.compression.codec\n.\n1.4.0\nspark.io.compression.zstd.level\n1\nCompression level for Zstd compression codec. Increasing the compression level will result in better\n    compression at the expense of more CPU and memory. This configuration only applies to\nspark.io.compression.codec\n.\n2.3.0\nspark.io.compression.zstd.bufferSize\n32k\nBuffer size in bytes used in Zstd compression, in the case when Zstd compression codec\n    is used. Lowering this size will lower the shuffle memory usage when Zstd is used, but it\n    might increase the compression cost because of excessive JNI call overhead. This\n    configuration only applies to\nspark.io.compression.codec\n.\n2.3.0\nspark.io.compression.zstd.bufferPool.enabled\ntrue\nIf true, enable buffer pool of ZSTD JNI library.\n3.2.0\nspark.io.compression.zstd.workers\n0\nThread size spawned to compress in parallel when using Zstd. When value is 0\n    no worker is spawned, it works in single-threaded mode. When value > 0, it triggers\n    asynchronous mode, corresponding number of threads are spawned. More workers improve\n    performance, but also increase memory cost.\n4.0.0\nspark.io.compression.lzf.parallel.enabled\nfalse\nWhen true, LZF compression will use multiple threads to compress data in parallel.\n4.0.0\nspark.kryo.classesToRegister\n(none)\nIf you use Kryo serialization, give a comma-separated list of custom class names to register\n    with Kryo.\n    See the\ntuning guide\nfor more details.\n1.2.0\nspark.kryo.referenceTracking\ntrue\nWhether to track references to the same object when serializing data with Kryo, which is\n    necessary if your object graphs have loops and useful for efficiency if they contain multiple\n    copies of the same object. Can be disabled to improve performance if you know this is not the\n    case.\n0.8.0\nspark.kryo.registrationRequired\nfalse\nWhether to require registration with Kryo. If set to 'true', Kryo will throw an exception\n    if an unregistered class is serialized. If set to false (the default), Kryo will write\n    unregistered class names along with each object. Writing class names can cause\n    significant performance overhead, so enabling this option can enforce strictly that a\n    user has not omitted classes from registration.\n1.1.0\nspark.kryo.registrator\n(none)\nIf you use Kryo serialization, give a comma-separated list of classes that register your custom classes with Kryo. This\n    property is useful if you need to register your classes in a custom way, e.g. to specify a custom\n    field serializer. Otherwise\nspark.kryo.classesToRegister\nis simpler. It should be\n    set to classes that extend\nKryoRegistrator\n.\n    See the\ntuning guide\nfor more details.\n0.5.0\nspark.kryo.unsafe\ntrue\nWhether to use unsafe based Kryo serializer. Can be\n    substantially faster by using Unsafe Based IO.\n2.1.0\nspark.kryoserializer.buffer.max\n64m\nMaximum allowable size of Kryo serialization buffer, in MiB unless otherwise specified.\n    This must be larger than any object you attempt to serialize and must be less than 2048m.\n    Increase this if you get a \"buffer limit exceeded\" exception inside Kryo.\n1.4.0\nspark.kryoserializer.buffer\n64k\nInitial size of Kryo's serialization buffer, in KiB unless otherwise specified.\n    Note that there will be one buffer\nper core\non each worker. This buffer will grow up to\nspark.kryoserializer.buffer.max\nif needed.\n1.4.0\nspark.rdd.compress\nfalse\nWhether to compress serialized RDD partitions (e.g. for\nStorageLevel.MEMORY_ONLY_SER\nin Java\n    and Scala or\nStorageLevel.MEMORY_ONLY\nin Python).\n    Can save substantial space at the cost of some extra CPU time.\n    Compression will use\nspark.io.compression.codec\n.\n0.6.0\nspark.serializer\norg.apache.spark.serializer.\nJavaSerializer\nClass to use for serializing objects that will be sent over the network or need to be cached\n    in serialized form. The default of Java serialization works with any Serializable Java object\n    but is quite slow, so we recommend\nusing\norg.apache.spark.serializer.KryoSerializer\nand configuring Kryo serialization\nwhen speed is necessary. Can be any subclass of\norg.apache.spark.Serializer\n.\n0.5.0\nspark.serializer.objectStreamReset\n100\nWhen serializing using org.apache.spark.serializer.JavaSerializer, the serializer caches\n    objects to prevent writing redundant data, however that stops garbage collection of those\n    objects. By calling 'reset' you flush that info from the serializer, and allow old\n    objects to be collected. To turn off this periodic reset set it to -1.\n    By default it will reset the serializer every 100 objects.\n1.0.0\nMemory Management\nProperty Name\nDefault\nMeaning\nSince Version\nspark.memory.fraction\n0.6\nFraction of (heap space - 300MB) used for execution and storage. The lower this is, the\n    more frequently spills and cached data eviction occur. The purpose of this config is to set\n    aside memory for internal metadata, user data structures, and imprecise size estimation\n    in the case of sparse, unusually large records. Leaving this at the default value is\n    recommended. For more detail, including important information about correctly tuning JVM\n    garbage collection when increasing this value, see\nthis description\n.\n1.6.0\nspark.memory.storageFraction\n0.5\nAmount of storage memory immune to eviction, expressed as a fraction of the size of the\n    region set aside by\nspark.memory.fraction\n. The higher this is, the less\n    working memory may be available to execution and tasks may spill to disk more often.\n    Leaving this at the default value is recommended. For more detail, see\nthis description\n.\n1.6.0\nspark.memory.offHeap.enabled\nfalse\nIf true, Spark will attempt to use off-heap memory for certain operations. If off-heap memory\n    use is enabled, then\nspark.memory.offHeap.size\nmust be positive.\n1.6.0\nspark.memory.offHeap.size\n0\nThe absolute amount of memory which can be used for off-heap allocation, in bytes unless otherwise specified.\n    This setting has no impact on heap memory usage, so if your executors' total memory consumption\n    must fit within some hard limit then be sure to shrink your JVM heap size accordingly.\n    This must be set to a positive value when\nspark.memory.offHeap.enabled=true\n.\n1.6.0\nspark.storage.unrollMemoryThreshold\n1024 * 1024\nInitial memory to request before unrolling any block.\n1.1.0\nspark.storage.replication.proactive\ntrue\nEnables proactive block replication for RDD blocks. Cached RDD block replicas lost due to\n    executor failures are replenished if there are any existing available replicas. This tries\n    to get the replication level of the block to the initial number.\n2.2.0\nspark.storage.localDiskByExecutors.cacheSize\n1000\nThe max number of executors for which the local dirs are stored. This size is both applied for the driver and\n    both for the executors side to avoid having an unbounded store. This cache will be used to avoid the network\n    in case of fetching disk persisted RDD blocks or shuffle blocks (when\nspark.shuffle.readHostLocalDisk\nis set) from the same host.\n3.0.0\nspark.cleaner.periodicGC.interval\n30min\nControls how often to trigger a garbage collection.\nThis context cleaner triggers cleanups only when weak references are garbage collected.\n    In long-running applications with large driver JVMs, where there is little memory pressure\n    on the driver, this may happen very occasionally or not at all. Not cleaning at all may\n    lead to executors running out of disk space after a while.\n1.6.0\nspark.cleaner.referenceTracking\ntrue\nEnables or disables context cleaning.\n1.0.0\nspark.cleaner.referenceTracking.blocking\ntrue\nControls whether the cleaning thread should block on cleanup tasks (other than shuffle, which is controlled by\nspark.cleaner.referenceTracking.blocking.shuffle\nSpark property).\n1.0.0\nspark.cleaner.referenceTracking.blocking.shuffle\nfalse\nControls whether the cleaning thread should block on shuffle cleanup tasks.\n1.1.1\nspark.cleaner.referenceTracking.cleanCheckpoints\nfalse\nControls whether to clean checkpoint files if the reference is out of scope.\n1.4.0\nExecution Behavior\nProperty Name\nDefault\nMeaning\nSince Version\nspark.broadcast.blockSize\n4m\nSize of each piece of a block for\nTorrentBroadcastFactory\n, in KiB unless otherwise\n    specified. Too large a value decreases parallelism during broadcast (makes it slower); however,\n    if it is too small,\nBlockManager\nmight take a performance hit.\n0.5.0\nspark.broadcast.checksum\ntrue\nWhether to enable checksum for broadcast. If enabled, broadcasts will include a checksum, which can\n    help detect corrupted blocks, at the cost of computing and sending a little more data. It's possible\n    to disable it if the network has other mechanisms to guarantee data won't be corrupted during broadcast.\n2.1.1\nspark.broadcast.UDFCompressionThreshold\n1 * 1024 * 1024\nThe threshold at which user-defined functions (UDFs) and Python RDD commands are compressed by broadcast in bytes unless otherwise specified.\n3.0.0\nspark.executor.cores\n1 in YARN mode, all the available cores on the worker in standalone mode.\nThe number of cores to use on each executor.\n1.0.0\nspark.default.parallelism\nFor distributed shuffle operations like\nreduceByKey\nand\njoin\n, the\n    largest number of partitions in a parent RDD.  For operations like\nparallelize\nwith no parent RDDs, it depends on the cluster manager:\nLocal mode: number of cores on the local machine\nOthers: total number of cores on all executor nodes or 2, whichever is larger\nDefault number of partitions in RDDs returned by transformations like\njoin\n,\nreduceByKey\n, and\nparallelize\nwhen not set by user.\n0.5.0\nspark.executor.heartbeatInterval\n10s\nInterval between each executor's heartbeats to the driver.  Heartbeats let\n    the driver know that the executor is still alive and update it with metrics for in-progress\n    tasks. spark.executor.heartbeatInterval should be significantly less than\n    spark.network.timeout\n1.1.0\nspark.files.fetchTimeout\n60s\nCommunication timeout to use when fetching files added through SparkContext.addFile() from\n    the driver.\n1.0.0\nspark.files.useFetchCache\ntrue\nIf set to true (default), file fetching will use a local cache that is shared by executors\n    that belong to the same application, which can improve task launching performance when\n    running many executors on the same host. If set to false, these caching optimizations will\n    be disabled and all executors will fetch their own copies of files. This optimization may be\n    disabled in order to use Spark local directories that reside on NFS filesystems (see\nSPARK-6313\nfor more details).\n1.2.2\nspark.files.overwrite\nfalse\nWhether to overwrite any files which exist at the startup. Users can not overwrite the files added by\nSparkContext.addFile\nor\nSparkContext.addJar\nbefore even if this option is set\ntrue\n.\n1.0.0\nspark.files.ignoreCorruptFiles\nfalse\nWhether to ignore corrupt files. If true, the Spark jobs will continue to run when encountering corrupted or\n    non-existing files and contents that have been read will still be returned.\n2.1.0\nspark.files.ignoreMissingFiles\nfalse\nWhether to ignore missing files. If true, the Spark jobs will continue to run when encountering missing files and\n    the contents that have been read will still be returned.\n2.4.0\nspark.files.maxPartitionBytes\n134217728 (128 MiB)\nThe maximum number of bytes to pack into a single partition when reading files.\n2.1.0\nspark.files.openCostInBytes\n4194304 (4 MiB)\nThe estimated cost to open a file, measured by the number of bytes could be scanned at the same\n    time. This is used when putting multiple files into a partition. It is better to overestimate,\n    then the partitions with small files will be faster than partitions with bigger files.\n2.1.0\nspark.hadoop.cloneConf\nfalse\nIf set to true, clones a new Hadoop\nConfiguration\nobject for each task.  This\n    option should be enabled to work around\nConfiguration\nthread-safety issues (see\nSPARK-2546\nfor more details).\n    This is disabled by default in order to avoid unexpected performance regressions for jobs that\n    are not affected by these issues.\n1.0.3\nspark.hadoop.validateOutputSpecs\ntrue\nIf set to true, validates the output specification (e.g. checking if the output directory already exists)\n    used in saveAsHadoopFile and other variants. This can be disabled to silence exceptions due to pre-existing\n    output directories. We recommend that users do not disable this except if trying to achieve compatibility\n    with previous versions of Spark. Simply use Hadoop's FileSystem API to delete output directories by hand.\n    This setting is ignored for jobs generated through Spark Streaming's StreamingContext, since data may\n    need to be rewritten to pre-existing output directories during checkpoint recovery.\n1.0.1\nspark.storage.memoryMapThreshold\n2m\nSize of a block above which Spark memory maps when reading a block from disk. Default unit is bytes,\n    unless specified otherwise. This prevents Spark from memory mapping very small blocks. In general,\n    memory mapping has high overhead for blocks close to or below the page size of the operating system.\n0.9.2\nspark.storage.decommission.enabled\nfalse\nWhether to decommission the block manager when decommissioning executor.\n3.1.0\nspark.storage.decommission.shuffleBlocks.enabled\ntrue\nWhether to transfer shuffle blocks during block manager decommissioning. Requires a migratable shuffle resolver\n    (like sort based shuffle).\n3.1.0\nspark.storage.decommission.shuffleBlocks.maxThreads\n8\nMaximum number of threads to use in migrating shuffle files.\n3.1.0\nspark.storage.decommission.rddBlocks.enabled\ntrue\nWhether to transfer RDD blocks during block manager decommissioning.\n3.1.0\nspark.storage.decommission.fallbackStorage.path\n(none)\nThe location for fallback storage during block manager decommissioning. For example,\ns3a://spark-storage/\n.\n    In case of empty, fallback storage is disabled. The storage should be managed by TTL because Spark will not clean it up.\n3.1.0\nspark.storage.decommission.fallbackStorage.cleanUp\nfalse\nIf true, Spark cleans up its fallback storage data during shutting down.\n3.2.0\nspark.storage.decommission.shuffleBlocks.maxDiskSize\n(none)\nMaximum disk space to use to store shuffle blocks before rejecting remote shuffle blocks.\n    Rejecting remote shuffle blocks means that an executor will not receive any shuffle migrations,\n    and if there are no other executors available for migration then shuffle blocks will be lost unless\nspark.storage.decommission.fallbackStorage.path\nis configured.\n3.2.0\nspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\n1\nThe file output committer algorithm version, valid algorithm version number: 1 or 2.\n    Note that 2 may cause a correctness issue like MAPREDUCE-7282.\n2.2.0\nExecutor Metrics\nProperty Name\nDefault\nMeaning\nSince Version\nspark.eventLog.logStageExecutorMetrics\nfalse\nWhether to write per-stage peaks of executor metrics (for each executor) to the event log.\nNote:\nThe metrics are polled (collected) and sent in the executor heartbeat,\n    and this is always done; this configuration is only to determine if aggregated metric peaks\n    are written to the event log.\n3.0.0\nspark.executor.processTreeMetrics.enabled\nfalse\nWhether to collect process tree metrics (from the /proc filesystem) when collecting\n    executor metrics.\nNote:\nThe process tree metrics are collected only if the /proc filesystem\n    exists.\n3.0.0\nspark.executor.metrics.pollingInterval\n0\nHow often to collect executor metrics (in milliseconds).\nIf 0, the polling is done on executor heartbeats (thus at the heartbeat interval,\n    specified by\nspark.executor.heartbeatInterval\n).\n    If positive, the polling is done at this interval.\n3.0.0\nspark.eventLog.gcMetrics.youngGenerationGarbageCollectors\nCopy,PS Scavenge,ParNew,G1 Young Generation\nNames of supported young generation garbage collector. A name usually is the return of GarbageCollectorMXBean.getName.\n    The built-in young generation garbage collectors are Copy,PS Scavenge,ParNew,G1 Young Generation.\n3.0.0\nspark.eventLog.gcMetrics.oldGenerationGarbageCollectors\nMarkSweepCompact,PS MarkSweep,ConcurrentMarkSweep,G1 Old Generation\nNames of supported old generation garbage collector. A name usually is the return of GarbageCollectorMXBean.getName.\n    The built-in old generation garbage collectors are MarkSweepCompact,PS MarkSweep,ConcurrentMarkSweep,G1 Old Generation.\n3.0.0\nspark.executor.metrics.fileSystemSchemes\nfile,hdfs\nThe file system schemes to report in executor metrics.\n3.1.0\nNetworking\nProperty Name\nDefault\nMeaning\nSince Version\nspark.rpc.message.maxSize\n128\nMaximum message size (in MiB) to allow in \"control plane\" communication; generally only applies to map\n    output size information sent between executors and the driver. Increase this if you are running\n    jobs with many thousands of map and reduce tasks and see messages about the RPC message size.\n2.0.0\nspark.blockManager.port\n(random)\nPort for all block managers to listen on. These exist on both the driver and the executors.\n1.1.0\nspark.driver.blockManager.port\n(value of spark.blockManager.port)\nDriver-specific port for the block manager to listen on, for cases where it cannot use the same\n    configuration as executors.\n2.1.0\nspark.driver.bindAddress\n(value of spark.driver.host)\nHostname or IP address where to bind listening sockets. This config overrides the SPARK_LOCAL_IP\n    environment variable (see below).\nIt also allows a different address from the local one to be advertised to executors or external systems.\n    This is useful, for example, when running containers with bridged networking. For this to properly work,\n    the different ports used by the driver (RPC, block manager and UI) need to be forwarded from the\n    container's host.\n2.1.0\nspark.driver.host\n(local hostname)\nHostname or IP address for the driver.\n    This is used for communicating with the executors and the standalone Master.\n0.7.0\nspark.driver.port\n(random)\nPort for the driver to listen on.\n    This is used for communicating with the executors and the standalone Master.\n0.7.0\nspark.rpc.io.backLog\n64\nLength of the accept queue for the RPC server. For large applications, this value may\n    need to be increased, so that incoming connections are not dropped when a large number of\n    connections arrives in a short period of time.\n3.0.0\nspark.network.timeout\n120s\nDefault timeout for all network interactions. This config will be used in place of\nspark.storage.blockManagerHeartbeatTimeoutMs\n,\nspark.shuffle.io.connectionTimeout\n,\nspark.rpc.askTimeout\nor\nspark.rpc.lookupTimeout\nif they are not configured.\n1.3.0\nspark.network.timeoutInterval\n60s\nInterval for the driver to check and expire dead executors.\n1.3.2\nspark.network.io.preferDirectBufs\ntrue\nIf enabled then off-heap buffer allocations are preferred by the shared allocators.\n    Off-heap buffers are used to reduce garbage collection during shuffle and cache\n    block transfer. For environments where off-heap memory is tightly limited, users may wish to\n    turn this off to force all allocations to be on-heap.\n3.0.0\nspark.port.maxRetries\n16\nMaximum number of retries when binding to a port before giving up.\n    When a port is given a specific value (non 0), each subsequent retry will\n    increment the port used in the previous attempt by 1 before retrying. This\n    essentially allows it to try a range of ports from the start port specified\n    to port + maxRetries.\n1.1.1\nspark.rpc.askTimeout\nspark.network.timeout\nDuration for an RPC ask operation to wait before timing out.\n1.4.0\nspark.rpc.lookupTimeout\n120s\nDuration for an RPC remote endpoint lookup operation to wait before timing out.\n1.4.0\nspark.network.maxRemoteBlockSizeFetchToMem\n200m\nRemote block will be fetched to disk when size of the block is above this threshold\n    in bytes. This is to avoid a giant request takes too much memory. Note this\n    configuration will affect both shuffle fetch and block manager remote block fetch.\n    For users who enabled external shuffle service, this feature can only work when\n    external shuffle service is at least 2.3.0.\n3.0.0\nspark.rpc.io.connectionTimeout\nvalue of\nspark.network.timeout\nTimeout for the established connections between RPC peers to be marked as idled and closed\n    if there are outstanding RPC requests but no traffic on the channel for at least\nconnectionTimeout\n.\n1.2.0\nspark.rpc.io.connectionCreationTimeout\nvalue of\nspark.rpc.io.connectionTimeout\nTimeout for establishing a connection between RPC peers.\n3.2.0\nScheduling\nProperty Name\nDefault\nMeaning\nSince Version\nspark.cores.max\n(not set)\nWhen running on a\nstandalone deploy cluster\n, \n    the maximum amount of CPU cores to request for the application from\n    across the cluster (not from each machine). If not set, the default will be\nspark.deploy.defaultCores\non Spark's standalone cluster manager.\n0.6.0\nspark.locality.wait\n3s\nHow long to wait to launch a data-local task before giving up and launching it\n    on a less-local node. The same wait will be used to step through multiple locality levels\n    (process-local, node-local, rack-local and then any). It is also possible to customize the\n    waiting time for each level by setting\nspark.locality.wait.node\n, etc.\n    You should increase this setting if your tasks are long and see poor locality, but the\n    default usually works well.\n0.5.0\nspark.locality.wait.node\nspark.locality.wait\nCustomize the locality wait for node locality. For example, you can set this to 0 to skip\n    node locality and search immediately for rack locality (if your cluster has rack information).\n0.8.0\nspark.locality.wait.process\nspark.locality.wait\nCustomize the locality wait for process locality. This affects tasks that attempt to access\n    cached data in a particular executor process.\n0.8.0\nspark.locality.wait.rack\nspark.locality.wait\nCustomize the locality wait for rack locality.\n0.8.0\nspark.scheduler.maxRegisteredResourcesWaitingTime\n30s\nMaximum amount of time to wait for resources to register before scheduling begins.\n1.1.1\nspark.scheduler.minRegisteredResourcesRatio\n0.8 for KUBERNETES mode; 0.8 for YARN mode; 0.0 for standalone mode\nThe minimum ratio of registered resources (registered resources / total expected resources)\n    (resources are executors in yarn mode and Kubernetes mode, CPU cores in standalone mode)\n    to wait for before scheduling begins. Specified as a double between 0.0 and 1.0.\n    Regardless of whether the minimum ratio of resources has been reached,\n    the maximum amount of time it will wait before scheduling begins is controlled by config\nspark.scheduler.maxRegisteredResourcesWaitingTime\n.\n1.1.1\nspark.scheduler.mode\nFIFO\nThe\nscheduling mode\nbetween\n    jobs submitted to the same SparkContext. Can be set to\nFAIR\nto use fair sharing instead of queueing jobs one after another. Useful for\n    multi-user services.\n0.8.0\nspark.scheduler.revive.interval\n1s\nThe interval length for the scheduler to revive the worker resource offers to run tasks.\n0.8.1\nspark.scheduler.listenerbus.eventqueue.capacity\n10000\nThe default capacity for event queues. Spark will try to initialize an event queue\n    using capacity specified by\nspark.scheduler.listenerbus.eventqueue.queueName.capacity\nfirst. If it's not configured, Spark will use the default capacity specified by this\n    config. Note that capacity must be greater than 0. Consider increasing value (e.g. 20000)\n    if listener events are dropped. Increasing this value may result in the driver using more memory.\n2.3.0\nspark.scheduler.listenerbus.eventqueue.shared.capacity\nspark.scheduler.listenerbus.eventqueue.capacity\nCapacity for shared event queue in Spark listener bus, which hold events for external listener(s)\n    that register to the listener bus. Consider increasing value, if the listener events corresponding\n    to shared queue are dropped. Increasing this value may result in the driver using more memory.\n3.0.0\nspark.scheduler.listenerbus.eventqueue.appStatus.capacity\nspark.scheduler.listenerbus.eventqueue.capacity\nCapacity for appStatus event queue, which hold events for internal application status listeners.\n    Consider increasing value, if the listener events corresponding to appStatus queue are dropped.\n    Increasing this value may result in the driver using more memory.\n3.0.0\nspark.scheduler.listenerbus.eventqueue.executorManagement.capacity\nspark.scheduler.listenerbus.eventqueue.capacity\nCapacity for executorManagement event queue in Spark listener bus, which hold events for internal\n    executor management listeners. Consider increasing value if the listener events corresponding to\n    executorManagement queue are dropped. Increasing this value may result in the driver using more memory.\n3.0.0\nspark.scheduler.listenerbus.eventqueue.eventLog.capacity\nspark.scheduler.listenerbus.eventqueue.capacity\nCapacity for eventLog queue in Spark listener bus, which hold events for Event logging listeners\n    that write events to eventLogs. Consider increasing value if the listener events corresponding to eventLog queue\n    are dropped. Increasing this value may result in the driver using more memory.\n3.0.0\nspark.scheduler.listenerbus.eventqueue.streams.capacity\nspark.scheduler.listenerbus.eventqueue.capacity\nCapacity for streams queue in Spark listener bus, which hold events for internal streaming listener.\n    Consider increasing value if the listener events corresponding to streams queue are dropped. Increasing\n    this value may result in the driver using more memory.\n3.0.0\nspark.scheduler.resource.profileMergeConflicts\nfalse\nIf set to \"true\", Spark will merge ResourceProfiles when different profiles are specified\n    in RDDs that get combined into a single stage. When they are merged, Spark chooses the maximum of\n    each resource and creates a new ResourceProfile. The default of false results in Spark throwing\n    an exception if multiple different ResourceProfiles are found in RDDs going into the same stage.\n3.1.0\nspark.scheduler.excludeOnFailure.unschedulableTaskSetTimeout\n120s\nThe timeout in seconds to wait to acquire a new executor and schedule a task before aborting a\n    TaskSet which is unschedulable because all executors are excluded due to task failures.\n2.4.1\nspark.standalone.submit.waitAppCompletion\nfalse\nIf set to true, Spark will merge ResourceProfiles when different profiles are specified in RDDs that get combined into a single stage.\n    When they are merged, Spark chooses the maximum of each resource and creates a new ResourceProfile.\n    The default of false results in Spark throwing an exception if multiple different ResourceProfiles are found in RDDs going into the same stage.\n3.1.0\nspark.excludeOnFailure.enabled\nfalse\nIf set to \"true\", prevent Spark from scheduling tasks on executors that have been excluded\n    due to too many task failures. The algorithm used to exclude executors and nodes can be further\n    controlled by the other \"spark.excludeOnFailure\" configuration options.\n    This config will be overridden by \"spark.excludeOnFailure.application.enabled\" and\n    \"spark.excludeOnFailure.taskAndStage.enabled\" to specify exclusion enablement on individual\n    levels.\n2.1.0\nspark.excludeOnFailure.application.enabled\nfalse\nIf set to \"true\", enables excluding executors for the entire application due to too many task\n    failures and prevent Spark from scheduling tasks on them.\n    This config overrides \"spark.excludeOnFailure.enabled\".\n4.0.0\nspark.excludeOnFailure.taskAndStage.enabled\nfalse\nIf set to \"true\", enables excluding executors on a task set level due to too many task\n    failures and prevent Spark from scheduling tasks on them.\n    This config overrides \"spark.excludeOnFailure.enabled\".\n4.0.0\nspark.excludeOnFailure.timeout\n1h\n(Experimental) How long a node or executor is excluded for the entire application, before it\n    is unconditionally removed from the excludelist to attempt running new tasks.\n2.1.0\nspark.excludeOnFailure.task.maxTaskAttemptsPerExecutor\n1\n(Experimental) For a given task, how many times it can be retried on one executor before the\n    executor is excluded for that task.\n2.1.0\nspark.excludeOnFailure.task.maxTaskAttemptsPerNode\n2\n(Experimental) For a given task, how many times it can be retried on one node, before the entire\n    node is excluded for that task.\n2.1.0\nspark.excludeOnFailure.stage.maxFailedTasksPerExecutor\n2\n(Experimental) How many different tasks must fail on one executor, within one stage, before the\n    executor is excluded for that stage.\n2.1.0\nspark.excludeOnFailure.stage.maxFailedExecutorsPerNode\n2\n(Experimental) How many different executors are marked as excluded for a given stage, before\n    the entire node is marked as failed for the stage.\n2.1.0\nspark.excludeOnFailure.application.maxFailedTasksPerExecutor\n2\n(Experimental) How many different tasks must fail on one executor, in successful task sets,\n    before the executor is excluded for the entire application.  Excluded executors will\n    be automatically added back to the pool of available resources after the timeout specified by\nspark.excludeOnFailure.timeout\n.  Note that with dynamic allocation, though, the executors\n    may get marked as idle and be reclaimed by the cluster manager.\n2.2.0\nspark.excludeOnFailure.application.maxFailedExecutorsPerNode\n2\n(Experimental) How many different executors must be excluded for the entire application,\n    before the node is excluded for the entire application.  Excluded nodes will\n    be automatically added back to the pool of available resources after the timeout specified by\nspark.excludeOnFailure.timeout\n.  Note that with dynamic allocation, though, the\n    executors on the node may get marked as idle and be reclaimed by the cluster manager.\n2.2.0\nspark.excludeOnFailure.killExcludedExecutors\nfalse\n(Experimental) If set to \"true\", allow Spark to automatically kill the executors\n    when they are excluded on fetch failure or excluded for the entire application,\n    as controlled by spark.killExcludedExecutors.application.*. Note that, when an entire node is added\n    excluded, all of the executors on that node will be killed.\n2.2.0\nspark.excludeOnFailure.application.fetchFailure.enabled\nfalse\n(Experimental) If set to \"true\", Spark will exclude the executor immediately when a fetch\n    failure happens. If external shuffle service is enabled, then the whole node will be\n    excluded.\n2.3.0\nspark.speculation\nfalse\nIf set to \"true\", performs speculative execution of tasks. This means if one or more tasks are\n    running slowly in a stage, they will be re-launched.\n0.6.0\nspark.speculation.interval\n100ms\nHow often Spark will check for tasks to speculate.\n0.6.0\nspark.speculation.multiplier\n3\nHow many times slower a task is than the median to be considered for speculation.\n0.6.0\nspark.speculation.quantile\n0.9\nFraction of tasks which must be complete before speculation is enabled for a particular stage.\n0.6.0\nspark.speculation.minTaskRuntime\n100ms\nMinimum amount of time a task runs before being considered for speculation.\n    This can be used to avoid launching speculative copies of tasks that are very short.\n3.2.0\nspark.speculation.task.duration.threshold\nNone\nTask duration after which scheduler would try to speculative run the task. If provided, tasks\n    would be speculatively run if current stage contains less tasks than or equal to the number of\n    slots on a single executor and the task is taking longer time than the threshold. This config\n    helps speculate stage with very few tasks. Regular speculation configs may also apply if the\n    executor slots are large enough. E.g. tasks might be re-launched if there are enough successful\n    runs even though the threshold hasn't been reached. The number of slots is computed based on\n    the conf values of spark.executor.cores and spark.task.cpus minimum 1.\n    Default unit is bytes, unless otherwise specified.\n3.0.0\nspark.speculation.efficiency.processRateMultiplier\n0.75\nA multiplier that used when evaluating inefficient tasks. The higher the multiplier\n    is, the more tasks will be possibly considered as inefficient.\n3.4.0\nspark.speculation.efficiency.longRunTaskFactor\n2\nA task will be speculated anyway as long as its duration has exceeded the value of multiplying\n    the factor and the time threshold (either be\nspark.speculation.multiplier\n* successfulTaskDurations.median or\nspark.speculation.minTaskRuntime\n) regardless\n    of it's data process rate is good or not. This avoids missing the inefficient tasks when task\n    slow isn't related to data process rate.\n3.4.0\nspark.speculation.efficiency.enabled\ntrue\nWhen set to true, spark will evaluate the efficiency of task processing through the stage task\n    metrics or its duration, and only need to speculate the inefficient tasks. A task is inefficient\n    when 1)its data process rate is less than the average data process rate of all successful tasks\n    in the stage multiplied by a multiplier or 2)its duration has exceeded the value of multiplying\nspark.speculation.efficiency.longRunTaskFactor\nand the time threshold (either be\nspark.speculation.multiplier\n* successfulTaskDurations.median or\nspark.speculation.minTaskRuntime\n).\n3.4.0\nspark.task.cpus\n1\nNumber of cores to allocate for each task.\n0.5.0\nspark.task.resource.{resourceName}.amount\n1\nAmount of a particular resource type to allocate for each task, note that this can be a double.\n    If this is specified you must also provide the executor config\nspark.executor.resource.{resourceName}.amount\nand any corresponding discovery configs\n    so that your executors are created with that resource type. In addition to whole amounts,\n    a fractional amount (for example, 0.25, which means 1/4th of a resource) may be specified.\n    Fractional amounts must be less than or equal to 0.5, or in other words, the minimum amount of\n    resource sharing is 2 tasks per resource. Additionally, fractional amounts are floored\n    in order to assign resource slots (e.g. a 0.2222 configuration, or 1/0.2222 slots will become\n    4 tasks/resource, not 5).\n3.0.0\nspark.task.maxFailures\n4\nNumber of continuous failures of any particular task before giving up on the job.\n    The total number of failures spread across different tasks will not cause the job\n    to fail; a particular task has to fail this number of attempts continuously.\n    If any attempt succeeds, the failure count for the task will be reset.\n    Should be greater than or equal to 1. Number of allowed retries = this value - 1.\n0.8.0\nspark.task.reaper.enabled\nfalse\nEnables monitoring of killed / interrupted tasks. When set to true, any task which is killed\n    will be monitored by the executor until that task actually finishes executing. See the other\nspark.task.reaper.*\nconfigurations for details on how to control the exact behavior\n    of this monitoring. When set to false (the default), task killing will use an older code\n    path which lacks such monitoring.\n2.0.3\nspark.task.reaper.pollingInterval\n10s\nWhen\nspark.task.reaper.enabled = true\n, this setting controls the frequency at which\n    executors will poll the status of killed tasks. If a killed task is still running when polled\n    then a warning will be logged and, by default, a thread-dump of the task will be logged\n    (this thread dump can be disabled via the\nspark.task.reaper.threadDump\nsetting,\n    which is documented below).\n2.0.3\nspark.task.reaper.threadDump\ntrue\nWhen\nspark.task.reaper.enabled = true\n, this setting controls whether task thread\n    dumps are logged during periodic polling of killed tasks. Set this to false to disable\n    collection of thread dumps.\n2.0.3\nspark.task.reaper.killTimeout\n-1\nWhen\nspark.task.reaper.enabled = true\n, this setting specifies a timeout after\n    which the executor JVM will kill itself if a killed task has not stopped running. The default\n    value, -1, disables this mechanism and prevents the executor from self-destructing. The purpose\n    of this setting is to act as a safety-net to prevent runaway noncancellable tasks from rendering\n    an executor unusable.\n2.0.3\nspark.stage.maxConsecutiveAttempts\n4\nNumber of consecutive stage attempts allowed before a stage is aborted.\n2.2.0\nspark.stage.ignoreDecommissionFetchFailure\ntrue\nWhether ignore stage fetch failure caused by executor decommission when\n    count\nspark.stage.maxConsecutiveAttempts\n3.4.0\nBarrier Execution Mode\nProperty Name\nDefault\nMeaning\nSince Version\nspark.barrier.sync.timeout\n365d\nThe timeout in seconds for each\nbarrier()\ncall from a barrier task. If the\n    coordinator didn't receive all the sync messages from barrier tasks within the\n    configured time, throw a SparkException to fail all the tasks. The default value is set\n    to 31536000(3600 * 24 * 365) so the\nbarrier()\ncall shall wait for one year.\n2.4.0\nspark.scheduler.barrier.maxConcurrentTasksCheck.interval\n15s\nTime in seconds to wait between a max concurrent tasks check failure and the next\n    check. A max concurrent tasks check ensures the cluster can launch more concurrent\n    tasks than required by a barrier stage on job submitted. The check can fail in case\n    a cluster has just started and not enough executors have registered, so we wait for a\n    little while and try to perform the check again. If the check fails more than a\n    configured max failure times for a job then fail current job submission. Note this\n    config only applies to jobs that contain one or more barrier stages, we won't perform\n    the check on non-barrier jobs.\n2.4.0\nspark.scheduler.barrier.maxConcurrentTasksCheck.maxFailures\n40\nNumber of max concurrent tasks check failures allowed before fail a job submission.\n    A max concurrent tasks check ensures the cluster can launch more concurrent tasks than\n    required by a barrier stage on job submitted. The check can fail in case a cluster\n    has just started and not enough executors have registered, so we wait for a little\n    while and try to perform the check again. If the check fails more than a configured\n    max failure times for a job then fail current job submission. Note this config only\n    applies to jobs that contain one or more barrier stages, we won't perform the check on\n    non-barrier jobs.\n2.4.0\nDynamic Allocation\nProperty Name\nDefault\nMeaning\nSince Version\nspark.dynamicAllocation.enabled\nfalse\nWhether to use dynamic resource allocation, which scales the number of executors registered\n    with this application up and down based on the workload.\n    For more detail, see the description\nhere\n.\nThis requires one of the following conditions: \n    1) enabling external shuffle service through\nspark.shuffle.service.enabled\n, or\n    2) enabling shuffle tracking through\nspark.dynamicAllocation.shuffleTracking.enabled\n, or\n    3) enabling shuffle blocks decommission through\nspark.decommission.enabled\nand\nspark.storage.decommission.shuffleBlocks.enabled\n, or\n    4) (Experimental) configuring\nspark.shuffle.sort.io.plugin.class\nto use a custom\nShuffleDataIO\nwho's\nShuffleDriverComponents\nsupports reliable storage.\n    The following configurations are also relevant:\nspark.dynamicAllocation.minExecutors\n,\nspark.dynamicAllocation.maxExecutors\n, and\nspark.dynamicAllocation.initialExecutors\nspark.dynamicAllocation.executorAllocationRatio\n1.2.0\nspark.dynamicAllocation.executorIdleTimeout\n60s\nIf dynamic allocation is enabled and an executor has been idle for more than this duration,\n    the executor will be removed. For more detail, see this\ndescription\n.\n1.2.0\nspark.dynamicAllocation.cachedExecutorIdleTimeout\ninfinity\nIf dynamic allocation is enabled and an executor which has cached data blocks has been idle for more than this duration,\n    the executor will be removed. For more details, see this\ndescription\n.\n1.4.0\nspark.dynamicAllocation.initialExecutors\nspark.dynamicAllocation.minExecutors\nInitial number of executors to run if dynamic allocation is enabled.\nIf\n--num-executors\n(or\nspark.executor.instances\n) is set and larger than this value, it will\n    be used as the initial number of executors.\n1.3.0\nspark.dynamicAllocation.maxExecutors\ninfinity\nUpper bound for the number of executors if dynamic allocation is enabled.\n1.2.0\nspark.dynamicAllocation.minExecutors\n0\nLower bound for the number of executors if dynamic allocation is enabled.\n1.2.0\nspark.dynamicAllocation.executorAllocationRatio\n1\nBy default, the dynamic allocation will request enough executors to maximize the\n    parallelism according to the number of tasks to process. While this minimizes the\n    latency of the job, with small tasks this setting can waste a lot of resources due to\n    executor allocation overhead, as some executor might not even do any work.\n    This setting allows to set a ratio that will be used to reduce the number of\n    executors w.r.t. full parallelism.\n    Defaults to 1.0 to give maximum parallelism.\n    0.5 will divide the target number of executors by 2\n    The target number of executors computed by the dynamicAllocation can still be overridden\n    by the\nspark.dynamicAllocation.minExecutors\nand\nspark.dynamicAllocation.maxExecutors\nsettings\n2.4.0\nspark.dynamicAllocation.schedulerBacklogTimeout\n1s\nIf dynamic allocation is enabled and there have been pending tasks backlogged for more than\n    this duration, new executors will be requested. For more detail, see this\ndescription\n.\n1.2.0\nspark.dynamicAllocation.sustainedSchedulerBacklogTimeout\nschedulerBacklogTimeout\nSame as\nspark.dynamicAllocation.schedulerBacklogTimeout\n, but used only for\n    subsequent executor requests. For more detail, see this\ndescription\n.\n1.2.0\nspark.dynamicAllocation.shuffleTracking.enabled\ntrue\nEnables shuffle file tracking for executors, which allows dynamic allocation\n    without the need for an external shuffle service. This option will try to keep alive executors\n    that are storing shuffle data for active jobs.\n3.0.0\nspark.dynamicAllocation.shuffleTracking.timeout\ninfinity\nWhen shuffle tracking is enabled, controls the timeout for executors that are holding shuffle\n    data. The default value means that Spark will rely on the shuffles being garbage collected to be\n    able to release executors. If for some reason garbage collection is not cleaning up shuffles\n    quickly enough, this option can be used to control when to time out executors even when they are\n    storing shuffle data.\n3.0.0\nThread Configurations\nDepending on jobs and cluster configurations, we can set number of threads in several places in Spark to utilize\navailable resources efficiently to get better performance. Prior to Spark 3.0, these thread configurations apply\nto all roles of Spark, such as driver, executor, worker and master. From Spark 3.0, we can configure threads in\nfiner granularity starting from driver and executor. Take RPC module as example in below table. For other modules,\nlike shuffle, just replace “rpc” with “shuffle” in the property names except\nspark.{driver|executor}.rpc.netty.dispatcher.numThreads\n, which is only for RPC module.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.{driver|executor}.rpc.io.serverThreads\nFall back on\nspark.rpc.io.serverThreads\nNumber of threads used in the server thread pool\n1.6.0\nspark.{driver|executor}.rpc.io.clientThreads\nFall back on\nspark.rpc.io.clientThreads\nNumber of threads used in the client thread pool\n1.6.0\nspark.{driver|executor}.rpc.netty.dispatcher.numThreads\nFall back on\nspark.rpc.netty.dispatcher.numThreads\nNumber of threads used in RPC message dispatcher thread pool\n3.0.0\nThe default value for number of thread-related config keys is the minimum of the number of cores requested for\nthe driver or executor, or, in the absence of that value, the number of cores available for the JVM (with a hardcoded upper limit of 8).\nSpark Connect\nServer Configuration\nServer configurations are set in Spark Connect server, for example, when you start the Spark Connect server with\n./sbin/start-connect-server.sh\n.\nThey are typically set via the config file and command-line options with\n--conf/-c\n.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.api.mode\nclassic\nFor Spark Classic applications, specify whether to automatically use Spark Connect by running a local Spark Connect server. The value can be\nclassic\nor\nconnect\n.\n4.0.0\nspark.connect.grpc.binding.address\n(none)\nAddress for Spark Connect server to bind.\n4.0.0\nspark.connect.grpc.binding.port\n15002\nPort for Spark Connect server to bind.\n3.4.0\nspark.connect.grpc.port.maxRetries\n0\nThe max port retry attempts for the gRPC server binding. By default, it's set to 0, and the server will fail fast in case of port conflicts.\n4.0.0\nspark.connect.grpc.interceptor.classes\n(none)\nComma separated list of class names that must implement the\nio.grpc.ServerInterceptor\ninterface\n3.4.0\nspark.connect.grpc.arrow.maxBatchSize\n4m\nWhen using Apache Arrow, limit the maximum size of one arrow batch that can be sent from server side to client side. Currently, we conservatively use 70% of it because the size is not accurate but estimated.\n3.4.0\nspark.connect.grpc.maxInboundMessageSize\n134217728\nSets the maximum inbound message size for the gRPC requests. Requests with a larger payload will fail.\n3.4.0\nspark.connect.extensions.relation.classes\n(none)\nComma separated list of classes that implement the trait\norg.apache.spark.sql.connect.plugin.RelationPlugin\nto support custom\nRelation types in proto.\n3.4.0\nspark.connect.extensions.expression.classes\n(none)\nComma separated list of classes that implement the trait\norg.apache.spark.sql.connect.plugin.ExpressionPlugin\nto support custom\nExpression types in proto.\n3.4.0\nspark.connect.extensions.command.classes\n(none)\nComma separated list of classes that implement the trait\norg.apache.spark.sql.connect.plugin.CommandPlugin\nto support custom\nCommand types in proto.\n3.4.0\nspark.connect.ml.backend.classes\n(none)\nComma separated list of classes that implement the trait\norg.apache.spark.sql.connect.plugin.MLBackendPlugin\nto replace the specified Spark ML operators with a backend-specific implementation.\n4.0.0\nspark.connect.jvmStacktrace.maxSize\n1024\nSets the maximum stack trace size to display when `spark.sql.pyspark.jvmStacktrace.enabled` is true.\n3.5.0\nspark.sql.connect.ui.retainedSessions\n200\nThe number of client sessions kept in the Spark Connect UI history.\n3.5.0\nspark.sql.connect.ui.retainedStatements\n200\nThe number of statements kept in the Spark Connect UI history.\n3.5.0\nspark.sql.connect.enrichError.enabled\ntrue\nWhen true, it enriches errors with full exception messages and optionally server-side stacktrace on the client side via an additional RPC.\n4.0.0\nspark.sql.connect.serverStacktrace.enabled\ntrue\nWhen true, it sets the server-side stacktrace in the user-facing Spark exception.\n4.0.0\nspark.connect.grpc.maxMetadataSize\n1024\nSets the maximum size of metadata fields. For instance, it restricts metadata fields in `ErrorInfo`.\n4.0.0\nspark.connect.progress.reportInterval\n2s\nThe interval at which the progress of a query is reported to the client. If the value is set to a negative value the progress reports will be disabled.\n4.0.0\nSecurity\nPlease refer to the\nSecurity\npage for available options on how to secure different\nSpark subsystems.\nSpark SQL\nRuntime SQL Configuration\nRuntime SQL configurations are per-session, mutable Spark SQL configurations. They can be set with initial values by the config file\nand command-line options with\n--conf/-c\nprefixed, or by setting\nSparkConf\nthat are used to create\nSparkSession\n.\nAlso, they can be set and queried by SET commands and reset to their initial values by RESET command,\nor by\nSparkSession.conf\n’s setter and getter methods in runtime.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.adaptive.advisoryPartitionSizeInBytes\n(value of\nspark.sql.adaptive.shuffle.targetPostShuffleInputSize\n)\nThe advisory size in bytes of the shuffle partition during adaptive optimization (when spark.sql.adaptive.enabled is true). It takes effect when Spark coalesces small shuffle partitions or splits skewed shuffle partition.\n3.0.0\nspark.sql.adaptive.autoBroadcastJoinThreshold\n(none)\nConfigures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 broadcasting can be disabled. The default value is same with spark.sql.autoBroadcastJoinThreshold. Note that, this config is used only in adaptive framework.\n3.2.0\nspark.sql.adaptive.coalescePartitions.enabled\ntrue\nWhen true and 'spark.sql.adaptive.enabled' is true, Spark will coalesce contiguous shuffle partitions according to the target size (specified by 'spark.sql.adaptive.advisoryPartitionSizeInBytes'), to avoid too many small tasks.\n3.0.0\nspark.sql.adaptive.coalescePartitions.initialPartitionNum\n(none)\nThe initial number of shuffle partitions before coalescing. If not set, it equals to spark.sql.shuffle.partitions. This configuration only has an effect when 'spark.sql.adaptive.enabled' and 'spark.sql.adaptive.coalescePartitions.enabled' are both true.\n3.0.0\nspark.sql.adaptive.coalescePartitions.minPartitionSize\n1MB\nThe minimum size of shuffle partitions after coalescing. This is useful when the adaptively calculated target size is too small during partition coalescing.\n3.2.0\nspark.sql.adaptive.coalescePartitions.parallelismFirst\ntrue\nWhen true, Spark does not respect the target size specified by 'spark.sql.adaptive.advisoryPartitionSizeInBytes' (default 64MB) when coalescing contiguous shuffle partitions, but adaptively calculate the target size according to the default parallelism of the Spark cluster. The calculated size is usually smaller than the configured target size. This is to maximize the parallelism and avoid performance regressions when enabling adaptive query execution. It's recommended to set this config to false on a busy cluster to make resource utilization more efficient (not many small tasks).\n3.2.0\nspark.sql.adaptive.customCostEvaluatorClass\n(none)\nThe custom cost evaluator class to be used for adaptive execution. If not being set, Spark will use its own SimpleCostEvaluator by default.\n3.2.0\nspark.sql.adaptive.enabled\ntrue\nWhen true, enable adaptive query execution, which re-optimizes the query plan in the middle of query execution, based on accurate runtime statistics.\n1.6.0\nspark.sql.adaptive.forceOptimizeSkewedJoin\nfalse\nWhen true, force enable OptimizeSkewedJoin even if it introduces extra shuffle.\n3.3.0\nspark.sql.adaptive.localShuffleReader.enabled\ntrue\nWhen true and 'spark.sql.adaptive.enabled' is true, Spark tries to use local shuffle reader to read the shuffle data when the shuffle partitioning is not needed, for example, after converting sort-merge join to broadcast-hash join.\n3.0.0\nspark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold\n0b\nConfigures the maximum size in bytes per partition that can be allowed to build local hash map. If this value is not smaller than spark.sql.adaptive.advisoryPartitionSizeInBytes and all the partition size are not larger than this config, join selection prefer to use shuffled hash join instead of sort merge join regardless of the value of spark.sql.join.preferSortMergeJoin.\n3.2.0\nspark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled\ntrue\nWhen true and 'spark.sql.adaptive.enabled' is true, Spark will optimize the skewed shuffle partitions in RebalancePartitions and split them to smaller ones according to the target size (specified by 'spark.sql.adaptive.advisoryPartitionSizeInBytes'), to avoid data skew.\n3.2.0\nspark.sql.adaptive.optimizer.excludedRules\n(none)\nConfigures a list of rules to be disabled in the adaptive optimizer, in which the rules are specified by their rule names and separated by comma. The optimizer will log the rules that have indeed been excluded.\n3.1.0\nspark.sql.adaptive.rebalancePartitionsSmallPartitionFactor\n0.2\nA partition will be merged during splitting if its size is small than this factor multiply spark.sql.adaptive.advisoryPartitionSizeInBytes.\n3.3.0\nspark.sql.adaptive.skewJoin.enabled\ntrue\nWhen true and 'spark.sql.adaptive.enabled' is true, Spark dynamically handles skew in shuffled join (sort-merge and shuffled hash) by splitting (and replicating if needed) skewed partitions.\n3.0.0\nspark.sql.adaptive.skewJoin.skewedPartitionFactor\n5.0\nA partition is considered as skewed if its size is larger than this factor multiplying the median partition size and also larger than 'spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes'\n3.0.0\nspark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\n256MB\nA partition is considered as skewed if its size in bytes is larger than this threshold and also larger than 'spark.sql.adaptive.skewJoin.skewedPartitionFactor' multiplying the median partition size. Ideally this config should be set larger than 'spark.sql.adaptive.advisoryPartitionSizeInBytes'.\n3.0.0\nspark.sql.allowNamedFunctionArguments\ntrue\nIf true, Spark will turn on support for named parameters for all functions that has it implemented.\n3.5.0\nspark.sql.ansi.doubleQuotedIdentifiers\nfalse\nWhen true and 'spark.sql.ansi.enabled' is true, Spark SQL reads literals enclosed in double quoted (\") as identifiers. When false they are read as string literals.\n3.4.0\nspark.sql.ansi.enabled\ntrue\nWhen true, Spark SQL uses an ANSI compliant dialect instead of being Hive compliant. For example, Spark will throw an exception at runtime instead of returning null results when the inputs to a SQL operator/function are invalid. For full details of this dialect, you can find them in the section \"ANSI Compliance\" of Spark's documentation. Some ANSI dialect features may be not from the ANSI SQL standard directly, but their behaviors align with ANSI SQL's style\n3.0.0\nspark.sql.ansi.enforceReservedKeywords\nfalse\nWhen true and 'spark.sql.ansi.enabled' is true, the Spark SQL parser enforces the ANSI reserved keywords and forbids SQL queries that use reserved keywords as alias names and/or identifiers for table, view, function, etc.\n3.3.0\nspark.sql.ansi.relationPrecedence\nfalse\nWhen true and 'spark.sql.ansi.enabled' is true, JOIN takes precedence over comma when combining relation. For example,\nt1, t2 JOIN t3\nshould result to\nt1 X (t2 X t3)\n. If the config is false, the result is\n(t1 X t2) X t3\n.\n3.4.0\nspark.sql.autoBroadcastJoinThreshold\n10MB\nConfigures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 broadcasting can be disabled.\n1.1.0\nspark.sql.avro.compression.codec\nsnappy\nCompression codec used in writing of AVRO files. Supported codecs: uncompressed, deflate, snappy, bzip2, xz and zstandard. Default codec is snappy.\n2.4.0\nspark.sql.avro.deflate.level\n-1\nCompression level for the deflate codec used in writing of AVRO files. Valid value must be in the range of from 1 to 9 inclusive or -1. The default value is -1 which corresponds to 6 level in the current implementation.\n2.4.0\nspark.sql.avro.filterPushdown.enabled\ntrue\nWhen true, enable filter pushdown to Avro datasource.\n3.1.0\nspark.sql.avro.xz.level\n6\nCompression level for the xz codec used in writing of AVRO files. Valid value must be in the range of from 1 to 9 inclusive The default value is 6.\n4.0.0\nspark.sql.avro.zstandard.bufferPool.enabled\nfalse\nIf true, enable buffer pool of ZSTD JNI library when writing of AVRO files\n4.0.0\nspark.sql.avro.zstandard.level\n3\nCompression level for the zstandard codec used in writing of AVRO files.\n4.0.0\nspark.sql.binaryOutputStyle\n(none)\nThe output style used display binary data. Valid values are 'UTF-8', 'BASIC', 'BASE64', 'HEX', and 'HEX_DISCRETE'.\n4.0.0\nspark.sql.broadcastTimeout\n300\nTimeout in seconds for the broadcast wait time in broadcast joins.\n1.3.0\nspark.sql.bucketing.coalesceBucketsInJoin.enabled\nfalse\nWhen true, if two bucketed tables with the different number of buckets are joined, the side with a bigger number of buckets will be coalesced to have the same number of buckets as the other side. Bigger number of buckets is divisible by the smaller number of buckets. Bucket coalescing is applied to sort-merge joins and shuffled hash join. Note: Coalescing bucketed table can avoid unnecessary shuffling in join, but it also reduces parallelism and could possibly cause OOM for shuffled hash join.\n3.1.0\nspark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio\n4\nThe ratio of the number of two buckets being coalesced should be less than or equal to this value for bucket coalescing to be applied. This configuration only has an effect when 'spark.sql.bucketing.coalesceBucketsInJoin.enabled' is set to true.\n3.1.0\nspark.sql.catalog.spark_catalog\nbuiltin\nA catalog implementation that will be used as the v2 interface to Spark's built-in v1 catalog: spark_catalog. This catalog shares its identifier namespace with the spark_catalog and must be consistent with it; for example, if a table can be loaded by the spark_catalog, this catalog must also return the table metadata. To delegate operations to the spark_catalog, implementations can extend 'CatalogExtension'. The value should be either 'builtin' which represents the spark's builit-in V2SessionCatalog, or a fully qualified class name of the catalog implementation.\n3.0.0\nspark.sql.cbo.enabled\nfalse\nEnables CBO for estimation of plan statistics when set true.\n2.2.0\nspark.sql.cbo.joinReorder.dp.star.filter\nfalse\nApplies star-join filter heuristics to cost based join enumeration.\n2.2.0\nspark.sql.cbo.joinReorder.dp.threshold\n12\nThe maximum number of joined nodes allowed in the dynamic programming algorithm.\n2.2.0\nspark.sql.cbo.joinReorder.enabled\nfalse\nEnables join reorder in CBO.\n2.2.0\nspark.sql.cbo.planStats.enabled\nfalse\nWhen true, the logical plan will fetch row counts and column statistics from catalog.\n3.0.0\nspark.sql.cbo.starSchemaDetection\nfalse\nWhen true, it enables join reordering based on star schema detection.\n2.2.0\nspark.sql.charAsVarchar\nfalse\nWhen true, Spark replaces CHAR type with VARCHAR type in CREATE/REPLACE/ALTER TABLE commands, so that newly created/updated tables will not have CHAR type columns/fields. Existing tables with CHAR type columns/fields are not affected by this config.\n3.3.0\nspark.sql.chunkBase64String.enabled\ntrue\nWhether to truncate string generated by the\nBase64\nfunction. When true, base64 strings generated by the base64 function are chunked into lines of at most 76 characters. When false, the base64 strings are not chunked.\n3.5.2\nspark.sql.cli.print.header\nfalse\nWhen set to true, spark-sql CLI prints the names of the columns in query output.\n3.2.0\nspark.sql.columnNameOfCorruptRecord\n_corrupt_record\nThe name of internal column for storing raw/un-parsed JSON and CSV records that fail to parse.\n1.2.0\nspark.sql.csv.filterPushdown.enabled\ntrue\nWhen true, enable filter pushdown to CSV datasource.\n3.0.0\nspark.sql.datetime.java8API.enabled\nfalse\nIf the configuration property is set to true, java.time.Instant and java.time.LocalDate classes of Java 8 API are used as external types for Catalyst's TimestampType and DateType. If it is set to false, java.sql.Timestamp and java.sql.Date are used for the same purpose.\n3.0.0\nspark.sql.debug.maxToStringFields\n25\nMaximum number of fields of sequence-like entries can be converted to strings in debug output. Any elements beyond the limit will be dropped and replaced by a  \"... N more fields\" placeholder.\n3.0.0\nspark.sql.defaultCacheStorageLevel\nMEMORY_AND_DISK\nThe default storage level of\ndataset.cache()\n,\ncatalog.cacheTable()\nand sql query\nCACHE TABLE t\n.\n4.0.0\nspark.sql.defaultCatalog\nspark_catalog\nName of the default catalog. This will be the current catalog if users have not explicitly set the current catalog yet.\n3.0.0\nspark.sql.error.messageFormat\nPRETTY\nWhen PRETTY, the error message consists of textual representation of error class, message and query context. The MINIMAL and STANDARD formats are pretty JSON formats where STANDARD includes an additional JSON field\nmessage\n. This configuration property influences on error messages of Thrift Server and SQL CLI while running queries.\n3.4.0\nspark.sql.execution.arrow.enabled\nfalse\n(Deprecated since Spark 3.0, please set 'spark.sql.execution.arrow.pyspark.enabled'.)\n2.3.0\nspark.sql.execution.arrow.fallback.enabled\ntrue\n(Deprecated since Spark 3.0, please set 'spark.sql.execution.arrow.pyspark.fallback.enabled'.)\n2.4.0\nspark.sql.execution.arrow.localRelationThreshold\n48MB\nWhen converting Arrow batches to Spark DataFrame, local collections are used in the driver side if the byte size of Arrow batches is smaller than this threshold. Otherwise, the Arrow batches are sent and deserialized to Spark internal rows in the executors.\n3.4.0\nspark.sql.execution.arrow.maxRecordsPerBatch\n10000\nWhen using Apache Arrow, limit the maximum number of records that can be written to a single ArrowRecordBatch in memory. This configuration is not effective for the grouping API such as DataFrame(.cogroup).groupby.applyInPandas because each group becomes each ArrowRecordBatch. If set to zero or negative there is no limit. See also spark.sql.execution.arrow.maxBytesPerBatch. If both are set, each batch is created when any condition of both is met.\n2.3.0\nspark.sql.execution.arrow.pyspark.enabled\n(value of\nspark.sql.execution.arrow.enabled\n)\nWhen true, make use of Apache Arrow for columnar data transfers in PySpark. This optimization applies to: 1. pyspark.sql.DataFrame.toPandas. 2. pyspark.sql.SparkSession.createDataFrame when its input is a Pandas DataFrame or a NumPy ndarray. The following data type is unsupported: ArrayType of TimestampType.\n3.0.0\nspark.sql.execution.arrow.pyspark.fallback.enabled\n(value of\nspark.sql.execution.arrow.fallback.enabled\n)\nWhen true, optimizations enabled by 'spark.sql.execution.arrow.pyspark.enabled' will fallback automatically to non-optimized implementations if an error occurs.\n3.0.0\nspark.sql.execution.arrow.pyspark.selfDestruct.enabled\nfalse\n(Experimental) When true, make use of Apache Arrow's self-destruct and split-blocks options for columnar data transfers in PySpark, when converting from Arrow to Pandas. This reduces memory usage at the cost of some CPU time. This optimization applies to: pyspark.sql.DataFrame.toPandas when 'spark.sql.execution.arrow.pyspark.enabled' is set.\n3.2.0\nspark.sql.execution.arrow.sparkr.enabled\nfalse\nWhen true, make use of Apache Arrow for columnar data transfers in SparkR. This optimization applies to: 1. createDataFrame when its input is an R DataFrame 2. collect 3. dapply 4. gapply The following data types are unsupported: FloatType, BinaryType, ArrayType, StructType and MapType.\n3.0.0\nspark.sql.execution.arrow.transformWithStateInPandas.maxRecordsPerBatch\n10000\nWhen using TransformWithStateInPandas, limit the maximum number of state records that can be written to a single ArrowRecordBatch in memory.\n4.0.0\nspark.sql.execution.arrow.useLargeVarTypes\nfalse\nWhen using Apache Arrow, use large variable width vectors for string and binary types. Regular string and binary types have a 2GiB limit for a column in a single record batch. Large variable types remove this limitation at the cost of higher memory usage per value.\n3.5.0\nspark.sql.execution.interruptOnCancel\ntrue\nWhen true, all running tasks will be interrupted if one cancels a query.\n4.0.0\nspark.sql.execution.pandas.inferPandasDictAsMap\nfalse\nWhen true, spark.createDataFrame will infer dict from Pandas DataFrame as a MapType. When false, spark.createDataFrame infers dict from Pandas DataFrame as a StructType which is default inferring from PyArrow.\n4.0.0\nspark.sql.execution.pandas.structHandlingMode\nlegacy\nThe conversion mode of struct type when creating pandas DataFrame. When \"legacy\", 1. when Arrow optimization is disabled, convert to Row object, 2. when Arrow optimization is enabled, convert to dict or raise an Exception if there are duplicated nested field names. When \"row\", convert to Row object regardless of Arrow optimization. When \"dict\", convert to dict and use suffixed key names, e.g., a_0, a_1, if there are duplicated nested field names, regardless of Arrow optimization.\n3.5.0\nspark.sql.execution.pandas.udf.buffer.size\n(value of\nspark.buffer.size\n)\nSame as\nspark.buffer.size\nbut only applies to Pandas UDF executions. If it is not set, the fallback is\nspark.buffer.size\n. Note that Pandas execution requires more than 4 bytes. Lowering this value could make small Pandas UDF batch iterated and pipelined; however, it might degrade performance. See SPARK-27870.\n3.0.0\nspark.sql.execution.pyspark.udf.faulthandler.enabled\n(value of\nspark.python.worker.faulthandler.enabled\n)\nSame as spark.python.worker.faulthandler.enabled for Python execution with DataFrame and SQL. It can change during runtime.\n4.0.0\nspark.sql.execution.pyspark.udf.hideTraceback.enabled\nfalse\nWhen true, only show the message of the exception from Python UDFs, hiding the stack trace. If this is enabled, simplifiedTraceback has no effect.\n4.0.0\nspark.sql.execution.pyspark.udf.idleTimeoutSeconds\n(value of\nspark.python.worker.idleTimeoutSeconds\n)\nSame as spark.python.worker.idleTimeoutSeconds for Python execution with DataFrame and SQL. It can change during runtime.\n4.0.0\nspark.sql.execution.pyspark.udf.simplifiedTraceback.enabled\ntrue\nWhen true, the traceback from Python UDFs is simplified. It hides the Python worker, (de)serialization, etc from PySpark in tracebacks, and only shows the exception messages from UDFs. Note that this works only with CPython 3.7+.\n3.1.0\nspark.sql.execution.python.udf.buffer.size\n(value of\nspark.buffer.size\n)\nSame as\nspark.buffer.size\nbut only applies to Python UDF executions. If it is not set, the fallback is\nspark.buffer.size\n.\n4.0.0\nspark.sql.execution.python.udf.maxRecordsPerBatch\n100\nWhen using Python UDFs, limit the maximum number of records that can be batched for serialization/deserialization.\n4.0.0\nspark.sql.execution.pythonUDF.arrow.concurrency.level\n(none)\nThe level of concurrency to execute Arrow-optimized Python UDF. This can be useful if Python UDFs use I/O intensively.\n4.0.0\nspark.sql.execution.pythonUDF.arrow.enabled\nfalse\nEnable Arrow optimization in regular Python UDFs. This optimization can only be enabled when the given function takes at least one argument.\n3.4.0\nspark.sql.execution.pythonUDTF.arrow.enabled\nfalse\nEnable Arrow optimization for Python UDTFs.\n3.5.0\nspark.sql.execution.topKSortFallbackThreshold\n2147483632\nIn SQL queries with a SORT followed by a LIMIT like 'SELECT x FROM t ORDER BY y LIMIT m', if m is under this threshold, do a top-K sort in memory, otherwise do a global sort which spills to disk if necessary.\n2.4.0\nspark.sql.extendedExplainProviders\n(none)\nA comma-separated list of classes that implement the org.apache.spark.sql.ExtendedExplainGenerator trait. If provided, Spark will print extended plan information from the providers in explain plan and in the UI\n4.0.0\nspark.sql.files.ignoreCorruptFiles\nfalse\nWhether to ignore corrupt files. If true, the Spark jobs will continue to run when encountering corrupted files and the contents that have been read will still be returned. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.\n2.1.1\nspark.sql.files.ignoreInvalidPartitionPaths\nfalse\nWhether to ignore invalid partition paths that do not match <column>=<value>. When the option is enabled, table with two partition directories 'table/invalid' and 'table/col=1' will only load the latter directory and ignore the invalid partition\n4.0.0\nspark.sql.files.ignoreMissingFiles\nfalse\nWhether to ignore missing files. If true, the Spark jobs will continue to run when encountering missing files and the contents that have been read will still be returned. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.\n2.3.0\nspark.sql.files.maxPartitionBytes\n128MB\nThe maximum number of bytes to pack into a single partition when reading files. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.\n2.0.0\nspark.sql.files.maxPartitionNum\n(none)\nThe suggested (not guaranteed) maximum number of split file partitions. If it is set, Spark will rescale each partition to make the number of partitions is close to this value if the initial number of partitions exceeds this value. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.\n3.5.0\nspark.sql.files.maxRecordsPerFile\n0\nMaximum number of records to write out to a single file. If this value is zero or negative, there is no limit.\n2.2.0\nspark.sql.files.minPartitionNum\n(none)\nThe suggested (not guaranteed) minimum number of split file partitions. If not set, the default value is\nspark.sql.leafNodeDefaultParallelism\n. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.\n3.1.0\nspark.sql.function.concatBinaryAsString\nfalse\nWhen this option is set to false and all inputs are binary,\nfunctions.concat\nreturns an output as binary. Otherwise, it returns as a string.\n2.3.0\nspark.sql.function.eltOutputAsString\nfalse\nWhen this option is set to false and all inputs are binary,\nelt\nreturns an output as binary. Otherwise, it returns as a string.\n2.3.0\nspark.sql.groupByAliases\ntrue\nWhen true, aliases in a select list can be used in group by clauses. When false, an analysis exception is thrown in the case.\n2.2.0\nspark.sql.groupByOrdinal\ntrue\nWhen true, the ordinal numbers in group by clauses are treated as the position in the select list. When false, the ordinal numbers are ignored.\n2.0.0\nspark.sql.hive.convertInsertingPartitionedTable\ntrue\nWhen set to true, and\nspark.sql.hive.convertMetastoreParquet\nor\nspark.sql.hive.convertMetastoreOrc\nis true, the built-in ORC/Parquet writer is usedto process inserting into partitioned ORC/Parquet tables created by using the HiveSQL syntax.\n3.0.0\nspark.sql.hive.convertInsertingUnpartitionedTable\ntrue\nWhen set to true, and\nspark.sql.hive.convertMetastoreParquet\nor\nspark.sql.hive.convertMetastoreOrc\nis true, the built-in ORC/Parquet writer is usedto process inserting into unpartitioned ORC/Parquet tables created by using the HiveSQL syntax.\n4.0.0\nspark.sql.hive.convertMetastoreCtas\ntrue\nWhen set to true,  Spark will try to use built-in data source writer instead of Hive serde in CTAS. This flag is effective only if\nspark.sql.hive.convertMetastoreParquet\nor\nspark.sql.hive.convertMetastoreOrc\nis enabled respectively for Parquet and ORC formats\n3.0.0\nspark.sql.hive.convertMetastoreInsertDir\ntrue\nWhen set to true,  Spark will try to use built-in data source writer instead of Hive serde in INSERT OVERWRITE DIRECTORY. This flag is effective only if\nspark.sql.hive.convertMetastoreParquet\nor\nspark.sql.hive.convertMetastoreOrc\nis enabled respectively for Parquet and ORC formats\n3.3.0\nspark.sql.hive.convertMetastoreOrc\ntrue\nWhen set to true, the built-in ORC reader and writer are used to process ORC tables created by using the HiveQL syntax, instead of Hive serde.\n2.0.0\nspark.sql.hive.convertMetastoreParquet\ntrue\nWhen set to true, the built-in Parquet reader and writer are used to process parquet tables created by using the HiveQL syntax, instead of Hive serde.\n1.1.1\nspark.sql.hive.convertMetastoreParquet.mergeSchema\nfalse\nWhen true, also tries to merge possibly different but compatible Parquet schemas in different Parquet data files. This configuration is only effective when \"spark.sql.hive.convertMetastoreParquet\" is true.\n1.3.1\nspark.sql.hive.dropPartitionByName.enabled\nfalse\nWhen true, Spark will get partition name rather than partition object to drop partition, which can improve the performance of drop partition.\n3.4.0\nspark.sql.hive.filesourcePartitionFileCacheSize\n262144000\nWhen nonzero, enable caching of partition file metadata in memory. All tables share a cache that can use up to specified num bytes for file metadata. This conf only has an effect when hive filesource partition management is enabled.\n2.1.1\nspark.sql.hive.manageFilesourcePartitions\ntrue\nWhen true, enable metastore partition management for file source tables as well. This includes both datasource and converted Hive tables. When partition management is enabled, datasource tables store partition in the Hive metastore, and use the metastore to prune partitions during query planning when spark.sql.hive.metastorePartitionPruning is set to true.\n2.1.1\nspark.sql.hive.metastorePartitionPruning\ntrue\nWhen true, some predicates will be pushed down into the Hive metastore so that unmatching partitions can be eliminated earlier.\n1.5.0\nspark.sql.hive.metastorePartitionPruningFallbackOnException\nfalse\nWhether to fallback to get all partitions from Hive metastore and perform partition pruning on Spark client side, when encountering MetaException from the metastore. Note that Spark query performance may degrade if this is enabled and there are many partitions to be listed. If this is disabled, Spark will fail the query instead.\n3.3.0\nspark.sql.hive.metastorePartitionPruningFastFallback\nfalse\nWhen this config is enabled, if the predicates are not supported by Hive or Spark does fallback due to encountering MetaException from the metastore, Spark will instead prune partitions by getting the partition names first and then evaluating the filter expressions on the client side. Note that the predicates with TimeZoneAwareExpression is not supported.\n3.3.0\nspark.sql.hive.thriftServer.async\ntrue\nWhen set to true, Hive Thrift server executes SQL queries in an asynchronous way.\n1.5.0\nspark.sql.icu.caseMappings.enabled\ntrue\nWhen enabled we use the ICU library (instead of the JVM) to implement case mappings for strings under UTF8_BINARY collation.\n4.0.0\nspark.sql.inMemoryColumnarStorage.batchSize\n10000\nControls the size of batches for columnar caching.  Larger batch sizes can improve memory utilization and compression, but risk OOMs when caching data.\n1.1.1\nspark.sql.inMemoryColumnarStorage.compressed\ntrue\nWhen set to true Spark SQL will automatically select a compression codec for each column based on statistics of the data.\n1.0.1\nspark.sql.inMemoryColumnarStorage.enableVectorizedReader\ntrue\nEnables vectorized reader for columnar caching.\n2.3.1\nspark.sql.inMemoryColumnarStorage.hugeVectorReserveRatio\n1.2\nWhen spark.sql.inMemoryColumnarStorage.hugeVectorThreshold <= 0 or the required memory is smaller than spark.sql.inMemoryColumnarStorage.hugeVectorThreshold, spark reserves required memory * 2 memory; otherwise, spark reserves required memory * this ratio memory, and will release this column vector memory before reading the next batch rows.\n4.0.0\nspark.sql.inMemoryColumnarStorage.hugeVectorThreshold\n-1b\nWhen the required memory is larger than this, spark reserves required memory * spark.sql.inMemoryColumnarStorage.hugeVectorReserveRatio memory next time and release this column vector memory before reading the next batch rows. -1 means disabling the optimization.\n4.0.0\nspark.sql.json.filterPushdown.enabled\ntrue\nWhen true, enable filter pushdown to JSON datasource.\n3.1.0\nspark.sql.json.useUnsafeRow\nfalse\nWhen set to true, use UnsafeRow to represent struct result in the JSON parser. It can be overwritten by the JSON option\nuseUnsafeRow\n.\n4.0.0\nspark.sql.jsonGenerator.ignoreNullFields\ntrue\nWhether to ignore null fields when generating JSON objects in JSON data source and JSON functions such as to_json. If false, it generates null for null fields in JSON objects.\n3.0.0\nspark.sql.leafNodeDefaultParallelism\n(none)\nThe default parallelism of Spark SQL leaf nodes that produce data, such as the file scan node, the local data scan node, the range node, etc. The default value of this config is 'SparkContext#defaultParallelism'.\n3.2.0\nspark.sql.mapKeyDedupPolicy\nEXCEPTION\nThe policy to deduplicate map keys in builtin function: CreateMap, MapFromArrays, MapFromEntries, StringToMap, MapConcat and TransformKeys. When EXCEPTION, the query fails if duplicated map keys are detected. When LAST_WIN, the map key that is inserted at last takes precedence.\n3.0.0\nspark.sql.maven.additionalRemoteRepositories\nhttps://maven-central.storage-download.googleapis.com/maven2/\nA comma-delimited string config of the optional additional remote Maven mirror repositories. This is only used for downloading Hive jars in IsolatedClientLoader if the default Maven Central repo is unreachable.\n3.0.0\nspark.sql.maxMetadataStringLength\n100\nMaximum number of characters to output for a metadata string. e.g. file location in\nDataSourceScanExec\n, every value will be abbreviated if exceed length.\n3.1.0\nspark.sql.maxPlanStringLength\n2147483632\nMaximum number of characters to output for a plan string.  If the plan is longer, further output will be truncated.  The default setting always generates a full plan.  Set this to a lower value such as 8k if plan strings are taking up too much memory or are causing OutOfMemory errors in the driver or UI processes.\n3.0.0\nspark.sql.maxSinglePartitionBytes\n128m\nThe maximum number of bytes allowed for a single partition. Otherwise, The planner will introduce shuffle to improve parallelism.\n3.4.0\nspark.sql.operatorPipeSyntaxEnabled\ntrue\nIf true, enable operator pipe syntax for Apache Spark SQL. This uses the operator pipe marker |> to indicate separation between clauses of SQL in a manner that describes the sequence of steps that the query performs in a composable fashion.\n4.0.0\nspark.sql.optimizer.avoidCollapseUDFWithExpensiveExpr\ntrue\nWhether to avoid collapsing projections that would duplicate expensive expressions in UDFs.\n4.0.0\nspark.sql.optimizer.collapseProjectAlwaysInline\nfalse\nWhether to always collapse two adjacent projections and inline expressions even if it causes extra duplication.\n3.3.0\nspark.sql.optimizer.dynamicPartitionPruning.enabled\ntrue\nWhen true, we will generate predicate for partition column when it's used as join key\n3.0.0\nspark.sql.optimizer.enableCsvExpressionOptimization\ntrue\nWhether to optimize CSV expressions in SQL optimizer. It includes pruning unnecessary columns from from_csv.\n3.2.0\nspark.sql.optimizer.enableJsonExpressionOptimization\ntrue\nWhether to optimize JSON expressions in SQL optimizer. It includes pruning unnecessary columns from from_json, simplifying from_json + to_json, to_json + named_struct(from_json.col1, from_json.col2, ....).\n3.1.0\nspark.sql.optimizer.excludedRules\n(none)\nConfigures a list of rules to be disabled in the optimizer, in which the rules are specified by their rule names and separated by comma. It is not guaranteed that all the rules in this configuration will eventually be excluded, as some rules are necessary for correctness. The optimizer will log the rules that have indeed been excluded.\n2.4.0\nspark.sql.optimizer.runtime.bloomFilter.applicationSideScanSizeThreshold\n10GB\nByte size threshold of the Bloom filter application side plan's aggregated scan size. Aggregated scan byte size of the Bloom filter application side needs to be over this value to inject a bloom filter.\n3.3.0\nspark.sql.optimizer.runtime.bloomFilter.creationSideThreshold\n10MB\nSize threshold of the bloom filter creation side plan. Estimated size needs to be under this value to try to inject bloom filter.\n3.3.0\nspark.sql.optimizer.runtime.bloomFilter.enabled\ntrue\nWhen true and if one side of a shuffle join has a selective predicate, we attempt to insert a bloom filter in the other side to reduce the amount of shuffle data.\n3.3.0\nspark.sql.optimizer.runtime.bloomFilter.expectedNumItems\n1000000\nThe default number of expected items for the runtime bloomfilter\n3.3.0\nspark.sql.optimizer.runtime.bloomFilter.maxNumBits\n67108864\nThe max number of bits to use for the runtime bloom filter\n3.3.0\nspark.sql.optimizer.runtime.bloomFilter.maxNumItems\n4000000\nThe max allowed number of expected items for the runtime bloom filter\n3.3.0\nspark.sql.optimizer.runtime.bloomFilter.numBits\n8388608\nThe default number of bits to use for the runtime bloom filter\n3.3.0\nspark.sql.optimizer.runtime.rowLevelOperationGroupFilter.enabled\ntrue\nEnables runtime group filtering for group-based row-level operations. Data sources that replace groups of data (e.g. files, partitions) may prune entire groups using provided data source filters when planning a row-level operation scan. However, such filtering is limited as not all expressions can be converted into data source filters and some expressions can only be evaluated by Spark (e.g. subqueries). Since rewriting groups is expensive, Spark can execute a query at runtime to find what records match the condition of the row-level operation. The information about matching records will be passed back to the row-level operation scan, allowing data sources to discard groups that don't have to be rewritten.\n3.4.0\nspark.sql.optimizer.runtimeFilter.number.threshold\n10\nThe total number of injected runtime filters (non-DPP) for a single query. This is to prevent driver OOMs with too many Bloom filters.\n3.3.0\nspark.sql.orc.aggregatePushdown\nfalse\nIf true, aggregates will be pushed down to ORC for optimization. Support MIN, MAX and COUNT as aggregate expression. For MIN/MAX, support boolean, integer, float and date type. For COUNT, support all data types. If statistics is missing from any ORC file footer, exception would be thrown.\n3.3.0\nspark.sql.orc.columnarReaderBatchSize\n4096\nThe number of rows to include in a orc vectorized reader batch. The number should be carefully chosen to minimize overhead and avoid OOMs in reading data.\n2.4.0\nspark.sql.orc.columnarWriterBatchSize\n1024\nThe number of rows to include in a orc vectorized writer batch. The number should be carefully chosen to minimize overhead and avoid OOMs in writing data.\n3.4.0\nspark.sql.orc.compression.codec\nzstd\nSets the compression codec used when writing ORC files. If either\ncompression\nor\norc.compress\nis specified in the table-specific options/properties, the precedence would be\ncompression\n,\norc.compress\n,\nspark.sql.orc.compression.codec\n. Acceptable values include: none, uncompressed, snappy, zlib, lzo, zstd, lz4, brotli.\n2.3.0\nspark.sql.orc.enableNestedColumnVectorizedReader\ntrue\nEnables vectorized orc decoding for nested column.\n3.2.0\nspark.sql.orc.enableVectorizedReader\ntrue\nEnables vectorized orc decoding.\n2.3.0\nspark.sql.orc.filterPushdown\ntrue\nWhen true, enable filter pushdown for ORC files.\n1.4.0\nspark.sql.orc.mergeSchema\nfalse\nWhen true, the Orc data source merges schemas collected from all data files, otherwise the schema is picked from a random data file.\n3.0.0\nspark.sql.orderByOrdinal\ntrue\nWhen true, the ordinal numbers are treated as the position in the select list. When false, the ordinal numbers in order/sort by clause are ignored.\n2.0.0\nspark.sql.parquet.aggregatePushdown\nfalse\nIf true, aggregates will be pushed down to Parquet for optimization. Support MIN, MAX and COUNT as aggregate expression. For MIN/MAX, support boolean, integer, float and date type. For COUNT, support all data types. If statistics is missing from any Parquet file footer, exception would be thrown.\n3.3.0\nspark.sql.parquet.binaryAsString\nfalse\nSome other Parquet-producing systems, in particular Impala and older versions of Spark SQL, do not differentiate between binary data and strings when writing out the Parquet schema. This flag tells Spark SQL to interpret binary data as a string to provide compatibility with these systems.\n1.1.1\nspark.sql.parquet.columnarReaderBatchSize\n4096\nThe number of rows to include in a parquet vectorized reader batch. The number should be carefully chosen to minimize overhead and avoid OOMs in reading data.\n2.4.0\nspark.sql.parquet.compression.codec\nsnappy\nSets the compression codec used when writing Parquet files. If either\ncompression\nor\nparquet.compression\nis specified in the table-specific options/properties, the precedence would be\ncompression\n,\nparquet.compression\n,\nspark.sql.parquet.compression.codec\n. Acceptable values include: none, uncompressed, snappy, gzip, lzo, brotli, lz4, lz4_raw, zstd.\n1.1.1\nspark.sql.parquet.enableNestedColumnVectorizedReader\ntrue\nEnables vectorized Parquet decoding for nested columns (e.g., struct, list, map). Requires spark.sql.parquet.enableVectorizedReader to be enabled.\n3.3.0\nspark.sql.parquet.enableVectorizedReader\ntrue\nEnables vectorized parquet decoding.\n2.0.0\nspark.sql.parquet.fieldId.read.enabled\nfalse\nField ID is a native field of the Parquet schema spec. When enabled, Parquet readers will use field IDs (if present) in the requested Spark schema to look up Parquet fields instead of using column names\n3.3.0\nspark.sql.parquet.fieldId.read.ignoreMissing\nfalse\nWhen the Parquet file doesn't have any field IDs but the Spark read schema is using field IDs to read, we will silently return nulls when this flag is enabled, or error otherwise.\n3.3.0\nspark.sql.parquet.fieldId.write.enabled\ntrue\nField ID is a native field of the Parquet schema spec. When enabled, Parquet writers will populate the field Id metadata (if present) in the Spark schema to the Parquet schema.\n3.3.0\nspark.sql.parquet.filterPushdown\ntrue\nEnables Parquet filter push-down optimization when set to true.\n1.2.0\nspark.sql.parquet.inferTimestampNTZ.enabled\ntrue\nWhen enabled, Parquet timestamp columns with annotation isAdjustedToUTC = false are inferred as TIMESTAMP_NTZ type during schema inference. Otherwise, all the Parquet timestamp columns are inferred as TIMESTAMP_LTZ types. Note that Spark writes the output schema into Parquet's footer metadata on file writing and leverages it on file reading. Thus this configuration only affects the schema inference on Parquet files which are not written by Spark.\n3.4.0\nspark.sql.parquet.int96AsTimestamp\ntrue\nSome Parquet-producing systems, in particular Impala, store Timestamp into INT96. Spark would also store Timestamp as INT96 because we need to avoid precision lost of the nanoseconds field. This flag tells Spark SQL to interpret INT96 data as a timestamp to provide compatibility with these systems.\n1.3.0\nspark.sql.parquet.int96TimestampConversion\nfalse\nThis controls whether timestamp adjustments should be applied to INT96 data when converting to timestamps, for data written by Impala.  This is necessary because Impala stores INT96 data with a different timezone offset than Hive & Spark.\n2.3.0\nspark.sql.parquet.mergeSchema\nfalse\nWhen true, the Parquet data source merges schemas collected from all data files, otherwise the schema is picked from the summary file or a random data file if no summary file is available.\n1.5.0\nspark.sql.parquet.outputTimestampType\nINT96\nSets which Parquet timestamp type to use when Spark writes data to Parquet files. INT96 is a non-standard but commonly used timestamp type in Parquet. TIMESTAMP_MICROS is a standard timestamp type in Parquet, which stores number of microseconds from the Unix epoch. TIMESTAMP_MILLIS is also standard, but with millisecond precision, which means Spark has to truncate the microsecond portion of its timestamp value.\n2.3.0\nspark.sql.parquet.recordLevelFilter.enabled\nfalse\nIf true, enables Parquet's native record-level filtering using the pushed down filters. This configuration only has an effect when 'spark.sql.parquet.filterPushdown' is enabled and the vectorized reader is not used. You can ensure the vectorized reader is not used by setting 'spark.sql.parquet.enableVectorizedReader' to false.\n2.3.0\nspark.sql.parquet.respectSummaryFiles\nfalse\nWhen true, we make assumption that all part-files of Parquet are consistent with summary files and we will ignore them when merging schema. Otherwise, if this is false, which is the default, we will merge all part-files. This should be considered as expert-only option, and shouldn't be enabled before knowing what it means exactly.\n1.5.0\nspark.sql.parquet.writeLegacyFormat\nfalse\nIf true, data will be written in a way of Spark 1.4 and earlier. For example, decimal values will be written in Apache Parquet's fixed-length byte array format, which other systems such as Apache Hive and Apache Impala use. If false, the newer format in Parquet will be used. For example, decimals will be written in int-based format. If Parquet output is intended for use with systems that do not support this newer format, set to true.\n1.6.0\nspark.sql.parser.quotedRegexColumnNames\nfalse\nWhen true, quoted Identifiers (using backticks) in SELECT statement are interpreted as regular expressions.\n2.3.0\nspark.sql.pivotMaxValues\n10000\nWhen doing a pivot without specifying values for the pivot column this is the maximum number of (distinct) values that will be collected without error.\n1.6.0\nspark.sql.planner.pythonExecution.memory\n(none)\nSpecifies the memory allocation for executing Python code in Spark driver, in MiB. When set, it caps the memory for Python execution to the specified amount. If not set, Spark will not limit Python's memory usage and it is up to the application to avoid exceeding the overhead memory space shared with other non-JVM processes.\nNote: Windows does not support resource limiting and actual resource is not limited on MacOS.\n4.0.0\nspark.sql.preserveCharVarcharTypeInfo\nfalse\nWhen true, Spark does not replace CHAR/VARCHAR types the STRING type, which is the default behavior of Spark 3.0 and earlier versions. This means the length checks for CHAR/VARCHAR types is enforced and CHAR type is also properly padded.\n4.0.0\nspark.sql.pyspark.inferNestedDictAsStruct.enabled\nfalse\nPySpark's SparkSession.createDataFrame infers the nested dict as a map by default. When it set to true, it infers the nested dict as a struct.\n3.3.0\nspark.sql.pyspark.jvmStacktrace.enabled\nfalse\nWhen true, it shows the JVM stacktrace in the user-facing PySpark exception together with Python stacktrace. By default, it is disabled to hide JVM stacktrace and shows a Python-friendly exception only. Note that this is independent from log level settings.\n3.0.0\nspark.sql.pyspark.plotting.max_rows\n1000\nThe visual limit on plots. If set to 1000 for top-n-based plots (pie, bar, barh), the first 1000 data points will be used for plotting. For sampled-based plots (scatter, area, line), 1000 data points will be randomly sampled.\n4.0.0\nspark.sql.pyspark.udf.profiler\n(none)\nConfigure the Python/Pandas UDF profiler by enabling or disabling it with the option to choose between \"perf\" and \"memory\" types, or unsetting the config disables the profiler. This is disabled by default.\n4.0.0\nspark.sql.readSideCharPadding\ntrue\nWhen true, Spark applies string padding when reading CHAR type columns/fields, in addition to the write-side padding. This config is true by default to better enforce CHAR type semantic in cases such as external tables.\n3.4.0\nspark.sql.redaction.options.regex\n(?i)url\nRegex to decide which keys in a Spark SQL command's options map contain sensitive information. The values of options whose names that match this regex will be redacted in the explain output. This redaction is applied on top of the global redaction configuration defined by spark.redaction.regex.\n2.2.2\nspark.sql.redaction.string.regex\n(value of\nspark.redaction.string.regex\n)\nRegex to decide which parts of strings produced by Spark contain sensitive information. When this regex matches a string part, that string part is replaced by a dummy value. This is currently used to redact the output of SQL explain commands. When this conf is not set, the value from\nspark.redaction.string.regex\nis used.\n2.3.0\nspark.sql.repl.eagerEval.enabled\nfalse\nEnables eager evaluation or not. When true, the top K rows of Dataset will be displayed if and only if the REPL supports the eager evaluation. Currently, the eager evaluation is supported in PySpark and SparkR. In PySpark, for the notebooks like Jupyter, the HTML table (generated by\nrepr_html\n) will be returned. For plain Python REPL, the returned outputs are formatted like dataframe.show(). In SparkR, the returned outputs are showed similar to R data.frame would.\n2.4.0\nspark.sql.repl.eagerEval.maxNumRows\n20\nThe max number of rows that are returned by eager evaluation. This only takes effect when spark.sql.repl.eagerEval.enabled is set to true. The valid range of this config is from 0 to (Int.MaxValue - 1), so the invalid config like negative and greater than (Int.MaxValue - 1) will be normalized to 0 and (Int.MaxValue - 1).\n2.4.0\nspark.sql.repl.eagerEval.truncate\n20\nThe max number of characters for each cell that is returned by eager evaluation. This only takes effect when spark.sql.repl.eagerEval.enabled is set to true.\n2.4.0\nspark.sql.scripting.enabled\nfalse\nSQL Scripting feature is under development and its use should be done under this feature flag. SQL Scripting enables users to write procedural SQL including control flow and error handling.\n4.0.0\nspark.sql.session.localRelationCacheThreshold\n67108864\nThe threshold for the size in bytes of local relations to be cached at the driver side after serialization.\n3.5.0\nspark.sql.session.timeZone\n(value of local timezone)\nThe ID of session local timezone in the format of either region-based zone IDs or zone offsets. Region IDs must have the form 'area/city', such as 'America/Los_Angeles'. Zone offsets must be in the format '(+|-)HH', '(+|-)HH:mm' or '(+|-)HH:mm:ss', e.g '-08', '+01:00' or '-13:33:33'. Also 'UTC' and 'Z' are supported as aliases of '+00:00'. Other short names are not recommended to use because they can be ambiguous.\n2.2.0\nspark.sql.shuffle.partitions\n200\nThe default number of partitions to use when shuffling data for joins or aggregations. Note: For structured streaming, this configuration cannot be changed between query restarts from the same checkpoint location.\n1.1.0\nspark.sql.shuffleDependency.fileCleanup.enabled\nfalse\nWhen enabled, shuffle files will be cleaned up at the end of Spark Connect SQL executions.\n4.0.0\nspark.sql.shuffleDependency.skipMigration.enabled\nfalse\nWhen enabled, shuffle dependencies for a Spark Connect SQL execution are marked at the end of the execution, and they will not be migrated during decommissions.\n4.0.0\nspark.sql.shuffledHashJoinFactor\n3\nThe shuffle hash join can be selected if the data size of small side multiplied by this factor is still smaller than the large side.\n3.3.0\nspark.sql.sources.bucketing.autoBucketedScan.enabled\ntrue\nWhen true, decide whether to do bucketed scan on input tables based on query plan automatically. Do not use bucketed scan if 1. query does not have operators to utilize bucketing (e.g. join, group-by, etc), or 2. there's an exchange operator between these operators and table scan. Note when 'spark.sql.sources.bucketing.enabled' is set to false, this configuration does not take any effect.\n3.1.0\nspark.sql.sources.bucketing.enabled\ntrue\nWhen false, we will treat bucketed table as normal table\n2.0.0\nspark.sql.sources.bucketing.maxBuckets\n100000\nThe maximum number of buckets allowed.\n2.4.0\nspark.sql.sources.default\nparquet\nThe default data source to use in input/output.\n1.3.0\nspark.sql.sources.parallelPartitionDiscovery.threshold\n32\nThe maximum number of paths allowed for listing files at driver side. If the number of detected paths exceeds this value during partition discovery, it tries to list the files with another Spark distributed job. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.\n1.5.0\nspark.sql.sources.partitionColumnTypeInference.enabled\ntrue\nWhen true, automatically infer the data types for partitioned columns.\n1.5.0\nspark.sql.sources.partitionOverwriteMode\nSTATIC\nWhen INSERT OVERWRITE a partitioned data source table, we currently support 2 modes: static and dynamic. In static mode, Spark deletes all the partitions that match the partition specification(e.g. PARTITION(a=1,b)) in the INSERT statement, before overwriting. In dynamic mode, Spark doesn't delete partitions ahead, and only overwrite those partitions that have data written into it at runtime. By default we use static mode to keep the same behavior of Spark prior to 2.3. Note that this config doesn't affect Hive serde tables, as they are always overwritten with dynamic mode. This can also be set as an output option for a data source using key partitionOverwriteMode (which takes precedence over this setting), e.g. dataframe.write.option(\"partitionOverwriteMode\", \"dynamic\").save(path).\n2.3.0\nspark.sql.sources.v2.bucketing.allowCompatibleTransforms.enabled\nfalse\nWhether to allow storage-partition join in the case where the partition transforms are compatible but not identical.  This config requires both spark.sql.sources.v2.bucketing.enabled and spark.sql.sources.v2.bucketing.pushPartValues.enabled to be enabled and spark.sql.sources.v2.bucketing.partiallyClusteredDistribution.enabled to be disabled.\n4.0.0\nspark.sql.sources.v2.bucketing.allowJoinKeysSubsetOfPartitionKeys.enabled\nfalse\nWhether to allow storage-partition join in the case where join keys are a subset of the partition keys of the source tables. At planning time, Spark will group the partitions by only those keys that are in the join keys. This is currently enabled only if spark.sql.requireAllClusterKeysForDistribution is false.\n4.0.0\nspark.sql.sources.v2.bucketing.enabled\nfalse\nSimilar to spark.sql.sources.bucketing.enabled, this config is used to enable bucketing for V2 data sources. When turned on, Spark will recognize the specific distribution reported by a V2 data source through SupportsReportPartitioning, and will try to avoid shuffle if necessary.\n3.3.0\nspark.sql.sources.v2.bucketing.partiallyClusteredDistribution.enabled\nfalse\nDuring a storage-partitioned join, whether to allow input partitions to be partially clustered, when both sides of the join are of KeyGroupedPartitioning. At planning time, Spark will pick the side with less data size based on table statistics, group and replicate them to match the other side. This is an optimization on skew join and can help to reduce data skewness when certain partitions are assigned large amount of data. This config requires both spark.sql.sources.v2.bucketing.enabled and spark.sql.sources.v2.bucketing.pushPartValues.enabled to be enabled\n3.4.0\nspark.sql.sources.v2.bucketing.partition.filter.enabled\nfalse\nWhether to filter partitions when running storage-partition join. When enabled, partitions without matches on the other side can be omitted for scanning, if allowed by the join type. This config requires both spark.sql.sources.v2.bucketing.enabled and spark.sql.sources.v2.bucketing.pushPartValues.enabled to be enabled.\n4.0.0\nspark.sql.sources.v2.bucketing.pushPartValues.enabled\ntrue\nWhether to pushdown common partition values when spark.sql.sources.v2.bucketing.enabled is enabled. When turned on, if both sides of a join are of KeyGroupedPartitioning and if they share compatible partition keys, even if they don't have the exact same partition values, Spark will calculate a superset of partition values and pushdown that info to scan nodes, which will use empty partitions for the missing partition values on either side. This could help to eliminate unnecessary shuffles\n3.4.0\nspark.sql.sources.v2.bucketing.shuffle.enabled\nfalse\nDuring a storage-partitioned join, whether to allow to shuffle only one side. When only one side is KeyGroupedPartitioning, if the conditions are met, spark will only shuffle the other side. This optimization will reduce the amount of data that needs to be shuffle. This config requires spark.sql.sources.v2.bucketing.enabled to be enabled\n4.0.0\nspark.sql.sources.v2.bucketing.sorting.enabled\nfalse\nWhen turned on, Spark will recognize the specific distribution reported by a V2 data source through SupportsReportPartitioning, and will try to avoid a shuffle if possible when sorting by those columns. This config requires spark.sql.sources.v2.bucketing.enabled to be enabled.\n4.0.0\nspark.sql.stackTracesInDataFrameContext\n1\nThe number of non-Spark stack traces in the captured DataFrame query context.\n4.0.0\nspark.sql.statistics.fallBackToHdfs\nfalse\nWhen true, it will fall back to HDFS if the table statistics are not available from table metadata. This is useful in determining if a table is small enough to use broadcast joins. This flag is effective only for non-partitioned Hive tables. For non-partitioned data source tables, it will be automatically recalculated if table statistics are not available. For partitioned data source and partitioned Hive tables, It is 'spark.sql.defaultSizeInBytes' if table statistics are not available.\n2.0.0\nspark.sql.statistics.histogram.enabled\nfalse\nGenerates histograms when computing column statistics if enabled. Histograms can provide better estimation accuracy. Currently, Spark only supports equi-height histogram. Note that collecting histograms takes extra cost. For example, collecting column statistics usually takes only one table scan, but generating equi-height histogram will cause an extra table scan.\n2.3.0\nspark.sql.statistics.size.autoUpdate.enabled\nfalse\nEnables automatic update for table size once table's data is changed. Note that if the total number of files of the table is very large, this can be expensive and slow down data change commands.\n2.3.0\nspark.sql.statistics.updatePartitionStatsInAnalyzeTable.enabled\nfalse\nWhen this config is enabled, Spark will also update partition statistics in analyze table command (i.e., ANALYZE TABLE .. COMPUTE STATISTICS [NOSCAN]). Note the command will also become more expensive. When this config is disabled, Spark will only update table level statistics.\n4.0.0\nspark.sql.storeAssignmentPolicy\nANSI\nWhen inserting a value into a column with different data type, Spark will perform type coercion. Currently, we support 3 policies for the type coercion rules: ANSI, legacy and strict. With ANSI policy, Spark performs the type coercion as per ANSI SQL. In practice, the behavior is mostly the same as PostgreSQL. It disallows certain unreasonable type conversions such as converting\nstring\nto\nint\nor\ndouble\nto\nboolean\n. With legacy policy, Spark allows the type coercion as long as it is a valid\nCast\n, which is very loose. e.g. converting\nstring\nto\nint\nor\ndouble\nto\nboolean\nis allowed. It is also the only behavior in Spark 2.x and it is compatible with Hive. With strict policy, Spark doesn't allow any possible precision loss or data truncation in type coercion, e.g. converting\ndouble\nto\nint\nor\ndecimal\nto\ndouble\nis not allowed.\n3.0.0\nspark.sql.streaming.checkpointLocation\n(none)\nThe default location for storing checkpoint data for streaming queries.\n2.0.0\nspark.sql.streaming.continuous.epochBacklogQueueSize\n10000\nThe max number of entries to be stored in queue to wait for late epochs. If this parameter is exceeded by the size of the queue, stream will stop with an error.\n3.0.0\nspark.sql.streaming.disabledV2Writers\nA comma-separated list of fully qualified data source register class names for which StreamWriteSupport is disabled. Writes to these sources will fall back to the V1 Sinks.\n2.3.1\nspark.sql.streaming.fileSource.cleaner.numThreads\n1\nNumber of threads used in the file source completed file cleaner.\n3.0.0\nspark.sql.streaming.forceDeleteTempCheckpointLocation\nfalse\nWhen true, enable temporary checkpoint locations force delete.\n3.0.0\nspark.sql.streaming.metricsEnabled\nfalse\nWhether Dropwizard/Codahale metrics will be reported for active streaming queries.\n2.0.2\nspark.sql.streaming.multipleWatermarkPolicy\nmin\nPolicy to calculate the global watermark value when there are multiple watermark operators in a streaming query. The default value is 'min' which chooses the minimum watermark reported across multiple operators. Other alternative value is 'max' which chooses the maximum across multiple operators. Note: This configuration cannot be changed between query restarts from the same checkpoint location.\n2.4.0\nspark.sql.streaming.noDataMicroBatches.enabled\ntrue\nWhether streaming micro-batch engine will execute batches without data for eager state management for stateful streaming queries.\n2.4.1\nspark.sql.streaming.numRecentProgressUpdates\n100\nThe number of progress updates to retain for a streaming query\n2.1.1\nspark.sql.streaming.sessionWindow.merge.sessions.in.local.partition\nfalse\nWhen true, streaming session window sorts and merge sessions in local partition prior to shuffle. This is to reduce the rows to shuffle, but only beneficial when there're lots of rows in a batch being assigned to same sessions.\n3.2.0\nspark.sql.streaming.stateStore.encodingFormat\nunsaferow\nThe encoding format used for stateful operators to store information in the state store\n4.0.0\nspark.sql.streaming.stateStore.stateSchemaCheck\ntrue\nWhen true, Spark will validate the state schema against schema on existing state and fail query if it's incompatible.\n3.1.0\nspark.sql.streaming.stopActiveRunOnRestart\ntrue\nRunning multiple runs of the same streaming query concurrently is not supported. If we find a concurrent active run for a streaming query (in the same or different SparkSessions on the same cluster) and this flag is true, we will stop the old streaming query run to start the new one.\n3.0.0\nspark.sql.streaming.stopTimeout\n0\nHow long to wait in milliseconds for the streaming execution thread to stop when calling the streaming query's stop() method. 0 or negative values wait indefinitely.\n3.0.0\nspark.sql.streaming.transformWithState.stateSchemaVersion\n3\nThe version of the state schema used by the transformWithState operator\n4.0.0\nspark.sql.thriftServer.interruptOnCancel\n(value of\nspark.sql.execution.interruptOnCancel\n)\nWhen true, all running tasks will be interrupted if one cancels a query. When false, all running tasks will remain until finished.\n3.2.0\nspark.sql.thriftServer.queryTimeout\n0ms\nSet a query duration timeout in seconds in Thrift Server. If the timeout is set to a positive value, a running query will be cancelled automatically when the timeout is exceeded, otherwise the query continues to run till completion. If timeout values are set for each statement via\njava.sql.Statement.setQueryTimeout\nand they are smaller than this configuration value, they take precedence. If you set this timeout and prefer to cancel the queries right away without waiting task to finish, consider enabling spark.sql.thriftServer.interruptOnCancel together.\n3.1.0\nspark.sql.thriftserver.scheduler.pool\n(none)\nSet a Fair Scheduler pool for a JDBC client session.\n1.1.1\nspark.sql.thriftserver.ui.retainedSessions\n200\nThe number of SQL client sessions kept in the JDBC/ODBC web UI history.\n1.4.0\nspark.sql.thriftserver.ui.retainedStatements\n200\nThe number of SQL statements kept in the JDBC/ODBC web UI history.\n1.4.0\nspark.sql.timeTravelTimestampKey\ntimestampAsOf\nThe option name to specify the time travel timestamp when reading a table.\n4.0.0\nspark.sql.timeTravelVersionKey\nversionAsOf\nThe option name to specify the time travel table version when reading a table.\n4.0.0\nspark.sql.timestampType\nTIMESTAMP_LTZ\nConfigures the default timestamp type of Spark SQL, including SQL DDL, Cast clause, type literal and the schema inference of data sources. Setting the configuration as TIMESTAMP_NTZ will use TIMESTAMP WITHOUT TIME ZONE as the default type while putting it as TIMESTAMP_LTZ will use TIMESTAMP WITH LOCAL TIME ZONE. Before the 3.4.0 release, Spark only supports the TIMESTAMP WITH LOCAL TIME ZONE type.\n3.4.0\nspark.sql.transposeMaxValues\n500\nWhen doing a transpose without specifying values for the index column this is the maximum number of values that will be transposed without error.\n4.0.0\nspark.sql.tvf.allowMultipleTableArguments.enabled\nfalse\nWhen true, allows multiple table arguments for table-valued functions, receiving the cartesian product of all the rows of these tables.\n3.5.0\nspark.sql.ui.explainMode\nformatted\nConfigures the query explain mode used in the Spark SQL UI. The value can be 'simple', 'extended', 'codegen', 'cost', or 'formatted'. The default value is 'formatted'.\n3.1.0\nspark.sql.variable.substitute\ntrue\nThis enables substitution using syntax like\n${var}\n,\n${system:var}\n, and\n${env:var}\n.\n2.0.0\nStatic SQL Configuration\nStatic SQL configurations are cross-session, immutable Spark SQL configurations. They can be set with final values by the config file\nand command-line options with\n--conf/-c\nprefixed, or by setting\nSparkConf\nthat are used to create\nSparkSession\n.\nExternal users can query the static sql config values via\nSparkSession.conf\nor via set command, e.g.\nSET spark.sql.extensions;\n, but cannot set/unset them.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.cache.serializer\norg.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer\nThe name of a class that implements org.apache.spark.sql.columnar.CachedBatchSerializer. It will be used to translate SQL data into a format that can more efficiently be cached. The underlying API is subject to change so use with caution. Multiple classes cannot be specified. The class must have a no-arg constructor.\n3.1.0\nspark.sql.catalog.spark_catalog.defaultDatabase\ndefault\nThe default database for session catalog.\n3.4.0\nspark.sql.event.truncate.length\n2147483647\nThreshold of SQL length beyond which it will be truncated before adding to event. Defaults to no truncation. If set to 0, callsite will be logged instead.\n3.0.0\nspark.sql.extensions\n(none)\nA comma-separated list of classes that implement Function1[SparkSessionExtensions, Unit] used to configure Spark Session extensions. The classes must have a no-args constructor. If multiple extensions are specified, they are applied in the specified order. For the case of rules and planner strategies, they are applied in the specified order. For the case of parsers, the last parser is used and each parser can delegate to its predecessor. For the case of function name conflicts, the last registered function name is used.\n2.2.0\nspark.sql.extensions.test.loadFromCp\ntrue\nFlag that determines if we should load extensions from the classpath using the SparkSessionExtensionsProvider mechanism. This is a test only flag.\nspark.sql.hive.metastore.barrierPrefixes\nA comma separated list of class prefixes that should explicitly be reloaded for each version of Hive that Spark SQL is communicating with. For example, Hive UDFs that are declared in a prefix that typically would be shared (i.e.\norg.apache.spark.*\n).\n1.4.0\nspark.sql.hive.metastore.jars\nbuiltin\nLocation of the jars that should be used to instantiate the HiveMetastoreClient.\nThis property can be one of four options:\n1. \"builtin\"\n  Use Hive 2.3.10, which is bundled with the Spark assembly when\n-Phive\nis enabled. When this option is chosen,\nspark.sql.hive.metastore.version\nmust be either\n2.3.10\nor not defined.\n2. \"maven\"\n  Use Hive jars of specified version downloaded from Maven repositories.\n3. \"path\"\n  Use Hive jars configured by\nspark.sql.hive.metastore.jars.path\nin comma separated format. Support both local or remote paths.The provided jars\n  should be the same version as\nspark.sql.hive.metastore.version\n.\n4. A classpath in the standard format for both Hive and Hadoop. The provided jars\n  should be the same version as\nspark.sql.hive.metastore.version\n.\n1.4.0\nspark.sql.hive.metastore.jars.path\nComma-separated paths of the jars that used to instantiate the HiveMetastoreClient.\nThis configuration is useful only when\nspark.sql.hive.metastore.jars\nis set as\npath\n.\nThe paths can be any of the following format:\n1. file://path/to/jar/foo.jar\n2. hdfs://nameservice/path/to/jar/foo.jar\n3. /path/to/jar/ (path without URI scheme follow conf\nfs.defaultFS\n's URI schema)\n4. [http/https/ftp]://path/to/jar/foo.jar\nNote that 1, 2, and 3 support wildcard. For example:\n1. file://path/to/jar/\n,file://path2/to/jar/\n/\n.jar\n2. hdfs://nameservice/path/to/jar/\n,hdfs://nameservice2/path/to/jar/\n/\n.jar\n3.1.0\nspark.sql.hive.metastore.sharedPrefixes\ncom.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc\nA comma separated list of class prefixes that should be loaded using the classloader that is shared between Spark SQL and a specific version of Hive. An example of classes that should be shared is JDBC drivers that are needed to talk to the metastore. Other classes that need to be shared are those that interact with classes that are already shared. For example, custom appenders that are used by log4j.\n1.4.0\nspark.sql.hive.metastore.version\n2.3.10\nVersion of the Hive metastore. Available options are\n2.0.0\nthrough\n2.3.10\n,\n3.0.0\nthrough\n3.1.3\nand\n4.0.0\nthrough\n4.0.1\n.\n1.4.0\nspark.sql.hive.thriftServer.singleSession\nfalse\nWhen set to true, Hive Thrift server is running in a single session mode. All the JDBC/ODBC connections share the temporary views, function registries, SQL configuration and the current database.\n1.6.0\nspark.sql.hive.version\n2.3.10\nThe compiled, a.k.a, builtin Hive version of the Spark distribution bundled with. Note that, this a read-only conf and only used to report the built-in hive version. If you want a different metastore client for Spark to call, please refer to spark.sql.hive.metastore.version.\n1.1.1\nspark.sql.metadataCacheTTLSeconds\n-1000ms\nTime-to-live (TTL) value for the metadata caches: partition file metadata cache and session catalog cache. This configuration only has an effect when this value having a positive value (> 0). It also requires setting 'spark.sql.catalogImplementation' to\nhive\n, setting 'spark.sql.hive.filesourcePartitionFileCacheSize' > 0 and setting 'spark.sql.hive.manageFilesourcePartitions' to\ntrue\nto be applied to the partition file metadata cache.\n3.1.0\nspark.sql.queryExecutionListeners\n(none)\nList of class names implementing QueryExecutionListener that will be automatically added to newly created sessions. The classes should have either a no-arg constructor, or a constructor that expects a SparkConf argument.\n2.3.0\nspark.sql.sources.disabledJdbcConnProviderList\nConfigures a list of JDBC connection providers, which are disabled. The list contains the name of the JDBC connection providers separated by comma.\n3.1.0\nspark.sql.streaming.streamingQueryListeners\n(none)\nList of class names implementing StreamingQueryListener that will be automatically added to newly created sessions. The classes should have either a no-arg constructor, or a constructor that expects a SparkConf argument.\n2.4.0\nspark.sql.streaming.ui.enabled\ntrue\nWhether to run the Structured Streaming Web UI for the Spark application when the Spark Web UI is enabled.\n3.0.0\nspark.sql.streaming.ui.retainedProgressUpdates\n100\nThe number of progress updates to retain for a streaming query for Structured Streaming UI.\n3.0.0\nspark.sql.streaming.ui.retainedQueries\n100\nThe number of inactive queries to retain for Structured Streaming UI.\n3.0.0\nspark.sql.ui.retainedExecutions\n1000\nNumber of executions to retain in the Spark UI.\n1.5.0\nspark.sql.warehouse.dir\n(value of\n$PWD/spark-warehouse\n)\nThe default location for managed databases and tables.\n2.0.0\nSpark Streaming\nProperty Name\nDefault\nMeaning\nSince Version\nspark.streaming.backpressure.enabled\nfalse\nEnables or disables Spark Streaming's internal backpressure mechanism (since 1.5).\n    This enables the Spark Streaming to control the receiving rate based on the\n    current batch scheduling delays and processing times so that the system receives\n    only as fast as the system can process. Internally, this dynamically sets the\n    maximum receiving rate of receivers. This rate is upper bounded by the values\nspark.streaming.receiver.maxRate\nand\nspark.streaming.kafka.maxRatePerPartition\nif they are set (see below).\n1.5.0\nspark.streaming.backpressure.initialRate\nnot set\nThis is the initial maximum receiving rate at which each receiver will receive data for the\n    first batch when the backpressure mechanism is enabled.\n2.0.0\nspark.streaming.blockInterval\n200ms\nInterval at which data received by Spark Streaming receivers is chunked\n    into blocks of data before storing them in Spark. Minimum recommended - 50 ms. See the\nperformance\n     tuning\nsection in the Spark Streaming programming guide for more details.\n0.8.0\nspark.streaming.receiver.maxRate\nnot set\nMaximum rate (number of records per second) at which each receiver will receive data.\n    Effectively, each stream will consume at most this number of records per second.\n    Setting this configuration to 0 or a negative number will put no limit on the rate.\n    See the\ndeployment guide\nin the Spark Streaming programming guide for mode details.\n1.0.2\nspark.streaming.receiver.writeAheadLog.enable\nfalse\nEnable write-ahead logs for receivers. All the input data received through receivers\n    will be saved to write-ahead logs that will allow it to be recovered after driver failures.\n    See the\ndeployment guide\nin the Spark Streaming programming guide for more details.\n1.2.1\nspark.streaming.unpersist\ntrue\nForce RDDs generated and persisted by Spark Streaming to be automatically unpersisted from\n    Spark's memory. The raw input data received by Spark Streaming is also automatically cleared.\n    Setting this to false will allow the raw data and persisted RDDs to be accessible outside the\n    streaming application as they will not be cleared automatically. But it comes at the cost of\n    higher memory usage in Spark.\n0.9.0\nspark.streaming.stopGracefullyOnShutdown\nfalse\nIf\ntrue\n, Spark shuts down the\nStreamingContext\ngracefully on JVM\n    shutdown rather than immediately.\n1.4.0\nspark.streaming.kafka.maxRatePerPartition\nnot set\nMaximum rate (number of records per second) at which data will be read from each Kafka\n    partition when using the new Kafka direct stream API. See the\nKafka Integration guide\nfor more details.\n1.3.0\nspark.streaming.kafka.minRatePerPartition\n1\nMinimum rate (number of records per second) at which data will be read from each Kafka\n    partition when using the new Kafka direct stream API.\n2.4.0\nspark.streaming.ui.retainedBatches\n1000\nHow many batches the Spark Streaming UI and status APIs remember before garbage collecting.\n1.0.0\nspark.streaming.driver.writeAheadLog.closeFileAfterWrite\nfalse\nWhether to close the file after writing a write-ahead log record on the driver. Set this to 'true'\n    when you want to use S3 (or any file system that does not support flushing) for the metadata WAL\n    on the driver.\n1.6.0\nspark.streaming.receiver.writeAheadLog.closeFileAfterWrite\nfalse\nWhether to close the file after writing a write-ahead log record on the receivers. Set this to 'true'\n    when you want to use S3 (or any file system that does not support flushing) for the data WAL\n    on the receivers.\n1.6.0\nSparkR (deprecated)\nProperty Name\nDefault\nMeaning\nSince Version\nspark.r.numRBackendThreads\n2\nNumber of threads used by RBackend to handle RPC calls from SparkR package.\n1.4.0\nspark.r.command\nRscript\nExecutable for executing R scripts in cluster modes for both driver and workers.\n1.5.3\nspark.r.driver.command\nspark.r.command\nExecutable for executing R scripts in client modes for driver. Ignored in cluster modes.\n1.5.3\nspark.r.shell.command\nR\nExecutable for executing sparkR shell in client modes for driver. Ignored in cluster modes. It is the same as environment variable\nSPARKR_DRIVER_R\n, but take precedence over it.\nspark.r.shell.command\nis used for sparkR shell while\nspark.r.driver.command\nis used for running R script.\n2.1.0\nspark.r.backendConnectionTimeout\n6000\nConnection timeout set by R process on its connection to RBackend in seconds.\n2.1.0\nspark.r.heartBeatInterval\n100\nInterval for heartbeats sent from SparkR backend to R process to prevent connection timeout.\n2.1.0\nGraphX\nProperty Name\nDefault\nMeaning\nSince Version\nspark.graphx.pregel.checkpointInterval\n-1\nCheckpoint interval for graph and message in Pregel. It used to avoid stackOverflowError due to long lineage chains\n  after lots of iterations. The checkpoint is disabled by default.\n2.2.0\nCluster Managers\nEach cluster manager in Spark has additional configuration options. Configurations\ncan be found on the pages for each mode:\nYARN\nKubernetes\nStandalone Mode\nEnvironment Variables\nCertain Spark settings can be configured through environment variables, which are read from the\nconf/spark-env.sh\nscript in the directory where Spark is installed (or\nconf/spark-env.cmd\non\nWindows). In Standalone mode, this file can give machine specific information such as\nhostnames. It is also sourced when running local Spark applications or submission scripts.\nNote that\nconf/spark-env.sh\ndoes not exist by default when Spark is installed. However, you can\ncopy\nconf/spark-env.sh.template\nto create it. Make sure you make the copy executable.\nThe following variables can be set in\nspark-env.sh\n:\nEnvironment Variable\nMeaning\nJAVA_HOME\nLocation where Java is installed (if it's not on your default\nPATH\n).\nPYSPARK_PYTHON\nPython binary executable to use for PySpark in both driver and workers (default is\npython3\nif available, otherwise\npython\n).\n    Property\nspark.pyspark.python\ntake precedence if it is set\nPYSPARK_DRIVER_PYTHON\nPython binary executable to use for PySpark in driver only (default is\nPYSPARK_PYTHON\n).\n    Property\nspark.pyspark.driver.python\ntake precedence if it is set\nSPARKR_DRIVER_R\nR binary executable to use for SparkR shell (default is\nR\n).\n    Property\nspark.r.shell.command\ntake precedence if it is set\nSPARK_LOCAL_IP\nIP address of the machine to bind to.\nSPARK_PUBLIC_DNS\nHostname your Spark program will advertise to other machines.\nIn addition to the above, there are also options for setting up the Spark\nstandalone cluster scripts\n, such as number of cores\nto use on each machine and maximum memory.\nSince\nspark-env.sh\nis a shell script, some of these can be set programmatically – for example, you might\ncompute\nSPARK_LOCAL_IP\nby looking up the IP of a specific network interface.\nNote: When running Spark on YARN in\ncluster\nmode, environment variables need to be set using the\nspark.yarn.appMasterEnv.[EnvironmentVariableName]\nproperty in your\nconf/spark-defaults.conf\nfile.  Environment variables that are set in\nspark-env.sh\nwill not be reflected in the YARN Application Master process in\ncluster\nmode.  See the\nYARN-related Spark Properties\nfor more information.\nConfiguring Logging\nSpark uses\nlog4j\nfor logging. You can configure it by adding a\nlog4j2.properties\nfile in the\nconf\ndirectory. To get started, copy one of the provided templates:\nlog4j2.properties.template\n(for plain text logging) or\nlog4j2-json-layout.properties.template\n(for structured logging).\nPlain Text Logging\nThe default logging format is plain text, using Log4j’s\nPattern Layout\n.\nMDC (Mapped Diagnostic Context) information is not included by default in plain text logs. To include it, update the\nPatternLayout\nconfiguration in the\nlog4j2.properties\nfile. For example, add\n%X{task_name}\nto include the task name in logs. Additionally, use\nspark.sparkContext.setLocalProperty(\"key\", \"value\")\nto add custom data to the MDC.\nStructured Logging\nStarting with version 4.0.0,\nspark-submit\nsupports optional structured logging using the\nJSON Template Layout\n. This format enables efficient querying of logs with Spark SQL using the JSON data source and includes all MDC information for improved searchability and debugging.\nTo enable structured logging and include MDC information, set the configuration\nspark.log.structuredLogging.enabled\nto\ntrue\n(default is\nfalse\n). For additional customization, copy\nlog4j2-json-layout.properties.template\nto\nconf/log4j2.properties\nand adjust as needed.\nQuerying Structured Logs with Spark SQL\nTo query structured logs in JSON format, use the following code snippet:\nPython:\nfrom\npyspark.logger\nimport\nSPARK_LOG_SCHEMA\nlogDf\n=\nspark\n.\nread\n.\nschema\n(\nSPARK_LOG_SCHEMA\n).\njson\n(\n\"\npath/to/logs\n\"\n)\nScala:\nimport\norg.apache.spark.util.LogUtils.SPARK_LOG_SCHEMA\nval\nlogDf\n=\nspark\n.\nread\n.\nschema\n(\nSPARK_LOG_SCHEMA\n).\njson\n(\n\"path/to/logs\"\n)\nNote\n: If you’re using the interactive shell (pyspark shell or spark-shell), you can omit the import statement in the code because SPARK_LOG_SCHEMA is already available in the shell’s context.\nOverriding configuration directory\nTo specify a different configuration directory other than the default “SPARK_HOME/conf”,\nyou can set SPARK_CONF_DIR. Spark will use the configuration files (spark-defaults.conf, spark-env.sh, log4j2.properties, etc)\nfrom this directory.\nInheriting Hadoop Cluster Configuration\nIf you plan to read and write from HDFS using Spark, there are two Hadoop configuration files that\nshould be included on Spark’s classpath:\nhdfs-site.xml\n, which provides default behaviors for the HDFS client.\ncore-site.xml\n, which sets the default filesystem name.\nThe location of these configuration files varies across Hadoop versions, but\na common location is inside of\n/etc/hadoop/conf\n. Some tools create\nconfigurations on-the-fly, but offer a mechanism to download copies of them.\nTo make these files visible to Spark, set\nHADOOP_CONF_DIR\nin\n$SPARK_HOME/conf/spark-env.sh\nto a location containing the configuration files.\nCustom Hadoop/Hive Configuration\nIf your Spark application is interacting with Hadoop, Hive, or both, there are probably Hadoop/Hive\nconfiguration files in Spark’s classpath.\nMultiple running applications might require different Hadoop/Hive client side configurations.\nYou can copy and modify\nhdfs-site.xml\n,\ncore-site.xml\n,\nyarn-site.xml\n,\nhive-site.xml\nin\nSpark’s classpath for each application. In a Spark cluster running on YARN, these configuration\nfiles are set cluster-wide, and cannot safely be changed by the application.\nThe better choice is to use spark hadoop properties in the form of\nspark.hadoop.*\n, and use\nspark hive properties in the form of\nspark.hive.*\n.\nFor example, adding configuration “spark.hadoop.abc.def=xyz” represents adding hadoop property “abc.def=xyz”,\nand adding configuration “spark.hive.abc=xyz” represents adding hive property “hive.abc=xyz”.\nThey can be considered as same as normal spark properties which can be set in\n$SPARK_HOME/conf/spark-defaults.conf\nIn some cases, you may want to avoid hard-coding certain configurations in a\nSparkConf\n. For\ninstance, Spark allows you to simply create an empty conf and set spark/spark hadoop/spark hive properties.\nval\nconf\n=\nnew\nSparkConf\n().\nset\n(\n\"spark.hadoop.abc.def\"\n,\n\"xyz\"\n)\nval\nsc\n=\nnew\nSparkContext\n(\nconf\n)\nAlso, you can modify or add configurations at runtime:\n./bin/spark-submit\n\\\n--name\n\"My app\"\n\\\n--master\n\"local[4]\"\n\\\n--conf\nspark.eventLog.enabled\n=\nfalse\n\\\n--conf\n\"spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps\"\n\\\n--conf\nspark.hadoop.abc.def\n=\nxyz\n\\\n--conf\nspark.hive.abc\n=\nxyz\n  myApp.jar\nCustom Resource Scheduling and Configuration Overview\nGPUs and other accelerators have been widely used for accelerating special workloads, e.g.,\ndeep learning and signal processing. Spark now supports requesting and scheduling generic resources, such as GPUs, with a few caveats. The current implementation requires that the resource have addresses that can be allocated by the scheduler. It requires your cluster manager to support and be properly configured with the resources.\nThere are configurations available to request resources for the driver:\nspark.driver.resource.{resourceName}.amount\n, request resources for the executor(s):\nspark.executor.resource.{resourceName}.amount\nand specify the requirements for each task:\nspark.task.resource.{resourceName}.amount\n. The\nspark.driver.resource.{resourceName}.discoveryScript\nconfig is required on YARN, Kubernetes and a client side Driver on Spark Standalone.\nspark.executor.resource.{resourceName}.discoveryScript\nconfig is required for YARN and Kubernetes. Kubernetes also requires\nspark.driver.resource.{resourceName}.vendor\nand/or\nspark.executor.resource.{resourceName}.vendor\n. See the config descriptions above for more information on each.\nSpark will use the configurations specified to first request containers with the corresponding resources from the cluster manager. Once it gets the container, Spark launches an Executor in that container which will discover what resources the container has and the addresses associated with each resource. The Executor will register with the Driver and report back the resources available to that Executor. The Spark scheduler can then schedule tasks to each Executor and assign specific resource addresses based on the resource requirements the user specified. The user can see the resources assigned to a task using the\nTaskContext.get().resources\napi. On the driver, the user can see the resources assigned with the SparkContext\nresources\ncall. It’s then up to the user to use the assigned addresses to do the processing they want or pass those into the ML/AI framework they are using.\nSee your cluster manager specific page for requirements and details on each of -\nYARN\n,\nKubernetes\nand\nStandalone Mode\n. It is currently not available with local mode. And please also note that local-cluster mode with multiple workers is not supported(see Standalone documentation).\nStage Level Scheduling Overview\nThe stage level scheduling feature allows users to specify task and executor resource requirements at the stage level. This allows for different stages to run with executors that have different resources. A prime example of this is one ETL stage runs with executors with just CPUs, the next stage is an ML stage that needs GPUs. Stage level scheduling allows for user to request different executors that have GPUs when the ML stage runs rather then having to acquire executors with GPUs at the start of the application and them be idle while the ETL stage is being run.\nThis is only available for the RDD API in Scala, Java, and Python.  It is available on YARN, Kubernetes and Standalone when dynamic allocation is enabled. When dynamic allocation is disabled, it allows users to specify different task resource requirements at stage level, and this is supported on YARN, Kubernetes and Standalone cluster right now. See the\nYARN\npage or\nKubernetes\npage or\nStandalone\npage for more implementation details.\nSee the\nRDD.withResources\nand\nResourceProfileBuilder\nAPI’s for using this feature. When dynamic allocation is disabled, tasks with different task resource requirements will share executors with\nDEFAULT_RESOURCE_PROFILE\n. While when dynamic allocation is enabled, the current implementation acquires new executors for each\nResourceProfile\ncreated and currently has to be an exact match. Spark does not try to fit tasks into an executor that require a different ResourceProfile than the executor was created with. Executors that are not in use will idle timeout with the dynamic allocation logic. The default configuration for this feature is to only allow one ResourceProfile per stage. If the user associates more then 1 ResourceProfile to an RDD, Spark will throw an exception by default. See config\nspark.scheduler.resource.profileMergeConflicts\nto control that behavior. The current merge strategy Spark implements when\nspark.scheduler.resource.profileMergeConflicts\nis enabled is a simple max of each resource within the conflicting ResourceProfiles. Spark will create a new ResourceProfile with the max of each of the resources.\nPush-based shuffle overview\nPush-based shuffle helps improve the reliability and performance of spark shuffle. It takes a best-effort approach to push the shuffle blocks generated by the map tasks to remote external shuffle services to be merged per shuffle partition. Reduce tasks fetch a combination of merged shuffle partitions and original shuffle blocks as their input data, resulting in converting small random disk reads by external shuffle services into large sequential reads. Possibility of better data locality for reduce tasks additionally helps minimize network IO. Push-based shuffle takes priority over batch fetch for some scenarios, like partition coalesce when merged output is available.\nPush-based shuffle improves performance for long running jobs/queries which involves large disk I/O during shuffle. Currently it is not well suited for jobs/queries which runs quickly dealing with lesser amount of shuffle data. This will be further improved in the future releases.\nCurrently push-based shuffle is only supported for Spark on YARN with external shuffle service.\nExternal Shuffle service(server) side configuration options\nProperty Name\nDefault\nMeaning\nSince Version\nspark.shuffle.push.server.mergedShuffleFileManagerImpl\norg.apache.spark.network.shuffle.\nNoOpMergedShuffleFileManager\nClass name of the implementation of\nMergedShuffleFileManager\nthat manages push-based shuffle. This acts as a server side config to disable or enable push-based shuffle. By default, push-based shuffle is disabled at the server side.\nTo enable push-based shuffle on the server side, set this config to\norg.apache.spark.network.shuffle.RemoteBlockPushResolver\n3.2.0\nspark.shuffle.push.server.minChunkSizeInMergedShuffleFile\n2m\nThe minimum size of a chunk when dividing a merged shuffle file into multiple chunks during push-based shuffle. A merged shuffle file consists of multiple small shuffle blocks. Fetching the complete merged shuffle file in a single disk I/O increases the memory requirements for both the clients and the external shuffle services. Instead, the external shuffle service serves the merged file in\nMB-sized chunks\n.\nThis configuration controls how big a chunk can get. A corresponding index file for each merged shuffle file will be generated indicating chunk boundaries.\nSetting this too high would increase the memory requirements on both the clients and the external shuffle service.\nSetting this too low would increase the overall number of RPC requests to external shuffle service unnecessarily.\n3.2.0\nspark.shuffle.push.server.mergedIndexCacheSize\n100m\nThe maximum size of cache in memory which could be used in push-based shuffle for storing merged index files. This cache is in addition to the one configured via\nspark.shuffle.service.index.cache.size\n.\n3.2.0\nClient side configuration options\nProperty Name\nDefault\nMeaning\nSince Version\nspark.shuffle.push.enabled\nfalse\nSet to true to enable push-based shuffle on the client side and works in conjunction with the server side flag\nspark.shuffle.push.server.mergedShuffleFileManagerImpl\n.\n3.2.0\nspark.shuffle.push.finalize.timeout\n10s\nThe amount of time driver waits in seconds, after all mappers have finished for a given shuffle map stage, before it sends merge finalize requests to remote external shuffle services. This gives the external shuffle services extra time to merge blocks. Setting this too long could potentially lead to performance regression.\n3.2.0\nspark.shuffle.push.maxRetainedMergerLocations\n500\nMaximum number of merger locations cached for push-based shuffle. Currently, merger locations are hosts of external shuffle services responsible for handling pushed blocks, merging them and serving merged blocks for later shuffle fetch.\n3.2.0\nspark.shuffle.push.mergersMinThresholdRatio\n0.05\nRatio used to compute the minimum number of shuffle merger locations required for a stage based on the number of partitions for the reducer stage. For example, a reduce stage which has 100 partitions and uses the default value 0.05 requires at least 5 unique merger locations to enable push-based shuffle.\n3.2.0\nspark.shuffle.push.mergersMinStaticThreshold\n5\nThe static threshold for number of shuffle push merger locations should be available in order to enable push-based shuffle for a stage. Note this config works in conjunction with\nspark.shuffle.push.mergersMinThresholdRatio\n. Maximum of\nspark.shuffle.push.mergersMinStaticThreshold\nand\nspark.shuffle.push.mergersMinThresholdRatio\nratio number of mergers needed to enable push-based shuffle for a stage. For example: with 1000 partitions for the child stage with spark.shuffle.push.mergersMinStaticThreshold as 5 and spark.shuffle.push.mergersMinThresholdRatio set to 0.05, we would need at least 50 mergers to enable push-based shuffle for that stage.\n3.2.0\nspark.shuffle.push.numPushThreads\n(none)\nSpecify the number of threads in the block pusher pool. These threads assist in creating connections and pushing blocks to remote external shuffle services.\n    By default, the threadpool size is equal to the number of spark executor cores.\n3.2.0\nspark.shuffle.push.maxBlockSizeToPush\n1m\nThe max size of an individual block to push to the remote external shuffle services. Blocks larger than this threshold are not pushed to be merged remotely. These shuffle blocks will be fetched in the original manner.\nSetting this too high would result in more blocks to be pushed to remote external shuffle services but those are already efficiently fetched with the existing mechanisms resulting in additional overhead of pushing the large blocks to remote external shuffle services. It is recommended to set\nspark.shuffle.push.maxBlockSizeToPush\nlesser than\nspark.shuffle.push.maxBlockBatchSize\nconfig's value.\nSetting this too low would result in lesser number of blocks getting merged and directly fetched from mapper external shuffle service results in higher small random reads affecting overall disk I/O performance.\n3.2.0\nspark.shuffle.push.maxBlockBatchSize\n3m\nThe max size of a batch of shuffle blocks to be grouped into a single push request. Default is set to\n3m\nin order to keep it slightly higher than\nspark.storage.memoryMapThreshold\ndefault which is\n2m\nas it is very likely that each batch of block gets memory mapped which incurs higher overhead.\n3.2.0\nspark.shuffle.push.merge.finalizeThreads\n8\nNumber of threads used by driver to finalize shuffle merge. Since it could potentially take seconds for a large shuffle to finalize,\n    having multiple threads helps driver to handle concurrent shuffle merge finalize requests when push-based shuffle is enabled.\n3.3.0\nspark.shuffle.push.minShuffleSizeToWait\n500m\nDriver will wait for merge finalization to complete only if total shuffle data size is more than this threshold. If total shuffle size is less, driver will immediately finalize the shuffle output.\n3.3.0\nspark.shuffle.push.minCompletedPushRatio\n1.0\nFraction of minimum map partitions that should be push complete before driver starts shuffle merge finalization during push based shuffle.\n3.3.0"}
{"url": "https://spark.apache.org/docs/latest/cloud-integration.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nIntegration with Cloud Infrastructures\nIntroduction\nImportant: Cloud Object Stores are Not Real Filesystems\nConsistency\nInstallation\nAuthenticating\nConfiguring\nRecommended settings for writing to object stores\nParquet I/O Settings\nORC I/O Settings\nSpark Streaming and Object Storage\nCommitting work into cloud storage safely and fast.\nHadoop S3A committers\nAmazon EMR: the EMRFS S3-optimized committer\nAzure and Google cloud storage: MapReduce Intermediate Manifest Committer.\nIBM Cloud Object Storage: Stocator\nCloud Committers and\nINSERT OVERWRITE TABLE\nFurther Reading\nIntroduction\nAll major cloud providers offer persistent data storage in\nobject stores\n.\nThese are not classic “POSIX” file systems.\nIn order to store hundreds of petabytes of data without any single points of failure,\nobject stores replace the classic file system directory tree\nwith a simpler model of\nobject-name => data\n. To enable remote access, operations\non objects are usually offered as (slow) HTTP REST operations.\nSpark can read and write data in object stores through filesystem connectors implemented\nin Hadoop or provided by the infrastructure suppliers themselves.\nThese connectors make the object stores look\nalmost\nlike file systems, with directories and files\nand the classic operations on them such as list, delete and rename.\nImportant: Cloud Object Stores are Not Real Filesystems\nWhile the stores appear to be filesystems, underneath\nthey are still object stores,\nand the difference is significant\nThey cannot be used as a direct replacement for a cluster filesystem such as HDFS\nexcept where this is explicitly stated\n.\nKey differences are:\nThe means by which directories are emulated may make working with them slow.\nRename operations may be very slow and, on failure, leave the store in an unknown state.\nSeeking within a file may require new HTTP calls, hurting performance.\nHow does this affect Spark?\nReading and writing data can be significantly slower than working with a normal filesystem.\nSome directory structures may be very inefficient to scan during query split calculation.\nThe rename-based algorithm by which Spark normally commits work when saving an RDD, DataFrame or Dataset\n is potentially both slow and unreliable.\nFor these reasons, it is not always safe to use an object store as a direct destination of queries, or as\nan intermediate store in a chain of queries. Consult the documentation of the object store and its\nconnector to determine which uses are considered safe.\nConsistency\nAs of 2021, the object stores of Amazon (S3), Google Cloud (GCS) and Microsoft (Azure Storage, ADLS Gen1, ADLS Gen2) are all\nconsistent\n.\nThis means that as soon as a file is written/updated it can be listed, viewed and opened by other processes\n-and the latest version will be retrieved. This was a known issue with AWS S3, especially with 404 caching\nof HEAD requests made before an object was created.\nEven so: none of the store connectors provide any guarantees as to how their clients cope with objects\nwhich are overwritten while a stream is reading them. Do not assume that the old file can be safely\nread, nor that there is any bounded time period for changes to become visible -or indeed, that\nthe clients will not simply fail if a file being read is overwritten.\nFor this reason: avoid overwriting files where it is known/likely that other clients\nwill be actively reading them.\nOther object stores are\ninconsistent\nThis includes\nOpenStack Swift\n.\nSuch stores are not always safe to use as a destination of work -consult\neach store’s specific documentation.\nInstallation\nWith the relevant libraries on the classpath and Spark configured with valid credentials,\nobjects can be read or written by using their URLs as the path to data.\nFor example\nsparkContext.textFile(\"s3a://landsat-pds/scene_list.gz\")\nwill create\nan RDD of the file\nscene_list.gz\nstored in S3, using the s3a connector.\nTo add the relevant libraries to an application’s classpath, include the\nhadoop-cloud\nmodule and its dependencies.\nIn Maven, add the following to the\npom.xml\nfile, assuming\nspark.version\nis set to the chosen version of Spark:\n<dependencyManagement>\n...\n<dependency>\n<groupId>\norg.apache.spark\n</groupId>\n<artifactId>\nspark-hadoop-cloud_2.13\n</artifactId>\n<version>\n${spark.version}\n</version>\n<scope>\nprovided\n</scope>\n</dependency>\n...\n</dependencyManagement>\nCommercial products based on Apache Spark generally directly set up the classpath\nfor talking to cloud infrastructures, in which case this module may not be needed.\nAuthenticating\nSpark jobs must authenticate with the object stores to access data within them.\nWhen Spark is running in a cloud infrastructure, the credentials are usually automatically set up.\nspark-submit\nis able to read the\nAWS_ENDPOINT_URL\n,\nAWS_ACCESS_KEY_ID\n,\nAWS_SECRET_ACCESS_KEY\nand\nAWS_SESSION_TOKEN\nenvironment variables and sets the associated authentication options\nfor the\ns3n\nand\ns3a\nconnectors to Amazon S3.\nIn a Hadoop cluster, settings may be set in the\ncore-site.xml\nfile.\nAuthentication details may be manually added to the Spark configuration in\nspark-defaults.conf\nAlternatively, they can be programmatically set in the\nSparkConf\ninstance used to configure \nthe application’s\nSparkContext\n.\nImportant: never check authentication secrets into source code repositories,\nespecially public ones\nConsult\nthe Hadoop documentation\nfor the relevant\nconfiguration and security options.\nConfiguring\nEach cloud connector has its own set of configuration parameters, again, \nconsult the relevant documentation.\nRecommended settings for writing to object stores\nFor object stores whose consistency model means that rename-based commits are safe\nuse the\nFileOutputCommitter\nv2 algorithm for performance; v1 for safety.\nspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version 2\nThis does less renaming at the end of a job than the “version 1” algorithm.\nAs it still uses\nrename()\nto commit files, it is unsafe to use\nwhen the object store does not have consistent metadata/listings.\nThe committer can also be set to ignore failures when cleaning up temporary\nfiles; this reduces the risk that a transient network problem is escalated into a \njob failure:\nspark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored true\nThe original v1 commit algorithm renames the output of successful tasks\nto a job attempt directory, and then renames all the files in that directory\ninto the final destination during the job commit phase:\nspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version 1\nThe slow performance of mimicked renames on Amazon S3 makes this algorithm\nvery, very slow. The recommended solution to this is switch to an S3 “Zero Rename”\ncommitter (see below).\nFor reference, here are the performance and safety characteristics of\ndifferent stores and connectors when renaming directories:\nStore\nConnector\nDirectory Rename Safety\nRename Performance\nAmazon S3\ns3a\nUnsafe\nO(data)\nAzure Storage\nwasb\nSafe\nO(files)\nAzure Datalake Gen 2\nabfs\nSafe\nO(1)\nGoogle Cloud Storage\ngs\nMixed\nO(files)\nAs storing temporary files can run up charges; delete\ndirectories called\n\"_temporary\"\non a regular basis.\nFor AWS S3, set a limit on how long multipart uploads can remain outstanding.\nThis avoids incurring bills from incompleted uploads.\nFor Google cloud, directory rename is file-by-file. Consider using the v2 committer\nand only write code which generates idempotent output -including filenames,\nas it is\nno more unsafe\nthan the v1 committer, and faster.\nParquet I/O Settings\nFor optimal performance when working with Parquet data use the following settings:\nspark.hadoop.parquet.enable.summary-metadata false\nspark.sql.parquet.mergeSchema false\nspark.sql.parquet.filterPushdown true\nspark.sql.hive.metastorePartitionPruning true\nThese minimise the amount of data read during queries.\nORC I/O Settings\nFor best performance when working with ORC data, use these settings:\nspark.sql.orc.filterPushdown true\nspark.sql.orc.splits.include.file.footer true\nspark.sql.orc.cache.stripe.details.size 10000\nspark.sql.hive.metastorePartitionPruning true\nAgain, these minimise the amount of data read during queries.\nSpark Streaming and Object Storage\nSpark Streaming can monitor files added to object stores, by\ncreating a\nFileInputDStream\nto monitor a path in the store through a call to\nStreamingContext.textFileStream()\n.\nThe time to scan for new files is proportional to the number of files\nunder the path, not the number of\nnew\nfiles, so it can become a slow operation.\nThe size of the window needs to be set to handle this.\nFiles only appear in an object store once they are completely written; there\nis no need for a workflow of write-then-rename to ensure that files aren’t picked up\nwhile they are still being written. Applications can write straight to the monitored directory.\nIn case of the default checkpoint file manager called\nFileContextBasedCheckpointFileManager\nstreams should only be checkpointed to a store implementing a fast and\natomic\nrename()\noperation. Otherwise the checkpointing may be slow and potentially unreliable.\nOn AWS S3 with Hadoop 3.3.1 or later using the S3A connector the abortable stream based checkpoint\nfile manager can be used (by setting the\nspark.sql.streaming.checkpointFileManagerClass\nconfiguration to\norg.apache.spark.internal.io.cloud.AbortableStreamBasedCheckpointFileManager\n)\nwhich eliminates the slow rename. In this case users must be extra careful to avoid the reuse of\nthe checkpoint location among multiple queries running parallelly as that could lead to corruption\nof the checkpointing data.\nCommitting work into cloud storage safely and fast.\nAs covered earlier, commit-by-rename is dangerous on any object store which\nexhibits eventual consistency (example: S3), and often slower than classic\nfilesystem renames.\nSome object store connectors provide custom committers to commit tasks and\njobs without using rename.\nHadoop S3A committers\nIn versions of Spark built with Hadoop 3.1 or later,\nthe hadoop-aws JAR contains committers safe to use for S3 storage\naccessed via the s3a connector.\nInstead of writing data to a temporary directory on the store for renaming,\nthese committers write the files to the final destination, but do not issue\nthe final POST command to make a large “multi-part” upload visible. Those\noperations are postponed until the job commit itself. As a result, task and\njob commit are much faster, and task failures do not affect the result.\nTo switch to the S3A committers, use a version of Spark was built with Hadoop\n3.1 or later, and switch the committers through the following options.\nspark.hadoop.fs.s3a.committer.name directory\nspark.sql.sources.commitProtocolClass org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\nspark.sql.parquet.output.committer.class org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\nIt has been tested with the most common formats supported by Spark.\nmydataframe\n.\nwrite\n.\nformat\n(\n\"\nparquet\n\"\n).\nsave\n(\n\"\ns3a://bucket/destination\n\"\n)\nMore details on these committers can be found in\nthe latest Hadoop documentation\nwith S3A committer detail covered in\nCommitting work to S3 with the S3A Committers\n.\nNote: depending upon the committer used, in-progress statistics may be\nunder-reported with Hadoop versions before 3.3.1.\nAmazon EMR: the EMRFS S3-optimized committer\nAmazon EMR has its own S3-aware committers for parquet data.\nFor instructions on use, see\nthe EMRFS S3-optimized committer\nFor implementation and performance details, see\n[“Improve Apache Spark write performance on Apache Parquet formats with the EMRFS S3-optimized committer”](https://aws.amazon.com/blogs/big-data/improve-apache-spark-write-performance-on-apache-parquet-formats-with-the-emrfs-s3-optimized-committer/\nAzure and Google cloud storage: MapReduce Intermediate Manifest Committer.\nVersions of the hadoop-mapreduce-core JAR shipped after September 2022 (3.3.5 and later)\ncontain a committer optimized for performance and resilience on\nAzure ADLS Generation 2 and Google Cloud Storage.\nThis committer, the “manifest committer” uses a manifest file to propagate\ndirectory listing information from the task committers to the job committer.\nThese manifests can be written atomically, without relying on atomic directory rename,\nsomething GCS lacks.\nThe job committer reads these manifests and will rename files from the task output\ndirectories directly into the destination directory, in parallel, with optional\nrate limiting to avoid throttling IO.\nThis delivers performance and scalability on the object stores.\nIt is not critical for job correctness to use this with Azure storage; the\nclassic FileOutputCommitter is safe there -however this new committer scales\nbetter for large jobs with deep and wide directory trees.\nBecause Google GCS does not support atomic directory renaming,\nthe manifest committer should be used where available.\nThis committer does support  “dynamic partition overwrite” (see below).\nFor details on availability and use of this committer, consult\nthe hadoop documentation for the Hadoop release used.\nIt is not available on Hadoop 3.3.4 or earlier.\nIBM Cloud Object Storage: Stocator\nIBM provide the Stocator output committer for IBM Cloud Object Storage and OpenStack Swift.\nSource, documentation and releasea can be found at\nStocator - Storage Connector for Apache Spark\n.\nCloud Committers and\nINSERT OVERWRITE TABLE\nSpark has a feature called “dynamic partition overwrite”; a table can be updated and only those\npartitions into which new data is added will have their contents replaced.\nThis is used in SQL statements of the form\nINSERT OVERWRITE TABLE\n,\nand when Datasets are written in mode “overwrite”\neventDataset\n.\nwrite\n.\nmode\n(\n\"overwrite\"\n)\n.\npartitionBy\n(\n\"year\"\n,\n\"month\"\n)\n.\nformat\n(\n\"parquet\"\n)\n.\nsave\n(\ntablePath\n)\nThis feature uses file renaming and has specific requirements of\nboth the committer and the filesystem:\nThe committer’s working directory must be in the destination filesystem.\nThe target filesystem must support file rename efficiently.\nThese conditions are\nnot\nmet by the S3A committers and AWS S3 storage.\nCommitters for other cloud stores\nmay\nsupport this feature, and\ndeclare to spark that they are compatible. If dynamic partition overwrite\nis required when writing data through a hadoop committer, Spark\nwill always permit this when the original\nFileOutputCommitter\nis used. For other committers, after their instantiation, Spark\nwill probe for their declaration of compatibility, and\npermit the operation if state that they are compatible.\nIf the committer is not compatible, the operation will fail with\nthe error message\nPathOutputCommitter does not support dynamicPartitionOverwrite\nUnless there is a compatible committer for the target filesystem,\nthe sole solution is to use a cloud-friendly format for data\nstorage.\nFurther Reading\nHere is the documentation on the standard connectors both from Apache and the cloud providers.\nAzure Blob Storage\n.\nAzure Blob Filesystem (ABFS) and Azure Datalake Gen 2\n.\nAzure Data Lake Gen 1\n.\nAmazon S3 Strong Consistency\nHadoop-AWS module (Hadoop 3.x)\n.\nAmazon EMR File System (EMRFS)\n. From Amazon.\nUsing the EMRFS S3-optimized Committer\nGoogle Cloud Storage Connector for Spark and Hadoop\n. From Google.\nThe Azure Blob Filesystem driver (ABFS)\nIBM Cloud Object Storage connector for Apache Spark:\nStocator\n,\nIBM Object Storage\n. From IBM.\nUsing JindoFS SDK to access Alibaba Cloud OSS\n.\nThe Cloud Committer problem and hive-compatible solutions\nCommitting work to S3 with the S3A Committers\nImprove Apache Spark write performance on Apache Parquet formats with the EMRFS S3-optimized committer\nThe Manifest Committer for Azure and Google Cloud Storage\nA Zero-rename committer\n.\nStocator: A High Performance Object Store Connector for Spark"}
{"url": "https://spark.apache.org/docs/latest/migration-guide.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMigration Guide\nThis page documents sections of the migration guide for each component in order\nfor users to migrate effectively.\nSpark Core\nSQL, Datasets, and DataFrame\nStructured Streaming\nMLlib (Machine Learning)\nPySpark (Python on Spark)\nSparkR (R on Spark)"}
{"url": "https://spark.apache.org/docs/latest/spark-connect-overview.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark Connect Overview\nBuilding client-side Spark applications\nIn Apache Spark 3.4, Spark Connect introduced a decoupled client-server\narchitecture that allows remote connectivity to Spark clusters using the\nDataFrame API and unresolved logical plans as the protocol. The separation\nbetween client and server allows Spark and its open ecosystem to be\nleveraged from everywhere. It can be embedded in modern data applications,\nin IDEs, Notebooks and programming languages.\nTo get started, see\nQuickstart: Spark Connect\n.\nHow Spark Connect works\nThe Spark Connect client library is designed to simplify Spark application\ndevelopment. It is a thin API that can be embedded everywhere: in application\nservers, IDEs, notebooks, and programming languages. The Spark Connect API\nbuilds on Spark’s DataFrame API using unresolved logical plans as a\nlanguage-agnostic protocol between the client and the Spark driver.\nThe Spark Connect client translates DataFrame operations into unresolved\nlogical query plans which are encoded using protocol buffers. These are sent\nto the server using the gRPC framework.\nThe Spark Connect endpoint embedded on the Spark Server receives and\ntranslates unresolved logical plans into Spark’s logical plan operators.\nThis is similar to parsing a SQL query, where attributes and relations are\nparsed and an initial parse plan is built. From there, the standard Spark\nexecution process kicks in, ensuring that Spark Connect leverages all of\nSpark’s optimizations and enhancements. Results are streamed back to the\nclient through gRPC as Apache Arrow-encoded row batches.\nHow Spark Connect client applications differ from classic Spark applications\nOne of the main design goals of Spark Connect is to enable a full separation and\nisolation of the client from the server. As a consequence, there are some changes\nthat developers need to be aware of when using Spark Connect:\nThe client does not run in the same process as the Spark driver. This means that\nthe client cannot directly access and interact with the driver JVM to manipulate\nthe execution environment. In particular, in PySpark, the client does not use Py4J\nand thus the accessing the private fields holding the JVM implementation of\nDataFrame\n,\nColumn\n,\nSparkSession\n, etc. is not possible (e.g.\ndf._jdf\n).\nBy design, the Spark Connect protocol uses Sparks logical\nplans as the abstraction to be able to declaratively describe the operations to be executed\non the server. Consequently, the Spark Connect protocol does not support all the\nexecution APIs of Spark, most importantly RDDs.\nSpark Connect provides a session-based client for its consumers. This means that the\nclient does not have access to properties of the cluster that manipulate the\nenvironment for all connected clients. Most importantly, the client does not have access\nto the static Spark configuration or the SparkContext.\nOperational benefits of Spark Connect\nWith this new architecture, Spark Connect mitigates several multi-tenant\noperational issues:\nStability\n: Applications that use too much memory will now only impact their\nown environment as they can run in their own processes. Users can define their\nown dependencies on the client and don’t need to worry about potential conflicts\nwith the Spark driver.\nUpgradability\n: The Spark driver can now seamlessly be upgraded independently\nof applications, for example to benefit from performance improvements and security fixes.\nThis means applications can be forward-compatible, as long as the server-side RPC\ndefinitions are designed to be backwards compatible.\nDebuggability and observability\n: Spark Connect enables interactive debugging\nduring development directly from your favorite IDE. Similarly, applications can\nbe monitored using the application’s framework native metrics and logging libraries.\nHow to use Spark Connect\nSpark Connect is available and supports PySpark and Scala\napplications. We will walk through how to run an Apache Spark server with Spark\nConnect and connect to it from a client application using the Spark Connect client\nlibrary.\nDownload and start Spark server with Spark Connect\nFirst, download Spark from the\nDownload Apache Spark\npage. Choose the\nlatest release in  the release drop down at the top of the page. Then choose your package type, typically\n“Pre-built for Apache Hadoop 3.3 and later”, and click the link to download.\nNow extract the Spark package you just downloaded on your computer, for example:\ntar\n-xvf\nspark-4.0.0-bin-hadoop3.tgz\nIn a terminal window, go to the\nspark\nfolder in the location where you extracted\nSpark before and run the\nstart-connect-server.sh\nscript to start Spark server with\nSpark Connect, like in this example:\n./sbin/start-connect-server.sh\nMake sure to use the same version  of the package as the Spark version you\ndownloaded previously. In this example, Spark 4.0.0 with Scala 2.13.\nNow Spark server is running and ready to accept Spark Connect sessions from client\napplications. In the next section we will walk through how to use Spark Connect\nwhen writing client applications.\nUse Spark Connect for interactive analysis\nWhen creating a Spark session, you can specify that you want to use Spark Connect\nand there are a few ways to do that outlined as follows.\nIf you do not use one of the mechanisms outlined here, your Spark session will\nwork just like before, without leveraging Spark Connect.\nSet SPARK_REMOTE environment variable\nIf you set the\nSPARK_REMOTE\nenvironment variable on the client machine where your\nSpark client application is running and create a new Spark Session as in the following\nexample, the session will be a Spark Connect session. With this approach, there is no\ncode change needed to start using Spark Connect.\nIn a terminal window, set the\nSPARK_REMOTE\nenvironment variable to point to the\nlocal Spark server you started previously on your computer:\nexport\nSPARK_REMOTE\n=\n\"sc://localhost\"\nAnd start the Spark shell as usual:\n./bin/pyspark\nThe PySpark shell is now connected to Spark using Spark Connect as indicated in the welcome message:\nClient\nconnected\nto\nthe\nSpark\nConnect\nserver\nat\nlocalhost\nSpecify Spark Connect when creating Spark session\nYou can also specify that you want to use Spark Connect explicitly when you\ncreate a Spark session.\nFor example, you can launch the PySpark shell with Spark Connect as\nillustrated here.\nTo launch the PySpark shell with Spark Connect, simply include the\nremote\nparameter and specify the location of your Spark server. We are using\nlocalhost\nin this example to connect to the local Spark server we started previously:\n./bin/pyspark\n--remote\n\"sc://localhost\"\nAnd you will notice that the PySpark shell welcome message tells you that\nyou have connected to Spark using Spark Connect:\nClient\nconnected\nto\nthe\nSpark\nConnect\nserver\nat\nlocalhost\nYou can also check the Spark session type. If it includes\n.connect.\nyou\nare using Spark Connect as shown in this example:\nSparkSession\navailable\nas\n'\nspark\n'\n.\n>>>\ntype\n(\nspark\n)\n<\nclass\n'\npyspark\n.\nsql\n.\nconnect\n.\nsession\n.\nSparkSession\n'\n>\nNow you can run PySpark code in the shell to see Spark Connect in action:\n>>>\ncolumns\n=\n[\n\"\nid\n\"\n,\n\"\nname\n\"\n]\n>>>\ndata\n=\n[(\n1\n,\n\"\nSarah\n\"\n),\n(\n2\n,\n\"\nMaria\n\"\n)]\n>>>\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n).\ntoDF\n(\n*\ncolumns\n)\n>>>\ndf\n.\nshow\n()\n+---+-----+\n|\nid\n|\nname\n|\n+---+-----+\n|\n1\n|\nSarah\n|\n|\n2\n|\nMaria\n|\n+---+-----+\nFor the Scala shell, we use an Ammonite-based REPL. Otherwise, very similar with PySpark shell.\n./bin/spark-shell\n--remote\n\"sc://localhost\"\nA greeting message will appear when the REPL successfully initializes:\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\n\\ \\/\n_\n\\/\n_\n`\n/ __/\n'_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 4.0.0-SNAPSHOT\n      /_/\n\nType in expressions to have them evaluated.\nSpark session available as '\nspark\n'.\nBy default, the REPL will attempt to connect to a local Spark Server.\nRun the following Scala code in the shell to see Spark Connect in action:\n@\nspark\n.\nrange\n(\n10\n).\ncount\nres0\n:\nLong\n=\n10L\nConfigure client-server connection\nBy default, the REPL will attempt to connect to a local Spark Server on port 15002.\nThe connection, however, may be configured in several ways as described in this configuration\nreference\n.\nSet SPARK_REMOTE environment variable\nThe SPARK_REMOTE environment variable can be set on the client machine to customize the client-server\nconnection that is initialized at REPL startup.\nexport\nSPARK_REMOTE\n=\n\"sc://myhost.com:443/;token=ABCDEFG\"\n./bin/spark-shell\nor\nSPARK_REMOTE\n=\n\"sc://myhost.com:443/;token=ABCDEFG\"\nspark-connect-repl\nConfigure programmatically with a connection string\nThe connection may also be programmatically created using\nSparkSession#builder\nas in this example:\n@\nimport\norg.apache.spark.sql.SparkSession\n@\nval\nspark\n=\nSparkSession\n.\nbuilder\n.\nremote\n(\n\"sc://localhost:443/;token=ABCDEFG\"\n).\ngetOrCreate\n()\nUse Spark Connect in standalone applications\nFirst, install PySpark with\npip install pyspark[connect]==4.0.0\nor if building a packaged PySpark application/library,\nadd it your setup.py file as:\ninstall_requires\n=\n[\n'\npyspark[connect]==4.0.0\n'\n]\nWhen writing your own code, include the\nremote\nfunction with a reference to\nyour Spark server when you create a Spark session, as in this example:\nfrom\npyspark.sql\nimport\nSparkSession\nspark\n=\nSparkSession\n.\nbuilder\n.\nremote\n(\n\"\nsc://localhost\n\"\n).\ngetOrCreate\n()\nFor illustration purposes, we’ll create a simple Spark Connect application, SimpleApp.py:\n\"\"\"\nSimpleApp.py\n\"\"\"\nfrom\npyspark.sql\nimport\nSparkSession\nlogFile\n=\n\"\nYOUR_SPARK_HOME/README.md\n\"\n# Should be some file on your system\nspark\n=\nSparkSession\n.\nbuilder\n.\nremote\n(\n\"\nsc://localhost\n\"\n).\nappName\n(\n\"\nSimpleApp\n\"\n).\ngetOrCreate\n()\nlogData\n=\nspark\n.\nread\n.\ntext\n(\nlogFile\n).\ncache\n()\nnumAs\n=\nlogData\n.\nfilter\n(\nlogData\n.\nvalue\n.\ncontains\n(\n'\na\n'\n)).\ncount\n()\nnumBs\n=\nlogData\n.\nfilter\n(\nlogData\n.\nvalue\n.\ncontains\n(\n'\nb\n'\n)).\ncount\n()\nprint\n(\n\"\nLines with a: %i, lines with b: %i\n\"\n%\n(\nnumAs\n,\nnumBs\n))\nspark\n.\nstop\n()\nThis program just counts the number of lines containing ‘a’ and the number containing ‘b’ in a text file.\nNote that you’ll need to replace YOUR_SPARK_HOME with the location where Spark is installed.\nWe can run this application with the regular Python interpreter as follows:\n# Use the Python interpreter to run your application\n$\npython\nSimpleApp\n.\npy\n...\nLines\nwith\na\n:\n72\n,\nlines\nwith\nb\n:\n39\nTo use Spark Connect as part of a Scala application/project, we first need to include the right dependencies.\nUsing the\nsbt\nbuild system as an example, we add the following dependencies to the\nbuild.sbt\nfile:\nlibraryDependencies += \"org.apache.spark\" %% \"spark-connect-client-jvm\" % \"4.0.0\"\nWhen writing your own code, include the\nremote\nfunction with a reference to\nyour Spark server when you create a Spark session, as in this example:\nimport\norg.apache.spark.sql.SparkSession\nval\nspark\n=\nSparkSession\n.\nbuilder\n().\nremote\n(\n\"sc://localhost\"\n).\ngetOrCreate\n()\nNote\n: Operations that reference User Defined Code such as UDFs, filter, map, etc require a\nClassFinder\nto be registered to pickup and upload any required classfiles. Also, any JAR dependencies must be uploaded to the server using\nSparkSession#AddArtifact\n.\nExample:\nimport\norg.apache.spark.sql.connect.client.REPLClassDirMonitor\n// Register a ClassFinder to monitor and upload the classfiles from the build output.\nval\nclassFinder\n=\nnew\nREPLClassDirMonitor\n(<\nABSOLUTE_PATH_TO_BUILD_OUTPUT_DIR\n>)\nspark\n.\nregisterClassFinder\n(\nclassFinder\n)\n// Upload JAR dependencies\nspark\n.\naddArtifact\n(<\nABSOLUTE_PATH_JAR_DEP\n>)\nHere,\nABSOLUTE_PATH_TO_BUILD_OUTPUT_DIR\nis the output directory where the build system writes classfiles into\nand\nABSOLUTE_PATH_JAR_DEP\nis the location of the JAR on the local file system.\nThe\nREPLClassDirMonitor\nis a provided implementation of\nClassFinder\nthat monitors a specific directory but\none may implement their own class extending\nClassFinder\nfor customized search and monitoring.\nFor more information on application development with Spark Connect as well as extending Spark Connect\nwith custom functionality, see\nApplication Development with Spark Connect\n.\nClient application authentication\nWhile Spark Connect does not have built-in authentication, it is designed to\nwork seamlessly with your existing authentication infrastructure. Its gRPC\nHTTP/2 interface allows for the use of authenticating proxies, which makes\nit possible to secure Spark Connect without having to implement authentication\nlogic in Spark directly.\nWhat is supported\nPySpark\n: Since Spark 3.4, Spark Connect supports most PySpark APIs, including\nDataFrame\n,\nFunctions\n, and\nColumn\n. However,\nsome APIs such as\nSparkContext\nand\nRDD\nare not supported.\nYou can check which APIs are currently\nsupported in the\nAPI reference\ndocumentation.\nSupported APIs are labeled “Supports Spark Connect” so you can check whether the\nAPIs you are using are available before migrating existing code to Spark Connect.\nScala\n: Since Spark 3.5, Spark Connect supports most Scala APIs, including\nDataset\n,\nfunctions\n,\nColumn\n,\nCatalog\nand\nKeyValueGroupedDataset\n.\nUser-Defined Functions (UDFs) are supported, by default for the shell and in standalone applications with\nadditional set-up requirements.\nMajority of the Streaming API is supported, including\nDataStreamReader\n,\nDataStreamWriter\n,\nStreamingQuery\nand\nStreamingQueryListener\n.\nAPIs such as\nSparkContext\nand\nRDD\nare unsupported in Spark Connect.\nSupport for more APIs is planned for upcoming Spark releases."}
{"url": "https://spark.apache.org/docs/latest/web-ui.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nWeb UI\nApache Spark provides a suite of web user interfaces (UIs) that you can use\nto monitor the status and resource consumption of your Spark cluster.\nTable of Contents\nJobs Tab\nJobs detail\nStages Tab\nStage detail\nStorage Tab\nEnvironment Tab\nExecutors Tab\nSQL Tab\nSQL metrics\nStructured Streaming Tab\nStreaming (DStreams) Tab\nJDBC/ODBC Server Tab\nJobs Tab\nThe Jobs tab displays a summary page of all jobs in the Spark application and a details page\nfor each job. The summary page shows high-level information, such as the status, duration, and\nprogress of all jobs and the overall event timeline. When you click on a job on the summary\npage, you see the details page for that job. The details page further shows the event timeline,\nDAG visualization, and all stages of the job.\nThe information that is displayed in this section is\nUser: Current Spark user\nStarted At: The startup time of Spark application\nTotal uptime: Time since Spark application started\nScheduling mode: See\njob scheduling\nNumber of jobs per status: Active, Completed, Failed\nEvent timeline: Displays in chronological order the events related to the executors (added, removed) and the jobs\nDetails of jobs grouped by status: Displays detailed information of the jobs including Job ID, description (with a link to detailed job page), submitted time, duration, stages summary and tasks progress bar\nWhen you click on a specific job, you can see the detailed information of this job.\nJobs detail\nThis page displays the details of a specific job identified by its job ID.\nJob Status: (running, succeeded, failed)\nNumber of stages per status (active, pending, completed, skipped, failed)\nAssociated SQL Query: Link to the sql tab for this job\nEvent timeline: Displays in chronological order the events related to the executors (added, removed) and the stages of the job\nDAG visualization: Visual representation of the directed acyclic graph of this job where vertices represent the RDDs or DataFrames and the edges represent an operation to be applied on RDD.\nAn example of DAG visualization for\nsc.parallelize(1 to 100).toDF.count()\nList of stages (grouped by state active, pending, completed, skipped, and failed)\nStage ID\nDescription of the stage\nSubmitted timestamp\nDuration of the stage\nTasks progress bar\nInput: Bytes read from storage in this stage\nOutput: Bytes written in storage in this stage\nShuffle read: Total shuffle bytes and records read, includes both data read locally and data read from remote executors\nShuffle write: Bytes and records written to disk in order to be read by a shuffle in a future stage\nStages Tab\nThe Stages tab displays a summary page that shows the current state of all stages of all jobs in\nthe Spark application.\nAt the beginning of the page is the summary with the count of all stages by status (active, pending, completed, skipped, and failed)\nIn\nFair scheduling mode\nthere is a table that displays\npools properties\nAfter that are the details of stages per status (active, pending, completed, skipped, failed). In active stages, it’s possible to kill the stage with the kill link. Only in failed stages, failure reason is shown. Task detail can be accessed by clicking on the description.\nStage detail\nThe stage detail page begins with information like total time across all tasks,\nLocality level summary\n,\nShuffle Read Size / Records\nand Associated Job IDs.\nThere is also a visual representation of the directed acyclic graph (DAG) of this stage, where vertices represent the RDDs or DataFrames and the edges represent an operation to be applied.\nNodes are grouped by operation scope in the DAG visualization and labelled with the operation scope name (BatchScan, WholeStageCodegen, Exchange, etc).\nNotably, Whole Stage Code Generation operations are also annotated with the code generation id. For stages belonging to Spark DataFrame or SQL execution, this allows to cross-reference Stage execution details to the relevant details in the Web-UI SQL Tab page where SQL plan graphs and execution plans are reported.\nSummary metrics for all task are represented in a table and in a timeline.\nTasks deserialization time\nDuration of tasks\n.\nGC time\nis the total JVM garbage collection time.\nResult serialization time\nis the time spent serializing the task result on an executor before sending it back to the driver.\nGetting result time\nis the time that the driver spends fetching task results from workers.\nScheduler delay\nis the time the task waits to be scheduled for execution.\nPeak execution memory\nis the maximum memory used by the internal data structures created during shuffles, aggregations and joins.\nShuffle Read Size / Records\n. Total shuffle bytes read, includes both data read locally and data read from remote executors.\nShuffle Read Fetch Wait Time\nis the time that tasks spent blocked waiting for shuffle data to be read from remote machines.\nShuffle Remote Reads\nis the total shuffle bytes read from remote executors.\nShuffle Write Time\nis the time that tasks spent writing shuffle data.\nShuffle spill (memory)\nis the size of the deserialized form of the shuffled data in memory.\nShuffle spill (disk)\nis the size of the serialized form of the data on disk.\nAggregated metrics by executor show the same information aggregated by executor.\nAccumulators\nare a type of shared variables. It provides a mutable variable that can be updated inside of a variety of transformations. It is possible to create accumulators with and without name, but only named accumulators are displayed.\nTasks details basically includes the same information as in the summary section but detailed by task. It also includes links to review the logs and the task attempt number if it fails for any reason. If there are named accumulators, here it is possible to see the accumulator value at the end of each task.\nStorage Tab\nThe Storage tab displays the persisted RDDs and DataFrames, if any, in the application. The summary\npage shows the storage levels, sizes and partitions of all RDDs, and the details page shows the\nsizes and using executors for all partitions in an RDD or DataFrame.\nscala\n>\nimport\norg.apache.spark.storage.StorageLevel._\nimport\norg.apache.spark.storage.StorageLevel._\nscala\n>\nval\nrdd\n=\nsc\n.\nrange\n(\n0\n,\n100\n,\n1\n,\n5\n).\nsetName\n(\n\"rdd\"\n)\nrdd\n:\norg.apache.spark.rdd.RDD\n[\nLong\n]\n=\nrdd\nMapPartitionsRDD\n[\n1\n]\nat\nrange\nat\n<\nconsole\n>:\n27\nscala\n>\nrdd\n.\npersist\n(\nMEMORY_ONLY_SER\n)\nres0\n:\nrdd.\ntype\n=\nrdd\nMapPartitionsRDD\n[\n1\n]\nat\nrange\nat\n<\nconsole\n>:\n27\nscala\n>\nrdd\n.\ncount\nres1\n:\nLong\n=\n100\nscala\n>\nval\ndf\n=\nSeq\n((\n1\n,\n\"andy\"\n),\n(\n2\n,\n\"bob\"\n),\n(\n2\n,\n\"andy\"\n)).\ntoDF\n(\n\"count\"\n,\n\"name\"\n)\ndf\n:\norg.apache.spark.sql.DataFrame\n=\n[\ncount:\nint\n,\nname:\nstring\n]\nscala\n>\ndf\n.\npersist\n(\nDISK_ONLY\n)\nres2\n:\ndf.\ntype\n=\n[\ncount:\nint\n,\nname:\nstring\n]\nscala\n>\ndf\n.\ncount\nres3\n:\nLong\n=\n3\nAfter running the above example, we can find two RDDs listed in the Storage tab. Basic information like\nstorage level, number of partitions and memory overhead are provided. Note that the newly persisted RDDs\nor DataFrames are not shown in the tab before they are materialized. To monitor a specific RDD or DataFrame,\nmake sure an action operation has been triggered.\nYou can click the RDD name ‘rdd’ for obtaining the details of data persistence, such as the data\ndistribution on the cluster.\nEnvironment Tab\nThe Environment tab displays the values for the different environment and configuration variables,\nincluding JVM, Spark, and system properties.\nThis environment page has five parts. It is a useful place to check whether your properties have\nbeen set correctly.\nThe first part ‘Runtime Information’ simply contains the\nruntime properties\nlike versions of Java and Scala.\nThe second part ‘Spark Properties’ lists the\napplication properties\nlike\n‘spark.app.name’\nand ‘spark.driver.memory’.\nClicking the ‘Hadoop Properties’ link displays properties relative to Hadoop and YARN. Note that properties like\n‘spark.hadoop.*’\nare shown not in this part but in ‘Spark Properties’.\n‘System Properties’ shows more details about the JVM.\nThe last part ‘Classpath Entries’ lists the classes loaded from different sources, which is very useful\nto resolve class conflicts.\nExecutors Tab\nThe Executors tab displays summary information about the executors that were created for the\napplication, including memory and disk usage and task and shuffle information. The Storage Memory\ncolumn shows the amount of memory used and reserved for caching data.\nThe Executors tab provides not only resource information (amount of memory, disk, and cores used by each executor)\nbut also performance information (\nGC time\nand shuffle information).\nClicking the ‘stderr’ link of executor 0 displays detailed\nstandard error log\nin its console.\nClicking the ‘Thread Dump’ link of executor 0 displays the thread dump of JVM on executor 0, which is pretty useful\nfor performance analysis.\nSQL Tab\nIf the application executes Spark SQL queries, the SQL tab displays information, such as the duration,\njobs, and physical and logical plans for the queries. Here we include a basic example to illustrate\nthis tab:\nscala\n>\nval\ndf\n=\nSeq\n((\n1\n,\n\"andy\"\n),\n(\n2\n,\n\"bob\"\n),\n(\n2\n,\n\"andy\"\n)).\ntoDF\n(\n\"count\"\n,\n\"name\"\n)\ndf\n:\norg.apache.spark.sql.DataFrame\n=\n[\ncount:\nint\n,\nname:\nstring\n]\nscala\n>\ndf\n.\ncount\nres0\n:\nLong\n=\n3\nscala\n>\ndf\n.\ncreateGlobalTempView\n(\n\"df\"\n)\nscala\n>\nspark\n.\nsql\n(\n\"select name,sum(count) from global_temp.df group by name\"\n).\nshow\n+----+----------+\n|\nname\n|\nsum\n(\ncount\n)|\n+----+----------+\n|\nandy\n|\n3\n|\n|\nbob\n|\n2\n|\n+----+----------+\nNow the above three dataframe/SQL operators are shown in the list. If we click the\n‘show at <console>: 24’ link of the last query, we will see the DAG and details of the query execution.\nThe query details page displays information about the query execution time, its duration,\nthe list of associated jobs, and the query execution DAG.\nThe first block ‘WholeStageCodegen (1)’ compiles multiple operators (‘LocalTableScan’ and ‘HashAggregate’) together into a single Java\nfunction to improve performance, and metrics like number of rows and spill size are listed in the block.\nThe annotation ‘(1)’ in the block name is the code generation id.\nThe second block ‘Exchange’ shows the metrics on the shuffle exchange, including\nnumber of written shuffle records, total data size, etc.\nClicking the ‘Details’ link on the bottom displays the logical plans and the physical plan, which\nillustrate how Spark parses, analyzes, optimizes and performs the query.\nSteps in the physical plan subject to whole stage code generation optimization, are prefixed by a star followed by\nthe code generation id, for example: ‘*(1) LocalTableScan’\nSQL metrics\nThe metrics of SQL operators are shown in the block of physical operators. The SQL metrics can be useful\nwhen we want to dive into the execution details of each operator. For example, “number of output rows”\ncan answer how many rows are output after a Filter operator, “shuffle bytes written total” in an Exchange\noperator shows the number of bytes written by a shuffle.\nHere is the list of SQL metrics:\nSQL metrics\nMeaning\nOperators\nnumber of output rows\nthe number of output rows of the operator\nAggregate operators, Join operators, Sample, Range, Scan operators, Filter, etc.\ndata size\nthe size of broadcast/shuffled/collected data of the operator\nBroadcastExchange, ShuffleExchange, Subquery\ntime to collect\nthe time spent on collecting data\nBroadcastExchange, Subquery\nscan time\nthe time spent on scanning data\nColumnarBatchScan, FileSourceScan\nmetadata time\nthe time spent on getting metadata like number of partitions, number of files\nFileSourceScan\nshuffle bytes written\nthe number of bytes written\nCollectLimit, TakeOrderedAndProject, ShuffleExchange\nshuffle records written\nthe number of records written\nCollectLimit, TakeOrderedAndProject, ShuffleExchange\nshuffle write time\nthe time spent on shuffle writing\nCollectLimit, TakeOrderedAndProject, ShuffleExchange\nremote blocks read\nthe number of blocks read remotely\nCollectLimit, TakeOrderedAndProject, ShuffleExchange\nremote bytes read\nthe number of bytes read remotely\nCollectLimit, TakeOrderedAndProject, ShuffleExchange\nremote bytes read to disk\nthe number of bytes read from remote to local disk\nCollectLimit, TakeOrderedAndProject, ShuffleExchange\nlocal blocks read\nthe number of blocks read locally\nCollectLimit, TakeOrderedAndProject, ShuffleExchange\nlocal bytes read\nthe number of bytes read locally\nCollectLimit, TakeOrderedAndProject, ShuffleExchange\nfetch wait time\nthe time spent on fetching data (local and remote)\nCollectLimit, TakeOrderedAndProject, ShuffleExchange\nrecords read\nthe number of read records\nCollectLimit, TakeOrderedAndProject, ShuffleExchange\nsort time\nthe time spent on sorting\nSort\npeak memory\nthe peak memory usage in the operator\nSort, HashAggregate\nspill size\nnumber of bytes spilled to disk from memory in the operator\nSort, HashAggregate\ntime in aggregation build\nthe time spent on aggregation\nHashAggregate, ObjectHashAggregate\navg hash probe bucket list iters\nthe average bucket list iterations per lookup during aggregation\nHashAggregate\ndata size of build side\nthe size of built hash map\nShuffledHashJoin\ntime to build hash map\nthe time spent on building hash map\nShuffledHashJoin\ntask commit time\nthe time spent on committing the output of a task after the writes succeed\nany write operation on a file-based table\njob commit time\nthe time spent on committing the output of a job after the writes succeed\nany write operation on a file-based table\ndata sent to Python workers\nthe number of bytes of serialized data sent to the Python workers\nPython UDFs, Pandas UDFs, Pandas Functions API and Python Data Source\ndata returned from Python workers\nthe number of bytes of serialized data received back from the Python workers\nPython UDFs, Pandas UDFS, Pandas Functions API and Python Data Source\nStructured Streaming Tab\nWhen running Structured Streaming jobs in micro-batch mode, a Structured Streaming tab will be\navailable on the Web UI. The overview page displays some brief statistics for running and completed\nqueries. Also, you can check the latest exception of a failed query. For detailed statistics, please\nclick a “run id” in the tables.\nThe statistics page displays some useful metrics for insight into the status of your streaming\nqueries. Currently, it contains the following metrics.\nInput Rate.\nThe aggregate (across all sources) rate of data arriving.\nProcess Rate.\nThe aggregate (across all sources) rate at which Spark is processing data.\nInput Rows.\nThe aggregate (across all sources) number of records processed in a trigger.\nBatch Duration.\nThe process duration of each batch.\nOperation Duration.\nThe amount of time taken to perform various operations in milliseconds.\nThe tracked operations are listed as follows.\naddBatch: Time taken to read the micro-batch’s input data from the sources, process it, and write the batch’s output to the sink. This should take the bulk of the micro-batch’s time.\ngetBatch: Time taken to prepare the logical query to read the input of the current micro-batch from the sources.\nlatestOffset & getOffset: Time taken to query the maximum available offset for this source.\nqueryPlanning: Time taken to generates the execution plan.\nwalCommit: Time taken to write the offsets to the metadata log.\nGlobal Watermark Gap.\nThe gap between batch timestamp and global watermark for the batch.\nAggregated Number Of Total State Rows.\nThe aggregated number of total state rows.\nAggregated Number Of Updated State Rows.\nThe aggregated number of updated state rows.\nAggregated State Memory Used In Bytes.\nThe aggregated state memory used in bytes.\nAggregated Number Of State Rows Dropped By Watermark.\nThe aggregated number of state rows dropped by watermark.\nAs an early-release version, the statistics page is still under development and will be improved in\nfuture releases.\nStreaming (DStreams) Tab\nThe web UI includes a Streaming tab if the application uses Spark Streaming with DStream API.\nThis tab displays scheduling delay and processing time for each micro-batch in the data stream,\nwhich can be useful for troubleshooting the streaming application.\nJDBC/ODBC Server Tab\nWe can see this tab when Spark is running as a\ndistributed SQL engine\n. It shows information about sessions and submitted SQL operations.\nThe first section of the page displays general information about the JDBC/ODBC server: start time and uptime.\nThe second section contains information about active and finished sessions.\nUser\nand\nIP\nof the connection.\nSession id\nlink to access to session info.\nStart time\n,\nfinish time\nand\nduration\nof the session.\nTotal execute\nis the number of operations submitted in this session.\nThe third section has the SQL statistics of the submitted operations.\nUser\nthat submit the operation.\nJob id\nlink to\njobs tab\n.\nGroup id\nof the query that group all jobs together. An application can cancel all running jobs using this group id.\nStart time\nof the operation.\nFinish time\nof the execution, before fetching the results.\nClose time\nof the operation after fetching the results.\nExecution time\nis the difference between finish time and start time.\nDuration time\nis the difference between close time and start time.\nStatement\nis the operation being executed.\nState\nof the process.\nStarted\n, first state, when the process begins.\nCompiled\n, execution plan generated.\nFailed\n, final state when the execution failed or finished with error.\nCanceled\n, final state when the execution is canceled.\nFinished\nprocessing and waiting to fetch results.\nClosed\n, final state when client closed the statement.\nDetail\nof the execution plan with parsed logical plan, analyzed logical plan, optimized logical plan and physical plan or errors in the SQL statement."}
{"url": "https://spark.apache.org/docs/latest/job-scheduling.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nJob Scheduling\nOverview\nScheduling Across Applications\nDynamic Resource Allocation\nConfiguration and Setup\nCaveats\nResource Allocation Policy\nRequest Policy\nRemove Policy\nGraceful Decommission of Executors\nScheduling Within an Application\nFair Scheduler Pools\nDefault Behavior of Pools\nConfiguring Pool Properties\nScheduling using JDBC Connections\nConcurrent Jobs in PySpark\nOverview\nSpark has several facilities for scheduling resources between computations. First, recall that, as described\nin the\ncluster mode overview\n, each Spark application (instance of SparkContext)\nruns an independent set of executor processes. The cluster managers that Spark runs on provide\nfacilities for\nscheduling across applications\n. Second,\nwithin\neach Spark application, multiple “jobs” (Spark actions) may be running concurrently\nif they were submitted by different threads. This is common if your application is serving requests\nover the network. Spark includes a\nfair scheduler\nto schedule resources within each SparkContext.\nScheduling Across Applications\nWhen running on a cluster, each Spark application gets an independent set of executor JVMs that only\nrun tasks and store data for that application. If multiple users need to share your cluster, there are\ndifferent options to manage allocation, depending on the cluster manager.\nThe simplest option, available on all cluster managers, is\nstatic partitioning\nof resources. With\nthis approach, each application is given a maximum amount of resources it can use and holds onto them\nfor its whole duration. This is the approach used in Spark’s\nstandalone\nand\nYARN\nmodes, as well as the\nK8s\nmode.\nResource allocation can be configured as follows, based on the cluster type:\nStandalone mode:\nBy default, applications submitted to the standalone mode cluster will run in\nFIFO (first-in-first-out) order, and each application will try to use all available nodes. You can limit\nthe number of nodes an application uses by setting the\nspark.cores.max\nconfiguration property in it,\nor change the default for applications that don’t set this setting through\nspark.deploy.defaultCores\n.\nFinally, in addition to controlling cores, each application’s\nspark.executor.memory\nsetting controls\nits memory use.\nYARN:\nThe\n--num-executors\noption to the Spark YARN client controls how many executors it will allocate\non the cluster (\nspark.executor.instances\nas configuration property), while\n--executor-memory\n(\nspark.executor.memory\nconfiguration property) and\n--executor-cores\n(\nspark.executor.cores\nconfiguration\nproperty) control the resources per executor. For more information, see the\nYARN Spark Properties\n.\nK8s:\nThe same as the situation with Yarn, please refer to the description of Yarn above. Furthermore,\nSpark on K8s offers higher priority versions of\nspark.kubernetes.executor.limit.cores\nand\nspark.kubernetes.executor.request.cores\nthan\nspark.executor.cores\n. For more information, see the\nK8s Spark Properties\n.\nNote that none of the modes currently provide memory sharing across applications. If you would like to share\ndata this way, we recommend running a single server application that can serve multiple requests by querying\nthe same RDDs.\nDynamic Resource Allocation\nSpark provides a mechanism to dynamically adjust the resources your application occupies based\non the workload. This means that your application may give resources back to the cluster if they\nare no longer used and request them again later when there is demand. This feature is particularly\nuseful if multiple applications share resources in your Spark cluster.\nThis feature is disabled by default and available on all coarse-grained cluster managers, i.e.\nstandalone mode\n,\nYARN mode\nand\nK8s mode\n.\nConfiguration and Setup\nThere are several ways for using this feature.\nRegardless of which approach you choose, your application must set\nspark.dynamicAllocation.enabled\nto\ntrue\nfirst, additionally,\nyour application must set\nspark.shuffle.service.enabled\nto\ntrue\nafter you set up an\nexternal shuffle service\non each worker node in the same cluster, or\nyour application must set\nspark.dynamicAllocation.shuffleTracking.enabled\nto\ntrue\n, or\nyour application must set both\nspark.decommission.enabled\nand\nspark.storage.decommission.shuffleBlocks.enabled\nto\ntrue\n, or\nyour application must configure\nspark.shuffle.sort.io.plugin.class\nto use a custom\nShuffleDataIO\nwho’s\nShuffleDriverComponents\nsupports reliable storage.\nThe purpose of the external shuffle service or the shuffle tracking or the\nShuffleDriverComponents\nsupports reliable storage is to allow executors to be removed\nwithout deleting shuffle files written by them (more detail described\nbelow\n). While it is simple to enable shuffle tracking, the way to set up the external shuffle service varies across cluster managers:\nIn standalone mode, simply start your workers with\nspark.shuffle.service.enabled\nset to\ntrue\n.\nIn YARN mode, follow the instructions\nhere\n.\nAll other relevant configurations are optional and under the\nspark.dynamicAllocation.*\nand\nspark.shuffle.service.*\nnamespaces. For more detail, see the\nconfigurations page\n.\nCaveats\nIn\nstandalone mode\n, without explicitly setting\nspark.executor.cores\n, each executor will get all the available cores of a worker. In this case, when dynamic allocation enabled, spark will possibly acquire much more executors than expected. When you want to use dynamic allocation in\nstandalone mode\n, you are recommended to explicitly set cores for each executor before the issue\nSPARK-30299\ngot fixed.\nIn\nK8s mode\n, we can not use this feature by setting\nspark.shuffle.service.enabled\nto\ntrue\ndue to Spark on K8s doesn’t yet support the external shuffle service.\nResource Allocation Policy\nAt a high level, Spark should relinquish executors when they are no longer used and acquire\nexecutors when they are needed. Since there is no definitive way to predict whether an executor\nthat is about to be removed will run a task in the near future, or whether a new executor that is\nabout to be added will actually be idle, we need a set of heuristics to determine when to remove\nand request executors.\nRequest Policy\nA Spark application with dynamic allocation enabled requests additional executors when it has\npending tasks waiting to be scheduled. This condition necessarily implies that the existing set\nof executors is insufficient to simultaneously saturate all tasks that have been submitted but\nnot yet finished.\nSpark requests executors in rounds. The actual request is triggered when there have been pending\ntasks for\nspark.dynamicAllocation.schedulerBacklogTimeout\nseconds, and then triggered again\nevery\nspark.dynamicAllocation.sustainedSchedulerBacklogTimeout\nseconds thereafter if the queue\nof pending tasks persists. Additionally, the number of executors requested in each round increases\nexponentially from the previous round. For instance, an application will add 1 executor in the\nfirst round, and then 2, 4, 8 and so on executors in the subsequent rounds.\nThe motivation for an exponential increase policy is twofold. First, an application should request\nexecutors cautiously in the beginning in case it turns out that only a few additional executors is\nsufficient. This echoes the justification for TCP slow start. Second, the application should be\nable to ramp up its resource usage in a timely manner in case it turns out that many executors are\nactually needed.\nRemove Policy\nThe policy for removing executors is much simpler. A Spark application removes an executor when\nit has been idle for more than\nspark.dynamicAllocation.executorIdleTimeout\nseconds. Note that,\nunder most circumstances, this condition is mutually exclusive with the request condition, in that\nan executor should not be idle if there are still pending tasks to be scheduled.\nGraceful Decommission of Executors\nBefore dynamic allocation, if a Spark executor exits when the associated application has also exited \nthen all state associated with the executor is no longer needed and can be safely discarded. \nWith dynamic allocation, however, the application is still running when an executor is explicitly \nremoved. If the application attempts to access state stored in or written by the executor, it will \nhave to perform a recompute the state. Thus, Spark needs a mechanism to decommission an executor \ngracefully by preserving its state before removing it.\nThis requirement is especially important for shuffles. During a shuffle, the Spark executor first\nwrites its own map outputs locally to disk, and then acts as the server for those files when other\nexecutors attempt to fetch them. In the event of stragglers, which are tasks that run for much\nlonger than their peers, dynamic allocation may remove an executor before the shuffle completes,\nin which case the shuffle files written by that executor must be recomputed unnecessarily.\nThe solution for preserving shuffle files is to use an external shuffle service, also introduced\nin Spark 1.2. This service refers to a long-running process that runs on each node of your cluster\nindependently of your Spark applications and their executors. If the service is enabled, Spark\nexecutors will fetch shuffle files from the service instead of from each other. This means any\nshuffle state written by an executor may continue to be served beyond the executor’s lifetime.\nIn addition to writing shuffle files, executors also cache data either on disk or in memory.\nWhen an executor is removed, however, all cached data will no longer be accessible.  To mitigate this,\nby default executors containing cached data are never removed.  You can configure this behavior with\nspark.dynamicAllocation.cachedExecutorIdleTimeout\n. When set\nspark.shuffle.service.fetch.rdd.enabled\nto\ntrue\n, Spark can use ExternalShuffleService for fetching disk persisted RDD blocks. In case of \ndynamic allocation if this feature is enabled executors having only disk persisted blocks are considered\nidle after\nspark.dynamicAllocation.executorIdleTimeout\nand will be released accordingly. In future releases,\nthe cached data may be preserved through an off-heap storage similar in spirit to how shuffle files are preserved \nthrough the external shuffle service.\nScheduling Within an Application\nInside a given Spark application (SparkContext instance), multiple parallel jobs can run simultaneously if\nthey were submitted from separate threads. By “job”, in this section, we mean a Spark action (e.g.\nsave\n,\ncollect\n) and any tasks that need to run to evaluate that action. Spark’s scheduler is fully thread-safe\nand supports this use case to enable applications that serve multiple requests (e.g. queries for\nmultiple users).\nBy default, Spark’s scheduler runs jobs in FIFO fashion. Each job is divided into “stages” (e.g. map and\nreduce phases), and the first job gets priority on all available resources while its stages have tasks to\nlaunch, then the second job gets priority, etc. If the jobs at the head of the queue don’t need to use\nthe whole cluster, later jobs can start to run right away, but if the jobs at the head of the queue are\nlarge, then later jobs may be delayed significantly.\nStarting in Spark 0.8, it is also possible to configure fair sharing between jobs. Under fair sharing,\nSpark assigns tasks between jobs in a “round robin” fashion, so that all jobs get a roughly equal share\nof cluster resources. This means that short jobs submitted while a long job is running can start receiving\nresources right away and still get good response times, without waiting for the long job to finish. This\nmode is best for multi-user settings.\nThis feature is disabled by default and available on all coarse-grained cluster managers, i.e.\nstandalone mode\n,\nYARN mode\n,\nK8s mode\n.\nTo enable the fair scheduler, simply set the\nspark.scheduler.mode\nproperty to\nFAIR\nwhen configuring\na SparkContext:\nval\nconf\n=\nnew\nSparkConf\n().\nsetMaster\n(...).\nsetAppName\n(...)\nconf\n.\nset\n(\n\"spark.scheduler.mode\"\n,\n\"FAIR\"\n)\nval\nsc\n=\nnew\nSparkContext\n(\nconf\n)\nFair Scheduler Pools\nThe fair scheduler also supports grouping jobs into\npools\n, and setting different scheduling options\n(e.g. weight) for each pool. This can be useful to create a “high-priority” pool for more important jobs,\nfor example, or to group the jobs of each user together and give\nusers\nequal shares regardless of how\nmany concurrent jobs they have instead of giving\njobs\nequal shares. This approach is modeled after the\nHadoop Fair Scheduler\n.\nWithout any intervention, newly submitted jobs go into a\ndefault pool\n, but jobs’ pools can be set by\nadding the\nspark.scheduler.pool\n“local property” to the SparkContext in the thread that’s submitting them.\nThis is done as follows:\n// Assuming sc is your SparkContext variable\nsc\n.\nsetLocalProperty\n(\n\"spark.scheduler.pool\"\n,\n\"pool1\"\n)\nAfter setting this local property,\nall\njobs submitted within this thread (by calls in this thread\nto\nRDD.save\n,\ncount\n,\ncollect\n, etc) will use this pool name. The setting is per-thread to make\nit easy to have a thread run multiple jobs on behalf of the same user. If you’d like to clear the\npool that a thread is associated with, simply call:\nsc\n.\nsetLocalProperty\n(\n\"spark.scheduler.pool\"\n,\nnull\n)\nDefault Behavior of Pools\nBy default, each pool gets an equal share of the cluster (also equal in share to each job in the default\npool), but inside each pool, jobs run in FIFO order. For example, if you create one pool per user, this\nmeans that each user will get an equal share of the cluster, and that each user’s queries will run in\norder instead of later queries taking resources from that user’s earlier ones.\nConfiguring Pool Properties\nSpecific pools’ properties can also be modified through a configuration file. Each pool supports three\nproperties:\nschedulingMode\n: This can be FIFO or FAIR, to control whether jobs within the pool queue up behind\neach other (the default) or share the pool’s resources fairly.\nweight\n: This controls the pool’s share of the cluster relative to other pools. By default, all pools\nhave a weight of 1. If you give a specific pool a weight of 2, for example, it will get 2x more\nresources as other active pools. Setting a high weight such as 1000 also makes it possible to implement\npriority\nbetween pools—in essence, the weight-1000 pool will always get to launch tasks first\nwhenever it has jobs active.\nminShare\n: Apart from an overall weight, each pool can be given a\nminimum shares\n(as a number of\nCPU cores) that the administrator would like it to have. The fair scheduler always attempts to meet\nall active pools’ minimum shares before redistributing extra resources according to the weights.\nThe\nminShare\nproperty can, therefore, be another way to ensure that a pool can always get up to a\ncertain number of resources (e.g. 10 cores) quickly without giving it a high priority for the rest\nof the cluster. By default, each pool’s\nminShare\nis 0.\nThe pool properties can be set by creating an XML file, similar to\nconf/fairscheduler.xml.template\n,\nand either putting a file named\nfairscheduler.xml\non the classpath, or setting\nspark.scheduler.allocation.file\nproperty in your\nSparkConf\n. The file path respects the hadoop configuration and can either be a local file path or HDFS file path.\n// scheduler file at local\nconf\n.\nset\n(\n\"spark.scheduler.allocation.file\"\n,\n\"file:///path/to/file\"\n)\n// scheduler file at hdfs\nconf\n.\nset\n(\n\"spark.scheduler.allocation.file\"\n,\n\"hdfs:///path/to/file\"\n)\nThe format of the XML file is simply a\n<pool>\nelement for each pool, with different elements\nwithin it for the various settings. For example:\n<?xml version=\"1.0\"?>\n<allocations>\n<pool\nname=\n\"production\"\n>\n<schedulingMode>\nFAIR\n</schedulingMode>\n<weight>\n1\n</weight>\n<minShare>\n2\n</minShare>\n</pool>\n<pool\nname=\n\"test\"\n>\n<schedulingMode>\nFIFO\n</schedulingMode>\n<weight>\n2\n</weight>\n<minShare>\n3\n</minShare>\n</pool>\n</allocations>\nA full example is also available in\nconf/fairscheduler.xml.template\n. Note that any pools not\nconfigured in the XML file will simply get default values for all settings (scheduling mode FIFO,\nweight 1, and minShare 0).\nScheduling using JDBC Connections\nTo set a\nFair Scheduler\npool for a JDBC client session,\nusers can set the\nspark.sql.thriftserver.scheduler.pool\nvariable:\nSET\nspark\n.\nsql\n.\nthriftserver\n.\nscheduler\n.\npool\n=\naccounting\n;\nConcurrent Jobs in PySpark\nPySpark, by default, does not support to synchronize PVM threads with JVM threads and \nlaunching multiple jobs in multiple PVM threads does not guarantee to launch each job\nin each corresponding JVM thread. Due to this limitation, it is unable to set a different job group\nvia\nsc.setJobGroup\nin a separate PVM thread, which also disallows to cancel the job via\nsc.cancelJobGroup\nlater.\npyspark.InheritableThread\nis recommended to use together for a PVM thread to inherit the inheritable attributes\n such as local properties in a JVM thread."}
{"url": "https://spark.apache.org/docs/latest/hadoop-provided.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nUsing Spark's \"Hadoop Free\" Build\nSpark uses Hadoop client libraries for HDFS and YARN. Starting in version Spark 1.4, the project packages “Hadoop free” builds that lets you more easily connect a single Spark binary to any Hadoop version. To use these builds, you need to modify\nSPARK_DIST_CLASSPATH\nto include Hadoop’s package jars. The most convenient place to do this is by adding an entry in\nconf/spark-env.sh\n.\nThis page describes how to connect Spark to Hadoop for different types of distributions.\nApache Hadoop\nFor Apache distributions, you can use Hadoop’s ‘classpath’ command. For instance:\n### in conf/spark-env.sh ###\n# If 'hadoop' binary is on your PATH\nexport\nSPARK_DIST_CLASSPATH\n=\n$(\nhadoop classpath\n)\n# With explicit path to 'hadoop' binary\nexport\nSPARK_DIST_CLASSPATH\n=\n$(\n/path/to/hadoop/bin/hadoop classpath\n)\n# Passing a Hadoop configuration directory\nexport\nSPARK_DIST_CLASSPATH\n=\n$(\nhadoop\n--config\n/path/to/configs classpath\n)\nHadoop Free Build Setup for Spark on Kubernetes\nTo run the Hadoop free build of Spark on Kubernetes, the executor image must have the appropriate version of Hadoop binaries and the correct\nSPARK_DIST_CLASSPATH\nvalue set. See the example below for the relevant changes needed in the executor Dockerfile:\n### Set environment variables in the executor dockerfile ###\nENV\nSPARK_HOME\n=\n\"/opt/spark\"\nENV\nHADOOP_HOME\n=\n\"/opt/hadoop\"\nENV\nPATH\n=\n\"\n$SPARK_HOME\n/bin:\n$HADOOP_HOME\n/bin:\n$PATH\n\"\n...\n#Copy your target hadoop binaries to the executor hadoop home\nCOPY /opt/hadoop3\n$HADOOP_HOME\n...\n#Copy and use the Spark provided entrypoint.sh. It sets your SPARK_DIST_CLASSPATH using the hadoop binary in $HADOOP_HOME and starts the executor. If you choose to customize the value of SPARK_DIST_CLASSPATH here, the value will be retained in entrypoint.sh\nENTRYPOINT\n[\n\"/opt/entrypoint.sh\"\n]\n..."}
{"url": "https://spark.apache.org/docs/latest/building-spark.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nBuilding Spark\nBuilding Apache Spark\nApache Maven\nSetting up Maven’s Memory Usage\nbuild/mvn\nBuilding a Runnable Distribution\nSpecifying the Hadoop Version and Enabling YARN\nBuilding With Hive and JDBC Support\nPackaging without Hadoop Dependencies for YARN\nBuilding with Kubernetes support\nBuilding submodules individually\nBuilding with JVM Profile support\nContinuous Compilation\nBuilding with SBT\nSetting up SBT’s Memory Usage\nSpeeding up Compilation\nEncrypted Filesystems\nIntelliJ IDEA or Eclipse\nRunning Tests\nTesting with SBT\nRunning Individual Tests\nPySpark pip installable\nPySpark Tests with Maven or SBT\nRunning R Tests (deprecated)\nRunning Docker-based Integration Test Suites\nBuilding and testing on an IPv6-only environment\nBuilding with a user-defined\nprotoc\nBuilding Apache Spark\nApache Maven\nThe Maven-based build is the build of reference for Apache Spark.\nBuilding Spark using Maven requires Maven 3.9.9 and Java 17/21.\nSpark requires Scala 2.13; support for Scala 2.12 was removed in Spark 4.0.0.\nSetting up Maven’s Memory Usage\nYou’ll need to configure Maven to use more memory than usual by setting\nMAVEN_OPTS\n:\nexport\nMAVEN_OPTS\n=\n\"-Xss64m -Xmx2g -XX:ReservedCodeCacheSize=1g\"\n(The\nReservedCodeCacheSize\nsetting is optional but recommended.)\nIf you don’t add these parameters to\nMAVEN_OPTS\n, you may see errors and warnings like the following:\n[INFO] Compiling 203 Scala sources and 9 Java sources to /Users/me/Development/spark/core/target/scala-2.13/classes...\n[ERROR] Java heap space -> [Help 1]\nYou can fix these problems by setting the\nMAVEN_OPTS\nvariable as discussed before.\nNote:\nIf using\nbuild/mvn\nwith no\nMAVEN_OPTS\nset, the script will automatically add the above options to the\nMAVEN_OPTS\nenvironment variable.\nThe\ntest\nphase of the Spark build will automatically add these options to\nMAVEN_OPTS\n, even when not using\nbuild/mvn\n.\nbuild/mvn\nSpark now comes packaged with a self-contained Maven installation to ease building and deployment of Spark from source located under the\nbuild/\ndirectory. This script will automatically download and setup all necessary build requirements (\nMaven\n,\nScala\n) locally within the\nbuild/\ndirectory itself. It honors any\nmvn\nbinary if present already, however, will pull down its own copy of Scala regardless to ensure proper version requirements are met.\nbuild/mvn\nexecution acts as a pass through to the\nmvn\ncall allowing easy transition from previous build methods. As an example, one can build a version of Spark as follows:\n./build/mvn -DskipTests clean package\nOther build examples can be found below.\nBuilding a Runnable Distribution\nTo create a Spark distribution like those distributed by the\nSpark Downloads\npage, and that is laid out so as\nto be runnable, use\n./dev/make-distribution.sh\nin the project root directory. It can be configured\nwith Maven profile settings and so on like the direct Maven build. Example:\n./dev/make-distribution.sh --name custom-spark --pip --r --tgz -Psparkr -Phive -Phive-thriftserver -Pyarn -Pkubernetes\nThis will build Spark distribution along with Python pip and R packages. For more information on usage, run\n./dev/make-distribution.sh --help\nSpecifying the Hadoop Version and Enabling YARN\nYou can enable the\nyarn\nprofile and specify the exact version of Hadoop to compile against through the\nhadoop.version\nproperty.\nExample:\n./build/mvn -Pyarn -Dhadoop.version=3.4.1 -DskipTests clean package\nBuilding With Hive and JDBC Support\nTo enable Hive integration for Spark SQL along with its JDBC server and CLI,\nadd the\n-Phive\nand\n-Phive-thriftserver\nprofiles to your existing build options.\nBy default Spark will build with Hive 2.3.10.\n# With Hive 2.3.10 support\n./build/mvn -Pyarn -Phive -Phive-thriftserver -DskipTests clean package\nPackaging without Hadoop Dependencies for YARN\nThe assembly directory produced by\nmvn package\nwill, by default, include all of Spark’s\ndependencies, including Hadoop and some of its ecosystem projects. On YARN deployments, this\ncauses multiple versions of these to appear on executor classpaths: the version packaged in\nthe Spark assembly and the version on each node, included with\nyarn.application.classpath\n.\nThe\nhadoop-provided\nprofile builds the assembly without including Hadoop-ecosystem projects,\nlike ZooKeeper and Hadoop itself.\nBuilding with Kubernetes support\n./build/mvn -Pkubernetes -DskipTests clean package\nBuilding submodules individually\nIt’s possible to build Spark submodules using the\nmvn -pl\noption.\nFor instance, you can build the Spark Streaming module using:\n./build/mvn -pl :spark-streaming_2.13 clean install\nwhere\nspark-streaming_2.13\nis the\nartifactId\nas defined in\nstreaming/pom.xml\nfile.\nBuilding with JVM Profile support\n./build/mvn -Pjvm-profiler -DskipTests clean package\nNote:\nThe\njvm-profiler\nprofile builds the assembly without including the dependency\nap-loader\n,\nyou can download it manually from maven central repo and use it together with\nspark-profiler_2.13\n.\nContinuous Compilation\nWe use the scala-maven-plugin which supports incremental and continuous compilation. E.g.\n./build/mvn scala:cc\nshould run continuous compilation (i.e. wait for changes). However, this has not been tested\nextensively. A couple of gotchas to note:\nit only scans the paths\nsrc/main\nand\nsrc/test\n(see\ndocs\n), so it will only work\nfrom within certain submodules that have that structure.\nyou’ll typically need to run\nmvn install\nfrom the project root for compilation within\nspecific submodules to work; this is because submodules that depend on other submodules do so via\nthe\nspark-parent\nmodule).\nThus, the full flow for running continuous-compilation of the\ncore\nsubmodule may look more like:\n$ ./build/mvn install\n$ cd core\n$ ../build/mvn scala:cc\nBuilding with SBT\nMaven is the official build tool recommended for packaging Spark, and is the\nbuild of reference\n.\nBut SBT is supported for day-to-day development since it can provide much faster iterative\ncompilation. More advanced developers may wish to use SBT.\nThe SBT build is derived from the Maven POM files, and so the same Maven profiles and variables\ncan be set to control the SBT build. For example:\n./build/sbt package\nTo avoid the overhead of launching sbt each time you need to re-compile, you can launch sbt\nin interactive mode by running\nbuild/sbt\n, and then run all build commands at the command\nprompt.\nSetting up SBT’s Memory Usage\nConfigure the JVM options for SBT in\n.jvmopts\nat the project root, for example:\n-Xmx2g\n-XX:ReservedCodeCacheSize=1g\nFor the meanings of these two options, please carefully read the\nSetting up Maven’s Memory Usage section\n.\nSpeeding up Compilation\nDevelopers who compile Spark frequently may want to speed up compilation; e.g., by avoiding re-compilation of the\nassembly JAR (for developers who build with SBT).  For more information about how to do this, refer to the\nUseful Developer Tools page\n.\nEncrypted Filesystems\nWhen building on an encrypted filesystem (if your home directory is encrypted, for example), then the Spark build might fail with a “Filename too long” error. As a workaround, add the following in the configuration args of the\nscala-maven-plugin\nin the project\npom.xml\n:\n<arg>\n-Xmax-classfile-name\n</arg>\n<arg>\n128\n</arg>\nand in\nproject/SparkBuild.scala\nadd:\nscalacOptions\nin\nCompile\n++=\nSeq\n(\n\"-Xmax-classfile-name\"\n,\n\"128\"\n),\nto the\nsharedSettings\nval. See also\nthis PR\nif you are unsure of where to add these lines.\nIntelliJ IDEA or Eclipse\nFor help in setting up IntelliJ IDEA or Eclipse for Spark development, and troubleshooting, refer to the\nUseful Developer Tools page\n.\nRunning Tests\nTests are run by default via the\nScalaTest Maven plugin\n.\nNote that tests should not be run as root or an admin user.\nThe following is an example of a command to run the tests:\n./build/mvn test\nTesting with SBT\nThe following is an example of a command to run the tests:\n./build/sbt test\nRunning Individual Tests\nFor information about how to run individual tests, refer to the\nUseful Developer Tools page\n.\nPySpark pip installable\nIf you are building Spark for use in a Python environment and you wish to pip install it, you will first need to build the Spark JARs as described above. Then you can construct an sdist package suitable for setup.py and pip installable package.\ncd python; python packaging/classic/setup.py sdist\nNote:\nDue to packaging requirements you can not directly pip install from the Python directory, rather you must first build the sdist package as described above.\nAlternatively, you can also run\nmake-distribution.sh\nwith the\n--pip\noption.\nPySpark Tests with Maven or SBT\nIf you are building PySpark and wish to run the PySpark tests you will need to build Spark with Hive support.\n./build/mvn -DskipTests clean package -Phive\n./python/run-tests\nIf you are building PySpark with SBT and wish to run the PySpark tests, you will need to build Spark with Hive support and also build the test components:\n./build/sbt -Phive clean package\n./build/sbt test:compile\n./python/run-tests\nThe run-tests script also can be limited to a specific Python version or a specific module\n./python/run-tests --python-executables=python --modules=pyspark-sql\nRunning R Tests (deprecated)\nTo run the SparkR tests you will need to install the\nknitr\n,\nrmarkdown\n,\ntestthat\n,\ne1071\nand\nsurvival\npackages first:\nRscript -e \"install.packages(c('knitr', 'rmarkdown', 'devtools', 'testthat', 'e1071', 'survival'), repos='https://cloud.r-project.org/')\"\nYou can run just the SparkR tests using the command:\n./R/run-tests.sh\nRunning Docker-based Integration Test Suites\nIn order to run Docker integration tests, you have to install the\ndocker\nengine on your box.\nThe instructions for installation can be found at\nthe Docker site\n.\nOnce installed, the\ndocker\nservice needs to be started, if not already running.\nOn Linux, this can be done by\nsudo service docker start\n.\n./build/mvn install -DskipTests\n./build/mvn test -Pdocker-integration-tests -pl :spark-docker-integration-tests_2.13\nor\n./build/sbt docker-integration-tests/test\nBuilding and testing on an IPv6-only environment\nUse Apache Spark GitBox URL because GitHub doesn’t support IPv6 yet.\nhttps://gitbox.apache.org/repos/asf/spark.git\nTo build and run tests on IPv6-only environment, the following configurations are required.\nexport\nSPARK_LOCAL_HOSTNAME\n=\n\"your-IPv6-address\"\n# e.g. '[2600:1700:232e:3de0:...]'\nexport\nDEFAULT_ARTIFACT_REPOSITORY\n=\nhttps://ipv6.repo1.maven.org/maven2/\nexport\nMAVEN_OPTS\n=\n\"-Djava.net.preferIPv6Addresses=true\"\nexport\nSBT_OPTS\n=\n\"-Djava.net.preferIPv6Addresses=true\"\nexport\nSERIAL_SBT_TESTS\n=\n1\nBuilding with a user-defined\nprotoc\nWhen the user cannot use the official\nprotoc\nbinary files to build the\ncore\nmodule in the compilation environment, for example, compiling\ncore\nmodule on CentOS 6 or CentOS 7 which the default\nglibc\nversion is less than 2.14, we can try to compile and test by specifying the user-defined\nprotoc\nbinary files as follows:\nexport\nSPARK_PROTOC_EXEC_PATH\n=\n/path-to-protoc-exe\n./build/mvn\n-Puser-defined-protoc\n-DskipDefaultProtoc\nclean package\nor\nexport\nSPARK_PROTOC_EXEC_PATH\n=\n/path-to-protoc-exe\n./build/sbt\n-Puser-defined-protoc\nclean package\nThe user-defined\nprotoc\nbinary files can be produced in the user’s compilation environment by source code compilation, for compilation steps, please refer to\nprotobuf\n."}
{"url": "https://spark.apache.org/docs/latest/ml-datasource.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nData sources\nIn this section, we introduce how to use data source in ML to load data.\nBesides some general data sources such as Parquet, CSV, JSON and JDBC, we also provide some specific data sources for ML.\nTable of Contents\nImage data source\nLIBSVM data source\nImage data source\nThis image data source is used to load image files from a directory, it can load compressed image (jpeg, png, etc.) into raw image representation via\nImageIO\nin Java library.\nThe loaded DataFrame has one\nStructType\ncolumn: “image”, containing image data stored as image schema.\nThe schema of the\nimage\ncolumn is:\norigin:\nStringType\n(represents the file path of the image)\nheight:\nIntegerType\n(height of the image)\nwidth:\nIntegerType\n(width of the image)\nnChannels:\nIntegerType\n(number of image channels)\nmode:\nIntegerType\n(OpenCV-compatible type)\ndata:\nBinaryType\n(Image bytes in OpenCV-compatible order: row-wise BGR in most cases)\nIn PySpark we provide Spark SQL data source API for loading image data as a DataFrame.\n>>>\ndf\n=\nspark\n.\nread\n.\nformat\n(\n\"\nimage\n\"\n).\noption\n(\n\"\ndropInvalid\n\"\n,\nTrue\n).\nload\n(\n\"\ndata/mllib/images/origin/kittens\n\"\n)\n>>>\ndf\n.\nselect\n(\n\"\nimage.origin\n\"\n,\n\"\nimage.width\n\"\n,\n\"\nimage.height\n\"\n).\nshow\n(\ntruncate\n=\nFalse\n)\n+-----------------------------------------------------------------------+-----+------+\n|\norigin\n|\nwidth\n|\nheight\n|\n+-----------------------------------------------------------------------+-----+------+\n|\nfile\n:\n///\nspark\n/\ndata\n/\nmllib\n/\nimages\n/\norigin\n/\nkittens\n/\n54893.j\npg\n|\n300\n|\n311\n|\n|\nfile\n:\n///\nspark\n/\ndata\n/\nmllib\n/\nimages\n/\norigin\n/\nkittens\n/\nDP802813\n.\njpg\n|\n199\n|\n313\n|\n|\nfile\n:\n///\nspark\n/\ndata\n/\nmllib\n/\nimages\n/\norigin\n/\nkittens\n/\n29.5\n.\na_b_EGDP022204\n.\njpg\n|\n300\n|\n200\n|\n|\nfile\n:\n///\nspark\n/\ndata\n/\nmllib\n/\nimages\n/\norigin\n/\nkittens\n/\nDP153539\n.\njpg\n|\n300\n|\n296\n|\n+-----------------------------------------------------------------------+-----+------+\nImageDataSource\nimplements a Spark SQL data source API for loading image data as a DataFrame.\nscala\n>\nval\ndf\n=\nspark\n.\nread\n.\nformat\n(\n\"image\"\n).\noption\n(\n\"dropInvalid\"\n,\ntrue\n).\nload\n(\n\"data/mllib/images/origin/kittens\"\n)\ndf\n:\norg.apache.spark.sql.DataFrame\n=\n[\nimage:\nstruct<origin:\nstring\n,\nheight:\nint\n...\n4\nmore\nfields>\n]\nscala\n>\ndf\n.\nselect\n(\n\"image.origin\"\n,\n\"image.width\"\n,\n\"image.height\"\n).\nshow\n(\ntruncate\n=\nfalse\n)\n+-----------------------------------------------------------------------+-----+------+\n|\norigin\n|\nwidth\n|\nheight\n|\n+-----------------------------------------------------------------------+-----+------+\n|\nfile\n:///\nspark\n/\ndata\n/\nmllib\n/\nimages\n/\norigin\n/\nkittens\n/\n54893.\njpg\n|\n300\n|\n311\n|\n|\nfile\n:///\nspark\n/\ndata\n/\nmllib\n/\nimages\n/\norigin\n/\nkittens\n/\nDP802813\n.\njpg\n|\n199\n|\n313\n|\n|\nfile\n:///\nspark\n/\ndata\n/\nmllib\n/\nimages\n/\norigin\n/\nkittens\n/\n29.5\n.\na_b_EGDP022204\n.\njpg\n|\n300\n|\n200\n|\n|\nfile\n:///\nspark\n/\ndata\n/\nmllib\n/\nimages\n/\norigin\n/\nkittens\n/\nDP153539\n.\njpg\n|\n300\n|\n296\n|\n+-----------------------------------------------------------------------+-----+------+\nImageDataSource\nimplements Spark SQL data source API for loading image data as a DataFrame.\nDataset\n<\nRow\n>\nimagesDF\n=\nspark\n.\nread\n().\nformat\n(\n\"image\"\n).\noption\n(\n\"dropInvalid\"\n,\ntrue\n).\nload\n(\n\"data/mllib/images/origin/kittens\"\n);\nimageDF\n.\nselect\n(\n\"image.origin\"\n,\n\"image.width\"\n,\n\"image.height\"\n).\nshow\n(\nfalse\n);\n/*\nWill output:\n+-----------------------------------------------------------------------+-----+------+\n|origin                                                                 |width|height|\n+-----------------------------------------------------------------------+-----+------+\n|file:///spark/data/mllib/images/origin/kittens/54893.jpg               |300  |311   |\n|file:///spark/data/mllib/images/origin/kittens/DP802813.jpg            |199  |313   |\n|file:///spark/data/mllib/images/origin/kittens/29.5.a_b_EGDP022204.jpg |300  |200   |\n|file:///spark/data/mllib/images/origin/kittens/DP153539.jpg            |300  |296   |\n+-----------------------------------------------------------------------+-----+------+\n*/\nIn SparkR we provide Spark SQL data source API for loading image data as a DataFrame.\n>\ndf\n=\nread.df\n(\n\"data/mllib/images/origin/kittens\"\n,\n\"image\"\n)\n>\nhead\n(\nselect\n(\ndf\n,\ndf\n$\nimage.origin\n,\ndf\n$\nimage.width\n,\ndf\n$\nimage.height\n))\n1\nfile\n:///\nspark\n/\ndata\n/\nmllib\n/\nimages\n/\norigin\n/\nkittens\n/\n54893\n.jpg\n2\nfile\n:///\nspark\n/\ndata\n/\nmllib\n/\nimages\n/\norigin\n/\nkittens\n/\nDP802813.jpg\n3\nfile\n:///\nspark\n/\ndata\n/\nmllib\n/\nimages\n/\norigin\n/\nkittens\n/\n29.5\n.a_b_EGDP022204.jpg\n4\nfile\n:///\nspark\n/\ndata\n/\nmllib\n/\nimages\n/\norigin\n/\nkittens\n/\nDP153539.jpg\nwidth\nheight\n1\n300\n311\n2\n199\n313\n3\n300\n200\n4\n300\n296\nLIBSVM data source\nThis\nLIBSVM\ndata source is used to load ‘libsvm’ type files from a directory.\nThe loaded DataFrame has two columns: label containing labels stored as doubles and features containing feature vectors stored as Vectors.\nThe schemas of the columns are:\nlabel:\nDoubleType\n(represents the instance label)\nfeatures:\nVectorUDT\n(represents the feature vector)\nIn PySpark we provide Spark SQL data source API for loading\nLIBSVM\ndata as a DataFrame.\n>>>\ndf\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\noption\n(\n\"\nnumFeatures\n\"\n,\n\"\n780\n\"\n).\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\n>>>\ndf\n.\nshow\n(\n10\n)\n+-----+--------------------+\n|\nlabel\n|\nfeatures\n|\n+-----+--------------------+\n|\n0.0\n|\n(\n780\n,[\n127\n,\n128\n,\n129.\n..\n|\n|\n1.0\n|\n(\n780\n,[\n158\n,\n159\n,\n160.\n..\n|\n|\n1.0\n|\n(\n780\n,[\n124\n,\n125\n,\n126.\n..\n|\n|\n1.0\n|\n(\n780\n,[\n152\n,\n153\n,\n154.\n..\n|\n|\n1.0\n|\n(\n780\n,[\n151\n,\n152\n,\n153.\n..\n|\n|\n0.0\n|\n(\n780\n,[\n129\n,\n130\n,\n131.\n..\n|\n|\n1.0\n|\n(\n780\n,[\n158\n,\n159\n,\n160.\n..\n|\n|\n1.0\n|\n(\n780\n,[\n99\n,\n100\n,\n101\n,...\n|\n|\n0.0\n|\n(\n780\n,[\n154\n,\n155\n,\n156.\n..\n|\n|\n0.0\n|\n(\n780\n,[\n127\n,\n128\n,\n129.\n..\n|\n+-----+--------------------+\nonly\nshowing\ntop\n10\nrows\nLibSVMDataSource\nimplements a Spark SQL data source API for loading\nLIBSVM\ndata as a DataFrame.\nscala\n>\nval\ndf\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\noption\n(\n\"numFeatures\"\n,\n\"780\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\ndf\n:\norg.apache.spark.sql.DataFrame\n=\n[\nlabel:\ndouble\n,\nfeatures:\nvector\n]\nscala\n>\ndf\n.\nshow\n(\n10\n)\n+-----+--------------------+\n|\nlabel\n|\nfeatures\n|\n+-----+--------------------+\n|\n0.0\n|(\n780\n,[\n127\n,\n128\n,\n129\n...|\n|\n1\n.\n0\n|\n(\n780\n,\n[\n158\n,\n159\n,\n160\n...|\n|\n1\n.\n0\n|\n(\n780\n,\n[\n124\n,\n125\n,\n126\n...|\n|\n1\n.\n0\n|\n(\n780\n,\n[\n152\n,\n153\n,\n154\n...|\n|\n1\n.\n0\n|\n(\n780\n,\n[\n151\n,\n152\n,\n153\n...|\n|\n0\n.\n0\n|\n(\n780\n,\n[\n129\n,\n130\n,\n131\n...|\n|\n1\n.\n0\n|\n(\n780\n,\n[\n158\n,\n159\n,\n160\n...|\n|\n1\n.\n0\n|\n(\n780\n,\n[\n99\n,\n100\n,\n101\n,\n...|\n|\n0\n.\n0\n|\n(\n780\n,\n[\n154\n,\n155\n,\n156\n...|\n|\n0\n.\n0\n|\n(\n780\n,\n[\n127\n,\n128\n,\n129\n...|\n+-----+--------------------+\nonly\nshowing\ntop\n10\nrows\nLibSVMDataSource\nimplements Spark SQL data source API for loading\nLIBSVM\ndata as a DataFrame.\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\noption\n(\n\"numFeatures\"\n,\n\"780\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n);\ndf\n.\nshow\n(\n10\n);\n/*\nWill output:\n+-----+--------------------+\n|label|            features|\n+-----+--------------------+\n|  0.0|(780,[127,128,129...|\n|  1.0|(780,[158,159,160...|\n|  1.0|(780,[124,125,126...|\n|  1.0|(780,[152,153,154...|\n|  1.0|(780,[151,152,153...|\n|  0.0|(780,[129,130,131...|\n|  1.0|(780,[158,159,160...|\n|  1.0|(780,[99,100,101,...|\n|  0.0|(780,[154,155,156...|\n|  0.0|(780,[127,128,129...|\n+-----+--------------------+\nonly showing top 10 rows\n*/\nIn SparkR we provide Spark SQL data source API for loading\nLIBSVM\ndata as a DataFrame.\n>\ndf\n=\nread.df\n(\n\"data/mllib/sample_libsvm_data.txt\"\n,\n\"libsvm\"\n)\n>\nhead\n(\nselect\n(\ndf\n,\ndf\n$\nlabel\n,\ndf\n$\nfeatures\n),\n10\n)\nlabel\nfeatures\n1\n0\n<\nenvironment\n:\n0x7fe6d35366e8\n>\n2\n1\n<\nenvironment\n:\n0x7fe6d353bf78\n>\n3\n1\n<\nenvironment\n:\n0x7fe6d3541840\n>\n4\n1\n<\nenvironment\n:\n0x7fe6d3545108\n>\n5\n1\n<\nenvironment\n:\n0x7fe6d354c8e0\n>\n6\n0\n<\nenvironment\n:\n0x7fe6d35501a8\n>\n7\n1\n<\nenvironment\n:\n0x7fe6d3555a70\n>\n8\n1\n<\nenvironment\n:\n0x7fe6d3559338\n>\n9\n0\n<\nenvironment\n:\n0x7fe6d355cc00\n>\n10\n0\n<\nenvironment\n:\n0x7fe6d35643d8\n>"}
{"url": "https://spark.apache.org/docs/latest/ml-pipeline.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nML Pipelines\n\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\wv}{\\mathbf{w}}\n\\newcommand{\\av}{\\mathbf{\\alpha}}\n\\newcommand{\\bv}{\\mathbf{b}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\id}{\\mathbf{I}}\n\\newcommand{\\ind}{\\mathbf{1}}\n\\newcommand{\\0}{\\mathbf{0}}\n\\newcommand{\\unit}{\\mathbf{e}}\n\\newcommand{\\one}{\\mathbf{1}}\n\\newcommand{\\zero}{\\mathbf{0}}\n\\]\nIn this section, we introduce the concept of\nML Pipelines\n.\nML Pipelines provide a uniform set of high-level APIs built on top of\nDataFrames\nthat help users create and tune practical\nmachine learning pipelines.\nTable of Contents\nMain concepts in Pipelines\nDataFrame\nPipeline components\nTransformers\nEstimators\nProperties of pipeline components\nPipeline\nHow it works\nDetails\nParameters\nML persistence: Saving and Loading Pipelines\nBackwards compatibility for ML persistence\nCode examples\nExample: Estimator, Transformer, and Param\nExample: Pipeline\nModel selection (hyperparameter tuning)\nMain concepts in Pipelines\nMLlib standardizes APIs for machine learning algorithms to make it easier to combine multiple\nalgorithms into a single pipeline, or workflow.\nThis section covers the key concepts introduced by the Pipelines API, where the pipeline concept is\nmostly inspired by the\nscikit-learn\nproject.\nDataFrame\n: This ML API uses\nDataFrame\nfrom Spark SQL as an ML\ndataset, which can hold a variety of data types.\nE.g., a\nDataFrame\ncould have different columns storing text, feature vectors, true labels, and predictions.\nTransformer\n: A\nTransformer\nis an algorithm which can transform one\nDataFrame\ninto another\nDataFrame\n.\nE.g., an ML model is a\nTransformer\nwhich transforms a\nDataFrame\nwith features into a\nDataFrame\nwith predictions.\nEstimator\n: An\nEstimator\nis an algorithm which can be fit on a\nDataFrame\nto produce a\nTransformer\n.\nE.g., a learning algorithm is an\nEstimator\nwhich trains on a\nDataFrame\nand produces a model.\nPipeline\n: A\nPipeline\nchains multiple\nTransformer\ns and\nEstimator\ns together to specify an ML workflow.\nParameter\n: All\nTransformer\ns and\nEstimator\ns now share a common API for specifying parameters.\nDataFrame\nMachine learning can be applied to a wide variety of data types, such as vectors, text, images, and structured data.\nThis API adopts the\nDataFrame\nfrom Spark SQL in order to support a variety of data types.\nDataFrame\nsupports many basic and structured types; see the\nSpark SQL datatype reference\nfor a list of supported types.\nIn addition to the types listed in the Spark SQL guide,\nDataFrame\ncan use ML\nVector\ntypes.\nA\nDataFrame\ncan be created either implicitly or explicitly from a regular\nRDD\n.  See the code examples below and the\nSpark SQL programming guide\nfor examples.\nColumns in a\nDataFrame\nare named. The code examples below use names such as “text”, “features”, and “label”.\nPipeline components\nTransformers\nA\nTransformer\nis an abstraction that includes feature transformers and learned models.\nTechnically, a\nTransformer\nimplements a method\ntransform()\n, which converts one\nDataFrame\ninto\nanother, generally by appending one or more columns.\nFor example:\nA feature transformer might take a\nDataFrame\n, read a column (e.g., text), map it into a new\ncolumn (e.g., feature vectors), and output a new\nDataFrame\nwith the mapped column appended.\nA learning model might take a\nDataFrame\n, read the column containing feature vectors, predict the\nlabel for each feature vector, and output a new\nDataFrame\nwith predicted labels appended as a\ncolumn.\nEstimators\nAn\nEstimator\nabstracts the concept of a learning algorithm or any algorithm that fits or trains on\ndata.\nTechnically, an\nEstimator\nimplements a method\nfit()\n, which accepts a\nDataFrame\nand produces a\nModel\n, which is a\nTransformer\n.\nFor example, a learning algorithm such as\nLogisticRegression\nis an\nEstimator\n, and calling\nfit()\ntrains a\nLogisticRegressionModel\n, which is a\nModel\nand hence a\nTransformer\n.\nProperties of pipeline components\nTransformer.transform()\ns and\nEstimator.fit()\ns are both stateless.  In the future, stateful algorithms may be supported via alternative concepts.\nEach instance of a\nTransformer\nor\nEstimator\nhas a unique ID, which is useful in specifying parameters (discussed below).\nPipeline\nIn machine learning, it is common to run a sequence of algorithms to process and learn from data.\nE.g., a simple text document processing workflow might include several stages:\nSplit each document’s text into words.\nConvert each document’s words into a numerical feature vector.\nLearn a prediction model using the feature vectors and labels.\nMLlib represents such a workflow as a\nPipeline\n, which consists of a sequence of\nPipelineStage\ns (\nTransformer\ns and\nEstimator\ns) to be run in a specific order.\nWe will use this simple workflow as a running example in this section.\nHow it works\nA\nPipeline\nis specified as a sequence of stages, and each stage is either a\nTransformer\nor an\nEstimator\n.\nThese stages are run in order, and the input\nDataFrame\nis transformed as it passes through each stage.\nFor\nTransformer\nstages, the\ntransform()\nmethod is called on the\nDataFrame\n.\nFor\nEstimator\nstages, the\nfit()\nmethod is called to produce a\nTransformer\n(which becomes part of the\nPipelineModel\n, or fitted\nPipeline\n), and that\nTransformer\n’s\ntransform()\nmethod is called on the\nDataFrame\n.\nWe illustrate this for the simple text document workflow.  The figure below is for the\ntraining time\nusage of a\nPipeline\n.\nAbove, the top row represents a\nPipeline\nwith three stages.\nThe first two (\nTokenizer\nand\nHashingTF\n) are\nTransformer\ns (blue), and the third (\nLogisticRegression\n) is an\nEstimator\n(red).\nThe bottom row represents data flowing through the pipeline, where cylinders indicate\nDataFrame\ns.\nThe\nPipeline.fit()\nmethod is called on the original\nDataFrame\n, which has raw text documents and labels.\nThe\nTokenizer.transform()\nmethod splits the raw text documents into words, adding a new column with words to the\nDataFrame\n.\nThe\nHashingTF.transform()\nmethod converts the words column into feature vectors, adding a new column with those vectors to the\nDataFrame\n.\nNow, since\nLogisticRegression\nis an\nEstimator\n, the\nPipeline\nfirst calls\nLogisticRegression.fit()\nto produce a\nLogisticRegressionModel\n.\nIf the\nPipeline\nhad more\nEstimator\ns, it would call the\nLogisticRegressionModel\n’s\ntransform()\nmethod on the\nDataFrame\nbefore passing the\nDataFrame\nto the next stage.\nA\nPipeline\nis an\nEstimator\n.\nThus, after a\nPipeline\n’s\nfit()\nmethod runs, it produces a\nPipelineModel\n, which is a\nTransformer\n.\nThis\nPipelineModel\nis used at\ntest time\n; the figure below illustrates this usage.\nIn the figure above, the\nPipelineModel\nhas the same number of stages as the original\nPipeline\n, but all\nEstimator\ns in the original\nPipeline\nhave become\nTransformer\ns.\nWhen the\nPipelineModel\n’s\ntransform()\nmethod is called on a test dataset, the data are passed\nthrough the fitted pipeline in order.\nEach stage’s\ntransform()\nmethod updates the dataset and passes it to the next stage.\nPipeline\ns and\nPipelineModel\ns help to ensure that training and test data go through identical feature processing steps.\nDetails\nDAG\nPipeline\ns\n: A\nPipeline\n’s stages are specified as an ordered array.  The examples given here are all for linear\nPipeline\ns, i.e.,\nPipeline\ns in which each stage uses data produced by the previous stage.  It is possible to create non-linear\nPipeline\ns as long as the data flow graph forms a Directed Acyclic Graph (DAG).  This graph is currently specified implicitly based on the input and output column names of each stage (generally specified as parameters).  If the\nPipeline\nforms a DAG, then the stages must be specified in topological order.\nRuntime checking\n: Since\nPipeline\ns can operate on\nDataFrame\ns with varied types, they cannot use\ncompile-time type checking.\nPipeline\ns and\nPipelineModel\ns instead do runtime checking before actually running the\nPipeline\n.\nThis type checking is done using the\nDataFrame\nschema\n, a description of the data types of columns in the\nDataFrame\n.\nUnique Pipeline stages\n: A\nPipeline\n’s stages should be unique instances.  E.g., the same instance\nmyHashingTF\nshould not be inserted into the\nPipeline\ntwice since\nPipeline\nstages must have\nunique IDs.  However, different instances\nmyHashingTF1\nand\nmyHashingTF2\n(both of type\nHashingTF\n)\ncan be put into the same\nPipeline\nsince different instances will be created with different IDs.\nParameters\nMLlib\nEstimator\ns and\nTransformer\ns use a uniform API for specifying parameters.\nA\nParam\nis a named parameter with self-contained documentation.\nA\nParamMap\nis a set of (parameter, value) pairs.\nThere are two main ways to pass parameters to an algorithm:\nSet parameters for an instance.  E.g., if\nlr\nis an instance of\nLogisticRegression\n, one could\ncall\nlr.setMaxIter(10)\nto make\nlr.fit()\nuse at most 10 iterations.\nThis API resembles the API used in\nspark.mllib\npackage.\nPass a\nParamMap\nto\nfit()\nor\ntransform()\n.  Any parameters in the\nParamMap\nwill override parameters previously specified via setter methods.\nParameters belong to specific instances of\nEstimator\ns and\nTransformer\ns.\nFor example, if we have two\nLogisticRegression\ninstances\nlr1\nand\nlr2\n, then we can build a\nParamMap\nwith both\nmaxIter\nparameters specified:\nParamMap(lr1.maxIter -> 10, lr2.maxIter -> 20)\n.\nThis is useful if there are two algorithms with the\nmaxIter\nparameter in a\nPipeline\n.\nML persistence: Saving and Loading Pipelines\nOften times it is worth it to save a model or a pipeline to disk for later use. In Spark 1.6, a model import/export functionality was added to the Pipeline API.\nAs of Spark 2.3, the DataFrame-based API in\nspark.ml\nand\npyspark.ml\nhas complete coverage.\nML persistence works across Scala, Java and Python.  However, R currently uses a modified format,\nso models saved in R can only be loaded back in R; this should be fixed in the future and is\ntracked in\nSPARK-15572\n.\nBackwards compatibility for ML persistence\nIn general, MLlib maintains backwards compatibility for ML persistence.  I.e., if you save an ML\nmodel or Pipeline in one version of Spark, then you should be able to load it back and use it in a\nfuture version of Spark.  However, there are rare exceptions, described below.\nModel persistence: Is a model or Pipeline saved using Apache Spark ML persistence in Spark\nversion X loadable by Spark version Y?\nMajor versions: No guarantees, but best-effort.\nMinor and patch versions: Yes; these are backwards compatible.\nNote about the format: There are no guarantees for a stable persistence format, but model loading itself is designed to be backwards compatible.\nModel behavior: Does a model or Pipeline in Spark version X behave identically in Spark version Y?\nMajor versions: No guarantees, but best-effort.\nMinor and patch versions: Identical behavior, except for bug fixes.\nFor both model persistence and model behavior, any breaking changes across a minor version or patch\nversion are reported in the Spark version release notes. If a breakage is not reported in release\nnotes, then it should be treated as a bug to be fixed.\nCode examples\nThis section gives code examples illustrating the functionality discussed above.\nFor more info, please refer to the API documentation\n(\nPython\n,\nScala\n,\nand\nJava\n).\nExample: Estimator, Transformer, and Param\nThis example covers the concepts of\nEstimator\n,\nTransformer\n, and\nParam\n.\nRefer to the\nEstimator\nPython docs\n,\nthe\nTransformer\nPython docs\nand\nthe\nParams\nPython docs\nfor more details on the API.\nfrom\npyspark.ml.linalg\nimport\nVectors\nfrom\npyspark.ml.classification\nimport\nLogisticRegression\n# Prepare training data from a list of (label, features) tuples.\ntraining\n=\nspark\n.\ncreateDataFrame\n([\n(\n1.0\n,\nVectors\n.\ndense\n([\n0.0\n,\n1.1\n,\n0.1\n])),\n(\n0.0\n,\nVectors\n.\ndense\n([\n2.0\n,\n1.0\n,\n-\n1.0\n])),\n(\n0.0\n,\nVectors\n.\ndense\n([\n2.0\n,\n1.3\n,\n1.0\n])),\n(\n1.0\n,\nVectors\n.\ndense\n([\n0.0\n,\n1.2\n,\n-\n0.5\n]))],\n[\n\"\nlabel\n\"\n,\n\"\nfeatures\n\"\n])\n# Create a LogisticRegression instance. This instance is an Estimator.\nlr\n=\nLogisticRegression\n(\nmaxIter\n=\n10\n,\nregParam\n=\n0.01\n)\n# Print out the parameters, documentation, and any default values.\nprint\n(\n\"\nLogisticRegression parameters:\n\\n\n\"\n+\nlr\n.\nexplainParams\n()\n+\n\"\n\\n\n\"\n)\n# Learn a LogisticRegression model. This uses the parameters stored in lr.\nmodel1\n=\nlr\n.\nfit\n(\ntraining\n)\n# Since model1 is a Model (i.e., a transformer produced by an Estimator),\n# we can view the parameters it used during fit().\n# This prints the parameter (name: value) pairs, where names are unique IDs for this\n# LogisticRegression instance.\nprint\n(\n\"\nModel 1 was fit using parameters:\n\"\n)\nprint\n(\nmodel1\n.\nextractParamMap\n())\n# We may alternatively specify parameters using a Python dictionary as a paramMap\nparamMap\n=\n{\nlr\n.\nmaxIter\n:\n20\n}\nparamMap\n[\nlr\n.\nmaxIter\n]\n=\n30\n# Specify 1 Param, overwriting the original maxIter.\n# Specify multiple Params.\nparamMap\n.\nupdate\n({\nlr\n.\nregParam\n:\n0.1\n,\nlr\n.\nthreshold\n:\n0.55\n})\n# type: ignore\n# You can combine paramMaps, which are python dictionaries.\n# Change output column name\nparamMap2\n=\n{\nlr\n.\nprobabilityCol\n:\n\"\nmyProbability\n\"\n}\nparamMapCombined\n=\nparamMap\n.\ncopy\n()\nparamMapCombined\n.\nupdate\n(\nparamMap2\n)\n# type: ignore\n# Now learn a new model using the paramMapCombined parameters.\n# paramMapCombined overrides all parameters set earlier via lr.set* methods.\nmodel2\n=\nlr\n.\nfit\n(\ntraining\n,\nparamMapCombined\n)\nprint\n(\n\"\nModel 2 was fit using parameters:\n\"\n)\nprint\n(\nmodel2\n.\nextractParamMap\n())\n# Prepare test data\ntest\n=\nspark\n.\ncreateDataFrame\n([\n(\n1.0\n,\nVectors\n.\ndense\n([\n-\n1.0\n,\n1.5\n,\n1.3\n])),\n(\n0.0\n,\nVectors\n.\ndense\n([\n3.0\n,\n2.0\n,\n-\n0.1\n])),\n(\n1.0\n,\nVectors\n.\ndense\n([\n0.0\n,\n2.2\n,\n-\n1.5\n]))],\n[\n\"\nlabel\n\"\n,\n\"\nfeatures\n\"\n])\n# Make predictions on test data using the Transformer.transform() method.\n# LogisticRegression.transform will only use the 'features' column.\n# Note that model2.transform() outputs a \"myProbability\" column instead of the usual\n# 'probability' column since we renamed the lr.probabilityCol parameter previously.\nprediction\n=\nmodel2\n.\ntransform\n(\ntest\n)\nresult\n=\nprediction\n.\nselect\n(\n\"\nfeatures\n\"\n,\n\"\nlabel\n\"\n,\n\"\nmyProbability\n\"\n,\n\"\nprediction\n\"\n)\n\\\n.\ncollect\n()\nfor\nrow\nin\nresult\n:\nprint\n(\n\"\nfeatures=%s, label=%s -> prob=%s, prediction=%s\n\"\n%\n(\nrow\n.\nfeatures\n,\nrow\n.\nlabel\n,\nrow\n.\nmyProbability\n,\nrow\n.\nprediction\n))\nFind full example code at \"examples/src/main/python/ml/estimator_transformer_param_example.py\" in the Spark repo.\nRefer to the\nEstimator\nScala docs\n,\nthe\nTransformer\nScala docs\nand\nthe\nParams\nScala docs\nfor details on the API.\nimport\norg.apache.spark.ml.classification.LogisticRegression\nimport\norg.apache.spark.ml.linalg.\n{\nVector\n,\nVectors\n}\nimport\norg.apache.spark.ml.param.ParamMap\nimport\norg.apache.spark.sql.Row\n// Prepare training data from a list of (label, features) tuples.\nval\ntraining\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n1.0\n,\nVectors\n.\ndense\n(\n0.0\n,\n1.1\n,\n0.1\n)),\n(\n0.0\n,\nVectors\n.\ndense\n(\n2.0\n,\n1.0\n,\n-\n1.0\n)),\n(\n0.0\n,\nVectors\n.\ndense\n(\n2.0\n,\n1.3\n,\n1.0\n)),\n(\n1.0\n,\nVectors\n.\ndense\n(\n0.0\n,\n1.2\n,\n-\n0.5\n))\n)).\ntoDF\n(\n\"label\"\n,\n\"features\"\n)\n// Create a LogisticRegression instance. This instance is an Estimator.\nval\nlr\n=\nnew\nLogisticRegression\n()\n// Print out the parameters, documentation, and any default values.\nprintln\n(\ns\n\"LogisticRegression parameters:\\n ${lr.explainParams()}\\n\"\n)\n// We may set parameters using setter methods.\nlr\n.\nsetMaxIter\n(\n10\n)\n.\nsetRegParam\n(\n0.01\n)\n// Learn a LogisticRegression model. This uses the parameters stored in lr.\nval\nmodel1\n=\nlr\n.\nfit\n(\ntraining\n)\n// Since model1 is a Model (i.e., a Transformer produced by an Estimator),\n// we can view the parameters it used during fit().\n// This prints the parameter (name: value) pairs, where names are unique IDs for this\n// LogisticRegression instance.\nprintln\n(\ns\n\"Model 1 was fit using parameters: ${model1.parent.extractParamMap()}\"\n)\n// We may alternatively specify parameters using a ParamMap,\n// which supports several methods for specifying parameters.\nval\nparamMap\n=\nParamMap\n(\nlr\n.\nmaxIter\n->\n20\n)\n.\nput\n(\nlr\n.\nmaxIter\n,\n30\n)\n// Specify 1 Param. This overwrites the original maxIter.\n.\nput\n(\nlr\n.\nregParam\n->\n0.1\n,\nlr\n.\nthreshold\n->\n0.55\n)\n// Specify multiple Params.\n// One can also combine ParamMaps.\nval\nparamMap2\n=\nParamMap\n(\nlr\n.\nprobabilityCol\n->\n\"myProbability\"\n)\n// Change output column name.\nval\nparamMapCombined\n=\nparamMap\n++\nparamMap2\n// Now learn a new model using the paramMapCombined parameters.\n// paramMapCombined overrides all parameters set earlier via lr.set* methods.\nval\nmodel2\n=\nlr\n.\nfit\n(\ntraining\n,\nparamMapCombined\n)\nprintln\n(\ns\n\"Model 2 was fit using parameters: ${model2.parent.extractParamMap()}\"\n)\n// Prepare test data.\nval\ntest\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n1.0\n,\nVectors\n.\ndense\n(-\n1.0\n,\n1.5\n,\n1.3\n)),\n(\n0.0\n,\nVectors\n.\ndense\n(\n3.0\n,\n2.0\n,\n-\n0.1\n)),\n(\n1.0\n,\nVectors\n.\ndense\n(\n0.0\n,\n2.2\n,\n-\n1.5\n))\n)).\ntoDF\n(\n\"label\"\n,\n\"features\"\n)\n// Make predictions on test data using the Transformer.transform() method.\n// LogisticRegression.transform will only use the 'features' column.\n// Note that model2.transform() outputs a 'myProbability' column instead of the usual\n// 'probability' column since we renamed the lr.probabilityCol parameter previously.\nmodel2\n.\ntransform\n(\ntest\n)\n.\nselect\n(\n\"features\"\n,\n\"label\"\n,\n\"myProbability\"\n,\n\"prediction\"\n)\n.\ncollect\n()\n.\nforeach\n{\ncase\nRow\n(\nfeatures\n:\nVector\n,\nlabel\n:\nDouble\n,\nprob\n:\nVector\n,\nprediction\n:\nDouble\n)\n=>\nprintln\n(\ns\n\"($features, $label) -> prob=$prob, prediction=$prediction\"\n)\n}\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\" in the Spark repo.\nRefer to the\nEstimator\nJava docs\n,\nthe\nTransformer\nJava docs\nand\nthe\nParams\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.classification.LogisticRegression\n;\nimport\norg.apache.spark.ml.classification.LogisticRegressionModel\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.ml.param.ParamMap\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\n// Prepare training data.\nList\n<\nRow\n>\ndataTraining\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n1.0\n,\nVectors\n.\ndense\n(\n0.0\n,\n1.1\n,\n0.1\n)),\nRowFactory\n.\ncreate\n(\n0.0\n,\nVectors\n.\ndense\n(\n2.0\n,\n1.0\n,\n-\n1.0\n)),\nRowFactory\n.\ncreate\n(\n0.0\n,\nVectors\n.\ndense\n(\n2.0\n,\n1.3\n,\n1.0\n)),\nRowFactory\n.\ncreate\n(\n1.0\n,\nVectors\n.\ndense\n(\n0.0\n,\n1.2\n,\n-\n0.5\n))\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"label\"\n,\nDataTypes\n.\nDoubleType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"features\"\n,\nnew\nVectorUDT\n(),\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ntraining\n=\nspark\n.\ncreateDataFrame\n(\ndataTraining\n,\nschema\n);\n// Create a LogisticRegression instance. This instance is an Estimator.\nLogisticRegression\nlr\n=\nnew\nLogisticRegression\n();\n// Print out the parameters, documentation, and any default values.\nSystem\n.\nout\n.\nprintln\n(\n\"LogisticRegression parameters:\\n\"\n+\nlr\n.\nexplainParams\n()\n+\n\"\\n\"\n);\n// We may set parameters using setter methods.\nlr\n.\nsetMaxIter\n(\n10\n).\nsetRegParam\n(\n0.01\n);\n// Learn a LogisticRegression model. This uses the parameters stored in lr.\nLogisticRegressionModel\nmodel1\n=\nlr\n.\nfit\n(\ntraining\n);\n// Since model1 is a Model (i.e., a Transformer produced by an Estimator),\n// we can view the parameters it used during fit().\n// This prints the parameter (name: value) pairs, where names are unique IDs for this\n// LogisticRegression instance.\nSystem\n.\nout\n.\nprintln\n(\n\"Model 1 was fit using parameters: \"\n+\nmodel1\n.\nparent\n().\nextractParamMap\n());\n// We may alternatively specify parameters using a ParamMap.\nParamMap\nparamMap\n=\nnew\nParamMap\n()\n.\nput\n(\nlr\n.\nmaxIter\n().\nw\n(\n20\n))\n// Specify 1 Param.\n.\nput\n(\nlr\n.\nmaxIter\n(),\n30\n)\n// This overwrites the original maxIter.\n.\nput\n(\nlr\n.\nregParam\n().\nw\n(\n0.1\n),\nlr\n.\nthreshold\n().\nw\n(\n0.55\n));\n// Specify multiple Params.\n// One can also combine ParamMaps.\nParamMap\nparamMap2\n=\nnew\nParamMap\n()\n.\nput\n(\nlr\n.\nprobabilityCol\n().\nw\n(\n\"myProbability\"\n));\n// Change output column name\nParamMap\nparamMapCombined\n=\nparamMap\n.\n$plus$plus\n(\nparamMap2\n);\n// Now learn a new model using the paramMapCombined parameters.\n// paramMapCombined overrides all parameters set earlier via lr.set* methods.\nLogisticRegressionModel\nmodel2\n=\nlr\n.\nfit\n(\ntraining\n,\nparamMapCombined\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Model 2 was fit using parameters: \"\n+\nmodel2\n.\nparent\n().\nextractParamMap\n());\n// Prepare test documents.\nList\n<\nRow\n>\ndataTest\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n1.0\n,\nVectors\n.\ndense\n(-\n1.0\n,\n1.5\n,\n1.3\n)),\nRowFactory\n.\ncreate\n(\n0.0\n,\nVectors\n.\ndense\n(\n3.0\n,\n2.0\n,\n-\n0.1\n)),\nRowFactory\n.\ncreate\n(\n1.0\n,\nVectors\n.\ndense\n(\n0.0\n,\n2.2\n,\n-\n1.5\n))\n);\nDataset\n<\nRow\n>\ntest\n=\nspark\n.\ncreateDataFrame\n(\ndataTest\n,\nschema\n);\n// Make predictions on test documents using the Transformer.transform() method.\n// LogisticRegression.transform will only use the 'features' column.\n// Note that model2.transform() outputs a 'myProbability' column instead of the usual\n// 'probability' column since we renamed the lr.probabilityCol parameter previously.\nDataset\n<\nRow\n>\nresults\n=\nmodel2\n.\ntransform\n(\ntest\n);\nDataset\n<\nRow\n>\nrows\n=\nresults\n.\nselect\n(\n\"features\"\n,\n\"label\"\n,\n\"myProbability\"\n,\n\"prediction\"\n);\nfor\n(\nRow\nr:\nrows\n.\ncollectAsList\n())\n{\nSystem\n.\nout\n.\nprintln\n(\n\"(\"\n+\nr\n.\nget\n(\n0\n)\n+\n\", \"\n+\nr\n.\nget\n(\n1\n)\n+\n\") -> prob=\"\n+\nr\n.\nget\n(\n2\n)\n+\n\", prediction=\"\n+\nr\n.\nget\n(\n3\n));\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\" in the Spark repo.\nExample: Pipeline\nThis example follows the simple text document\nPipeline\nillustrated in the figures above.\nRefer to the\nPipeline\nPython docs\nfor more details on the API.\nfrom\npyspark.ml\nimport\nPipeline\nfrom\npyspark.ml.classification\nimport\nLogisticRegression\nfrom\npyspark.ml.feature\nimport\nHashingTF\n,\nTokenizer\n# Prepare training documents from a list of (id, text, label) tuples.\ntraining\n=\nspark\n.\ncreateDataFrame\n([\n(\n0\n,\n\"\na b c d e spark\n\"\n,\n1.0\n),\n(\n1\n,\n\"\nb d\n\"\n,\n0.0\n),\n(\n2\n,\n\"\nspark f g h\n\"\n,\n1.0\n),\n(\n3\n,\n\"\nhadoop mapreduce\n\"\n,\n0.0\n)\n],\n[\n\"\nid\n\"\n,\n\"\ntext\n\"\n,\n\"\nlabel\n\"\n])\n# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\ntokenizer\n=\nTokenizer\n(\ninputCol\n=\n\"\ntext\n\"\n,\noutputCol\n=\n\"\nwords\n\"\n)\nhashingTF\n=\nHashingTF\n(\ninputCol\n=\ntokenizer\n.\ngetOutputCol\n(),\noutputCol\n=\n\"\nfeatures\n\"\n)\nlr\n=\nLogisticRegression\n(\nmaxIter\n=\n10\n,\nregParam\n=\n0.001\n)\npipeline\n=\nPipeline\n(\nstages\n=\n[\ntokenizer\n,\nhashingTF\n,\nlr\n])\n# Fit the pipeline to training documents.\nmodel\n=\npipeline\n.\nfit\n(\ntraining\n)\n# Prepare test documents, which are unlabeled (id, text) tuples.\ntest\n=\nspark\n.\ncreateDataFrame\n([\n(\n4\n,\n\"\nspark i j k\n\"\n),\n(\n5\n,\n\"\nl m n\n\"\n),\n(\n6\n,\n\"\nspark hadoop spark\n\"\n),\n(\n7\n,\n\"\napache hadoop\n\"\n)\n],\n[\n\"\nid\n\"\n,\n\"\ntext\n\"\n])\n# Make predictions on test documents and print columns of interest.\nprediction\n=\nmodel\n.\ntransform\n(\ntest\n)\nselected\n=\nprediction\n.\nselect\n(\n\"\nid\n\"\n,\n\"\ntext\n\"\n,\n\"\nprobability\n\"\n,\n\"\nprediction\n\"\n)\nfor\nrow\nin\nselected\n.\ncollect\n():\nrid\n,\ntext\n,\nprob\n,\nprediction\n=\nrow\nprint\n(\n\"\n(%d, %s) --> prob=%s, prediction=%f\n\"\n%\n(\nrid\n,\ntext\n,\nstr\n(\nprob\n),\nprediction\n# type: ignore\n)\n)\nFind full example code at \"examples/src/main/python/ml/pipeline_example.py\" in the Spark repo.\nRefer to the\nPipeline\nScala docs\nfor details on the API.\nimport\norg.apache.spark.ml.\n{\nPipeline\n,\nPipelineModel\n}\nimport\norg.apache.spark.ml.classification.LogisticRegression\nimport\norg.apache.spark.ml.feature.\n{\nHashingTF\n,\nTokenizer\n}\nimport\norg.apache.spark.ml.linalg.Vector\nimport\norg.apache.spark.sql.Row\n// Prepare training documents from a list of (id, text, label) tuples.\nval\ntraining\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0L\n,\n\"a b c d e spark\"\n,\n1.0\n),\n(\n1L\n,\n\"b d\"\n,\n0.0\n),\n(\n2L\n,\n\"spark f g h\"\n,\n1.0\n),\n(\n3L\n,\n\"hadoop mapreduce\"\n,\n0.0\n)\n)).\ntoDF\n(\n\"id\"\n,\n\"text\"\n,\n\"label\"\n)\n// Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\nval\ntokenizer\n=\nnew\nTokenizer\n()\n.\nsetInputCol\n(\n\"text\"\n)\n.\nsetOutputCol\n(\n\"words\"\n)\nval\nhashingTF\n=\nnew\nHashingTF\n()\n.\nsetNumFeatures\n(\n1000\n)\n.\nsetInputCol\n(\ntokenizer\n.\ngetOutputCol\n)\n.\nsetOutputCol\n(\n\"features\"\n)\nval\nlr\n=\nnew\nLogisticRegression\n()\n.\nsetMaxIter\n(\n10\n)\n.\nsetRegParam\n(\n0.001\n)\nval\npipeline\n=\nnew\nPipeline\n()\n.\nsetStages\n(\nArray\n(\ntokenizer\n,\nhashingTF\n,\nlr\n))\n// Fit the pipeline to training documents.\nval\nmodel\n=\npipeline\n.\nfit\n(\ntraining\n)\n// Now we can optionally save the fitted pipeline to disk\nmodel\n.\nwrite\n.\noverwrite\n().\nsave\n(\n\"/tmp/spark-logistic-regression-model\"\n)\n// We can also save this unfit pipeline to disk\npipeline\n.\nwrite\n.\noverwrite\n().\nsave\n(\n\"/tmp/unfit-lr-model\"\n)\n// And load it back in during production\nval\nsameModel\n=\nPipelineModel\n.\nload\n(\n\"/tmp/spark-logistic-regression-model\"\n)\n// Prepare test documents, which are unlabeled (id, text) tuples.\nval\ntest\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n4L\n,\n\"spark i j k\"\n),\n(\n5L\n,\n\"l m n\"\n),\n(\n6L\n,\n\"spark hadoop spark\"\n),\n(\n7L\n,\n\"apache hadoop\"\n)\n)).\ntoDF\n(\n\"id\"\n,\n\"text\"\n)\n// Make predictions on test documents.\nmodel\n.\ntransform\n(\ntest\n)\n.\nselect\n(\n\"id\"\n,\n\"text\"\n,\n\"probability\"\n,\n\"prediction\"\n)\n.\ncollect\n()\n.\nforeach\n{\ncase\nRow\n(\nid\n:\nLong\n,\ntext\n:\nString\n,\nprob\n:\nVector\n,\nprediction\n:\nDouble\n)\n=>\nprintln\n(\ns\n\"($id, $text) --> prob=$prob, prediction=$prediction\"\n)\n}\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\" in the Spark repo.\nRefer to the\nPipeline\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\norg.apache.spark.ml.Pipeline\n;\nimport\norg.apache.spark.ml.PipelineModel\n;\nimport\norg.apache.spark.ml.PipelineStage\n;\nimport\norg.apache.spark.ml.classification.LogisticRegression\n;\nimport\norg.apache.spark.ml.feature.HashingTF\n;\nimport\norg.apache.spark.ml.feature.Tokenizer\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\n// Prepare training documents, which are labeled.\nDataset\n<\nRow\n>\ntraining\n=\nspark\n.\ncreateDataFrame\n(\nArrays\n.\nasList\n(\nnew\nJavaLabeledDocument\n(\n0L\n,\n\"a b c d e spark\"\n,\n1.0\n),\nnew\nJavaLabeledDocument\n(\n1L\n,\n\"b d\"\n,\n0.0\n),\nnew\nJavaLabeledDocument\n(\n2L\n,\n\"spark f g h\"\n,\n1.0\n),\nnew\nJavaLabeledDocument\n(\n3L\n,\n\"hadoop mapreduce\"\n,\n0.0\n)\n),\nJavaLabeledDocument\n.\nclass\n);\n// Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\nTokenizer\ntokenizer\n=\nnew\nTokenizer\n()\n.\nsetInputCol\n(\n\"text\"\n)\n.\nsetOutputCol\n(\n\"words\"\n);\nHashingTF\nhashingTF\n=\nnew\nHashingTF\n()\n.\nsetNumFeatures\n(\n1000\n)\n.\nsetInputCol\n(\ntokenizer\n.\ngetOutputCol\n())\n.\nsetOutputCol\n(\n\"features\"\n);\nLogisticRegression\nlr\n=\nnew\nLogisticRegression\n()\n.\nsetMaxIter\n(\n10\n)\n.\nsetRegParam\n(\n0.001\n);\nPipeline\npipeline\n=\nnew\nPipeline\n()\n.\nsetStages\n(\nnew\nPipelineStage\n[]\n{\ntokenizer\n,\nhashingTF\n,\nlr\n});\n// Fit the pipeline to training documents.\nPipelineModel\nmodel\n=\npipeline\n.\nfit\n(\ntraining\n);\n// Prepare test documents, which are unlabeled.\nDataset\n<\nRow\n>\ntest\n=\nspark\n.\ncreateDataFrame\n(\nArrays\n.\nasList\n(\nnew\nJavaDocument\n(\n4L\n,\n\"spark i j k\"\n),\nnew\nJavaDocument\n(\n5L\n,\n\"l m n\"\n),\nnew\nJavaDocument\n(\n6L\n,\n\"spark hadoop spark\"\n),\nnew\nJavaDocument\n(\n7L\n,\n\"apache hadoop\"\n)\n),\nJavaDocument\n.\nclass\n);\n// Make predictions on test documents.\nDataset\n<\nRow\n>\npredictions\n=\nmodel\n.\ntransform\n(\ntest\n);\nfor\n(\nRow\nr\n:\npredictions\n.\nselect\n(\n\"id\"\n,\n\"text\"\n,\n\"probability\"\n,\n\"prediction\"\n).\ncollectAsList\n())\n{\nSystem\n.\nout\n.\nprintln\n(\n\"(\"\n+\nr\n.\nget\n(\n0\n)\n+\n\", \"\n+\nr\n.\nget\n(\n1\n)\n+\n\") --> prob=\"\n+\nr\n.\nget\n(\n2\n)\n+\n\", prediction=\"\n+\nr\n.\nget\n(\n3\n));\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\" in the Spark repo.\nModel selection (hyperparameter tuning)\nA big benefit of using ML Pipelines is hyperparameter optimization.  See the\nML Tuning Guide\nfor more information on automatic model selection."}
{"url": "https://spark.apache.org/docs/latest/ml-features.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nExtracting, transforming and selecting features\nThis section covers algorithms for working with features, roughly divided into these groups:\nExtraction: Extracting features from “raw” data\nTransformation: Scaling, converting, or modifying features\nSelection: Selecting a subset from a larger set of features\nLocality Sensitive Hashing (LSH): This class of algorithms combines aspects of feature transformation with other algorithms.\nTable of Contents\nFeature Extractors\nTF-IDF\nWord2Vec\nCountVectorizer\nFeatureHasher\nFeature Transformers\nTokenizer\nStopWordsRemover\n$n$-gram\nBinarizer\nPCA\nPolynomialExpansion\nDiscrete Cosine Transform (DCT)\nStringIndexer\nIndexToString\nOneHotEncoder\nTargetEncoder\nVectorIndexer\nInteraction\nNormalizer\nStandardScaler\nRobustScaler\nMinMaxScaler\nMaxAbsScaler\nBucketizer\nElementwiseProduct\nSQLTransformer\nVectorAssembler\nVectorSizeHint\nQuantileDiscretizer\nImputer\nFeature Selectors\nVectorSlicer\nRFormula\nChiSqSelector\nUnivariateFeatureSelector\nVarianceThresholdSelector\nLocality Sensitive Hashing\nLSH Operations\nFeature Transformation\nApproximate Similarity Join\nApproximate Nearest Neighbor Search\nLSH Algorithms\nBucketed Random Projection for Euclidean Distance\nMinHash for Jaccard Distance\nFeature Extractors\nTF-IDF\nTerm frequency-inverse document frequency (TF-IDF)\nis a feature vectorization method widely used in text mining to reflect the importance of a term \nto a document in the corpus. Denote a term by\n$t$\n, a document by\n$d$\n, and the corpus by\n$D$\n.\nTerm frequency\n$TF(t, d)$\nis the number of times that term\n$t$\nappears in document\n$d$\n, while \ndocument frequency\n$DF(t, D)$\nis the number of documents that contains term\n$t$\n. If we only use \nterm frequency to measure the importance, it is very easy to over-emphasize terms that appear very \noften but carry little information about the document, e.g. “a”, “the”, and “of”. If a term appears \nvery often across the corpus, it means it doesn’t carry special information about a particular document.\nInverse document frequency is a numerical measure of how much information a term provides:\n\\[\nIDF(t, D) = \\log \\frac{|D| + 1}{DF(t, D) + 1},\n\\]\nwhere\n$|D|$\nis the total number of documents in the corpus. Since logarithm is used, if a term \nappears in all documents, its IDF value becomes 0. Note that a smoothing term is applied to avoid \ndividing by zero for terms outside the corpus. The TF-IDF measure is simply the product of TF and IDF:\n\\[\nTFIDF(t, d, D) = TF(t, d) \\cdot IDF(t, D).\n\\]\nThere are several variants on the definition of term frequency and document frequency.\nIn MLlib, we separate TF and IDF to make them flexible.\nTF\n: Both\nHashingTF\nand\nCountVectorizer\ncan be used to generate the term frequency vectors.\nHashingTF\nis a\nTransformer\nwhich takes sets of terms and converts those sets into \nfixed-length feature vectors.  In text processing, a “set of terms” might be a bag of words.\nHashingTF\nutilizes the\nhashing trick\n.\nA raw feature is mapped into an index (term) by applying a hash function. The hash function\nused here is\nMurmurHash 3\n. Then term frequencies\nare calculated based on the mapped indices. This approach avoids the need to compute a global \nterm-to-index map, which can be expensive for a large corpus, but it suffers from potential hash \ncollisions, where different raw features may become the same term after hashing. To reduce the \nchance of collision, we can increase the target feature dimension, i.e. the number of buckets \nof the hash table. Since a simple modulo on the hashed value is used to determine the vector index,\nit is advisable to use a power of two as the feature dimension, otherwise the features will not\nbe mapped evenly to the vector indices. The default feature dimension is\n$2^{18} = 262,144$\n.\nAn optional binary toggle parameter controls term frequency counts. When set to true all nonzero\nfrequency counts are set to 1. This is especially useful for discrete probabilistic models that\nmodel binary, rather than integer, counts.\nCountVectorizer\nconverts text documents to vectors of term counts. Refer to\nCountVectorizer\nfor more details.\nIDF\n:\nIDF\nis an\nEstimator\nwhich is fit on a dataset and produces an\nIDFModel\n.  The\nIDFModel\ntakes feature vectors (generally created from\nHashingTF\nor\nCountVectorizer\n) and \nscales each feature. Intuitively, it down-weights features which appear frequently in a corpus.\nNote:\nspark.ml\ndoesn’t provide tools for text segmentation.\nWe refer users to the\nStanford NLP Group\nand\nscalanlp/chalk\n.\nExamples\nIn the following code segment, we start with a set of sentences.  We split each sentence into words \nusing\nTokenizer\n.  For each sentence (bag of words), we use\nHashingTF\nto hash the sentence into \na feature vector.  We use\nIDF\nto rescale the feature vectors; this generally improves performance \nwhen using text as features.  Our feature vectors could then be passed to a learning algorithm.\nRefer to the\nHashingTF Python docs\nand\nthe\nIDF Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nHashingTF\n,\nIDF\n,\nTokenizer\nsentenceData\n=\nspark\n.\ncreateDataFrame\n([\n(\n0.0\n,\n\"\nHi I heard about Spark\n\"\n),\n(\n0.0\n,\n\"\nI wish Java could use case classes\n\"\n),\n(\n1.0\n,\n\"\nLogistic regression models are neat\n\"\n)\n],\n[\n\"\nlabel\n\"\n,\n\"\nsentence\n\"\n])\ntokenizer\n=\nTokenizer\n(\ninputCol\n=\n\"\nsentence\n\"\n,\noutputCol\n=\n\"\nwords\n\"\n)\nwordsData\n=\ntokenizer\n.\ntransform\n(\nsentenceData\n)\nhashingTF\n=\nHashingTF\n(\ninputCol\n=\n\"\nwords\n\"\n,\noutputCol\n=\n\"\nrawFeatures\n\"\n,\nnumFeatures\n=\n20\n)\nfeaturizedData\n=\nhashingTF\n.\ntransform\n(\nwordsData\n)\n# alternatively, CountVectorizer can also be used to get term frequency vectors\nidf\n=\nIDF\n(\ninputCol\n=\n\"\nrawFeatures\n\"\n,\noutputCol\n=\n\"\nfeatures\n\"\n)\nidfModel\n=\nidf\n.\nfit\n(\nfeaturizedData\n)\nrescaledData\n=\nidfModel\n.\ntransform\n(\nfeaturizedData\n)\nrescaledData\n.\nselect\n(\n\"\nlabel\n\"\n,\n\"\nfeatures\n\"\n).\nshow\n()\nFind full example code at \"examples/src/main/python/ml/tf_idf_example.py\" in the Spark repo.\nRefer to the\nHashingTF Scala docs\nand\nthe\nIDF Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.\n{\nHashingTF\n,\nIDF\n,\nTokenizer\n}\nval\nsentenceData\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0.0\n,\n\"Hi I heard about Spark\"\n),\n(\n0.0\n,\n\"I wish Java could use case classes\"\n),\n(\n1.0\n,\n\"Logistic regression models are neat\"\n)\n)).\ntoDF\n(\n\"label\"\n,\n\"sentence\"\n)\nval\ntokenizer\n=\nnew\nTokenizer\n().\nsetInputCol\n(\n\"sentence\"\n).\nsetOutputCol\n(\n\"words\"\n)\nval\nwordsData\n=\ntokenizer\n.\ntransform\n(\nsentenceData\n)\nval\nhashingTF\n=\nnew\nHashingTF\n()\n.\nsetInputCol\n(\n\"words\"\n).\nsetOutputCol\n(\n\"rawFeatures\"\n).\nsetNumFeatures\n(\n20\n)\nval\nfeaturizedData\n=\nhashingTF\n.\ntransform\n(\nwordsData\n)\n// alternatively, CountVectorizer can also be used to get term frequency vectors\nval\nidf\n=\nnew\nIDF\n().\nsetInputCol\n(\n\"rawFeatures\"\n).\nsetOutputCol\n(\n\"features\"\n)\nval\nidfModel\n=\nidf\n.\nfit\n(\nfeaturizedData\n)\nval\nrescaledData\n=\nidfModel\n.\ntransform\n(\nfeaturizedData\n)\nrescaledData\n.\nselect\n(\n\"label\"\n,\n\"features\"\n).\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\" in the Spark repo.\nRefer to the\nHashingTF Java docs\nand the\nIDF Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.HashingTF\n;\nimport\norg.apache.spark.ml.feature.IDF\n;\nimport\norg.apache.spark.ml.feature.IDFModel\n;\nimport\norg.apache.spark.ml.feature.Tokenizer\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.SparkSession\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n0.0\n,\n\"Hi I heard about Spark\"\n),\nRowFactory\n.\ncreate\n(\n0.0\n,\n\"I wish Java could use case classes\"\n),\nRowFactory\n.\ncreate\n(\n1.0\n,\n\"Logistic regression models are neat\"\n)\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"label\"\n,\nDataTypes\n.\nDoubleType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"sentence\"\n,\nDataTypes\n.\nStringType\n,\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\nsentenceData\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nTokenizer\ntokenizer\n=\nnew\nTokenizer\n().\nsetInputCol\n(\n\"sentence\"\n).\nsetOutputCol\n(\n\"words\"\n);\nDataset\n<\nRow\n>\nwordsData\n=\ntokenizer\n.\ntransform\n(\nsentenceData\n);\nint\nnumFeatures\n=\n20\n;\nHashingTF\nhashingTF\n=\nnew\nHashingTF\n()\n.\nsetInputCol\n(\n\"words\"\n)\n.\nsetOutputCol\n(\n\"rawFeatures\"\n)\n.\nsetNumFeatures\n(\nnumFeatures\n);\nDataset\n<\nRow\n>\nfeaturizedData\n=\nhashingTF\n.\ntransform\n(\nwordsData\n);\n// alternatively, CountVectorizer can also be used to get term frequency vectors\nIDF\nidf\n=\nnew\nIDF\n().\nsetInputCol\n(\n\"rawFeatures\"\n).\nsetOutputCol\n(\n\"features\"\n);\nIDFModel\nidfModel\n=\nidf\n.\nfit\n(\nfeaturizedData\n);\nDataset\n<\nRow\n>\nrescaledData\n=\nidfModel\n.\ntransform\n(\nfeaturizedData\n);\nrescaledData\n.\nselect\n(\n\"label\"\n,\n\"features\"\n).\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\" in the Spark repo.\nWord2Vec\nWord2Vec\nis an\nEstimator\nwhich takes sequences of words representing documents and trains a\nWord2VecModel\n. The model maps each word to a unique fixed-size vector. The\nWord2VecModel\ntransforms each document into a vector using the average of all words in the document; this vector\ncan then be used as features for prediction, document similarity calculations, etc.\nPlease refer to the\nMLlib user guide on Word2Vec\nfor more\ndetails.\nExamples\nIn the following code segment, we start with a set of documents, each of which is represented as a sequence of words. For each document, we transform it into a feature vector. This feature vector could then be passed to a learning algorithm.\nRefer to the\nWord2Vec Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nWord2Vec\n# Input data: Each row is a bag of words from a sentence or document.\ndocumentDF\n=\nspark\n.\ncreateDataFrame\n([\n(\n\"\nHi I heard about Spark\n\"\n.\nsplit\n(\n\"\n\"\n),\n),\n(\n\"\nI wish Java could use case classes\n\"\n.\nsplit\n(\n\"\n\"\n),\n),\n(\n\"\nLogistic regression models are neat\n\"\n.\nsplit\n(\n\"\n\"\n),\n)\n],\n[\n\"\ntext\n\"\n])\n# Learn a mapping from words to Vectors.\nword2Vec\n=\nWord2Vec\n(\nvectorSize\n=\n3\n,\nminCount\n=\n0\n,\ninputCol\n=\n\"\ntext\n\"\n,\noutputCol\n=\n\"\nresult\n\"\n)\nmodel\n=\nword2Vec\n.\nfit\n(\ndocumentDF\n)\nresult\n=\nmodel\n.\ntransform\n(\ndocumentDF\n)\nfor\nrow\nin\nresult\n.\ncollect\n():\ntext\n,\nvector\n=\nrow\nprint\n(\n\"\nText: [%s] =>\n\\n\nVector: %s\n\\n\n\"\n%\n(\n\"\n,\n\"\n.\njoin\n(\ntext\n),\nstr\n(\nvector\n)))\nFind full example code at \"examples/src/main/python/ml/word2vec_example.py\" in the Spark repo.\nRefer to the\nWord2Vec Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.Word2Vec\nimport\norg.apache.spark.ml.linalg.Vector\nimport\norg.apache.spark.sql.Row\n// Input data: Each row is a bag of words from a sentence or document.\nval\ndocumentDF\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n\"Hi I heard about Spark\"\n.\nsplit\n(\n\" \"\n),\n\"I wish Java could use case classes\"\n.\nsplit\n(\n\" \"\n),\n\"Logistic regression models are neat\"\n.\nsplit\n(\n\" \"\n)\n).\nmap\n(\nTuple1\n.\napply\n)).\ntoDF\n(\n\"text\"\n)\n// Learn a mapping from words to Vectors.\nval\nword2Vec\n=\nnew\nWord2Vec\n()\n.\nsetInputCol\n(\n\"text\"\n)\n.\nsetOutputCol\n(\n\"result\"\n)\n.\nsetVectorSize\n(\n3\n)\n.\nsetMinCount\n(\n0\n)\nval\nmodel\n=\nword2Vec\n.\nfit\n(\ndocumentDF\n)\nval\nresult\n=\nmodel\n.\ntransform\n(\ndocumentDF\n)\nresult\n.\ncollect\n().\nforeach\n{\ncase\nRow\n(\ntext\n:\nSeq\n[\n_\n],\nfeatures\n:\nVector\n)\n=>\nprintln\n(\ns\n\"Text: [${text.mkString(\"\n,\n\")}] => \\nVector: $features\\n\"\n)\n}\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\" in the Spark repo.\nRefer to the\nWord2Vec Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.Word2Vec\n;\nimport\norg.apache.spark.ml.feature.Word2VecModel\n;\nimport\norg.apache.spark.ml.linalg.Vector\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.SparkSession\n;\nimport\norg.apache.spark.sql.types.*\n;\n// Input data: Each row is a bag of words from a sentence or document.\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\nArrays\n.\nasList\n(\n\"Hi I heard about Spark\"\n.\nsplit\n(\n\" \"\n))),\nRowFactory\n.\ncreate\n(\nArrays\n.\nasList\n(\n\"I wish Java could use case classes\"\n.\nsplit\n(\n\" \"\n))),\nRowFactory\n.\ncreate\n(\nArrays\n.\nasList\n(\n\"Logistic regression models are neat\"\n.\nsplit\n(\n\" \"\n)))\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"text\"\n,\nnew\nArrayType\n(\nDataTypes\n.\nStringType\n,\ntrue\n),\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndocumentDF\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\n// Learn a mapping from words to Vectors.\nWord2Vec\nword2Vec\n=\nnew\nWord2Vec\n()\n.\nsetInputCol\n(\n\"text\"\n)\n.\nsetOutputCol\n(\n\"result\"\n)\n.\nsetVectorSize\n(\n3\n)\n.\nsetMinCount\n(\n0\n);\nWord2VecModel\nmodel\n=\nword2Vec\n.\nfit\n(\ndocumentDF\n);\nDataset\n<\nRow\n>\nresult\n=\nmodel\n.\ntransform\n(\ndocumentDF\n);\nfor\n(\nRow\nrow\n:\nresult\n.\ncollectAsList\n())\n{\nList\n<\nString\n>\ntext\n=\nrow\n.\ngetList\n(\n0\n);\nVector\nvector\n=\n(\nVector\n)\nrow\n.\nget\n(\n1\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Text: \"\n+\ntext\n+\n\" => \\nVector: \"\n+\nvector\n+\n\"\\n\"\n);\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\" in the Spark repo.\nCountVectorizer\nCountVectorizer\nand\nCountVectorizerModel\naim to help convert a collection of text documents\n to vectors of token counts. When an a-priori dictionary is not available,\nCountVectorizer\ncan\n be used as an\nEstimator\nto extract the vocabulary, and generates a\nCountVectorizerModel\n. The\n model produces sparse representations for the documents over the vocabulary, which can then be\n passed to other algorithms like LDA.\nDuring the fitting process,\nCountVectorizer\nwill select the top\nvocabSize\nwords ordered by\n term frequency across the corpus. An optional parameter\nminDF\nalso affects the fitting process\n by specifying the minimum number (or fraction if < 1.0) of documents a term must appear in to be\n included in the vocabulary. Another optional binary toggle parameter controls the output vector.\n If set to true all nonzero counts are set to 1. This is especially useful for discrete probabilistic\n models that model binary, rather than integer, counts.\nExamples\nAssume that we have the following DataFrame with columns\nid\nand\ntexts\n:\nid | texts\n----|----------\n 0  | Array(\"a\", \"b\", \"c\")\n 1  | Array(\"a\", \"b\", \"b\", \"c\", \"a\")\neach row in\ntexts\nis a document of type Array[String].\nInvoking fit of\nCountVectorizer\nproduces a\nCountVectorizerModel\nwith vocabulary (a, b, c).\nThen the output column “vector” after transformation contains:\nid | texts                           | vector\n----|---------------------------------|---------------\n 0  | Array(\"a\", \"b\", \"c\")            | (3,[0,1,2],[1.0,1.0,1.0])\n 1  | Array(\"a\", \"b\", \"b\", \"c\", \"a\")  | (3,[0,1,2],[2.0,2.0,1.0])\nEach vector represents the token counts of the document over the vocabulary.\nRefer to the\nCountVectorizer Python docs\nand the\nCountVectorizerModel Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nCountVectorizer\n# Input data: Each row is a bag of words with a ID.\ndf\n=\nspark\n.\ncreateDataFrame\n([\n(\n0\n,\n\"\na b c\n\"\n.\nsplit\n(\n\"\n\"\n)),\n(\n1\n,\n\"\na b b c a\n\"\n.\nsplit\n(\n\"\n\"\n))\n],\n[\n\"\nid\n\"\n,\n\"\nwords\n\"\n])\n# fit a CountVectorizerModel from the corpus.\ncv\n=\nCountVectorizer\n(\ninputCol\n=\n\"\nwords\n\"\n,\noutputCol\n=\n\"\nfeatures\n\"\n,\nvocabSize\n=\n3\n,\nminDF\n=\n2.0\n)\nmodel\n=\ncv\n.\nfit\n(\ndf\n)\nresult\n=\nmodel\n.\ntransform\n(\ndf\n)\nresult\n.\nshow\n(\ntruncate\n=\nFalse\n)\nFind full example code at \"examples/src/main/python/ml/count_vectorizer_example.py\" in the Spark repo.\nRefer to the\nCountVectorizer Scala docs\nand the\nCountVectorizerModel Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.\n{\nCountVectorizer\n,\nCountVectorizerModel\n}\nval\ndf\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0\n,\nArray\n(\n\"a\"\n,\n\"b\"\n,\n\"c\"\n)),\n(\n1\n,\nArray\n(\n\"a\"\n,\n\"b\"\n,\n\"b\"\n,\n\"c\"\n,\n\"a\"\n))\n)).\ntoDF\n(\n\"id\"\n,\n\"words\"\n)\n// fit a CountVectorizerModel from the corpus\nval\ncvModel\n:\nCountVectorizerModel\n=\nnew\nCountVectorizer\n()\n.\nsetInputCol\n(\n\"words\"\n)\n.\nsetOutputCol\n(\n\"features\"\n)\n.\nsetVocabSize\n(\n3\n)\n.\nsetMinDF\n(\n2\n)\n.\nfit\n(\ndf\n)\n// alternatively, define CountVectorizerModel with a-priori vocabulary\nval\ncvm\n=\nnew\nCountVectorizerModel\n(\nArray\n(\n\"a\"\n,\n\"b\"\n,\n\"c\"\n))\n.\nsetInputCol\n(\n\"words\"\n)\n.\nsetOutputCol\n(\n\"features\"\n)\ncvModel\n.\ntransform\n(\ndf\n).\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\" in the Spark repo.\nRefer to the\nCountVectorizer Java docs\nand the\nCountVectorizerModel Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.CountVectorizer\n;\nimport\norg.apache.spark.ml.feature.CountVectorizerModel\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.SparkSession\n;\nimport\norg.apache.spark.sql.types.*\n;\n// Input data: Each row is a bag of words from a sentence or document.\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\nArrays\n.\nasList\n(\n\"a\"\n,\n\"b\"\n,\n\"c\"\n)),\nRowFactory\n.\ncreate\n(\nArrays\n.\nasList\n(\n\"a\"\n,\n\"b\"\n,\n\"b\"\n,\n\"c\"\n,\n\"a\"\n))\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]\n{\nnew\nStructField\n(\n\"text\"\n,\nnew\nArrayType\n(\nDataTypes\n.\nStringType\n,\ntrue\n),\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\n// fit a CountVectorizerModel from the corpus\nCountVectorizerModel\ncvModel\n=\nnew\nCountVectorizer\n()\n.\nsetInputCol\n(\n\"text\"\n)\n.\nsetOutputCol\n(\n\"feature\"\n)\n.\nsetVocabSize\n(\n3\n)\n.\nsetMinDF\n(\n2\n)\n.\nfit\n(\ndf\n);\n// alternatively, define CountVectorizerModel with a-priori vocabulary\nCountVectorizerModel\ncvm\n=\nnew\nCountVectorizerModel\n(\nnew\nString\n[]{\n\"a\"\n,\n\"b\"\n,\n\"c\"\n})\n.\nsetInputCol\n(\n\"text\"\n)\n.\nsetOutputCol\n(\n\"feature\"\n);\ncvModel\n.\ntransform\n(\ndf\n).\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\" in the Spark repo.\nFeatureHasher\nFeature hashing projects a set of categorical or numerical features into a feature vector of\nspecified dimension (typically substantially smaller than that of the original feature\nspace). This is done using the\nhashing trick\nto map features to indices in the feature vector.\nThe\nFeatureHasher\ntransformer operates on multiple columns. Each column may contain either\nnumeric or categorical features. Behavior and handling of column data types is as follows:\nNumeric columns: For numeric features, the hash value of the column name is used to map the\nfeature value to its index in the feature vector. By default, numeric features are not treated\nas categorical (even when they are integers). To treat them as categorical, specify the relevant\ncolumns using the\ncategoricalCols\nparameter.\nString columns: For categorical features, the hash value of the string “column_name=value”\nis used to map to the vector index, with an indicator value of\n1.0\n. Thus, categorical features\nare “one-hot” encoded (similarly to using\nOneHotEncoder\nwith\ndropLast=false\n).\nBoolean columns: Boolean values are treated in the same way as string columns. That is,\nboolean features are represented as “column_name=true” or “column_name=false”, with an indicator\nvalue of\n1.0\n.\nNull (missing) values are ignored (implicitly zero in the resulting feature vector).\nThe hash function used here is also the\nMurmurHash 3\nused in\nHashingTF\n. Since a simple modulo on the hashed value is used to\ndetermine the vector index, it is advisable to use a power of two as the numFeatures parameter;\notherwise the features will not be mapped evenly to the vector indices.\nExamples\nAssume that we have a DataFrame with 4 input columns\nreal\n,\nbool\n,\nstringNum\n, and\nstring\n.\nThese different data types as input will illustrate the behavior of the transform to produce a\ncolumn of feature vectors.\nreal| bool|stringNum|string\n----|-----|---------|------\n 2.2| true|        1|   foo\n 3.3|false|        2|   bar\n 4.4|false|        3|   baz\n 5.5|false|        4|   foo\nThen the output of\nFeatureHasher.transform\non this DataFrame is:\nreal|bool |stringNum|string|features\n----|-----|---------|------|-------------------------------------------------------\n2.2 |true |1        |foo   |(262144,[51871, 63643,174475,253195],[1.0,1.0,2.2,1.0])\n3.3 |false|2        |bar   |(262144,[6031,  80619,140467,174475],[1.0,1.0,1.0,3.3])\n4.4 |false|3        |baz   |(262144,[24279,140467,174475,196810],[1.0,1.0,4.4,1.0])\n5.5 |false|4        |foo   |(262144,[63643,140467,168512,174475],[1.0,1.0,1.0,5.5])\nThe resulting feature vectors could then be passed to a learning algorithm.\nRefer to the\nFeatureHasher Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nFeatureHasher\ndataset\n=\nspark\n.\ncreateDataFrame\n([\n(\n2.2\n,\nTrue\n,\n\"\n1\n\"\n,\n\"\nfoo\n\"\n),\n(\n3.3\n,\nFalse\n,\n\"\n2\n\"\n,\n\"\nbar\n\"\n),\n(\n4.4\n,\nFalse\n,\n\"\n3\n\"\n,\n\"\nbaz\n\"\n),\n(\n5.5\n,\nFalse\n,\n\"\n4\n\"\n,\n\"\nfoo\n\"\n)\n],\n[\n\"\nreal\n\"\n,\n\"\nbool\n\"\n,\n\"\nstringNum\n\"\n,\n\"\nstring\n\"\n])\nhasher\n=\nFeatureHasher\n(\ninputCols\n=\n[\n\"\nreal\n\"\n,\n\"\nbool\n\"\n,\n\"\nstringNum\n\"\n,\n\"\nstring\n\"\n],\noutputCol\n=\n\"\nfeatures\n\"\n)\nfeaturized\n=\nhasher\n.\ntransform\n(\ndataset\n)\nfeaturized\n.\nshow\n(\ntruncate\n=\nFalse\n)\nFind full example code at \"examples/src/main/python/ml/feature_hasher_example.py\" in the Spark repo.\nRefer to the\nFeatureHasher Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.FeatureHasher\nval\ndataset\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n2.2\n,\ntrue\n,\n\"1\"\n,\n\"foo\"\n),\n(\n3.3\n,\nfalse\n,\n\"2\"\n,\n\"bar\"\n),\n(\n4.4\n,\nfalse\n,\n\"3\"\n,\n\"baz\"\n),\n(\n5.5\n,\nfalse\n,\n\"4\"\n,\n\"foo\"\n)\n)).\ntoDF\n(\n\"real\"\n,\n\"bool\"\n,\n\"stringNum\"\n,\n\"string\"\n)\nval\nhasher\n=\nnew\nFeatureHasher\n()\n.\nsetInputCols\n(\n\"real\"\n,\n\"bool\"\n,\n\"stringNum\"\n,\n\"string\"\n)\n.\nsetOutputCol\n(\n\"features\"\n)\nval\nfeaturized\n=\nhasher\n.\ntransform\n(\ndataset\n)\nfeaturized\n.\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/FeatureHasherExample.scala\" in the Spark repo.\nRefer to the\nFeatureHasher Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.FeatureHasher\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n2.2\n,\ntrue\n,\n\"1\"\n,\n\"foo\"\n),\nRowFactory\n.\ncreate\n(\n3.3\n,\nfalse\n,\n\"2\"\n,\n\"bar\"\n),\nRowFactory\n.\ncreate\n(\n4.4\n,\nfalse\n,\n\"3\"\n,\n\"baz\"\n),\nRowFactory\n.\ncreate\n(\n5.5\n,\nfalse\n,\n\"4\"\n,\n\"foo\"\n)\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"real\"\n,\nDataTypes\n.\nDoubleType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"bool\"\n,\nDataTypes\n.\nBooleanType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"stringNum\"\n,\nDataTypes\n.\nStringType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"string\"\n,\nDataTypes\n.\nStringType\n,\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndataset\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nFeatureHasher\nhasher\n=\nnew\nFeatureHasher\n()\n.\nsetInputCols\n(\nnew\nString\n[]{\n\"real\"\n,\n\"bool\"\n,\n\"stringNum\"\n,\n\"string\"\n})\n.\nsetOutputCol\n(\n\"features\"\n);\nDataset\n<\nRow\n>\nfeaturized\n=\nhasher\n.\ntransform\n(\ndataset\n);\nfeaturized\n.\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaFeatureHasherExample.java\" in the Spark repo.\nFeature Transformers\nTokenizer\nTokenization\nis the process of taking text (such as a sentence) and breaking it into individual terms (usually words).  A simple\nTokenizer\nclass provides this functionality.  The example below shows how to split sentences into sequences of words.\nRegexTokenizer\nallows more\n advanced tokenization based on regular expression (regex) matching.\n By default, the parameter “pattern” (regex, default:\n\"\\\\s+\"\n) is used as delimiters to split the input text.\n Alternatively, users can set parameter “gaps” to false indicating the regex “pattern” denotes\n “tokens” rather than splitting gaps, and find all matching occurrences as the tokenization result.\nExamples\nRefer to the\nTokenizer Python docs\nand\nthe\nRegexTokenizer Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nTokenizer\n,\nRegexTokenizer\nfrom\npyspark.sql.functions\nimport\ncol\n,\nudf\nfrom\npyspark.sql.types\nimport\nIntegerType\nsentenceDataFrame\n=\nspark\n.\ncreateDataFrame\n([\n(\n0\n,\n\"\nHi I heard about Spark\n\"\n),\n(\n1\n,\n\"\nI wish Java could use case classes\n\"\n),\n(\n2\n,\n\"\nLogistic,regression,models,are,neat\n\"\n)\n],\n[\n\"\nid\n\"\n,\n\"\nsentence\n\"\n])\ntokenizer\n=\nTokenizer\n(\ninputCol\n=\n\"\nsentence\n\"\n,\noutputCol\n=\n\"\nwords\n\"\n)\nregexTokenizer\n=\nRegexTokenizer\n(\ninputCol\n=\n\"\nsentence\n\"\n,\noutputCol\n=\n\"\nwords\n\"\n,\npattern\n=\n\"\n\\\\\nW\n\"\n)\n# alternatively, pattern=\"\\\\w+\", gaps(False)\ncountTokens\n=\nudf\n(\nlambda\nwords\n:\nlen\n(\nwords\n),\nIntegerType\n())\ntokenized\n=\ntokenizer\n.\ntransform\n(\nsentenceDataFrame\n)\ntokenized\n.\nselect\n(\n\"\nsentence\n\"\n,\n\"\nwords\n\"\n)\n\\\n.\nwithColumn\n(\n\"\ntokens\n\"\n,\ncountTokens\n(\ncol\n(\n\"\nwords\n\"\n))).\nshow\n(\ntruncate\n=\nFalse\n)\nregexTokenized\n=\nregexTokenizer\n.\ntransform\n(\nsentenceDataFrame\n)\nregexTokenized\n.\nselect\n(\n\"\nsentence\n\"\n,\n\"\nwords\n\"\n)\n\\\n.\nwithColumn\n(\n\"\ntokens\n\"\n,\ncountTokens\n(\ncol\n(\n\"\nwords\n\"\n))).\nshow\n(\ntruncate\n=\nFalse\n)\nFind full example code at \"examples/src/main/python/ml/tokenizer_example.py\" in the Spark repo.\nRefer to the\nTokenizer Scala docs\nand the\nRegexTokenizer Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.\n{\nRegexTokenizer\n,\nTokenizer\n}\nimport\norg.apache.spark.sql.SparkSession\nimport\norg.apache.spark.sql.functions._\nval\nsentenceDataFrame\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0\n,\n\"Hi I heard about Spark\"\n),\n(\n1\n,\n\"I wish Java could use case classes\"\n),\n(\n2\n,\n\"Logistic,regression,models,are,neat\"\n)\n)).\ntoDF\n(\n\"id\"\n,\n\"sentence\"\n)\nval\ntokenizer\n=\nnew\nTokenizer\n().\nsetInputCol\n(\n\"sentence\"\n).\nsetOutputCol\n(\n\"words\"\n)\nval\nregexTokenizer\n=\nnew\nRegexTokenizer\n()\n.\nsetInputCol\n(\n\"sentence\"\n)\n.\nsetOutputCol\n(\n\"words\"\n)\n.\nsetPattern\n(\n\"\\\\W\"\n)\n// alternatively .setPattern(\"\\\\w+\").setGaps(false)\nval\ncountTokens\n=\nudf\n{\n(\nwords\n:\nSeq\n[\nString\n])\n=>\nwords\n.\nlength\n}\nval\ntokenized\n=\ntokenizer\n.\ntransform\n(\nsentenceDataFrame\n)\ntokenized\n.\nselect\n(\n\"sentence\"\n,\n\"words\"\n)\n.\nwithColumn\n(\n\"tokens\"\n,\ncountTokens\n(\ncol\n(\n\"words\"\n))).\nshow\n(\nfalse\n)\nval\nregexTokenized\n=\nregexTokenizer\n.\ntransform\n(\nsentenceDataFrame\n)\nregexTokenized\n.\nselect\n(\n\"sentence\"\n,\n\"words\"\n)\n.\nwithColumn\n(\n\"tokens\"\n,\ncountTokens\n(\ncol\n(\n\"words\"\n))).\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\" in the Spark repo.\nRefer to the\nTokenizer Java docs\nand the\nRegexTokenizer Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\nscala.collection.mutable.Seq\n;\nimport\norg.apache.spark.ml.feature.RegexTokenizer\n;\nimport\norg.apache.spark.ml.feature.Tokenizer\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\n// col(\"...\") is preferable to df.col(\"...\")\nimport\nstatic\norg\n.\napache\n.\nspark\n.\nsql\n.\nfunctions\n.\ncall_udf\n;\nimport\nstatic\norg\n.\napache\n.\nspark\n.\nsql\n.\nfunctions\n.\ncol\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n0\n,\n\"Hi I heard about Spark\"\n),\nRowFactory\n.\ncreate\n(\n1\n,\n\"I wish Java could use case classes\"\n),\nRowFactory\n.\ncreate\n(\n2\n,\n\"Logistic,regression,models,are,neat\"\n)\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"id\"\n,\nDataTypes\n.\nIntegerType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"sentence\"\n,\nDataTypes\n.\nStringType\n,\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\nsentenceDataFrame\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nTokenizer\ntokenizer\n=\nnew\nTokenizer\n().\nsetInputCol\n(\n\"sentence\"\n).\nsetOutputCol\n(\n\"words\"\n);\nRegexTokenizer\nregexTokenizer\n=\nnew\nRegexTokenizer\n()\n.\nsetInputCol\n(\n\"sentence\"\n)\n.\nsetOutputCol\n(\n\"words\"\n)\n.\nsetPattern\n(\n\"\\\\W\"\n);\n// alternatively .setPattern(\"\\\\w+\").setGaps(false);\nspark\n.\nudf\n().\nregister\n(\n\"countTokens\"\n,\n(\nSeq\n<?>\nwords\n)\n->\nwords\n.\nsize\n(),\nDataTypes\n.\nIntegerType\n);\nDataset\n<\nRow\n>\ntokenized\n=\ntokenizer\n.\ntransform\n(\nsentenceDataFrame\n);\ntokenized\n.\nselect\n(\n\"sentence\"\n,\n\"words\"\n)\n.\nwithColumn\n(\n\"tokens\"\n,\ncall_udf\n(\n\"countTokens\"\n,\ncol\n(\n\"words\"\n)))\n.\nshow\n(\nfalse\n);\nDataset\n<\nRow\n>\nregexTokenized\n=\nregexTokenizer\n.\ntransform\n(\nsentenceDataFrame\n);\nregexTokenized\n.\nselect\n(\n\"sentence\"\n,\n\"words\"\n)\n.\nwithColumn\n(\n\"tokens\"\n,\ncall_udf\n(\n\"countTokens\"\n,\ncol\n(\n\"words\"\n)))\n.\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\" in the Spark repo.\nStopWordsRemover\nStop words\nare words which\nshould be excluded from the input, typically because the words appear\nfrequently and don’t carry as much meaning.\nStopWordsRemover\ntakes as input a sequence of strings (e.g. the output\nof a\nTokenizer\n) and drops all the stop\nwords from the input sequences. The list of stopwords is specified by\nthe\nstopWords\nparameter. Default stop words for some languages are accessible \nby calling\nStopWordsRemover.loadDefaultStopWords(language)\n, for which available \noptions are “danish”, “dutch”, “english”, “finnish”, “french”, “german”, “hungarian”, \n“italian”, “norwegian”, “portuguese”, “russian”, “spanish”, “swedish” and “turkish”. \nA boolean parameter\ncaseSensitive\nindicates if the matches should be case sensitive \n(false by default).\nExamples\nAssume that we have the following DataFrame with columns\nid\nand\nraw\n:\nid | raw\n----|----------\n 0  | [I, saw, the, red, balloon]\n 1  | [Mary, had, a, little, lamb]\nApplying\nStopWordsRemover\nwith\nraw\nas the input column and\nfiltered\nas the output\ncolumn, we should get the following:\nid | raw                         | filtered\n----|-----------------------------|--------------------\n 0  | [I, saw, the, red, balloon]  |  [saw, red, balloon]\n 1  | [Mary, had, a, little, lamb]|[Mary, little, lamb]\nIn\nfiltered\n, the stop words “I”, “the”, “had”, and “a” have been\nfiltered out.\nRefer to the\nStopWordsRemover Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nStopWordsRemover\nsentenceData\n=\nspark\n.\ncreateDataFrame\n([\n(\n0\n,\n[\n\"\nI\n\"\n,\n\"\nsaw\n\"\n,\n\"\nthe\n\"\n,\n\"\nred\n\"\n,\n\"\nballoon\n\"\n]),\n(\n1\n,\n[\n\"\nMary\n\"\n,\n\"\nhad\n\"\n,\n\"\na\n\"\n,\n\"\nlittle\n\"\n,\n\"\nlamb\n\"\n])\n],\n[\n\"\nid\n\"\n,\n\"\nraw\n\"\n])\nremover\n=\nStopWordsRemover\n(\ninputCol\n=\n\"\nraw\n\"\n,\noutputCol\n=\n\"\nfiltered\n\"\n)\nremover\n.\ntransform\n(\nsentenceData\n).\nshow\n(\ntruncate\n=\nFalse\n)\nFind full example code at \"examples/src/main/python/ml/stopwords_remover_example.py\" in the Spark repo.\nRefer to the\nStopWordsRemover Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.StopWordsRemover\nval\nremover\n=\nnew\nStopWordsRemover\n()\n.\nsetInputCol\n(\n\"raw\"\n)\n.\nsetOutputCol\n(\n\"filtered\"\n)\nval\ndataSet\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0\n,\nSeq\n(\n\"I\"\n,\n\"saw\"\n,\n\"the\"\n,\n\"red\"\n,\n\"balloon\"\n)),\n(\n1\n,\nSeq\n(\n\"Mary\"\n,\n\"had\"\n,\n\"a\"\n,\n\"little\"\n,\n\"lamb\"\n))\n)).\ntoDF\n(\n\"id\"\n,\n\"raw\"\n)\nremover\n.\ntransform\n(\ndataSet\n).\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\" in the Spark repo.\nRefer to the\nStopWordsRemover Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.StopWordsRemover\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nStopWordsRemover\nremover\n=\nnew\nStopWordsRemover\n()\n.\nsetInputCol\n(\n\"raw\"\n)\n.\nsetOutputCol\n(\n\"filtered\"\n);\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\nArrays\n.\nasList\n(\n\"I\"\n,\n\"saw\"\n,\n\"the\"\n,\n\"red\"\n,\n\"balloon\"\n)),\nRowFactory\n.\ncreate\n(\nArrays\n.\nasList\n(\n\"Mary\"\n,\n\"had\"\n,\n\"a\"\n,\n\"little\"\n,\n\"lamb\"\n))\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"raw\"\n,\nDataTypes\n.\ncreateArrayType\n(\nDataTypes\n.\nStringType\n),\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndataset\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nremover\n.\ntransform\n(\ndataset\n).\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\" in the Spark repo.\n$n$-gram\nAn\nn-gram\nis a sequence of $n$ tokens (typically words) for some integer $n$. The\nNGram\nclass can be used to transform input features into $n$-grams.\nNGram\ntakes as input a sequence of strings (e.g. the output of a\nTokenizer\n).  The parameter\nn\nis used to determine the number of terms in each $n$-gram. The output will consist of a sequence of $n$-grams where each $n$-gram is represented by a space-delimited string of $n$ consecutive words.  If the input sequence contains fewer than\nn\nstrings, no output is produced.\nExamples\nRefer to the\nNGram Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nNGram\nwordDataFrame\n=\nspark\n.\ncreateDataFrame\n([\n(\n0\n,\n[\n\"\nHi\n\"\n,\n\"\nI\n\"\n,\n\"\nheard\n\"\n,\n\"\nabout\n\"\n,\n\"\nSpark\n\"\n]),\n(\n1\n,\n[\n\"\nI\n\"\n,\n\"\nwish\n\"\n,\n\"\nJava\n\"\n,\n\"\ncould\n\"\n,\n\"\nuse\n\"\n,\n\"\ncase\n\"\n,\n\"\nclasses\n\"\n]),\n(\n2\n,\n[\n\"\nLogistic\n\"\n,\n\"\nregression\n\"\n,\n\"\nmodels\n\"\n,\n\"\nare\n\"\n,\n\"\nneat\n\"\n])\n],\n[\n\"\nid\n\"\n,\n\"\nwords\n\"\n])\nngram\n=\nNGram\n(\nn\n=\n2\n,\ninputCol\n=\n\"\nwords\n\"\n,\noutputCol\n=\n\"\nngrams\n\"\n)\nngramDataFrame\n=\nngram\n.\ntransform\n(\nwordDataFrame\n)\nngramDataFrame\n.\nselect\n(\n\"\nngrams\n\"\n).\nshow\n(\ntruncate\n=\nFalse\n)\nFind full example code at \"examples/src/main/python/ml/n_gram_example.py\" in the Spark repo.\nRefer to the\nNGram Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.NGram\nval\nwordDataFrame\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0\n,\nArray\n(\n\"Hi\"\n,\n\"I\"\n,\n\"heard\"\n,\n\"about\"\n,\n\"Spark\"\n)),\n(\n1\n,\nArray\n(\n\"I\"\n,\n\"wish\"\n,\n\"Java\"\n,\n\"could\"\n,\n\"use\"\n,\n\"case\"\n,\n\"classes\"\n)),\n(\n2\n,\nArray\n(\n\"Logistic\"\n,\n\"regression\"\n,\n\"models\"\n,\n\"are\"\n,\n\"neat\"\n))\n)).\ntoDF\n(\n\"id\"\n,\n\"words\"\n)\nval\nngram\n=\nnew\nNGram\n().\nsetN\n(\n2\n).\nsetInputCol\n(\n\"words\"\n).\nsetOutputCol\n(\n\"ngrams\"\n)\nval\nngramDataFrame\n=\nngram\n.\ntransform\n(\nwordDataFrame\n)\nngramDataFrame\n.\nselect\n(\n\"ngrams\"\n).\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\" in the Spark repo.\nRefer to the\nNGram Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.NGram\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n0\n,\nArrays\n.\nasList\n(\n\"Hi\"\n,\n\"I\"\n,\n\"heard\"\n,\n\"about\"\n,\n\"Spark\"\n)),\nRowFactory\n.\ncreate\n(\n1\n,\nArrays\n.\nasList\n(\n\"I\"\n,\n\"wish\"\n,\n\"Java\"\n,\n\"could\"\n,\n\"use\"\n,\n\"case\"\n,\n\"classes\"\n)),\nRowFactory\n.\ncreate\n(\n2\n,\nArrays\n.\nasList\n(\n\"Logistic\"\n,\n\"regression\"\n,\n\"models\"\n,\n\"are\"\n,\n\"neat\"\n))\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"id\"\n,\nDataTypes\n.\nIntegerType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"words\"\n,\nDataTypes\n.\ncreateArrayType\n(\nDataTypes\n.\nStringType\n),\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\nwordDataFrame\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nNGram\nngramTransformer\n=\nnew\nNGram\n().\nsetN\n(\n2\n).\nsetInputCol\n(\n\"words\"\n).\nsetOutputCol\n(\n\"ngrams\"\n);\nDataset\n<\nRow\n>\nngramDataFrame\n=\nngramTransformer\n.\ntransform\n(\nwordDataFrame\n);\nngramDataFrame\n.\nselect\n(\n\"ngrams\"\n).\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\" in the Spark repo.\nBinarizer\nBinarization is the process of thresholding numerical features to binary (0/1) features.\nBinarizer\ntakes the common parameters\ninputCol\nand\noutputCol\n, as well as the\nthreshold\nfor binarization. Feature values greater than the threshold are binarized to 1.0; values equal\nto or less than the threshold are binarized to 0.0. Both Vector and Double types are supported\nfor\ninputCol\n.\nExamples\nRefer to the\nBinarizer Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nBinarizer\ncontinuousDataFrame\n=\nspark\n.\ncreateDataFrame\n([\n(\n0\n,\n0.1\n),\n(\n1\n,\n0.8\n),\n(\n2\n,\n0.2\n)\n],\n[\n\"\nid\n\"\n,\n\"\nfeature\n\"\n])\nbinarizer\n=\nBinarizer\n(\nthreshold\n=\n0.5\n,\ninputCol\n=\n\"\nfeature\n\"\n,\noutputCol\n=\n\"\nbinarized_feature\n\"\n)\nbinarizedDataFrame\n=\nbinarizer\n.\ntransform\n(\ncontinuousDataFrame\n)\nprint\n(\n\"\nBinarizer output with Threshold = %f\n\"\n%\nbinarizer\n.\ngetThreshold\n())\nbinarizedDataFrame\n.\nshow\n()\nFind full example code at \"examples/src/main/python/ml/binarizer_example.py\" in the Spark repo.\nRefer to the\nBinarizer Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.Binarizer\nval\ndata\n=\nimmutable\n.\nArraySeq\n.\nunsafeWrapArray\n(\nArray\n((\n0\n,\n0.1\n),\n(\n1\n,\n0.8\n),\n(\n2\n,\n0.2\n)))\nval\ndataFrame\n=\nspark\n.\ncreateDataFrame\n(\ndata\n).\ntoDF\n(\n\"id\"\n,\n\"feature\"\n)\nval\nbinarizer\n:\nBinarizer\n=\nnew\nBinarizer\n()\n.\nsetInputCol\n(\n\"feature\"\n)\n.\nsetOutputCol\n(\n\"binarized_feature\"\n)\n.\nsetThreshold\n(\n0.5\n)\nval\nbinarizedDataFrame\n=\nbinarizer\n.\ntransform\n(\ndataFrame\n)\nprintln\n(\ns\n\"Binarizer output with Threshold = ${binarizer.getThreshold}\"\n)\nbinarizedDataFrame\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\" in the Spark repo.\nRefer to the\nBinarizer Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.Binarizer\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n0\n,\n0.1\n),\nRowFactory\n.\ncreate\n(\n1\n,\n0.8\n),\nRowFactory\n.\ncreate\n(\n2\n,\n0.2\n)\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"id\"\n,\nDataTypes\n.\nIntegerType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"feature\"\n,\nDataTypes\n.\nDoubleType\n,\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ncontinuousDataFrame\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nBinarizer\nbinarizer\n=\nnew\nBinarizer\n()\n.\nsetInputCol\n(\n\"feature\"\n)\n.\nsetOutputCol\n(\n\"binarized_feature\"\n)\n.\nsetThreshold\n(\n0.5\n);\nDataset\n<\nRow\n>\nbinarizedDataFrame\n=\nbinarizer\n.\ntransform\n(\ncontinuousDataFrame\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Binarizer output with Threshold = \"\n+\nbinarizer\n.\ngetThreshold\n());\nbinarizedDataFrame\n.\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\" in the Spark repo.\nPCA\nPCA\nis a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. A\nPCA\nclass trains a model to project vectors to a low-dimensional space using PCA. The example below shows how to project 5-dimensional feature vectors into 3-dimensional principal components.\nExamples\nRefer to the\nPCA Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nPCA\nfrom\npyspark.ml.linalg\nimport\nVectors\ndata\n=\n[(\nVectors\n.\nsparse\n(\n5\n,\n[(\n1\n,\n1.0\n),\n(\n3\n,\n7.0\n)]),),\n(\nVectors\n.\ndense\n([\n2.0\n,\n0.0\n,\n3.0\n,\n4.0\n,\n5.0\n]),),\n(\nVectors\n.\ndense\n([\n4.0\n,\n0.0\n,\n0.0\n,\n6.0\n,\n7.0\n]),)]\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\n[\n\"\nfeatures\n\"\n])\npca\n=\nPCA\n(\nk\n=\n3\n,\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\npcaFeatures\n\"\n)\nmodel\n=\npca\n.\nfit\n(\ndf\n)\nresult\n=\nmodel\n.\ntransform\n(\ndf\n).\nselect\n(\n\"\npcaFeatures\n\"\n)\nresult\n.\nshow\n(\ntruncate\n=\nFalse\n)\nFind full example code at \"examples/src/main/python/ml/pca_example.py\" in the Spark repo.\nRefer to the\nPCA Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.PCA\nimport\norg.apache.spark.ml.linalg.Vectors\nval\ndata\n=\nArray\n(\nVectors\n.\nsparse\n(\n5\n,\nSeq\n((\n1\n,\n1.0\n),\n(\n3\n,\n7.0\n))),\nVectors\n.\ndense\n(\n2.0\n,\n0.0\n,\n3.0\n,\n4.0\n,\n5.0\n),\nVectors\n.\ndense\n(\n4.0\n,\n0.0\n,\n0.0\n,\n6.0\n,\n7.0\n)\n)\nval\ndf\n=\nspark\n.\ncreateDataFrame\n(\nimmutable\n.\nArraySeq\n.\nunsafeWrapArray\n(\ndata\n.\nmap\n(\nTuple1\n.\napply\n)))\n.\ntoDF\n(\n\"features\"\n)\nval\npca\n=\nnew\nPCA\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"pcaFeatures\"\n)\n.\nsetK\n(\n3\n)\n.\nfit\n(\ndf\n)\nval\nresult\n=\npca\n.\ntransform\n(\ndf\n).\nselect\n(\n\"pcaFeatures\"\n)\nresult\n.\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\" in the Spark repo.\nRefer to the\nPCA Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.PCA\n;\nimport\norg.apache.spark.ml.feature.PCAModel\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\nVectors\n.\nsparse\n(\n5\n,\nnew\nint\n[]{\n1\n,\n3\n},\nnew\ndouble\n[]{\n1.0\n,\n7.0\n})),\nRowFactory\n.\ncreate\n(\nVectors\n.\ndense\n(\n2.0\n,\n0.0\n,\n3.0\n,\n4.0\n,\n5.0\n)),\nRowFactory\n.\ncreate\n(\nVectors\n.\ndense\n(\n4.0\n,\n0.0\n,\n0.0\n,\n6.0\n,\n7.0\n))\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"features\"\n,\nnew\nVectorUDT\n(),\nfalse\n,\nMetadata\n.\nempty\n()),\n});\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nPCAModel\npca\n=\nnew\nPCA\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"pcaFeatures\"\n)\n.\nsetK\n(\n3\n)\n.\nfit\n(\ndf\n);\nDataset\n<\nRow\n>\nresult\n=\npca\n.\ntransform\n(\ndf\n).\nselect\n(\n\"pcaFeatures\"\n);\nresult\n.\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\" in the Spark repo.\nPolynomialExpansion\nPolynomial expansion\nis the process of expanding your features into a polynomial space, which is formulated by an n-degree combination of original dimensions. A\nPolynomialExpansion\nclass provides this functionality.  The example below shows how to expand your features into a 3-degree polynomial space.\nExamples\nRefer to the\nPolynomialExpansion Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nPolynomialExpansion\nfrom\npyspark.ml.linalg\nimport\nVectors\ndf\n=\nspark\n.\ncreateDataFrame\n([\n(\nVectors\n.\ndense\n([\n2.0\n,\n1.0\n]),),\n(\nVectors\n.\ndense\n([\n0.0\n,\n0.0\n]),),\n(\nVectors\n.\ndense\n([\n3.0\n,\n-\n1.0\n]),)\n],\n[\n\"\nfeatures\n\"\n])\npolyExpansion\n=\nPolynomialExpansion\n(\ndegree\n=\n3\n,\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\npolyFeatures\n\"\n)\npolyDF\n=\npolyExpansion\n.\ntransform\n(\ndf\n)\npolyDF\n.\nshow\n(\ntruncate\n=\nFalse\n)\nFind full example code at \"examples/src/main/python/ml/polynomial_expansion_example.py\" in the Spark repo.\nRefer to the\nPolynomialExpansion Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.PolynomialExpansion\nimport\norg.apache.spark.ml.linalg.Vectors\nval\ndata\n=\nArray\n(\nVectors\n.\ndense\n(\n2.0\n,\n1.0\n),\nVectors\n.\ndense\n(\n0.0\n,\n0.0\n),\nVectors\n.\ndense\n(\n3.0\n,\n-\n1.0\n)\n)\nval\ndf\n=\nspark\n.\ncreateDataFrame\n(\nimmutable\n.\nArraySeq\n.\nunsafeWrapArray\n(\ndata\n.\nmap\n(\nTuple1\n.\napply\n)))\n.\ntoDF\n(\n\"features\"\n)\nval\npolyExpansion\n=\nnew\nPolynomialExpansion\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"polyFeatures\"\n)\n.\nsetDegree\n(\n3\n)\nval\npolyDF\n=\npolyExpansion\n.\ntransform\n(\ndf\n)\npolyDF\n.\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\" in the Spark repo.\nRefer to the\nPolynomialExpansion Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.PolynomialExpansion\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nPolynomialExpansion\npolyExpansion\n=\nnew\nPolynomialExpansion\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"polyFeatures\"\n)\n.\nsetDegree\n(\n3\n);\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\nVectors\n.\ndense\n(\n2.0\n,\n1.0\n)),\nRowFactory\n.\ncreate\n(\nVectors\n.\ndense\n(\n0.0\n,\n0.0\n)),\nRowFactory\n.\ncreate\n(\nVectors\n.\ndense\n(\n3.0\n,\n-\n1.0\n))\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"features\"\n,\nnew\nVectorUDT\n(),\nfalse\n,\nMetadata\n.\nempty\n()),\n});\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nDataset\n<\nRow\n>\npolyDF\n=\npolyExpansion\n.\ntransform\n(\ndf\n);\npolyDF\n.\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\" in the Spark repo.\nDiscrete Cosine Transform (DCT)\nThe\nDiscrete Cosine\nTransform\ntransforms a length $N$ real-valued sequence in the time domain into\nanother length $N$ real-valued sequence in the frequency domain. A\nDCT\nclass\nprovides this functionality, implementing the\nDCT-II\nand scaling the result by $1/\\sqrt{2}$ such that the representing matrix\nfor the transform is unitary. No shift is applied to the transformed\nsequence (e.g. the $0$th element of the transformed sequence is the\n$0$th DCT coefficient and\nnot\nthe $N/2$th).\nExamples\nRefer to the\nDCT Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nDCT\nfrom\npyspark.ml.linalg\nimport\nVectors\ndf\n=\nspark\n.\ncreateDataFrame\n([\n(\nVectors\n.\ndense\n([\n0.0\n,\n1.0\n,\n-\n2.0\n,\n3.0\n]),),\n(\nVectors\n.\ndense\n([\n-\n1.0\n,\n2.0\n,\n4.0\n,\n-\n7.0\n]),),\n(\nVectors\n.\ndense\n([\n14.0\n,\n-\n2.0\n,\n-\n5.0\n,\n1.0\n]),)],\n[\n\"\nfeatures\n\"\n])\ndct\n=\nDCT\n(\ninverse\n=\nFalse\n,\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nfeaturesDCT\n\"\n)\ndctDf\n=\ndct\n.\ntransform\n(\ndf\n)\ndctDf\n.\nselect\n(\n\"\nfeaturesDCT\n\"\n).\nshow\n(\ntruncate\n=\nFalse\n)\nFind full example code at \"examples/src/main/python/ml/dct_example.py\" in the Spark repo.\nRefer to the\nDCT Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.DCT\nimport\norg.apache.spark.ml.linalg.Vectors\nval\ndata\n=\nSeq\n(\nVectors\n.\ndense\n(\n0.0\n,\n1.0\n,\n-\n2.0\n,\n3.0\n),\nVectors\n.\ndense\n(-\n1.0\n,\n2.0\n,\n4.0\n,\n-\n7.0\n),\nVectors\n.\ndense\n(\n14.0\n,\n-\n2.0\n,\n-\n5.0\n,\n1.0\n))\nval\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n.\nmap\n(\nTuple1\n.\napply\n)).\ntoDF\n(\n\"features\"\n)\nval\ndct\n=\nnew\nDCT\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"featuresDCT\"\n)\n.\nsetInverse\n(\nfalse\n)\nval\ndctDf\n=\ndct\n.\ntransform\n(\ndf\n)\ndctDf\n.\nselect\n(\n\"featuresDCT\"\n).\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\" in the Spark repo.\nRefer to the\nDCT Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.DCT\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\nVectors\n.\ndense\n(\n0.0\n,\n1.0\n,\n-\n2.0\n,\n3.0\n)),\nRowFactory\n.\ncreate\n(\nVectors\n.\ndense\n(-\n1.0\n,\n2.0\n,\n4.0\n,\n-\n7.0\n)),\nRowFactory\n.\ncreate\n(\nVectors\n.\ndense\n(\n14.0\n,\n-\n2.0\n,\n-\n5.0\n,\n1.0\n))\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"features\"\n,\nnew\nVectorUDT\n(),\nfalse\n,\nMetadata\n.\nempty\n()),\n});\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nDCT\ndct\n=\nnew\nDCT\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"featuresDCT\"\n)\n.\nsetInverse\n(\nfalse\n);\nDataset\n<\nRow\n>\ndctDf\n=\ndct\n.\ntransform\n(\ndf\n);\ndctDf\n.\nselect\n(\n\"featuresDCT\"\n).\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\" in the Spark repo.\nStringIndexer\nStringIndexer\nencodes a string column of labels to a column of label indices.\nStringIndexer\ncan encode multiple columns. The indices are in\n[0, numLabels)\n, and four ordering options are supported:\n“frequencyDesc”: descending order by label frequency (most frequent label assigned 0),\n“frequencyAsc”: ascending order by label frequency (least frequent label assigned 0),\n“alphabetDesc”: descending alphabetical order, and “alphabetAsc”: ascending alphabetical order \n(default = “frequencyDesc”). Note that in case of equal frequency when under\n“frequencyDesc”/”frequencyAsc”, the strings are further sorted by alphabet.\nThe unseen labels will be put at index numLabels if user chooses to keep them.\nIf the input column is numeric, we cast it to string and index the string\nvalues. When downstream pipeline components such as\nEstimator\nor\nTransformer\nmake use of this string-indexed label, you must set the input\ncolumn of the component to this string-indexed column name. In many cases,\nyou can set the input column with\nsetInputCol\n.\nExamples\nAssume that we have the following DataFrame with columns\nid\nand\ncategory\n:\nid | category\n----|----------\n 0  | a\n 1  | b\n 2  | c\n 3  | a\n 4  | a\n 5  | c\ncategory\nis a string column with three labels: “a”, “b”, and “c”.\nApplying\nStringIndexer\nwith\ncategory\nas the input column and\ncategoryIndex\nas the output\ncolumn, we should get the following:\nid | category | categoryIndex\n----|----------|---------------\n 0  | a        | 0.0\n 1  | b        | 2.0\n 2  | c        | 1.0\n 3  | a        | 0.0\n 4  | a        | 0.0\n 5  | c        | 1.0\n“a” gets index\n0\nbecause it is the most frequent, followed by “c” with index\n1\nand “b” with\nindex\n2\n.\nAdditionally, there are three strategies regarding how\nStringIndexer\nwill handle\nunseen labels when you have fit a\nStringIndexer\non one dataset and then use it\nto transform another:\nthrow an exception (which is the default)\nskip the row containing the unseen label entirely\nput unseen labels in a special additional bucket, at index numLabels\nExamples\nLet’s go back to our previous example but this time reuse our previously defined\nStringIndexer\non the following dataset:\nid | category\n----|----------\n 0  | a\n 1  | b\n 2  | c\n 3  | d\n 4  | e\nIf you’ve not set how\nStringIndexer\nhandles unseen labels or set it to\n“error”, an exception will be thrown.\nHowever, if you had called\nsetHandleInvalid(\"skip\")\n, the following dataset\nwill be generated:\nid | category | categoryIndex\n----|----------|---------------\n 0  | a        | 0.0\n 1  | b        | 2.0\n 2  | c        | 1.0\nNotice that the rows containing “d” or “e” do not appear.\nIf you call\nsetHandleInvalid(\"keep\")\n, the following dataset\nwill be generated:\nid | category | categoryIndex\n----|----------|---------------\n 0  | a        | 0.0\n 1  | b        | 2.0\n 2  | c        | 1.0\n 3  | d        | 3.0\n 4  | e        | 3.0\nNotice that the rows containing “d” or “e” are mapped to index “3.0”\nRefer to the\nStringIndexer Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nStringIndexer\ndf\n=\nspark\n.\ncreateDataFrame\n(\n[(\n0\n,\n\"\na\n\"\n),\n(\n1\n,\n\"\nb\n\"\n),\n(\n2\n,\n\"\nc\n\"\n),\n(\n3\n,\n\"\na\n\"\n),\n(\n4\n,\n\"\na\n\"\n),\n(\n5\n,\n\"\nc\n\"\n)],\n[\n\"\nid\n\"\n,\n\"\ncategory\n\"\n])\nindexer\n=\nStringIndexer\n(\ninputCol\n=\n\"\ncategory\n\"\n,\noutputCol\n=\n\"\ncategoryIndex\n\"\n)\nindexed\n=\nindexer\n.\nfit\n(\ndf\n).\ntransform\n(\ndf\n)\nindexed\n.\nshow\n()\nFind full example code at \"examples/src/main/python/ml/string_indexer_example.py\" in the Spark repo.\nRefer to the\nStringIndexer Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.StringIndexer\nval\ndf\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n((\n0\n,\n\"a\"\n),\n(\n1\n,\n\"b\"\n),\n(\n2\n,\n\"c\"\n),\n(\n3\n,\n\"a\"\n),\n(\n4\n,\n\"a\"\n),\n(\n5\n,\n\"c\"\n))\n).\ntoDF\n(\n\"id\"\n,\n\"category\"\n)\nval\nindexer\n=\nnew\nStringIndexer\n()\n.\nsetInputCol\n(\n\"category\"\n)\n.\nsetOutputCol\n(\n\"categoryIndex\"\n)\nval\nindexed\n=\nindexer\n.\nfit\n(\ndf\n).\ntransform\n(\ndf\n)\nindexed\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\" in the Spark repo.\nRefer to the\nStringIndexer Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.StringIndexer\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nimport\nstatic\norg\n.\napache\n.\nspark\n.\nsql\n.\ntypes\n.\nDataTypes\n.*;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n0\n,\n\"a\"\n),\nRowFactory\n.\ncreate\n(\n1\n,\n\"b\"\n),\nRowFactory\n.\ncreate\n(\n2\n,\n\"c\"\n),\nRowFactory\n.\ncreate\n(\n3\n,\n\"a\"\n),\nRowFactory\n.\ncreate\n(\n4\n,\n\"a\"\n),\nRowFactory\n.\ncreate\n(\n5\n,\n\"c\"\n)\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\ncreateStructField\n(\n\"id\"\n,\nIntegerType\n,\nfalse\n),\ncreateStructField\n(\n\"category\"\n,\nStringType\n,\nfalse\n)\n});\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nStringIndexer\nindexer\n=\nnew\nStringIndexer\n()\n.\nsetInputCol\n(\n\"category\"\n)\n.\nsetOutputCol\n(\n\"categoryIndex\"\n);\nDataset\n<\nRow\n>\nindexed\n=\nindexer\n.\nfit\n(\ndf\n).\ntransform\n(\ndf\n);\nindexed\n.\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\" in the Spark repo.\nIndexToString\nSymmetrically to\nStringIndexer\n,\nIndexToString\nmaps a column of label indices\nback to a column containing the original labels as strings. A common use case\nis to produce indices from labels with\nStringIndexer\n, train a model with those\nindices and retrieve the original labels from the column of predicted indices\nwith\nIndexToString\n. However, you are free to supply your own labels.\nExamples\nBuilding on the\nStringIndexer\nexample, let’s assume we have the following\nDataFrame with columns\nid\nand\ncategoryIndex\n:\nid | categoryIndex\n----|---------------\n 0  | 0.0\n 1  | 2.0\n 2  | 1.0\n 3  | 0.0\n 4  | 0.0\n 5  | 1.0\nApplying\nIndexToString\nwith\ncategoryIndex\nas the input column,\noriginalCategory\nas the output column, we are able to retrieve our original\nlabels (they will be inferred from the columns’ metadata):\nid | categoryIndex | originalCategory\n----|---------------|-----------------\n 0  | 0.0           | a\n 1  | 2.0           | b\n 2  | 1.0           | c\n 3  | 0.0           | a\n 4  | 0.0           | a\n 5  | 1.0           | c\nRefer to the\nIndexToString Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nIndexToString\n,\nStringIndexer\ndf\n=\nspark\n.\ncreateDataFrame\n(\n[(\n0\n,\n\"\na\n\"\n),\n(\n1\n,\n\"\nb\n\"\n),\n(\n2\n,\n\"\nc\n\"\n),\n(\n3\n,\n\"\na\n\"\n),\n(\n4\n,\n\"\na\n\"\n),\n(\n5\n,\n\"\nc\n\"\n)],\n[\n\"\nid\n\"\n,\n\"\ncategory\n\"\n])\nindexer\n=\nStringIndexer\n(\ninputCol\n=\n\"\ncategory\n\"\n,\noutputCol\n=\n\"\ncategoryIndex\n\"\n)\nmodel\n=\nindexer\n.\nfit\n(\ndf\n)\nindexed\n=\nmodel\n.\ntransform\n(\ndf\n)\nprint\n(\n\"\nTransformed string column\n'\n%s\n'\nto indexed column\n'\n%s\n'\"\n%\n(\nindexer\n.\ngetInputCol\n(),\nindexer\n.\ngetOutputCol\n()))\nindexed\n.\nshow\n()\nprint\n(\n\"\nStringIndexer will store labels in output column metadata\n\\n\n\"\n)\nconverter\n=\nIndexToString\n(\ninputCol\n=\n\"\ncategoryIndex\n\"\n,\noutputCol\n=\n\"\noriginalCategory\n\"\n)\nconverted\n=\nconverter\n.\ntransform\n(\nindexed\n)\nprint\n(\n\"\nTransformed indexed column\n'\n%s\n'\nback to original string column\n'\n%s\n'\nusing\n\"\n\"\nlabels in metadata\n\"\n%\n(\nconverter\n.\ngetInputCol\n(),\nconverter\n.\ngetOutputCol\n()))\nconverted\n.\nselect\n(\n\"\nid\n\"\n,\n\"\ncategoryIndex\n\"\n,\n\"\noriginalCategory\n\"\n).\nshow\n()\nFind full example code at \"examples/src/main/python/ml/index_to_string_example.py\" in the Spark repo.\nRefer to the\nIndexToString Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.attribute.Attribute\nimport\norg.apache.spark.ml.feature.\n{\nIndexToString\n,\nStringIndexer\n}\nval\ndf\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0\n,\n\"a\"\n),\n(\n1\n,\n\"b\"\n),\n(\n2\n,\n\"c\"\n),\n(\n3\n,\n\"a\"\n),\n(\n4\n,\n\"a\"\n),\n(\n5\n,\n\"c\"\n)\n)).\ntoDF\n(\n\"id\"\n,\n\"category\"\n)\nval\nindexer\n=\nnew\nStringIndexer\n()\n.\nsetInputCol\n(\n\"category\"\n)\n.\nsetOutputCol\n(\n\"categoryIndex\"\n)\n.\nfit\n(\ndf\n)\nval\nindexed\n=\nindexer\n.\ntransform\n(\ndf\n)\nprintln\n(\ns\n\"Transformed string column '${indexer.getInputCol}' \"\n+\ns\n\"to indexed column '${indexer.getOutputCol}'\"\n)\nindexed\n.\nshow\n()\nval\ninputColSchema\n=\nindexed\n.\nschema\n(\nindexer\n.\ngetOutputCol\n)\nprintln\n(\ns\n\"StringIndexer will store labels in output column metadata: \"\n+\ns\n\"${Attribute.fromStructField(inputColSchema).toString}\\n\"\n)\nval\nconverter\n=\nnew\nIndexToString\n()\n.\nsetInputCol\n(\n\"categoryIndex\"\n)\n.\nsetOutputCol\n(\n\"originalCategory\"\n)\nval\nconverted\n=\nconverter\n.\ntransform\n(\nindexed\n)\nprintln\n(\ns\n\"Transformed indexed column '${converter.getInputCol}' back to original string \"\n+\ns\n\"column '${converter.getOutputCol}' using labels in metadata\"\n)\nconverted\n.\nselect\n(\n\"id\"\n,\n\"categoryIndex\"\n,\n\"originalCategory\"\n).\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\" in the Spark repo.\nRefer to the\nIndexToString Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.attribute.Attribute\n;\nimport\norg.apache.spark.ml.feature.IndexToString\n;\nimport\norg.apache.spark.ml.feature.StringIndexer\n;\nimport\norg.apache.spark.ml.feature.StringIndexerModel\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n0\n,\n\"a\"\n),\nRowFactory\n.\ncreate\n(\n1\n,\n\"b\"\n),\nRowFactory\n.\ncreate\n(\n2\n,\n\"c\"\n),\nRowFactory\n.\ncreate\n(\n3\n,\n\"a\"\n),\nRowFactory\n.\ncreate\n(\n4\n,\n\"a\"\n),\nRowFactory\n.\ncreate\n(\n5\n,\n\"c\"\n)\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"id\"\n,\nDataTypes\n.\nIntegerType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"category\"\n,\nDataTypes\n.\nStringType\n,\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nStringIndexerModel\nindexer\n=\nnew\nStringIndexer\n()\n.\nsetInputCol\n(\n\"category\"\n)\n.\nsetOutputCol\n(\n\"categoryIndex\"\n)\n.\nfit\n(\ndf\n);\nDataset\n<\nRow\n>\nindexed\n=\nindexer\n.\ntransform\n(\ndf\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Transformed string column '\"\n+\nindexer\n.\ngetInputCol\n()\n+\n\"' \"\n+\n\"to indexed column '\"\n+\nindexer\n.\ngetOutputCol\n()\n+\n\"'\"\n);\nindexed\n.\nshow\n();\nStructField\ninputColSchema\n=\nindexed\n.\nschema\n().\napply\n(\nindexer\n.\ngetOutputCol\n());\nSystem\n.\nout\n.\nprintln\n(\n\"StringIndexer will store labels in output column metadata: \"\n+\nAttribute\n.\nfromStructField\n(\ninputColSchema\n).\ntoString\n()\n+\n\"\\n\"\n);\nIndexToString\nconverter\n=\nnew\nIndexToString\n()\n.\nsetInputCol\n(\n\"categoryIndex\"\n)\n.\nsetOutputCol\n(\n\"originalCategory\"\n);\nDataset\n<\nRow\n>\nconverted\n=\nconverter\n.\ntransform\n(\nindexed\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Transformed indexed column '\"\n+\nconverter\n.\ngetInputCol\n()\n+\n\"' back to \"\n+\n\"original string column '\"\n+\nconverter\n.\ngetOutputCol\n()\n+\n\"' using labels in metadata\"\n);\nconverted\n.\nselect\n(\n\"id\"\n,\n\"categoryIndex\"\n,\n\"originalCategory\"\n).\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\" in the Spark repo.\nOneHotEncoder\nOne-hot encoding\nmaps a categorical feature, represented as a label index, to a binary vector with at most a single one-value indicating the presence of a specific feature value from among the set of all feature values. This encoding allows algorithms which expect continuous features, such as Logistic Regression, to use categorical features. For string type input data, it is common to encode categorical features using\nStringIndexer\nfirst.\nOneHotEncoder\ncan transform multiple columns, returning an one-hot-encoded output vector column for each input column. It is common to merge these vectors into a single feature vector using\nVectorAssembler\n.\nOneHotEncoder\nsupports the\nhandleInvalid\nparameter to choose how to handle invalid input during transforming data. Available options include ‘keep’ (any invalid inputs are assigned to an extra categorical index) and ‘error’ (throw an error).\nExamples\nRefer to the\nOneHotEncoder Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nOneHotEncoder\ndf\n=\nspark\n.\ncreateDataFrame\n([\n(\n0.0\n,\n1.0\n),\n(\n1.0\n,\n0.0\n),\n(\n2.0\n,\n1.0\n),\n(\n0.0\n,\n2.0\n),\n(\n0.0\n,\n1.0\n),\n(\n2.0\n,\n0.0\n)\n],\n[\n\"\ncategoryIndex1\n\"\n,\n\"\ncategoryIndex2\n\"\n])\nencoder\n=\nOneHotEncoder\n(\ninputCols\n=\n[\n\"\ncategoryIndex1\n\"\n,\n\"\ncategoryIndex2\n\"\n],\noutputCols\n=\n[\n\"\ncategoryVec1\n\"\n,\n\"\ncategoryVec2\n\"\n])\nmodel\n=\nencoder\n.\nfit\n(\ndf\n)\nencoded\n=\nmodel\n.\ntransform\n(\ndf\n)\nencoded\n.\nshow\n()\nFind full example code at \"examples/src/main/python/ml/onehot_encoder_example.py\" in the Spark repo.\nRefer to the\nOneHotEncoder Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.OneHotEncoder\nval\ndf\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0.0\n,\n1.0\n),\n(\n1.0\n,\n0.0\n),\n(\n2.0\n,\n1.0\n),\n(\n0.0\n,\n2.0\n),\n(\n0.0\n,\n1.0\n),\n(\n2.0\n,\n0.0\n)\n)).\ntoDF\n(\n\"categoryIndex1\"\n,\n\"categoryIndex2\"\n)\nval\nencoder\n=\nnew\nOneHotEncoder\n()\n.\nsetInputCols\n(\nArray\n(\n\"categoryIndex1\"\n,\n\"categoryIndex2\"\n))\n.\nsetOutputCols\n(\nArray\n(\n\"categoryVec1\"\n,\n\"categoryVec2\"\n))\nval\nmodel\n=\nencoder\n.\nfit\n(\ndf\n)\nval\nencoded\n=\nmodel\n.\ntransform\n(\ndf\n)\nencoded\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala\" in the Spark repo.\nRefer to the\nOneHotEncoder Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.OneHotEncoder\n;\nimport\norg.apache.spark.ml.feature.OneHotEncoderModel\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n0.0\n,\n1.0\n),\nRowFactory\n.\ncreate\n(\n1.0\n,\n0.0\n),\nRowFactory\n.\ncreate\n(\n2.0\n,\n1.0\n),\nRowFactory\n.\ncreate\n(\n0.0\n,\n2.0\n),\nRowFactory\n.\ncreate\n(\n0.0\n,\n1.0\n),\nRowFactory\n.\ncreate\n(\n2.0\n,\n0.0\n)\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"categoryIndex1\"\n,\nDataTypes\n.\nDoubleType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"categoryIndex2\"\n,\nDataTypes\n.\nDoubleType\n,\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nOneHotEncoder\nencoder\n=\nnew\nOneHotEncoder\n()\n.\nsetInputCols\n(\nnew\nString\n[]\n{\n\"categoryIndex1\"\n,\n\"categoryIndex2\"\n})\n.\nsetOutputCols\n(\nnew\nString\n[]\n{\n\"categoryVec1\"\n,\n\"categoryVec2\"\n});\nOneHotEncoderModel\nmodel\n=\nencoder\n.\nfit\n(\ndf\n);\nDataset\n<\nRow\n>\nencoded\n=\nmodel\n.\ntransform\n(\ndf\n);\nencoded\n.\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java\" in the Spark repo.\nTargetEncoder\nTarget Encoding\nis a data-preprocessing technique that transforms high-cardinality categorical features into quasi-continuous scalar attributes suited for use in regression-type models. This paradigm maps individual values of an independent feature to a scalar, representing some estimate of the dependent attribute (meaning categorical values that exhibit similar statistics with respect to the target will have a similar representation).\nBy leveraging the relationship between categorical features and the target variable, Target Encoding usually performs better than One-Hot and does not require a final binary vector encoding, decreasing the overall dimensionality of the dataset.\nUser can specify input and output column names by setting\ninputCol\nand\noutputCol\nfor single-column use cases, or\ninputCols\nand\noutputCols\nfor multi-column use cases (both arrays required to have the same size). These columns are expected to contain categorical indices (positive integers), being missing values (null) treated as a separate category. Data type must be any subclass of ‘NumericType’. For string type input data, it is common to encode categorical features using\nStringIndexer\nfirst.\nUser can specify the target column name by setting\nlabel\n. This column is expected to contain the ground-truth labels from which encodings will be derived. Observations with missing label (null) are not considered when calculating estimates. Data type must be any subclass of ‘NumericType’.\nTargetEncoder\nsupports the\nhandleInvalid\nparameter to choose how to handle invalid input, meaning categories not seen at training, when encoding new data. Available options include ‘keep’ (any invalid inputs are assigned to an extra categorical index) and ‘error’ (throw an exception).\nTargetEncoder\nsupports the\ntargetType\nparameter to choose the label type when fitting data, affecting how estimates are calculated. Available options include ‘binary’  and ‘continuous’.\nWhen set to ‘binary’, the target attribute $Y$ is expected to be binary, $Y\\in{ 0,1 }$. The transformation maps individual values $X_{i}$ to the conditional probability of $Y$ given that $X=X_{i}\\;$: $\\;\\; S_{i}=P(Y\\mid X=X_{i})$. This approach is also known as bin-counting.\nWhen set to ‘continuous’, the target attribute $Y$ is expected to be continuous, $Y\\in\\mathbb{Q}$. The transformation maps individual values $X_{i}$ to the average of $Y$ given that $X=X_{i}\\;$: $\\;\\; S_{i}=E[Y\\mid X=X_{i}]$. This approach is also known as mean-encoding.\nTargetEncoder\nsupports the\nsmoothing\nparameter to tune how in-category stats and overall stats are blended. High-cardinality categorical features are usually unevenly distributed across all possible values of $X$.\nTherefore, calculating encodings $S_{i}$ according only to in-class statistics makes this estimates very unreliable, and rarely seen categories will very likely cause overfitting in learning.\nSmoothing prevents this behaviour by weighting in-class estimates with overall estimates according to the relative size of the particular class on the whole dataset.\n$\\;\\;\\; S_{i}=\\lambda(n_{i})\\, P(Y\\mid X=X_{i})+(1-\\lambda(n_{i}))\\, P(Y)$ for the binary case\n$\\;\\;\\; S_{i}=\\lambda(n_{i})\\, E[Y\\mid X=X_{i}]+(1-\\lambda(n_{i}))\\, E[Y]$ for the continuous case\nbeing $\\lambda(n_{i})$ a monotonically increasing function on $n_{i}$, bounded between 0 and 1.\nUsually $\\lambda(n_{i})$ is implemented as the parametric function $\\lambda(n_{i})=\\frac{n_{i}}{n_{i}+m}$, where $m$ is the smoothing factor, represented by\nsmoothing\nparameter in\nTargetEncoder\n.\nExamples\nBuilding on the\nTargetEncoder\nexample, let’s assume we have the following\nDataFrame with columns\nfeature\nand\ntarget\n(binary & continuous):\nfeature | target | target\n         | (bin)  | (cont)\n --------|--------|--------\n 1       | 0      | 1.3\n 1       | 1      | 2.5\n 1       | 0      | 1.6\n 2       | 1      | 1.8\n 2       | 0      | 2.4\n 3       | 1      | 3.2\nApplying\nTargetEncoder\nwith ‘binary’ target type,\nfeature\nas the input column,\ntarget (bin)\nas the label column\nand\nencoded\nas the output column, we are able to fit a model\non the data to learn encodings and transform the data according\nto these mappings:\nfeature | target | encoded\n         | (bin)  |\n --------|--------|--------\n 1       | 0      | 0.333\n 1       | 1      | 0.333\n 1       | 0      | 0.333\n 2       | 1      | 0.5\n 2       | 0      | 0.5\n 3       | 1      | 1.0\nApplying\nTargetEncoder\nwith ‘continuous’  target type,\nfeature\nas the input column,\ntarget (cont)\nas the label column\nand\nencoded\nas the output column, we are able to fit a model\non the data to learn encodings and transform the data according\nto these mappings:\nfeature | target | encoded\n         | (cont) |\n --------|--------|--------\n 1       | 1.3    | 1.8\n 1       | 2.5    | 1.8\n 1       | 1.6    | 1.8\n 2       | 1.8    | 2.1\n 2       | 2.4    | 2.1\n 3       | 3.2    | 3.2\nRefer to the\nTargetEncoder Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nTargetEncoder\ndf\n=\nspark\n.\ncreateDataFrame\n(\n[\n(\n0.0\n,\n1.0\n,\n0\n,\n10.0\n),\n(\n1.0\n,\n0.0\n,\n1\n,\n20.0\n),\n(\n2.0\n,\n1.0\n,\n0\n,\n30.0\n),\n(\n0.0\n,\n2.0\n,\n1\n,\n40.0\n),\n(\n0.0\n,\n1.0\n,\n0\n,\n50.0\n),\n(\n2.0\n,\n0.0\n,\n1\n,\n60.0\n),\n],\n[\n\"\ncategoryIndex1\n\"\n,\n\"\ncategoryIndex2\n\"\n,\n\"\nbinaryLabel\n\"\n,\n\"\ncontinuousLabel\n\"\n],\n)\n# binary target\nencoder\n=\nTargetEncoder\n(\ninputCols\n=\n[\n\"\ncategoryIndex1\n\"\n,\n\"\ncategoryIndex2\n\"\n],\noutputCols\n=\n[\n\"\ncategoryIndex1Target\n\"\n,\n\"\ncategoryIndex2Target\n\"\n],\nlabelCol\n=\n\"\nbinaryLabel\n\"\n,\ntargetType\n=\n\"\nbinary\n\"\n)\nmodel\n=\nencoder\n.\nfit\n(\ndf\n)\nencoded\n=\nmodel\n.\ntransform\n(\ndf\n)\nencoded\n.\nshow\n()\n# continuous target\nencoder\n=\nTargetEncoder\n(\ninputCols\n=\n[\n\"\ncategoryIndex1\n\"\n,\n\"\ncategoryIndex2\n\"\n],\noutputCols\n=\n[\n\"\ncategoryIndex1Target\n\"\n,\n\"\ncategoryIndex2Target\n\"\n],\nlabelCol\n=\n\"\ncontinuousLabel\n\"\n,\ntargetType\n=\n\"\ncontinuous\n\"\n)\nmodel\n=\nencoder\n.\nfit\n(\ndf\n)\nencoded\n=\nmodel\n.\ntransform\n(\ndf\n)\nencoded\n.\nshow\n()\nFind full example code at \"examples/src/main/python/ml/target_encoder_example.py\" in the Spark repo.\nRefer to the\nTargetEncoder Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.TargetEncoder\nval\ndf\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0.0\n,\n1.0\n,\n0\n,\n10.0\n),\n(\n1.0\n,\n0.0\n,\n1\n,\n20.0\n),\n(\n2.0\n,\n1.0\n,\n0\n,\n30.0\n),\n(\n0.0\n,\n2.0\n,\n1\n,\n40.0\n),\n(\n0.0\n,\n1.0\n,\n0\n,\n50.0\n),\n(\n2.0\n,\n0.0\n,\n1\n,\n60.0\n)\n)).\ntoDF\n(\n\"categoryIndex1\"\n,\n\"categoryIndex2\"\n,\n\"binaryLabel\"\n,\n\"continuousLabel\"\n)\n// binary target\nval\nbin_encoder\n=\nnew\nTargetEncoder\n()\n.\nsetInputCols\n(\nArray\n(\n\"categoryIndex1\"\n,\n\"categoryIndex2\"\n))\n.\nsetOutputCols\n(\nArray\n(\n\"categoryIndex1Target\"\n,\n\"categoryIndex2Target\"\n))\n.\nsetLabelCol\n(\n\"binaryLabel\"\n)\n.\nsetTargetType\n(\n\"binary\"\n);\nval\nbin_model\n=\nbin_encoder\n.\nfit\n(\ndf\n)\nval\nbin_encoded\n=\nbin_model\n.\ntransform\n(\ndf\n)\nbin_encoded\n.\nshow\n()\n// continuous target\nval\ncont_encoder\n=\nnew\nTargetEncoder\n()\n.\nsetInputCols\n(\nArray\n(\n\"categoryIndex1\"\n,\n\"categoryIndex2\"\n))\n.\nsetOutputCols\n(\nArray\n(\n\"categoryIndex1Target\"\n,\n\"categoryIndex2Target\"\n))\n.\nsetLabelCol\n(\n\"continuousLabel\"\n)\n.\nsetTargetType\n(\n\"continuous\"\n);\nval\ncont_model\n=\ncont_encoder\n.\nfit\n(\ndf\n)\nval\ncont_encoded\n=\ncont_model\n.\ntransform\n(\ndf\n)\ncont_encoded\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/TargetEncoderExample.scala\" in the Spark repo.\nRefer to the\nTargetEncoder Java docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.TargetEncoder\n;\nimport\norg.apache.spark.ml.feature.TargetEncoderModel\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n0.0\n,\n1.0\n,\n0\n,\n10.0\n),\nRowFactory\n.\ncreate\n(\n1.0\n,\n0.0\n,\n1\n,\n20.0\n),\nRowFactory\n.\ncreate\n(\n2.0\n,\n1.0\n,\n0\n,\n30.0\n),\nRowFactory\n.\ncreate\n(\n0.0\n,\n2.0\n,\n1\n,\n40.0\n),\nRowFactory\n.\ncreate\n(\n0.0\n,\n1.0\n,\n0\n,\n50.0\n),\nRowFactory\n.\ncreate\n(\n2.0\n,\n0.0\n,\n1\n,\n60.0\n)\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"categoryIndex1\"\n,\nDataTypes\n.\nDoubleType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"categoryIndex2\"\n,\nDataTypes\n.\nDoubleType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"binaryLabel\"\n,\nDataTypes\n.\nDoubleType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"continuousLabel\"\n,\nDataTypes\n.\nDoubleType\n,\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\n// binary target\nTargetEncoder\nbin_encoder\n=\nnew\nTargetEncoder\n()\n.\nsetInputCols\n(\nnew\nString\n[]\n{\n\"categoryIndex1\"\n,\n\"categoryIndex2\"\n})\n.\nsetOutputCols\n(\nnew\nString\n[]\n{\n\"categoryIndex1Target\"\n,\n\"categoryIndex2Target\"\n})\n.\nsetLabelCol\n(\n\"binaryLabel\"\n)\n.\nsetTargetType\n(\n\"binary\"\n);\nTargetEncoderModel\nbin_model\n=\nbin_encoder\n.\nfit\n(\ndf\n);\nDataset\n<\nRow\n>\nbin_encoded\n=\nbin_model\n.\ntransform\n(\ndf\n);\nbin_encoded\n.\nshow\n();\n// continuous target\nTargetEncoder\ncont_encoder\n=\nnew\nTargetEncoder\n()\n.\nsetInputCols\n(\nnew\nString\n[]\n{\n\"categoryIndex1\"\n,\n\"categoryIndex2\"\n})\n.\nsetOutputCols\n(\nnew\nString\n[]\n{\n\"categoryIndex1Target\"\n,\n\"categoryIndex2Target\"\n})\n.\nsetLabelCol\n(\n\"continuousLabel\"\n)\n.\nsetTargetType\n(\n\"continuous\"\n);\nTargetEncoderModel\ncont_model\n=\ncont_encoder\n.\nfit\n(\ndf\n);\nDataset\n<\nRow\n>\ncont_encoded\n=\ncont_model\n.\ntransform\n(\ndf\n);\ncont_encoded\n.\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaTargetEncoderExample.java\" in the Spark repo.\nVectorIndexer\nVectorIndexer\nhelps index categorical features in datasets of\nVector\ns.\nIt can both automatically decide which features are categorical and convert original values to category indices.  Specifically, it does the following:\nTake an input column of type\nVector\nand a parameter\nmaxCategories\n.\nDecide which features should be categorical based on the number of distinct values, where features with at most\nmaxCategories\nare declared categorical.\nCompute 0-based category indices for each categorical feature.\nIndex categorical features and transform original feature values to indices.\nIndexing categorical features allows algorithms such as Decision Trees and Tree Ensembles to treat categorical features appropriately, improving performance.\nExamples\nIn the example below, we read in a dataset of labeled points and then use\nVectorIndexer\nto decide which features should be treated as categorical.  We transform the categorical feature values to their indices.  This transformed data could then be passed to algorithms such as\nDecisionTreeRegressor\nthat handle categorical features.\nRefer to the\nVectorIndexer Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nVectorIndexer\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\nindexer\n=\nVectorIndexer\n(\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nindexed\n\"\n,\nmaxCategories\n=\n10\n)\nindexerModel\n=\nindexer\n.\nfit\n(\ndata\n)\ncategoricalFeatures\n=\nindexerModel\n.\ncategoryMaps\nprint\n(\n\"\nChose %d categorical features: %s\n\"\n%\n(\nlen\n(\ncategoricalFeatures\n),\n\"\n,\n\"\n.\njoin\n(\nstr\n(\nk\n)\nfor\nk\nin\ncategoricalFeatures\n.\nkeys\n())))\n# Create new column \"indexed\" with categorical values transformed to indices\nindexedData\n=\nindexerModel\n.\ntransform\n(\ndata\n)\nindexedData\n.\nshow\n()\nFind full example code at \"examples/src/main/python/ml/vector_indexer_example.py\" in the Spark repo.\nRefer to the\nVectorIndexer Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.VectorIndexer\nval\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\nval\nindexer\n=\nnew\nVectorIndexer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"indexed\"\n)\n.\nsetMaxCategories\n(\n10\n)\nval\nindexerModel\n=\nindexer\n.\nfit\n(\ndata\n)\nval\ncategoricalFeatures\n:\nSet\n[\nInt\n]\n=\nindexerModel\n.\ncategoryMaps\n.\nkeys\n.\ntoSet\nprintln\n(\ns\n\"Chose ${categoricalFeatures.size} \"\n+\ns\n\"categorical features: ${categoricalFeatures.mkString(\"\n,\n\")}\"\n)\n// Create new column \"indexed\" with categorical values transformed to indices\nval\nindexedData\n=\nindexerModel\n.\ntransform\n(\ndata\n)\nindexedData\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\" in the Spark repo.\nRefer to the\nVectorIndexer Java docs\nfor more details on the API.\nimport\njava.util.Map\n;\nimport\norg.apache.spark.ml.feature.VectorIndexer\n;\nimport\norg.apache.spark.ml.feature.VectorIndexerModel\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nDataset\n<\nRow\n>\ndata\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n);\nVectorIndexer\nindexer\n=\nnew\nVectorIndexer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"indexed\"\n)\n.\nsetMaxCategories\n(\n10\n);\nVectorIndexerModel\nindexerModel\n=\nindexer\n.\nfit\n(\ndata\n);\nMap\n<\nInteger\n,\nMap\n<\nDouble\n,\nInteger\n>>\ncategoryMaps\n=\nindexerModel\n.\njavaCategoryMaps\n();\nSystem\n.\nout\n.\nprint\n(\n\"Chose \"\n+\ncategoryMaps\n.\nsize\n()\n+\n\" categorical features:\"\n);\nfor\n(\nInteger\nfeature\n:\ncategoryMaps\n.\nkeySet\n())\n{\nSystem\n.\nout\n.\nprint\n(\n\" \"\n+\nfeature\n);\n}\nSystem\n.\nout\n.\nprintln\n();\n// Create new column \"indexed\" with categorical values transformed to indices\nDataset\n<\nRow\n>\nindexedData\n=\nindexerModel\n.\ntransform\n(\ndata\n);\nindexedData\n.\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\" in the Spark repo.\nInteraction\nInteraction\nis a\nTransformer\nwhich takes vector or double-valued columns, and generates a single vector column that contains the product of all combinations of one value from each input column.\nFor example, if you have 2 vector type columns each of which has 3 dimensions as input columns, then you’ll get a 9-dimensional vector as the output column.\nExamples\nAssume that we have the following DataFrame with the columns “id1”, “vec1”, and “vec2”:\nid1|vec1          |vec2          \n  ---|--------------|--------------\n  1  |[1.0,2.0,3.0] |[8.0,4.0,5.0] \n  2  |[4.0,3.0,8.0] |[7.0,9.0,8.0] \n  3  |[6.0,1.0,9.0] |[2.0,3.0,6.0] \n  4  |[10.0,8.0,6.0]|[9.0,4.0,5.0] \n  5  |[9.0,2.0,7.0] |[10.0,7.0,3.0]\n  6  |[1.0,1.0,4.0] |[2.0,8.0,4.0]\nApplying\nInteraction\nwith those input columns,\nthen\ninteractedCol\nas the output column contains:\nid1|vec1          |vec2          |interactedCol                                         \n  ---|--------------|--------------|------------------------------------------------------\n  1  |[1.0,2.0,3.0] |[8.0,4.0,5.0] |[8.0,4.0,5.0,16.0,8.0,10.0,24.0,12.0,15.0]            \n  2  |[4.0,3.0,8.0] |[7.0,9.0,8.0] |[56.0,72.0,64.0,42.0,54.0,48.0,112.0,144.0,128.0]     \n  3  |[6.0,1.0,9.0] |[2.0,3.0,6.0] |[36.0,54.0,108.0,6.0,9.0,18.0,54.0,81.0,162.0]        \n  4  |[10.0,8.0,6.0]|[9.0,4.0,5.0] |[360.0,160.0,200.0,288.0,128.0,160.0,216.0,96.0,120.0]\n  5  |[9.0,2.0,7.0] |[10.0,7.0,3.0]|[450.0,315.0,135.0,100.0,70.0,30.0,350.0,245.0,105.0] \n  6  |[1.0,1.0,4.0] |[2.0,8.0,4.0] |[12.0,48.0,24.0,12.0,48.0,24.0,48.0,192.0,96.0]\nRefer to the\nInteraction Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nInteraction\n,\nVectorAssembler\ndf\n=\nspark\n.\ncreateDataFrame\n(\n[(\n1\n,\n1\n,\n2\n,\n3\n,\n8\n,\n4\n,\n5\n),\n(\n2\n,\n4\n,\n3\n,\n8\n,\n7\n,\n9\n,\n8\n),\n(\n3\n,\n6\n,\n1\n,\n9\n,\n2\n,\n3\n,\n6\n),\n(\n4\n,\n10\n,\n8\n,\n6\n,\n9\n,\n4\n,\n5\n),\n(\n5\n,\n9\n,\n2\n,\n7\n,\n10\n,\n7\n,\n3\n),\n(\n6\n,\n1\n,\n1\n,\n4\n,\n2\n,\n8\n,\n4\n)],\n[\n\"\nid1\n\"\n,\n\"\nid2\n\"\n,\n\"\nid3\n\"\n,\n\"\nid4\n\"\n,\n\"\nid5\n\"\n,\n\"\nid6\n\"\n,\n\"\nid7\n\"\n])\nassembler1\n=\nVectorAssembler\n(\ninputCols\n=\n[\n\"\nid2\n\"\n,\n\"\nid3\n\"\n,\n\"\nid4\n\"\n],\noutputCol\n=\n\"\nvec1\n\"\n)\nassembled1\n=\nassembler1\n.\ntransform\n(\ndf\n)\nassembler2\n=\nVectorAssembler\n(\ninputCols\n=\n[\n\"\nid5\n\"\n,\n\"\nid6\n\"\n,\n\"\nid7\n\"\n],\noutputCol\n=\n\"\nvec2\n\"\n)\nassembled2\n=\nassembler2\n.\ntransform\n(\nassembled1\n).\nselect\n(\n\"\nid1\n\"\n,\n\"\nvec1\n\"\n,\n\"\nvec2\n\"\n)\ninteraction\n=\nInteraction\n(\ninputCols\n=\n[\n\"\nid1\n\"\n,\n\"\nvec1\n\"\n,\n\"\nvec2\n\"\n],\noutputCol\n=\n\"\ninteractedCol\n\"\n)\ninteracted\n=\ninteraction\n.\ntransform\n(\nassembled2\n)\ninteracted\n.\nshow\n(\ntruncate\n=\nFalse\n)\nFind full example code at \"examples/src/main/python/ml/interaction_example.py\" in the Spark repo.\nRefer to the\nInteraction Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.Interaction\nimport\norg.apache.spark.ml.feature.VectorAssembler\nval\ndf\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n1\n,\n1\n,\n2\n,\n3\n,\n8\n,\n4\n,\n5\n),\n(\n2\n,\n4\n,\n3\n,\n8\n,\n7\n,\n9\n,\n8\n),\n(\n3\n,\n6\n,\n1\n,\n9\n,\n2\n,\n3\n,\n6\n),\n(\n4\n,\n10\n,\n8\n,\n6\n,\n9\n,\n4\n,\n5\n),\n(\n5\n,\n9\n,\n2\n,\n7\n,\n10\n,\n7\n,\n3\n),\n(\n6\n,\n1\n,\n1\n,\n4\n,\n2\n,\n8\n,\n4\n)\n)).\ntoDF\n(\n\"id1\"\n,\n\"id2\"\n,\n\"id3\"\n,\n\"id4\"\n,\n\"id5\"\n,\n\"id6\"\n,\n\"id7\"\n)\nval\nassembler1\n=\nnew\nVectorAssembler\n().\nsetInputCols\n(\nArray\n(\n\"id2\"\n,\n\"id3\"\n,\n\"id4\"\n)).\nsetOutputCol\n(\n\"vec1\"\n)\nval\nassembled1\n=\nassembler1\n.\ntransform\n(\ndf\n)\nval\nassembler2\n=\nnew\nVectorAssembler\n().\nsetInputCols\n(\nArray\n(\n\"id5\"\n,\n\"id6\"\n,\n\"id7\"\n)).\nsetOutputCol\n(\n\"vec2\"\n)\nval\nassembled2\n=\nassembler2\n.\ntransform\n(\nassembled1\n).\nselect\n(\n\"id1\"\n,\n\"vec1\"\n,\n\"vec2\"\n)\nval\ninteraction\n=\nnew\nInteraction\n()\n.\nsetInputCols\n(\nArray\n(\n\"id1\"\n,\n\"vec1\"\n,\n\"vec2\"\n))\n.\nsetOutputCol\n(\n\"interactedCol\"\n)\nval\ninteracted\n=\ninteraction\n.\ntransform\n(\nassembled2\n)\ninteracted\n.\nshow\n(\ntruncate\n=\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\" in the Spark repo.\nRefer to the\nInteraction Java docs\nfor more details on the API.\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n1\n,\n1\n,\n2\n,\n3\n,\n8\n,\n4\n,\n5\n),\nRowFactory\n.\ncreate\n(\n2\n,\n4\n,\n3\n,\n8\n,\n7\n,\n9\n,\n8\n),\nRowFactory\n.\ncreate\n(\n3\n,\n6\n,\n1\n,\n9\n,\n2\n,\n3\n,\n6\n),\nRowFactory\n.\ncreate\n(\n4\n,\n10\n,\n8\n,\n6\n,\n9\n,\n4\n,\n5\n),\nRowFactory\n.\ncreate\n(\n5\n,\n9\n,\n2\n,\n7\n,\n10\n,\n7\n,\n3\n),\nRowFactory\n.\ncreate\n(\n6\n,\n1\n,\n1\n,\n4\n,\n2\n,\n8\n,\n4\n)\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"id1\"\n,\nDataTypes\n.\nIntegerType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"id2\"\n,\nDataTypes\n.\nIntegerType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"id3\"\n,\nDataTypes\n.\nIntegerType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"id4\"\n,\nDataTypes\n.\nIntegerType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"id5\"\n,\nDataTypes\n.\nIntegerType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"id6\"\n,\nDataTypes\n.\nIntegerType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"id7\"\n,\nDataTypes\n.\nIntegerType\n,\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nVectorAssembler\nassembler1\n=\nnew\nVectorAssembler\n()\n.\nsetInputCols\n(\nnew\nString\n[]{\n\"id2\"\n,\n\"id3\"\n,\n\"id4\"\n})\n.\nsetOutputCol\n(\n\"vec1\"\n);\nDataset\n<\nRow\n>\nassembled1\n=\nassembler1\n.\ntransform\n(\ndf\n);\nVectorAssembler\nassembler2\n=\nnew\nVectorAssembler\n()\n.\nsetInputCols\n(\nnew\nString\n[]{\n\"id5\"\n,\n\"id6\"\n,\n\"id7\"\n})\n.\nsetOutputCol\n(\n\"vec2\"\n);\nDataset\n<\nRow\n>\nassembled2\n=\nassembler2\n.\ntransform\n(\nassembled1\n).\nselect\n(\n\"id1\"\n,\n\"vec1\"\n,\n\"vec2\"\n);\nInteraction\ninteraction\n=\nnew\nInteraction\n()\n.\nsetInputCols\n(\nnew\nString\n[]{\n\"id1\"\n,\n\"vec1\"\n,\n\"vec2\"\n})\n.\nsetOutputCol\n(\n\"interactedCol\"\n);\nDataset\n<\nRow\n>\ninteracted\n=\ninteraction\n.\ntransform\n(\nassembled2\n);\ninteracted\n.\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\" in the Spark repo.\nNormalizer\nNormalizer\nis a\nTransformer\nwhich transforms a dataset of\nVector\nrows, normalizing each\nVector\nto have unit norm.  It takes parameter\np\n, which specifies the\np-norm\nused for normalization.  ($p = 2$ by default.)  This normalization can help standardize your input data and improve the behavior of learning algorithms.\nExamples\nThe following example demonstrates how to load a dataset in libsvm format and then normalize each row to have unit $L^1$ norm and unit $L^\\infty$ norm.\nRefer to the\nNormalizer Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nNormalizer\nfrom\npyspark.ml.linalg\nimport\nVectors\ndataFrame\n=\nspark\n.\ncreateDataFrame\n([\n(\n0\n,\nVectors\n.\ndense\n([\n1.0\n,\n0.5\n,\n-\n1.0\n]),),\n(\n1\n,\nVectors\n.\ndense\n([\n2.0\n,\n1.0\n,\n1.0\n]),),\n(\n2\n,\nVectors\n.\ndense\n([\n4.0\n,\n10.0\n,\n2.0\n]),)\n],\n[\n\"\nid\n\"\n,\n\"\nfeatures\n\"\n])\n# Normalize each Vector using $L^1$ norm.\nnormalizer\n=\nNormalizer\n(\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nnormFeatures\n\"\n,\np\n=\n1.0\n)\nl1NormData\n=\nnormalizer\n.\ntransform\n(\ndataFrame\n)\nprint\n(\n\"\nNormalized using L^1 norm\n\"\n)\nl1NormData\n.\nshow\n()\n# Normalize each Vector using $L^\\infty$ norm.\nlInfNormData\n=\nnormalizer\n.\ntransform\n(\ndataFrame\n,\n{\nnormalizer\n.\np\n:\nfloat\n(\n\"\ninf\n\"\n)})\nprint\n(\n\"\nNormalized using L^inf norm\n\"\n)\nlInfNormData\n.\nshow\n()\nFind full example code at \"examples/src/main/python/ml/normalizer_example.py\" in the Spark repo.\nRefer to the\nNormalizer Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.Normalizer\nimport\norg.apache.spark.ml.linalg.Vectors\nval\ndataFrame\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0\n,\nVectors\n.\ndense\n(\n1.0\n,\n0.5\n,\n-\n1.0\n)),\n(\n1\n,\nVectors\n.\ndense\n(\n2.0\n,\n1.0\n,\n1.0\n)),\n(\n2\n,\nVectors\n.\ndense\n(\n4.0\n,\n10.0\n,\n2.0\n))\n)).\ntoDF\n(\n\"id\"\n,\n\"features\"\n)\n// Normalize each Vector using $L^1$ norm.\nval\nnormalizer\n=\nnew\nNormalizer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"normFeatures\"\n)\n.\nsetP\n(\n1.0\n)\nval\nl1NormData\n=\nnormalizer\n.\ntransform\n(\ndataFrame\n)\nprintln\n(\n\"Normalized using L^1 norm\"\n)\nl1NormData\n.\nshow\n()\n// Normalize each Vector using $L^\\infty$ norm.\nval\nlInfNormData\n=\nnormalizer\n.\ntransform\n(\ndataFrame\n,\nnormalizer\n.\np\n->\nDouble\n.\nPositiveInfinity\n)\nprintln\n(\n\"Normalized using L^inf norm\"\n)\nlInfNormData\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\" in the Spark repo.\nRefer to the\nNormalizer Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.Normalizer\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n0\n,\nVectors\n.\ndense\n(\n1.0\n,\n0.1\n,\n-\n8.0\n)),\nRowFactory\n.\ncreate\n(\n1\n,\nVectors\n.\ndense\n(\n2.0\n,\n1.0\n,\n-\n4.0\n)),\nRowFactory\n.\ncreate\n(\n2\n,\nVectors\n.\ndense\n(\n4.0\n,\n10.0\n,\n8.0\n))\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"id\"\n,\nDataTypes\n.\nIntegerType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"features\"\n,\nnew\nVectorUDT\n(),\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndataFrame\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\n// Normalize each Vector using $L^1$ norm.\nNormalizer\nnormalizer\n=\nnew\nNormalizer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"normFeatures\"\n)\n.\nsetP\n(\n1.0\n);\nDataset\n<\nRow\n>\nl1NormData\n=\nnormalizer\n.\ntransform\n(\ndataFrame\n);\nl1NormData\n.\nshow\n();\n// Normalize each Vector using $L^\\infty$ norm.\nDataset\n<\nRow\n>\nlInfNormData\n=\nnormalizer\n.\ntransform\n(\ndataFrame\n,\nnormalizer\n.\np\n().\nw\n(\nDouble\n.\nPOSITIVE_INFINITY\n));\nlInfNormData\n.\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\" in the Spark repo.\nStandardScaler\nStandardScaler\ntransforms a dataset of\nVector\nrows, normalizing each feature to have unit standard deviation and/or zero mean.  It takes parameters:\nwithStd\n: True by default. Scales the data to unit standard deviation.\nwithMean\n: False by default. Centers the data with mean before scaling. It will build a dense output, so take care when applying to sparse input.\nStandardScaler\nis an\nEstimator\nwhich can be\nfit\non a dataset to produce a\nStandardScalerModel\n; this amounts to computing summary statistics.  The model can then transform a\nVector\ncolumn in a dataset to have unit standard deviation and/or zero mean features.\nNote that if the standard deviation of a feature is zero, it will return default\n0.0\nvalue in the\nVector\nfor that feature.\nExamples\nThe following example demonstrates how to load a dataset in libsvm format and then normalize each feature to have unit standard deviation.\nRefer to the\nStandardScaler Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nStandardScaler\ndataFrame\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\nscaler\n=\nStandardScaler\n(\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nscaledFeatures\n\"\n,\nwithStd\n=\nTrue\n,\nwithMean\n=\nFalse\n)\n# Compute summary statistics by fitting the StandardScaler\nscalerModel\n=\nscaler\n.\nfit\n(\ndataFrame\n)\n# Normalize each feature to have unit standard deviation.\nscaledData\n=\nscalerModel\n.\ntransform\n(\ndataFrame\n)\nscaledData\n.\nshow\n()\nFind full example code at \"examples/src/main/python/ml/standard_scaler_example.py\" in the Spark repo.\nRefer to the\nStandardScaler Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.StandardScaler\nval\ndataFrame\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\nval\nscaler\n=\nnew\nStandardScaler\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"scaledFeatures\"\n)\n.\nsetWithStd\n(\ntrue\n)\n.\nsetWithMean\n(\nfalse\n)\n// Compute summary statistics by fitting the StandardScaler.\nval\nscalerModel\n=\nscaler\n.\nfit\n(\ndataFrame\n)\n// Normalize each feature to have unit standard deviation.\nval\nscaledData\n=\nscalerModel\n.\ntransform\n(\ndataFrame\n)\nscaledData\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\" in the Spark repo.\nRefer to the\nStandardScaler Java docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.StandardScaler\n;\nimport\norg.apache.spark.ml.feature.StandardScalerModel\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nDataset\n<\nRow\n>\ndataFrame\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n);\nStandardScaler\nscaler\n=\nnew\nStandardScaler\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"scaledFeatures\"\n)\n.\nsetWithStd\n(\ntrue\n)\n.\nsetWithMean\n(\nfalse\n);\n// Compute summary statistics by fitting the StandardScaler\nStandardScalerModel\nscalerModel\n=\nscaler\n.\nfit\n(\ndataFrame\n);\n// Normalize each feature to have unit standard deviation.\nDataset\n<\nRow\n>\nscaledData\n=\nscalerModel\n.\ntransform\n(\ndataFrame\n);\nscaledData\n.\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\" in the Spark repo.\nRobustScaler\nRobustScaler\ntransforms a dataset of\nVector\nrows, removing the median and scaling the data according to a specific quantile range (by default the IQR: Interquartile Range, quantile range between the 1st quartile and the 3rd quartile). Its behavior is quite similar to\nStandardScaler\n, however the median and the quantile range are used instead of mean and standard deviation, which make it robust to outliers. It takes parameters:\nlower\n: 0.25 by default. Lower quantile to calculate quantile range, shared by all features.\nupper\n: 0.75 by default. Upper quantile to calculate quantile range, shared by all features.\nwithScaling\n: True by default. Scales the data to quantile range.\nwithCentering\n: False by default. Centers the data with median before scaling. It will build a dense output, so take care when applying to sparse input.\nRobustScaler\nis an\nEstimator\nwhich can be\nfit\non a dataset to produce a\nRobustScalerModel\n; this amounts to computing quantile statistics.  The model can then transform a\nVector\ncolumn in a dataset to have unit quantile range and/or zero median features.\nNote that if the quantile range of a feature is zero, it will return default\n0.0\nvalue in the\nVector\nfor that feature.\nExamples\nThe following example demonstrates how to load a dataset in libsvm format and then normalize each feature to have unit quantile range.\nRefer to the\nRobustScaler Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nRobustScaler\ndataFrame\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\nscaler\n=\nRobustScaler\n(\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nscaledFeatures\n\"\n,\nwithScaling\n=\nTrue\n,\nwithCentering\n=\nFalse\n,\nlower\n=\n0.25\n,\nupper\n=\n0.75\n)\n# Compute summary statistics by fitting the RobustScaler\nscalerModel\n=\nscaler\n.\nfit\n(\ndataFrame\n)\n# Transform each feature to have unit quantile range.\nscaledData\n=\nscalerModel\n.\ntransform\n(\ndataFrame\n)\nscaledData\n.\nshow\n()\nFind full example code at \"examples/src/main/python/ml/robust_scaler_example.py\" in the Spark repo.\nRefer to the\nRobustScaler Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.RobustScaler\nval\ndataFrame\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\nval\nscaler\n=\nnew\nRobustScaler\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"scaledFeatures\"\n)\n.\nsetWithScaling\n(\ntrue\n)\n.\nsetWithCentering\n(\nfalse\n)\n.\nsetLower\n(\n0.25\n)\n.\nsetUpper\n(\n0.75\n)\n// Compute summary statistics by fitting the RobustScaler.\nval\nscalerModel\n=\nscaler\n.\nfit\n(\ndataFrame\n)\n// Transform each feature to have unit quantile range.\nval\nscaledData\n=\nscalerModel\n.\ntransform\n(\ndataFrame\n)\nscaledData\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/RobustScalerExample.scala\" in the Spark repo.\nRefer to the\nRobustScaler Java docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.RobustScaler\n;\nimport\norg.apache.spark.ml.feature.RobustScalerModel\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nDataset\n<\nRow\n>\ndataFrame\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n);\nRobustScaler\nscaler\n=\nnew\nRobustScaler\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"scaledFeatures\"\n)\n.\nsetWithScaling\n(\ntrue\n)\n.\nsetWithCentering\n(\nfalse\n)\n.\nsetLower\n(\n0.25\n)\n.\nsetUpper\n(\n0.75\n);\n// Compute summary statistics by fitting the RobustScaler\nRobustScalerModel\nscalerModel\n=\nscaler\n.\nfit\n(\ndataFrame\n);\n// Transform each feature to have unit quantile range.\nDataset\n<\nRow\n>\nscaledData\n=\nscalerModel\n.\ntransform\n(\ndataFrame\n);\nscaledData\n.\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaRobustScalerExample.java\" in the Spark repo.\nMinMaxScaler\nMinMaxScaler\ntransforms a dataset of\nVector\nrows, rescaling each feature to a specific range (often [0, 1]).  It takes parameters:\nmin\n: 0.0 by default. Lower bound after transformation, shared by all features.\nmax\n: 1.0 by default. Upper bound after transformation, shared by all features.\nMinMaxScaler\ncomputes summary statistics on a data set and produces a\nMinMaxScalerModel\n. The model can then transform each feature individually such that it is in the given range.\nThe rescaled value for a feature E is calculated as,\n\\begin{equation}\n  Rescaled(e_i) = \\frac{e_i - E_{min}}{E_{max} - E_{min}} * (max - min) + min\n\\end{equation}\nFor the case\n$E_{max} == E_{min}$\n,\n$Rescaled(e_i) = 0.5 * (max + min)$\nNote that since zero values will probably be transformed to non-zero values, output of the transformer will be\nDenseVector\neven for sparse input.\nExamples\nThe following example demonstrates how to load a dataset in libsvm format and then rescale each feature to [0, 1].\nRefer to the\nMinMaxScaler Python docs\nand the\nMinMaxScalerModel Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nMinMaxScaler\nfrom\npyspark.ml.linalg\nimport\nVectors\ndataFrame\n=\nspark\n.\ncreateDataFrame\n([\n(\n0\n,\nVectors\n.\ndense\n([\n1.0\n,\n0.1\n,\n-\n1.0\n]),),\n(\n1\n,\nVectors\n.\ndense\n([\n2.0\n,\n1.1\n,\n1.0\n]),),\n(\n2\n,\nVectors\n.\ndense\n([\n3.0\n,\n10.1\n,\n3.0\n]),)\n],\n[\n\"\nid\n\"\n,\n\"\nfeatures\n\"\n])\nscaler\n=\nMinMaxScaler\n(\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nscaledFeatures\n\"\n)\n# Compute summary statistics and generate MinMaxScalerModel\nscalerModel\n=\nscaler\n.\nfit\n(\ndataFrame\n)\n# rescale each feature to range [min, max].\nscaledData\n=\nscalerModel\n.\ntransform\n(\ndataFrame\n)\nprint\n(\n\"\nFeatures scaled to range: [%f, %f]\n\"\n%\n(\nscaler\n.\ngetMin\n(),\nscaler\n.\ngetMax\n()))\nscaledData\n.\nselect\n(\n\"\nfeatures\n\"\n,\n\"\nscaledFeatures\n\"\n).\nshow\n()\nFind full example code at \"examples/src/main/python/ml/min_max_scaler_example.py\" in the Spark repo.\nRefer to the\nMinMaxScaler Scala docs\nand the\nMinMaxScalerModel Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.MinMaxScaler\nimport\norg.apache.spark.ml.linalg.Vectors\nval\ndataFrame\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0\n,\nVectors\n.\ndense\n(\n1.0\n,\n0.1\n,\n-\n1.0\n)),\n(\n1\n,\nVectors\n.\ndense\n(\n2.0\n,\n1.1\n,\n1.0\n)),\n(\n2\n,\nVectors\n.\ndense\n(\n3.0\n,\n10.1\n,\n3.0\n))\n)).\ntoDF\n(\n\"id\"\n,\n\"features\"\n)\nval\nscaler\n=\nnew\nMinMaxScaler\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"scaledFeatures\"\n)\n// Compute summary statistics and generate MinMaxScalerModel\nval\nscalerModel\n=\nscaler\n.\nfit\n(\ndataFrame\n)\n// rescale each feature to range [min, max].\nval\nscaledData\n=\nscalerModel\n.\ntransform\n(\ndataFrame\n)\nprintln\n(\ns\n\"Features scaled to range: [${scaler.getMin}, ${scaler.getMax}]\"\n)\nscaledData\n.\nselect\n(\n\"features\"\n,\n\"scaledFeatures\"\n).\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\" in the Spark repo.\nRefer to the\nMinMaxScaler Java docs\nand the\nMinMaxScalerModel Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.MinMaxScaler\n;\nimport\norg.apache.spark.ml.feature.MinMaxScalerModel\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n0\n,\nVectors\n.\ndense\n(\n1.0\n,\n0.1\n,\n-\n1.0\n)),\nRowFactory\n.\ncreate\n(\n1\n,\nVectors\n.\ndense\n(\n2.0\n,\n1.1\n,\n1.0\n)),\nRowFactory\n.\ncreate\n(\n2\n,\nVectors\n.\ndense\n(\n3.0\n,\n10.1\n,\n3.0\n))\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"id\"\n,\nDataTypes\n.\nIntegerType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"features\"\n,\nnew\nVectorUDT\n(),\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndataFrame\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nMinMaxScaler\nscaler\n=\nnew\nMinMaxScaler\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"scaledFeatures\"\n);\n// Compute summary statistics and generate MinMaxScalerModel\nMinMaxScalerModel\nscalerModel\n=\nscaler\n.\nfit\n(\ndataFrame\n);\n// rescale each feature to range [min, max].\nDataset\n<\nRow\n>\nscaledData\n=\nscalerModel\n.\ntransform\n(\ndataFrame\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Features scaled to range: [\"\n+\nscaler\n.\ngetMin\n()\n+\n\", \"\n+\nscaler\n.\ngetMax\n()\n+\n\"]\"\n);\nscaledData\n.\nselect\n(\n\"features\"\n,\n\"scaledFeatures\"\n).\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\" in the Spark repo.\nMaxAbsScaler\nMaxAbsScaler\ntransforms a dataset of\nVector\nrows, rescaling each feature to range [-1, 1] \nby dividing through the maximum absolute value in each feature. It does not shift/center the \ndata, and thus does not destroy any sparsity.\nMaxAbsScaler\ncomputes summary statistics on a data set and produces a\nMaxAbsScalerModel\n. The \nmodel can then transform each feature individually to range [-1, 1].\nExamples\nThe following example demonstrates how to load a dataset in libsvm format and then rescale each feature to [-1, 1].\nRefer to the\nMaxAbsScaler Python docs\nand the\nMaxAbsScalerModel Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nMaxAbsScaler\nfrom\npyspark.ml.linalg\nimport\nVectors\ndataFrame\n=\nspark\n.\ncreateDataFrame\n([\n(\n0\n,\nVectors\n.\ndense\n([\n1.0\n,\n0.1\n,\n-\n8.0\n]),),\n(\n1\n,\nVectors\n.\ndense\n([\n2.0\n,\n1.0\n,\n-\n4.0\n]),),\n(\n2\n,\nVectors\n.\ndense\n([\n4.0\n,\n10.0\n,\n8.0\n]),)\n],\n[\n\"\nid\n\"\n,\n\"\nfeatures\n\"\n])\nscaler\n=\nMaxAbsScaler\n(\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nscaledFeatures\n\"\n)\n# Compute summary statistics and generate MaxAbsScalerModel\nscalerModel\n=\nscaler\n.\nfit\n(\ndataFrame\n)\n# rescale each feature to range [-1, 1].\nscaledData\n=\nscalerModel\n.\ntransform\n(\ndataFrame\n)\nscaledData\n.\nselect\n(\n\"\nfeatures\n\"\n,\n\"\nscaledFeatures\n\"\n).\nshow\n()\nFind full example code at \"examples/src/main/python/ml/max_abs_scaler_example.py\" in the Spark repo.\nRefer to the\nMaxAbsScaler Scala docs\nand the\nMaxAbsScalerModel Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.MaxAbsScaler\nimport\norg.apache.spark.ml.linalg.Vectors\nval\ndataFrame\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0\n,\nVectors\n.\ndense\n(\n1.0\n,\n0.1\n,\n-\n8.0\n)),\n(\n1\n,\nVectors\n.\ndense\n(\n2.0\n,\n1.0\n,\n-\n4.0\n)),\n(\n2\n,\nVectors\n.\ndense\n(\n4.0\n,\n10.0\n,\n8.0\n))\n)).\ntoDF\n(\n\"id\"\n,\n\"features\"\n)\nval\nscaler\n=\nnew\nMaxAbsScaler\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"scaledFeatures\"\n)\n// Compute summary statistics and generate MaxAbsScalerModel\nval\nscalerModel\n=\nscaler\n.\nfit\n(\ndataFrame\n)\n// rescale each feature to range [-1, 1]\nval\nscaledData\n=\nscalerModel\n.\ntransform\n(\ndataFrame\n)\nscaledData\n.\nselect\n(\n\"features\"\n,\n\"scaledFeatures\"\n).\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\" in the Spark repo.\nRefer to the\nMaxAbsScaler Java docs\nand the\nMaxAbsScalerModel Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.MaxAbsScaler\n;\nimport\norg.apache.spark.ml.feature.MaxAbsScalerModel\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n0\n,\nVectors\n.\ndense\n(\n1.0\n,\n0.1\n,\n-\n8.0\n)),\nRowFactory\n.\ncreate\n(\n1\n,\nVectors\n.\ndense\n(\n2.0\n,\n1.0\n,\n-\n4.0\n)),\nRowFactory\n.\ncreate\n(\n2\n,\nVectors\n.\ndense\n(\n4.0\n,\n10.0\n,\n8.0\n))\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"id\"\n,\nDataTypes\n.\nIntegerType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"features\"\n,\nnew\nVectorUDT\n(),\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndataFrame\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nMaxAbsScaler\nscaler\n=\nnew\nMaxAbsScaler\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"scaledFeatures\"\n);\n// Compute summary statistics and generate MaxAbsScalerModel\nMaxAbsScalerModel\nscalerModel\n=\nscaler\n.\nfit\n(\ndataFrame\n);\n// rescale each feature to range [-1, 1].\nDataset\n<\nRow\n>\nscaledData\n=\nscalerModel\n.\ntransform\n(\ndataFrame\n);\nscaledData\n.\nselect\n(\n\"features\"\n,\n\"scaledFeatures\"\n).\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\" in the Spark repo.\nBucketizer\nBucketizer\ntransforms a column of continuous features to a column of feature buckets, where the buckets are specified by users. It takes a parameter:\nsplits\n: Parameter for mapping continuous features into buckets. With n+1 splits, there are n buckets. A bucket defined by splits x,y holds values in the range [x,y) except the last bucket, which also includes y. Splits should be strictly increasing. Values at -inf, inf must be explicitly provided to cover all Double values; Otherwise, values outside the splits specified will be treated as errors. Two examples of\nsplits\nare\nArray(Double.NegativeInfinity, 0.0, 1.0, Double.PositiveInfinity)\nand\nArray(0.0, 1.0, 2.0)\n.\nNote that if you have no idea of the upper and lower bounds of the targeted column, you should add\nDouble.NegativeInfinity\nand\nDouble.PositiveInfinity\nas the bounds of your splits to prevent a potential out of Bucketizer bounds exception.\nNote also that the splits that you provided have to be in strictly increasing order, i.e.\ns0 < s1 < s2 < ... < sn\n.\nMore details can be found in the API docs for\nBucketizer\n.\nExamples\nThe following example demonstrates how to bucketize a column of\nDouble\ns into another index-wised column.\nRefer to the\nBucketizer Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nBucketizer\nsplits\n=\n[\n-\nfloat\n(\n\"\ninf\n\"\n),\n-\n0.5\n,\n0.0\n,\n0.5\n,\nfloat\n(\n\"\ninf\n\"\n)]\ndata\n=\n[(\n-\n999.9\n,),\n(\n-\n0.5\n,),\n(\n-\n0.3\n,),\n(\n0.0\n,),\n(\n0.2\n,),\n(\n999.9\n,)]\ndataFrame\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\n[\n\"\nfeatures\n\"\n])\nbucketizer\n=\nBucketizer\n(\nsplits\n=\nsplits\n,\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nbucketedFeatures\n\"\n)\n# Transform original data into its bucket index.\nbucketedData\n=\nbucketizer\n.\ntransform\n(\ndataFrame\n)\nprint\n(\n\"\nBucketizer output with %d buckets\n\"\n%\n(\nlen\n(\nbucketizer\n.\ngetSplits\n())\n-\n1\n))\nbucketedData\n.\nshow\n()\nFind full example code at \"examples/src/main/python/ml/bucketizer_example.py\" in the Spark repo.\nRefer to the\nBucketizer Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.Bucketizer\nval\nsplits\n=\nArray\n(\nDouble\n.\nNegativeInfinity\n,\n-\n0.5\n,\n0.0\n,\n0.5\n,\nDouble\n.\nPositiveInfinity\n)\nval\ndata\n=\nArray\n(-\n999.9\n,\n-\n0.5\n,\n-\n0.3\n,\n0.0\n,\n0.2\n,\n999.9\n)\nval\ndataFrame\n=\nspark\n.\ncreateDataFrame\n(\nimmutable\n.\nArraySeq\n.\nunsafeWrapArray\n(\ndata\n.\nmap\n(\nTuple1\n.\napply\n))).\ntoDF\n(\n\"features\"\n)\nval\nbucketizer\n=\nnew\nBucketizer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"bucketedFeatures\"\n)\n.\nsetSplits\n(\nsplits\n)\n// Transform original data into its bucket index.\nval\nbucketedData\n=\nbucketizer\n.\ntransform\n(\ndataFrame\n)\nprintln\n(\ns\n\"Bucketizer output with ${bucketizer.getSplits.length-1} buckets\"\n)\nbucketedData\n.\nshow\n()\nval\nsplitsArray\n=\nArray\n(\nArray\n(\nDouble\n.\nNegativeInfinity\n,\n-\n0.5\n,\n0.0\n,\n0.5\n,\nDouble\n.\nPositiveInfinity\n),\nArray\n(\nDouble\n.\nNegativeInfinity\n,\n-\n0.3\n,\n0.0\n,\n0.3\n,\nDouble\n.\nPositiveInfinity\n))\nval\ndata2\n=\nArray\n(\n(-\n999.9\n,\n-\n999.9\n),\n(-\n0.5\n,\n-\n0.2\n),\n(-\n0.3\n,\n-\n0.1\n),\n(\n0.0\n,\n0.0\n),\n(\n0.2\n,\n0.4\n),\n(\n999.9\n,\n999.9\n))\nval\ndataFrame2\n=\nspark\n.\ncreateDataFrame\n(\nimmutable\n.\nArraySeq\n.\nunsafeWrapArray\n(\ndata2\n))\n.\ntoDF\n(\n\"features1\"\n,\n\"features2\"\n)\nval\nbucketizer2\n=\nnew\nBucketizer\n()\n.\nsetInputCols\n(\nArray\n(\n\"features1\"\n,\n\"features2\"\n))\n.\nsetOutputCols\n(\nArray\n(\n\"bucketedFeatures1\"\n,\n\"bucketedFeatures2\"\n))\n.\nsetSplitsArray\n(\nsplitsArray\n)\n// Transform original data into its bucket index.\nval\nbucketedData2\n=\nbucketizer2\n.\ntransform\n(\ndataFrame2\n)\nprintln\n(\ns\n\"Bucketizer output with [\"\n+\ns\n\"${bucketizer2.getSplitsArray(0).length-1}, \"\n+\ns\n\"${bucketizer2.getSplitsArray(1).length-1}] buckets for each input column\"\n)\nbucketedData2\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\" in the Spark repo.\nRefer to the\nBucketizer Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.Bucketizer\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\ndouble\n[]\nsplits\n=\n{\nDouble\n.\nNEGATIVE_INFINITY\n,\n-\n0.5\n,\n0.0\n,\n0.5\n,\nDouble\n.\nPOSITIVE_INFINITY\n};\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(-\n999.9\n),\nRowFactory\n.\ncreate\n(-\n0.5\n),\nRowFactory\n.\ncreate\n(-\n0.3\n),\nRowFactory\n.\ncreate\n(\n0.0\n),\nRowFactory\n.\ncreate\n(\n0.2\n),\nRowFactory\n.\ncreate\n(\n999.9\n)\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"features\"\n,\nDataTypes\n.\nDoubleType\n,\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndataFrame\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nBucketizer\nbucketizer\n=\nnew\nBucketizer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"bucketedFeatures\"\n)\n.\nsetSplits\n(\nsplits\n);\n// Transform original data into its bucket index.\nDataset\n<\nRow\n>\nbucketedData\n=\nbucketizer\n.\ntransform\n(\ndataFrame\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Bucketizer output with \"\n+\n(\nbucketizer\n.\ngetSplits\n().\nlength\n-\n1\n)\n+\n\" buckets\"\n);\nbucketedData\n.\nshow\n();\n// Bucketize multiple columns at one pass.\ndouble\n[][]\nsplitsArray\n=\n{\n{\nDouble\n.\nNEGATIVE_INFINITY\n,\n-\n0.5\n,\n0.0\n,\n0.5\n,\nDouble\n.\nPOSITIVE_INFINITY\n},\n{\nDouble\n.\nNEGATIVE_INFINITY\n,\n-\n0.3\n,\n0.0\n,\n0.3\n,\nDouble\n.\nPOSITIVE_INFINITY\n}\n};\nList\n<\nRow\n>\ndata2\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(-\n999.9\n,\n-\n999.9\n),\nRowFactory\n.\ncreate\n(-\n0.5\n,\n-\n0.2\n),\nRowFactory\n.\ncreate\n(-\n0.3\n,\n-\n0.1\n),\nRowFactory\n.\ncreate\n(\n0.0\n,\n0.0\n),\nRowFactory\n.\ncreate\n(\n0.2\n,\n0.4\n),\nRowFactory\n.\ncreate\n(\n999.9\n,\n999.9\n)\n);\nStructType\nschema2\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"features1\"\n,\nDataTypes\n.\nDoubleType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"features2\"\n,\nDataTypes\n.\nDoubleType\n,\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndataFrame2\n=\nspark\n.\ncreateDataFrame\n(\ndata2\n,\nschema2\n);\nBucketizer\nbucketizer2\n=\nnew\nBucketizer\n()\n.\nsetInputCols\n(\nnew\nString\n[]\n{\n\"features1\"\n,\n\"features2\"\n})\n.\nsetOutputCols\n(\nnew\nString\n[]\n{\n\"bucketedFeatures1\"\n,\n\"bucketedFeatures2\"\n})\n.\nsetSplitsArray\n(\nsplitsArray\n);\n// Transform original data into its bucket index.\nDataset\n<\nRow\n>\nbucketedData2\n=\nbucketizer2\n.\ntransform\n(\ndataFrame2\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Bucketizer output with [\"\n+\n(\nbucketizer2\n.\ngetSplitsArray\n()[\n0\n].\nlength\n-\n1\n)\n+\n\", \"\n+\n(\nbucketizer2\n.\ngetSplitsArray\n()[\n1\n].\nlength\n-\n1\n)\n+\n\"] buckets for each input column\"\n);\nbucketedData2\n.\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\" in the Spark repo.\nElementwiseProduct\nElementwiseProduct multiplies each input vector by a provided “weight” vector, using element-wise multiplication. In other words, it scales each column of the dataset by a scalar multiplier.  This represents the\nHadamard product\nbetween the input vector,\nv\nand transforming vector,\nw\n, to yield a result vector.\n\\[ \\begin{pmatrix}\nv_1 \\\\\n\\vdots \\\\\nv_N\n\\end{pmatrix} \\circ \\begin{pmatrix}\n                    w_1 \\\\\n                    \\vdots \\\\\n                    w_N\n                    \\end{pmatrix}\n= \\begin{pmatrix}\n  v_1 w_1 \\\\\n  \\vdots \\\\\n  v_N w_N\n  \\end{pmatrix}\n\\]\nExamples\nThis example below demonstrates how to transform vectors using a transforming vector value.\nRefer to the\nElementwiseProduct Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nElementwiseProduct\nfrom\npyspark.ml.linalg\nimport\nVectors\n# Create some vector data; also works for sparse vectors\ndata\n=\n[(\nVectors\n.\ndense\n([\n1.0\n,\n2.0\n,\n3.0\n]),),\n(\nVectors\n.\ndense\n([\n4.0\n,\n5.0\n,\n6.0\n]),)]\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\n[\n\"\nvector\n\"\n])\ntransformer\n=\nElementwiseProduct\n(\nscalingVec\n=\nVectors\n.\ndense\n([\n0.0\n,\n1.0\n,\n2.0\n]),\ninputCol\n=\n\"\nvector\n\"\n,\noutputCol\n=\n\"\ntransformedVector\n\"\n)\n# Batch transform the vectors to create new column:\ntransformer\n.\ntransform\n(\ndf\n).\nshow\n()\nFind full example code at \"examples/src/main/python/ml/elementwise_product_example.py\" in the Spark repo.\nRefer to the\nElementwiseProduct Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.ElementwiseProduct\nimport\norg.apache.spark.ml.linalg.Vectors\n// Create some vector data; also works for sparse vectors\nval\ndataFrame\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n\"a\"\n,\nVectors\n.\ndense\n(\n1.0\n,\n2.0\n,\n3.0\n)),\n(\n\"b\"\n,\nVectors\n.\ndense\n(\n4.0\n,\n5.0\n,\n6.0\n)))).\ntoDF\n(\n\"id\"\n,\n\"vector\"\n)\nval\ntransformingVector\n=\nVectors\n.\ndense\n(\n0.0\n,\n1.0\n,\n2.0\n)\nval\ntransformer\n=\nnew\nElementwiseProduct\n()\n.\nsetScalingVec\n(\ntransformingVector\n)\n.\nsetInputCol\n(\n\"vector\"\n)\n.\nsetOutputCol\n(\n\"transformedVector\"\n)\n// Batch transform the vectors to create new column:\ntransformer\n.\ntransform\n(\ndataFrame\n).\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\" in the Spark repo.\nRefer to the\nElementwiseProduct Java docs\nfor more details on the API.\nimport\njava.util.ArrayList\n;\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.ElementwiseProduct\n;\nimport\norg.apache.spark.ml.linalg.Vector\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\n// Create some vector data; also works for sparse vectors\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n\"a\"\n,\nVectors\n.\ndense\n(\n1.0\n,\n2.0\n,\n3.0\n)),\nRowFactory\n.\ncreate\n(\n\"b\"\n,\nVectors\n.\ndense\n(\n4.0\n,\n5.0\n,\n6.0\n))\n);\nList\n<\nStructField\n>\nfields\n=\nnew\nArrayList\n<>(\n2\n);\nfields\n.\nadd\n(\nDataTypes\n.\ncreateStructField\n(\n\"id\"\n,\nDataTypes\n.\nStringType\n,\nfalse\n));\nfields\n.\nadd\n(\nDataTypes\n.\ncreateStructField\n(\n\"vector\"\n,\nnew\nVectorUDT\n(),\nfalse\n));\nStructType\nschema\n=\nDataTypes\n.\ncreateStructType\n(\nfields\n);\nDataset\n<\nRow\n>\ndataFrame\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nVector\ntransformingVector\n=\nVectors\n.\ndense\n(\n0.0\n,\n1.0\n,\n2.0\n);\nElementwiseProduct\ntransformer\n=\nnew\nElementwiseProduct\n()\n.\nsetScalingVec\n(\ntransformingVector\n)\n.\nsetInputCol\n(\n\"vector\"\n)\n.\nsetOutputCol\n(\n\"transformedVector\"\n);\n// Batch transform the vectors to create new column:\ntransformer\n.\ntransform\n(\ndataFrame\n).\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\" in the Spark repo.\nSQLTransformer\nSQLTransformer\nimplements the transformations which are defined by SQL statement.\nCurrently, we only support SQL syntax like\n\"SELECT ... FROM __THIS__ ...\"\nwhere\n\"__THIS__\"\nrepresents the underlying table of the input dataset.\nThe select clause specifies the fields, constants, and expressions to display in\nthe output, and can be any select clause that Spark SQL supports. Users can also\nuse Spark SQL built-in function and UDFs to operate on these selected columns.\nFor example,\nSQLTransformer\nsupports statements like:\nSELECT a, a + b AS a_b FROM __THIS__\nSELECT a, SQRT(b) AS b_sqrt FROM __THIS__ where a > 5\nSELECT a, b, SUM(c) AS c_sum FROM __THIS__ GROUP BY a, b\nExamples\nAssume that we have the following DataFrame with columns\nid\n,\nv1\nand\nv2\n:\nid |  v1 |  v2\n----|-----|-----\n 0  | 1.0 | 3.0  \n 2  | 2.0 | 5.0\nThis is the output of the\nSQLTransformer\nwith statement\n\"SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__\"\n:\nid |  v1 |  v2 |  v3 |  v4\n----|-----|-----|-----|-----\n 0  | 1.0 | 3.0 | 4.0 | 3.0\n 2  | 2.0 | 5.0 | 7.0 |10.0\nRefer to the\nSQLTransformer Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nSQLTransformer\ndf\n=\nspark\n.\ncreateDataFrame\n([\n(\n0\n,\n1.0\n,\n3.0\n),\n(\n2\n,\n2.0\n,\n5.0\n)\n],\n[\n\"\nid\n\"\n,\n\"\nv1\n\"\n,\n\"\nv2\n\"\n])\nsqlTrans\n=\nSQLTransformer\n(\nstatement\n=\n\"\nSELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__\n\"\n)\nsqlTrans\n.\ntransform\n(\ndf\n).\nshow\n()\nFind full example code at \"examples/src/main/python/ml/sql_transformer.py\" in the Spark repo.\nRefer to the\nSQLTransformer Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.SQLTransformer\nval\ndf\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n((\n0\n,\n1.0\n,\n3.0\n),\n(\n2\n,\n2.0\n,\n5.0\n))).\ntoDF\n(\n\"id\"\n,\n\"v1\"\n,\n\"v2\"\n)\nval\nsqlTrans\n=\nnew\nSQLTransformer\n().\nsetStatement\n(\n\"SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__\"\n)\nsqlTrans\n.\ntransform\n(\ndf\n).\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\" in the Spark repo.\nRefer to the\nSQLTransformer Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.SQLTransformer\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.SparkSession\n;\nimport\norg.apache.spark.sql.types.*\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n0\n,\n1.0\n,\n3.0\n),\nRowFactory\n.\ncreate\n(\n2\n,\n2.0\n,\n5.0\n)\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]\n{\nnew\nStructField\n(\n\"id\"\n,\nDataTypes\n.\nIntegerType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"v1\"\n,\nDataTypes\n.\nDoubleType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"v2\"\n,\nDataTypes\n.\nDoubleType\n,\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nSQLTransformer\nsqlTrans\n=\nnew\nSQLTransformer\n().\nsetStatement\n(\n\"SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__\"\n);\nsqlTrans\n.\ntransform\n(\ndf\n).\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\" in the Spark repo.\nVectorAssembler\nVectorAssembler\nis a transformer that combines a given list of columns into a single vector\ncolumn.\nIt is useful for combining raw features and features generated by different feature transformers\ninto a single feature vector, in order to train ML models like logistic regression and decision\ntrees.\nVectorAssembler\naccepts the following input column types: all numeric types, boolean type,\nand vector type.\nIn each row, the values of the input columns will be concatenated into a vector in the specified\norder.\nExamples\nAssume that we have a DataFrame with the columns\nid\n,\nhour\n,\nmobile\n,\nuserFeatures\n,\nand\nclicked\n:\nid | hour | mobile | userFeatures     | clicked\n----|------|--------|------------------|---------\n 0  | 18   | 1.0    | [0.0, 10.0, 0.5] | 1.0\nuserFeatures\nis a vector column that contains three user features.\nWe want to combine\nhour\n,\nmobile\n, and\nuserFeatures\ninto a single feature vector\ncalled\nfeatures\nand use it to predict\nclicked\nor not.\nIf we set\nVectorAssembler\n’s input columns to\nhour\n,\nmobile\n, and\nuserFeatures\nand\noutput column to\nfeatures\n, after transformation we should get the following DataFrame:\nid | hour | mobile | userFeatures     | clicked | features\n----|------|--------|------------------|---------|-----------------------------\n 0  | 18   | 1.0    | [0.0, 10.0, 0.5] | 1.0     | [18.0, 1.0, 0.0, 10.0, 0.5]\nRefer to the\nVectorAssembler Python docs\nfor more details on the API.\nfrom\npyspark.ml.linalg\nimport\nVectors\nfrom\npyspark.ml.feature\nimport\nVectorAssembler\ndataset\n=\nspark\n.\ncreateDataFrame\n(\n[(\n0\n,\n18\n,\n1.0\n,\nVectors\n.\ndense\n([\n0.0\n,\n10.0\n,\n0.5\n]),\n1.0\n)],\n[\n\"\nid\n\"\n,\n\"\nhour\n\"\n,\n\"\nmobile\n\"\n,\n\"\nuserFeatures\n\"\n,\n\"\nclicked\n\"\n])\nassembler\n=\nVectorAssembler\n(\ninputCols\n=\n[\n\"\nhour\n\"\n,\n\"\nmobile\n\"\n,\n\"\nuserFeatures\n\"\n],\noutputCol\n=\n\"\nfeatures\n\"\n)\noutput\n=\nassembler\n.\ntransform\n(\ndataset\n)\nprint\n(\n\"\nAssembled columns\n'\nhour\n'\n,\n'\nmobile\n'\n,\n'\nuserFeatures\n'\nto vector column\n'\nfeatures\n'\"\n)\noutput\n.\nselect\n(\n\"\nfeatures\n\"\n,\n\"\nclicked\n\"\n).\nshow\n(\ntruncate\n=\nFalse\n)\nFind full example code at \"examples/src/main/python/ml/vector_assembler_example.py\" in the Spark repo.\nRefer to the\nVectorAssembler Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.VectorAssembler\nimport\norg.apache.spark.ml.linalg.Vectors\nval\ndataset\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n((\n0\n,\n18\n,\n1.0\n,\nVectors\n.\ndense\n(\n0.0\n,\n10.0\n,\n0.5\n),\n1.0\n))\n).\ntoDF\n(\n\"id\"\n,\n\"hour\"\n,\n\"mobile\"\n,\n\"userFeatures\"\n,\n\"clicked\"\n)\nval\nassembler\n=\nnew\nVectorAssembler\n()\n.\nsetInputCols\n(\nArray\n(\n\"hour\"\n,\n\"mobile\"\n,\n\"userFeatures\"\n))\n.\nsetOutputCol\n(\n\"features\"\n)\nval\noutput\n=\nassembler\n.\ntransform\n(\ndataset\n)\nprintln\n(\n\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\"\n)\noutput\n.\nselect\n(\n\"features\"\n,\n\"clicked\"\n).\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\" in the Spark repo.\nRefer to the\nVectorAssembler Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\norg.apache.spark.ml.feature.VectorAssembler\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.*\n;\nimport\nstatic\norg\n.\napache\n.\nspark\n.\nsql\n.\ntypes\n.\nDataTypes\n.*;\nStructType\nschema\n=\ncreateStructType\n(\nnew\nStructField\n[]{\ncreateStructField\n(\n\"id\"\n,\nIntegerType\n,\nfalse\n),\ncreateStructField\n(\n\"hour\"\n,\nIntegerType\n,\nfalse\n),\ncreateStructField\n(\n\"mobile\"\n,\nDoubleType\n,\nfalse\n),\ncreateStructField\n(\n\"userFeatures\"\n,\nnew\nVectorUDT\n(),\nfalse\n),\ncreateStructField\n(\n\"clicked\"\n,\nDoubleType\n,\nfalse\n)\n});\nRow\nrow\n=\nRowFactory\n.\ncreate\n(\n0\n,\n18\n,\n1.0\n,\nVectors\n.\ndense\n(\n0.0\n,\n10.0\n,\n0.5\n),\n1.0\n);\nDataset\n<\nRow\n>\ndataset\n=\nspark\n.\ncreateDataFrame\n(\nArrays\n.\nasList\n(\nrow\n),\nschema\n);\nVectorAssembler\nassembler\n=\nnew\nVectorAssembler\n()\n.\nsetInputCols\n(\nnew\nString\n[]{\n\"hour\"\n,\n\"mobile\"\n,\n\"userFeatures\"\n})\n.\nsetOutputCol\n(\n\"features\"\n);\nDataset\n<\nRow\n>\noutput\n=\nassembler\n.\ntransform\n(\ndataset\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column \"\n+\n\"'features'\"\n);\noutput\n.\nselect\n(\n\"features\"\n,\n\"clicked\"\n).\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\" in the Spark repo.\nVectorSizeHint\nIt can sometimes be useful to explicitly specify the size of the vectors for a column of\nVectorType\n. For example,\nVectorAssembler\nuses size information from its input columns to\nproduce size information and metadata for its output column. While in some cases this information\ncan be obtained by inspecting the contents of the column, in a streaming dataframe the contents are\nnot available until the stream is started.\nVectorSizeHint\nallows a user to explicitly specify the\nvector size for a column so that\nVectorAssembler\n, or other transformers that might\nneed to know vector size, can use that column as an input.\nTo use\nVectorSizeHint\na user must set the\ninputCol\nand\nsize\nparameters. Applying this\ntransformer to a dataframe produces a new dataframe with updated metadata for\ninputCol\nspecifying\nthe vector size. Downstream operations on the resulting dataframe can get this size using the\nmetadata.\nVectorSizeHint\ncan also take an optional\nhandleInvalid\nparameter which controls its\nbehaviour when the vector column contains nulls or vectors of the wrong size. By default\nhandleInvalid\nis set to “error”, indicating an exception should be thrown. This parameter can\nalso be set to “skip”, indicating that rows containing invalid values should be filtered out from\nthe resulting dataframe, or “optimistic”, indicating that the column should not be checked for\ninvalid values and all rows should be kept. Note that the use of “optimistic” can cause the\nresulting dataframe to be in an inconsistent state, meaning the metadata for the column\nVectorSizeHint\nwas applied to does not match the contents of that column. Users should take care\nto avoid this kind of inconsistent state.\nRefer to the\nVectorSizeHint Python docs\nfor more details on the API.\nfrom\npyspark.ml.linalg\nimport\nVectors\nfrom\npyspark.ml.feature\nimport\n(\nVectorSizeHint\n,\nVectorAssembler\n)\ndataset\n=\nspark\n.\ncreateDataFrame\n(\n[(\n0\n,\n18\n,\n1.0\n,\nVectors\n.\ndense\n([\n0.0\n,\n10.0\n,\n0.5\n]),\n1.0\n),\n(\n0\n,\n18\n,\n1.0\n,\nVectors\n.\ndense\n([\n0.0\n,\n10.0\n]),\n0.0\n)],\n[\n\"\nid\n\"\n,\n\"\nhour\n\"\n,\n\"\nmobile\n\"\n,\n\"\nuserFeatures\n\"\n,\n\"\nclicked\n\"\n])\nsizeHint\n=\nVectorSizeHint\n(\ninputCol\n=\n\"\nuserFeatures\n\"\n,\nhandleInvalid\n=\n\"\nskip\n\"\n,\nsize\n=\n3\n)\ndatasetWithSize\n=\nsizeHint\n.\ntransform\n(\ndataset\n)\nprint\n(\n\"\nRows where\n'\nuserFeatures\n'\nis not the right size are filtered out\n\"\n)\ndatasetWithSize\n.\nshow\n(\ntruncate\n=\nFalse\n)\nassembler\n=\nVectorAssembler\n(\ninputCols\n=\n[\n\"\nhour\n\"\n,\n\"\nmobile\n\"\n,\n\"\nuserFeatures\n\"\n],\noutputCol\n=\n\"\nfeatures\n\"\n)\n# This dataframe can be used by downstream transformers as before\noutput\n=\nassembler\n.\ntransform\n(\ndatasetWithSize\n)\nprint\n(\n\"\nAssembled columns\n'\nhour\n'\n,\n'\nmobile\n'\n,\n'\nuserFeatures\n'\nto vector column\n'\nfeatures\n'\"\n)\noutput\n.\nselect\n(\n\"\nfeatures\n\"\n,\n\"\nclicked\n\"\n).\nshow\n(\ntruncate\n=\nFalse\n)\nFind full example code at \"examples/src/main/python/ml/vector_size_hint_example.py\" in the Spark repo.\nRefer to the\nVectorSizeHint Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.\n{\nVectorAssembler\n,\nVectorSizeHint\n}\nimport\norg.apache.spark.ml.linalg.Vectors\nval\ndataset\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0\n,\n18\n,\n1.0\n,\nVectors\n.\ndense\n(\n0.0\n,\n10.0\n,\n0.5\n),\n1.0\n),\n(\n0\n,\n18\n,\n1.0\n,\nVectors\n.\ndense\n(\n0.0\n,\n10.0\n),\n0.0\n))\n).\ntoDF\n(\n\"id\"\n,\n\"hour\"\n,\n\"mobile\"\n,\n\"userFeatures\"\n,\n\"clicked\"\n)\nval\nsizeHint\n=\nnew\nVectorSizeHint\n()\n.\nsetInputCol\n(\n\"userFeatures\"\n)\n.\nsetHandleInvalid\n(\n\"skip\"\n)\n.\nsetSize\n(\n3\n)\nval\ndatasetWithSize\n=\nsizeHint\n.\ntransform\n(\ndataset\n)\nprintln\n(\n\"Rows where 'userFeatures' is not the right size are filtered out\"\n)\ndatasetWithSize\n.\nshow\n(\nfalse\n)\nval\nassembler\n=\nnew\nVectorAssembler\n()\n.\nsetInputCols\n(\nArray\n(\n\"hour\"\n,\n\"mobile\"\n,\n\"userFeatures\"\n))\n.\nsetOutputCol\n(\n\"features\"\n)\n// This dataframe can be used by downstream transformers as before\nval\noutput\n=\nassembler\n.\ntransform\n(\ndatasetWithSize\n)\nprintln\n(\n\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\"\n)\noutput\n.\nselect\n(\n\"features\"\n,\n\"clicked\"\n).\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/VectorSizeHintExample.scala\" in the Spark repo.\nRefer to the\nVectorSizeHint Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\norg.apache.spark.ml.feature.VectorAssembler\n;\nimport\norg.apache.spark.ml.feature.VectorSizeHint\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nimport\nstatic\norg\n.\napache\n.\nspark\n.\nsql\n.\ntypes\n.\nDataTypes\n.*;\nStructType\nschema\n=\ncreateStructType\n(\nnew\nStructField\n[]{\ncreateStructField\n(\n\"id\"\n,\nIntegerType\n,\nfalse\n),\ncreateStructField\n(\n\"hour\"\n,\nIntegerType\n,\nfalse\n),\ncreateStructField\n(\n\"mobile\"\n,\nDoubleType\n,\nfalse\n),\ncreateStructField\n(\n\"userFeatures\"\n,\nnew\nVectorUDT\n(),\nfalse\n),\ncreateStructField\n(\n\"clicked\"\n,\nDoubleType\n,\nfalse\n)\n});\nRow\nrow0\n=\nRowFactory\n.\ncreate\n(\n0\n,\n18\n,\n1.0\n,\nVectors\n.\ndense\n(\n0.0\n,\n10.0\n,\n0.5\n),\n1.0\n);\nRow\nrow1\n=\nRowFactory\n.\ncreate\n(\n0\n,\n18\n,\n1.0\n,\nVectors\n.\ndense\n(\n0.0\n,\n10.0\n),\n0.0\n);\nDataset\n<\nRow\n>\ndataset\n=\nspark\n.\ncreateDataFrame\n(\nArrays\n.\nasList\n(\nrow0\n,\nrow1\n),\nschema\n);\nVectorSizeHint\nsizeHint\n=\nnew\nVectorSizeHint\n()\n.\nsetInputCol\n(\n\"userFeatures\"\n)\n.\nsetHandleInvalid\n(\n\"skip\"\n)\n.\nsetSize\n(\n3\n);\nDataset\n<\nRow\n>\ndatasetWithSize\n=\nsizeHint\n.\ntransform\n(\ndataset\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Rows where 'userFeatures' is not the right size are filtered out\"\n);\ndatasetWithSize\n.\nshow\n(\nfalse\n);\nVectorAssembler\nassembler\n=\nnew\nVectorAssembler\n()\n.\nsetInputCols\n(\nnew\nString\n[]{\n\"hour\"\n,\n\"mobile\"\n,\n\"userFeatures\"\n})\n.\nsetOutputCol\n(\n\"features\"\n);\n// This dataframe can be used by downstream transformers as before\nDataset\n<\nRow\n>\noutput\n=\nassembler\n.\ntransform\n(\ndatasetWithSize\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column \"\n+\n\"'features'\"\n);\noutput\n.\nselect\n(\n\"features\"\n,\n\"clicked\"\n).\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSizeHintExample.java\" in the Spark repo.\nQuantileDiscretizer\nQuantileDiscretizer\ntakes a column with continuous features and outputs a column with binned\ncategorical features. The number of bins is set by the\nnumBuckets\nparameter. It is possible\nthat the number of buckets used will be smaller than this value, for example, if there are too few\ndistinct values of the input to create enough distinct quantiles.\nNaN values:\nNaN values will be removed from the column during\nQuantileDiscretizer\nfitting. This will produce\na\nBucketizer\nmodel for making predictions. During the transformation,\nBucketizer\nwill raise an error when it finds NaN values in the dataset, but the user can also choose to either\nkeep or remove NaN values within the dataset by setting\nhandleInvalid\n. If the user chooses to keep\nNaN values, they will be handled specially and placed into their own bucket, for example, if 4 buckets\nare used, then non-NaN data will be put into buckets[0-3], but NaNs will be counted in a special bucket[4].\nAlgorithm: The bin ranges are chosen using an approximate algorithm (see the documentation for\napproxQuantile\nfor a\ndetailed description). The precision of the approximation can be controlled with the\nrelativeError\nparameter. When set to zero, exact quantiles are calculated\n(\nNote:\nComputing exact quantiles is an expensive operation). The lower and upper bin bounds\nwill be\n-Infinity\nand\n+Infinity\ncovering all real values.\nExamples\nAssume that we have a DataFrame with the columns\nid\n,\nhour\n:\nid | hour\n----|------\n 0  | 18.0\n----|------\n 1  | 19.0\n----|------\n 2  | 8.0\n----|------\n 3  | 5.0\n----|------\n 4  | 2.2\nhour\nis a continuous feature with\nDouble\ntype. We want to turn the continuous feature into\na categorical one. Given\nnumBuckets = 3\n, we should get the following DataFrame:\nid | hour | result\n----|------|------\n 0  | 18.0 | 2.0\n----|------|------\n 1  | 19.0 | 2.0\n----|------|------\n 2  | 8.0  | 1.0\n----|------|------\n 3  | 5.0  | 1.0\n----|------|------\n 4  | 2.2  | 0.0\nRefer to the\nQuantileDiscretizer Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nQuantileDiscretizer\ndata\n=\n[(\n0\n,\n18.0\n),\n(\n1\n,\n19.0\n),\n(\n2\n,\n8.0\n),\n(\n3\n,\n5.0\n),\n(\n4\n,\n2.2\n)]\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\n[\n\"\nid\n\"\n,\n\"\nhour\n\"\n])\ndiscretizer\n=\nQuantileDiscretizer\n(\nnumBuckets\n=\n3\n,\ninputCol\n=\n\"\nhour\n\"\n,\noutputCol\n=\n\"\nresult\n\"\n)\nresult\n=\ndiscretizer\n.\nfit\n(\ndf\n).\ntransform\n(\ndf\n)\nresult\n.\nshow\n()\nFind full example code at \"examples/src/main/python/ml/quantile_discretizer_example.py\" in the Spark repo.\nRefer to the\nQuantileDiscretizer Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.QuantileDiscretizer\nval\ndata\n=\nArray\n((\n0\n,\n18.0\n),\n(\n1\n,\n19.0\n),\n(\n2\n,\n8.0\n),\n(\n3\n,\n5.0\n),\n(\n4\n,\n2.2\n))\nval\ndf\n=\nspark\n.\ncreateDataFrame\n(\nimmutable\n.\nArraySeq\n.\nunsafeWrapArray\n(\ndata\n)).\ntoDF\n(\n\"id\"\n,\n\"hour\"\n)\nval\ndiscretizer\n=\nnew\nQuantileDiscretizer\n()\n.\nsetInputCol\n(\n\"hour\"\n)\n.\nsetOutputCol\n(\n\"result\"\n)\n.\nsetNumBuckets\n(\n3\n)\nval\nresult\n=\ndiscretizer\n.\nfit\n(\ndf\n).\ntransform\n(\ndf\n)\nresult\n.\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\" in the Spark repo.\nRefer to the\nQuantileDiscretizer Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.QuantileDiscretizer\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n0\n,\n18.0\n),\nRowFactory\n.\ncreate\n(\n1\n,\n19.0\n),\nRowFactory\n.\ncreate\n(\n2\n,\n8.0\n),\nRowFactory\n.\ncreate\n(\n3\n,\n5.0\n),\nRowFactory\n.\ncreate\n(\n4\n,\n2.2\n)\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"id\"\n,\nDataTypes\n.\nIntegerType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"hour\"\n,\nDataTypes\n.\nDoubleType\n,\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nQuantileDiscretizer\ndiscretizer\n=\nnew\nQuantileDiscretizer\n()\n.\nsetInputCol\n(\n\"hour\"\n)\n.\nsetOutputCol\n(\n\"result\"\n)\n.\nsetNumBuckets\n(\n3\n);\nDataset\n<\nRow\n>\nresult\n=\ndiscretizer\n.\nfit\n(\ndf\n).\ntransform\n(\ndf\n);\nresult\n.\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\" in the Spark repo.\nImputer\nThe\nImputer\nestimator completes missing values in a dataset, using the mean, median or mode\nof the columns in which the missing values are located. The input columns should be of\nnumeric type. Currently\nImputer\ndoes not support categorical features and possibly\ncreates incorrect values for columns containing categorical features. Imputer can impute custom values \nother than ‘NaN’ by\n.setMissingValue(custom_value)\n. For example,\n.setMissingValue(0)\nwill impute \nall occurrences of (0).\nNote\nall\nnull\nvalues in the input columns are treated as missing, and so are also imputed.\nExamples\nSuppose that we have a DataFrame with the columns\na\nand\nb\n:\na     |      b      \n------------|-----------\n     1.0    | Double.NaN\n     2.0    | Double.NaN\n Double.NaN |     3.0   \n     4.0    |     4.0   \n     5.0    |     5.0\nIn this example, Imputer will replace all occurrences of\nDouble.NaN\n(the default for the missing value)\nwith the mean (the default imputation strategy) computed from the other values in the corresponding columns.\nIn this example, the surrogate values for columns\na\nand\nb\nare 3.0 and 4.0 respectively. After\ntransformation, the missing values in the output columns will be replaced by the surrogate value for\nthe relevant column.\na     |      b     | out_a | out_b   \n------------|------------|-------|-------\n     1.0    | Double.NaN |  1.0  |  4.0 \n     2.0    | Double.NaN |  2.0  |  4.0 \n Double.NaN |     3.0    |  3.0  |  3.0 \n     4.0    |     4.0    |  4.0  |  4.0\n     5.0    |     5.0    |  5.0  |  5.0\nRefer to the\nImputer Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nImputer\ndf\n=\nspark\n.\ncreateDataFrame\n([\n(\n1.0\n,\nfloat\n(\n\"\nnan\n\"\n)),\n(\n2.0\n,\nfloat\n(\n\"\nnan\n\"\n)),\n(\nfloat\n(\n\"\nnan\n\"\n),\n3.0\n),\n(\n4.0\n,\n4.0\n),\n(\n5.0\n,\n5.0\n)\n],\n[\n\"\na\n\"\n,\n\"\nb\n\"\n])\nimputer\n=\nImputer\n(\ninputCols\n=\n[\n\"\na\n\"\n,\n\"\nb\n\"\n],\noutputCols\n=\n[\n\"\nout_a\n\"\n,\n\"\nout_b\n\"\n])\nmodel\n=\nimputer\n.\nfit\n(\ndf\n)\nmodel\n.\ntransform\n(\ndf\n).\nshow\n()\nFind full example code at \"examples/src/main/python/ml/imputer_example.py\" in the Spark repo.\nRefer to the\nImputer Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.Imputer\nval\ndf\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n1.0\n,\nDouble\n.\nNaN\n),\n(\n2.0\n,\nDouble\n.\nNaN\n),\n(\nDouble\n.\nNaN\n,\n3.0\n),\n(\n4.0\n,\n4.0\n),\n(\n5.0\n,\n5.0\n)\n)).\ntoDF\n(\n\"a\"\n,\n\"b\"\n)\nval\nimputer\n=\nnew\nImputer\n()\n.\nsetInputCols\n(\nArray\n(\n\"a\"\n,\n\"b\"\n))\n.\nsetOutputCols\n(\nArray\n(\n\"out_a\"\n,\n\"out_b\"\n))\nval\nmodel\n=\nimputer\n.\nfit\n(\ndf\n)\nmodel\n.\ntransform\n(\ndf\n).\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\" in the Spark repo.\nRefer to the\nImputer Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.Imputer\n;\nimport\norg.apache.spark.ml.feature.ImputerModel\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.SparkSession\n;\nimport\norg.apache.spark.sql.types.*\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n1.0\n,\nDouble\n.\nNaN\n),\nRowFactory\n.\ncreate\n(\n2.0\n,\nDouble\n.\nNaN\n),\nRowFactory\n.\ncreate\n(\nDouble\n.\nNaN\n,\n3.0\n),\nRowFactory\n.\ncreate\n(\n4.0\n,\n4.0\n),\nRowFactory\n.\ncreate\n(\n5.0\n,\n5.0\n)\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\ncreateStructField\n(\n\"a\"\n,\nDoubleType\n,\nfalse\n),\ncreateStructField\n(\n\"b\"\n,\nDoubleType\n,\nfalse\n)\n});\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nImputer\nimputer\n=\nnew\nImputer\n()\n.\nsetInputCols\n(\nnew\nString\n[]{\n\"a\"\n,\n\"b\"\n})\n.\nsetOutputCols\n(\nnew\nString\n[]{\n\"out_a\"\n,\n\"out_b\"\n});\nImputerModel\nmodel\n=\nimputer\n.\nfit\n(\ndf\n);\nmodel\n.\ntransform\n(\ndf\n).\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\" in the Spark repo.\nFeature Selectors\nVectorSlicer\nVectorSlicer\nis a transformer that takes a feature vector and outputs a new feature vector with a\nsub-array of the original features. It is useful for extracting features from a vector column.\nVectorSlicer\naccepts a vector column with specified indices, then outputs a new vector column\nwhose values are selected via those indices. There are two types of indices,\nInteger indices that represent the indices into the vector,\nsetIndices()\n.\nString indices that represent the names of features into the vector,\nsetNames()\n.\nThis requires the vector column to have an\nAttributeGroup\nsince the implementation matches on\n the name field of an\nAttribute\n.\nSpecification by integer and string are both acceptable. Moreover, you can use integer index and\nstring name simultaneously. At least one feature must be selected. Duplicate features are not\nallowed, so there can be no overlap between selected indices and names. Note that if names of\nfeatures are selected, an exception will be thrown if empty input attributes are encountered.\nThe output vector will order features with the selected indices first (in the order given),\nfollowed by the selected names (in the order given).\nExamples\nSuppose that we have a DataFrame with the column\nuserFeatures\n:\nuserFeatures\n------------------\n [0.0, 10.0, 0.5]\nuserFeatures\nis a vector column that contains three user features. Assume that the first column\nof\nuserFeatures\nare all zeros, so we want to remove it and select only the last two columns.\nThe\nVectorSlicer\nselects the last two elements with\nsetIndices(1, 2)\nthen produces a new vector\ncolumn named\nfeatures\n:\nuserFeatures     | features\n------------------|-----------------------------\n [0.0, 10.0, 0.5] | [10.0, 0.5]\nSuppose also that we have potential input attributes for the\nuserFeatures\n, i.e.\n[\"f1\", \"f2\", \"f3\"]\n, then we can use\nsetNames(\"f2\", \"f3\")\nto select them.\nuserFeatures     | features\n------------------|-----------------------------\n [0.0, 10.0, 0.5] | [10.0, 0.5]\n [\"f1\", \"f2\", \"f3\"] | [\"f2\", \"f3\"]\nRefer to the\nVectorSlicer Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nVectorSlicer\nfrom\npyspark.ml.linalg\nimport\nVectors\nfrom\npyspark.sql.types\nimport\nRow\ndf\n=\nspark\n.\ncreateDataFrame\n([\nRow\n(\nuserFeatures\n=\nVectors\n.\nsparse\n(\n3\n,\n{\n0\n:\n-\n2.0\n,\n1\n:\n2.3\n})),\nRow\n(\nuserFeatures\n=\nVectors\n.\ndense\n([\n-\n2.0\n,\n2.3\n,\n0.0\n]))])\nslicer\n=\nVectorSlicer\n(\ninputCol\n=\n\"\nuserFeatures\n\"\n,\noutputCol\n=\n\"\nfeatures\n\"\n,\nindices\n=\n[\n1\n])\noutput\n=\nslicer\n.\ntransform\n(\ndf\n)\noutput\n.\nselect\n(\n\"\nuserFeatures\n\"\n,\n\"\nfeatures\n\"\n).\nshow\n()\nFind full example code at \"examples/src/main/python/ml/vector_slicer_example.py\" in the Spark repo.\nRefer to the\nVectorSlicer Scala docs\nfor more details on the API.\nimport\njava.util.Arrays\nimport\norg.apache.spark.ml.attribute.\n{\nAttribute\n,\nAttributeGroup\n,\nNumericAttribute\n}\nimport\norg.apache.spark.ml.feature.VectorSlicer\nimport\norg.apache.spark.ml.linalg.Vectors\nimport\norg.apache.spark.sql.\n{\nRow\n,\nSparkSession\n}\nimport\norg.apache.spark.sql.types.StructType\nval\ndata\n=\nArrays\n.\nasList\n(\nRow\n(\nVectors\n.\nsparse\n(\n3\n,\nSeq\n((\n0\n,\n-\n2.0\n),\n(\n1\n,\n2.3\n)))),\nRow\n(\nVectors\n.\ndense\n(-\n2.0\n,\n2.3\n,\n0.0\n))\n)\nval\ndefaultAttr\n=\nNumericAttribute\n.\ndefaultAttr\nval\nattrs\n=\nArray\n(\n\"f1\"\n,\n\"f2\"\n,\n\"f3\"\n).\nmap\n(\ndefaultAttr\n.\nwithName\n)\nval\nattrGroup\n=\nnew\nAttributeGroup\n(\n\"userFeatures\"\n,\nattrs\n.\nasInstanceOf\n[\nArray\n[\nAttribute\n]])\nval\ndataset\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nStructType\n(\nArray\n(\nattrGroup\n.\ntoStructField\n())))\nval\nslicer\n=\nnew\nVectorSlicer\n().\nsetInputCol\n(\n\"userFeatures\"\n).\nsetOutputCol\n(\n\"features\"\n)\nslicer\n.\nsetIndices\n(\nArray\n(\n1\n)).\nsetNames\n(\nArray\n(\n\"f3\"\n))\n// or slicer.setIndices(Array(1, 2)), or slicer.setNames(Array(\"f2\", \"f3\"))\nval\noutput\n=\nslicer\n.\ntransform\n(\ndataset\n)\noutput\n.\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\" in the Spark repo.\nRefer to the\nVectorSlicer Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.attribute.Attribute\n;\nimport\norg.apache.spark.ml.attribute.AttributeGroup\n;\nimport\norg.apache.spark.ml.attribute.NumericAttribute\n;\nimport\norg.apache.spark.ml.feature.VectorSlicer\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.*\n;\nAttribute\n[]\nattrs\n=\n{\nNumericAttribute\n.\ndefaultAttr\n().\nwithName\n(\n\"f1\"\n),\nNumericAttribute\n.\ndefaultAttr\n().\nwithName\n(\n\"f2\"\n),\nNumericAttribute\n.\ndefaultAttr\n().\nwithName\n(\n\"f3\"\n)\n};\nAttributeGroup\ngroup\n=\nnew\nAttributeGroup\n(\n\"userFeatures\"\n,\nattrs\n);\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\nVectors\n.\nsparse\n(\n3\n,\nnew\nint\n[]{\n0\n,\n1\n},\nnew\ndouble\n[]{-\n2.0\n,\n2.3\n})),\nRowFactory\n.\ncreate\n(\nVectors\n.\ndense\n(-\n2.0\n,\n2.3\n,\n0.0\n))\n);\nDataset\n<\nRow\n>\ndataset\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\n(\nnew\nStructType\n()).\nadd\n(\ngroup\n.\ntoStructField\n()));\nVectorSlicer\nvectorSlicer\n=\nnew\nVectorSlicer\n()\n.\nsetInputCol\n(\n\"userFeatures\"\n).\nsetOutputCol\n(\n\"features\"\n);\nvectorSlicer\n.\nsetIndices\n(\nnew\nint\n[]{\n1\n}).\nsetNames\n(\nnew\nString\n[]{\n\"f3\"\n});\n// or slicer.setIndices(new int[]{1, 2}), or slicer.setNames(new String[]{\"f2\", \"f3\"})\nDataset\n<\nRow\n>\noutput\n=\nvectorSlicer\n.\ntransform\n(\ndataset\n);\noutput\n.\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\" in the Spark repo.\nRFormula\nRFormula\nselects columns specified by an\nR model formula\n. \nCurrently we support a limited subset of the R operators, including ‘~’, ‘.’, ‘:’, ‘+’, and ‘-‘.\nThe basic operators are:\n~\nseparate target and terms\n+\nconcat terms, “+ 0” means removing intercept\n-\nremove a term, “- 1” means removing intercept\n:\ninteraction (multiplication for numeric values, or binarized categorical values)\n.\nall columns except target\nSuppose\na\nand\nb\nare double columns, we use the following simple examples to illustrate the effect of\nRFormula\n:\ny ~ a + b\nmeans model\ny ~ w0 + w1 * a + w2 * b\nwhere\nw0\nis the intercept and\nw1, w2\nare coefficients.\ny ~ a + b + a:b - 1\nmeans model\ny ~ w1 * a + w2 * b + w3 * a * b\nwhere\nw1, w2, w3\nare coefficients.\nRFormula\nproduces a vector column of features and a double or string column of label. \nLike when formulas are used in R for linear regression, numeric columns will be cast to doubles.\nAs to string input columns, they will first be transformed with\nStringIndexer\nusing ordering determined by\nstringOrderType\n,\nand the last category after ordering is dropped, then the doubles will be one-hot encoded.\nSuppose a string feature column containing values\n{'b', 'a', 'b', 'a', 'c', 'b'}\n, we set\nstringOrderType\nto control the encoding:\nstringOrderType | Category mapped to 0 by StringIndexer |  Category dropped by RFormula\n----------------|---------------------------------------|---------------------------------\n'frequencyDesc' | most frequent category ('b')          | least frequent category ('c')\n'frequencyAsc'  | least frequent category ('c')         | most frequent category ('b')\n'alphabetDesc'  | last alphabetical category ('c')      | first alphabetical category ('a')\n'alphabetAsc'   | first alphabetical category ('a')     | last alphabetical category ('c')\nIf the label column is of type string, it will be first transformed to double with\nStringIndexer\nusing\nfrequencyDesc\nordering.\nIf the label column does not exist in the DataFrame, the output label column will be created from the specified response variable in the formula.\nNote:\nThe ordering option\nstringOrderType\nis NOT used for the label column. When the label column is indexed, it uses the default descending frequency ordering in\nStringIndexer\n.\nExamples\nAssume that we have a DataFrame with the columns\nid\n,\ncountry\n,\nhour\n, and\nclicked\n:\nid | country | hour | clicked\n---|---------|------|---------\n 7 | \"US\"    | 18   | 1.0\n 8 | \"CA\"    | 12   | 0.0\n 9 | \"NZ\"    | 15   | 0.0\nIf we use\nRFormula\nwith a formula string of\nclicked ~ country + hour\n, which indicates that we want to\npredict\nclicked\nbased on\ncountry\nand\nhour\n, after transformation we should get the following DataFrame:\nid | country | hour | clicked | features         | label\n---|---------|------|---------|------------------|-------\n 7 | \"US\"    | 18   | 1.0     | [0.0, 0.0, 18.0] | 1.0\n 8 | \"CA\"    | 12   | 0.0     | [0.0, 1.0, 12.0] | 0.0\n 9 | \"NZ\"    | 15   | 0.0     | [1.0, 0.0, 15.0] | 0.0\nRefer to the\nRFormula Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nRFormula\ndataset\n=\nspark\n.\ncreateDataFrame\n(\n[(\n7\n,\n\"\nUS\n\"\n,\n18\n,\n1.0\n),\n(\n8\n,\n\"\nCA\n\"\n,\n12\n,\n0.0\n),\n(\n9\n,\n\"\nNZ\n\"\n,\n15\n,\n0.0\n)],\n[\n\"\nid\n\"\n,\n\"\ncountry\n\"\n,\n\"\nhour\n\"\n,\n\"\nclicked\n\"\n])\nformula\n=\nRFormula\n(\nformula\n=\n\"\nclicked ~ country + hour\n\"\n,\nfeaturesCol\n=\n\"\nfeatures\n\"\n,\nlabelCol\n=\n\"\nlabel\n\"\n)\noutput\n=\nformula\n.\nfit\n(\ndataset\n).\ntransform\n(\ndataset\n)\noutput\n.\nselect\n(\n\"\nfeatures\n\"\n,\n\"\nlabel\n\"\n).\nshow\n()\nFind full example code at \"examples/src/main/python/ml/rformula_example.py\" in the Spark repo.\nRefer to the\nRFormula Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.RFormula\nval\ndataset\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n7\n,\n\"US\"\n,\n18\n,\n1.0\n),\n(\n8\n,\n\"CA\"\n,\n12\n,\n0.0\n),\n(\n9\n,\n\"NZ\"\n,\n15\n,\n0.0\n)\n)).\ntoDF\n(\n\"id\"\n,\n\"country\"\n,\n\"hour\"\n,\n\"clicked\"\n)\nval\nformula\n=\nnew\nRFormula\n()\n.\nsetFormula\n(\n\"clicked ~ country + hour\"\n)\n.\nsetFeaturesCol\n(\n\"features\"\n)\n.\nsetLabelCol\n(\n\"label\"\n)\nval\noutput\n=\nformula\n.\nfit\n(\ndataset\n).\ntransform\n(\ndataset\n)\noutput\n.\nselect\n(\n\"features\"\n,\n\"label\"\n).\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\" in the Spark repo.\nRefer to the\nRFormula Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.RFormula\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nimport\nstatic\norg\n.\napache\n.\nspark\n.\nsql\n.\ntypes\n.\nDataTypes\n.*;\nStructType\nschema\n=\ncreateStructType\n(\nnew\nStructField\n[]{\ncreateStructField\n(\n\"id\"\n,\nIntegerType\n,\nfalse\n),\ncreateStructField\n(\n\"country\"\n,\nStringType\n,\nfalse\n),\ncreateStructField\n(\n\"hour\"\n,\nIntegerType\n,\nfalse\n),\ncreateStructField\n(\n\"clicked\"\n,\nDoubleType\n,\nfalse\n)\n});\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n7\n,\n\"US\"\n,\n18\n,\n1.0\n),\nRowFactory\n.\ncreate\n(\n8\n,\n\"CA\"\n,\n12\n,\n0.0\n),\nRowFactory\n.\ncreate\n(\n9\n,\n\"NZ\"\n,\n15\n,\n0.0\n)\n);\nDataset\n<\nRow\n>\ndataset\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nRFormula\nformula\n=\nnew\nRFormula\n()\n.\nsetFormula\n(\n\"clicked ~ country + hour\"\n)\n.\nsetFeaturesCol\n(\n\"features\"\n)\n.\nsetLabelCol\n(\n\"label\"\n);\nDataset\n<\nRow\n>\noutput\n=\nformula\n.\nfit\n(\ndataset\n).\ntransform\n(\ndataset\n);\noutput\n.\nselect\n(\n\"features\"\n,\n\"label\"\n).\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\" in the Spark repo.\nChiSqSelector\nChiSqSelector\nstands for Chi-Squared feature selection. It operates on labeled data with\ncategorical features. ChiSqSelector uses the\nChi-Squared test of independence\nto decide which\nfeatures to choose. It supports five selection methods:\nnumTopFeatures\n,\npercentile\n,\nfpr\n,\nfdr\n,\nfwe\n:\nnumTopFeatures\nchooses a fixed number of top features according to a chi-squared test. This is akin to yielding the features with the most predictive power.\npercentile\nis similar to\nnumTopFeatures\nbut chooses a fraction of all features instead of a fixed number.\nfpr\nchooses all features whose p-values are below a threshold, thus controlling the false positive rate of selection.\nfdr\nuses the\nBenjamini-Hochberg procedure\nto choose all features whose false discovery rate is below a threshold.\nfwe\nchooses all features whose p-values are below a threshold. The threshold is scaled by 1/numFeatures, thus controlling the family-wise error rate of selection.\nBy default, the selection method is\nnumTopFeatures\n, with the default number of top features set to 50.\nThe user can choose a selection method using\nsetSelectorType\n.\nExamples\nAssume that we have a DataFrame with the columns\nid\n,\nfeatures\n, and\nclicked\n, which is used as\nour target to be predicted:\nid | features              | clicked\n---|-----------------------|---------\n 7 | [0.0, 0.0, 18.0, 1.0] | 1.0\n 8 | [0.0, 1.0, 12.0, 0.0] | 0.0\n 9 | [1.0, 0.0, 15.0, 0.1] | 0.0\nIf we use\nChiSqSelector\nwith\nnumTopFeatures = 1\n, then according to our label\nclicked\nthe\nlast column in our\nfeatures\nis chosen as the most useful feature:\nid | features              | clicked | selectedFeatures\n---|-----------------------|---------|------------------\n 7 | [0.0, 0.0, 18.0, 1.0] | 1.0     | [1.0]\n 8 | [0.0, 1.0, 12.0, 0.0] | 0.0     | [0.0]\n 9 | [1.0, 0.0, 15.0, 0.1] | 0.0     | [0.1]\nRefer to the\nChiSqSelector Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nChiSqSelector\nfrom\npyspark.ml.linalg\nimport\nVectors\ndf\n=\nspark\n.\ncreateDataFrame\n([\n(\n7\n,\nVectors\n.\ndense\n([\n0.0\n,\n0.0\n,\n18.0\n,\n1.0\n]),\n1.0\n,),\n(\n8\n,\nVectors\n.\ndense\n([\n0.0\n,\n1.0\n,\n12.0\n,\n0.0\n]),\n0.0\n,),\n(\n9\n,\nVectors\n.\ndense\n([\n1.0\n,\n0.0\n,\n15.0\n,\n0.1\n]),\n0.0\n,)],\n[\n\"\nid\n\"\n,\n\"\nfeatures\n\"\n,\n\"\nclicked\n\"\n])\nselector\n=\nChiSqSelector\n(\nnumTopFeatures\n=\n1\n,\nfeaturesCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nselectedFeatures\n\"\n,\nlabelCol\n=\n\"\nclicked\n\"\n)\nresult\n=\nselector\n.\nfit\n(\ndf\n).\ntransform\n(\ndf\n)\nprint\n(\n\"\nChiSqSelector output with top %d features selected\n\"\n%\nselector\n.\ngetNumTopFeatures\n())\nresult\n.\nshow\n()\nFind full example code at \"examples/src/main/python/ml/chisq_selector_example.py\" in the Spark repo.\nRefer to the\nChiSqSelector Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.ChiSqSelector\nimport\norg.apache.spark.ml.linalg.Vectors\nval\ndata\n=\nSeq\n(\n(\n7\n,\nVectors\n.\ndense\n(\n0.0\n,\n0.0\n,\n18.0\n,\n1.0\n),\n1.0\n),\n(\n8\n,\nVectors\n.\ndense\n(\n0.0\n,\n1.0\n,\n12.0\n,\n0.0\n),\n0.0\n),\n(\n9\n,\nVectors\n.\ndense\n(\n1.0\n,\n0.0\n,\n15.0\n,\n0.1\n),\n0.0\n)\n)\nval\ndf\n=\nspark\n.\ncreateDataset\n(\ndata\n).\ntoDF\n(\n\"id\"\n,\n\"features\"\n,\n\"clicked\"\n)\nval\nselector\n=\nnew\nChiSqSelector\n()\n.\nsetNumTopFeatures\n(\n1\n)\n.\nsetFeaturesCol\n(\n\"features\"\n)\n.\nsetLabelCol\n(\n\"clicked\"\n)\n.\nsetOutputCol\n(\n\"selectedFeatures\"\n)\nval\nresult\n=\nselector\n.\nfit\n(\ndf\n).\ntransform\n(\ndf\n)\nprintln\n(\ns\n\"ChiSqSelector output with top ${selector.getNumTopFeatures} features selected\"\n)\nresult\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\" in the Spark repo.\nRefer to the\nChiSqSelector Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.ChiSqSelector\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n7\n,\nVectors\n.\ndense\n(\n0.0\n,\n0.0\n,\n18.0\n,\n1.0\n),\n1.0\n),\nRowFactory\n.\ncreate\n(\n8\n,\nVectors\n.\ndense\n(\n0.0\n,\n1.0\n,\n12.0\n,\n0.0\n),\n0.0\n),\nRowFactory\n.\ncreate\n(\n9\n,\nVectors\n.\ndense\n(\n1.0\n,\n0.0\n,\n15.0\n,\n0.1\n),\n0.0\n)\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"id\"\n,\nDataTypes\n.\nIntegerType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"features\"\n,\nnew\nVectorUDT\n(),\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"clicked\"\n,\nDataTypes\n.\nDoubleType\n,\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nChiSqSelector\nselector\n=\nnew\nChiSqSelector\n()\n.\nsetNumTopFeatures\n(\n1\n)\n.\nsetFeaturesCol\n(\n\"features\"\n)\n.\nsetLabelCol\n(\n\"clicked\"\n)\n.\nsetOutputCol\n(\n\"selectedFeatures\"\n);\nDataset\n<\nRow\n>\nresult\n=\nselector\n.\nfit\n(\ndf\n).\ntransform\n(\ndf\n);\nSystem\n.\nout\n.\nprintln\n(\n\"ChiSqSelector output with top \"\n+\nselector\n.\ngetNumTopFeatures\n()\n+\n\" features selected\"\n);\nresult\n.\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\" in the Spark repo.\nUnivariateFeatureSelector\nUnivariateFeatureSelector\noperates on categorical/continuous labels with categorical/continuous features. \nUser can set\nfeatureType\nand\nlabelType\n, and Spark will pick the score function to use based on the specified\nfeatureType\nand\nlabelType\n.\nfeatureType |  labelType |score function\n------------|------------|--------------\ncategorical |categorical | chi-squared (chi2)\ncontinuous  |categorical | ANOVATest (f_classif)\ncontinuous  |continuous  | F-value (f_regression)\nIt supports five selection modes:\nnumTopFeatures\n,\npercentile\n,\nfpr\n,\nfdr\n,\nfwe\n:\nnumTopFeatures\nchooses a fixed number of top features.\npercentile\nis similar to\nnumTopFeatures\nbut chooses a fraction of all features instead of a fixed number.\nfpr\nchooses all features whose p-values are below a threshold, thus controlling the false positive rate of selection.\nfdr\nuses the\nBenjamini-Hochberg procedure\nto choose all features whose false discovery rate is below a threshold.\nfwe\nchooses all features whose p-values are below a threshold. The threshold is scaled by 1/numFeatures, thus controlling the family-wise error rate of selection.\nBy default, the selection mode is\nnumTopFeatures\n, with the default selectionThreshold sets to 50.\nExamples\nAssume that we have a DataFrame with the columns\nid\n,\nfeatures\n, and\nlabel\n, which is used as\nour target to be predicted:\nid | features                       | label\n---|--------------------------------|---------\n 1 | [1.7, 4.4, 7.6, 5.8, 9.6, 2.3] | 3.0\n 2 | [8.8, 7.3, 5.7, 7.3, 2.2, 4.1] | 2.0\n 3 | [1.2, 9.5, 2.5, 3.1, 8.7, 2.5] | 3.0\n 4 | [3.7, 9.2, 6.1, 4.1, 7.5, 3.8] | 2.0\n 5 | [8.9, 5.2, 7.8, 8.3, 5.2, 3.0] | 4.0\n 6 | [7.9, 8.5, 9.2, 4.0, 9.4, 2.1] | 4.0\nIf we set\nfeatureType\nto\ncontinuous\nand\nlabelType\nto\ncategorical\nwith\nnumTopFeatures = 1\n, the\nlast column in our\nfeatures\nis chosen as the most useful feature:\nid | features                       | label   | selectedFeatures\n---|--------------------------------|---------|------------------\n 1 | [1.7, 4.4, 7.6, 5.8, 9.6, 2.3] | 3.0     | [2.3]\n 2 | [8.8, 7.3, 5.7, 7.3, 2.2, 4.1] | 2.0     | [4.1]\n 3 | [1.2, 9.5, 2.5, 3.1, 8.7, 2.5] | 3.0     | [2.5]\n 4 | [3.7, 9.2, 6.1, 4.1, 7.5, 3.8] | 2.0     | [3.8]\n 5 | [8.9, 5.2, 7.8, 8.3, 5.2, 3.0] | 4.0     | [3.0]\n 6 | [7.9, 8.5, 9.2, 4.0, 9.4, 2.1] | 4.0     | [2.1]\nRefer to the\nUnivariateFeatureSelector Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nUnivariateFeatureSelector\nfrom\npyspark.ml.linalg\nimport\nVectors\ndf\n=\nspark\n.\ncreateDataFrame\n([\n(\n1\n,\nVectors\n.\ndense\n([\n1.7\n,\n4.4\n,\n7.6\n,\n5.8\n,\n9.6\n,\n2.3\n]),\n3.0\n,),\n(\n2\n,\nVectors\n.\ndense\n([\n8.8\n,\n7.3\n,\n5.7\n,\n7.3\n,\n2.2\n,\n4.1\n]),\n2.0\n,),\n(\n3\n,\nVectors\n.\ndense\n([\n1.2\n,\n9.5\n,\n2.5\n,\n3.1\n,\n8.7\n,\n2.5\n]),\n3.0\n,),\n(\n4\n,\nVectors\n.\ndense\n([\n3.7\n,\n9.2\n,\n6.1\n,\n4.1\n,\n7.5\n,\n3.8\n]),\n2.0\n,),\n(\n5\n,\nVectors\n.\ndense\n([\n8.9\n,\n5.2\n,\n7.8\n,\n8.3\n,\n5.2\n,\n3.0\n]),\n4.0\n,),\n(\n6\n,\nVectors\n.\ndense\n([\n7.9\n,\n8.5\n,\n9.2\n,\n4.0\n,\n9.4\n,\n2.1\n]),\n4.0\n,)],\n[\n\"\nid\n\"\n,\n\"\nfeatures\n\"\n,\n\"\nlabel\n\"\n])\nselector\n=\nUnivariateFeatureSelector\n(\nfeaturesCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nselectedFeatures\n\"\n,\nlabelCol\n=\n\"\nlabel\n\"\n,\nselectionMode\n=\n\"\nnumTopFeatures\n\"\n)\nselector\n.\nsetFeatureType\n(\n\"\ncontinuous\n\"\n).\nsetLabelType\n(\n\"\ncategorical\n\"\n).\nsetSelectionThreshold\n(\n1\n)\nresult\n=\nselector\n.\nfit\n(\ndf\n).\ntransform\n(\ndf\n)\nprint\n(\n\"\nUnivariateFeatureSelector output with top %d features selected using f_classif\n\"\n%\nselector\n.\ngetSelectionThreshold\n())\nresult\n.\nshow\n()\nFind full example code at \"examples/src/main/python/ml/univariate_feature_selector_example.py\" in the Spark repo.\nRefer to the\nUnivariateFeatureSelector Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.UnivariateFeatureSelector\nimport\norg.apache.spark.ml.linalg.Vectors\nval\ndata\n=\nSeq\n(\n(\n1\n,\nVectors\n.\ndense\n(\n1.7\n,\n4.4\n,\n7.6\n,\n5.8\n,\n9.6\n,\n2.3\n),\n3.0\n),\n(\n2\n,\nVectors\n.\ndense\n(\n8.8\n,\n7.3\n,\n5.7\n,\n7.3\n,\n2.2\n,\n4.1\n),\n2.0\n),\n(\n3\n,\nVectors\n.\ndense\n(\n1.2\n,\n9.5\n,\n2.5\n,\n3.1\n,\n8.7\n,\n2.5\n),\n3.0\n),\n(\n4\n,\nVectors\n.\ndense\n(\n3.7\n,\n9.2\n,\n6.1\n,\n4.1\n,\n7.5\n,\n3.8\n),\n2.0\n),\n(\n5\n,\nVectors\n.\ndense\n(\n8.9\n,\n5.2\n,\n7.8\n,\n8.3\n,\n5.2\n,\n3.0\n),\n4.0\n),\n(\n6\n,\nVectors\n.\ndense\n(\n7.9\n,\n8.5\n,\n9.2\n,\n4.0\n,\n9.4\n,\n2.1\n),\n4.0\n)\n)\nval\ndf\n=\nspark\n.\ncreateDataset\n(\ndata\n).\ntoDF\n(\n\"id\"\n,\n\"features\"\n,\n\"label\"\n)\nval\nselector\n=\nnew\nUnivariateFeatureSelector\n()\n.\nsetFeatureType\n(\n\"continuous\"\n)\n.\nsetLabelType\n(\n\"categorical\"\n)\n.\nsetSelectionMode\n(\n\"numTopFeatures\"\n)\n.\nsetSelectionThreshold\n(\n1\n)\n.\nsetFeaturesCol\n(\n\"features\"\n)\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetOutputCol\n(\n\"selectedFeatures\"\n)\nval\nresult\n=\nselector\n.\nfit\n(\ndf\n).\ntransform\n(\ndf\n)\nprintln\n(\ns\n\"UnivariateFeatureSelector output with top ${selector.getSelectionThreshold}\"\n+\ns\n\" features selected using f_classif\"\n)\nresult\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/UnivariateFeatureSelectorExample.scala\" in the Spark repo.\nRefer to the\nUnivariateFeatureSelector Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.UnivariateFeatureSelector\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.*\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n1\n,\nVectors\n.\ndense\n(\n1.7\n,\n4.4\n,\n7.6\n,\n5.8\n,\n9.6\n,\n2.3\n),\n3.0\n),\nRowFactory\n.\ncreate\n(\n2\n,\nVectors\n.\ndense\n(\n8.8\n,\n7.3\n,\n5.7\n,\n7.3\n,\n2.2\n,\n4.1\n),\n2.0\n),\nRowFactory\n.\ncreate\n(\n3\n,\nVectors\n.\ndense\n(\n1.2\n,\n9.5\n,\n2.5\n,\n3.1\n,\n8.7\n,\n2.5\n),\n3.0\n),\nRowFactory\n.\ncreate\n(\n4\n,\nVectors\n.\ndense\n(\n3.7\n,\n9.2\n,\n6.1\n,\n4.1\n,\n7.5\n,\n3.8\n),\n2.0\n),\nRowFactory\n.\ncreate\n(\n5\n,\nVectors\n.\ndense\n(\n8.9\n,\n5.2\n,\n7.8\n,\n8.3\n,\n5.2\n,\n3.0\n),\n4.0\n),\nRowFactory\n.\ncreate\n(\n6\n,\nVectors\n.\ndense\n(\n7.9\n,\n8.5\n,\n9.2\n,\n4.0\n,\n9.4\n,\n2.1\n),\n4.0\n)\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"id\"\n,\nDataTypes\n.\nIntegerType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"features\"\n,\nnew\nVectorUDT\n(),\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"label\"\n,\nDataTypes\n.\nDoubleType\n,\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nUnivariateFeatureSelector\nselector\n=\nnew\nUnivariateFeatureSelector\n()\n.\nsetFeatureType\n(\n\"continuous\"\n)\n.\nsetLabelType\n(\n\"categorical\"\n)\n.\nsetSelectionMode\n(\n\"numTopFeatures\"\n)\n.\nsetSelectionThreshold\n(\n1\n)\n.\nsetFeaturesCol\n(\n\"features\"\n)\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetOutputCol\n(\n\"selectedFeatures\"\n);\nDataset\n<\nRow\n>\nresult\n=\nselector\n.\nfit\n(\ndf\n).\ntransform\n(\ndf\n);\nSystem\n.\nout\n.\nprintln\n(\n\"UnivariateFeatureSelector output with top \"\n+\nselector\n.\ngetSelectionThreshold\n()\n+\n\" features selected using f_classif\"\n);\nresult\n.\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaUnivariateFeatureSelectorExample.java\" in the Spark repo.\nVarianceThresholdSelector\nVarianceThresholdSelector\nis a selector that removes low-variance features. Features with a\n (sample) variance not greater than the\nvarianceThreshold\nwill be removed. If not set,\nvarianceThreshold\ndefaults to 0, which means only features with variance 0 (i.e. features that have the same value in all samples)\n will be removed.\nExamples\nAssume that we have a DataFrame with the columns\nid\nand\nfeatures\n, which is used as\nour target to be predicted:\nid | features\n---|--------------------------------\n 1 | [6.0, 7.0, 0.0, 7.0, 6.0, 0.0]\n 2 | [0.0, 9.0, 6.0, 0.0, 5.0, 9.0]\n 3 | [0.0, 9.0, 3.0, 0.0, 5.0, 5.0]\n 4 | [0.0, 9.0, 8.0, 5.0, 6.0, 4.0]\n 5 | [8.0, 9.0, 6.0, 5.0, 4.0, 4.0]\n 6 | [8.0, 9.0, 6.0, 0.0, 0.0, 0.0]\nThe sample variances for the 6 features are 16.67, 0.67, 8.17, 10.17,\n5.07, and 11.47 respectively. If we use\nVarianceThresholdSelector\nwith\nvarianceThreshold = 8.0\n, then the features with variance <= 8.0 are removed:\nid | features                       | selectedFeatures\n---|--------------------------------|-------------------\n 1 | [6.0, 7.0, 0.0, 7.0, 6.0, 0.0] | [6.0,0.0,7.0,0.0]\n 2 | [0.0, 9.0, 6.0, 0.0, 5.0, 9.0] | [0.0,6.0,0.0,9.0]\n 3 | [0.0, 9.0, 3.0, 0.0, 5.0, 5.0] | [0.0,3.0,0.0,5.0]\n 4 | [0.0, 9.0, 8.0, 5.0, 6.0, 4.0] | [0.0,8.0,5.0,4.0]\n 5 | [8.0, 9.0, 6.0, 5.0, 4.0, 4.0] | [8.0,6.0,5.0,4.0]\n 6 | [8.0, 9.0, 6.0, 0.0, 0.0, 0.0] | [8.0,6.0,0.0,0.0]\nRefer to the\nVarianceThresholdSelector Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nVarianceThresholdSelector\nfrom\npyspark.ml.linalg\nimport\nVectors\ndf\n=\nspark\n.\ncreateDataFrame\n([\n(\n1\n,\nVectors\n.\ndense\n([\n6.0\n,\n7.0\n,\n0.0\n,\n7.0\n,\n6.0\n,\n0.0\n])),\n(\n2\n,\nVectors\n.\ndense\n([\n0.0\n,\n9.0\n,\n6.0\n,\n0.0\n,\n5.0\n,\n9.0\n])),\n(\n3\n,\nVectors\n.\ndense\n([\n0.0\n,\n9.0\n,\n3.0\n,\n0.0\n,\n5.0\n,\n5.0\n])),\n(\n4\n,\nVectors\n.\ndense\n([\n0.0\n,\n9.0\n,\n8.0\n,\n5.0\n,\n6.0\n,\n4.0\n])),\n(\n5\n,\nVectors\n.\ndense\n([\n8.0\n,\n9.0\n,\n6.0\n,\n5.0\n,\n4.0\n,\n4.0\n])),\n(\n6\n,\nVectors\n.\ndense\n([\n8.0\n,\n9.0\n,\n6.0\n,\n0.0\n,\n0.0\n,\n0.0\n]))],\n[\n\"\nid\n\"\n,\n\"\nfeatures\n\"\n])\nselector\n=\nVarianceThresholdSelector\n(\nvarianceThreshold\n=\n8.0\n,\noutputCol\n=\n\"\nselectedFeatures\n\"\n)\nresult\n=\nselector\n.\nfit\n(\ndf\n).\ntransform\n(\ndf\n)\nprint\n(\n\"\nOutput: Features with variance lower than %f are removed.\n\"\n%\nselector\n.\ngetVarianceThreshold\n())\nresult\n.\nshow\n()\nFind full example code at \"examples/src/main/python/ml/variance_threshold_selector_example.py\" in the Spark repo.\nRefer to the\nVarianceThresholdSelector Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.VarianceThresholdSelector\nimport\norg.apache.spark.ml.linalg.Vectors\nval\ndata\n=\nSeq\n(\n(\n1\n,\nVectors\n.\ndense\n(\n6.0\n,\n7.0\n,\n0.0\n,\n7.0\n,\n6.0\n,\n0.0\n)),\n(\n2\n,\nVectors\n.\ndense\n(\n0.0\n,\n9.0\n,\n6.0\n,\n0.0\n,\n5.0\n,\n9.0\n)),\n(\n3\n,\nVectors\n.\ndense\n(\n0.0\n,\n9.0\n,\n3.0\n,\n0.0\n,\n5.0\n,\n5.0\n)),\n(\n4\n,\nVectors\n.\ndense\n(\n0.0\n,\n9.0\n,\n8.0\n,\n5.0\n,\n6.0\n,\n4.0\n)),\n(\n5\n,\nVectors\n.\ndense\n(\n8.0\n,\n9.0\n,\n6.0\n,\n5.0\n,\n4.0\n,\n4.0\n)),\n(\n6\n,\nVectors\n.\ndense\n(\n8.0\n,\n9.0\n,\n6.0\n,\n0.0\n,\n0.0\n,\n0.0\n))\n)\nval\ndf\n=\nspark\n.\ncreateDataset\n(\ndata\n).\ntoDF\n(\n\"id\"\n,\n\"features\"\n)\nval\nselector\n=\nnew\nVarianceThresholdSelector\n()\n.\nsetVarianceThreshold\n(\n8.0\n)\n.\nsetFeaturesCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"selectedFeatures\"\n)\nval\nresult\n=\nselector\n.\nfit\n(\ndf\n).\ntransform\n(\ndf\n)\nprintln\n(\ns\n\"Output: Features with variance lower than\"\n+\ns\n\" ${selector.getVarianceThreshold} are removed.\"\n)\nresult\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/VarianceThresholdSelectorExample.scala\" in the Spark repo.\nRefer to the\nVarianceThresholdSelector Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.VarianceThresholdSelector\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.*\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n1\n,\nVectors\n.\ndense\n(\n6.0\n,\n7.0\n,\n0.0\n,\n7.0\n,\n6.0\n,\n0.0\n)),\nRowFactory\n.\ncreate\n(\n2\n,\nVectors\n.\ndense\n(\n0.0\n,\n9.0\n,\n6.0\n,\n0.0\n,\n5.0\n,\n9.0\n)),\nRowFactory\n.\ncreate\n(\n3\n,\nVectors\n.\ndense\n(\n0.0\n,\n9.0\n,\n3.0\n,\n0.0\n,\n5.0\n,\n5.0\n)),\nRowFactory\n.\ncreate\n(\n4\n,\nVectors\n.\ndense\n(\n0.0\n,\n9.0\n,\n8.0\n,\n5.0\n,\n6.0\n,\n4.0\n)),\nRowFactory\n.\ncreate\n(\n5\n,\nVectors\n.\ndense\n(\n8.0\n,\n9.0\n,\n6.0\n,\n5.0\n,\n4.0\n,\n4.0\n)),\nRowFactory\n.\ncreate\n(\n6\n,\nVectors\n.\ndense\n(\n8.0\n,\n9.0\n,\n6.0\n,\n0.0\n,\n0.0\n,\n0.0\n))\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"id\"\n,\nDataTypes\n.\nIntegerType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"features\"\n,\nnew\nVectorUDT\n(),\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nVarianceThresholdSelector\nselector\n=\nnew\nVarianceThresholdSelector\n()\n.\nsetVarianceThreshold\n(\n8.0\n)\n.\nsetFeaturesCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"selectedFeatures\"\n);\nDataset\n<\nRow\n>\nresult\n=\nselector\n.\nfit\n(\ndf\n).\ntransform\n(\ndf\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Output: Features with variance lower than \"\n+\nselector\n.\ngetVarianceThreshold\n()\n+\n\" are removed.\"\n);\nresult\n.\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaVarianceThresholdSelectorExample.java\" in the Spark repo.\nLocality Sensitive Hashing\nLocality Sensitive Hashing (LSH)\nis an important class of hashing techniques, which is commonly used in clustering, approximate nearest neighbor search and outlier detection with large datasets.\nThe general idea of LSH is to use a family of functions (“LSH families”) to hash data points into buckets, so that the data points which are close to each other are in the same buckets with high probability, while data points that are far away from each other are very likely in different buckets. An LSH family is formally defined as follows.\nIn a metric space\n(M, d)\n, where\nM\nis a set and\nd\nis a distance function on\nM\n, an LSH family is a family of functions\nh\nthat satisfy the following properties:\n\\[\n\\forall p, q \\in M,\\\\\nd(p,q) \\leq r1 \\Rightarrow Pr(h(p)=h(q)) \\geq p1\\\\\nd(p,q) \\geq r2 \\Rightarrow Pr(h(p)=h(q)) \\leq p2\n\\]\nThis LSH family is called\n(r1, r2, p1, p2)\n-sensitive.\nIn Spark, different LSH families are implemented in separate classes (e.g.,\nMinHash\n), and APIs for feature transformation, approximate similarity join and approximate nearest neighbor are provided in each class.\nIn LSH, we define a false positive as a pair of distant input features (with\n$d(p,q) \\geq r2$\n) which are hashed into the same bucket, and we define a false negative as a pair of nearby features (with\n$d(p,q) \\leq r1$\n) which are hashed into different buckets.\nLSH Operations\nWe describe the major types of operations which LSH can be used for.  A fitted LSH model has methods for each of these operations.\nFeature Transformation\nFeature transformation is the basic functionality to add hashed values as a new column. This can be useful for dimensionality reduction. Users can specify input and output column names by setting\ninputCol\nand\noutputCol\n.\nLSH also supports multiple LSH hash tables. Users can specify the number of hash tables by setting\nnumHashTables\n. This is also used for\nOR-amplification\nin approximate similarity join and approximate nearest neighbor. Increasing the number of hash tables will increase the accuracy but will also increase communication cost and running time.\nThe type of\noutputCol\nis\nSeq[Vector]\nwhere the dimension of the array equals\nnumHashTables\n, and the dimensions of the vectors are currently set to 1. In future releases, we will implement AND-amplification so that users can specify the dimensions of these vectors.\nApproximate Similarity Join\nApproximate similarity join takes two datasets and approximately returns pairs of rows in the datasets whose distance is smaller than a user-defined threshold. Approximate similarity join supports both joining two different datasets and self-joining. Self-joining will produce some duplicate pairs.\nApproximate similarity join accepts both transformed and untransformed datasets as input. If an untransformed dataset is used, it will be transformed automatically. In this case, the hash signature will be created as\noutputCol\n.\nIn the joined dataset, the origin datasets can be queried in\ndatasetA\nand\ndatasetB\n. A distance column will be added to the output dataset to show the true distance between each pair of rows returned.\nApproximate Nearest Neighbor Search\nApproximate nearest neighbor search takes a dataset (of feature vectors) and a key (a single feature vector), and it approximately returns a specified number of rows in the dataset that are closest to the vector.\nApproximate nearest neighbor search accepts both transformed and untransformed datasets as input. If an untransformed dataset is used, it will be transformed automatically. In this case, the hash signature will be created as\noutputCol\n.\nA distance column will be added to the output dataset to show the true distance between each output row and the searched key.\nNote:\nApproximate nearest neighbor search will return fewer than\nk\nrows when there are not enough candidates in the hash bucket.\nLSH Algorithms\nBucketed Random Projection for Euclidean Distance\nBucketed Random Projection\nis an LSH family for Euclidean distance. The Euclidean distance is defined as follows:\n\\[\nd(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum_i (x_i - y_i)^2}\n\\]\nIts LSH family projects feature vectors\n$\\mathbf{x}$\nonto a random unit vector\n$\\mathbf{v}$\nand portions the projected results into hash buckets:\n\\[\nh(\\mathbf{x}) = \\Big\\lfloor \\frac{\\mathbf{x} \\cdot \\mathbf{v}}{r} \\Big\\rfloor\n\\]\nwhere\nr\nis a user-defined bucket length. The bucket length can be used to control the average size of hash buckets (and thus the number of buckets). A larger bucket length (i.e., fewer buckets) increases the probability of features being hashed to the same bucket (increasing the numbers of true and false positives).\nBucketed Random Projection accepts arbitrary vectors as input features, and supports both sparse and dense vectors.\nRefer to the\nBucketedRandomProjectionLSH Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nBucketedRandomProjectionLSH\nfrom\npyspark.ml.linalg\nimport\nVectors\nfrom\npyspark.sql.functions\nimport\ncol\ndataA\n=\n[(\n0\n,\nVectors\n.\ndense\n([\n1.0\n,\n1.0\n]),),\n(\n1\n,\nVectors\n.\ndense\n([\n1.0\n,\n-\n1.0\n]),),\n(\n2\n,\nVectors\n.\ndense\n([\n-\n1.0\n,\n-\n1.0\n]),),\n(\n3\n,\nVectors\n.\ndense\n([\n-\n1.0\n,\n1.0\n]),)]\ndfA\n=\nspark\n.\ncreateDataFrame\n(\ndataA\n,\n[\n\"\nid\n\"\n,\n\"\nfeatures\n\"\n])\ndataB\n=\n[(\n4\n,\nVectors\n.\ndense\n([\n1.0\n,\n0.0\n]),),\n(\n5\n,\nVectors\n.\ndense\n([\n-\n1.0\n,\n0.0\n]),),\n(\n6\n,\nVectors\n.\ndense\n([\n0.0\n,\n1.0\n]),),\n(\n7\n,\nVectors\n.\ndense\n([\n0.0\n,\n-\n1.0\n]),)]\ndfB\n=\nspark\n.\ncreateDataFrame\n(\ndataB\n,\n[\n\"\nid\n\"\n,\n\"\nfeatures\n\"\n])\nkey\n=\nVectors\n.\ndense\n([\n1.0\n,\n0.0\n])\nbrp\n=\nBucketedRandomProjectionLSH\n(\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nhashes\n\"\n,\nbucketLength\n=\n2.0\n,\nnumHashTables\n=\n3\n)\nmodel\n=\nbrp\n.\nfit\n(\ndfA\n)\n# Feature Transformation\nprint\n(\n\"\nThe hashed dataset where hashed values are stored in the column\n'\nhashes\n'\n:\n\"\n)\nmodel\n.\ntransform\n(\ndfA\n).\nshow\n()\n# Compute the locality sensitive hashes for the input rows, then perform approximate\n# similarity join.\n# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n# `model.approxSimilarityJoin(transformedA, transformedB, 1.5)`\nprint\n(\n\"\nApproximately joining dfA and dfB on Euclidean distance smaller than 1.5:\n\"\n)\nmodel\n.\napproxSimilarityJoin\n(\ndfA\n,\ndfB\n,\n1.5\n,\ndistCol\n=\n\"\nEuclideanDistance\n\"\n)\n\\\n.\nselect\n(\ncol\n(\n\"\ndatasetA.id\n\"\n).\nalias\n(\n\"\nidA\n\"\n),\ncol\n(\n\"\ndatasetB.id\n\"\n).\nalias\n(\n\"\nidB\n\"\n),\ncol\n(\n\"\nEuclideanDistance\n\"\n)).\nshow\n()\n# Compute the locality sensitive hashes for the input rows, then perform approximate nearest\n# neighbor search.\n# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n# `model.approxNearestNeighbors(transformedA, key, 2)`\nprint\n(\n\"\nApproximately searching dfA for 2 nearest neighbors of the key:\n\"\n)\nmodel\n.\napproxNearestNeighbors\n(\ndfA\n,\nkey\n,\n2\n).\nshow\n()\nFind full example code at \"examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\" in the Spark repo.\nRefer to the\nBucketedRandomProjectionLSH Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.BucketedRandomProjectionLSH\nimport\norg.apache.spark.ml.linalg.Vectors\nimport\norg.apache.spark.sql.SparkSession\nimport\norg.apache.spark.sql.functions.col\nval\ndfA\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0\n,\nVectors\n.\ndense\n(\n1.0\n,\n1.0\n)),\n(\n1\n,\nVectors\n.\ndense\n(\n1.0\n,\n-\n1.0\n)),\n(\n2\n,\nVectors\n.\ndense\n(-\n1.0\n,\n-\n1.0\n)),\n(\n3\n,\nVectors\n.\ndense\n(-\n1.0\n,\n1.0\n))\n)).\ntoDF\n(\n\"id\"\n,\n\"features\"\n)\nval\ndfB\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n4\n,\nVectors\n.\ndense\n(\n1.0\n,\n0.0\n)),\n(\n5\n,\nVectors\n.\ndense\n(-\n1.0\n,\n0.0\n)),\n(\n6\n,\nVectors\n.\ndense\n(\n0.0\n,\n1.0\n)),\n(\n7\n,\nVectors\n.\ndense\n(\n0.0\n,\n-\n1.0\n))\n)).\ntoDF\n(\n\"id\"\n,\n\"features\"\n)\nval\nkey\n=\nVectors\n.\ndense\n(\n1.0\n,\n0.0\n)\nval\nbrp\n=\nnew\nBucketedRandomProjectionLSH\n()\n.\nsetBucketLength\n(\n2.0\n)\n.\nsetNumHashTables\n(\n3\n)\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"hashes\"\n)\nval\nmodel\n=\nbrp\n.\nfit\n(\ndfA\n)\n// Feature Transformation\nprintln\n(\n\"The hashed dataset where hashed values are stored in the column 'hashes':\"\n)\nmodel\n.\ntransform\n(\ndfA\n).\nshow\n()\n// Compute the locality sensitive hashes for the input rows, then perform approximate\n// similarity join.\n// We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n// `model.approxSimilarityJoin(transformedA, transformedB, 1.5)`\nprintln\n(\n\"Approximately joining dfA and dfB on Euclidean distance smaller than 1.5:\"\n)\nmodel\n.\napproxSimilarityJoin\n(\ndfA\n,\ndfB\n,\n1.5\n,\n\"EuclideanDistance\"\n)\n.\nselect\n(\ncol\n(\n\"datasetA.id\"\n).\nalias\n(\n\"idA\"\n),\ncol\n(\n\"datasetB.id\"\n).\nalias\n(\n\"idB\"\n),\ncol\n(\n\"EuclideanDistance\"\n)).\nshow\n()\n// Compute the locality sensitive hashes for the input rows, then perform approximate nearest\n// neighbor search.\n// We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n// `model.approxNearestNeighbors(transformedA, key, 2)`\nprintln\n(\n\"Approximately searching dfA for 2 nearest neighbors of the key:\"\n)\nmodel\n.\napproxNearestNeighbors\n(\ndfA\n,\nkey\n,\n2\n).\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\" in the Spark repo.\nRefer to the\nBucketedRandomProjectionLSH Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.BucketedRandomProjectionLSH\n;\nimport\norg.apache.spark.ml.feature.BucketedRandomProjectionLSHModel\n;\nimport\norg.apache.spark.ml.linalg.Vector\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nimport\nstatic\norg\n.\napache\n.\nspark\n.\nsql\n.\nfunctions\n.\ncol\n;\nList\n<\nRow\n>\ndataA\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n0\n,\nVectors\n.\ndense\n(\n1.0\n,\n1.0\n)),\nRowFactory\n.\ncreate\n(\n1\n,\nVectors\n.\ndense\n(\n1.0\n,\n-\n1.0\n)),\nRowFactory\n.\ncreate\n(\n2\n,\nVectors\n.\ndense\n(-\n1.0\n,\n-\n1.0\n)),\nRowFactory\n.\ncreate\n(\n3\n,\nVectors\n.\ndense\n(-\n1.0\n,\n1.0\n))\n);\nList\n<\nRow\n>\ndataB\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n4\n,\nVectors\n.\ndense\n(\n1.0\n,\n0.0\n)),\nRowFactory\n.\ncreate\n(\n5\n,\nVectors\n.\ndense\n(-\n1.0\n,\n0.0\n)),\nRowFactory\n.\ncreate\n(\n6\n,\nVectors\n.\ndense\n(\n0.0\n,\n1.0\n)),\nRowFactory\n.\ncreate\n(\n7\n,\nVectors\n.\ndense\n(\n0.0\n,\n-\n1.0\n))\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"id\"\n,\nDataTypes\n.\nIntegerType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"features\"\n,\nnew\nVectorUDT\n(),\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndfA\n=\nspark\n.\ncreateDataFrame\n(\ndataA\n,\nschema\n);\nDataset\n<\nRow\n>\ndfB\n=\nspark\n.\ncreateDataFrame\n(\ndataB\n,\nschema\n);\nVector\nkey\n=\nVectors\n.\ndense\n(\n1.0\n,\n0.0\n);\nBucketedRandomProjectionLSH\nmh\n=\nnew\nBucketedRandomProjectionLSH\n()\n.\nsetBucketLength\n(\n2.0\n)\n.\nsetNumHashTables\n(\n3\n)\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"hashes\"\n);\nBucketedRandomProjectionLSHModel\nmodel\n=\nmh\n.\nfit\n(\ndfA\n);\n// Feature Transformation\nSystem\n.\nout\n.\nprintln\n(\n\"The hashed dataset where hashed values are stored in the column 'hashes':\"\n);\nmodel\n.\ntransform\n(\ndfA\n).\nshow\n();\n// Compute the locality sensitive hashes for the input rows, then perform approximate\n// similarity join.\n// We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n// `model.approxSimilarityJoin(transformedA, transformedB, 1.5)`\nSystem\n.\nout\n.\nprintln\n(\n\"Approximately joining dfA and dfB on distance smaller than 1.5:\"\n);\nmodel\n.\napproxSimilarityJoin\n(\ndfA\n,\ndfB\n,\n1.5\n,\n\"EuclideanDistance\"\n)\n.\nselect\n(\ncol\n(\n\"datasetA.id\"\n).\nalias\n(\n\"idA\"\n),\ncol\n(\n\"datasetB.id\"\n).\nalias\n(\n\"idB\"\n),\ncol\n(\n\"EuclideanDistance\"\n)).\nshow\n();\n// Compute the locality sensitive hashes for the input rows, then perform approximate nearest\n// neighbor search.\n// We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n// `model.approxNearestNeighbors(transformedA, key, 2)`\nSystem\n.\nout\n.\nprintln\n(\n\"Approximately searching dfA for 2 nearest neighbors of the key:\"\n);\nmodel\n.\napproxNearestNeighbors\n(\ndfA\n,\nkey\n,\n2\n).\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\" in the Spark repo.\nMinHash for Jaccard Distance\nMinHash\nis an LSH family for Jaccard distance where input features are sets of natural numbers. Jaccard distance of two sets is defined by the cardinality of their intersection and union:\n\\[\nd(\\mathbf{A}, \\mathbf{B}) = 1 - \\frac{|\\mathbf{A} \\cap \\mathbf{B}|}{|\\mathbf{A} \\cup \\mathbf{B}|}\n\\]\nMinHash applies a random hash function\ng\nto each element in the set and take the minimum of all hashed values:\n\\[\nh(\\mathbf{A}) = \\min_{a \\in \\mathbf{A}}(g(a))\n\\]\nThe input sets for MinHash are represented as binary vectors, where the vector indices represent the elements themselves and the non-zero values in the vector represent the presence of that element in the set. While both dense and sparse vectors are supported, typically sparse vectors are recommended for efficiency. For example,\nVectors.sparse(10, Array[(2, 1.0), (3, 1.0), (5, 1.0)])\nmeans there are 10 elements in the space. This set contains elem 2, elem 3 and elem 5. All non-zero values are treated as binary “1” values.\nNote:\nEmpty sets cannot be transformed by MinHash, which means any input vector must have at least 1 non-zero entry.\nRefer to the\nMinHashLSH Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nMinHashLSH\nfrom\npyspark.ml.linalg\nimport\nVectors\nfrom\npyspark.sql.functions\nimport\ncol\ndataA\n=\n[(\n0\n,\nVectors\n.\nsparse\n(\n6\n,\n[\n0\n,\n1\n,\n2\n],\n[\n1.0\n,\n1.0\n,\n1.0\n]),),\n(\n1\n,\nVectors\n.\nsparse\n(\n6\n,\n[\n2\n,\n3\n,\n4\n],\n[\n1.0\n,\n1.0\n,\n1.0\n]),),\n(\n2\n,\nVectors\n.\nsparse\n(\n6\n,\n[\n0\n,\n2\n,\n4\n],\n[\n1.0\n,\n1.0\n,\n1.0\n]),)]\ndfA\n=\nspark\n.\ncreateDataFrame\n(\ndataA\n,\n[\n\"\nid\n\"\n,\n\"\nfeatures\n\"\n])\ndataB\n=\n[(\n3\n,\nVectors\n.\nsparse\n(\n6\n,\n[\n1\n,\n3\n,\n5\n],\n[\n1.0\n,\n1.0\n,\n1.0\n]),),\n(\n4\n,\nVectors\n.\nsparse\n(\n6\n,\n[\n2\n,\n3\n,\n5\n],\n[\n1.0\n,\n1.0\n,\n1.0\n]),),\n(\n5\n,\nVectors\n.\nsparse\n(\n6\n,\n[\n1\n,\n2\n,\n4\n],\n[\n1.0\n,\n1.0\n,\n1.0\n]),)]\ndfB\n=\nspark\n.\ncreateDataFrame\n(\ndataB\n,\n[\n\"\nid\n\"\n,\n\"\nfeatures\n\"\n])\nkey\n=\nVectors\n.\nsparse\n(\n6\n,\n[\n1\n,\n3\n],\n[\n1.0\n,\n1.0\n])\nmh\n=\nMinHashLSH\n(\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nhashes\n\"\n,\nnumHashTables\n=\n5\n)\nmodel\n=\nmh\n.\nfit\n(\ndfA\n)\n# Feature Transformation\nprint\n(\n\"\nThe hashed dataset where hashed values are stored in the column\n'\nhashes\n'\n:\n\"\n)\nmodel\n.\ntransform\n(\ndfA\n).\nshow\n()\n# Compute the locality sensitive hashes for the input rows, then perform approximate\n# similarity join.\n# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n# `model.approxSimilarityJoin(transformedA, transformedB, 0.6)`\nprint\n(\n\"\nApproximately joining dfA and dfB on distance smaller than 0.6:\n\"\n)\nmodel\n.\napproxSimilarityJoin\n(\ndfA\n,\ndfB\n,\n0.6\n,\ndistCol\n=\n\"\nJaccardDistance\n\"\n)\n\\\n.\nselect\n(\ncol\n(\n\"\ndatasetA.id\n\"\n).\nalias\n(\n\"\nidA\n\"\n),\ncol\n(\n\"\ndatasetB.id\n\"\n).\nalias\n(\n\"\nidB\n\"\n),\ncol\n(\n\"\nJaccardDistance\n\"\n)).\nshow\n()\n# Compute the locality sensitive hashes for the input rows, then perform approximate nearest\n# neighbor search.\n# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n# `model.approxNearestNeighbors(transformedA, key, 2)`\n# It may return less than 2 rows when not enough approximate near-neighbor candidates are\n# found.\nprint\n(\n\"\nApproximately searching dfA for 2 nearest neighbors of the key:\n\"\n)\nmodel\n.\napproxNearestNeighbors\n(\ndfA\n,\nkey\n,\n2\n).\nshow\n()\nFind full example code at \"examples/src/main/python/ml/min_hash_lsh_example.py\" in the Spark repo.\nRefer to the\nMinHashLSH Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.MinHashLSH\nimport\norg.apache.spark.ml.linalg.Vectors\nimport\norg.apache.spark.sql.SparkSession\nimport\norg.apache.spark.sql.functions.col\nval\ndfA\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0\n,\nVectors\n.\nsparse\n(\n6\n,\nSeq\n((\n0\n,\n1.0\n),\n(\n1\n,\n1.0\n),\n(\n2\n,\n1.0\n)))),\n(\n1\n,\nVectors\n.\nsparse\n(\n6\n,\nSeq\n((\n2\n,\n1.0\n),\n(\n3\n,\n1.0\n),\n(\n4\n,\n1.0\n)))),\n(\n2\n,\nVectors\n.\nsparse\n(\n6\n,\nSeq\n((\n0\n,\n1.0\n),\n(\n2\n,\n1.0\n),\n(\n4\n,\n1.0\n))))\n)).\ntoDF\n(\n\"id\"\n,\n\"features\"\n)\nval\ndfB\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n3\n,\nVectors\n.\nsparse\n(\n6\n,\nSeq\n((\n1\n,\n1.0\n),\n(\n3\n,\n1.0\n),\n(\n5\n,\n1.0\n)))),\n(\n4\n,\nVectors\n.\nsparse\n(\n6\n,\nSeq\n((\n2\n,\n1.0\n),\n(\n3\n,\n1.0\n),\n(\n5\n,\n1.0\n)))),\n(\n5\n,\nVectors\n.\nsparse\n(\n6\n,\nSeq\n((\n1\n,\n1.0\n),\n(\n2\n,\n1.0\n),\n(\n4\n,\n1.0\n))))\n)).\ntoDF\n(\n\"id\"\n,\n\"features\"\n)\nval\nkey\n=\nVectors\n.\nsparse\n(\n6\n,\nSeq\n((\n1\n,\n1.0\n),\n(\n3\n,\n1.0\n)))\nval\nmh\n=\nnew\nMinHashLSH\n()\n.\nsetNumHashTables\n(\n5\n)\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"hashes\"\n)\nval\nmodel\n=\nmh\n.\nfit\n(\ndfA\n)\n// Feature Transformation\nprintln\n(\n\"The hashed dataset where hashed values are stored in the column 'hashes':\"\n)\nmodel\n.\ntransform\n(\ndfA\n).\nshow\n()\n// Compute the locality sensitive hashes for the input rows, then perform approximate\n// similarity join.\n// We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n// `model.approxSimilarityJoin(transformedA, transformedB, 0.6)`\nprintln\n(\n\"Approximately joining dfA and dfB on Jaccard distance smaller than 0.6:\"\n)\nmodel\n.\napproxSimilarityJoin\n(\ndfA\n,\ndfB\n,\n0.6\n,\n\"JaccardDistance\"\n)\n.\nselect\n(\ncol\n(\n\"datasetA.id\"\n).\nalias\n(\n\"idA\"\n),\ncol\n(\n\"datasetB.id\"\n).\nalias\n(\n\"idB\"\n),\ncol\n(\n\"JaccardDistance\"\n)).\nshow\n()\n// Compute the locality sensitive hashes for the input rows, then perform approximate nearest\n// neighbor search.\n// We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n// `model.approxNearestNeighbors(transformedA, key, 2)`\n// It may return less than 2 rows when not enough approximate near-neighbor candidates are\n// found.\nprintln\n(\n\"Approximately searching dfA for 2 nearest neighbors of the key:\"\n)\nmodel\n.\napproxNearestNeighbors\n(\ndfA\n,\nkey\n,\n2\n).\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\" in the Spark repo.\nRefer to the\nMinHashLSH Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.MinHashLSH\n;\nimport\norg.apache.spark.ml.feature.MinHashLSHModel\n;\nimport\norg.apache.spark.ml.linalg.Vector\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nimport\nstatic\norg\n.\napache\n.\nspark\n.\nsql\n.\nfunctions\n.\ncol\n;\nList\n<\nRow\n>\ndataA\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n0\n,\nVectors\n.\nsparse\n(\n6\n,\nnew\nint\n[]{\n0\n,\n1\n,\n2\n},\nnew\ndouble\n[]{\n1.0\n,\n1.0\n,\n1.0\n})),\nRowFactory\n.\ncreate\n(\n1\n,\nVectors\n.\nsparse\n(\n6\n,\nnew\nint\n[]{\n2\n,\n3\n,\n4\n},\nnew\ndouble\n[]{\n1.0\n,\n1.0\n,\n1.0\n})),\nRowFactory\n.\ncreate\n(\n2\n,\nVectors\n.\nsparse\n(\n6\n,\nnew\nint\n[]{\n0\n,\n2\n,\n4\n},\nnew\ndouble\n[]{\n1.0\n,\n1.0\n,\n1.0\n}))\n);\nList\n<\nRow\n>\ndataB\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n0\n,\nVectors\n.\nsparse\n(\n6\n,\nnew\nint\n[]{\n1\n,\n3\n,\n5\n},\nnew\ndouble\n[]{\n1.0\n,\n1.0\n,\n1.0\n})),\nRowFactory\n.\ncreate\n(\n1\n,\nVectors\n.\nsparse\n(\n6\n,\nnew\nint\n[]{\n2\n,\n3\n,\n5\n},\nnew\ndouble\n[]{\n1.0\n,\n1.0\n,\n1.0\n})),\nRowFactory\n.\ncreate\n(\n2\n,\nVectors\n.\nsparse\n(\n6\n,\nnew\nint\n[]{\n1\n,\n2\n,\n4\n},\nnew\ndouble\n[]{\n1.0\n,\n1.0\n,\n1.0\n}))\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"id\"\n,\nDataTypes\n.\nIntegerType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"features\"\n,\nnew\nVectorUDT\n(),\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndfA\n=\nspark\n.\ncreateDataFrame\n(\ndataA\n,\nschema\n);\nDataset\n<\nRow\n>\ndfB\n=\nspark\n.\ncreateDataFrame\n(\ndataB\n,\nschema\n);\nint\n[]\nindices\n=\n{\n1\n,\n3\n};\ndouble\n[]\nvalues\n=\n{\n1.0\n,\n1.0\n};\nVector\nkey\n=\nVectors\n.\nsparse\n(\n6\n,\nindices\n,\nvalues\n);\nMinHashLSH\nmh\n=\nnew\nMinHashLSH\n()\n.\nsetNumHashTables\n(\n5\n)\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"hashes\"\n);\nMinHashLSHModel\nmodel\n=\nmh\n.\nfit\n(\ndfA\n);\n// Feature Transformation\nSystem\n.\nout\n.\nprintln\n(\n\"The hashed dataset where hashed values are stored in the column 'hashes':\"\n);\nmodel\n.\ntransform\n(\ndfA\n).\nshow\n();\n// Compute the locality sensitive hashes for the input rows, then perform approximate\n// similarity join.\n// We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n// `model.approxSimilarityJoin(transformedA, transformedB, 0.6)`\nSystem\n.\nout\n.\nprintln\n(\n\"Approximately joining dfA and dfB on Jaccard distance smaller than 0.6:\"\n);\nmodel\n.\napproxSimilarityJoin\n(\ndfA\n,\ndfB\n,\n0.6\n,\n\"JaccardDistance\"\n)\n.\nselect\n(\ncol\n(\n\"datasetA.id\"\n).\nalias\n(\n\"idA\"\n),\ncol\n(\n\"datasetB.id\"\n).\nalias\n(\n\"idB\"\n),\ncol\n(\n\"JaccardDistance\"\n)).\nshow\n();\n// Compute the locality sensitive hashes for the input rows, then perform approximate nearest\n// neighbor search.\n// We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n// `model.approxNearestNeighbors(transformedA, key, 2)`\n// It may return less than 2 rows when not enough approximate near-neighbor candidates are\n// found.\nSystem\n.\nout\n.\nprintln\n(\n\"Approximately searching dfA for 2 nearest neighbors of the key:\"\n);\nmodel\n.\napproxNearestNeighbors\n(\ndfA\n,\nkey\n,\n2\n).\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\" in the Spark repo."}
{"url": "https://spark.apache.org/docs/latest/ml-classification-regression.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nClassification and regression\n\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\wv}{\\mathbf{w}}\n\\newcommand{\\av}{\\mathbf{\\alpha}}\n\\newcommand{\\bv}{\\mathbf{b}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\id}{\\mathbf{I}}\n\\newcommand{\\ind}{\\mathbf{1}}\n\\newcommand{\\0}{\\mathbf{0}}\n\\newcommand{\\unit}{\\mathbf{e}}\n\\newcommand{\\one}{\\mathbf{1}}\n\\newcommand{\\zero}{\\mathbf{0}}\n\\]\nThis page covers algorithms for Classification and Regression.  It also includes sections\ndiscussing specific classes of algorithms, such as linear methods, trees, and ensembles.\nTable of Contents\nClassification\nLogistic regression\nBinomial logistic regression\nMultinomial logistic regression\nDecision tree classifier\nRandom forest classifier\nGradient-boosted tree classifier\nMultilayer perceptron classifier\nLinear Support Vector Machine\nOne-vs-Rest classifier (a.k.a. One-vs-All)\nNaive Bayes\nFactorization machines classifier\nRegression\nLinear regression\nGeneralized linear regression\nAvailable families\nDecision tree regression\nRandom forest regression\nGradient-boosted tree regression\nSurvival regression\nIsotonic regression\nFactorization machines regressor\nLinear methods\nFactorization Machines\nDecision trees\nInputs and Outputs\nInput Columns\nOutput Columns\nTree Ensembles\nRandom Forests\nInputs and Outputs\nInput Columns\nOutput Columns (Predictions)\nGradient-Boosted Trees (GBTs)\nInputs and Outputs\nInput Columns\nOutput Columns (Predictions)\nClassification\nLogistic regression\nLogistic regression is a popular method to predict a categorical response. It is a special case of\nGeneralized Linear models\nthat predicts the probability of the outcomes.\nIn\nspark.ml\nlogistic regression can be used to predict a binary outcome by using binomial logistic regression, or it can be used to predict a multiclass outcome by using multinomial logistic regression. Use the\nfamily\nparameter to select between these two algorithms, or leave it unset and Spark will infer the correct variant.\nMultinomial logistic regression can be used for binary classification by setting the\nfamily\nparam to “multinomial”. It will produce two sets of coefficients and two intercepts.\nWhen fitting LogisticRegressionModel without intercept on dataset with constant nonzero column, Spark MLlib outputs zero coefficients for constant nonzero columns. This behavior is the same as R glmnet but different from LIBSVM.\nBinomial logistic regression\nFor more background and more details about the implementation of binomial logistic regression, refer to the documentation of\nlogistic regression in\nspark.mllib\n.\nExamples\nThe following example shows how to train binomial and multinomial logistic regression\nmodels for binary classification with elastic net regularization.\nelasticNetParam\ncorresponds to\n$\\alpha$ and\nregParam\ncorresponds to $\\lambda$.\nMore details on parameters can be found in the\nPython API documentation\n.\nfrom\npyspark.ml.classification\nimport\nLogisticRegression\n# Load training data\ntraining\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\nlr\n=\nLogisticRegression\n(\nmaxIter\n=\n10\n,\nregParam\n=\n0.3\n,\nelasticNetParam\n=\n0.8\n)\n# Fit the model\nlrModel\n=\nlr\n.\nfit\n(\ntraining\n)\n# Print the coefficients and intercept for logistic regression\nprint\n(\n\"\nCoefficients:\n\"\n+\nstr\n(\nlrModel\n.\ncoefficients\n))\nprint\n(\n\"\nIntercept:\n\"\n+\nstr\n(\nlrModel\n.\nintercept\n))\n# We can also use the multinomial family for binary classification\nmlr\n=\nLogisticRegression\n(\nmaxIter\n=\n10\n,\nregParam\n=\n0.3\n,\nelasticNetParam\n=\n0.8\n,\nfamily\n=\n\"\nmultinomial\n\"\n)\n# Fit the model\nmlrModel\n=\nmlr\n.\nfit\n(\ntraining\n)\n# Print the coefficients and intercepts for logistic regression with multinomial family\nprint\n(\n\"\nMultinomial coefficients:\n\"\n+\nstr\n(\nmlrModel\n.\ncoefficientMatrix\n))\nprint\n(\n\"\nMultinomial intercepts:\n\"\n+\nstr\n(\nmlrModel\n.\ninterceptVector\n))\nFind full example code at \"examples/src/main/python/ml/logistic_regression_with_elastic_net.py\" in the Spark repo.\nMore details on parameters can be found in the\nScala API documentation\n.\nimport\norg.apache.spark.ml.classification.LogisticRegression\n// Load training data\nval\ntraining\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\nval\nlr\n=\nnew\nLogisticRegression\n()\n.\nsetMaxIter\n(\n10\n)\n.\nsetRegParam\n(\n0.3\n)\n.\nsetElasticNetParam\n(\n0.8\n)\n// Fit the model\nval\nlrModel\n=\nlr\n.\nfit\n(\ntraining\n)\n// Print the coefficients and intercept for logistic regression\nprintln\n(\ns\n\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\"\n)\n// We can also use the multinomial family for binary classification\nval\nmlr\n=\nnew\nLogisticRegression\n()\n.\nsetMaxIter\n(\n10\n)\n.\nsetRegParam\n(\n0.3\n)\n.\nsetElasticNetParam\n(\n0.8\n)\n.\nsetFamily\n(\n\"multinomial\"\n)\nval\nmlrModel\n=\nmlr\n.\nfit\n(\ntraining\n)\n// Print the coefficients and intercepts for logistic regression with multinomial family\nprintln\n(\ns\n\"Multinomial coefficients: ${mlrModel.coefficientMatrix}\"\n)\nprintln\n(\ns\n\"Multinomial intercepts: ${mlrModel.interceptVector}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\" in the Spark repo.\nMore details on parameters can be found in the\nJava API documentation\n.\nimport\norg.apache.spark.ml.classification.LogisticRegression\n;\nimport\norg.apache.spark.ml.classification.LogisticRegressionModel\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Load training data\nDataset\n<\nRow\n>\ntraining\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n);\nLogisticRegression\nlr\n=\nnew\nLogisticRegression\n()\n.\nsetMaxIter\n(\n10\n)\n.\nsetRegParam\n(\n0.3\n)\n.\nsetElasticNetParam\n(\n0.8\n);\n// Fit the model\nLogisticRegressionModel\nlrModel\n=\nlr\n.\nfit\n(\ntraining\n);\n// Print the coefficients and intercept for logistic regression\nSystem\n.\nout\n.\nprintln\n(\n\"Coefficients: \"\n+\nlrModel\n.\ncoefficients\n()\n+\n\" Intercept: \"\n+\nlrModel\n.\nintercept\n());\n// We can also use the multinomial family for binary classification\nLogisticRegression\nmlr\n=\nnew\nLogisticRegression\n()\n.\nsetMaxIter\n(\n10\n)\n.\nsetRegParam\n(\n0.3\n)\n.\nsetElasticNetParam\n(\n0.8\n)\n.\nsetFamily\n(\n\"multinomial\"\n);\n// Fit the model\nLogisticRegressionModel\nmlrModel\n=\nmlr\n.\nfit\n(\ntraining\n);\n// Print the coefficients and intercepts for logistic regression with multinomial family\nSystem\n.\nout\n.\nprintln\n(\n\"Multinomial coefficients: \"\n+\nlrModel\n.\ncoefficientMatrix\n()\n+\n\"\\nMultinomial intercepts: \"\n+\nmlrModel\n.\ninterceptVector\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\" in the Spark repo.\nMore details on parameters can be found in the\nR API documentation\n.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_libsvm_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit an binomial logistic regression model with spark.logit\nmodel\n<-\nspark.logit\n(\ntraining\n,\nlabel\n~\nfeatures\n,\nmaxIter\n=\n10\n,\nregParam\n=\n0.3\n,\nelasticNetParam\n=\n0.8\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\nFind full example code at \"examples/src/main/r/ml/logit.R\" in the Spark repo.\nThe\nspark.ml\nimplementation of logistic regression also supports\nextracting a summary of the model over the training set. Note that the\npredictions and metrics which are stored as\nDataFrame\nin\nLogisticRegressionSummary\nare annotated\n@transient\nand hence\nonly available on the driver.\nLogisticRegressionTrainingSummary\nprovides a summary for a\nLogisticRegressionModel\n.\nIn the case of binary classification, certain additional metrics are\navailable, e.g. ROC curve. See\nBinaryLogisticRegressionTrainingSummary\n.\nContinuing the earlier example:\nfrom\npyspark.ml.classification\nimport\nLogisticRegression\n# Extract the summary from the returned LogisticRegressionModel instance trained\n# in the earlier example\ntrainingSummary\n=\nlrModel\n.\nsummary\n# Obtain the objective per iteration\nobjectiveHistory\n=\ntrainingSummary\n.\nobjectiveHistory\nprint\n(\n\"\nobjectiveHistory:\n\"\n)\nfor\nobjective\nin\nobjectiveHistory\n:\nprint\n(\nobjective\n)\n# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\ntrainingSummary\n.\nroc\n.\nshow\n()\nprint\n(\n\"\nareaUnderROC:\n\"\n+\nstr\n(\ntrainingSummary\n.\nareaUnderROC\n))\n# Set the model threshold to maximize F-Measure\nfMeasure\n=\ntrainingSummary\n.\nfMeasureByThreshold\nmaxFMeasure\n=\nfMeasure\n.\ngroupBy\n().\nmax\n(\n'\nF-Measure\n'\n).\nselect\n(\n'\nmax(F-Measure)\n'\n).\nhead\n()\nbestThreshold\n=\nfMeasure\n.\nwhere\n(\nfMeasure\n[\n'\nF-Measure\n'\n]\n==\nmaxFMeasure\n[\n'\nmax(F-Measure)\n'\n])\n\\\n.\nselect\n(\n'\nthreshold\n'\n).\nhead\n()[\n'\nthreshold\n'\n]\nlr\n.\nsetThreshold\n(\nbestThreshold\n)\nFind full example code at \"examples/src/main/python/ml/logistic_regression_summary_example.py\" in the Spark repo.\nLogisticRegressionTrainingSummary\nprovides a summary for a\nLogisticRegressionModel\n.\nIn the case of binary classification, certain additional metrics are\navailable, e.g. ROC curve. The binary summary can be accessed via the\nbinarySummary\nmethod. See\nBinaryLogisticRegressionTrainingSummary\n.\nContinuing the earlier example:\nimport\norg.apache.spark.ml.classification.LogisticRegression\n// Extract the summary from the returned LogisticRegressionModel instance trained in the earlier\n// example\nval\ntrainingSummary\n=\nlrModel\n.\nbinarySummary\n// Obtain the objective per iteration.\nval\nobjectiveHistory\n=\ntrainingSummary\n.\nobjectiveHistory\nprintln\n(\n\"objectiveHistory:\"\n)\nobjectiveHistory\n.\nforeach\n(\nloss\n=>\nprintln\n(\nloss\n))\n// Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\nval\nroc\n=\ntrainingSummary\n.\nroc\nroc\n.\nshow\n()\nprintln\n(\ns\n\"areaUnderROC: ${trainingSummary.areaUnderROC}\"\n)\n// Set the model threshold to maximize F-Measure\nval\nfMeasure\n=\ntrainingSummary\n.\nfMeasureByThreshold\nval\nmaxFMeasure\n=\nfMeasure\n.\nselect\n(\nmax\n(\n\"F-Measure\"\n)).\nhead\n().\ngetDouble\n(\n0\n)\nval\nbestThreshold\n=\nfMeasure\n.\nwhere\n(\n$\n\"F-Measure\"\n===\nmaxFMeasure\n)\n.\nselect\n(\n\"threshold\"\n).\nhead\n().\ngetDouble\n(\n0\n)\nlrModel\n.\nsetThreshold\n(\nbestThreshold\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\" in the Spark repo.\nLogisticRegressionTrainingSummary\nprovides a summary for a\nLogisticRegressionModel\n.\nIn the case of binary classification, certain additional metrics are\navailable, e.g. ROC curve. The binary summary can be accessed via the\nbinarySummary\nmethod. See\nBinaryLogisticRegressionTrainingSummary\n.\nContinuing the earlier example:\nimport\norg.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary\n;\nimport\norg.apache.spark.ml.classification.LogisticRegression\n;\nimport\norg.apache.spark.ml.classification.LogisticRegressionModel\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\nimport\norg.apache.spark.sql.functions\n;\n// Extract the summary from the returned LogisticRegressionModel instance trained in the earlier\n// example\nBinaryLogisticRegressionTrainingSummary\ntrainingSummary\n=\nlrModel\n.\nbinarySummary\n();\n// Obtain the loss per iteration.\ndouble\n[]\nobjectiveHistory\n=\ntrainingSummary\n.\nobjectiveHistory\n();\nfor\n(\ndouble\nlossPerIteration\n:\nobjectiveHistory\n)\n{\nSystem\n.\nout\n.\nprintln\n(\nlossPerIteration\n);\n}\n// Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\nDataset\n<\nRow\n>\nroc\n=\ntrainingSummary\n.\nroc\n();\nroc\n.\nshow\n();\nroc\n.\nselect\n(\n\"FPR\"\n).\nshow\n();\nSystem\n.\nout\n.\nprintln\n(\ntrainingSummary\n.\nareaUnderROC\n());\n// Get the threshold corresponding to the maximum F-Measure and rerun LogisticRegression with\n// this selected threshold.\nDataset\n<\nRow\n>\nfMeasure\n=\ntrainingSummary\n.\nfMeasureByThreshold\n();\ndouble\nmaxFMeasure\n=\nfMeasure\n.\nselect\n(\nfunctions\n.\nmax\n(\n\"F-Measure\"\n)).\nhead\n().\ngetDouble\n(\n0\n);\ndouble\nbestThreshold\n=\nfMeasure\n.\nwhere\n(\nfMeasure\n.\ncol\n(\n\"F-Measure\"\n).\nequalTo\n(\nmaxFMeasure\n))\n.\nselect\n(\n\"threshold\"\n).\nhead\n().\ngetDouble\n(\n0\n);\nlrModel\n.\nsetThreshold\n(\nbestThreshold\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\" in the Spark repo.\nMultinomial logistic regression\nMulticlass classification is supported via multinomial logistic (softmax) regression. In multinomial logistic regression,\nthe algorithm produces $K$ sets of coefficients, or a matrix of dimension $K \\times J$ where $K$ is the number of outcome\nclasses and $J$ is the number of features. If the algorithm is fit with an intercept term then a length $K$ vector of\nintercepts is available.\nMultinomial coefficients are available as\ncoefficientMatrix\nand intercepts are available as\ninterceptVector\n.\ncoefficients\nand\nintercept\nmethods on a logistic regression model trained with multinomial family are not supported. Use\ncoefficientMatrix\nand\ninterceptVector\ninstead.\nThe conditional probabilities of the outcome classes $k \\in {1, 2, …, K}$ are modeled using the softmax function.\n\\[\n   P(Y=k|\\mathbf{X}, \\boldsymbol{\\beta}_k, \\beta_{0k}) =  \\frac{e^{\\boldsymbol{\\beta}_k \\cdot \\mathbf{X}  + \\beta_{0k}}}{\\sum_{k'=0}^{K-1} e^{\\boldsymbol{\\beta}_{k'} \\cdot \\mathbf{X}  + \\beta_{0k'}}}\n\\]\nWe minimize the weighted negative log-likelihood, using a multinomial response model, with elastic-net penalty to control for overfitting.\n\\[\n\\min_{\\beta, \\beta_0} -\\left[\\sum_{i=1}^L w_i \\cdot \\log P(Y = y_i|\\mathbf{x}_i)\\right] + \\lambda \\left[\\frac{1}{2}\\left(1 - \\alpha\\right)||\\boldsymbol{\\beta}||_2^2 + \\alpha ||\\boldsymbol{\\beta}||_1\\right]\n\\]\nFor a detailed derivation please see\nhere\n.\nExamples\nThe following example shows how to train a multiclass logistic regression\nmodel with elastic net regularization, as well as extract the multiclass\ntraining summary for evaluating the model.\nfrom\npyspark.ml.classification\nimport\nLogisticRegression\n# Load training data\ntraining\n=\nspark\n\\\n.\nread\n\\\n.\nformat\n(\n\"\nlibsvm\n\"\n)\n\\\n.\nload\n(\n\"\ndata/mllib/sample_multiclass_classification_data.txt\n\"\n)\nlr\n=\nLogisticRegression\n(\nmaxIter\n=\n10\n,\nregParam\n=\n0.3\n,\nelasticNetParam\n=\n0.8\n)\n# Fit the model\nlrModel\n=\nlr\n.\nfit\n(\ntraining\n)\n# Print the coefficients and intercept for multinomial logistic regression\nprint\n(\n\"\nCoefficients:\n\\n\n\"\n+\nstr\n(\nlrModel\n.\ncoefficientMatrix\n))\nprint\n(\n\"\nIntercept:\n\"\n+\nstr\n(\nlrModel\n.\ninterceptVector\n))\ntrainingSummary\n=\nlrModel\n.\nsummary\n# Obtain the objective per iteration\nobjectiveHistory\n=\ntrainingSummary\n.\nobjectiveHistory\nprint\n(\n\"\nobjectiveHistory:\n\"\n)\nfor\nobjective\nin\nobjectiveHistory\n:\nprint\n(\nobjective\n)\n# for multiclass, we can inspect metrics on a per-label basis\nprint\n(\n\"\nFalse positive rate by label:\n\"\n)\nfor\ni\n,\nrate\nin\nenumerate\n(\ntrainingSummary\n.\nfalsePositiveRateByLabel\n):\nprint\n(\n\"\nlabel %d: %s\n\"\n%\n(\ni\n,\nrate\n))\nprint\n(\n\"\nTrue positive rate by label:\n\"\n)\nfor\ni\n,\nrate\nin\nenumerate\n(\ntrainingSummary\n.\ntruePositiveRateByLabel\n):\nprint\n(\n\"\nlabel %d: %s\n\"\n%\n(\ni\n,\nrate\n))\nprint\n(\n\"\nPrecision by label:\n\"\n)\nfor\ni\n,\nprec\nin\nenumerate\n(\ntrainingSummary\n.\nprecisionByLabel\n):\nprint\n(\n\"\nlabel %d: %s\n\"\n%\n(\ni\n,\nprec\n))\nprint\n(\n\"\nRecall by label:\n\"\n)\nfor\ni\n,\nrec\nin\nenumerate\n(\ntrainingSummary\n.\nrecallByLabel\n):\nprint\n(\n\"\nlabel %d: %s\n\"\n%\n(\ni\n,\nrec\n))\nprint\n(\n\"\nF-measure by label:\n\"\n)\nfor\ni\n,\nf\nin\nenumerate\n(\ntrainingSummary\n.\nfMeasureByLabel\n()):\nprint\n(\n\"\nlabel %d: %s\n\"\n%\n(\ni\n,\nf\n))\naccuracy\n=\ntrainingSummary\n.\naccuracy\nfalsePositiveRate\n=\ntrainingSummary\n.\nweightedFalsePositiveRate\ntruePositiveRate\n=\ntrainingSummary\n.\nweightedTruePositiveRate\nfMeasure\n=\ntrainingSummary\n.\nweightedFMeasure\n()\nprecision\n=\ntrainingSummary\n.\nweightedPrecision\nrecall\n=\ntrainingSummary\n.\nweightedRecall\nprint\n(\n\"\nAccuracy: %s\n\\n\nFPR: %s\n\\n\nTPR: %s\n\\n\nF-measure: %s\n\\n\nPrecision: %s\n\\n\nRecall: %s\n\"\n%\n(\naccuracy\n,\nfalsePositiveRate\n,\ntruePositiveRate\n,\nfMeasure\n,\nprecision\n,\nrecall\n))\nFind full example code at \"examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\" in the Spark repo.\nimport\norg.apache.spark.ml.classification.LogisticRegression\n// Load training data\nval\ntraining\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_multiclass_classification_data.txt\"\n)\nval\nlr\n=\nnew\nLogisticRegression\n()\n.\nsetMaxIter\n(\n10\n)\n.\nsetRegParam\n(\n0.3\n)\n.\nsetElasticNetParam\n(\n0.8\n)\n// Fit the model\nval\nlrModel\n=\nlr\n.\nfit\n(\ntraining\n)\n// Print the coefficients and intercept for multinomial logistic regression\nprintln\n(\ns\n\"Coefficients: \\n${lrModel.coefficientMatrix}\"\n)\nprintln\n(\ns\n\"Intercepts: \\n${lrModel.interceptVector}\"\n)\nval\ntrainingSummary\n=\nlrModel\n.\nsummary\n// Obtain the objective per iteration\nval\nobjectiveHistory\n=\ntrainingSummary\n.\nobjectiveHistory\nprintln\n(\n\"objectiveHistory:\"\n)\nobjectiveHistory\n.\nforeach\n(\nprintln\n)\n// for multiclass, we can inspect metrics on a per-label basis\nprintln\n(\n\"False positive rate by label:\"\n)\ntrainingSummary\n.\nfalsePositiveRateByLabel\n.\nzipWithIndex\n.\nforeach\n{\ncase\n(\nrate\n,\nlabel\n)\n=>\nprintln\n(\ns\n\"label $label: $rate\"\n)\n}\nprintln\n(\n\"True positive rate by label:\"\n)\ntrainingSummary\n.\ntruePositiveRateByLabel\n.\nzipWithIndex\n.\nforeach\n{\ncase\n(\nrate\n,\nlabel\n)\n=>\nprintln\n(\ns\n\"label $label: $rate\"\n)\n}\nprintln\n(\n\"Precision by label:\"\n)\ntrainingSummary\n.\nprecisionByLabel\n.\nzipWithIndex\n.\nforeach\n{\ncase\n(\nprec\n,\nlabel\n)\n=>\nprintln\n(\ns\n\"label $label: $prec\"\n)\n}\nprintln\n(\n\"Recall by label:\"\n)\ntrainingSummary\n.\nrecallByLabel\n.\nzipWithIndex\n.\nforeach\n{\ncase\n(\nrec\n,\nlabel\n)\n=>\nprintln\n(\ns\n\"label $label: $rec\"\n)\n}\nprintln\n(\n\"F-measure by label:\"\n)\ntrainingSummary\n.\nfMeasureByLabel\n.\nzipWithIndex\n.\nforeach\n{\ncase\n(\nf\n,\nlabel\n)\n=>\nprintln\n(\ns\n\"label $label: $f\"\n)\n}\nval\naccuracy\n=\ntrainingSummary\n.\naccuracy\nval\nfalsePositiveRate\n=\ntrainingSummary\n.\nweightedFalsePositiveRate\nval\ntruePositiveRate\n=\ntrainingSummary\n.\nweightedTruePositiveRate\nval\nfMeasure\n=\ntrainingSummary\n.\nweightedFMeasure\nval\nprecision\n=\ntrainingSummary\n.\nweightedPrecision\nval\nrecall\n=\ntrainingSummary\n.\nweightedRecall\nprintln\n(\ns\n\"Accuracy: $accuracy\\nFPR: $falsePositiveRate\\nTPR: $truePositiveRate\\n\"\n+\ns\n\"F-measure: $fMeasure\\nPrecision: $precision\\nRecall: $recall\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\" in the Spark repo.\nimport\norg.apache.spark.ml.classification.LogisticRegression\n;\nimport\norg.apache.spark.ml.classification.LogisticRegressionModel\n;\nimport\norg.apache.spark.ml.classification.LogisticRegressionTrainingSummary\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Load training data\nDataset\n<\nRow\n>\ntraining\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_multiclass_classification_data.txt\"\n);\nLogisticRegression\nlr\n=\nnew\nLogisticRegression\n()\n.\nsetMaxIter\n(\n10\n)\n.\nsetRegParam\n(\n0.3\n)\n.\nsetElasticNetParam\n(\n0.8\n);\n// Fit the model\nLogisticRegressionModel\nlrModel\n=\nlr\n.\nfit\n(\ntraining\n);\n// Print the coefficients and intercept for multinomial logistic regression\nSystem\n.\nout\n.\nprintln\n(\n\"Coefficients: \\n\"\n+\nlrModel\n.\ncoefficientMatrix\n()\n+\n\" \\nIntercept: \"\n+\nlrModel\n.\ninterceptVector\n());\nLogisticRegressionTrainingSummary\ntrainingSummary\n=\nlrModel\n.\nsummary\n();\n// Obtain the loss per iteration.\ndouble\n[]\nobjectiveHistory\n=\ntrainingSummary\n.\nobjectiveHistory\n();\nfor\n(\ndouble\nlossPerIteration\n:\nobjectiveHistory\n)\n{\nSystem\n.\nout\n.\nprintln\n(\nlossPerIteration\n);\n}\n// for multiclass, we can inspect metrics on a per-label basis\nSystem\n.\nout\n.\nprintln\n(\n\"False positive rate by label:\"\n);\nint\ni\n=\n0\n;\ndouble\n[]\nfprLabel\n=\ntrainingSummary\n.\nfalsePositiveRateByLabel\n();\nfor\n(\ndouble\nfpr\n:\nfprLabel\n)\n{\nSystem\n.\nout\n.\nprintln\n(\n\"label \"\n+\ni\n+\n\": \"\n+\nfpr\n);\ni\n++;\n}\nSystem\n.\nout\n.\nprintln\n(\n\"True positive rate by label:\"\n);\ni\n=\n0\n;\ndouble\n[]\ntprLabel\n=\ntrainingSummary\n.\ntruePositiveRateByLabel\n();\nfor\n(\ndouble\ntpr\n:\ntprLabel\n)\n{\nSystem\n.\nout\n.\nprintln\n(\n\"label \"\n+\ni\n+\n\": \"\n+\ntpr\n);\ni\n++;\n}\nSystem\n.\nout\n.\nprintln\n(\n\"Precision by label:\"\n);\ni\n=\n0\n;\ndouble\n[]\nprecLabel\n=\ntrainingSummary\n.\nprecisionByLabel\n();\nfor\n(\ndouble\nprec\n:\nprecLabel\n)\n{\nSystem\n.\nout\n.\nprintln\n(\n\"label \"\n+\ni\n+\n\": \"\n+\nprec\n);\ni\n++;\n}\nSystem\n.\nout\n.\nprintln\n(\n\"Recall by label:\"\n);\ni\n=\n0\n;\ndouble\n[]\nrecLabel\n=\ntrainingSummary\n.\nrecallByLabel\n();\nfor\n(\ndouble\nrec\n:\nrecLabel\n)\n{\nSystem\n.\nout\n.\nprintln\n(\n\"label \"\n+\ni\n+\n\": \"\n+\nrec\n);\ni\n++;\n}\nSystem\n.\nout\n.\nprintln\n(\n\"F-measure by label:\"\n);\ni\n=\n0\n;\ndouble\n[]\nfLabel\n=\ntrainingSummary\n.\nfMeasureByLabel\n();\nfor\n(\ndouble\nf\n:\nfLabel\n)\n{\nSystem\n.\nout\n.\nprintln\n(\n\"label \"\n+\ni\n+\n\": \"\n+\nf\n);\ni\n++;\n}\ndouble\naccuracy\n=\ntrainingSummary\n.\naccuracy\n();\ndouble\nfalsePositiveRate\n=\ntrainingSummary\n.\nweightedFalsePositiveRate\n();\ndouble\ntruePositiveRate\n=\ntrainingSummary\n.\nweightedTruePositiveRate\n();\ndouble\nfMeasure\n=\ntrainingSummary\n.\nweightedFMeasure\n();\ndouble\nprecision\n=\ntrainingSummary\n.\nweightedPrecision\n();\ndouble\nrecall\n=\ntrainingSummary\n.\nweightedRecall\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Accuracy: \"\n+\naccuracy\n);\nSystem\n.\nout\n.\nprintln\n(\n\"FPR: \"\n+\nfalsePositiveRate\n);\nSystem\n.\nout\n.\nprintln\n(\n\"TPR: \"\n+\ntruePositiveRate\n);\nSystem\n.\nout\n.\nprintln\n(\n\"F-measure: \"\n+\nfMeasure\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Precision: \"\n+\nprecision\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Recall: \"\n+\nrecall\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\" in the Spark repo.\nMore details on parameters can be found in the\nR API documentation\n.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_multiclass_classification_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit a multinomial logistic regression model with spark.logit\nmodel\n<-\nspark.logit\n(\ntraining\n,\nlabel\n~\nfeatures\n,\nmaxIter\n=\n10\n,\nregParam\n=\n0.3\n,\nelasticNetParam\n=\n0.8\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\nFind full example code at \"examples/src/main/r/ml/logit.R\" in the Spark repo.\nDecision tree classifier\nDecision trees are a popular family of classification and regression methods.\nMore information about the\nspark.ml\nimplementation can be found further in the\nsection on decision trees\n.\nExamples\nThe following examples load a dataset in LibSVM format, split it into training and test sets, train on the first dataset, and then evaluate on the held-out test set.\nWe use two feature transformers to prepare the data; these help index categories for the label and categorical features, adding metadata to the\nDataFrame\nwhich the Decision Tree algorithm can recognize.\nMore details on parameters can be found in the\nPython API documentation\n.\nfrom\npyspark.ml\nimport\nPipeline\nfrom\npyspark.ml.classification\nimport\nDecisionTreeClassifier\nfrom\npyspark.ml.feature\nimport\nStringIndexer\n,\nVectorIndexer\nfrom\npyspark.ml.evaluation\nimport\nMulticlassClassificationEvaluator\n# Load the data stored in LIBSVM format as a DataFrame.\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer\n=\nStringIndexer\n(\ninputCol\n=\n\"\nlabel\n\"\n,\noutputCol\n=\n\"\nindexedLabel\n\"\n).\nfit\n(\ndata\n)\n# Automatically identify categorical features, and index them.\n# We specify maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer\n=\n\\\nVectorIndexer\n(\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nindexedFeatures\n\"\n,\nmaxCategories\n=\n4\n).\nfit\n(\ndata\n)\n# Split the data into training and test sets (30% held out for testing)\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n([\n0.7\n,\n0.3\n])\n# Train a DecisionTree model.\ndt\n=\nDecisionTreeClassifier\n(\nlabelCol\n=\n\"\nindexedLabel\n\"\n,\nfeaturesCol\n=\n\"\nindexedFeatures\n\"\n)\n# Chain indexers and tree in a Pipeline\npipeline\n=\nPipeline\n(\nstages\n=\n[\nlabelIndexer\n,\nfeatureIndexer\n,\ndt\n])\n# Train model.  This also runs the indexers.\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n)\n# Make predictions.\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n)\n# Select example rows to display.\npredictions\n.\nselect\n(\n\"\nprediction\n\"\n,\n\"\nindexedLabel\n\"\n,\n\"\nfeatures\n\"\n).\nshow\n(\n5\n)\n# Select (prediction, true label) and compute test error\nevaluator\n=\nMulticlassClassificationEvaluator\n(\nlabelCol\n=\n\"\nindexedLabel\n\"\n,\npredictionCol\n=\n\"\nprediction\n\"\n,\nmetricName\n=\n\"\naccuracy\n\"\n)\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprint\n(\n\"\nTest Error = %g\n\"\n%\n(\n1.0\n-\naccuracy\n))\ntreeModel\n=\nmodel\n.\nstages\n[\n2\n]\n# summary only\nprint\n(\ntreeModel\n)\nFind full example code at \"examples/src/main/python/ml/decision_tree_classification_example.py\" in the Spark repo.\nMore details on parameters can be found in the\nScala API documentation\n.\nimport\norg.apache.spark.ml.Pipeline\nimport\norg.apache.spark.ml.classification.DecisionTreeClassificationModel\nimport\norg.apache.spark.ml.classification.DecisionTreeClassifier\nimport\norg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport\norg.apache.spark.ml.feature.\n{\nIndexToString\n,\nStringIndexer\n,\nVectorIndexer\n}\n// Load the data stored in LIBSVM format as a DataFrame.\nval\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Index labels, adding metadata to the label column.\n// Fit on whole dataset to include all labels in index.\nval\nlabelIndexer\n=\nnew\nStringIndexer\n()\n.\nsetInputCol\n(\n\"label\"\n)\n.\nsetOutputCol\n(\n\"indexedLabel\"\n)\n.\nfit\n(\ndata\n)\n// Automatically identify categorical features, and index them.\nval\nfeatureIndexer\n=\nnew\nVectorIndexer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxCategories\n(\n4\n)\n// features with > 4 distinct values are treated as continuous.\n.\nfit\n(\ndata\n)\n// Split the data into training and test sets (30% held out for testing).\nval\nArray\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.7\n,\n0.3\n))\n// Train a DecisionTree model.\nval\ndt\n=\nnew\nDecisionTreeClassifier\n()\n.\nsetLabelCol\n(\n\"indexedLabel\"\n)\n.\nsetFeaturesCol\n(\n\"indexedFeatures\"\n)\n// Convert indexed labels back to original labels.\nval\nlabelConverter\n=\nnew\nIndexToString\n()\n.\nsetInputCol\n(\n\"prediction\"\n)\n.\nsetOutputCol\n(\n\"predictedLabel\"\n)\n.\nsetLabels\n(\nlabelIndexer\n.\nlabelsArray\n(\n0\n))\n// Chain indexers and tree in a Pipeline.\nval\npipeline\n=\nnew\nPipeline\n()\n.\nsetStages\n(\nArray\n(\nlabelIndexer\n,\nfeatureIndexer\n,\ndt\n,\nlabelConverter\n))\n// Train model. This also runs the indexers.\nval\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n)\n// Make predictions.\nval\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n)\n// Select example rows to display.\npredictions\n.\nselect\n(\n\"predictedLabel\"\n,\n\"label\"\n,\n\"features\"\n).\nshow\n(\n5\n)\n// Select (prediction, true label) and compute test error.\nval\nevaluator\n=\nnew\nMulticlassClassificationEvaluator\n()\n.\nsetLabelCol\n(\n\"indexedLabel\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"accuracy\"\n)\nval\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprintln\n(\ns\n\"Test Error = ${(1.0 - accuracy)}\"\n)\nval\ntreeModel\n=\nmodel\n.\nstages\n(\n2\n).\nasInstanceOf\n[\nDecisionTreeClassificationModel\n]\nprintln\n(\ns\n\"Learned classification tree model:\\n ${treeModel.toDebugString}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\" in the Spark repo.\nMore details on parameters can be found in the\nJava API documentation\n.\nimport\norg.apache.spark.ml.Pipeline\n;\nimport\norg.apache.spark.ml.PipelineModel\n;\nimport\norg.apache.spark.ml.PipelineStage\n;\nimport\norg.apache.spark.ml.classification.DecisionTreeClassifier\n;\nimport\norg.apache.spark.ml.classification.DecisionTreeClassificationModel\n;\nimport\norg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n;\nimport\norg.apache.spark.ml.feature.*\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Load the data stored in LIBSVM format as a DataFrame.\nDataset\n<\nRow\n>\ndata\n=\nspark\n.\nread\n()\n.\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n);\n// Index labels, adding metadata to the label column.\n// Fit on whole dataset to include all labels in index.\nStringIndexerModel\nlabelIndexer\n=\nnew\nStringIndexer\n()\n.\nsetInputCol\n(\n\"label\"\n)\n.\nsetOutputCol\n(\n\"indexedLabel\"\n)\n.\nfit\n(\ndata\n);\n// Automatically identify categorical features, and index them.\nVectorIndexerModel\nfeatureIndexer\n=\nnew\nVectorIndexer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxCategories\n(\n4\n)\n// features with > 4 distinct values are treated as continuous.\n.\nfit\n(\ndata\n);\n// Split the data into training and test sets (30% held out for testing).\nDataset\n<\nRow\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]{\n0.7\n,\n0.3\n});\nDataset\n<\nRow\n>\ntrainingData\n=\nsplits\n[\n0\n];\nDataset\n<\nRow\n>\ntestData\n=\nsplits\n[\n1\n];\n// Train a DecisionTree model.\nDecisionTreeClassifier\ndt\n=\nnew\nDecisionTreeClassifier\n()\n.\nsetLabelCol\n(\n\"indexedLabel\"\n)\n.\nsetFeaturesCol\n(\n\"indexedFeatures\"\n);\n// Convert indexed labels back to original labels.\nIndexToString\nlabelConverter\n=\nnew\nIndexToString\n()\n.\nsetInputCol\n(\n\"prediction\"\n)\n.\nsetOutputCol\n(\n\"predictedLabel\"\n)\n.\nsetLabels\n(\nlabelIndexer\n.\nlabelsArray\n()[\n0\n]);\n// Chain indexers and tree in a Pipeline.\nPipeline\npipeline\n=\nnew\nPipeline\n()\n.\nsetStages\n(\nnew\nPipelineStage\n[]{\nlabelIndexer\n,\nfeatureIndexer\n,\ndt\n,\nlabelConverter\n});\n// Train model. This also runs the indexers.\nPipelineModel\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n);\n// Make predictions.\nDataset\n<\nRow\n>\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n);\n// Select example rows to display.\npredictions\n.\nselect\n(\n\"predictedLabel\"\n,\n\"label\"\n,\n\"features\"\n).\nshow\n(\n5\n);\n// Select (prediction, true label) and compute test error.\nMulticlassClassificationEvaluator\nevaluator\n=\nnew\nMulticlassClassificationEvaluator\n()\n.\nsetLabelCol\n(\n\"indexedLabel\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"accuracy\"\n);\ndouble\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Test Error = \"\n+\n(\n1.0\n-\naccuracy\n));\nDecisionTreeClassificationModel\ntreeModel\n=\n(\nDecisionTreeClassificationModel\n)\n(\nmodel\n.\nstages\n()[\n2\n]);\nSystem\n.\nout\n.\nprintln\n(\n\"Learned classification tree model:\\n\"\n+\ntreeModel\n.\ntoDebugString\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_libsvm_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit a DecisionTree classification model with spark.decisionTree\nmodel\n<-\nspark.decisionTree\n(\ntraining\n,\nlabel\n~\nfeatures\n,\n\"classification\"\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\nFind full example code at \"examples/src/main/r/ml/decisionTree.R\" in the Spark repo.\nRandom forest classifier\nRandom forests are a popular family of classification and regression methods.\nMore information about the\nspark.ml\nimplementation can be found further in the\nsection on random forests\n.\nExamples\nThe following examples load a dataset in LibSVM format, split it into training and test sets, train on the first dataset, and then evaluate on the held-out test set.\nWe use two feature transformers to prepare the data; these help index categories for the label and categorical features, adding metadata to the\nDataFrame\nwhich the tree-based algorithms can recognize.\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml\nimport\nPipeline\nfrom\npyspark.ml.classification\nimport\nRandomForestClassifier\nfrom\npyspark.ml.feature\nimport\nIndexToString\n,\nStringIndexer\n,\nVectorIndexer\nfrom\npyspark.ml.evaluation\nimport\nMulticlassClassificationEvaluator\n# Load and parse the data file, converting it to a DataFrame.\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer\n=\nStringIndexer\n(\ninputCol\n=\n\"\nlabel\n\"\n,\noutputCol\n=\n\"\nindexedLabel\n\"\n).\nfit\n(\ndata\n)\n# Automatically identify categorical features, and index them.\n# Set maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer\n=\n\\\nVectorIndexer\n(\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nindexedFeatures\n\"\n,\nmaxCategories\n=\n4\n).\nfit\n(\ndata\n)\n# Split the data into training and test sets (30% held out for testing)\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n([\n0.7\n,\n0.3\n])\n# Train a RandomForest model.\nrf\n=\nRandomForestClassifier\n(\nlabelCol\n=\n\"\nindexedLabel\n\"\n,\nfeaturesCol\n=\n\"\nindexedFeatures\n\"\n,\nnumTrees\n=\n10\n)\n# Convert indexed labels back to original labels.\nlabelConverter\n=\nIndexToString\n(\ninputCol\n=\n\"\nprediction\n\"\n,\noutputCol\n=\n\"\npredictedLabel\n\"\n,\nlabels\n=\nlabelIndexer\n.\nlabels\n)\n# Chain indexers and forest in a Pipeline\npipeline\n=\nPipeline\n(\nstages\n=\n[\nlabelIndexer\n,\nfeatureIndexer\n,\nrf\n,\nlabelConverter\n])\n# Train model.  This also runs the indexers.\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n)\n# Make predictions.\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n)\n# Select example rows to display.\npredictions\n.\nselect\n(\n\"\npredictedLabel\n\"\n,\n\"\nlabel\n\"\n,\n\"\nfeatures\n\"\n).\nshow\n(\n5\n)\n# Select (prediction, true label) and compute test error\nevaluator\n=\nMulticlassClassificationEvaluator\n(\nlabelCol\n=\n\"\nindexedLabel\n\"\n,\npredictionCol\n=\n\"\nprediction\n\"\n,\nmetricName\n=\n\"\naccuracy\n\"\n)\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprint\n(\n\"\nTest Error = %g\n\"\n%\n(\n1.0\n-\naccuracy\n))\nrfModel\n=\nmodel\n.\nstages\n[\n2\n]\nprint\n(\nrfModel\n)\n# summary only\nFind full example code at \"examples/src/main/python/ml/random_forest_classifier_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.Pipeline\nimport\norg.apache.spark.ml.classification.\n{\nRandomForestClassificationModel\n,\nRandomForestClassifier\n}\nimport\norg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport\norg.apache.spark.ml.feature.\n{\nIndexToString\n,\nStringIndexer\n,\nVectorIndexer\n}\n// Load and parse the data file, converting it to a DataFrame.\nval\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Index labels, adding metadata to the label column.\n// Fit on whole dataset to include all labels in index.\nval\nlabelIndexer\n=\nnew\nStringIndexer\n()\n.\nsetInputCol\n(\n\"label\"\n)\n.\nsetOutputCol\n(\n\"indexedLabel\"\n)\n.\nfit\n(\ndata\n)\n// Automatically identify categorical features, and index them.\n// Set maxCategories so features with > 4 distinct values are treated as continuous.\nval\nfeatureIndexer\n=\nnew\nVectorIndexer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxCategories\n(\n4\n)\n.\nfit\n(\ndata\n)\n// Split the data into training and test sets (30% held out for testing).\nval\nArray\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.7\n,\n0.3\n))\n// Train a RandomForest model.\nval\nrf\n=\nnew\nRandomForestClassifier\n()\n.\nsetLabelCol\n(\n\"indexedLabel\"\n)\n.\nsetFeaturesCol\n(\n\"indexedFeatures\"\n)\n.\nsetNumTrees\n(\n10\n)\n// Convert indexed labels back to original labels.\nval\nlabelConverter\n=\nnew\nIndexToString\n()\n.\nsetInputCol\n(\n\"prediction\"\n)\n.\nsetOutputCol\n(\n\"predictedLabel\"\n)\n.\nsetLabels\n(\nlabelIndexer\n.\nlabelsArray\n(\n0\n))\n// Chain indexers and forest in a Pipeline.\nval\npipeline\n=\nnew\nPipeline\n()\n.\nsetStages\n(\nArray\n(\nlabelIndexer\n,\nfeatureIndexer\n,\nrf\n,\nlabelConverter\n))\n// Train model. This also runs the indexers.\nval\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n)\n// Make predictions.\nval\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n)\n// Select example rows to display.\npredictions\n.\nselect\n(\n\"predictedLabel\"\n,\n\"label\"\n,\n\"features\"\n).\nshow\n(\n5\n)\n// Select (prediction, true label) and compute test error.\nval\nevaluator\n=\nnew\nMulticlassClassificationEvaluator\n()\n.\nsetLabelCol\n(\n\"indexedLabel\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"accuracy\"\n)\nval\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprintln\n(\ns\n\"Test Error = ${(1.0 - accuracy)}\"\n)\nval\nrfModel\n=\nmodel\n.\nstages\n(\n2\n).\nasInstanceOf\n[\nRandomForestClassificationModel\n]\nprintln\n(\ns\n\"Learned classification forest model:\\n ${rfModel.toDebugString}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.ml.Pipeline\n;\nimport\norg.apache.spark.ml.PipelineModel\n;\nimport\norg.apache.spark.ml.PipelineStage\n;\nimport\norg.apache.spark.ml.classification.RandomForestClassificationModel\n;\nimport\norg.apache.spark.ml.classification.RandomForestClassifier\n;\nimport\norg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n;\nimport\norg.apache.spark.ml.feature.*\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Load and parse the data file, converting it to a DataFrame.\nDataset\n<\nRow\n>\ndata\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n);\n// Index labels, adding metadata to the label column.\n// Fit on whole dataset to include all labels in index.\nStringIndexerModel\nlabelIndexer\n=\nnew\nStringIndexer\n()\n.\nsetInputCol\n(\n\"label\"\n)\n.\nsetOutputCol\n(\n\"indexedLabel\"\n)\n.\nfit\n(\ndata\n);\n// Automatically identify categorical features, and index them.\n// Set maxCategories so features with > 4 distinct values are treated as continuous.\nVectorIndexerModel\nfeatureIndexer\n=\nnew\nVectorIndexer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxCategories\n(\n4\n)\n.\nfit\n(\ndata\n);\n// Split the data into training and test sets (30% held out for testing)\nDataset\n<\nRow\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]\n{\n0.7\n,\n0.3\n});\nDataset\n<\nRow\n>\ntrainingData\n=\nsplits\n[\n0\n];\nDataset\n<\nRow\n>\ntestData\n=\nsplits\n[\n1\n];\n// Train a RandomForest model.\nRandomForestClassifier\nrf\n=\nnew\nRandomForestClassifier\n()\n.\nsetLabelCol\n(\n\"indexedLabel\"\n)\n.\nsetFeaturesCol\n(\n\"indexedFeatures\"\n);\n// Convert indexed labels back to original labels.\nIndexToString\nlabelConverter\n=\nnew\nIndexToString\n()\n.\nsetInputCol\n(\n\"prediction\"\n)\n.\nsetOutputCol\n(\n\"predictedLabel\"\n)\n.\nsetLabels\n(\nlabelIndexer\n.\nlabelsArray\n()[\n0\n]);\n// Chain indexers and forest in a Pipeline\nPipeline\npipeline\n=\nnew\nPipeline\n()\n.\nsetStages\n(\nnew\nPipelineStage\n[]\n{\nlabelIndexer\n,\nfeatureIndexer\n,\nrf\n,\nlabelConverter\n});\n// Train model. This also runs the indexers.\nPipelineModel\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n);\n// Make predictions.\nDataset\n<\nRow\n>\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n);\n// Select example rows to display.\npredictions\n.\nselect\n(\n\"predictedLabel\"\n,\n\"label\"\n,\n\"features\"\n).\nshow\n(\n5\n);\n// Select (prediction, true label) and compute test error\nMulticlassClassificationEvaluator\nevaluator\n=\nnew\nMulticlassClassificationEvaluator\n()\n.\nsetLabelCol\n(\n\"indexedLabel\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"accuracy\"\n);\ndouble\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Test Error = \"\n+\n(\n1.0\n-\naccuracy\n));\nRandomForestClassificationModel\nrfModel\n=\n(\nRandomForestClassificationModel\n)(\nmodel\n.\nstages\n()[\n2\n]);\nSystem\n.\nout\n.\nprintln\n(\n\"Learned classification forest model:\\n\"\n+\nrfModel\n.\ntoDebugString\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_libsvm_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit a random forest classification model with spark.randomForest\nmodel\n<-\nspark.randomForest\n(\ntraining\n,\nlabel\n~\nfeatures\n,\n\"classification\"\n,\nnumTrees\n=\n10\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\nFind full example code at \"examples/src/main/r/ml/randomForest.R\" in the Spark repo.\nGradient-boosted tree classifier\nGradient-boosted trees (GBTs) are a popular classification and regression method using ensembles of decision trees.\nMore information about the\nspark.ml\nimplementation can be found further in the\nsection on GBTs\n.\nExamples\nThe following examples load a dataset in LibSVM format, split it into training and test sets, train on the first dataset, and then evaluate on the held-out test set.\nWe use two feature transformers to prepare the data; these help index categories for the label and categorical features, adding metadata to the\nDataFrame\nwhich the tree-based algorithms can recognize.\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml\nimport\nPipeline\nfrom\npyspark.ml.classification\nimport\nGBTClassifier\nfrom\npyspark.ml.feature\nimport\nStringIndexer\n,\nVectorIndexer\nfrom\npyspark.ml.evaluation\nimport\nMulticlassClassificationEvaluator\n# Load and parse the data file, converting it to a DataFrame.\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer\n=\nStringIndexer\n(\ninputCol\n=\n\"\nlabel\n\"\n,\noutputCol\n=\n\"\nindexedLabel\n\"\n).\nfit\n(\ndata\n)\n# Automatically identify categorical features, and index them.\n# Set maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer\n=\n\\\nVectorIndexer\n(\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nindexedFeatures\n\"\n,\nmaxCategories\n=\n4\n).\nfit\n(\ndata\n)\n# Split the data into training and test sets (30% held out for testing)\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n([\n0.7\n,\n0.3\n])\n# Train a GBT model.\ngbt\n=\nGBTClassifier\n(\nlabelCol\n=\n\"\nindexedLabel\n\"\n,\nfeaturesCol\n=\n\"\nindexedFeatures\n\"\n,\nmaxIter\n=\n10\n)\n# Chain indexers and GBT in a Pipeline\npipeline\n=\nPipeline\n(\nstages\n=\n[\nlabelIndexer\n,\nfeatureIndexer\n,\ngbt\n])\n# Train model.  This also runs the indexers.\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n)\n# Make predictions.\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n)\n# Select example rows to display.\npredictions\n.\nselect\n(\n\"\nprediction\n\"\n,\n\"\nindexedLabel\n\"\n,\n\"\nfeatures\n\"\n).\nshow\n(\n5\n)\n# Select (prediction, true label) and compute test error\nevaluator\n=\nMulticlassClassificationEvaluator\n(\nlabelCol\n=\n\"\nindexedLabel\n\"\n,\npredictionCol\n=\n\"\nprediction\n\"\n,\nmetricName\n=\n\"\naccuracy\n\"\n)\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprint\n(\n\"\nTest Error = %g\n\"\n%\n(\n1.0\n-\naccuracy\n))\ngbtModel\n=\nmodel\n.\nstages\n[\n2\n]\nprint\n(\ngbtModel\n)\n# summary only\nFind full example code at \"examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.Pipeline\nimport\norg.apache.spark.ml.classification.\n{\nGBTClassificationModel\n,\nGBTClassifier\n}\nimport\norg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport\norg.apache.spark.ml.feature.\n{\nIndexToString\n,\nStringIndexer\n,\nVectorIndexer\n}\n// Load and parse the data file, converting it to a DataFrame.\nval\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Index labels, adding metadata to the label column.\n// Fit on whole dataset to include all labels in index.\nval\nlabelIndexer\n=\nnew\nStringIndexer\n()\n.\nsetInputCol\n(\n\"label\"\n)\n.\nsetOutputCol\n(\n\"indexedLabel\"\n)\n.\nfit\n(\ndata\n)\n// Automatically identify categorical features, and index them.\n// Set maxCategories so features with > 4 distinct values are treated as continuous.\nval\nfeatureIndexer\n=\nnew\nVectorIndexer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxCategories\n(\n4\n)\n.\nfit\n(\ndata\n)\n// Split the data into training and test sets (30% held out for testing).\nval\nArray\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.7\n,\n0.3\n))\n// Train a GBT model.\nval\ngbt\n=\nnew\nGBTClassifier\n()\n.\nsetLabelCol\n(\n\"indexedLabel\"\n)\n.\nsetFeaturesCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxIter\n(\n10\n)\n.\nsetFeatureSubsetStrategy\n(\n\"auto\"\n)\n// Convert indexed labels back to original labels.\nval\nlabelConverter\n=\nnew\nIndexToString\n()\n.\nsetInputCol\n(\n\"prediction\"\n)\n.\nsetOutputCol\n(\n\"predictedLabel\"\n)\n.\nsetLabels\n(\nlabelIndexer\n.\nlabelsArray\n(\n0\n))\n// Chain indexers and GBT in a Pipeline.\nval\npipeline\n=\nnew\nPipeline\n()\n.\nsetStages\n(\nArray\n(\nlabelIndexer\n,\nfeatureIndexer\n,\ngbt\n,\nlabelConverter\n))\n// Train model. This also runs the indexers.\nval\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n)\n// Make predictions.\nval\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n)\n// Select example rows to display.\npredictions\n.\nselect\n(\n\"predictedLabel\"\n,\n\"label\"\n,\n\"features\"\n).\nshow\n(\n5\n)\n// Select (prediction, true label) and compute test error.\nval\nevaluator\n=\nnew\nMulticlassClassificationEvaluator\n()\n.\nsetLabelCol\n(\n\"indexedLabel\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"accuracy\"\n)\nval\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprintln\n(\ns\n\"Test Error = ${1.0 - accuracy}\"\n)\nval\ngbtModel\n=\nmodel\n.\nstages\n(\n2\n).\nasInstanceOf\n[\nGBTClassificationModel\n]\nprintln\n(\ns\n\"Learned classification GBT model:\\n ${gbtModel.toDebugString}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.ml.Pipeline\n;\nimport\norg.apache.spark.ml.PipelineModel\n;\nimport\norg.apache.spark.ml.PipelineStage\n;\nimport\norg.apache.spark.ml.classification.GBTClassificationModel\n;\nimport\norg.apache.spark.ml.classification.GBTClassifier\n;\nimport\norg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n;\nimport\norg.apache.spark.ml.feature.*\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Load and parse the data file, converting it to a DataFrame.\nDataset\n<\nRow\n>\ndata\n=\nspark\n.\nread\n()\n.\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n);\n// Index labels, adding metadata to the label column.\n// Fit on whole dataset to include all labels in index.\nStringIndexerModel\nlabelIndexer\n=\nnew\nStringIndexer\n()\n.\nsetInputCol\n(\n\"label\"\n)\n.\nsetOutputCol\n(\n\"indexedLabel\"\n)\n.\nfit\n(\ndata\n);\n// Automatically identify categorical features, and index them.\n// Set maxCategories so features with > 4 distinct values are treated as continuous.\nVectorIndexerModel\nfeatureIndexer\n=\nnew\nVectorIndexer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxCategories\n(\n4\n)\n.\nfit\n(\ndata\n);\n// Split the data into training and test sets (30% held out for testing)\nDataset\n<\nRow\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]\n{\n0.7\n,\n0.3\n});\nDataset\n<\nRow\n>\ntrainingData\n=\nsplits\n[\n0\n];\nDataset\n<\nRow\n>\ntestData\n=\nsplits\n[\n1\n];\n// Train a GBT model.\nGBTClassifier\ngbt\n=\nnew\nGBTClassifier\n()\n.\nsetLabelCol\n(\n\"indexedLabel\"\n)\n.\nsetFeaturesCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxIter\n(\n10\n);\n// Convert indexed labels back to original labels.\nIndexToString\nlabelConverter\n=\nnew\nIndexToString\n()\n.\nsetInputCol\n(\n\"prediction\"\n)\n.\nsetOutputCol\n(\n\"predictedLabel\"\n)\n.\nsetLabels\n(\nlabelIndexer\n.\nlabelsArray\n()[\n0\n]);\n// Chain indexers and GBT in a Pipeline.\nPipeline\npipeline\n=\nnew\nPipeline\n()\n.\nsetStages\n(\nnew\nPipelineStage\n[]\n{\nlabelIndexer\n,\nfeatureIndexer\n,\ngbt\n,\nlabelConverter\n});\n// Train model. This also runs the indexers.\nPipelineModel\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n);\n// Make predictions.\nDataset\n<\nRow\n>\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n);\n// Select example rows to display.\npredictions\n.\nselect\n(\n\"predictedLabel\"\n,\n\"label\"\n,\n\"features\"\n).\nshow\n(\n5\n);\n// Select (prediction, true label) and compute test error.\nMulticlassClassificationEvaluator\nevaluator\n=\nnew\nMulticlassClassificationEvaluator\n()\n.\nsetLabelCol\n(\n\"indexedLabel\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"accuracy\"\n);\ndouble\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Test Error = \"\n+\n(\n1.0\n-\naccuracy\n));\nGBTClassificationModel\ngbtModel\n=\n(\nGBTClassificationModel\n)(\nmodel\n.\nstages\n()[\n2\n]);\nSystem\n.\nout\n.\nprintln\n(\n\"Learned classification GBT model:\\n\"\n+\ngbtModel\n.\ntoDebugString\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_libsvm_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit a GBT classification model with spark.gbt\nmodel\n<-\nspark.gbt\n(\ntraining\n,\nlabel\n~\nfeatures\n,\n\"classification\"\n,\nmaxIter\n=\n10\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\nFind full example code at \"examples/src/main/r/ml/gbt.R\" in the Spark repo.\nMultilayer perceptron classifier\nMultilayer perceptron classifier (MLPC) is a classifier based on the\nfeedforward artificial neural network\n.\nMLPC consists of multiple layers of nodes.\nEach layer is fully connected to the next layer in the network. Nodes in the input layer represent the input data. All other nodes map inputs to outputs\nby a linear combination of the inputs with the node’s weights\n$\\wv$\nand bias\n$\\bv$\nand applying an activation function.\nThis can be written in matrix form for MLPC with\n$K+1$\nlayers as follows:\n\\[\n\\mathrm{y}(\\x) = \\mathrm{f_K}(...\\mathrm{f_2}(\\wv_2^T\\mathrm{f_1}(\\wv_1^T \\x+b_1)+b_2)...+b_K)\n\\]\nNodes in intermediate layers use sigmoid (logistic) function:\n\\[\n\\mathrm{f}(z_i) = \\frac{1}{1 + e^{-z_i}}\n\\]\nNodes in the output layer use softmax function:\n\\[\n\\mathrm{f}(z_i) = \\frac{e^{z_i}}{\\sum_{k=1}^N e^{z_k}}\n\\]\nThe number of nodes\n$N$\nin the output layer corresponds to the number of classes.\nMLPC employs backpropagation for learning the model. We use the logistic loss function for optimization and L-BFGS as an optimization routine.\nExamples\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml.classification\nimport\nMultilayerPerceptronClassifier\nfrom\npyspark.ml.evaluation\nimport\nMulticlassClassificationEvaluator\n# Load training data\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n)\n\\\n.\nload\n(\n\"\ndata/mllib/sample_multiclass_classification_data.txt\n\"\n)\n# Split the data into train and test\nsplits\n=\ndata\n.\nrandomSplit\n([\n0.6\n,\n0.4\n],\n1234\n)\ntrain\n=\nsplits\n[\n0\n]\ntest\n=\nsplits\n[\n1\n]\n# specify layers for the neural network:\n# input layer of size 4 (features), two intermediate of size 5 and 4\n# and output of size 3 (classes)\nlayers\n=\n[\n4\n,\n5\n,\n4\n,\n3\n]\n# create the trainer and set its parameters\ntrainer\n=\nMultilayerPerceptronClassifier\n(\nmaxIter\n=\n100\n,\nlayers\n=\nlayers\n,\nblockSize\n=\n128\n,\nseed\n=\n1234\n)\n# train the model\nmodel\n=\ntrainer\n.\nfit\n(\ntrain\n)\n# compute accuracy on the test set\nresult\n=\nmodel\n.\ntransform\n(\ntest\n)\npredictionAndLabels\n=\nresult\n.\nselect\n(\n\"\nprediction\n\"\n,\n\"\nlabel\n\"\n)\nevaluator\n=\nMulticlassClassificationEvaluator\n(\nmetricName\n=\n\"\naccuracy\n\"\n)\nprint\n(\n\"\nTest set accuracy =\n\"\n+\nstr\n(\nevaluator\n.\nevaluate\n(\npredictionAndLabels\n)))\nFind full example code at \"examples/src/main/python/ml/multilayer_perceptron_classification.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.classification.MultilayerPerceptronClassifier\nimport\norg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n// Load the data stored in LIBSVM format as a DataFrame.\nval\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_multiclass_classification_data.txt\"\n)\n// Split the data into train and test\nval\nsplits\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.6\n,\n0.4\n),\nseed\n=\n1234L\n)\nval\ntrain\n=\nsplits\n(\n0\n)\nval\ntest\n=\nsplits\n(\n1\n)\n// specify layers for the neural network:\n// input layer of size 4 (features), two intermediate of size 5 and 4\n// and output of size 3 (classes)\nval\nlayers\n=\nArray\n[\nInt\n](\n4\n,\n5\n,\n4\n,\n3\n)\n// create the trainer and set its parameters\nval\ntrainer\n=\nnew\nMultilayerPerceptronClassifier\n()\n.\nsetLayers\n(\nlayers\n)\n.\nsetBlockSize\n(\n128\n)\n.\nsetSeed\n(\n1234L\n)\n.\nsetMaxIter\n(\n100\n)\n// train the model\nval\nmodel\n=\ntrainer\n.\nfit\n(\ntrain\n)\n// compute accuracy on the test set\nval\nresult\n=\nmodel\n.\ntransform\n(\ntest\n)\nval\npredictionAndLabels\n=\nresult\n.\nselect\n(\n\"prediction\"\n,\n\"label\"\n)\nval\nevaluator\n=\nnew\nMulticlassClassificationEvaluator\n()\n.\nsetMetricName\n(\n\"accuracy\"\n)\nprintln\n(\ns\n\"Test set accuracy = ${evaluator.evaluate(predictionAndLabels)}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\nimport\norg.apache.spark.ml.classification.MultilayerPerceptronClassificationModel\n;\nimport\norg.apache.spark.ml.classification.MultilayerPerceptronClassifier\n;\nimport\norg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n;\n// Load training data\nString\npath\n=\n\"data/mllib/sample_multiclass_classification_data.txt\"\n;\nDataset\n<\nRow\n>\ndataFrame\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n).\nload\n(\npath\n);\n// Split the data into train and test\nDataset\n<\nRow\n>[]\nsplits\n=\ndataFrame\n.\nrandomSplit\n(\nnew\ndouble\n[]{\n0.6\n,\n0.4\n},\n1234L\n);\nDataset\n<\nRow\n>\ntrain\n=\nsplits\n[\n0\n];\nDataset\n<\nRow\n>\ntest\n=\nsplits\n[\n1\n];\n// specify layers for the neural network:\n// input layer of size 4 (features), two intermediate of size 5 and 4\n// and output of size 3 (classes)\nint\n[]\nlayers\n=\nnew\nint\n[]\n{\n4\n,\n5\n,\n4\n,\n3\n};\n// create the trainer and set its parameters\nMultilayerPerceptronClassifier\ntrainer\n=\nnew\nMultilayerPerceptronClassifier\n()\n.\nsetLayers\n(\nlayers\n)\n.\nsetBlockSize\n(\n128\n)\n.\nsetSeed\n(\n1234L\n)\n.\nsetMaxIter\n(\n100\n);\n// train the model\nMultilayerPerceptronClassificationModel\nmodel\n=\ntrainer\n.\nfit\n(\ntrain\n);\n// compute accuracy on the test set\nDataset\n<\nRow\n>\nresult\n=\nmodel\n.\ntransform\n(\ntest\n);\nDataset\n<\nRow\n>\npredictionAndLabels\n=\nresult\n.\nselect\n(\n\"prediction\"\n,\n\"label\"\n);\nMulticlassClassificationEvaluator\nevaluator\n=\nnew\nMulticlassClassificationEvaluator\n()\n.\nsetMetricName\n(\n\"accuracy\"\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Test set accuracy = \"\n+\nevaluator\n.\nevaluate\n(\npredictionAndLabels\n));\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_multiclass_classification_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# specify layers for the neural network:\n# input layer of size 4 (features), two intermediate of size 5 and 4\n# and output of size 3 (classes)\nlayers\n=\nc\n(\n4\n,\n5\n,\n4\n,\n3\n)\n# Fit a multi-layer perceptron neural network model with spark.mlp\nmodel\n<-\nspark.mlp\n(\ntraining\n,\nlabel\n~\nfeatures\n,\nmaxIter\n=\n100\n,\nlayers\n=\nlayers\n,\nblockSize\n=\n128\n,\nseed\n=\n1234\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\nFind full example code at \"examples/src/main/r/ml/mlp.R\" in the Spark repo.\nLinear Support Vector Machine\nA\nsupport vector machine\nconstructs a hyperplane\nor set of hyperplanes in a high- or infinite-dimensional space, which can be used for classification,\nregression, or other tasks. Intuitively, a good separation is achieved by the hyperplane that has\nthe largest distance to the nearest training-data points of any class (so-called functional margin),\nsince in general the larger the margin the lower the generalization error of the classifier. LinearSVC\nin Spark ML supports binary classification with linear SVM. Internally, it optimizes the\nHinge Loss\nusing OWLQN optimizer.\nExamples\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml.classification\nimport\nLinearSVC\n# Load training data\ntraining\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\nlsvc\n=\nLinearSVC\n(\nmaxIter\n=\n10\n,\nregParam\n=\n0.1\n)\n# Fit the model\nlsvcModel\n=\nlsvc\n.\nfit\n(\ntraining\n)\n# Print the coefficients and intercept for linear SVC\nprint\n(\n\"\nCoefficients:\n\"\n+\nstr\n(\nlsvcModel\n.\ncoefficients\n))\nprint\n(\n\"\nIntercept:\n\"\n+\nstr\n(\nlsvcModel\n.\nintercept\n))\nFind full example code at \"examples/src/main/python/ml/linearsvc.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.classification.LinearSVC\n// Load training data\nval\ntraining\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\nval\nlsvc\n=\nnew\nLinearSVC\n()\n.\nsetMaxIter\n(\n10\n)\n.\nsetRegParam\n(\n0.1\n)\n// Fit the model\nval\nlsvcModel\n=\nlsvc\n.\nfit\n(\ntraining\n)\n// Print the coefficients and intercept for linear svc\nprintln\n(\ns\n\"Coefficients: ${lsvcModel.coefficients} Intercept: ${lsvcModel.intercept}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.ml.classification.LinearSVC\n;\nimport\norg.apache.spark.ml.classification.LinearSVCModel\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Load training data\nDataset\n<\nRow\n>\ntraining\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n);\nLinearSVC\nlsvc\n=\nnew\nLinearSVC\n()\n.\nsetMaxIter\n(\n10\n)\n.\nsetRegParam\n(\n0.1\n);\n// Fit the model\nLinearSVCModel\nlsvcModel\n=\nlsvc\n.\nfit\n(\ntraining\n);\n// Print the coefficients and intercept for LinearSVC\nSystem\n.\nout\n.\nprintln\n(\n\"Coefficients: \"\n+\nlsvcModel\n.\ncoefficients\n()\n+\n\" Intercept: \"\n+\nlsvcModel\n.\nintercept\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# load training data\nt\n<-\nas.data.frame\n(\nTitanic\n)\ntraining\n<-\ncreateDataFrame\n(\nt\n)\n# fit Linear SVM model\nmodel\n<-\nspark.svmLinear\n(\ntraining\n,\nSurvived\n~\n.\n,\nregParam\n=\n0.01\n,\nmaxIter\n=\n10\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\nprediction\n<-\npredict\n(\nmodel\n,\ntraining\n)\nshowDF\n(\nprediction\n)\nFind full example code at \"examples/src/main/r/ml/svmLinear.R\" in the Spark repo.\nOne-vs-Rest classifier (a.k.a. One-vs-All)\nOneVsRest\nis an example of a machine learning reduction for performing multiclass classification given a base classifier that can perform binary classification efficiently.  It is also known as “One-vs-All.”\nOneVsRest\nis implemented as an\nEstimator\n. For the base classifier, it takes instances of\nClassifier\nand creates a binary classification problem for each of the k classes. The classifier for class i is trained to predict whether the label is i or not, distinguishing class i from all other classes.\nPredictions are done by evaluating each binary classifier and the index of the most confident classifier is output as label.\nExamples\nThe example below demonstrates how to load the\nIris dataset\n, parse it as a DataFrame and perform multiclass classification using\nOneVsRest\n. The test error is calculated to measure the algorithm accuracy.\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml.classification\nimport\nLogisticRegression\n,\nOneVsRest\nfrom\npyspark.ml.evaluation\nimport\nMulticlassClassificationEvaluator\n# load data file.\ninputData\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n)\n\\\n.\nload\n(\n\"\ndata/mllib/sample_multiclass_classification_data.txt\n\"\n)\n# generate the train/test split.\n(\ntrain\n,\ntest\n)\n=\ninputData\n.\nrandomSplit\n([\n0.8\n,\n0.2\n])\n# instantiate the base classifier.\nlr\n=\nLogisticRegression\n(\nmaxIter\n=\n10\n,\ntol\n=\n1E-6\n,\nfitIntercept\n=\nTrue\n)\n# instantiate the One Vs Rest Classifier.\novr\n=\nOneVsRest\n(\nclassifier\n=\nlr\n)\n# train the multiclass model.\novrModel\n=\novr\n.\nfit\n(\ntrain\n)\n# score the model on test data.\npredictions\n=\novrModel\n.\ntransform\n(\ntest\n)\n# obtain evaluator.\nevaluator\n=\nMulticlassClassificationEvaluator\n(\nmetricName\n=\n\"\naccuracy\n\"\n)\n# compute the classification error on test data.\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprint\n(\n\"\nTest Error = %g\n\"\n%\n(\n1.0\n-\naccuracy\n))\nFind full example code at \"examples/src/main/python/ml/one_vs_rest_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.classification.\n{\nLogisticRegression\n,\nOneVsRest\n}\nimport\norg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n// load data file.\nval\ninputData\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_multiclass_classification_data.txt\"\n)\n// generate the train/test split.\nval\nArray\n(\ntrain\n,\ntest\n)\n=\ninputData\n.\nrandomSplit\n(\nArray\n(\n0.8\n,\n0.2\n))\n// instantiate the base classifier\nval\nclassifier\n=\nnew\nLogisticRegression\n()\n.\nsetMaxIter\n(\n10\n)\n.\nsetTol\n(\n1\nE\n-\n6\n)\n.\nsetFitIntercept\n(\ntrue\n)\n// instantiate the One Vs Rest Classifier.\nval\novr\n=\nnew\nOneVsRest\n().\nsetClassifier\n(\nclassifier\n)\n// train the multiclass model.\nval\novrModel\n=\novr\n.\nfit\n(\ntrain\n)\n// score the model on test data.\nval\npredictions\n=\novrModel\n.\ntransform\n(\ntest\n)\n// obtain evaluator.\nval\nevaluator\n=\nnew\nMulticlassClassificationEvaluator\n()\n.\nsetMetricName\n(\n\"accuracy\"\n)\n// compute the classification error on test data.\nval\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprintln\n(\ns\n\"Test Error = ${1 - accuracy}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.ml.classification.LogisticRegression\n;\nimport\norg.apache.spark.ml.classification.OneVsRest\n;\nimport\norg.apache.spark.ml.classification.OneVsRestModel\n;\nimport\norg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\n// load data file.\nDataset\n<\nRow\n>\ninputData\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_multiclass_classification_data.txt\"\n);\n// generate the train/test split.\nDataset\n<\nRow\n>[]\ntmp\n=\ninputData\n.\nrandomSplit\n(\nnew\ndouble\n[]{\n0.8\n,\n0.2\n});\nDataset\n<\nRow\n>\ntrain\n=\ntmp\n[\n0\n];\nDataset\n<\nRow\n>\ntest\n=\ntmp\n[\n1\n];\n// configure the base classifier.\nLogisticRegression\nclassifier\n=\nnew\nLogisticRegression\n()\n.\nsetMaxIter\n(\n10\n)\n.\nsetTol\n(\n1\nE\n-\n6\n)\n.\nsetFitIntercept\n(\ntrue\n);\n// instantiate the One Vs Rest Classifier.\nOneVsRest\novr\n=\nnew\nOneVsRest\n().\nsetClassifier\n(\nclassifier\n);\n// train the multiclass model.\nOneVsRestModel\novrModel\n=\novr\n.\nfit\n(\ntrain\n);\n// score the model on test data.\nDataset\n<\nRow\n>\npredictions\n=\novrModel\n.\ntransform\n(\ntest\n)\n.\nselect\n(\n\"prediction\"\n,\n\"label\"\n);\n// obtain evaluator.\nMulticlassClassificationEvaluator\nevaluator\n=\nnew\nMulticlassClassificationEvaluator\n()\n.\nsetMetricName\n(\n\"accuracy\"\n);\n// compute the classification error on test data.\ndouble\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Test Error = \"\n+\n(\n1\n-\naccuracy\n));\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\" in the Spark repo.\nNaive Bayes\nNaive Bayes classifiers\nare a family of simple\nprobabilistic, multiclass classifiers based on applying Bayes’ theorem with strong (naive) independence\nassumptions between every pair of features.\nNaive Bayes can be trained very efficiently. With a single pass over the training data,\nit computes the conditional probability distribution of each feature given each label.\nFor prediction, it applies Bayes’ theorem to compute the conditional probability distribution\nof each label given an observation.\nMLlib supports\nMultinomial naive Bayes\n,\nComplement naive Bayes\n,\nBernoulli naive Bayes\nand\nGaussian naive Bayes\n.\nInput data\n:\nThese Multinomial, Complement and Bernoulli models are typically used for\ndocument classification\n.\nWithin that context, each observation is a document and each feature represents a term.\nA feature’s value is the frequency of the term (in Multinomial or Complement Naive Bayes) or\na zero or one indicating whether the term was found in the document (in Bernoulli Naive Bayes).\nFeature values for Multinomial and Bernoulli models must be\nnon-negative\n. The model type is selected with an optional parameter\n“multinomial”, “complement”, “bernoulli” or “gaussian”, with “multinomial” as the default.\nFor document classification, the input feature vectors should usually be sparse vectors.\nSince the training data is only used once, it is not necessary to cache it.\nAdditive smoothing\ncan be used by\nsetting the parameter $\\lambda$ (default to $1.0$).\nExamples\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml.classification\nimport\nNaiveBayes\nfrom\npyspark.ml.evaluation\nimport\nMulticlassClassificationEvaluator\n# Load training data\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n)\n\\\n.\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\n# Split the data into train and test\nsplits\n=\ndata\n.\nrandomSplit\n([\n0.6\n,\n0.4\n],\n1234\n)\ntrain\n=\nsplits\n[\n0\n]\ntest\n=\nsplits\n[\n1\n]\n# create the trainer and set its parameters\nnb\n=\nNaiveBayes\n(\nsmoothing\n=\n1.0\n,\nmodelType\n=\n\"\nmultinomial\n\"\n)\n# train the model\nmodel\n=\nnb\n.\nfit\n(\ntrain\n)\n# select example rows to display.\npredictions\n=\nmodel\n.\ntransform\n(\ntest\n)\npredictions\n.\nshow\n()\n# compute accuracy on the test set\nevaluator\n=\nMulticlassClassificationEvaluator\n(\nlabelCol\n=\n\"\nlabel\n\"\n,\npredictionCol\n=\n\"\nprediction\n\"\n,\nmetricName\n=\n\"\naccuracy\n\"\n)\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprint\n(\n\"\nTest set accuracy =\n\"\n+\nstr\n(\naccuracy\n))\nFind full example code at \"examples/src/main/python/ml/naive_bayes_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.classification.NaiveBayes\nimport\norg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n// Load the data stored in LIBSVM format as a DataFrame.\nval\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Split the data into training and test sets (30% held out for testing)\nval\nArray\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.7\n,\n0.3\n),\nseed\n=\n1234L\n)\n// Train a NaiveBayes model.\nval\nmodel\n=\nnew\nNaiveBayes\n()\n.\nfit\n(\ntrainingData\n)\n// Select example rows to display.\nval\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n)\npredictions\n.\nshow\n()\n// Select (prediction, true label) and compute test error\nval\nevaluator\n=\nnew\nMulticlassClassificationEvaluator\n()\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"accuracy\"\n)\nval\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprintln\n(\ns\n\"Test set accuracy = $accuracy\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.ml.classification.NaiveBayes\n;\nimport\norg.apache.spark.ml.classification.NaiveBayesModel\n;\nimport\norg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Load training data\nDataset\n<\nRow\n>\ndataFrame\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n);\n// Split the data into train and test\nDataset\n<\nRow\n>[]\nsplits\n=\ndataFrame\n.\nrandomSplit\n(\nnew\ndouble\n[]{\n0.6\n,\n0.4\n},\n1234L\n);\nDataset\n<\nRow\n>\ntrain\n=\nsplits\n[\n0\n];\nDataset\n<\nRow\n>\ntest\n=\nsplits\n[\n1\n];\n// create the trainer and set its parameters\nNaiveBayes\nnb\n=\nnew\nNaiveBayes\n();\n// train the model\nNaiveBayesModel\nmodel\n=\nnb\n.\nfit\n(\ntrain\n);\n// Select example rows to display.\nDataset\n<\nRow\n>\npredictions\n=\nmodel\n.\ntransform\n(\ntest\n);\npredictions\n.\nshow\n();\n// compute accuracy on the test set\nMulticlassClassificationEvaluator\nevaluator\n=\nnew\nMulticlassClassificationEvaluator\n()\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"accuracy\"\n);\ndouble\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Test set accuracy = \"\n+\naccuracy\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# Fit a Bernoulli naive Bayes model with spark.naiveBayes\ntitanic\n<-\nas.data.frame\n(\nTitanic\n)\ntitanicDF\n<-\ncreateDataFrame\n(\ntitanic\n[\ntitanic\n$\nFreq\n>\n0\n,\n-5\n])\nnbDF\n<-\ntitanicDF\nnbTestDF\n<-\ntitanicDF\nnbModel\n<-\nspark.naiveBayes\n(\nnbDF\n,\nSurvived\n~\nClass\n+\nSex\n+\nAge\n)\n# Model summary\nsummary\n(\nnbModel\n)\n# Prediction\nnbPredictions\n<-\npredict\n(\nnbModel\n,\nnbTestDF\n)\nhead\n(\nnbPredictions\n)\nFind full example code at \"examples/src/main/r/ml/naiveBayes.R\" in the Spark repo.\nFactorization machines classifier\nFor more background and more details about the implementation of factorization machines,\nrefer to the\nFactorization Machines section\n.\nExamples\nThe following examples load a dataset in LibSVM format, split it into training and test sets,\ntrain on the first dataset, and then evaluate on the held-out test set.\nWe scale features to be between 0 and 1 to prevent the exploding gradient problem.\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml\nimport\nPipeline\nfrom\npyspark.ml.classification\nimport\nFMClassifier\nfrom\npyspark.ml.feature\nimport\nMinMaxScaler\n,\nStringIndexer\nfrom\npyspark.ml.evaluation\nimport\nMulticlassClassificationEvaluator\n# Load and parse the data file, converting it to a DataFrame.\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer\n=\nStringIndexer\n(\ninputCol\n=\n\"\nlabel\n\"\n,\noutputCol\n=\n\"\nindexedLabel\n\"\n).\nfit\n(\ndata\n)\n# Scale features.\nfeatureScaler\n=\nMinMaxScaler\n(\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nscaledFeatures\n\"\n).\nfit\n(\ndata\n)\n# Split the data into training and test sets (30% held out for testing)\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n([\n0.7\n,\n0.3\n])\n# Train a FM model.\nfm\n=\nFMClassifier\n(\nlabelCol\n=\n\"\nindexedLabel\n\"\n,\nfeaturesCol\n=\n\"\nscaledFeatures\n\"\n,\nstepSize\n=\n0.001\n)\n# Create a Pipeline.\npipeline\n=\nPipeline\n(\nstages\n=\n[\nlabelIndexer\n,\nfeatureScaler\n,\nfm\n])\n# Train model.\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n)\n# Make predictions.\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n)\n# Select example rows to display.\npredictions\n.\nselect\n(\n\"\nprediction\n\"\n,\n\"\nindexedLabel\n\"\n,\n\"\nfeatures\n\"\n).\nshow\n(\n5\n)\n# Select (prediction, true label) and compute test accuracy\nevaluator\n=\nMulticlassClassificationEvaluator\n(\nlabelCol\n=\n\"\nindexedLabel\n\"\n,\npredictionCol\n=\n\"\nprediction\n\"\n,\nmetricName\n=\n\"\naccuracy\n\"\n)\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprint\n(\n\"\nTest set accuracy = %g\n\"\n%\naccuracy\n)\nfmModel\n=\nmodel\n.\nstages\n[\n2\n]\nprint\n(\n\"\nFactors:\n\"\n+\nstr\n(\nfmModel\n.\nfactors\n))\n# type: ignore\nprint\n(\n\"\nLinear:\n\"\n+\nstr\n(\nfmModel\n.\nlinear\n))\n# type: ignore\nprint\n(\n\"\nIntercept:\n\"\n+\nstr\n(\nfmModel\n.\nintercept\n))\n# type: ignore\nFind full example code at \"examples/src/main/python/ml/fm_classifier_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.Pipeline\nimport\norg.apache.spark.ml.classification.\n{\nFMClassificationModel\n,\nFMClassifier\n}\nimport\norg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport\norg.apache.spark.ml.feature.\n{\nIndexToString\n,\nMinMaxScaler\n,\nStringIndexer\n}\n// Load and parse the data file, converting it to a DataFrame.\nval\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Index labels, adding metadata to the label column.\n// Fit on whole dataset to include all labels in index.\nval\nlabelIndexer\n=\nnew\nStringIndexer\n()\n.\nsetInputCol\n(\n\"label\"\n)\n.\nsetOutputCol\n(\n\"indexedLabel\"\n)\n.\nfit\n(\ndata\n)\n// Scale features.\nval\nfeatureScaler\n=\nnew\nMinMaxScaler\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"scaledFeatures\"\n)\n.\nfit\n(\ndata\n)\n// Split the data into training and test sets (30% held out for testing).\nval\nArray\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.7\n,\n0.3\n))\n// Train a FM model.\nval\nfm\n=\nnew\nFMClassifier\n()\n.\nsetLabelCol\n(\n\"indexedLabel\"\n)\n.\nsetFeaturesCol\n(\n\"scaledFeatures\"\n)\n.\nsetStepSize\n(\n0.001\n)\n// Convert indexed labels back to original labels.\nval\nlabelConverter\n=\nnew\nIndexToString\n()\n.\nsetInputCol\n(\n\"prediction\"\n)\n.\nsetOutputCol\n(\n\"predictedLabel\"\n)\n.\nsetLabels\n(\nlabelIndexer\n.\nlabelsArray\n(\n0\n))\n// Create a Pipeline.\nval\npipeline\n=\nnew\nPipeline\n()\n.\nsetStages\n(\nArray\n(\nlabelIndexer\n,\nfeatureScaler\n,\nfm\n,\nlabelConverter\n))\n// Train model.\nval\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n)\n// Make predictions.\nval\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n)\n// Select example rows to display.\npredictions\n.\nselect\n(\n\"predictedLabel\"\n,\n\"label\"\n,\n\"features\"\n).\nshow\n(\n5\n)\n// Select (prediction, true label) and compute test accuracy.\nval\nevaluator\n=\nnew\nMulticlassClassificationEvaluator\n()\n.\nsetLabelCol\n(\n\"indexedLabel\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"accuracy\"\n)\nval\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprintln\n(\ns\n\"Test set accuracy = $accuracy\"\n)\nval\nfmModel\n=\nmodel\n.\nstages\n(\n2\n).\nasInstanceOf\n[\nFMClassificationModel\n]\nprintln\n(\ns\n\"Factors: ${fmModel.factors} Linear: ${fmModel.linear} \"\n+\ns\n\"Intercept: ${fmModel.intercept}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/FMClassifierExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.ml.Pipeline\n;\nimport\norg.apache.spark.ml.PipelineModel\n;\nimport\norg.apache.spark.ml.PipelineStage\n;\nimport\norg.apache.spark.ml.classification.FMClassificationModel\n;\nimport\norg.apache.spark.ml.classification.FMClassifier\n;\nimport\norg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n;\nimport\norg.apache.spark.ml.feature.*\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Load and parse the data file, converting it to a DataFrame.\nDataset\n<\nRow\n>\ndata\n=\nspark\n.\nread\n()\n.\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n);\n// Index labels, adding metadata to the label column.\n// Fit on whole dataset to include all labels in index.\nStringIndexerModel\nlabelIndexer\n=\nnew\nStringIndexer\n()\n.\nsetInputCol\n(\n\"label\"\n)\n.\nsetOutputCol\n(\n\"indexedLabel\"\n)\n.\nfit\n(\ndata\n);\n// Scale features.\nMinMaxScalerModel\nfeatureScaler\n=\nnew\nMinMaxScaler\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"scaledFeatures\"\n)\n.\nfit\n(\ndata\n);\n// Split the data into training and test sets (30% held out for testing)\nDataset\n<\nRow\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]\n{\n0.7\n,\n0.3\n});\nDataset\n<\nRow\n>\ntrainingData\n=\nsplits\n[\n0\n];\nDataset\n<\nRow\n>\ntestData\n=\nsplits\n[\n1\n];\n// Train a FM model.\nFMClassifier\nfm\n=\nnew\nFMClassifier\n()\n.\nsetLabelCol\n(\n\"indexedLabel\"\n)\n.\nsetFeaturesCol\n(\n\"scaledFeatures\"\n)\n.\nsetStepSize\n(\n0.001\n);\n// Convert indexed labels back to original labels.\nIndexToString\nlabelConverter\n=\nnew\nIndexToString\n()\n.\nsetInputCol\n(\n\"prediction\"\n)\n.\nsetOutputCol\n(\n\"predictedLabel\"\n)\n.\nsetLabels\n(\nlabelIndexer\n.\nlabelsArray\n()[\n0\n]);\n// Create a Pipeline.\nPipeline\npipeline\n=\nnew\nPipeline\n()\n.\nsetStages\n(\nnew\nPipelineStage\n[]\n{\nlabelIndexer\n,\nfeatureScaler\n,\nfm\n,\nlabelConverter\n});\n// Train model.\nPipelineModel\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n);\n// Make predictions.\nDataset\n<\nRow\n>\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n);\n// Select example rows to display.\npredictions\n.\nselect\n(\n\"predictedLabel\"\n,\n\"label\"\n,\n\"features\"\n).\nshow\n(\n5\n);\n// Select (prediction, true label) and compute test accuracy.\nMulticlassClassificationEvaluator\nevaluator\n=\nnew\nMulticlassClassificationEvaluator\n()\n.\nsetLabelCol\n(\n\"indexedLabel\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"accuracy\"\n);\ndouble\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Test Accuracy = \"\n+\naccuracy\n);\nFMClassificationModel\nfmModel\n=\n(\nFMClassificationModel\n)(\nmodel\n.\nstages\n()[\n2\n]);\nSystem\n.\nout\n.\nprintln\n(\n\"Factors: \"\n+\nfmModel\n.\nfactors\n());\nSystem\n.\nout\n.\nprintln\n(\n\"Linear: \"\n+\nfmModel\n.\nlinear\n());\nSystem\n.\nout\n.\nprintln\n(\n\"Intercept: \"\n+\nfmModel\n.\nintercept\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaFMClassifierExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\nNote: At the moment SparkR doesn’t support feature scaling.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_libsvm_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit a FM classification model\nmodel\n<-\nspark.fmClassifier\n(\ntraining\n,\nlabel\n~\nfeatures\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\nFind full example code at \"examples/src/main/r/ml/fmClassifier.R\" in the Spark repo.\nRegression\nLinear regression\nThe interface for working with linear regression models and model\nsummaries is similar to the logistic regression case.\nWhen fitting LinearRegressionModel without intercept on dataset with constant nonzero column by “l-bfgs” solver, Spark MLlib outputs zero coefficients for constant nonzero columns. This behavior is the same as R glmnet but different from LIBSVM.\nExamples\nThe following\nexample demonstrates training an elastic net regularized linear\nregression model and extracting model summary statistics.\nMore details on parameters can be found in the\nPython API documentation\n.\nfrom\npyspark.ml.regression\nimport\nLinearRegression\n# Load training data\ntraining\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n)\n\\\n.\nload\n(\n\"\ndata/mllib/sample_linear_regression_data.txt\n\"\n)\nlr\n=\nLinearRegression\n(\nmaxIter\n=\n10\n,\nregParam\n=\n0.3\n,\nelasticNetParam\n=\n0.8\n)\n# Fit the model\nlrModel\n=\nlr\n.\nfit\n(\ntraining\n)\n# Print the coefficients and intercept for linear regression\nprint\n(\n\"\nCoefficients: %s\n\"\n%\nstr\n(\nlrModel\n.\ncoefficients\n))\nprint\n(\n\"\nIntercept: %s\n\"\n%\nstr\n(\nlrModel\n.\nintercept\n))\n# Summarize the model over the training set and print out some metrics\ntrainingSummary\n=\nlrModel\n.\nsummary\nprint\n(\n\"\nnumIterations: %d\n\"\n%\ntrainingSummary\n.\ntotalIterations\n)\nprint\n(\n\"\nobjectiveHistory: %s\n\"\n%\nstr\n(\ntrainingSummary\n.\nobjectiveHistory\n))\ntrainingSummary\n.\nresiduals\n.\nshow\n()\nprint\n(\n\"\nRMSE: %f\n\"\n%\ntrainingSummary\n.\nrootMeanSquaredError\n)\nprint\n(\n\"\nr2: %f\n\"\n%\ntrainingSummary\n.\nr2\n)\nFind full example code at \"examples/src/main/python/ml/linear_regression_with_elastic_net.py\" in the Spark repo.\nMore details on parameters can be found in the\nScala API documentation\n.\nimport\norg.apache.spark.ml.regression.LinearRegression\n// Load training data\nval\ntraining\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_linear_regression_data.txt\"\n)\nval\nlr\n=\nnew\nLinearRegression\n()\n.\nsetMaxIter\n(\n10\n)\n.\nsetRegParam\n(\n0.3\n)\n.\nsetElasticNetParam\n(\n0.8\n)\n// Fit the model\nval\nlrModel\n=\nlr\n.\nfit\n(\ntraining\n)\n// Print the coefficients and intercept for linear regression\nprintln\n(\ns\n\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\"\n)\n// Summarize the model over the training set and print out some metrics\nval\ntrainingSummary\n=\nlrModel\n.\nsummary\nprintln\n(\ns\n\"numIterations: ${trainingSummary.totalIterations}\"\n)\nprintln\n(\ns\n\"objectiveHistory: [${trainingSummary.objectiveHistory.mkString(\"\n,\n\")}]\"\n)\ntrainingSummary\n.\nresiduals\n.\nshow\n()\nprintln\n(\ns\n\"RMSE: ${trainingSummary.rootMeanSquaredError}\"\n)\nprintln\n(\ns\n\"r2: ${trainingSummary.r2}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\" in the Spark repo.\nMore details on parameters can be found in the\nJava API documentation\n.\nimport\norg.apache.spark.ml.regression.LinearRegression\n;\nimport\norg.apache.spark.ml.regression.LinearRegressionModel\n;\nimport\norg.apache.spark.ml.regression.LinearRegressionTrainingSummary\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Load training data.\nDataset\n<\nRow\n>\ntraining\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_linear_regression_data.txt\"\n);\nLinearRegression\nlr\n=\nnew\nLinearRegression\n()\n.\nsetMaxIter\n(\n10\n)\n.\nsetRegParam\n(\n0.3\n)\n.\nsetElasticNetParam\n(\n0.8\n);\n// Fit the model.\nLinearRegressionModel\nlrModel\n=\nlr\n.\nfit\n(\ntraining\n);\n// Print the coefficients and intercept for linear regression.\nSystem\n.\nout\n.\nprintln\n(\n\"Coefficients: \"\n+\nlrModel\n.\ncoefficients\n()\n+\n\" Intercept: \"\n+\nlrModel\n.\nintercept\n());\n// Summarize the model over the training set and print out some metrics.\nLinearRegressionTrainingSummary\ntrainingSummary\n=\nlrModel\n.\nsummary\n();\nSystem\n.\nout\n.\nprintln\n(\n\"numIterations: \"\n+\ntrainingSummary\n.\ntotalIterations\n());\nSystem\n.\nout\n.\nprintln\n(\n\"objectiveHistory: \"\n+\nVectors\n.\ndense\n(\ntrainingSummary\n.\nobjectiveHistory\n()));\ntrainingSummary\n.\nresiduals\n().\nshow\n();\nSystem\n.\nout\n.\nprintln\n(\n\"RMSE: \"\n+\ntrainingSummary\n.\nrootMeanSquaredError\n());\nSystem\n.\nout\n.\nprintln\n(\n\"r2: \"\n+\ntrainingSummary\n.\nr2\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\" in the Spark repo.\nMore details on parameters can be found in the\nR API documentation\n.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_linear_regression_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit a linear regression model\nmodel\n<-\nspark.lm\n(\ntraining\n,\nlabel\n~\nfeatures\n,\nregParam\n=\n0.3\n,\nelasticNetParam\n=\n0.8\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\n# Summarize\nsummary\n(\nmodel\n)\nFind full example code at \"examples/src/main/r/ml/lm_with_elastic_net.R\" in the Spark repo.\nGeneralized linear regression\nContrasted with linear regression where the output is assumed to follow a Gaussian\ndistribution,\ngeneralized linear models\n(GLMs) are specifications of linear models where the response variable $Y_i$ follows some\ndistribution from the\nexponential family of distributions\n.\nSpark’s\nGeneralizedLinearRegression\ninterface\nallows for flexible specification of GLMs which can be used for various types of\nprediction problems including linear regression, Poisson regression, logistic regression, and others.\nCurrently in\nspark.ml\n, only a subset of the exponential family distributions are supported and they are listed\nbelow\n.\nNOTE\n: Spark currently only supports up to 4096 features through its\nGeneralizedLinearRegression\ninterface, and will throw an exception if this constraint is exceeded. See the\nadvanced section\nfor more details.\n Still, for linear and logistic regression, models with an increased number of features can be trained\n using the\nLinearRegression\nand\nLogisticRegression\nestimators.\nGLMs require exponential family distributions that can be written in their “canonical” or “natural” form, aka\nnatural exponential family distributions\n. The form of a natural exponential family distribution is given as:\n\\[f_Y(y|\\theta, \\tau) = h(y, \\tau)\\exp{\\left( \\frac{\\theta \\cdot y - A(\\theta)}{d(\\tau)} \\right)}\\]\nwhere $\\theta$ is the parameter of interest and $\\tau$ is a dispersion parameter. In a GLM the response variable $Y_i$ is assumed to be drawn from a natural exponential family distribution:\n\\[Y_i \\sim f\\left(\\cdot|\\theta_i, \\tau \\right)\\]\nwhere the parameter of interest $\\theta_i$ is related to the expected value of the response variable $\\mu_i$ by\n\\[\\mu_i = A'(\\theta_i)\\]\nHere, $A’(\\theta_i)$ is defined by the form of the distribution selected. GLMs also allow specification\nof a link function, which defines the relationship between the expected value of the response variable $\\mu_i$\nand the so called\nlinear predictor\n$\\eta_i$:\n\\[g(\\mu_i) = \\eta_i = \\vec{x_i}^T \\cdot \\vec{\\beta}\\]\nOften, the link function is chosen such that $A’ = g^{-1}$, which yields a simplified relationship\nbetween the parameter of interest $\\theta$ and the linear predictor $\\eta$. In this case, the link\nfunction $g(\\mu)$ is said to be the “canonical” link function.\n\\[\\theta_i = A'^{-1}(\\mu_i) = g(g^{-1}(\\eta_i)) = \\eta_i\\]\nA GLM finds the regression coefficients $\\vec{\\beta}$ which maximize the likelihood function.\n\\[\\max_{\\vec{\\beta}} \\mathcal{L}(\\vec{\\theta}|\\vec{y},X) =\n\\prod_{i=1}^{N} h(y_i, \\tau) \\exp{\\left(\\frac{y_i\\theta_i - A(\\theta_i)}{d(\\tau)}\\right)}\\]\nwhere the parameter of interest $\\theta_i$ is related to the regression coefficients $\\vec{\\beta}$\nby\n\\[\\theta_i = A'^{-1}(g^{-1}(\\vec{x_i} \\cdot \\vec{\\beta}))\\]\nSpark’s generalized linear regression interface also provides summary statistics for diagnosing the\nfit of GLM models, including residuals, p-values, deviances, the Akaike information criterion, and\nothers.\nSee here\nfor a more comprehensive review of GLMs and their applications.\nAvailable families\nFamily\nResponse Type\nSupported Links\nGaussian\nContinuous\nIdentity*, Log, Inverse\nBinomial\nBinary\nLogit*, Probit, CLogLog\nPoisson\nCount\nLog*, Identity, Sqrt\nGamma\nContinuous\nInverse*, Identity, Log\nTweedie\nZero-inflated continuous\nPower link function\n* Canonical Link\nExamples\nThe following example demonstrates training a GLM with a Gaussian response and identity link\nfunction and extracting model summary statistics.\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml.regression\nimport\nGeneralizedLinearRegression\n# Load training data\ndataset\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n)\n\\\n.\nload\n(\n\"\ndata/mllib/sample_linear_regression_data.txt\n\"\n)\nglr\n=\nGeneralizedLinearRegression\n(\nfamily\n=\n\"\ngaussian\n\"\n,\nlink\n=\n\"\nidentity\n\"\n,\nmaxIter\n=\n10\n,\nregParam\n=\n0.3\n)\n# Fit the model\nmodel\n=\nglr\n.\nfit\n(\ndataset\n)\n# Print the coefficients and intercept for generalized linear regression model\nprint\n(\n\"\nCoefficients:\n\"\n+\nstr\n(\nmodel\n.\ncoefficients\n))\nprint\n(\n\"\nIntercept:\n\"\n+\nstr\n(\nmodel\n.\nintercept\n))\n# Summarize the model over the training set and print out some metrics\nsummary\n=\nmodel\n.\nsummary\nprint\n(\n\"\nCoefficient Standard Errors:\n\"\n+\nstr\n(\nsummary\n.\ncoefficientStandardErrors\n))\nprint\n(\n\"\nT Values:\n\"\n+\nstr\n(\nsummary\n.\ntValues\n))\nprint\n(\n\"\nP Values:\n\"\n+\nstr\n(\nsummary\n.\npValues\n))\nprint\n(\n\"\nDispersion:\n\"\n+\nstr\n(\nsummary\n.\ndispersion\n))\nprint\n(\n\"\nNull Deviance:\n\"\n+\nstr\n(\nsummary\n.\nnullDeviance\n))\nprint\n(\n\"\nResidual Degree Of Freedom Null:\n\"\n+\nstr\n(\nsummary\n.\nresidualDegreeOfFreedomNull\n))\nprint\n(\n\"\nDeviance:\n\"\n+\nstr\n(\nsummary\n.\ndeviance\n))\nprint\n(\n\"\nResidual Degree Of Freedom:\n\"\n+\nstr\n(\nsummary\n.\nresidualDegreeOfFreedom\n))\nprint\n(\n\"\nAIC:\n\"\n+\nstr\n(\nsummary\n.\naic\n))\nprint\n(\n\"\nDeviance Residuals:\n\"\n)\nsummary\n.\nresiduals\n().\nshow\n()\nFind full example code at \"examples/src/main/python/ml/generalized_linear_regression_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.regression.GeneralizedLinearRegression\n// Load training data\nval\ndataset\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_linear_regression_data.txt\"\n)\nval\nglr\n=\nnew\nGeneralizedLinearRegression\n()\n.\nsetFamily\n(\n\"gaussian\"\n)\n.\nsetLink\n(\n\"identity\"\n)\n.\nsetMaxIter\n(\n10\n)\n.\nsetRegParam\n(\n0.3\n)\n// Fit the model\nval\nmodel\n=\nglr\n.\nfit\n(\ndataset\n)\n// Print the coefficients and intercept for generalized linear regression model\nprintln\n(\ns\n\"Coefficients: ${model.coefficients}\"\n)\nprintln\n(\ns\n\"Intercept: ${model.intercept}\"\n)\n// Summarize the model over the training set and print out some metrics\nval\nsummary\n=\nmodel\n.\nsummary\nprintln\n(\ns\n\"Coefficient Standard Errors: ${summary.coefficientStandardErrors.mkString(\"\n,\n\")}\"\n)\nprintln\n(\ns\n\"T Values: ${summary.tValues.mkString(\"\n,\n\")}\"\n)\nprintln\n(\ns\n\"P Values: ${summary.pValues.mkString(\"\n,\n\")}\"\n)\nprintln\n(\ns\n\"Dispersion: ${summary.dispersion}\"\n)\nprintln\n(\ns\n\"Null Deviance: ${summary.nullDeviance}\"\n)\nprintln\n(\ns\n\"Residual Degree Of Freedom Null: ${summary.residualDegreeOfFreedomNull}\"\n)\nprintln\n(\ns\n\"Deviance: ${summary.deviance}\"\n)\nprintln\n(\ns\n\"Residual Degree Of Freedom: ${summary.residualDegreeOfFreedom}\"\n)\nprintln\n(\ns\n\"AIC: ${summary.aic}\"\n)\nprintln\n(\n\"Deviance Residuals: \"\n)\nsummary\n.\nresiduals\n().\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\njava.util.Arrays\n;\nimport\norg.apache.spark.ml.regression.GeneralizedLinearRegression\n;\nimport\norg.apache.spark.ml.regression.GeneralizedLinearRegressionModel\n;\nimport\norg.apache.spark.ml.regression.GeneralizedLinearRegressionTrainingSummary\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\n// Load training data\nDataset\n<\nRow\n>\ndataset\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_linear_regression_data.txt\"\n);\nGeneralizedLinearRegression\nglr\n=\nnew\nGeneralizedLinearRegression\n()\n.\nsetFamily\n(\n\"gaussian\"\n)\n.\nsetLink\n(\n\"identity\"\n)\n.\nsetMaxIter\n(\n10\n)\n.\nsetRegParam\n(\n0.3\n);\n// Fit the model\nGeneralizedLinearRegressionModel\nmodel\n=\nglr\n.\nfit\n(\ndataset\n);\n// Print the coefficients and intercept for generalized linear regression model\nSystem\n.\nout\n.\nprintln\n(\n\"Coefficients: \"\n+\nmodel\n.\ncoefficients\n());\nSystem\n.\nout\n.\nprintln\n(\n\"Intercept: \"\n+\nmodel\n.\nintercept\n());\n// Summarize the model over the training set and print out some metrics\nGeneralizedLinearRegressionTrainingSummary\nsummary\n=\nmodel\n.\nsummary\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Coefficient Standard Errors: \"\n+\nArrays\n.\ntoString\n(\nsummary\n.\ncoefficientStandardErrors\n()));\nSystem\n.\nout\n.\nprintln\n(\n\"T Values: \"\n+\nArrays\n.\ntoString\n(\nsummary\n.\ntValues\n()));\nSystem\n.\nout\n.\nprintln\n(\n\"P Values: \"\n+\nArrays\n.\ntoString\n(\nsummary\n.\npValues\n()));\nSystem\n.\nout\n.\nprintln\n(\n\"Dispersion: \"\n+\nsummary\n.\ndispersion\n());\nSystem\n.\nout\n.\nprintln\n(\n\"Null Deviance: \"\n+\nsummary\n.\nnullDeviance\n());\nSystem\n.\nout\n.\nprintln\n(\n\"Residual Degree Of Freedom Null: \"\n+\nsummary\n.\nresidualDegreeOfFreedomNull\n());\nSystem\n.\nout\n.\nprintln\n(\n\"Deviance: \"\n+\nsummary\n.\ndeviance\n());\nSystem\n.\nout\n.\nprintln\n(\n\"Residual Degree Of Freedom: \"\n+\nsummary\n.\nresidualDegreeOfFreedom\n());\nSystem\n.\nout\n.\nprintln\n(\n\"AIC: \"\n+\nsummary\n.\naic\n());\nSystem\n.\nout\n.\nprintln\n(\n\"Deviance Residuals: \"\n);\nsummary\n.\nresiduals\n().\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\ntraining\n<-\nread.df\n(\n\"data/mllib/sample_multiclass_classification_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\n# Fit a generalized linear model of family \"gaussian\" with spark.glm\ndf_list\n<-\nrandomSplit\n(\ntraining\n,\nc\n(\n7\n,\n3\n),\n2\n)\ngaussianDF\n<-\ndf_list\n[[\n1\n]]\ngaussianTestDF\n<-\ndf_list\n[[\n2\n]]\ngaussianGLM\n<-\nspark.glm\n(\ngaussianDF\n,\nlabel\n~\nfeatures\n,\nfamily\n=\n\"gaussian\"\n)\n# Model summary\nsummary\n(\ngaussianGLM\n)\n# Prediction\ngaussianPredictions\n<-\npredict\n(\ngaussianGLM\n,\ngaussianTestDF\n)\nhead\n(\ngaussianPredictions\n)\n# Fit a generalized linear model with glm (R-compliant)\ngaussianGLM2\n<-\nglm\n(\nlabel\n~\nfeatures\n,\ngaussianDF\n,\nfamily\n=\n\"gaussian\"\n)\nsummary\n(\ngaussianGLM2\n)\n# Fit a generalized linear model of family \"binomial\" with spark.glm\ntraining2\n<-\nread.df\n(\n\"data/mllib/sample_multiclass_classification_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining2\n<-\ntransform\n(\ntraining2\n,\nlabel\n=\ncast\n(\ntraining2\n$\nlabel\n>\n1\n,\n\"integer\"\n))\ndf_list2\n<-\nrandomSplit\n(\ntraining2\n,\nc\n(\n7\n,\n3\n),\n2\n)\nbinomialDF\n<-\ndf_list2\n[[\n1\n]]\nbinomialTestDF\n<-\ndf_list2\n[[\n2\n]]\nbinomialGLM\n<-\nspark.glm\n(\nbinomialDF\n,\nlabel\n~\nfeatures\n,\nfamily\n=\n\"binomial\"\n)\n# Model summary\nsummary\n(\nbinomialGLM\n)\n# Prediction\nbinomialPredictions\n<-\npredict\n(\nbinomialGLM\n,\nbinomialTestDF\n)\nhead\n(\nbinomialPredictions\n)\n# Fit a generalized linear model of family \"tweedie\" with spark.glm\ntraining3\n<-\nread.df\n(\n\"data/mllib/sample_multiclass_classification_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntweedieDF\n<-\ntransform\n(\ntraining3\n,\nlabel\n=\ntraining3\n$\nlabel\n*\nexp\n(\nrandn\n(\n10\n)))\ntweedieGLM\n<-\nspark.glm\n(\ntweedieDF\n,\nlabel\n~\nfeatures\n,\nfamily\n=\n\"tweedie\"\n,\nvar.power\n=\n1.2\n,\nlink.power\n=\n0\n)\n# Model summary\nsummary\n(\ntweedieGLM\n)\nFind full example code at \"examples/src/main/r/ml/glm.R\" in the Spark repo.\nDecision tree regression\nDecision trees are a popular family of classification and regression methods.\nMore information about the\nspark.ml\nimplementation can be found further in the\nsection on decision trees\n.\nExamples\nThe following examples load a dataset in LibSVM format, split it into training and test sets, train on the first dataset, and then evaluate on the held-out test set.\nWe use a feature transformer to index categorical features, adding metadata to the\nDataFrame\nwhich the Decision Tree algorithm can recognize.\nMore details on parameters can be found in the\nPython API documentation\n.\nfrom\npyspark.ml\nimport\nPipeline\nfrom\npyspark.ml.regression\nimport\nDecisionTreeRegressor\nfrom\npyspark.ml.feature\nimport\nVectorIndexer\nfrom\npyspark.ml.evaluation\nimport\nRegressionEvaluator\n# Load the data stored in LIBSVM format as a DataFrame.\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\n# Automatically identify categorical features, and index them.\n# We specify maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer\n=\n\\\nVectorIndexer\n(\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nindexedFeatures\n\"\n,\nmaxCategories\n=\n4\n).\nfit\n(\ndata\n)\n# Split the data into training and test sets (30% held out for testing)\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n([\n0.7\n,\n0.3\n])\n# Train a DecisionTree model.\ndt\n=\nDecisionTreeRegressor\n(\nfeaturesCol\n=\n\"\nindexedFeatures\n\"\n)\n# Chain indexer and tree in a Pipeline\npipeline\n=\nPipeline\n(\nstages\n=\n[\nfeatureIndexer\n,\ndt\n])\n# Train model.  This also runs the indexer.\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n)\n# Make predictions.\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n)\n# Select example rows to display.\npredictions\n.\nselect\n(\n\"\nprediction\n\"\n,\n\"\nlabel\n\"\n,\n\"\nfeatures\n\"\n).\nshow\n(\n5\n)\n# Select (prediction, true label) and compute test error\nevaluator\n=\nRegressionEvaluator\n(\nlabelCol\n=\n\"\nlabel\n\"\n,\npredictionCol\n=\n\"\nprediction\n\"\n,\nmetricName\n=\n\"\nrmse\n\"\n)\nrmse\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprint\n(\n\"\nRoot Mean Squared Error (RMSE) on test data = %g\n\"\n%\nrmse\n)\ntreeModel\n=\nmodel\n.\nstages\n[\n1\n]\n# summary only\nprint\n(\ntreeModel\n)\nFind full example code at \"examples/src/main/python/ml/decision_tree_regression_example.py\" in the Spark repo.\nMore details on parameters can be found in the\nScala API documentation\n.\nimport\norg.apache.spark.ml.Pipeline\nimport\norg.apache.spark.ml.evaluation.RegressionEvaluator\nimport\norg.apache.spark.ml.feature.VectorIndexer\nimport\norg.apache.spark.ml.regression.DecisionTreeRegressionModel\nimport\norg.apache.spark.ml.regression.DecisionTreeRegressor\n// Load the data stored in LIBSVM format as a DataFrame.\nval\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Automatically identify categorical features, and index them.\n// Here, we treat features with > 4 distinct values as continuous.\nval\nfeatureIndexer\n=\nnew\nVectorIndexer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxCategories\n(\n4\n)\n.\nfit\n(\ndata\n)\n// Split the data into training and test sets (30% held out for testing).\nval\nArray\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.7\n,\n0.3\n))\n// Train a DecisionTree model.\nval\ndt\n=\nnew\nDecisionTreeRegressor\n()\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetFeaturesCol\n(\n\"indexedFeatures\"\n)\n// Chain indexer and tree in a Pipeline.\nval\npipeline\n=\nnew\nPipeline\n()\n.\nsetStages\n(\nArray\n(\nfeatureIndexer\n,\ndt\n))\n// Train model. This also runs the indexer.\nval\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n)\n// Make predictions.\nval\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n)\n// Select example rows to display.\npredictions\n.\nselect\n(\n\"prediction\"\n,\n\"label\"\n,\n\"features\"\n).\nshow\n(\n5\n)\n// Select (prediction, true label) and compute test error.\nval\nevaluator\n=\nnew\nRegressionEvaluator\n()\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"rmse\"\n)\nval\nrmse\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprintln\n(\ns\n\"Root Mean Squared Error (RMSE) on test data = $rmse\"\n)\nval\ntreeModel\n=\nmodel\n.\nstages\n(\n1\n).\nasInstanceOf\n[\nDecisionTreeRegressionModel\n]\nprintln\n(\ns\n\"Learned regression tree model:\\n ${treeModel.toDebugString}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\" in the Spark repo.\nMore details on parameters can be found in the\nJava API documentation\n.\nimport\norg.apache.spark.ml.Pipeline\n;\nimport\norg.apache.spark.ml.PipelineModel\n;\nimport\norg.apache.spark.ml.PipelineStage\n;\nimport\norg.apache.spark.ml.evaluation.RegressionEvaluator\n;\nimport\norg.apache.spark.ml.feature.VectorIndexer\n;\nimport\norg.apache.spark.ml.feature.VectorIndexerModel\n;\nimport\norg.apache.spark.ml.regression.DecisionTreeRegressionModel\n;\nimport\norg.apache.spark.ml.regression.DecisionTreeRegressor\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Load the data stored in LIBSVM format as a DataFrame.\nDataset\n<\nRow\n>\ndata\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n);\n// Automatically identify categorical features, and index them.\n// Set maxCategories so features with > 4 distinct values are treated as continuous.\nVectorIndexerModel\nfeatureIndexer\n=\nnew\nVectorIndexer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxCategories\n(\n4\n)\n.\nfit\n(\ndata\n);\n// Split the data into training and test sets (30% held out for testing).\nDataset\n<\nRow\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]{\n0.7\n,\n0.3\n});\nDataset\n<\nRow\n>\ntrainingData\n=\nsplits\n[\n0\n];\nDataset\n<\nRow\n>\ntestData\n=\nsplits\n[\n1\n];\n// Train a DecisionTree model.\nDecisionTreeRegressor\ndt\n=\nnew\nDecisionTreeRegressor\n()\n.\nsetFeaturesCol\n(\n\"indexedFeatures\"\n);\n// Chain indexer and tree in a Pipeline.\nPipeline\npipeline\n=\nnew\nPipeline\n()\n.\nsetStages\n(\nnew\nPipelineStage\n[]{\nfeatureIndexer\n,\ndt\n});\n// Train model. This also runs the indexer.\nPipelineModel\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n);\n// Make predictions.\nDataset\n<\nRow\n>\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n);\n// Select example rows to display.\npredictions\n.\nselect\n(\n\"label\"\n,\n\"features\"\n).\nshow\n(\n5\n);\n// Select (prediction, true label) and compute test error.\nRegressionEvaluator\nevaluator\n=\nnew\nRegressionEvaluator\n()\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"rmse\"\n);\ndouble\nrmse\n=\nevaluator\n.\nevaluate\n(\npredictions\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Root Mean Squared Error (RMSE) on test data = \"\n+\nrmse\n);\nDecisionTreeRegressionModel\ntreeModel\n=\n(\nDecisionTreeRegressionModel\n)\n(\nmodel\n.\nstages\n()[\n1\n]);\nSystem\n.\nout\n.\nprintln\n(\n\"Learned regression tree model:\\n\"\n+\ntreeModel\n.\ntoDebugString\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_linear_regression_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit a DecisionTree regression model with spark.decisionTree\nmodel\n<-\nspark.decisionTree\n(\ntraining\n,\nlabel\n~\nfeatures\n,\n\"regression\"\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\nFind full example code at \"examples/src/main/r/ml/decisionTree.R\" in the Spark repo.\nRandom forest regression\nRandom forests are a popular family of classification and regression methods.\nMore information about the\nspark.ml\nimplementation can be found further in the\nsection on random forests\n.\nExamples\nThe following examples load a dataset in LibSVM format, split it into training and test sets, train on the first dataset, and then evaluate on the held-out test set.\nWe use a feature transformer to index categorical features, adding metadata to the\nDataFrame\nwhich the tree-based algorithms can recognize.\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml\nimport\nPipeline\nfrom\npyspark.ml.regression\nimport\nRandomForestRegressor\nfrom\npyspark.ml.feature\nimport\nVectorIndexer\nfrom\npyspark.ml.evaluation\nimport\nRegressionEvaluator\n# Load and parse the data file, converting it to a DataFrame.\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\n# Automatically identify categorical features, and index them.\n# Set maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer\n=\n\\\nVectorIndexer\n(\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nindexedFeatures\n\"\n,\nmaxCategories\n=\n4\n).\nfit\n(\ndata\n)\n# Split the data into training and test sets (30% held out for testing)\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n([\n0.7\n,\n0.3\n])\n# Train a RandomForest model.\nrf\n=\nRandomForestRegressor\n(\nfeaturesCol\n=\n\"\nindexedFeatures\n\"\n)\n# Chain indexer and forest in a Pipeline\npipeline\n=\nPipeline\n(\nstages\n=\n[\nfeatureIndexer\n,\nrf\n])\n# Train model.  This also runs the indexer.\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n)\n# Make predictions.\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n)\n# Select example rows to display.\npredictions\n.\nselect\n(\n\"\nprediction\n\"\n,\n\"\nlabel\n\"\n,\n\"\nfeatures\n\"\n).\nshow\n(\n5\n)\n# Select (prediction, true label) and compute test error\nevaluator\n=\nRegressionEvaluator\n(\nlabelCol\n=\n\"\nlabel\n\"\n,\npredictionCol\n=\n\"\nprediction\n\"\n,\nmetricName\n=\n\"\nrmse\n\"\n)\nrmse\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprint\n(\n\"\nRoot Mean Squared Error (RMSE) on test data = %g\n\"\n%\nrmse\n)\nrfModel\n=\nmodel\n.\nstages\n[\n1\n]\nprint\n(\nrfModel\n)\n# summary only\nFind full example code at \"examples/src/main/python/ml/random_forest_regressor_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.Pipeline\nimport\norg.apache.spark.ml.evaluation.RegressionEvaluator\nimport\norg.apache.spark.ml.feature.VectorIndexer\nimport\norg.apache.spark.ml.regression.\n{\nRandomForestRegressionModel\n,\nRandomForestRegressor\n}\n// Load and parse the data file, converting it to a DataFrame.\nval\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Automatically identify categorical features, and index them.\n// Set maxCategories so features with > 4 distinct values are treated as continuous.\nval\nfeatureIndexer\n=\nnew\nVectorIndexer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxCategories\n(\n4\n)\n.\nfit\n(\ndata\n)\n// Split the data into training and test sets (30% held out for testing).\nval\nArray\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.7\n,\n0.3\n))\n// Train a RandomForest model.\nval\nrf\n=\nnew\nRandomForestRegressor\n()\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetFeaturesCol\n(\n\"indexedFeatures\"\n)\n// Chain indexer and forest in a Pipeline.\nval\npipeline\n=\nnew\nPipeline\n()\n.\nsetStages\n(\nArray\n(\nfeatureIndexer\n,\nrf\n))\n// Train model. This also runs the indexer.\nval\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n)\n// Make predictions.\nval\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n)\n// Select example rows to display.\npredictions\n.\nselect\n(\n\"prediction\"\n,\n\"label\"\n,\n\"features\"\n).\nshow\n(\n5\n)\n// Select (prediction, true label) and compute test error.\nval\nevaluator\n=\nnew\nRegressionEvaluator\n()\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"rmse\"\n)\nval\nrmse\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprintln\n(\ns\n\"Root Mean Squared Error (RMSE) on test data = $rmse\"\n)\nval\nrfModel\n=\nmodel\n.\nstages\n(\n1\n).\nasInstanceOf\n[\nRandomForestRegressionModel\n]\nprintln\n(\ns\n\"Learned regression forest model:\\n ${rfModel.toDebugString}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.ml.Pipeline\n;\nimport\norg.apache.spark.ml.PipelineModel\n;\nimport\norg.apache.spark.ml.PipelineStage\n;\nimport\norg.apache.spark.ml.evaluation.RegressionEvaluator\n;\nimport\norg.apache.spark.ml.feature.VectorIndexer\n;\nimport\norg.apache.spark.ml.feature.VectorIndexerModel\n;\nimport\norg.apache.spark.ml.regression.RandomForestRegressionModel\n;\nimport\norg.apache.spark.ml.regression.RandomForestRegressor\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Load and parse the data file, converting it to a DataFrame.\nDataset\n<\nRow\n>\ndata\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n);\n// Automatically identify categorical features, and index them.\n// Set maxCategories so features with > 4 distinct values are treated as continuous.\nVectorIndexerModel\nfeatureIndexer\n=\nnew\nVectorIndexer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxCategories\n(\n4\n)\n.\nfit\n(\ndata\n);\n// Split the data into training and test sets (30% held out for testing)\nDataset\n<\nRow\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]\n{\n0.7\n,\n0.3\n});\nDataset\n<\nRow\n>\ntrainingData\n=\nsplits\n[\n0\n];\nDataset\n<\nRow\n>\ntestData\n=\nsplits\n[\n1\n];\n// Train a RandomForest model.\nRandomForestRegressor\nrf\n=\nnew\nRandomForestRegressor\n()\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetFeaturesCol\n(\n\"indexedFeatures\"\n);\n// Chain indexer and forest in a Pipeline\nPipeline\npipeline\n=\nnew\nPipeline\n()\n.\nsetStages\n(\nnew\nPipelineStage\n[]\n{\nfeatureIndexer\n,\nrf\n});\n// Train model. This also runs the indexer.\nPipelineModel\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n);\n// Make predictions.\nDataset\n<\nRow\n>\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n);\n// Select example rows to display.\npredictions\n.\nselect\n(\n\"prediction\"\n,\n\"label\"\n,\n\"features\"\n).\nshow\n(\n5\n);\n// Select (prediction, true label) and compute test error\nRegressionEvaluator\nevaluator\n=\nnew\nRegressionEvaluator\n()\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"rmse\"\n);\ndouble\nrmse\n=\nevaluator\n.\nevaluate\n(\npredictions\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Root Mean Squared Error (RMSE) on test data = \"\n+\nrmse\n);\nRandomForestRegressionModel\nrfModel\n=\n(\nRandomForestRegressionModel\n)(\nmodel\n.\nstages\n()[\n1\n]);\nSystem\n.\nout\n.\nprintln\n(\n\"Learned regression forest model:\\n\"\n+\nrfModel\n.\ntoDebugString\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_linear_regression_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit a random forest regression model with spark.randomForest\nmodel\n<-\nspark.randomForest\n(\ntraining\n,\nlabel\n~\nfeatures\n,\n\"regression\"\n,\nnumTrees\n=\n10\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\nFind full example code at \"examples/src/main/r/ml/randomForest.R\" in the Spark repo.\nGradient-boosted tree regression\nGradient-boosted trees (GBTs) are a popular regression method using ensembles of decision trees.\nMore information about the\nspark.ml\nimplementation can be found further in the\nsection on GBTs\n.\nExamples\nNote: For this example dataset,\nGBTRegressor\nactually only needs 1 iteration, but that will not\nbe true in general.\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml\nimport\nPipeline\nfrom\npyspark.ml.regression\nimport\nGBTRegressor\nfrom\npyspark.ml.feature\nimport\nVectorIndexer\nfrom\npyspark.ml.evaluation\nimport\nRegressionEvaluator\n# Load and parse the data file, converting it to a DataFrame.\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\n# Automatically identify categorical features, and index them.\n# Set maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer\n=\n\\\nVectorIndexer\n(\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nindexedFeatures\n\"\n,\nmaxCategories\n=\n4\n).\nfit\n(\ndata\n)\n# Split the data into training and test sets (30% held out for testing)\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n([\n0.7\n,\n0.3\n])\n# Train a GBT model.\ngbt\n=\nGBTRegressor\n(\nfeaturesCol\n=\n\"\nindexedFeatures\n\"\n,\nmaxIter\n=\n10\n)\n# Chain indexer and GBT in a Pipeline\npipeline\n=\nPipeline\n(\nstages\n=\n[\nfeatureIndexer\n,\ngbt\n])\n# Train model.  This also runs the indexer.\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n)\n# Make predictions.\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n)\n# Select example rows to display.\npredictions\n.\nselect\n(\n\"\nprediction\n\"\n,\n\"\nlabel\n\"\n,\n\"\nfeatures\n\"\n).\nshow\n(\n5\n)\n# Select (prediction, true label) and compute test error\nevaluator\n=\nRegressionEvaluator\n(\nlabelCol\n=\n\"\nlabel\n\"\n,\npredictionCol\n=\n\"\nprediction\n\"\n,\nmetricName\n=\n\"\nrmse\n\"\n)\nrmse\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprint\n(\n\"\nRoot Mean Squared Error (RMSE) on test data = %g\n\"\n%\nrmse\n)\ngbtModel\n=\nmodel\n.\nstages\n[\n1\n]\nprint\n(\ngbtModel\n)\n# summary only\nFind full example code at \"examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.Pipeline\nimport\norg.apache.spark.ml.evaluation.RegressionEvaluator\nimport\norg.apache.spark.ml.feature.VectorIndexer\nimport\norg.apache.spark.ml.regression.\n{\nGBTRegressionModel\n,\nGBTRegressor\n}\n// Load and parse the data file, converting it to a DataFrame.\nval\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Automatically identify categorical features, and index them.\n// Set maxCategories so features with > 4 distinct values are treated as continuous.\nval\nfeatureIndexer\n=\nnew\nVectorIndexer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxCategories\n(\n4\n)\n.\nfit\n(\ndata\n)\n// Split the data into training and test sets (30% held out for testing).\nval\nArray\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.7\n,\n0.3\n))\n// Train a GBT model.\nval\ngbt\n=\nnew\nGBTRegressor\n()\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetFeaturesCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxIter\n(\n10\n)\n// Chain indexer and GBT in a Pipeline.\nval\npipeline\n=\nnew\nPipeline\n()\n.\nsetStages\n(\nArray\n(\nfeatureIndexer\n,\ngbt\n))\n// Train model. This also runs the indexer.\nval\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n)\n// Make predictions.\nval\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n)\n// Select example rows to display.\npredictions\n.\nselect\n(\n\"prediction\"\n,\n\"label\"\n,\n\"features\"\n).\nshow\n(\n5\n)\n// Select (prediction, true label) and compute test error.\nval\nevaluator\n=\nnew\nRegressionEvaluator\n()\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"rmse\"\n)\nval\nrmse\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprintln\n(\ns\n\"Root Mean Squared Error (RMSE) on test data = $rmse\"\n)\nval\ngbtModel\n=\nmodel\n.\nstages\n(\n1\n).\nasInstanceOf\n[\nGBTRegressionModel\n]\nprintln\n(\ns\n\"Learned regression GBT model:\\n ${gbtModel.toDebugString}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.ml.Pipeline\n;\nimport\norg.apache.spark.ml.PipelineModel\n;\nimport\norg.apache.spark.ml.PipelineStage\n;\nimport\norg.apache.spark.ml.evaluation.RegressionEvaluator\n;\nimport\norg.apache.spark.ml.feature.VectorIndexer\n;\nimport\norg.apache.spark.ml.feature.VectorIndexerModel\n;\nimport\norg.apache.spark.ml.regression.GBTRegressionModel\n;\nimport\norg.apache.spark.ml.regression.GBTRegressor\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Load and parse the data file, converting it to a DataFrame.\nDataset\n<\nRow\n>\ndata\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n);\n// Automatically identify categorical features, and index them.\n// Set maxCategories so features with > 4 distinct values are treated as continuous.\nVectorIndexerModel\nfeatureIndexer\n=\nnew\nVectorIndexer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxCategories\n(\n4\n)\n.\nfit\n(\ndata\n);\n// Split the data into training and test sets (30% held out for testing).\nDataset\n<\nRow\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]\n{\n0.7\n,\n0.3\n});\nDataset\n<\nRow\n>\ntrainingData\n=\nsplits\n[\n0\n];\nDataset\n<\nRow\n>\ntestData\n=\nsplits\n[\n1\n];\n// Train a GBT model.\nGBTRegressor\ngbt\n=\nnew\nGBTRegressor\n()\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetFeaturesCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxIter\n(\n10\n);\n// Chain indexer and GBT in a Pipeline.\nPipeline\npipeline\n=\nnew\nPipeline\n().\nsetStages\n(\nnew\nPipelineStage\n[]\n{\nfeatureIndexer\n,\ngbt\n});\n// Train model. This also runs the indexer.\nPipelineModel\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n);\n// Make predictions.\nDataset\n<\nRow\n>\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n);\n// Select example rows to display.\npredictions\n.\nselect\n(\n\"prediction\"\n,\n\"label\"\n,\n\"features\"\n).\nshow\n(\n5\n);\n// Select (prediction, true label) and compute test error.\nRegressionEvaluator\nevaluator\n=\nnew\nRegressionEvaluator\n()\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"rmse\"\n);\ndouble\nrmse\n=\nevaluator\n.\nevaluate\n(\npredictions\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Root Mean Squared Error (RMSE) on test data = \"\n+\nrmse\n);\nGBTRegressionModel\ngbtModel\n=\n(\nGBTRegressionModel\n)(\nmodel\n.\nstages\n()[\n1\n]);\nSystem\n.\nout\n.\nprintln\n(\n\"Learned regression GBT model:\\n\"\n+\ngbtModel\n.\ntoDebugString\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_linear_regression_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit a GBT regression model with spark.gbt\nmodel\n<-\nspark.gbt\n(\ntraining\n,\nlabel\n~\nfeatures\n,\n\"regression\"\n,\nmaxIter\n=\n10\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\nFind full example code at \"examples/src/main/r/ml/gbt.R\" in the Spark repo.\nSurvival regression\nIn\nspark.ml\n, we implement the\nAccelerated failure time (AFT)\nmodel which is a parametric survival regression model for censored data.\nIt describes a model for the log of survival time, so it’s often called a\nlog-linear model for survival analysis. Different from a\nProportional hazards\nmodel\ndesigned for the same purpose, the AFT model is easier to parallelize\nbecause each instance contributes to the objective function independently.\nGiven the values of the covariates $x^{‘}$, for random lifetime $t_{i}$ of\nsubjects i = 1, …, n, with possible right-censoring,\nthe likelihood function under the AFT model is given as:\n\\[\nL(\\beta,\\sigma)=\\prod_{i=1}^n[\\frac{1}{\\sigma}f_{0}(\\frac{\\log{t_{i}}-x^{'}\\beta}{\\sigma})]^{\\delta_{i}}S_{0}(\\frac{\\log{t_{i}}-x^{'}\\beta}{\\sigma})^{1-\\delta_{i}}\n\\]\nWhere $\\delta_{i}$ is the indicator of the event has occurred i.e. uncensored or not.\nUsing $\\epsilon_{i}=\\frac{\\log{t_{i}}-x^{‘}\\beta}{\\sigma}$, the log-likelihood function\nassumes the form:\n\\[\n\\iota(\\beta,\\sigma)=\\sum_{i=1}^{n}[-\\delta_{i}\\log\\sigma+\\delta_{i}\\log{f_{0}}(\\epsilon_{i})+(1-\\delta_{i})\\log{S_{0}(\\epsilon_{i})}]\n\\]\nWhere $S_{0}(\\epsilon_{i})$ is the baseline survivor function,\nand $f_{0}(\\epsilon_{i})$ is the corresponding density function.\nThe most commonly used AFT model is based on the Weibull distribution of the survival time.\nThe Weibull distribution for lifetime corresponds to the extreme value distribution for the\nlog of the lifetime, and the $S_{0}(\\epsilon)$ function is:\n\\[\nS_{0}(\\epsilon_{i})=\\exp(-e^{\\epsilon_{i}})\n\\]\nthe $f_{0}(\\epsilon_{i})$ function is:\n\\[\nf_{0}(\\epsilon_{i})=e^{\\epsilon_{i}}\\exp(-e^{\\epsilon_{i}})\n\\]\nThe log-likelihood function for AFT model with a Weibull distribution of lifetime is:\n\\[\n\\iota(\\beta,\\sigma)= -\\sum_{i=1}^n[\\delta_{i}\\log\\sigma-\\delta_{i}\\epsilon_{i}+e^{\\epsilon_{i}}]\n\\]\nDue to minimizing the negative log-likelihood equivalent to maximum a posteriori probability,\nthe loss function we use to optimize is $-\\iota(\\beta,\\sigma)$.\nThe gradient functions for $\\beta$ and $\\log\\sigma$ respectively are:\n\\[\n\\frac{\\partial (-\\iota)}{\\partial \\beta}=\\sum_{1=1}^{n}[\\delta_{i}-e^{\\epsilon_{i}}]\\frac{x_{i}}{\\sigma}\n\\]\n\\[\n\\frac{\\partial (-\\iota)}{\\partial (\\log\\sigma)}=\\sum_{i=1}^{n}[\\delta_{i}+(\\delta_{i}-e^{\\epsilon_{i}})\\epsilon_{i}]\n\\]\nThe AFT model can be formulated as a convex optimization problem,\ni.e. the task of finding a minimizer of a convex function $-\\iota(\\beta,\\sigma)$\nthat depends on the coefficients vector $\\beta$ and the log of scale parameter $\\log\\sigma$.\nThe optimization algorithm underlying the implementation is L-BFGS.\nThe implementation matches the result from R’s survival function\nsurvreg\nWhen fitting AFTSurvivalRegressionModel without intercept on dataset with constant nonzero column, Spark MLlib outputs zero coefficients for constant nonzero columns. This behavior is different from R survival::survreg.\nExamples\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml.regression\nimport\nAFTSurvivalRegression\nfrom\npyspark.ml.linalg\nimport\nVectors\ntraining\n=\nspark\n.\ncreateDataFrame\n([\n(\n1.218\n,\n1.0\n,\nVectors\n.\ndense\n(\n1.560\n,\n-\n0.605\n)),\n(\n2.949\n,\n0.0\n,\nVectors\n.\ndense\n(\n0.346\n,\n2.158\n)),\n(\n3.627\n,\n0.0\n,\nVectors\n.\ndense\n(\n1.380\n,\n0.231\n)),\n(\n0.273\n,\n1.0\n,\nVectors\n.\ndense\n(\n0.520\n,\n1.151\n)),\n(\n4.199\n,\n0.0\n,\nVectors\n.\ndense\n(\n0.795\n,\n-\n0.226\n))],\n[\n\"\nlabel\n\"\n,\n\"\ncensor\n\"\n,\n\"\nfeatures\n\"\n])\nquantileProbabilities\n=\n[\n0.3\n,\n0.6\n]\naft\n=\nAFTSurvivalRegression\n(\nquantileProbabilities\n=\nquantileProbabilities\n,\nquantilesCol\n=\n\"\nquantiles\n\"\n)\nmodel\n=\naft\n.\nfit\n(\ntraining\n)\n# Print the coefficients, intercept and scale parameter for AFT survival regression\nprint\n(\n\"\nCoefficients:\n\"\n+\nstr\n(\nmodel\n.\ncoefficients\n))\nprint\n(\n\"\nIntercept:\n\"\n+\nstr\n(\nmodel\n.\nintercept\n))\nprint\n(\n\"\nScale:\n\"\n+\nstr\n(\nmodel\n.\nscale\n))\nmodel\n.\ntransform\n(\ntraining\n).\nshow\n(\ntruncate\n=\nFalse\n)\nFind full example code at \"examples/src/main/python/ml/aft_survival_regression.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.linalg.Vectors\nimport\norg.apache.spark.ml.regression.AFTSurvivalRegression\nval\ntraining\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n1.218\n,\n1.0\n,\nVectors\n.\ndense\n(\n1.560\n,\n-\n0.605\n)),\n(\n2.949\n,\n0.0\n,\nVectors\n.\ndense\n(\n0.346\n,\n2.158\n)),\n(\n3.627\n,\n0.0\n,\nVectors\n.\ndense\n(\n1.380\n,\n0.231\n)),\n(\n0.273\n,\n1.0\n,\nVectors\n.\ndense\n(\n0.520\n,\n1.151\n)),\n(\n4.199\n,\n0.0\n,\nVectors\n.\ndense\n(\n0.795\n,\n-\n0.226\n))\n)).\ntoDF\n(\n\"label\"\n,\n\"censor\"\n,\n\"features\"\n)\nval\nquantileProbabilities\n=\nArray\n(\n0.3\n,\n0.6\n)\nval\naft\n=\nnew\nAFTSurvivalRegression\n()\n.\nsetQuantileProbabilities\n(\nquantileProbabilities\n)\n.\nsetQuantilesCol\n(\n\"quantiles\"\n)\nval\nmodel\n=\naft\n.\nfit\n(\ntraining\n)\n// Print the coefficients, intercept and scale parameter for AFT survival regression\nprintln\n(\ns\n\"Coefficients: ${model.coefficients}\"\n)\nprintln\n(\ns\n\"Intercept: ${model.intercept}\"\n)\nprintln\n(\ns\n\"Scale: ${model.scale}\"\n)\nmodel\n.\ntransform\n(\ntraining\n).\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.regression.AFTSurvivalRegression\n;\nimport\norg.apache.spark.ml.regression.AFTSurvivalRegressionModel\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.SparkSession\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n1.218\n,\n1.0\n,\nVectors\n.\ndense\n(\n1.560\n,\n-\n0.605\n)),\nRowFactory\n.\ncreate\n(\n2.949\n,\n0.0\n,\nVectors\n.\ndense\n(\n0.346\n,\n2.158\n)),\nRowFactory\n.\ncreate\n(\n3.627\n,\n0.0\n,\nVectors\n.\ndense\n(\n1.380\n,\n0.231\n)),\nRowFactory\n.\ncreate\n(\n0.273\n,\n1.0\n,\nVectors\n.\ndense\n(\n0.520\n,\n1.151\n)),\nRowFactory\n.\ncreate\n(\n4.199\n,\n0.0\n,\nVectors\n.\ndense\n(\n0.795\n,\n-\n0.226\n))\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"label\"\n,\nDataTypes\n.\nDoubleType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"censor\"\n,\nDataTypes\n.\nDoubleType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"features\"\n,\nnew\nVectorUDT\n(),\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ntraining\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\ndouble\n[]\nquantileProbabilities\n=\nnew\ndouble\n[]{\n0.3\n,\n0.6\n};\nAFTSurvivalRegression\naft\n=\nnew\nAFTSurvivalRegression\n()\n.\nsetQuantileProbabilities\n(\nquantileProbabilities\n)\n.\nsetQuantilesCol\n(\n\"quantiles\"\n);\nAFTSurvivalRegressionModel\nmodel\n=\naft\n.\nfit\n(\ntraining\n);\n// Print the coefficients, intercept and scale parameter for AFT survival regression\nSystem\n.\nout\n.\nprintln\n(\n\"Coefficients: \"\n+\nmodel\n.\ncoefficients\n());\nSystem\n.\nout\n.\nprintln\n(\n\"Intercept: \"\n+\nmodel\n.\nintercept\n());\nSystem\n.\nout\n.\nprintln\n(\n\"Scale: \"\n+\nmodel\n.\nscale\n());\nmodel\n.\ntransform\n(\ntraining\n).\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# Use the ovarian dataset available in R survival package\nlibrary\n(\nsurvival\n)\n# Fit an accelerated failure time (AFT) survival regression model with spark.survreg\novarianDF\n<-\nsuppressWarnings\n(\ncreateDataFrame\n(\novarian\n))\naftDF\n<-\novarianDF\naftTestDF\n<-\novarianDF\naftModel\n<-\nspark.survreg\n(\naftDF\n,\nSurv\n(\nfutime\n,\nfustat\n)\n~\necog_ps\n+\nrx\n)\n# Model summary\nsummary\n(\naftModel\n)\n# Prediction\naftPredictions\n<-\npredict\n(\naftModel\n,\naftTestDF\n)\nhead\n(\naftPredictions\n)\nFind full example code at \"examples/src/main/r/ml/survreg.R\" in the Spark repo.\nIsotonic regression\nIsotonic regression\nbelongs to the family of regression algorithms. Formally isotonic regression is a problem where\ngiven a finite set of real numbers\n$Y = {y_1, y_2, ..., y_n}$\nrepresenting observed responses\nand\n$X = {x_1, x_2, ..., x_n}$\nthe unknown response values to be fitted\nfinding a function that minimizes\n\\begin{equation}\n  f(x) = \\sum_{i=1}^n w_i (y_i - x_i)^2\n\\end{equation}\nwith respect to complete order subject to\n$x_1\\le x_2\\le ...\\le x_n$\nwhere\n$w_i$\nare positive weights.\nThe resulting function is called isotonic regression and it is unique.\nIt can be viewed as least squares problem under order restriction.\nEssentially isotonic regression is a\nmonotonic function\nbest fitting the original data points.\nWe implement a\npool adjacent violators algorithm\nwhich uses an approach to\nparallelizing isotonic regression\n.\nThe training input is a DataFrame which contains three columns\nlabel, features and weight. Additionally, IsotonicRegression algorithm has one\noptional parameter called $isotonic$ defaulting to true.\nThis argument specifies if the isotonic regression is\nisotonic (monotonically increasing) or antitonic (monotonically decreasing).\nTraining returns an IsotonicRegressionModel that can be used to predict\nlabels for both known and unknown features. The result of isotonic regression\nis treated as piecewise linear function. The rules for prediction therefore are:\nIf the prediction input exactly matches a training feature\nthen associated prediction is returned. In case there are multiple predictions with the same\nfeature then one of them is returned. Which one is undefined\n(same as java.util.Arrays.binarySearch).\nIf the prediction input is lower or higher than all training features\nthen prediction with lowest or highest feature is returned respectively.\nIn case there are multiple predictions with the same feature\nthen the lowest or highest is returned respectively.\nIf the prediction input falls between two training features then prediction is treated\nas piecewise linear function and interpolated value is calculated from the\npredictions of the two closest features. In case there are multiple values\nwith the same feature then the same rules as in previous point are used.\nExamples\nRefer to the\nIsotonicRegression\nPython docs\nfor more details on the API.\nfrom\npyspark.ml.regression\nimport\nIsotonicRegression\n# Loads data.\ndataset\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n)\n\\\n.\nload\n(\n\"\ndata/mllib/sample_isotonic_regression_libsvm_data.txt\n\"\n)\n# Trains an isotonic regression model.\nmodel\n=\nIsotonicRegression\n().\nfit\n(\ndataset\n)\nprint\n(\n\"\nBoundaries in increasing order: %s\n\\n\n\"\n%\nstr\n(\nmodel\n.\nboundaries\n))\nprint\n(\n\"\nPredictions associated with the boundaries: %s\n\\n\n\"\n%\nstr\n(\nmodel\n.\npredictions\n))\n# Makes predictions.\nmodel\n.\ntransform\n(\ndataset\n).\nshow\n()\nFind full example code at \"examples/src/main/python/ml/isotonic_regression_example.py\" in the Spark repo.\nRefer to the\nIsotonicRegression\nScala docs\nfor details on the API.\nimport\norg.apache.spark.ml.regression.IsotonicRegression\n// Loads data.\nval\ndataset\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_isotonic_regression_libsvm_data.txt\"\n)\n// Trains an isotonic regression model.\nval\nir\n=\nnew\nIsotonicRegression\n()\nval\nmodel\n=\nir\n.\nfit\n(\ndataset\n)\nprintln\n(\ns\n\"Boundaries in increasing order: ${model.boundaries}\\n\"\n)\nprintln\n(\ns\n\"Predictions associated with the boundaries: ${model.predictions}\\n\"\n)\n// Makes predictions.\nmodel\n.\ntransform\n(\ndataset\n).\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\" in the Spark repo.\nRefer to the\nIsotonicRegression\nJava docs\nfor details on the API.\nimport\norg.apache.spark.ml.regression.IsotonicRegression\n;\nimport\norg.apache.spark.ml.regression.IsotonicRegressionModel\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\n// Loads data.\nDataset\n<\nRow\n>\ndataset\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_isotonic_regression_libsvm_data.txt\"\n);\n// Trains an isotonic regression model.\nIsotonicRegression\nir\n=\nnew\nIsotonicRegression\n();\nIsotonicRegressionModel\nmodel\n=\nir\n.\nfit\n(\ndataset\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Boundaries in increasing order: \"\n+\nmodel\n.\nboundaries\n()\n+\n\"\\n\"\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Predictions associated with the boundaries: \"\n+\nmodel\n.\npredictions\n()\n+\n\"\\n\"\n);\n// Makes predictions.\nmodel\n.\ntransform\n(\ndataset\n).\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\" in the Spark repo.\nRefer to the\nIsotonicRegression\nR API docs\nfor more details on the API.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_isotonic_regression_libsvm_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit an isotonic regression model with spark.isoreg\nmodel\n<-\nspark.isoreg\n(\ntraining\n,\nlabel\n~\nfeatures\n,\nisotonic\n=\nFALSE\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\nFind full example code at \"examples/src/main/r/ml/isoreg.R\" in the Spark repo.\nFactorization machines regressor\nFor more background and more details about the implementation of factorization machines,\nrefer to the\nFactorization Machines section\n.\nExamples\nThe following examples load a dataset in LibSVM format, split it into training and test sets,\ntrain on the first dataset, and then evaluate on the held-out test set.\nWe scale features to be between 0 and 1 to prevent the exploding gradient problem.\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml\nimport\nPipeline\nfrom\npyspark.ml.regression\nimport\nFMRegressor\nfrom\npyspark.ml.feature\nimport\nMinMaxScaler\nfrom\npyspark.ml.evaluation\nimport\nRegressionEvaluator\n# Load and parse the data file, converting it to a DataFrame.\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\n# Scale features.\nfeatureScaler\n=\nMinMaxScaler\n(\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nscaledFeatures\n\"\n).\nfit\n(\ndata\n)\n# Split the data into training and test sets (30% held out for testing)\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n([\n0.7\n,\n0.3\n])\n# Train a FM model.\nfm\n=\nFMRegressor\n(\nfeaturesCol\n=\n\"\nscaledFeatures\n\"\n,\nstepSize\n=\n0.001\n)\n# Create a Pipeline.\npipeline\n=\nPipeline\n(\nstages\n=\n[\nfeatureScaler\n,\nfm\n])\n# Train model.\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n)\n# Make predictions.\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n)\n# Select example rows to display.\npredictions\n.\nselect\n(\n\"\nprediction\n\"\n,\n\"\nlabel\n\"\n,\n\"\nfeatures\n\"\n).\nshow\n(\n5\n)\n# Select (prediction, true label) and compute test error\nevaluator\n=\nRegressionEvaluator\n(\nlabelCol\n=\n\"\nlabel\n\"\n,\npredictionCol\n=\n\"\nprediction\n\"\n,\nmetricName\n=\n\"\nrmse\n\"\n)\nrmse\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprint\n(\n\"\nRoot Mean Squared Error (RMSE) on test data = %g\n\"\n%\nrmse\n)\nfmModel\n=\nmodel\n.\nstages\n[\n1\n]\nprint\n(\n\"\nFactors:\n\"\n+\nstr\n(\nfmModel\n.\nfactors\n))\n# type: ignore\nprint\n(\n\"\nLinear:\n\"\n+\nstr\n(\nfmModel\n.\nlinear\n))\n# type: ignore\nprint\n(\n\"\nIntercept:\n\"\n+\nstr\n(\nfmModel\n.\nintercept\n))\n# type: ignore\nFind full example code at \"examples/src/main/python/ml/fm_regressor_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.Pipeline\nimport\norg.apache.spark.ml.evaluation.RegressionEvaluator\nimport\norg.apache.spark.ml.feature.MinMaxScaler\nimport\norg.apache.spark.ml.regression.\n{\nFMRegressionModel\n,\nFMRegressor\n}\n// Load and parse the data file, converting it to a DataFrame.\nval\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Scale features.\nval\nfeatureScaler\n=\nnew\nMinMaxScaler\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"scaledFeatures\"\n)\n.\nfit\n(\ndata\n)\n// Split the data into training and test sets (30% held out for testing).\nval\nArray\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.7\n,\n0.3\n))\n// Train a FM model.\nval\nfm\n=\nnew\nFMRegressor\n()\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetFeaturesCol\n(\n\"scaledFeatures\"\n)\n.\nsetStepSize\n(\n0.001\n)\n// Create a Pipeline.\nval\npipeline\n=\nnew\nPipeline\n()\n.\nsetStages\n(\nArray\n(\nfeatureScaler\n,\nfm\n))\n// Train model.\nval\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n)\n// Make predictions.\nval\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n)\n// Select example rows to display.\npredictions\n.\nselect\n(\n\"prediction\"\n,\n\"label\"\n,\n\"features\"\n).\nshow\n(\n5\n)\n// Select (prediction, true label) and compute test error.\nval\nevaluator\n=\nnew\nRegressionEvaluator\n()\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"rmse\"\n)\nval\nrmse\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprintln\n(\ns\n\"Root Mean Squared Error (RMSE) on test data = $rmse\"\n)\nval\nfmModel\n=\nmodel\n.\nstages\n(\n1\n).\nasInstanceOf\n[\nFMRegressionModel\n]\nprintln\n(\ns\n\"Factors: ${fmModel.factors} Linear: ${fmModel.linear} \"\n+\ns\n\"Intercept: ${fmModel.intercept}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/FMRegressorExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.ml.Pipeline\n;\nimport\norg.apache.spark.ml.PipelineModel\n;\nimport\norg.apache.spark.ml.PipelineStage\n;\nimport\norg.apache.spark.ml.evaluation.RegressionEvaluator\n;\nimport\norg.apache.spark.ml.feature.MinMaxScaler\n;\nimport\norg.apache.spark.ml.feature.MinMaxScalerModel\n;\nimport\norg.apache.spark.ml.regression.FMRegressionModel\n;\nimport\norg.apache.spark.ml.regression.FMRegressor\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Load and parse the data file, converting it to a DataFrame.\nDataset\n<\nRow\n>\ndata\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n);\n// Scale features.\nMinMaxScalerModel\nfeatureScaler\n=\nnew\nMinMaxScaler\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"scaledFeatures\"\n)\n.\nfit\n(\ndata\n);\n// Split the data into training and test sets (30% held out for testing).\nDataset\n<\nRow\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]\n{\n0.7\n,\n0.3\n});\nDataset\n<\nRow\n>\ntrainingData\n=\nsplits\n[\n0\n];\nDataset\n<\nRow\n>\ntestData\n=\nsplits\n[\n1\n];\n// Train a FM model.\nFMRegressor\nfm\n=\nnew\nFMRegressor\n()\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetFeaturesCol\n(\n\"scaledFeatures\"\n)\n.\nsetStepSize\n(\n0.001\n);\n// Create a Pipeline.\nPipeline\npipeline\n=\nnew\nPipeline\n().\nsetStages\n(\nnew\nPipelineStage\n[]\n{\nfeatureScaler\n,\nfm\n});\n// Train model.\nPipelineModel\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n);\n// Make predictions.\nDataset\n<\nRow\n>\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n);\n// Select example rows to display.\npredictions\n.\nselect\n(\n\"prediction\"\n,\n\"label\"\n,\n\"features\"\n).\nshow\n(\n5\n);\n// Select (prediction, true label) and compute test error.\nRegressionEvaluator\nevaluator\n=\nnew\nRegressionEvaluator\n()\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"rmse\"\n);\ndouble\nrmse\n=\nevaluator\n.\nevaluate\n(\npredictions\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Root Mean Squared Error (RMSE) on test data = \"\n+\nrmse\n);\nFMRegressionModel\nfmModel\n=\n(\nFMRegressionModel\n)(\nmodel\n.\nstages\n()[\n1\n]);\nSystem\n.\nout\n.\nprintln\n(\n\"Factors: \"\n+\nfmModel\n.\nfactors\n());\nSystem\n.\nout\n.\nprintln\n(\n\"Linear: \"\n+\nfmModel\n.\nlinear\n());\nSystem\n.\nout\n.\nprintln\n(\n\"Intercept: \"\n+\nfmModel\n.\nintercept\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaFMRegressorExample.java\" in the Spark repo.\nRefer to the\nR API documentation\nfor more details.\nNote: At the moment SparkR doesn’t support feature scaling.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_linear_regression_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining_test\n<-\nrandomSplit\n(\ndf\n,\nc\n(\n0.7\n,\n0.3\n))\ntraining\n<-\ntraining_test\n[[\n1\n]]\ntest\n<-\ntraining_test\n[[\n2\n]]\n# Fit a FM regression model\nmodel\n<-\nspark.fmRegressor\n(\ntraining\n,\nlabel\n~\nfeatures\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\nFind full example code at \"examples/src/main/r/ml/fmRegressor.R\" in the Spark repo.\nLinear methods\nWe implement popular linear methods such as logistic\nregression and linear least squares with $L_1$ or $L_2$ regularization.\nRefer to\nthe linear methods guide for the RDD-based API\nfor\ndetails about implementation and tuning; this information is still relevant.\nWe also include a DataFrame API for\nElastic\nnet\n, a hybrid\nof $L_1$ and $L_2$ regularization proposed in\nZou et al, Regularization\nand variable selection via the elastic\nnet\n.\nMathematically, it is defined as a convex combination of the $L_1$ and\nthe $L_2$ regularization terms:\n\\[\n\\alpha \\left( \\lambda \\|\\wv\\|_1 \\right) + (1-\\alpha) \\left( \\frac{\\lambda}{2}\\|\\wv\\|_2^2 \\right) , \\alpha \\in [0, 1], \\lambda \\geq 0\n\\]\nBy setting $\\alpha$ properly, elastic net contains both $L_1$ and $L_2$\nregularization as special cases. For example, if a\nlinear\nregression\nmodel is\ntrained with the elastic net parameter $\\alpha$ set to $1$, it is\nequivalent to a\nLasso\nmodel.\nOn the other hand, if $\\alpha$ is set to $0$, the trained model reduces\nto a\nridge\nregression\nmodel.\nWe implement Pipelines API for both linear regression and logistic\nregression with elastic net regularization.\nFactorization Machines\nFactorization Machines\nare able to estimate interactions\nbetween features even in problems with huge sparsity (like advertising and recommendation system).\nThe\nspark.ml\nimplementation supports factorization machines for binary classification and for regression.\nFactorization machines formula is:\n\\[\\hat{y} = w_0 + \\sum\\limits^n_{i-1} w_i x_i +\n  \\sum\\limits^n_{i=1} \\sum\\limits^n_{j=i+1} \\langle v_i, v_j \\rangle x_i x_j\\]\nThe first two terms denote intercept and linear term (same as in linear regression),\nand the last term denotes pairwise interactions term. \\(v_i\\) describes the i-th variable\nwith k factors.\nFM can be used for regression and optimization criterion is mean square error. FM also can be used for\nbinary classification through sigmoid function. The optimization criterion is logistic loss.\nThe pairwise interactions can be reformulated:\n\\[\\sum\\limits^n_{i=1} \\sum\\limits^n_{j=i+1} \\langle v_i, v_j \\rangle x_i x_j\n  = \\frac{1}{2}\\sum\\limits^k_{f=1}\n    \\left(\\left( \\sum\\limits^n_{i=1}v_{i,f}x_i \\right)^2 -\n    \\sum\\limits^n_{i=1}v_{i,f}^2x_i^2 \\right)\\]\nThis equation has only linear complexity in both k and n - i.e. its computation is in \\(O(kn)\\).\nIn general, in order to prevent the exploding gradient problem, it is best to scale continuous features to be between 0 and 1,\nor bin the continuous features and one-hot encode them.\nDecision trees\nDecision trees\nand their ensembles are popular methods for the machine learning tasks of\nclassification and regression. Decision trees are widely used since they are easy to interpret,\nhandle categorical features, extend to the multiclass classification setting, do not require\nfeature scaling, and are able to capture non-linearities and feature interactions. Tree ensemble\nalgorithms such as random forests and boosting are among the top performers for classification and\nregression tasks.\nThe\nspark.ml\nimplementation supports decision trees for binary and multiclass classification and for regression,\nusing both continuous and categorical features. The implementation partitions data by rows,\nallowing distributed training with millions or even billions of instances.\nUsers can find more information about the decision tree algorithm in the\nMLlib Decision Tree guide\n.\nThe main differences between this API and the\noriginal MLlib Decision Tree API\nare:\nsupport for ML Pipelines\nseparation of Decision Trees for classification vs. regression\nuse of DataFrame metadata to distinguish continuous and categorical features\nThe Pipelines API for Decision Trees offers a bit more functionality than the original API.\nIn particular, for classification, users can get the predicted probability of each class (a.k.a. class conditional probabilities);\nfor regression, users can get the biased sample variance of prediction.\nEnsembles of trees (Random Forests and Gradient-Boosted Trees) are described below in the\nTree ensembles section\n.\nInputs and Outputs\nWe list the input and output (prediction) column types here.\nAll output columns are optional; to exclude an output column, set its corresponding Param to an empty string.\nInput Columns\nParam name\nType(s)\nDefault\nDescription\nlabelCol\nDouble\n\"label\"\nLabel to predict\nfeaturesCol\nVector\n\"features\"\nFeature vector\nOutput Columns\nParam name\nType(s)\nDefault\nDescription\nNotes\npredictionCol\nDouble\n\"prediction\"\nPredicted label\nrawPredictionCol\nVector\n\"rawPrediction\"\nVector of length # classes, with the counts of training instance labels at the tree node which makes the prediction\nClassification only\nprobabilityCol\nVector\n\"probability\"\nVector of length # classes equal to rawPrediction normalized to a multinomial distribution\nClassification only\nvarianceCol\nDouble\nThe biased sample variance of prediction\nRegression only\nTree Ensembles\nThe DataFrame API supports two major tree ensemble algorithms:\nRandom Forests\nand\nGradient-Boosted Trees (GBTs)\n.\nBoth use\nspark.ml\ndecision trees\nas their base models.\nUsers can find more information about ensemble algorithms in the\nMLlib Ensemble guide\n.\nIn this section, we demonstrate the DataFrame API for ensembles.\nThe main differences between this API and the\noriginal MLlib ensembles API\nare:\nsupport for DataFrames and ML Pipelines\nseparation of classification vs. regression\nuse of DataFrame metadata to distinguish continuous and categorical features\nmore functionality for random forests: estimates of feature importance, as well as the predicted probability of each class (a.k.a. class conditional probabilities) for classification.\nRandom Forests\nRandom forests\nare ensembles of\ndecision trees\n.\nRandom forests combine many decision trees in order to reduce the risk of overfitting.\nThe\nspark.ml\nimplementation supports random forests for binary and multiclass classification and for regression,\nusing both continuous and categorical features.\nFor more information on the algorithm itself, please see the\nspark.mllib\ndocumentation on random forests\n.\nInputs and Outputs\nWe list the input and output (prediction) column types here.\nAll output columns are optional; to exclude an output column, set its corresponding Param to an empty string.\nInput Columns\nParam name\nType(s)\nDefault\nDescription\nlabelCol\nDouble\n\"label\"\nLabel to predict\nfeaturesCol\nVector\n\"features\"\nFeature vector\nOutput Columns (Predictions)\nParam name\nType(s)\nDefault\nDescription\nNotes\npredictionCol\nDouble\n\"prediction\"\nPredicted label\nrawPredictionCol\nVector\n\"rawPrediction\"\nVector of length # classes, with the counts of training instance labels at the tree node which makes the prediction\nClassification only\nprobabilityCol\nVector\n\"probability\"\nVector of length # classes equal to rawPrediction normalized to a multinomial distribution\nClassification only\nGradient-Boosted Trees (GBTs)\nGradient-Boosted Trees (GBTs)\nare ensembles of\ndecision trees\n.\nGBTs iteratively train decision trees in order to minimize a loss function.\nThe\nspark.ml\nimplementation supports GBTs for binary classification and for regression,\nusing both continuous and categorical features.\nFor more information on the algorithm itself, please see the\nspark.mllib\ndocumentation on GBTs\n.\nInputs and Outputs\nWe list the input and output (prediction) column types here.\nAll output columns are optional; to exclude an output column, set its corresponding Param to an empty string.\nInput Columns\nParam name\nType(s)\nDefault\nDescription\nlabelCol\nDouble\n\"label\"\nLabel to predict\nfeaturesCol\nVector\n\"features\"\nFeature vector\nNote that\nGBTClassifier\ncurrently only supports binary labels.\nOutput Columns (Predictions)\nParam name\nType(s)\nDefault\nDescription\nNotes\npredictionCol\nDouble\n\"prediction\"\nPredicted label\nIn the future,\nGBTClassifier\nwill also output columns for\nrawPrediction\nand\nprobability\n, just as\nRandomForestClassifier\ndoes."}
{"url": "https://spark.apache.org/docs/latest/ml-collaborative-filtering.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nCollaborative Filtering\nCollaborative filtering\nExplicit vs. implicit feedback\nScaling of the regularization parameter\nCold-start strategy\nCollaborative filtering\nCollaborative filtering\nis commonly used for recommender systems.  These techniques aim to fill in the\nmissing entries of a user-item association matrix.\nspark.ml\ncurrently supports\nmodel-based collaborative filtering, in which users and products are described\nby a small set of latent factors that can be used to predict missing entries.\nspark.ml\nuses the\nalternating least squares\n(ALS)\nalgorithm to learn these latent factors. The implementation in\nspark.ml\nhas the\nfollowing parameters:\nnumBlocks\nis the number of blocks the users and items will be partitioned into in order to parallelize computation (defaults to 10).\nrank\nis the number of latent factors in the model (defaults to 10).\nmaxIter\nis the maximum number of iterations to run (defaults to 10).\nregParam\nspecifies the regularization parameter in ALS (defaults to 1.0).\nimplicitPrefs\nspecifies whether to use the\nexplicit feedback\nALS variant or one adapted for\nimplicit feedback\ndata (defaults to\nfalse\nwhich means using\nexplicit feedback\n).\nalpha\nis a parameter applicable to the implicit feedback variant of ALS that governs the\nbaseline\nconfidence in preference observations (defaults to 1.0).\nnonnegative\nspecifies whether or not to use nonnegative constraints for least squares (defaults to\nfalse\n).\nNote:\nThe DataFrame-based API for ALS currently only supports integers for user and item ids.\nOther numeric types are supported for the user and item id columns, \nbut the ids must be within the integer value range.\nExplicit vs. implicit feedback\nThe standard approach to matrix factorization-based collaborative filtering treats\nthe entries in the user-item matrix as\nexplicit\npreferences given by the user to the item,\nfor example, users giving ratings to movies.\nIt is common in many real-world use cases to only have access to\nimplicit feedback\n(e.g. views,\nclicks, purchases, likes, shares etc.). The approach used in\nspark.ml\nto deal with such data is taken\nfrom\nCollaborative Filtering for Implicit Feedback Datasets\n.\nEssentially, instead of trying to model the matrix of ratings directly, this approach treats the data\nas numbers representing the\nstrength\nin observations of user actions (such as the number of clicks,\nor the cumulative duration someone spent viewing a movie). Those numbers are then related to the level of\nconfidence in observed user preferences, rather than explicit ratings given to items. The model\nthen tries to find latent factors that can be used to predict the expected preference of a user for\nan item.\nScaling of the regularization parameter\nWe scale the regularization parameter\nregParam\nin solving each least squares problem by\nthe number of ratings the user generated in updating user factors,\nor the number of ratings the product received in updating product factors.\nThis approach is named “ALS-WR” and discussed in the paper\n“\nLarge-Scale Parallel Collaborative Filtering for the Netflix Prize\n”.\nIt makes\nregParam\nless dependent on the scale of the dataset, so we can apply the\nbest parameter learned from a sampled subset to the full dataset and expect similar performance.\nCold-start strategy\nWhen making predictions using an\nALSModel\n, it is common to encounter users and/or items in the \ntest dataset that were not present during training the model. This typically occurs in two \nscenarios:\nIn production, for new users or items that have no rating history and on which the model has not \nbeen trained (this is the “cold start problem”).\nDuring cross-validation, the data is split between training and evaluation sets. When using \nsimple random splits as in Spark’s\nCrossValidator\nor\nTrainValidationSplit\n, it is actually \nvery common to encounter users and/or items in the evaluation set that are not in the training set\nBy default, Spark assigns\nNaN\npredictions during\nALSModel.transform\nwhen a user and/or item \nfactor is not present in the model. This can be useful in a production system, since it indicates \na new user or item, and so the system can make a decision on some fallback to use as the prediction.\nHowever, this is undesirable during cross-validation, since any\nNaN\npredicted values will result\nin\nNaN\nresults for the evaluation metric (for example when using\nRegressionEvaluator\n).\nThis makes model selection impossible.\nSpark allows users to set the\ncoldStartStrategy\nparameter\nto “drop” in order to drop any rows in the\nDataFrame\nof predictions that contain\nNaN\nvalues. \nThe evaluation metric will then be computed over the non-\nNaN\ndata and will be valid. \nUsage of this parameter is illustrated in the example below.\nNote:\ncurrently the supported cold start strategies are “nan” (the default behavior mentioned \nabove) and “drop”. Further strategies may be supported in future.\nExamples\nIn the following example, we load ratings data from the\nMovieLens dataset\n, each row\nconsisting of a user, a movie, a rating and a timestamp.\nWe then train an ALS model which assumes, by default, that the ratings are\nexplicit (\nimplicitPrefs\nis\nFalse\n).\nWe evaluate the recommendation model by measuring the root-mean-square error of\nrating prediction.\nRefer to the\nALS\nPython docs\nfor more details on the API.\nfrom\npyspark.ml.evaluation\nimport\nRegressionEvaluator\nfrom\npyspark.ml.recommendation\nimport\nALS\nfrom\npyspark.sql\nimport\nRow\nlines\n=\nspark\n.\nread\n.\ntext\n(\n\"\ndata/mllib/als/sample_movielens_ratings.txt\n\"\n).\nrdd\nparts\n=\nlines\n.\nmap\n(\nlambda\nrow\n:\nrow\n.\nvalue\n.\nsplit\n(\n\"\n::\n\"\n))\nratingsRDD\n=\nparts\n.\nmap\n(\nlambda\np\n:\nRow\n(\nuserId\n=\nint\n(\np\n[\n0\n]),\nmovieId\n=\nint\n(\np\n[\n1\n]),\nrating\n=\nfloat\n(\np\n[\n2\n]),\ntimestamp\n=\nint\n(\np\n[\n3\n])))\nratings\n=\nspark\n.\ncreateDataFrame\n(\nratingsRDD\n)\n(\ntraining\n,\ntest\n)\n=\nratings\n.\nrandomSplit\n([\n0.8\n,\n0.2\n])\n# Build the recommendation model using ALS on the training data\n# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\nals\n=\nALS\n(\nmaxIter\n=\n5\n,\nregParam\n=\n0.01\n,\nuserCol\n=\n\"\nuserId\n\"\n,\nitemCol\n=\n\"\nmovieId\n\"\n,\nratingCol\n=\n\"\nrating\n\"\n,\ncoldStartStrategy\n=\n\"\ndrop\n\"\n)\nmodel\n=\nals\n.\nfit\n(\ntraining\n)\n# Evaluate the model by computing the RMSE on the test data\npredictions\n=\nmodel\n.\ntransform\n(\ntest\n)\nevaluator\n=\nRegressionEvaluator\n(\nmetricName\n=\n\"\nrmse\n\"\n,\nlabelCol\n=\n\"\nrating\n\"\n,\npredictionCol\n=\n\"\nprediction\n\"\n)\nrmse\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprint\n(\n\"\nRoot-mean-square error =\n\"\n+\nstr\n(\nrmse\n))\n# Generate top 10 movie recommendations for each user\nuserRecs\n=\nmodel\n.\nrecommendForAllUsers\n(\n10\n)\n# Generate top 10 user recommendations for each movie\nmovieRecs\n=\nmodel\n.\nrecommendForAllItems\n(\n10\n)\n# Generate top 10 movie recommendations for a specified set of users\nusers\n=\nratings\n.\nselect\n(\nals\n.\ngetUserCol\n()).\ndistinct\n().\nlimit\n(\n3\n)\nuserSubsetRecs\n=\nmodel\n.\nrecommendForUserSubset\n(\nusers\n,\n10\n)\n# Generate top 10 user recommendations for a specified set of movies\nmovies\n=\nratings\n.\nselect\n(\nals\n.\ngetItemCol\n()).\ndistinct\n().\nlimit\n(\n3\n)\nmovieSubSetRecs\n=\nmodel\n.\nrecommendForItemSubset\n(\nmovies\n,\n10\n)\nFind full example code at \"examples/src/main/python/ml/als_example.py\" in the Spark repo.\nIf the rating matrix is derived from another source of information (i.e. it is\ninferred from other signals), you can set\nimplicitPrefs\nto\nTrue\nto get\nbetter results:\nals\n=\nALS\n(\nmaxIter\n=\n5\n,\nregParam\n=\n0.01\n,\nimplicitPrefs\n=\nTrue\n,\nuserCol\n=\n\"\nuserId\n\"\n,\nitemCol\n=\n\"\nmovieId\n\"\n,\nratingCol\n=\n\"\nrating\n\"\n)\nIn the following example, we load ratings data from the\nMovieLens dataset\n, each row\nconsisting of a user, a movie, a rating and a timestamp.\nWe then train an ALS model which assumes, by default, that the ratings are\nexplicit (\nimplicitPrefs\nis\nfalse\n).\nWe evaluate the recommendation model by measuring the root-mean-square error of\nrating prediction.\nRefer to the\nALS\nScala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.evaluation.RegressionEvaluator\nimport\norg.apache.spark.ml.recommendation.ALS\ncase\nclass\nRating\n(\nuserId\n:\nInt\n,\nmovieId\n:\nInt\n,\nrating\n:\nFloat\n,\ntimestamp\n:\nLong\n)\ndef\nparseRating\n(\nstr\n:\nString\n)\n:\nRating\n=\n{\nval\nfields\n=\nstr\n.\nsplit\n(\n\"::\"\n)\nassert\n(\nfields\n.\nsize\n==\n4\n)\nRating\n(\nfields\n(\n0\n).\ntoInt\n,\nfields\n(\n1\n).\ntoInt\n,\nfields\n(\n2\n).\ntoFloat\n,\nfields\n(\n3\n).\ntoLong\n)\n}\nval\nratings\n=\nspark\n.\nread\n.\ntextFile\n(\n\"data/mllib/als/sample_movielens_ratings.txt\"\n)\n.\nmap\n(\nparseRating\n)\n.\ntoDF\n()\nval\nArray\n(\ntraining\n,\ntest\n)\n=\nratings\n.\nrandomSplit\n(\nArray\n(\n0.8\n,\n0.2\n))\n// Build the recommendation model using ALS on the training data\nval\nals\n=\nnew\nALS\n()\n.\nsetMaxIter\n(\n5\n)\n.\nsetRegParam\n(\n0.01\n)\n.\nsetUserCol\n(\n\"userId\"\n)\n.\nsetItemCol\n(\n\"movieId\"\n)\n.\nsetRatingCol\n(\n\"rating\"\n)\nval\nmodel\n=\nals\n.\nfit\n(\ntraining\n)\n// Evaluate the model by computing the RMSE on the test data\n// Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\nmodel\n.\nsetColdStartStrategy\n(\n\"drop\"\n)\nval\npredictions\n=\nmodel\n.\ntransform\n(\ntest\n)\nval\nevaluator\n=\nnew\nRegressionEvaluator\n()\n.\nsetMetricName\n(\n\"rmse\"\n)\n.\nsetLabelCol\n(\n\"rating\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\nval\nrmse\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprintln\n(\ns\n\"Root-mean-square error = $rmse\"\n)\n// Generate top 10 movie recommendations for each user\nval\nuserRecs\n=\nmodel\n.\nrecommendForAllUsers\n(\n10\n)\n// Generate top 10 user recommendations for each movie\nval\nmovieRecs\n=\nmodel\n.\nrecommendForAllItems\n(\n10\n)\n// Generate top 10 movie recommendations for a specified set of users\nval\nusers\n=\nratings\n.\nselect\n(\nals\n.\ngetUserCol\n).\ndistinct\n().\nlimit\n(\n3\n)\nval\nuserSubsetRecs\n=\nmodel\n.\nrecommendForUserSubset\n(\nusers\n,\n10\n)\n// Generate top 10 user recommendations for a specified set of movies\nval\nmovies\n=\nratings\n.\nselect\n(\nals\n.\ngetItemCol\n).\ndistinct\n().\nlimit\n(\n3\n)\nval\nmovieSubSetRecs\n=\nmodel\n.\nrecommendForItemSubset\n(\nmovies\n,\n10\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\" in the Spark repo.\nIf the rating matrix is derived from another source of information (i.e. it is\ninferred from other signals), you can set\nimplicitPrefs\nto\ntrue\nto get\nbetter results:\nval\nals\n=\nnew\nALS\n()\n.\nsetMaxIter\n(\n5\n)\n.\nsetRegParam\n(\n0.01\n)\n.\nsetImplicitPrefs\n(\ntrue\n)\n.\nsetUserCol\n(\n\"userId\"\n)\n.\nsetItemCol\n(\n\"movieId\"\n)\n.\nsetRatingCol\n(\n\"rating\"\n)\nIn the following example, we load ratings data from the\nMovieLens dataset\n, each row\nconsisting of a user, a movie, a rating and a timestamp.\nWe then train an ALS model which assumes, by default, that the ratings are\nexplicit (\nimplicitPrefs\nis\nfalse\n).\nWe evaluate the recommendation model by measuring the root-mean-square error of\nrating prediction.\nRefer to the\nALS\nJava docs\nfor more details on the API.\nimport\njava.io.Serializable\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.ml.evaluation.RegressionEvaluator\n;\nimport\norg.apache.spark.ml.recommendation.ALS\n;\nimport\norg.apache.spark.ml.recommendation.ALSModel\n;\npublic\nstatic\nclass\nRating\nimplements\nSerializable\n{\nprivate\nint\nuserId\n;\nprivate\nint\nmovieId\n;\nprivate\nfloat\nrating\n;\nprivate\nlong\ntimestamp\n;\npublic\nRating\n()\n{}\npublic\nRating\n(\nint\nuserId\n,\nint\nmovieId\n,\nfloat\nrating\n,\nlong\ntimestamp\n)\n{\nthis\n.\nuserId\n=\nuserId\n;\nthis\n.\nmovieId\n=\nmovieId\n;\nthis\n.\nrating\n=\nrating\n;\nthis\n.\ntimestamp\n=\ntimestamp\n;\n}\npublic\nint\ngetUserId\n()\n{\nreturn\nuserId\n;\n}\npublic\nint\ngetMovieId\n()\n{\nreturn\nmovieId\n;\n}\npublic\nfloat\ngetRating\n()\n{\nreturn\nrating\n;\n}\npublic\nlong\ngetTimestamp\n()\n{\nreturn\ntimestamp\n;\n}\npublic\nstatic\nRating\nparseRating\n(\nString\nstr\n)\n{\nString\n[]\nfields\n=\nstr\n.\nsplit\n(\n\"::\"\n);\nif\n(\nfields\n.\nlength\n!=\n4\n)\n{\nthrow\nnew\nIllegalArgumentException\n(\n\"Each line must contain 4 fields\"\n);\n}\nint\nuserId\n=\nInteger\n.\nparseInt\n(\nfields\n[\n0\n]);\nint\nmovieId\n=\nInteger\n.\nparseInt\n(\nfields\n[\n1\n]);\nfloat\nrating\n=\nFloat\n.\nparseFloat\n(\nfields\n[\n2\n]);\nlong\ntimestamp\n=\nLong\n.\nparseLong\n(\nfields\n[\n3\n]);\nreturn\nnew\nRating\n(\nuserId\n,\nmovieId\n,\nrating\n,\ntimestamp\n);\n}\n}\nJavaRDD\n<\nRating\n>\nratingsRDD\n=\nspark\n.\nread\n().\ntextFile\n(\n\"data/mllib/als/sample_movielens_ratings.txt\"\n).\njavaRDD\n()\n.\nmap\n(\nRating:\n:\nparseRating\n);\nDataset\n<\nRow\n>\nratings\n=\nspark\n.\ncreateDataFrame\n(\nratingsRDD\n,\nRating\n.\nclass\n);\nDataset\n<\nRow\n>[]\nsplits\n=\nratings\n.\nrandomSplit\n(\nnew\ndouble\n[]{\n0.8\n,\n0.2\n});\nDataset\n<\nRow\n>\ntraining\n=\nsplits\n[\n0\n];\nDataset\n<\nRow\n>\ntest\n=\nsplits\n[\n1\n];\n// Build the recommendation model using ALS on the training data\nALS\nals\n=\nnew\nALS\n()\n.\nsetMaxIter\n(\n5\n)\n.\nsetRegParam\n(\n0.01\n)\n.\nsetUserCol\n(\n\"userId\"\n)\n.\nsetItemCol\n(\n\"movieId\"\n)\n.\nsetRatingCol\n(\n\"rating\"\n);\nALSModel\nmodel\n=\nals\n.\nfit\n(\ntraining\n);\n// Evaluate the model by computing the RMSE on the test data\n// Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\nmodel\n.\nsetColdStartStrategy\n(\n\"drop\"\n);\nDataset\n<\nRow\n>\npredictions\n=\nmodel\n.\ntransform\n(\ntest\n);\nRegressionEvaluator\nevaluator\n=\nnew\nRegressionEvaluator\n()\n.\nsetMetricName\n(\n\"rmse\"\n)\n.\nsetLabelCol\n(\n\"rating\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n);\ndouble\nrmse\n=\nevaluator\n.\nevaluate\n(\npredictions\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Root-mean-square error = \"\n+\nrmse\n);\n// Generate top 10 movie recommendations for each user\nDataset\n<\nRow\n>\nuserRecs\n=\nmodel\n.\nrecommendForAllUsers\n(\n10\n);\n// Generate top 10 user recommendations for each movie\nDataset\n<\nRow\n>\nmovieRecs\n=\nmodel\n.\nrecommendForAllItems\n(\n10\n);\n// Generate top 10 movie recommendations for a specified set of users\nDataset\n<\nRow\n>\nusers\n=\nratings\n.\nselect\n(\nals\n.\ngetUserCol\n()).\ndistinct\n().\nlimit\n(\n3\n);\nDataset\n<\nRow\n>\nuserSubsetRecs\n=\nmodel\n.\nrecommendForUserSubset\n(\nusers\n,\n10\n);\n// Generate top 10 user recommendations for a specified set of movies\nDataset\n<\nRow\n>\nmovies\n=\nratings\n.\nselect\n(\nals\n.\ngetItemCol\n()).\ndistinct\n().\nlimit\n(\n3\n);\nDataset\n<\nRow\n>\nmovieSubSetRecs\n=\nmodel\n.\nrecommendForItemSubset\n(\nmovies\n,\n10\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\" in the Spark repo.\nIf the rating matrix is derived from another source of information (i.e. it is\ninferred from other signals), you can set\nimplicitPrefs\nto\ntrue\nto get\nbetter results:\nALS\nals\n=\nnew\nALS\n()\n.\nsetMaxIter\n(\n5\n)\n.\nsetRegParam\n(\n0.01\n)\n.\nsetImplicitPrefs\n(\ntrue\n)\n.\nsetUserCol\n(\n\"userId\"\n)\n.\nsetItemCol\n(\n\"movieId\"\n)\n.\nsetRatingCol\n(\n\"rating\"\n);\nRefer to the\nR API docs\nfor more details.\n# Load training data\ndata\n<-\nlist\n(\nlist\n(\n0\n,\n0\n,\n4.0\n),\nlist\n(\n0\n,\n1\n,\n2.0\n),\nlist\n(\n1\n,\n1\n,\n3.0\n),\nlist\n(\n1\n,\n2\n,\n4.0\n),\nlist\n(\n2\n,\n1\n,\n1.0\n),\nlist\n(\n2\n,\n2\n,\n5.0\n))\ndf\n<-\ncreateDataFrame\n(\ndata\n,\nc\n(\n\"userId\"\n,\n\"movieId\"\n,\n\"rating\"\n))\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit a recommendation model using ALS with spark.als\nmodel\n<-\nspark.als\n(\ntraining\n,\nmaxIter\n=\n5\n,\nregParam\n=\n0.01\n,\nuserCol\n=\n\"userId\"\n,\nitemCol\n=\n\"movieId\"\n,\nratingCol\n=\n\"rating\"\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\nFind full example code at \"examples/src/main/r/ml/als.R\" in the Spark repo."}
{"url": "https://spark.apache.org/docs/latest/ml-clustering.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nClustering\nThis page describes clustering algorithms in MLlib.\nThe\nguide for clustering in the RDD-based API\nalso has relevant information\nabout these algorithms.\nTable of Contents\nK-means\nInput Columns\nOutput Columns\nLatent Dirichlet allocation (LDA)\nBisecting k-means\nGaussian Mixture Model (GMM)\nInput Columns\nOutput Columns\nPower Iteration Clustering (PIC)\nK-means\nk-means\nis one of the\nmost commonly used clustering algorithms that clusters the data points into a\npredefined number of clusters. The MLlib implementation includes a parallelized\nvariant of the\nk-means++\nmethod\ncalled\nkmeans||\n.\nKMeans\nis implemented as an\nEstimator\nand generates a\nKMeansModel\nas the base model.\nInput Columns\nParam name\nType(s)\nDefault\nDescription\nfeaturesCol\nVector\n\"features\"\nFeature vector\nOutput Columns\nParam name\nType(s)\nDefault\nDescription\npredictionCol\nInt\n\"prediction\"\nPredicted cluster center\nExamples\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml.clustering\nimport\nKMeans\nfrom\npyspark.ml.evaluation\nimport\nClusteringEvaluator\n# Loads data.\ndataset\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_kmeans_data.txt\n\"\n)\n# Trains a k-means model.\nkmeans\n=\nKMeans\n().\nsetK\n(\n2\n).\nsetSeed\n(\n1\n)\nmodel\n=\nkmeans\n.\nfit\n(\ndataset\n)\n# Make predictions\npredictions\n=\nmodel\n.\ntransform\n(\ndataset\n)\n# Evaluate clustering by computing Silhouette score\nevaluator\n=\nClusteringEvaluator\n()\nsilhouette\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprint\n(\n\"\nSilhouette with squared euclidean distance =\n\"\n+\nstr\n(\nsilhouette\n))\n# Shows the result.\ncenters\n=\nmodel\n.\nclusterCenters\n()\nprint\n(\n\"\nCluster Centers:\n\"\n)\nfor\ncenter\nin\ncenters\n:\nprint\n(\ncenter\n)\nFind full example code at \"examples/src/main/python/ml/kmeans_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.clustering.KMeans\nimport\norg.apache.spark.ml.evaluation.ClusteringEvaluator\n// Loads data.\nval\ndataset\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_kmeans_data.txt\"\n)\n// Trains a k-means model.\nval\nkmeans\n=\nnew\nKMeans\n().\nsetK\n(\n2\n).\nsetSeed\n(\n1L\n)\nval\nmodel\n=\nkmeans\n.\nfit\n(\ndataset\n)\n// Make predictions\nval\npredictions\n=\nmodel\n.\ntransform\n(\ndataset\n)\n// Evaluate clustering by computing Silhouette score\nval\nevaluator\n=\nnew\nClusteringEvaluator\n()\nval\nsilhouette\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprintln\n(\ns\n\"Silhouette with squared euclidean distance = $silhouette\"\n)\n// Shows the result.\nprintln\n(\n\"Cluster Centers: \"\n)\nmodel\n.\nclusterCenters\n.\nforeach\n(\nprintln\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.ml.clustering.KMeansModel\n;\nimport\norg.apache.spark.ml.clustering.KMeans\n;\nimport\norg.apache.spark.ml.evaluation.ClusteringEvaluator\n;\nimport\norg.apache.spark.ml.linalg.Vector\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\n// Loads data.\nDataset\n<\nRow\n>\ndataset\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_kmeans_data.txt\"\n);\n// Trains a k-means model.\nKMeans\nkmeans\n=\nnew\nKMeans\n().\nsetK\n(\n2\n).\nsetSeed\n(\n1L\n);\nKMeansModel\nmodel\n=\nkmeans\n.\nfit\n(\ndataset\n);\n// Make predictions\nDataset\n<\nRow\n>\npredictions\n=\nmodel\n.\ntransform\n(\ndataset\n);\n// Evaluate clustering by computing Silhouette score\nClusteringEvaluator\nevaluator\n=\nnew\nClusteringEvaluator\n();\ndouble\nsilhouette\n=\nevaluator\n.\nevaluate\n(\npredictions\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Silhouette with squared euclidean distance = \"\n+\nsilhouette\n);\n// Shows the result.\nVector\n[]\ncenters\n=\nmodel\n.\nclusterCenters\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Cluster Centers: \"\n);\nfor\n(\nVector\ncenter:\ncenters\n)\n{\nSystem\n.\nout\n.\nprintln\n(\ncenter\n);\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# Fit a k-means model with spark.kmeans\nt\n<-\nas.data.frame\n(\nTitanic\n)\ntraining\n<-\ncreateDataFrame\n(\nt\n)\ndf_list\n<-\nrandomSplit\n(\ntraining\n,\nc\n(\n7\n,\n3\n),\n2\n)\nkmeansDF\n<-\ndf_list\n[[\n1\n]]\nkmeansTestDF\n<-\ndf_list\n[[\n2\n]]\nkmeansModel\n<-\nspark.kmeans\n(\nkmeansDF\n,\n~\nClass\n+\nSex\n+\nAge\n+\nFreq\n,\nk\n=\n3\n)\n# Model summary\nsummary\n(\nkmeansModel\n)\n# Get fitted result from the k-means model\nhead\n(\nfitted\n(\nkmeansModel\n))\n# Prediction\nkmeansPredictions\n<-\npredict\n(\nkmeansModel\n,\nkmeansTestDF\n)\nhead\n(\nkmeansPredictions\n)\nFind full example code at \"examples/src/main/r/ml/kmeans.R\" in the Spark repo.\nLatent Dirichlet allocation (LDA)\nLDA\nis implemented as an\nEstimator\nthat supports both\nEMLDAOptimizer\nand\nOnlineLDAOptimizer\n,\nand generates a\nLDAModel\nas the base model. Expert users may cast a\nLDAModel\ngenerated by\nEMLDAOptimizer\nto a\nDistributedLDAModel\nif needed.\nExamples\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml.clustering\nimport\nLDA\n# Loads data.\ndataset\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_lda_libsvm_data.txt\n\"\n)\n# Trains a LDA model.\nlda\n=\nLDA\n(\nk\n=\n10\n,\nmaxIter\n=\n10\n)\nmodel\n=\nlda\n.\nfit\n(\ndataset\n)\nll\n=\nmodel\n.\nlogLikelihood\n(\ndataset\n)\nlp\n=\nmodel\n.\nlogPerplexity\n(\ndataset\n)\nprint\n(\n\"\nThe lower bound on the log likelihood of the entire corpus:\n\"\n+\nstr\n(\nll\n))\nprint\n(\n\"\nThe upper bound on perplexity:\n\"\n+\nstr\n(\nlp\n))\n# Describe topics.\ntopics\n=\nmodel\n.\ndescribeTopics\n(\n3\n)\nprint\n(\n\"\nThe topics described by their top-weighted terms:\n\"\n)\ntopics\n.\nshow\n(\ntruncate\n=\nFalse\n)\n# Shows the result\ntransformed\n=\nmodel\n.\ntransform\n(\ndataset\n)\ntransformed\n.\nshow\n(\ntruncate\n=\nFalse\n)\nFind full example code at \"examples/src/main/python/ml/lda_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.clustering.LDA\n// Loads data.\nval\ndataset\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_lda_libsvm_data.txt\"\n)\n// Trains a LDA model.\nval\nlda\n=\nnew\nLDA\n().\nsetK\n(\n10\n).\nsetMaxIter\n(\n10\n)\nval\nmodel\n=\nlda\n.\nfit\n(\ndataset\n)\nval\nll\n=\nmodel\n.\nlogLikelihood\n(\ndataset\n)\nval\nlp\n=\nmodel\n.\nlogPerplexity\n(\ndataset\n)\nprintln\n(\ns\n\"The lower bound on the log likelihood of the entire corpus: $ll\"\n)\nprintln\n(\ns\n\"The upper bound on perplexity: $lp\"\n)\n// Describe topics.\nval\ntopics\n=\nmodel\n.\ndescribeTopics\n(\n3\n)\nprintln\n(\n\"The topics described by their top-weighted terms:\"\n)\ntopics\n.\nshow\n(\nfalse\n)\n// Shows the result.\nval\ntransformed\n=\nmodel\n.\ntransform\n(\ndataset\n)\ntransformed\n.\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.ml.clustering.LDA\n;\nimport\norg.apache.spark.ml.clustering.LDAModel\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Loads data.\nDataset\n<\nRow\n>\ndataset\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_lda_libsvm_data.txt\"\n);\n// Trains a LDA model.\nLDA\nlda\n=\nnew\nLDA\n().\nsetK\n(\n10\n).\nsetMaxIter\n(\n10\n);\nLDAModel\nmodel\n=\nlda\n.\nfit\n(\ndataset\n);\ndouble\nll\n=\nmodel\n.\nlogLikelihood\n(\ndataset\n);\ndouble\nlp\n=\nmodel\n.\nlogPerplexity\n(\ndataset\n);\nSystem\n.\nout\n.\nprintln\n(\n\"The lower bound on the log likelihood of the entire corpus: \"\n+\nll\n);\nSystem\n.\nout\n.\nprintln\n(\n\"The upper bound on perplexity: \"\n+\nlp\n);\n// Describe topics.\nDataset\n<\nRow\n>\ntopics\n=\nmodel\n.\ndescribeTopics\n(\n3\n);\nSystem\n.\nout\n.\nprintln\n(\n\"The topics described by their top-weighted terms:\"\n);\ntopics\n.\nshow\n(\nfalse\n);\n// Shows the result.\nDataset\n<\nRow\n>\ntransformed\n=\nmodel\n.\ntransform\n(\ndataset\n);\ntransformed\n.\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_lda_libsvm_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit a latent dirichlet allocation model with spark.lda\nmodel\n<-\nspark.lda\n(\ntraining\n,\nk\n=\n10\n,\nmaxIter\n=\n10\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Posterior probabilities\nposterior\n<-\nspark.posterior\n(\nmodel\n,\ntest\n)\nhead\n(\nposterior\n)\n# The log perplexity of the LDA model\nlogPerplexity\n<-\nspark.perplexity\n(\nmodel\n,\ntest\n)\nprint\n(\npaste0\n(\n\"The upper bound bound on perplexity: \"\n,\nlogPerplexity\n))\nFind full example code at \"examples/src/main/r/ml/lda.R\" in the Spark repo.\nBisecting k-means\nBisecting k-means is a kind of\nhierarchical clustering\nusing a\ndivisive (or “top-down”) approach: all observations start in one cluster, and splits are performed recursively as one\nmoves down the hierarchy.\nBisecting K-means can often be much faster than regular K-means, but it will generally produce a different clustering.\nBisectingKMeans\nis implemented as an\nEstimator\nand generates a\nBisectingKMeansModel\nas the base model.\nExamples\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml.clustering\nimport\nBisectingKMeans\nfrom\npyspark.ml.evaluation\nimport\nClusteringEvaluator\n# Loads data.\ndataset\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_kmeans_data.txt\n\"\n)\n# Trains a bisecting k-means model.\nbkm\n=\nBisectingKMeans\n().\nsetK\n(\n2\n).\nsetSeed\n(\n1\n)\nmodel\n=\nbkm\n.\nfit\n(\ndataset\n)\n# Make predictions\npredictions\n=\nmodel\n.\ntransform\n(\ndataset\n)\n# Evaluate clustering by computing Silhouette score\nevaluator\n=\nClusteringEvaluator\n()\nsilhouette\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprint\n(\n\"\nSilhouette with squared euclidean distance =\n\"\n+\nstr\n(\nsilhouette\n))\n# Shows the result.\nprint\n(\n\"\nCluster Centers:\n\"\n)\ncenters\n=\nmodel\n.\nclusterCenters\n()\nfor\ncenter\nin\ncenters\n:\nprint\n(\ncenter\n)\nFind full example code at \"examples/src/main/python/ml/bisecting_k_means_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.clustering.BisectingKMeans\nimport\norg.apache.spark.ml.evaluation.ClusteringEvaluator\n// Loads data.\nval\ndataset\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_kmeans_data.txt\"\n)\n// Trains a bisecting k-means model.\nval\nbkm\n=\nnew\nBisectingKMeans\n().\nsetK\n(\n2\n).\nsetSeed\n(\n1\n)\nval\nmodel\n=\nbkm\n.\nfit\n(\ndataset\n)\n// Make predictions\nval\npredictions\n=\nmodel\n.\ntransform\n(\ndataset\n)\n// Evaluate clustering by computing Silhouette score\nval\nevaluator\n=\nnew\nClusteringEvaluator\n()\nval\nsilhouette\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprintln\n(\ns\n\"Silhouette with squared euclidean distance = $silhouette\"\n)\n// Shows the result.\nprintln\n(\n\"Cluster Centers: \"\n)\nval\ncenters\n=\nmodel\n.\nclusterCenters\ncenters\n.\nforeach\n(\nprintln\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.ml.clustering.BisectingKMeans\n;\nimport\norg.apache.spark.ml.clustering.BisectingKMeansModel\n;\nimport\norg.apache.spark.ml.evaluation.ClusteringEvaluator\n;\nimport\norg.apache.spark.ml.linalg.Vector\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\n// Loads data.\nDataset\n<\nRow\n>\ndataset\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_kmeans_data.txt\"\n);\n// Trains a bisecting k-means model.\nBisectingKMeans\nbkm\n=\nnew\nBisectingKMeans\n().\nsetK\n(\n2\n).\nsetSeed\n(\n1\n);\nBisectingKMeansModel\nmodel\n=\nbkm\n.\nfit\n(\ndataset\n);\n// Make predictions\nDataset\n<\nRow\n>\npredictions\n=\nmodel\n.\ntransform\n(\ndataset\n);\n// Evaluate clustering by computing Silhouette score\nClusteringEvaluator\nevaluator\n=\nnew\nClusteringEvaluator\n();\ndouble\nsilhouette\n=\nevaluator\n.\nevaluate\n(\npredictions\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Silhouette with squared euclidean distance = \"\n+\nsilhouette\n);\n// Shows the result.\nSystem\n.\nout\n.\nprintln\n(\n\"Cluster Centers: \"\n);\nVector\n[]\ncenters\n=\nmodel\n.\nclusterCenters\n();\nfor\n(\nVector\ncenter\n:\ncenters\n)\n{\nSystem\n.\nout\n.\nprintln\n(\ncenter\n);\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\nt\n<-\nas.data.frame\n(\nTitanic\n)\ntraining\n<-\ncreateDataFrame\n(\nt\n)\n# Fit bisecting k-means model with four centers\nmodel\n<-\nspark.bisectingKmeans\n(\ntraining\n,\nClass\n~\nSurvived\n,\nk\n=\n4\n)\n# get fitted result from a bisecting k-means model\nfitted.model\n<-\nfitted\n(\nmodel\n,\n\"centers\"\n)\n# Model summary\nhead\n(\nsummary\n(\nfitted.model\n))\n# fitted values on training data\nfitted\n<-\npredict\n(\nmodel\n,\ntraining\n)\nhead\n(\nselect\n(\nfitted\n,\n\"Class\"\n,\n\"prediction\"\n))\nFind full example code at \"examples/src/main/r/ml/bisectingKmeans.R\" in the Spark repo.\nGaussian Mixture Model (GMM)\nA\nGaussian Mixture Model\nrepresents a composite distribution whereby points are drawn from one of\nk\nGaussian sub-distributions,\neach with its own probability. The\nspark.ml\nimplementation uses the\nexpectation-maximization\nalgorithm to induce the maximum-likelihood model given a set of samples.\nGaussianMixture\nis implemented as an\nEstimator\nand generates a\nGaussianMixtureModel\nas the base\nmodel.\nInput Columns\nParam name\nType(s)\nDefault\nDescription\nfeaturesCol\nVector\n\"features\"\nFeature vector\nOutput Columns\nParam name\nType(s)\nDefault\nDescription\npredictionCol\nInt\n\"prediction\"\nPredicted cluster center\nprobabilityCol\nVector\n\"probability\"\nProbability of each cluster\nExamples\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml.clustering\nimport\nGaussianMixture\n# loads data\ndataset\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_kmeans_data.txt\n\"\n)\ngmm\n=\nGaussianMixture\n().\nsetK\n(\n2\n).\nsetSeed\n(\n538009335\n)\nmodel\n=\ngmm\n.\nfit\n(\ndataset\n)\nprint\n(\n\"\nGaussians shown as a DataFrame:\n\"\n)\nmodel\n.\ngaussiansDF\n.\nshow\n(\ntruncate\n=\nFalse\n)\nFind full example code at \"examples/src/main/python/ml/gaussian_mixture_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.clustering.GaussianMixture\n// Loads data\nval\ndataset\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_kmeans_data.txt\"\n)\n// Trains Gaussian Mixture Model\nval\ngmm\n=\nnew\nGaussianMixture\n()\n.\nsetK\n(\n2\n)\nval\nmodel\n=\ngmm\n.\nfit\n(\ndataset\n)\n// output parameters of mixture model model\nfor\n(\ni\n<-\n0\nuntil\nmodel\n.\ngetK\n)\n{\nprintln\n(\ns\n\"Gaussian $i:\\nweight=${model.weights(i)}\\n\"\n+\ns\n\"mu=${model.gaussians(i).mean}\\nsigma=\\n${model.gaussians(i).cov}\\n\"\n)\n}\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.ml.clustering.GaussianMixture\n;\nimport\norg.apache.spark.ml.clustering.GaussianMixtureModel\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\n// Loads data\nDataset\n<\nRow\n>\ndataset\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_kmeans_data.txt\"\n);\n// Trains a GaussianMixture model\nGaussianMixture\ngmm\n=\nnew\nGaussianMixture\n()\n.\nsetK\n(\n2\n);\nGaussianMixtureModel\nmodel\n=\ngmm\n.\nfit\n(\ndataset\n);\n// Output the parameters of the mixture model\nfor\n(\nint\ni\n=\n0\n;\ni\n<\nmodel\n.\ngetK\n();\ni\n++)\n{\nSystem\n.\nout\n.\nprintf\n(\n\"Gaussian %d:\\nweight=%f\\nmu=%s\\nsigma=\\n%s\\n\\n\"\n,\ni\n,\nmodel\n.\nweights\n()[\ni\n],\nmodel\n.\ngaussians\n()[\ni\n].\nmean\n(),\nmodel\n.\ngaussians\n()[\ni\n].\ncov\n());\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_kmeans_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit a gaussian mixture clustering model with spark.gaussianMixture\nmodel\n<-\nspark.gaussianMixture\n(\ntraining\n,\n~\nfeatures\n,\nk\n=\n2\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\nFind full example code at \"examples/src/main/r/ml/gaussianMixture.R\" in the Spark repo.\nPower Iteration Clustering (PIC)\nPower Iteration Clustering (PIC) is  a scalable graph clustering algorithm\ndeveloped by\nLin and Cohen\n.\nFrom the abstract: PIC finds a very low-dimensional embedding of a dataset\nusing truncated power iteration on a normalized pair-wise similarity matrix of the data.\nspark.ml\n’s PowerIterationClustering implementation takes the following parameters:\nk\n: the number of clusters to create\ninitMode\n: param for the initialization algorithm\nmaxIter\n: param for maximum number of iterations\nsrcCol\n: param for the name of the input column for source vertex IDs\ndstCol\n: name of the input column for destination vertex IDs\nweightCol\n: Param for weight column name\nExamples\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml.clustering\nimport\nPowerIterationClustering\ndf\n=\nspark\n.\ncreateDataFrame\n([\n(\n0\n,\n1\n,\n1.0\n),\n(\n0\n,\n2\n,\n1.0\n),\n(\n1\n,\n2\n,\n1.0\n),\n(\n3\n,\n4\n,\n1.0\n),\n(\n4\n,\n0\n,\n0.1\n)\n],\n[\n\"\nsrc\n\"\n,\n\"\ndst\n\"\n,\n\"\nweight\n\"\n])\npic\n=\nPowerIterationClustering\n(\nk\n=\n2\n,\nmaxIter\n=\n20\n,\ninitMode\n=\n\"\ndegree\n\"\n,\nweightCol\n=\n\"\nweight\n\"\n)\n# Shows the cluster assignment\npic\n.\nassignClusters\n(\ndf\n).\nshow\n()\nFind full example code at \"examples/src/main/python/ml/power_iteration_clustering_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.clustering.PowerIterationClustering\nval\ndataset\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0L\n,\n1L\n,\n1.0\n),\n(\n0L\n,\n2L\n,\n1.0\n),\n(\n1L\n,\n2L\n,\n1.0\n),\n(\n3L\n,\n4L\n,\n1.0\n),\n(\n4L\n,\n0L\n,\n0.1\n)\n)).\ntoDF\n(\n\"src\"\n,\n\"dst\"\n,\n\"weight\"\n)\nval\nmodel\n=\nnew\nPowerIterationClustering\n().\nsetK\n(\n2\n).\nsetMaxIter\n(\n20\n).\nsetInitMode\n(\n\"degree\"\n).\nsetWeightCol\n(\n\"weight\"\n)\nval\nprediction\n=\nmodel\n.\nassignClusters\n(\ndataset\n).\nselect\n(\n\"id\"\n,\n\"cluster\"\n)\n//  Shows the cluster assignment\nprediction\n.\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/PowerIterationClusteringExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.clustering.PowerIterationClustering\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.SparkSession\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n0L\n,\n1L\n,\n1.0\n),\nRowFactory\n.\ncreate\n(\n0L\n,\n2L\n,\n1.0\n),\nRowFactory\n.\ncreate\n(\n1L\n,\n2L\n,\n1.0\n),\nRowFactory\n.\ncreate\n(\n3L\n,\n4L\n,\n1.0\n),\nRowFactory\n.\ncreate\n(\n4L\n,\n0L\n,\n0.1\n)\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"src\"\n,\nDataTypes\n.\nLongType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"dst\"\n,\nDataTypes\n.\nLongType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"weight\"\n,\nDataTypes\n.\nDoubleType\n,\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nPowerIterationClustering\nmodel\n=\nnew\nPowerIterationClustering\n()\n.\nsetK\n(\n2\n)\n.\nsetMaxIter\n(\n10\n)\n.\nsetInitMode\n(\n\"degree\"\n)\n.\nsetWeightCol\n(\n\"weight\"\n);\nDataset\n<\nRow\n>\nresult\n=\nmodel\n.\nassignClusters\n(\ndf\n);\nresult\n.\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaPowerIterationClusteringExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\ndf\n<-\ncreateDataFrame\n(\nlist\n(\nlist\n(\n0L\n,\n1L\n,\n1.0\n),\nlist\n(\n0L\n,\n2L\n,\n1.0\n),\nlist\n(\n1L\n,\n2L\n,\n1.0\n),\nlist\n(\n3L\n,\n4L\n,\n1.0\n),\nlist\n(\n4L\n,\n0L\n,\n0.1\n)),\nschema\n=\nc\n(\n\"src\"\n,\n\"dst\"\n,\n\"weight\"\n))\n# assign clusters\nclusters\n<-\nspark.assignClusters\n(\ndf\n,\nk\n=\n2L\n,\nmaxIter\n=\n20L\n,\ninitMode\n=\n\"degree\"\n,\nweightCol\n=\n\"weight\"\n)\nshowDF\n(\narrange\n(\nclusters\n,\nclusters\n$\nid\n))\nFind full example code at \"examples/src/main/r/ml/powerIterationClustering.R\" in the Spark repo."}
{"url": "https://spark.apache.org/docs/latest/ml-frequent-pattern-mining.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nFrequent Pattern Mining\nMining frequent items, itemsets, subsequences, or other substructures is usually among the\nfirst steps to analyze a large-scale dataset, which has been an active research topic in\ndata mining for years.\nWe refer users to Wikipedia’s\nassociation rule learning\nfor more information.\nTable of Contents\nFP-Growth\nPrefixSpan\nFP-Growth\nThe FP-growth algorithm is described in the paper\nHan et al., Mining frequent patterns without candidate generation\n,\nwhere “FP” stands for frequent pattern.\nGiven a dataset of transactions, the first step of FP-growth is to calculate item frequencies and identify frequent items.\nDifferent from\nApriori-like\nalgorithms designed for the same purpose,\nthe second step of FP-growth uses a suffix tree (FP-tree) structure to encode transactions without generating candidate sets\nexplicitly, which are usually expensive to generate.\nAfter the second step, the frequent itemsets can be extracted from the FP-tree.\nIn\nspark.mllib\n, we implemented a parallel version of FP-growth called PFP,\nas described in\nLi et al., PFP: Parallel FP-growth for query recommendation\n.\nPFP distributes the work of growing FP-trees based on the suffixes of transactions,\nand hence is more scalable than a single-machine implementation.\nWe refer users to the papers for more details.\nFP-growth operates on\nitemsets\n. An itemset is an unordered collection of unique items. Spark does not have a\nset\ntype, so itemsets are represented as arrays.\nspark.ml\n’s FP-growth implementation takes the following (hyper-)parameters:\nminSupport\n: the minimum support for an itemset to be identified as frequent.\nFor example, if an item appears 3 out of 5 transactions, it has a support of 3/5=0.6.\nminConfidence\n: minimum confidence for generating Association Rule. Confidence is an indication of how often an\nassociation rule has been found to be true. For example, if in the transactions itemset\nX\nappears 4 times,\nX\nand\nY\nco-occur only 2 times, the confidence for the rule\nX => Y\nis then 2/4 = 0.5. The parameter will not\naffect the mining for frequent itemsets, but specify the minimum confidence for generating association rules\nfrom frequent itemsets.\nnumPartitions\n: the number of partitions used to distribute the work. By default the param is not set, and\nnumber of partitions of the input dataset is used.\nThe\nFPGrowthModel\nprovides:\nfreqItemsets\n: frequent itemsets in the format of a DataFrame with the following columns:\nitems: array\n: A given itemset.\nfreq: long\n: A count of how many times this itemset was seen, given the configured model parameters.\nassociationRules\n: association rules generated with confidence above\nminConfidence\n, in the format of a DataFrame with the following columns:\nantecedent: array\n: The itemset that is the hypothesis of the association rule.\nconsequent: array\n: An itemset that always contains a single element representing the conclusion of the association rule.\nconfidence: double\n: Refer to\nminConfidence\nabove for a definition of\nconfidence\n.\nlift: double\n: A measure of how well the antecedent predicts the consequent, calculated as\nsupport(antecedent U consequent) / (support(antecedent) x support(consequent))\nsupport: double\n: Refer to\nminSupport\nabove for a definition of\nsupport\n.\ntransform\n: For each transaction in\nitemsCol\n, the\ntransform\nmethod will compare its items against the antecedents\nof each association rule. If the record contains all the antecedents of a specific association rule, the rule\nwill be considered as applicable and its consequents will be added to the prediction result. The transform\nmethod will summarize the consequents from all the applicable rules as prediction. The prediction column has\nthe same data type as\nitemsCol\nand does not contain existing items in the\nitemsCol\n.\nExamples\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml.fpm\nimport\nFPGrowth\ndf\n=\nspark\n.\ncreateDataFrame\n([\n(\n0\n,\n[\n1\n,\n2\n,\n5\n]),\n(\n1\n,\n[\n1\n,\n2\n,\n3\n,\n5\n]),\n(\n2\n,\n[\n1\n,\n2\n])\n],\n[\n\"\nid\n\"\n,\n\"\nitems\n\"\n])\nfpGrowth\n=\nFPGrowth\n(\nitemsCol\n=\n\"\nitems\n\"\n,\nminSupport\n=\n0.5\n,\nminConfidence\n=\n0.6\n)\nmodel\n=\nfpGrowth\n.\nfit\n(\ndf\n)\n# Display frequent itemsets.\nmodel\n.\nfreqItemsets\n.\nshow\n()\n# Display generated association rules.\nmodel\n.\nassociationRules\n.\nshow\n()\n# transform examines the input items against all the association rules and summarize the\n# consequents as prediction\nmodel\n.\ntransform\n(\ndf\n).\nshow\n()\nFind full example code at \"examples/src/main/python/ml/fpgrowth_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.fpm.FPGrowth\nval\ndataset\n=\nspark\n.\ncreateDataset\n(\nSeq\n(\n\"1 2 5\"\n,\n\"1 2 3 5\"\n,\n\"1 2\"\n)\n).\nmap\n(\nt\n=>\nt\n.\nsplit\n(\n\" \"\n)).\ntoDF\n(\n\"items\"\n)\nval\nfpgrowth\n=\nnew\nFPGrowth\n().\nsetItemsCol\n(\n\"items\"\n).\nsetMinSupport\n(\n0.5\n).\nsetMinConfidence\n(\n0.6\n)\nval\nmodel\n=\nfpgrowth\n.\nfit\n(\ndataset\n)\n// Display frequent itemsets.\nmodel\n.\nfreqItemsets\n.\nshow\n()\n// Display generated association rules.\nmodel\n.\nassociationRules\n.\nshow\n()\n// transform examines the input items against all the association rules and summarize the\n// consequents as prediction\nmodel\n.\ntransform\n(\ndataset\n).\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.fpm.FPGrowth\n;\nimport\norg.apache.spark.ml.fpm.FPGrowthModel\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.SparkSession\n;\nimport\norg.apache.spark.sql.types.*\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\nArrays\n.\nasList\n(\n\"1 2 5\"\n.\nsplit\n(\n\" \"\n))),\nRowFactory\n.\ncreate\n(\nArrays\n.\nasList\n(\n\"1 2 3 5\"\n.\nsplit\n(\n\" \"\n))),\nRowFactory\n.\ncreate\n(\nArrays\n.\nasList\n(\n\"1 2\"\n.\nsplit\n(\n\" \"\n)))\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"items\"\n,\nnew\nArrayType\n(\nDataTypes\n.\nStringType\n,\ntrue\n),\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\nitemsDF\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nFPGrowthModel\nmodel\n=\nnew\nFPGrowth\n()\n.\nsetItemsCol\n(\n\"items\"\n)\n.\nsetMinSupport\n(\n0.5\n)\n.\nsetMinConfidence\n(\n0.6\n)\n.\nfit\n(\nitemsDF\n);\n// Display frequent itemsets.\nmodel\n.\nfreqItemsets\n().\nshow\n();\n// Display generated association rules.\nmodel\n.\nassociationRules\n().\nshow\n();\n// transform examines the input items against all the association rules and summarize the\n// consequents as prediction\nmodel\n.\ntransform\n(\nitemsDF\n).\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# Load training data\ndf\n<-\nselectExpr\n(\ncreateDataFrame\n(\ndata.frame\n(\nrawItems\n=\nc\n(\n\"1,2,5\"\n,\n\"1,2,3,5\"\n,\n\"1,2\"\n))),\n\"split(rawItems, ',') AS items\"\n)\nfpm\n<-\nspark.fpGrowth\n(\ndf\n,\nitemsCol\n=\n\"items\"\n,\nminSupport\n=\n0.5\n,\nminConfidence\n=\n0.6\n)\n# Extracting frequent itemsets\nspark.freqItemsets\n(\nfpm\n)\n# Extracting association rules\nspark.associationRules\n(\nfpm\n)\n# Predict uses association rules to and combines possible consequents\npredict\n(\nfpm\n,\ndf\n)\nFind full example code at \"examples/src/main/r/ml/fpm.R\" in the Spark repo.\nPrefixSpan\nPrefixSpan is a sequential pattern mining algorithm described in\nPei et al., Mining Sequential Patterns by Pattern-Growth: The\nPrefixSpan Approach\n. We refer\nthe reader to the referenced paper for formalizing the sequential\npattern mining problem.\nspark.ml\n’s PrefixSpan implementation takes the following parameters:\nminSupport\n: the minimum support required to be considered a frequent\nsequential pattern.\nmaxPatternLength\n: the maximum length of a frequent sequential\npattern. Any frequent pattern exceeding this length will not be\nincluded in the results.\nmaxLocalProjDBSize\n: the maximum number of items allowed in a\nprefix-projected database before local iterative processing of the\nprojected database begins. This parameter should be tuned with respect\nto the size of your executors.\nsequenceCol\n: the name of the sequence column in dataset (default “sequence”), rows with\nnulls in this column are ignored.\nExamples\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml.fpm\nimport\nPrefixSpan\ndf\n=\nsc\n.\nparallelize\n([\nRow\n(\nsequence\n=\n[[\n1\n,\n2\n],\n[\n3\n]]),\nRow\n(\nsequence\n=\n[[\n1\n],\n[\n3\n,\n2\n],\n[\n1\n,\n2\n]]),\nRow\n(\nsequence\n=\n[[\n1\n,\n2\n],\n[\n5\n]]),\nRow\n(\nsequence\n=\n[[\n6\n]])]).\ntoDF\n()\nprefixSpan\n=\nPrefixSpan\n(\nminSupport\n=\n0.5\n,\nmaxPatternLength\n=\n5\n,\nmaxLocalProjDBSize\n=\n32000000\n)\n# Find frequent sequential patterns.\nprefixSpan\n.\nfindFrequentSequentialPatterns\n(\ndf\n).\nshow\n()\nFind full example code at \"examples/src/main/python/ml/prefixspan_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.fpm.PrefixSpan\nval\nsmallTestData\n=\nSeq\n(\nSeq\n(\nSeq\n(\n1\n,\n2\n),\nSeq\n(\n3\n)),\nSeq\n(\nSeq\n(\n1\n),\nSeq\n(\n3\n,\n2\n),\nSeq\n(\n1\n,\n2\n)),\nSeq\n(\nSeq\n(\n1\n,\n2\n),\nSeq\n(\n5\n)),\nSeq\n(\nSeq\n(\n6\n)))\nval\ndf\n=\nsmallTestData\n.\ntoDF\n(\n\"sequence\"\n)\nval\nresult\n=\nnew\nPrefixSpan\n()\n.\nsetMinSupport\n(\n0.5\n)\n.\nsetMaxPatternLength\n(\n5\n)\n.\nsetMaxLocalProjDBSize\n(\n32000000\n)\n.\nfindFrequentSequentialPatterns\n(\ndf\n)\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/PrefixSpanExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.fpm.PrefixSpan\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.SparkSession\n;\nimport\norg.apache.spark.sql.types.*\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\nArrays\n.\nasList\n(\nArrays\n.\nasList\n(\n1\n,\n2\n),\nArrays\n.\nasList\n(\n3\n))),\nRowFactory\n.\ncreate\n(\nArrays\n.\nasList\n(\nArrays\n.\nasList\n(\n1\n),\nArrays\n.\nasList\n(\n3\n,\n2\n),\nArrays\n.\nasList\n(\n1\n,\n2\n))),\nRowFactory\n.\ncreate\n(\nArrays\n.\nasList\n(\nArrays\n.\nasList\n(\n1\n,\n2\n),\nArrays\n.\nasList\n(\n5\n))),\nRowFactory\n.\ncreate\n(\nArrays\n.\nasList\n(\nArrays\n.\nasList\n(\n6\n)))\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"sequence\"\n,\nnew\nArrayType\n(\nnew\nArrayType\n(\nDataTypes\n.\nIntegerType\n,\ntrue\n),\ntrue\n),\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\nsequenceDF\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nPrefixSpan\nprefixSpan\n=\nnew\nPrefixSpan\n().\nsetMinSupport\n(\n0.5\n).\nsetMaxPatternLength\n(\n5\n);\n// Finding frequent sequential patterns\nprefixSpan\n.\nfindFrequentSequentialPatterns\n(\nsequenceDF\n).\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaPrefixSpanExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# Load training data\ndf\n<-\ncreateDataFrame\n(\nlist\n(\nlist\n(\nlist\n(\nlist\n(\n1L\n,\n2L\n),\nlist\n(\n3L\n))),\nlist\n(\nlist\n(\nlist\n(\n1L\n),\nlist\n(\n3L\n,\n2L\n),\nlist\n(\n1L\n,\n2L\n))),\nlist\n(\nlist\n(\nlist\n(\n1L\n,\n2L\n),\nlist\n(\n5L\n))),\nlist\n(\nlist\n(\nlist\n(\n6L\n)))),\nschema\n=\nc\n(\n\"sequence\"\n))\n# Finding frequent sequential patterns\nfrequency\n<-\nspark.findFrequentSequentialPatterns\n(\ndf\n,\nminSupport\n=\n0.5\n,\nmaxPatternLength\n=\n5L\n,\nmaxLocalProjDBSize\n=\n32000000L\n)\nshowDF\n(\nfrequency\n)\nFind full example code at \"examples/src/main/r/ml/prefixSpan.R\" in the Spark repo."}
{"url": "https://spark.apache.org/docs/latest/ml-tuning.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nML Tuning: model selection and hyperparameter tuning\n\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\wv}{\\mathbf{w}}\n\\newcommand{\\av}{\\mathbf{\\alpha}}\n\\newcommand{\\bv}{\\mathbf{b}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\id}{\\mathbf{I}}\n\\newcommand{\\ind}{\\mathbf{1}}\n\\newcommand{\\0}{\\mathbf{0}}\n\\newcommand{\\unit}{\\mathbf{e}}\n\\newcommand{\\one}{\\mathbf{1}}\n\\newcommand{\\zero}{\\mathbf{0}}\n\\]\nThis section describes how to use MLlib’s tooling for tuning ML algorithms and Pipelines.\nBuilt-in Cross-Validation and other tooling allow users to optimize hyperparameters in algorithms and Pipelines.\nTable of contents\nModel selection (a.k.a. hyperparameter tuning)\nCross-Validation\nTrain-Validation Split\nModel selection (a.k.a. hyperparameter tuning)\nAn important task in ML is\nmodel selection\n, or using data to find the best model or parameters for a given task.  This is also called\ntuning\n.\nTuning may be done for individual\nEstimator\ns such as\nLogisticRegression\n, or for entire\nPipeline\ns which include multiple algorithms, featurization, and other steps.  Users can tune an entire\nPipeline\nat once, rather than tuning each element in the\nPipeline\nseparately.\nMLlib supports model selection using tools such as\nCrossValidator\nand\nTrainValidationSplit\n.\nThese tools require the following items:\nEstimator\n: algorithm or\nPipeline\nto tune\nSet of\nParamMap\ns: parameters to choose from, sometimes called a “parameter grid” to search over\nEvaluator\n: metric to measure how well a fitted\nModel\ndoes on held-out test data\nAt a high level, these model selection tools work as follows:\nThey split the input data into separate training and test datasets.\nFor each (training, test) pair, they iterate through the set of\nParamMap\ns:\nFor each\nParamMap\n, they fit the\nEstimator\nusing those parameters, get the fitted\nModel\n, and evaluate the\nModel\n’s performance using the\nEvaluator\n.\nThey select the\nModel\nproduced by the best-performing set of parameters.\nThe\nEvaluator\ncan be a\nRegressionEvaluator\nfor regression problems, a\nBinaryClassificationEvaluator\nfor binary data, a\nMulticlassClassificationEvaluator\nfor multiclass problems, a\nMultilabelClassificationEvaluator\nfor multi-label classifications, or a\nRankingEvaluator\nfor ranking problems. The default metric used to\nchoose the best\nParamMap\ncan be overridden by the\nsetMetricName\nmethod in each of these evaluators.\nTo help construct the parameter grid, users can use the\nParamGridBuilder\nutility.\nBy default, sets of parameters from the parameter grid are evaluated in serial. Parameter evaluation can be done in parallel by setting\nparallelism\nwith a value of 2 or more (a value of 1 will be serial) before running model selection with\nCrossValidator\nor\nTrainValidationSplit\n.\nThe value of\nparallelism\nshould be chosen carefully to maximize parallelism without exceeding cluster resources, and larger values may not always lead to improved performance.  Generally speaking, a value up to 10 should be sufficient for most clusters.\nCross-Validation\nCrossValidator\nbegins by splitting the dataset into a set of\nfolds\nwhich are used as separate training and test datasets. E.g., with\n$k=3$\nfolds,\nCrossValidator\nwill generate 3 (training, test) dataset pairs, each of which uses 2/3 of the data for training and 1/3 for testing.  To evaluate a particular\nParamMap\n,\nCrossValidator\ncomputes the average evaluation metric for the 3\nModel\ns produced by fitting the\nEstimator\non the 3 different (training, test) dataset pairs.\nAfter identifying the best\nParamMap\n,\nCrossValidator\nfinally re-fits the\nEstimator\nusing the best\nParamMap\nand the entire dataset.\nExamples: model selection via cross-validation\nThe following example demonstrates using\nCrossValidator\nto select from a grid of parameters.\nNote that cross-validation over a grid of parameters is expensive.\nE.g., in the example below, the parameter grid has 3 values for\nhashingTF.numFeatures\nand 2 values for\nlr.regParam\n, and\nCrossValidator\nuses 2 folds.  This multiplies out to\n$(3 \\times 2) \\times 2 = 12$\ndifferent models being trained.\nIn realistic settings, it can be common to try many more parameters and use more folds (\n$k=3$\nand\n$k=10$\nare common).\nIn other words, using\nCrossValidator\ncan be very expensive.\nHowever, it is also a well-established method for choosing parameters which is more statistically sound than heuristic hand-tuning.\nRefer to the\nCrossValidator\nPython docs\nfor more details on the API.\nfrom\npyspark.ml\nimport\nPipeline\nfrom\npyspark.ml.classification\nimport\nLogisticRegression\nfrom\npyspark.ml.evaluation\nimport\nBinaryClassificationEvaluator\nfrom\npyspark.ml.feature\nimport\nHashingTF\n,\nTokenizer\nfrom\npyspark.ml.tuning\nimport\nCrossValidator\n,\nParamGridBuilder\n# Prepare training documents, which are labeled.\ntraining\n=\nspark\n.\ncreateDataFrame\n([\n(\n0\n,\n\"\na b c d e spark\n\"\n,\n1.0\n),\n(\n1\n,\n\"\nb d\n\"\n,\n0.0\n),\n(\n2\n,\n\"\nspark f g h\n\"\n,\n1.0\n),\n(\n3\n,\n\"\nhadoop mapreduce\n\"\n,\n0.0\n),\n(\n4\n,\n\"\nb spark who\n\"\n,\n1.0\n),\n(\n5\n,\n\"\ng d a y\n\"\n,\n0.0\n),\n(\n6\n,\n\"\nspark fly\n\"\n,\n1.0\n),\n(\n7\n,\n\"\nwas mapreduce\n\"\n,\n0.0\n),\n(\n8\n,\n\"\ne spark program\n\"\n,\n1.0\n),\n(\n9\n,\n\"\na e c l\n\"\n,\n0.0\n),\n(\n10\n,\n\"\nspark compile\n\"\n,\n1.0\n),\n(\n11\n,\n\"\nhadoop software\n\"\n,\n0.0\n)\n],\n[\n\"\nid\n\"\n,\n\"\ntext\n\"\n,\n\"\nlabel\n\"\n])\n# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\ntokenizer\n=\nTokenizer\n(\ninputCol\n=\n\"\ntext\n\"\n,\noutputCol\n=\n\"\nwords\n\"\n)\nhashingTF\n=\nHashingTF\n(\ninputCol\n=\ntokenizer\n.\ngetOutputCol\n(),\noutputCol\n=\n\"\nfeatures\n\"\n)\nlr\n=\nLogisticRegression\n(\nmaxIter\n=\n10\n)\npipeline\n=\nPipeline\n(\nstages\n=\n[\ntokenizer\n,\nhashingTF\n,\nlr\n])\n# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n# This will allow us to jointly choose parameters for all Pipeline stages.\n# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n# We use a ParamGridBuilder to construct a grid of parameters to search over.\n# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\nparamGrid\n=\nParamGridBuilder\n()\n\\\n.\naddGrid\n(\nhashingTF\n.\nnumFeatures\n,\n[\n10\n,\n100\n,\n1000\n])\n\\\n.\naddGrid\n(\nlr\n.\nregParam\n,\n[\n0.1\n,\n0.01\n])\n\\\n.\nbuild\n()\ncrossval\n=\nCrossValidator\n(\nestimator\n=\npipeline\n,\nestimatorParamMaps\n=\nparamGrid\n,\nevaluator\n=\nBinaryClassificationEvaluator\n(),\nnumFolds\n=\n2\n)\n# use 3+ folds in practice\n# Run cross-validation, and choose the best set of parameters.\ncvModel\n=\ncrossval\n.\nfit\n(\ntraining\n)\n# Prepare test documents, which are unlabeled.\ntest\n=\nspark\n.\ncreateDataFrame\n([\n(\n4\n,\n\"\nspark i j k\n\"\n),\n(\n5\n,\n\"\nl m n\n\"\n),\n(\n6\n,\n\"\nmapreduce spark\n\"\n),\n(\n7\n,\n\"\napache hadoop\n\"\n)\n],\n[\n\"\nid\n\"\n,\n\"\ntext\n\"\n])\n# Make predictions on test documents. cvModel uses the best model found (lrModel).\nprediction\n=\ncvModel\n.\ntransform\n(\ntest\n)\nselected\n=\nprediction\n.\nselect\n(\n\"\nid\n\"\n,\n\"\ntext\n\"\n,\n\"\nprobability\n\"\n,\n\"\nprediction\n\"\n)\nfor\nrow\nin\nselected\n.\ncollect\n():\nprint\n(\nrow\n)\nFind full example code at \"examples/src/main/python/ml/cross_validator.py\" in the Spark repo.\nRefer to the\nCrossValidator\nScala docs\nfor details on the API.\nimport\norg.apache.spark.ml.Pipeline\nimport\norg.apache.spark.ml.classification.LogisticRegression\nimport\norg.apache.spark.ml.evaluation.BinaryClassificationEvaluator\nimport\norg.apache.spark.ml.feature.\n{\nHashingTF\n,\nTokenizer\n}\nimport\norg.apache.spark.ml.linalg.Vector\nimport\norg.apache.spark.ml.tuning.\n{\nCrossValidator\n,\nParamGridBuilder\n}\nimport\norg.apache.spark.sql.Row\n// Prepare training data from a list of (id, text, label) tuples.\nval\ntraining\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0L\n,\n\"a b c d e spark\"\n,\n1.0\n),\n(\n1L\n,\n\"b d\"\n,\n0.0\n),\n(\n2L\n,\n\"spark f g h\"\n,\n1.0\n),\n(\n3L\n,\n\"hadoop mapreduce\"\n,\n0.0\n),\n(\n4L\n,\n\"b spark who\"\n,\n1.0\n),\n(\n5L\n,\n\"g d a y\"\n,\n0.0\n),\n(\n6L\n,\n\"spark fly\"\n,\n1.0\n),\n(\n7L\n,\n\"was mapreduce\"\n,\n0.0\n),\n(\n8L\n,\n\"e spark program\"\n,\n1.0\n),\n(\n9L\n,\n\"a e c l\"\n,\n0.0\n),\n(\n10L\n,\n\"spark compile\"\n,\n1.0\n),\n(\n11L\n,\n\"hadoop software\"\n,\n0.0\n)\n)).\ntoDF\n(\n\"id\"\n,\n\"text\"\n,\n\"label\"\n)\n// Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\nval\ntokenizer\n=\nnew\nTokenizer\n()\n.\nsetInputCol\n(\n\"text\"\n)\n.\nsetOutputCol\n(\n\"words\"\n)\nval\nhashingTF\n=\nnew\nHashingTF\n()\n.\nsetInputCol\n(\ntokenizer\n.\ngetOutputCol\n)\n.\nsetOutputCol\n(\n\"features\"\n)\nval\nlr\n=\nnew\nLogisticRegression\n()\n.\nsetMaxIter\n(\n10\n)\nval\npipeline\n=\nnew\nPipeline\n()\n.\nsetStages\n(\nArray\n(\ntokenizer\n,\nhashingTF\n,\nlr\n))\n// We use a ParamGridBuilder to construct a grid of parameters to search over.\n// With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n// this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\nval\nparamGrid\n=\nnew\nParamGridBuilder\n()\n.\naddGrid\n(\nhashingTF\n.\nnumFeatures\n,\nArray\n(\n10\n,\n100\n,\n1000\n))\n.\naddGrid\n(\nlr\n.\nregParam\n,\nArray\n(\n0.1\n,\n0.01\n))\n.\nbuild\n()\n// We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n// This will allow us to jointly choose parameters for all Pipeline stages.\n// A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n// Note that the evaluator here is a BinaryClassificationEvaluator and its default metric\n// is areaUnderROC.\nval\ncv\n=\nnew\nCrossValidator\n()\n.\nsetEstimator\n(\npipeline\n)\n.\nsetEvaluator\n(\nnew\nBinaryClassificationEvaluator\n)\n.\nsetEstimatorParamMaps\n(\nparamGrid\n)\n.\nsetNumFolds\n(\n2\n)\n// Use 3+ in practice\n.\nsetParallelism\n(\n2\n)\n// Evaluate up to 2 parameter settings in parallel\n// Run cross-validation, and choose the best set of parameters.\nval\ncvModel\n=\ncv\n.\nfit\n(\ntraining\n)\n// Prepare test documents, which are unlabeled (id, text) tuples.\nval\ntest\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n4L\n,\n\"spark i j k\"\n),\n(\n5L\n,\n\"l m n\"\n),\n(\n6L\n,\n\"mapreduce spark\"\n),\n(\n7L\n,\n\"apache hadoop\"\n)\n)).\ntoDF\n(\n\"id\"\n,\n\"text\"\n)\n// Make predictions on test documents. cvModel uses the best model found (lrModel).\ncvModel\n.\ntransform\n(\ntest\n)\n.\nselect\n(\n\"id\"\n,\n\"text\"\n,\n\"probability\"\n,\n\"prediction\"\n)\n.\ncollect\n()\n.\nforeach\n{\ncase\nRow\n(\nid\n:\nLong\n,\ntext\n:\nString\n,\nprob\n:\nVector\n,\nprediction\n:\nDouble\n)\n=>\nprintln\n(\ns\n\"($id, $text) --> prob=$prob, prediction=$prediction\"\n)\n}\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\" in the Spark repo.\nRefer to the\nCrossValidator\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\norg.apache.spark.ml.Pipeline\n;\nimport\norg.apache.spark.ml.PipelineStage\n;\nimport\norg.apache.spark.ml.classification.LogisticRegression\n;\nimport\norg.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n;\nimport\norg.apache.spark.ml.feature.HashingTF\n;\nimport\norg.apache.spark.ml.feature.Tokenizer\n;\nimport\norg.apache.spark.ml.param.ParamMap\n;\nimport\norg.apache.spark.ml.tuning.CrossValidator\n;\nimport\norg.apache.spark.ml.tuning.CrossValidatorModel\n;\nimport\norg.apache.spark.ml.tuning.ParamGridBuilder\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\n// Prepare training documents, which are labeled.\nDataset\n<\nRow\n>\ntraining\n=\nspark\n.\ncreateDataFrame\n(\nArrays\n.\nasList\n(\nnew\nJavaLabeledDocument\n(\n0L\n,\n\"a b c d e spark\"\n,\n1.0\n),\nnew\nJavaLabeledDocument\n(\n1L\n,\n\"b d\"\n,\n0.0\n),\nnew\nJavaLabeledDocument\n(\n2L\n,\n\"spark f g h\"\n,\n1.0\n),\nnew\nJavaLabeledDocument\n(\n3L\n,\n\"hadoop mapreduce\"\n,\n0.0\n),\nnew\nJavaLabeledDocument\n(\n4L\n,\n\"b spark who\"\n,\n1.0\n),\nnew\nJavaLabeledDocument\n(\n5L\n,\n\"g d a y\"\n,\n0.0\n),\nnew\nJavaLabeledDocument\n(\n6L\n,\n\"spark fly\"\n,\n1.0\n),\nnew\nJavaLabeledDocument\n(\n7L\n,\n\"was mapreduce\"\n,\n0.0\n),\nnew\nJavaLabeledDocument\n(\n8L\n,\n\"e spark program\"\n,\n1.0\n),\nnew\nJavaLabeledDocument\n(\n9L\n,\n\"a e c l\"\n,\n0.0\n),\nnew\nJavaLabeledDocument\n(\n10L\n,\n\"spark compile\"\n,\n1.0\n),\nnew\nJavaLabeledDocument\n(\n11L\n,\n\"hadoop software\"\n,\n0.0\n)\n),\nJavaLabeledDocument\n.\nclass\n);\n// Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\nTokenizer\ntokenizer\n=\nnew\nTokenizer\n()\n.\nsetInputCol\n(\n\"text\"\n)\n.\nsetOutputCol\n(\n\"words\"\n);\nHashingTF\nhashingTF\n=\nnew\nHashingTF\n()\n.\nsetNumFeatures\n(\n1000\n)\n.\nsetInputCol\n(\ntokenizer\n.\ngetOutputCol\n())\n.\nsetOutputCol\n(\n\"features\"\n);\nLogisticRegression\nlr\n=\nnew\nLogisticRegression\n()\n.\nsetMaxIter\n(\n10\n)\n.\nsetRegParam\n(\n0.01\n);\nPipeline\npipeline\n=\nnew\nPipeline\n()\n.\nsetStages\n(\nnew\nPipelineStage\n[]\n{\ntokenizer\n,\nhashingTF\n,\nlr\n});\n// We use a ParamGridBuilder to construct a grid of parameters to search over.\n// With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n// this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\nParamMap\n[]\nparamGrid\n=\nnew\nParamGridBuilder\n()\n.\naddGrid\n(\nhashingTF\n.\nnumFeatures\n(),\nnew\nint\n[]\n{\n10\n,\n100\n,\n1000\n})\n.\naddGrid\n(\nlr\n.\nregParam\n(),\nnew\ndouble\n[]\n{\n0.1\n,\n0.01\n})\n.\nbuild\n();\n// We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n// This will allow us to jointly choose parameters for all Pipeline stages.\n// A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n// Note that the evaluator here is a BinaryClassificationEvaluator and its default metric\n// is areaUnderROC.\nCrossValidator\ncv\n=\nnew\nCrossValidator\n()\n.\nsetEstimator\n(\npipeline\n)\n.\nsetEvaluator\n(\nnew\nBinaryClassificationEvaluator\n())\n.\nsetEstimatorParamMaps\n(\nparamGrid\n)\n.\nsetNumFolds\n(\n2\n)\n// Use 3+ in practice\n.\nsetParallelism\n(\n2\n);\n// Evaluate up to 2 parameter settings in parallel\n// Run cross-validation, and choose the best set of parameters.\nCrossValidatorModel\ncvModel\n=\ncv\n.\nfit\n(\ntraining\n);\n// Prepare test documents, which are unlabeled.\nDataset\n<\nRow\n>\ntest\n=\nspark\n.\ncreateDataFrame\n(\nArrays\n.\nasList\n(\nnew\nJavaDocument\n(\n4L\n,\n\"spark i j k\"\n),\nnew\nJavaDocument\n(\n5L\n,\n\"l m n\"\n),\nnew\nJavaDocument\n(\n6L\n,\n\"mapreduce spark\"\n),\nnew\nJavaDocument\n(\n7L\n,\n\"apache hadoop\"\n)\n),\nJavaDocument\n.\nclass\n);\n// Make predictions on test documents. cvModel uses the best model found (lrModel).\nDataset\n<\nRow\n>\npredictions\n=\ncvModel\n.\ntransform\n(\ntest\n);\nfor\n(\nRow\nr\n:\npredictions\n.\nselect\n(\n\"id\"\n,\n\"text\"\n,\n\"probability\"\n,\n\"prediction\"\n).\ncollectAsList\n())\n{\nSystem\n.\nout\n.\nprintln\n(\n\"(\"\n+\nr\n.\nget\n(\n0\n)\n+\n\", \"\n+\nr\n.\nget\n(\n1\n)\n+\n\") --> prob=\"\n+\nr\n.\nget\n(\n2\n)\n+\n\", prediction=\"\n+\nr\n.\nget\n(\n3\n));\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\" in the Spark repo.\nTrain-Validation Split\nIn addition to\nCrossValidator\nSpark also offers\nTrainValidationSplit\nfor hyper-parameter tuning.\nTrainValidationSplit\nonly evaluates each combination of parameters once, as opposed to k times in\n the case of\nCrossValidator\n. It is, therefore, less expensive,\n but will not produce as reliable results when the training dataset is not sufficiently large.\nUnlike\nCrossValidator\n,\nTrainValidationSplit\ncreates a single (training, test) dataset pair.\nIt splits the dataset into these two parts using the\ntrainRatio\nparameter. For example with\n$trainRatio=0.75$\n,\nTrainValidationSplit\nwill generate a training and test dataset pair where 75% of the data is used for training and 25% for validation.\nLike\nCrossValidator\n,\nTrainValidationSplit\nfinally fits the\nEstimator\nusing the best\nParamMap\nand the entire dataset.\nExamples: model selection via train validation split\nRefer to the\nTrainValidationSplit\nPython docs\nfor more details on the API.\nfrom\npyspark.ml.evaluation\nimport\nRegressionEvaluator\nfrom\npyspark.ml.regression\nimport\nLinearRegression\nfrom\npyspark.ml.tuning\nimport\nParamGridBuilder\n,\nTrainValidationSplit\n# Prepare training and test data.\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n)\n\\\n.\nload\n(\n\"\ndata/mllib/sample_linear_regression_data.txt\n\"\n)\ntrain\n,\ntest\n=\ndata\n.\nrandomSplit\n([\n0.9\n,\n0.1\n],\nseed\n=\n12345\n)\nlr\n=\nLinearRegression\n(\nmaxIter\n=\n10\n)\n# We use a ParamGridBuilder to construct a grid of parameters to search over.\n# TrainValidationSplit will try all combinations of values and determine best model using\n# the evaluator.\nparamGrid\n=\nParamGridBuilder\n()\n\\\n.\naddGrid\n(\nlr\n.\nregParam\n,\n[\n0.1\n,\n0.01\n])\n\\\n.\naddGrid\n(\nlr\n.\nfitIntercept\n,\n[\nFalse\n,\nTrue\n])\n\\\n.\naddGrid\n(\nlr\n.\nelasticNetParam\n,\n[\n0.0\n,\n0.5\n,\n1.0\n])\n\\\n.\nbuild\n()\n# In this case the estimator is simply the linear regression.\n# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\ntvs\n=\nTrainValidationSplit\n(\nestimator\n=\nlr\n,\nestimatorParamMaps\n=\nparamGrid\n,\nevaluator\n=\nRegressionEvaluator\n(),\n# 80% of the data will be used for training, 20% for validation.\ntrainRatio\n=\n0.8\n)\n# Run TrainValidationSplit, and choose the best set of parameters.\nmodel\n=\ntvs\n.\nfit\n(\ntrain\n)\n# Make predictions on test data. model is the model with combination of parameters\n# that performed best.\nmodel\n.\ntransform\n(\ntest\n)\n\\\n.\nselect\n(\n\"\nfeatures\n\"\n,\n\"\nlabel\n\"\n,\n\"\nprediction\n\"\n)\n\\\n.\nshow\n()\nFind full example code at \"examples/src/main/python/ml/train_validation_split.py\" in the Spark repo.\nRefer to the\nTrainValidationSplit\nScala docs\nfor details on the API.\nimport\norg.apache.spark.ml.evaluation.RegressionEvaluator\nimport\norg.apache.spark.ml.regression.LinearRegression\nimport\norg.apache.spark.ml.tuning.\n{\nParamGridBuilder\n,\nTrainValidationSplit\n}\n// Prepare training and test data.\nval\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_linear_regression_data.txt\"\n)\nval\nArray\n(\ntraining\n,\ntest\n)\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.9\n,\n0.1\n),\nseed\n=\n12345\n)\nval\nlr\n=\nnew\nLinearRegression\n()\n.\nsetMaxIter\n(\n10\n)\n// We use a ParamGridBuilder to construct a grid of parameters to search over.\n// TrainValidationSplit will try all combinations of values and determine best model using\n// the evaluator.\nval\nparamGrid\n=\nnew\nParamGridBuilder\n()\n.\naddGrid\n(\nlr\n.\nregParam\n,\nArray\n(\n0.1\n,\n0.01\n))\n.\naddGrid\n(\nlr\n.\nfitIntercept\n)\n.\naddGrid\n(\nlr\n.\nelasticNetParam\n,\nArray\n(\n0.0\n,\n0.5\n,\n1.0\n))\n.\nbuild\n()\n// In this case the estimator is simply the linear regression.\n// A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\nval\ntrainValidationSplit\n=\nnew\nTrainValidationSplit\n()\n.\nsetEstimator\n(\nlr\n)\n.\nsetEvaluator\n(\nnew\nRegressionEvaluator\n)\n.\nsetEstimatorParamMaps\n(\nparamGrid\n)\n// 80% of the data will be used for training and the remaining 20% for validation.\n.\nsetTrainRatio\n(\n0.8\n)\n// Evaluate up to 2 parameter settings in parallel\n.\nsetParallelism\n(\n2\n)\n// Run train validation split, and choose the best set of parameters.\nval\nmodel\n=\ntrainValidationSplit\n.\nfit\n(\ntraining\n)\n// Make predictions on test data. model is the model with combination of parameters\n// that performed best.\nmodel\n.\ntransform\n(\ntest\n)\n.\nselect\n(\n\"features\"\n,\n\"label\"\n,\n\"prediction\"\n)\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\" in the Spark repo.\nRefer to the\nTrainValidationSplit\nJava docs\nfor details on the API.\nimport\norg.apache.spark.ml.evaluation.RegressionEvaluator\n;\nimport\norg.apache.spark.ml.param.ParamMap\n;\nimport\norg.apache.spark.ml.regression.LinearRegression\n;\nimport\norg.apache.spark.ml.tuning.ParamGridBuilder\n;\nimport\norg.apache.spark.ml.tuning.TrainValidationSplit\n;\nimport\norg.apache.spark.ml.tuning.TrainValidationSplitModel\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nDataset\n<\nRow\n>\ndata\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_linear_regression_data.txt\"\n);\n// Prepare training and test data.\nDataset\n<\nRow\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]\n{\n0.9\n,\n0.1\n},\n12345\n);\nDataset\n<\nRow\n>\ntraining\n=\nsplits\n[\n0\n];\nDataset\n<\nRow\n>\ntest\n=\nsplits\n[\n1\n];\nLinearRegression\nlr\n=\nnew\nLinearRegression\n();\n// We use a ParamGridBuilder to construct a grid of parameters to search over.\n// TrainValidationSplit will try all combinations of values and determine best model using\n// the evaluator.\nParamMap\n[]\nparamGrid\n=\nnew\nParamGridBuilder\n()\n.\naddGrid\n(\nlr\n.\nregParam\n(),\nnew\ndouble\n[]\n{\n0.1\n,\n0.01\n})\n.\naddGrid\n(\nlr\n.\nfitIntercept\n())\n.\naddGrid\n(\nlr\n.\nelasticNetParam\n(),\nnew\ndouble\n[]\n{\n0.0\n,\n0.5\n,\n1.0\n})\n.\nbuild\n();\n// In this case the estimator is simply the linear regression.\n// A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\nTrainValidationSplit\ntrainValidationSplit\n=\nnew\nTrainValidationSplit\n()\n.\nsetEstimator\n(\nlr\n)\n.\nsetEvaluator\n(\nnew\nRegressionEvaluator\n())\n.\nsetEstimatorParamMaps\n(\nparamGrid\n)\n.\nsetTrainRatio\n(\n0.8\n)\n// 80% for training and the remaining 20% for validation\n.\nsetParallelism\n(\n2\n);\n// Evaluate up to 2 parameter settings in parallel\n// Run train validation split, and choose the best set of parameters.\nTrainValidationSplitModel\nmodel\n=\ntrainValidationSplit\n.\nfit\n(\ntraining\n);\n// Make predictions on test data. model is the model with combination of parameters\n// that performed best.\nmodel\n.\ntransform\n(\ntest\n)\n.\nselect\n(\n\"features\"\n,\n\"label\"\n,\n\"prediction\"\n)\n.\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\" in the Spark repo."}
{"url": "https://spark.apache.org/docs/latest/ml-advanced.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nAdvanced topics\nOptimization of linear methods (developer)\nLimited-memory BFGS (L-BFGS)\nNormal equation solver for weighted least squares\nIteratively reweighted least squares (IRLS)\n\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}} \n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\wv}{\\mathbf{w}}\n\\newcommand{\\av}{\\mathbf{\\alpha}}\n\\newcommand{\\bv}{\\mathbf{b}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\id}{\\mathbf{I}} \n\\newcommand{\\ind}{\\mathbf{1}} \n\\newcommand{\\0}{\\mathbf{0}} \n\\newcommand{\\unit}{\\mathbf{e}} \n\\newcommand{\\one}{\\mathbf{1}} \n\\newcommand{\\zero}{\\mathbf{0}}\n\\]\nOptimization of linear methods (developer)\nLimited-memory BFGS (L-BFGS)\nL-BFGS\nis an optimization \nalgorithm in the family of quasi-Newton methods to solve the optimization problems of the form\n$\\min_{\\wv \\in\\R^d} \\; f(\\wv)$\n. The L-BFGS method approximates the objective function locally as a \nquadratic without evaluating the second partial derivatives of the objective function to construct the \nHessian matrix. The Hessian matrix is approximated by previous gradient evaluations, so there is no \nvertical scalability issue (the number of training features) unlike computing the Hessian matrix \nexplicitly in Newton’s method. As a result, L-BFGS often achieves faster convergence compared with \nother first-order optimizations.\nOrthant-Wise Limited-memory\nQuasi-Newton\n(OWL-QN) is an extension of L-BFGS that can effectively handle L1 and elastic net regularization.\nL-BFGS is used as a solver for\nLinearRegression\n,\nLogisticRegression\n,\nAFTSurvivalRegression\nand\nMultilayerPerceptronClassifier\n.\nMLlib L-BFGS solver calls the corresponding implementation in\nbreeze\n.\nNormal equation solver for weighted least squares\nMLlib implements normal equation solver for\nweighted least squares\nby\nWeightedLeastSquares\n.\nGiven $n$ weighted observations $(w_i, a_i, b_i)$:\n$w_i$ the weight of i-th observation\n$a_i$ the features vector of i-th observation\n$b_i$ the label of i-th observation\nThe number of features for each observation is $m$. We use the following weighted least squares formulation:\n\\[   \n\\min_{\\mathbf{x}}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(\\mathbf{a}_i^T \\mathbf{x} -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{\\lambda}{\\delta}\\left[\\frac{1}{2}(1 - \\alpha)\\sum_{j=1}^m(\\sigma_j x_j)^2 + \\alpha\\sum_{j=1}^m |\\sigma_j x_j|\\right]\n\\]\nwhere $\\lambda$ is the regularization parameter, $\\alpha$ is the elastic-net mixing parameter, $\\delta$ is the population standard deviation of the label\nand $\\sigma_j$ is the population standard deviation of the j-th feature column.\nThis objective function requires only one pass over the data to collect the statistics necessary to solve it. For an\n$n \\times m$ data matrix, these statistics require only $O(m^2)$ storage and so can be stored on a single machine when $m$ (the number of features) is\nrelatively small. We can then solve the normal equations on a single machine using local methods like direct Cholesky factorization or iterative optimization programs.\nSpark MLlib currently supports two types of solvers for the normal equations: Cholesky factorization and Quasi-Newton methods (L-BFGS/OWL-QN). Cholesky factorization\ndepends on a positive definite covariance matrix (i.e. columns of the data matrix must be linearly independent) and will fail if this condition is violated. Quasi-Newton methods\nare still capable of providing a reasonable solution even when the covariance matrix is not positive definite, so the normal equation solver can also fall back to \nQuasi-Newton methods in this case. This fallback is currently always enabled for the\nLinearRegression\nand\nGeneralizedLinearRegression\nestimators.\nWeightedLeastSquares\nsupports L1, L2, and elastic-net regularization and provides options to enable or disable regularization and standardization. In the case where no \nL1 regularization is applied (i.e. $\\alpha = 0$), there exists an analytical solution and either Cholesky or Quasi-Newton solver may be used. When $\\alpha > 0$ no analytical \nsolution exists and we instead use the Quasi-Newton solver to find the coefficients iteratively.\nIn order to make the normal equation approach efficient,\nWeightedLeastSquares\nrequires that the number of features is no more than 4096. For larger problems, use L-BFGS instead.\nIteratively reweighted least squares (IRLS)\nMLlib implements\niteratively reweighted least squares (IRLS)\nby\nIterativelyReweightedLeastSquares\n.\nIt can be used to find the maximum likelihood estimates of a generalized linear model (GLM), find M-estimator in robust regression and other optimization problems.\nRefer to\nIteratively Reweighted Least Squares for Maximum Likelihood Estimation, and some Robust and Resistant Alternatives\nfor more information.\nIt solves certain optimization problems iteratively through the following procedure:\nlinearize the objective at current solution and update corresponding weight.\nsolve a weighted least squares (WLS) problem by WeightedLeastSquares.\nrepeat above steps until convergence.\nSince it involves solving a weighted least squares (WLS) problem by\nWeightedLeastSquares\nin each iteration,\nit also requires the number of features to be no more than 4096.\nCurrently IRLS is used as the default solver of\nGeneralizedLinearRegression\n."}
{"url": "https://spark.apache.org/docs/latest/streaming-custom-receivers.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark Streaming Custom Receivers\nSpark Streaming can receive streaming data from any arbitrary data source beyond\nthe ones for which it has built-in support (that is, beyond Kafka, Kinesis, files, sockets, etc.).\nThis requires the developer to implement a\nreceiver\nthat is customized for receiving data from\nthe concerned data source. This guide walks through the process of implementing a custom receiver\nand using it in a Spark Streaming application. Note that custom receivers can be implemented\nin Scala or Java.\nImplementing a Custom Receiver\nThis starts with implementing a\nReceiver\n(\nScala doc\n,\nJava doc\n).\nA custom receiver must extend this abstract class by implementing two methods\nonStart()\n: Things to do to start receiving data.\nonStop()\n: Things to do to stop receiving data.\nBoth\nonStart()\nand\nonStop()\nmust not block indefinitely. Typically,\nonStart()\nwould start the threads\nthat are responsible for receiving the data, and\nonStop()\nwould ensure that these threads receiving the data\nare stopped. The receiving threads can also use\nisStopped()\n, a\nReceiver\nmethod, to check whether they\nshould stop receiving data.\nOnce the data is received, that data can be stored inside Spark\nby calling\nstore(data)\n, which is a method provided by the Receiver class.\nThere are a number of flavors of\nstore()\nwhich allow one to store the received data\nrecord-at-a-time or as whole collection of objects / serialized bytes. Note that the flavor of\nstore()\nused to implement a receiver affects its reliability and fault-tolerance semantics.\nThis is discussed\nlater\nin more detail.\nAny exception in the receiving threads should be caught and handled properly to avoid silent\nfailures of the receiver.\nrestart(<exception>)\nwill restart the receiver by\nasynchronously calling\nonStop()\nand then calling\nonStart()\nafter a delay.\nstop(<exception>)\nwill call\nonStop()\nand terminate the receiver. Also,\nreportError(<error>)\nreports an error message to the driver (visible in the logs and UI) without stopping / restarting\nthe receiver.\nThe following is a custom receiver that receives a stream of text over a socket. It treats\n‘\\n’ delimited lines in the text stream as records and stores them with Spark. If the receiving thread\nhas any error connecting or receiving, the receiver is restarted to make another attempt to connect.\nclass\nCustomReceiver\n(\nhost\n:\nString\n,\nport\n:\nInt\n)\nextends\nReceiver\n[\nString\n](\nStorageLevel\n.\nMEMORY_AND_DISK_2\n)\nwith\nLogging\n{\ndef\nonStart\n()\n{\n// Start the thread that receives data over a connection\nnew\nThread\n(\n\"Socket Receiver\"\n)\n{\noverride\ndef\nrun\n()\n{\nreceive\n()\n}\n}.\nstart\n()\n}\ndef\nonStop\n()\n{\n// There is nothing much to do as the thread calling receive()\n// is designed to stop by itself if isStopped() returns false\n}\n/** Create a socket connection and receive data until receiver is stopped */\nprivate\ndef\nreceive\n()\n{\nvar\nsocket\n:\nSocket\n=\nnull\nvar\nuserInput\n:\nString\n=\nnull\ntry\n{\n// Connect to host:port\nsocket\n=\nnew\nSocket\n(\nhost\n,\nport\n)\n// Until stopped or connection broken continue reading\nval\nreader\n=\nnew\nBufferedReader\n(\nnew\nInputStreamReader\n(\nsocket\n.\ngetInputStream\n(),\nStandardCharsets\n.\nUTF_8\n))\nuserInput\n=\nreader\n.\nreadLine\n()\nwhile\n(!\nisStopped\n&&\nuserInput\n!=\nnull\n)\n{\nstore\n(\nuserInput\n)\nuserInput\n=\nreader\n.\nreadLine\n()\n}\nreader\n.\nclose\n()\nsocket\n.\nclose\n()\n// Restart in an attempt to connect again when server is active again\nrestart\n(\n\"Trying to connect again\"\n)\n}\ncatch\n{\ncase\ne\n:\njava.net.ConnectException\n=>\n// restart if could not connect to server\nrestart\n(\n\"Error connecting to \"\n+\nhost\n+\n\":\"\n+\nport\n,\ne\n)\ncase\nt\n:\nThrowable\n=>\n// restart if there is any other error\nrestart\n(\n\"Error receiving data\"\n,\nt\n)\n}\n}\n}\npublic\nclass\nJavaCustomReceiver\nextends\nReceiver\n<\nString\n>\n{\nString\nhost\n=\nnull\n;\nint\nport\n=\n-\n1\n;\npublic\nJavaCustomReceiver\n(\nString\nhost_\n,\nint\nport_\n)\n{\nsuper\n(\nStorageLevel\n.\nMEMORY_AND_DISK_2\n());\nhost\n=\nhost_\n;\nport\n=\nport_\n;\n}\n@Override\npublic\nvoid\nonStart\n()\n{\n// Start the thread that receives data over a connection\nnew\nThread\n(\nthis\n::\nreceive\n).\nstart\n();\n}\n@Override\npublic\nvoid\nonStop\n()\n{\n// There is nothing much to do as the thread calling receive()\n// is designed to stop by itself if isStopped() returns false\n}\n/** Create a socket connection and receive data until receiver is stopped */\nprivate\nvoid\nreceive\n()\n{\nSocket\nsocket\n=\nnull\n;\nString\nuserInput\n=\nnull\n;\ntry\n{\n// connect to the server\nsocket\n=\nnew\nSocket\n(\nhost\n,\nport\n);\nBufferedReader\nreader\n=\nnew\nBufferedReader\n(\nnew\nInputStreamReader\n(\nsocket\n.\ngetInputStream\n(),\nStandardCharsets\n.\nUTF_8\n));\n// Until stopped or connection broken continue reading\nwhile\n(!\nisStopped\n()\n&&\n(\nuserInput\n=\nreader\n.\nreadLine\n())\n!=\nnull\n)\n{\nSystem\n.\nout\n.\nprintln\n(\n\"Received data '\"\n+\nuserInput\n+\n\"'\"\n);\nstore\n(\nuserInput\n);\n}\nreader\n.\nclose\n();\nsocket\n.\nclose\n();\n// Restart in an attempt to connect again when server is active again\nrestart\n(\n\"Trying to connect again\"\n);\n}\ncatch\n(\nConnectException\nce\n)\n{\n// restart if could not connect to server\nrestart\n(\n\"Could not connect\"\n,\nce\n);\n}\ncatch\n(\nThrowable\nt\n)\n{\n// restart if there is any other error\nrestart\n(\n\"Error receiving data\"\n,\nt\n);\n}\n}\n}\nUsing the custom receiver in a Spark Streaming application\nThe custom receiver can be used in a Spark Streaming application by using\nstreamingContext.receiverStream(<instance of custom receiver>)\n. This will create\nan input DStream using data received by the instance of custom receiver, as shown below:\n// Assuming ssc is the StreamingContext\nval\ncustomReceiverStream\n=\nssc\n.\nreceiverStream\n(\nnew\nCustomReceiver\n(\nhost\n,\nport\n))\nval\nwords\n=\ncustomReceiverStream\n.\nflatMap\n(\n_\n.\nsplit\n(\n\" \"\n))\n...\nThe full source code is in the example\nCustomReceiver.scala\n.\n// Assuming ssc is the JavaStreamingContext\nJavaDStream\n<\nString\n>\ncustomReceiverStream\n=\nssc\n.\nreceiverStream\n(\nnew\nJavaCustomReceiver\n(\nhost\n,\nport\n));\nJavaDStream\n<\nString\n>\nwords\n=\ncustomReceiverStream\n.\nflatMap\n(\ns\n->\n...);\n...\nThe full source code is in the example\nJavaCustomReceiver.java\n.\nReceiver Reliability\nAs discussed in brief in the\nSpark Streaming Programming Guide\n,\nthere are two kinds of receivers based on their reliability and fault-tolerance semantics.\nReliable Receiver\n- For\nreliable sources\nthat allow sent data to be acknowledged, a\nreliable receiver\ncorrectly acknowledges to the source that the data has been received\n  and stored in Spark reliably (that is, replicated successfully). Usually,\n  implementing this receiver involves careful consideration of the semantics of source\n  acknowledgements.\nUnreliable Receiver\n- An\nunreliable receiver\ndoes\nnot\nsend acknowledgement to a source. This can be used for sources that do not support acknowledgement, or even for reliable sources when one does not want or need to go into the complexity of acknowledgement.\nTo implement a\nreliable receiver\n, you have to use\nstore(multiple-records)\nto store data.\nThis flavor of\nstore\nis a blocking call which returns only after all the given records have\nbeen stored inside Spark. If the receiver’s configured storage level uses replication\n(enabled by default), then this call returns after replication has completed.\nThus it ensures that the data is reliably stored, and the receiver can now acknowledge the\nsource appropriately. This ensures that no data is lost when the receiver fails in the middle\nof replicating data – the buffered data will not be acknowledged and hence will be later resent\nby the source.\nAn\nunreliable receiver\ndoes not have to implement any of this logic. It can simply receive\nrecords from the source and insert them one-at-a-time using\nstore(single-record)\n. While it does\nnot get the reliability guarantees of\nstore(multiple-records)\n, it has the following advantages:\nThe system takes care of chunking that data into appropriate sized blocks (look for block\ninterval in the\nSpark Streaming Programming Guide\n).\nThe system takes care of controlling the receiving rates if the rate limits have been specified.\nBecause of these two, unreliable receivers are simpler to implement than reliable receivers.\nThe following table summarizes the characteristics of both types of receivers\nReceiver Type\nCharacteristics\nUnreliable Receivers\nSimple to implement.\nSystem takes care of block generation and rate control.\n    No fault-tolerance guarantees, can lose data on receiver failure.\nReliable Receivers\nStrong fault-tolerance guarantees, can ensure zero data loss.\nBlock generation and rate control to be handled by the receiver implementation.\nImplementation complexity depends on the acknowledgement mechanisms of the source."}
{"url": "https://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark Streaming + Kafka Integration Guide (Kafka broker version 0.10.0 or higher)\nThe Spark Streaming integration for Kafka 0.10 provides simple parallelism, 1:1 correspondence between Kafka \npartitions and Spark partitions, and access to offsets and metadata. However, because the newer integration uses \nthe\nnew Kafka consumer API\ninstead of the simple API, \nthere are notable differences in usage.\nLinking\nFor Scala/Java applications using SBT/Maven project definitions, link your streaming application with the following artifact (see\nLinking section\nin the main programming guide for further information).\ngroupId = org.apache.spark\nartifactId = spark-streaming-kafka-0-10_2.13\nversion = 4.0.0\nDo not\nmanually add dependencies on\norg.apache.kafka\nartifacts (e.g.\nkafka-clients\n).  The\nspark-streaming-kafka-0-10\nartifact has the appropriate transitive dependencies already, and different versions may be incompatible in hard to diagnose ways.\nCreating a Direct Stream\nNote that the namespace for the import includes the version, org.apache.spark.streaming.kafka010\nimport\norg.apache.kafka.clients.consumer.ConsumerRecord\nimport\norg.apache.kafka.common.serialization.StringDeserializer\nimport\norg.apache.spark.streaming.kafka010._\nimport\norg.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\nimport\norg.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\nval\nkafkaParams\n=\nMap\n[\nString\n,\nObject\n](\n\"bootstrap.servers\"\n->\n\"localhost:9092,anotherhost:9092\"\n,\n\"key.deserializer\"\n->\nclassOf\n[\nStringDeserializer\n],\n\"value.deserializer\"\n->\nclassOf\n[\nStringDeserializer\n],\n\"group.id\"\n->\n\"use_a_separate_group_id_for_each_stream\"\n,\n\"auto.offset.reset\"\n->\n\"latest\"\n,\n\"enable.auto.commit\"\n->\n(\nfalse\n:\njava.lang.Boolean\n)\n)\nval\ntopics\n=\nArray\n(\n\"topicA\"\n,\n\"topicB\"\n)\nval\nstream\n=\nKafkaUtils\n.\ncreateDirectStream\n[\nString\n,\nString\n](\nstreamingContext\n,\nPreferConsistent\n,\nSubscribe\n[\nString\n,\nString\n](\ntopics\n,\nkafkaParams\n)\n)\nstream\n.\nmap\n(\nrecord\n=>\n(\nrecord\n.\nkey\n,\nrecord\n.\nvalue\n))\nEach item in the stream is a\nConsumerRecord\nimport\njava.util.*\n;\nimport\norg.apache.spark.SparkConf\n;\nimport\norg.apache.spark.TaskContext\n;\nimport\norg.apache.spark.api.java.*\n;\nimport\norg.apache.spark.api.java.function.*\n;\nimport\norg.apache.spark.streaming.api.java.*\n;\nimport\norg.apache.spark.streaming.kafka010.*\n;\nimport\norg.apache.kafka.clients.consumer.ConsumerRecord\n;\nimport\norg.apache.kafka.common.TopicPartition\n;\nimport\norg.apache.kafka.common.serialization.StringDeserializer\n;\nimport\nscala.Tuple2\n;\nMap\n<\nString\n,\nObject\n>\nkafkaParams\n=\nnew\nHashMap\n<>();\nkafkaParams\n.\nput\n(\n\"bootstrap.servers\"\n,\n\"localhost:9092,anotherhost:9092\"\n);\nkafkaParams\n.\nput\n(\n\"key.deserializer\"\n,\nStringDeserializer\n.\nclass\n);\nkafkaParams\n.\nput\n(\n\"value.deserializer\"\n,\nStringDeserializer\n.\nclass\n);\nkafkaParams\n.\nput\n(\n\"group.id\"\n,\n\"use_a_separate_group_id_for_each_stream\"\n);\nkafkaParams\n.\nput\n(\n\"auto.offset.reset\"\n,\n\"latest\"\n);\nkafkaParams\n.\nput\n(\n\"enable.auto.commit\"\n,\nfalse\n);\nCollection\n<\nString\n>\ntopics\n=\nArrays\n.\nasList\n(\n\"topicA\"\n,\n\"topicB\"\n);\nJavaInputDStream\n<\nConsumerRecord\n<\nString\n,\nString\n>>\nstream\n=\nKafkaUtils\n.\ncreateDirectStream\n(\nstreamingContext\n,\nLocationStrategies\n.\nPreferConsistent\n(),\nConsumerStrategies\n.<\nString\n,\nString\n>\nSubscribe\n(\ntopics\n,\nkafkaParams\n)\n);\nstream\n.\nmapToPair\n(\nrecord\n->\nnew\nTuple2\n<>(\nrecord\n.\nkey\n(),\nrecord\n.\nvalue\n()));\nFor possible kafkaParams, see\nKafka consumer config docs\n.\nIf your Spark batch duration is larger than the default Kafka heartbeat session timeout (30 seconds), increase heartbeat.interval.ms and session.timeout.ms appropriately.  For batches larger than 5 minutes, this will require changing group.max.session.timeout.ms on the broker.\nNote that the example sets enable.auto.commit to false, for discussion see\nStoring Offsets\nbelow.\nLocationStrategies\nThe new Kafka consumer API will pre-fetch messages into buffers.  Therefore it is important for performance reasons that the Spark integration keep cached consumers on executors (rather than recreating them for each batch), and prefer to schedule partitions on the host locations that have the appropriate consumers.\nIn most cases, you should use\nLocationStrategies.PreferConsistent\nas shown above.  This will distribute partitions evenly across available executors.  If your executors are on the same hosts as your Kafka brokers, use\nPreferBrokers\n, which will prefer to schedule partitions on the Kafka leader for that partition.  Finally, if you have a significant skew in load among partitions, use\nPreferFixed\n. This allows you to specify an explicit mapping of partitions to hosts (any unspecified partitions will use a consistent location).\nThe cache for consumers has a default maximum size of 64.  If you expect to be handling more than (64 * number of executors) Kafka partitions, you can change this setting via\nspark.streaming.kafka.consumer.cache.maxCapacity\n.\nIf you would like to disable the caching for Kafka consumers, you can set\nspark.streaming.kafka.consumer.cache.enabled\nto\nfalse\n.\nThe cache is keyed by topicpartition and group.id, so use a\nseparate\ngroup.id\nfor each call to\ncreateDirectStream\n.\nConsumerStrategies\nThe new Kafka consumer API has a number of different ways to specify topics, some of which require considerable post-object-instantiation setup.\nConsumerStrategies\nprovides an abstraction that allows Spark to obtain properly configured consumers even after restart from checkpoint.\nConsumerStrategies.Subscribe\n, as shown above, allows you to subscribe to a fixed collection of topics.\nSubscribePattern\nallows you to use a regex to specify topics of interest. Note that unlike the 0.8 integration, using\nSubscribe\nor\nSubscribePattern\nshould respond to adding partitions during a running stream. Finally,\nAssign\nallows you to specify a fixed collection of partitions.  All three strategies have overloaded constructors that allow you to specify the starting offset for a particular partition.\nIf you have specific consumer setup needs that are not met by the options above,\nConsumerStrategy\nis a public class that you can extend.\nCreating an RDD\nIf you have a use case that is better suited to batch processing, you can create an RDD for a defined range of offsets.\n// Import dependencies and create kafka params as in Create Direct Stream above\nval\noffsetRanges\n=\nArray\n(\n// topic, partition, inclusive starting offset, exclusive ending offset\nOffsetRange\n(\n\"test\"\n,\n0\n,\n0\n,\n100\n),\nOffsetRange\n(\n\"test\"\n,\n1\n,\n0\n,\n100\n)\n)\nval\nrdd\n=\nKafkaUtils\n.\ncreateRDD\n[\nString\n,\nString\n](\nsparkContext\n,\nkafkaParams\n,\noffsetRanges\n,\nPreferConsistent\n)\n// Import dependencies and create kafka params as in Create Direct Stream above\nOffsetRange\n[]\noffsetRanges\n=\n{\n// topic, partition, inclusive starting offset, exclusive ending offset\nOffsetRange\n.\ncreate\n(\n\"test\"\n,\n0\n,\n0\n,\n100\n),\nOffsetRange\n.\ncreate\n(\n\"test\"\n,\n1\n,\n0\n,\n100\n)\n};\nJavaRDD\n<\nConsumerRecord\n<\nString\n,\nString\n>>\nrdd\n=\nKafkaUtils\n.\ncreateRDD\n(\nsparkContext\n,\nkafkaParams\n,\noffsetRanges\n,\nLocationStrategies\n.\nPreferConsistent\n()\n);\nNote that you cannot use\nPreferBrokers\n, because without the stream there is not a driver-side consumer to automatically look up broker metadata for you.  Use\nPreferFixed\nwith your own metadata lookups if necessary.\nObtaining Offsets\nstream\n.\nforeachRDD\n{\nrdd\n=>\nval\noffsetRanges\n=\nrdd\n.\nasInstanceOf\n[\nHasOffsetRanges\n].\noffsetRanges\nrdd\n.\nforeachPartition\n{\niter\n=>\nval\no\n:\nOffsetRange\n=\noffsetRanges\n(\nTaskContext\n.\nget\n.\npartitionId\n)\nprintln\n(\ns\n\"${o.topic} ${o.partition} ${o.fromOffset} ${o.untilOffset}\"\n)\n}\n}\nstream\n.\nforeachRDD\n(\nrdd\n->\n{\nOffsetRange\n[]\noffsetRanges\n=\n((\nHasOffsetRanges\n)\nrdd\n.\nrdd\n()).\noffsetRanges\n();\nrdd\n.\nforeachPartition\n(\nconsumerRecords\n->\n{\nOffsetRange\no\n=\noffsetRanges\n[\nTaskContext\n.\nget\n().\npartitionId\n()];\nSystem\n.\nout\n.\nprintln\n(\no\n.\ntopic\n()\n+\n\" \"\n+\no\n.\npartition\n()\n+\n\" \"\n+\no\n.\nfromOffset\n()\n+\n\" \"\n+\no\n.\nuntilOffset\n());\n});\n});\nNote that the typecast to\nHasOffsetRanges\nwill only succeed if it is done in the first method called on the result of\ncreateDirectStream\n, not later down a chain of methods. Be aware that the one-to-one mapping between RDD partition and Kafka partition does not remain after any methods that shuffle or repartition, e.g. reduceByKey() or window().\nStoring Offsets\nKafka delivery semantics in the case of failure depend on how and when offsets are stored.  Spark output operations are\nat-least-once\n.  So if you want the equivalent of exactly-once semantics, you must either store offsets after an idempotent output, or store offsets in an atomic transaction alongside output. With this integration, you have 3 options, in order of increasing reliability (and code complexity), for how to store offsets.\nCheckpoints\nIf you enable Spark\ncheckpointing\n, offsets will be stored in the checkpoint.  This is easy to enable, but there are drawbacks. Your output operation must be idempotent, since you will get repeated outputs; transactions are not an option.  Furthermore, you cannot recover from a checkpoint if your application code has changed.  For planned upgrades, you can mitigate this by running the new code at the same time as the old code (since outputs need to be idempotent anyway, they should not clash).  But for unplanned failures that require code changes, you will lose data unless you have another way to identify known good starting offsets.\nKafka itself\nKafka has an offset commit API that stores offsets in a special Kafka topic.  By default, the new consumer will periodically auto-commit offsets. This is almost certainly not what you want, because messages successfully polled by the consumer may not yet have resulted in a Spark output operation, resulting in undefined semantics. This is why the stream example above sets “enable.auto.commit” to false.  However, you can commit offsets to Kafka after you know your output has been stored, using the\ncommitAsync\nAPI. The benefit as compared to checkpoints is that Kafka is a durable store regardless of changes to your application code.  However, Kafka is not transactional, so your outputs must still be idempotent.\nstream\n.\nforeachRDD\n{\nrdd\n=>\nval\noffsetRanges\n=\nrdd\n.\nasInstanceOf\n[\nHasOffsetRanges\n].\noffsetRanges\n// some time later, after outputs have completed\nstream\n.\nasInstanceOf\n[\nCanCommitOffsets\n].\ncommitAsync\n(\noffsetRanges\n)\n}\nAs with HasOffsetRanges, the cast to CanCommitOffsets will only succeed if called on the result of createDirectStream, not after transformations.  The commitAsync call is threadsafe, but must occur after outputs if you want meaningful semantics.\nstream\n.\nforeachRDD\n(\nrdd\n->\n{\nOffsetRange\n[]\noffsetRanges\n=\n((\nHasOffsetRanges\n)\nrdd\n.\nrdd\n()).\noffsetRanges\n();\n// some time later, after outputs have completed\n((\nCanCommitOffsets\n)\nstream\n.\ninputDStream\n()).\ncommitAsync\n(\noffsetRanges\n);\n});\nYour own data store\nFor data stores that support transactions, saving offsets in the same transaction as the results can keep the two in sync, even in failure situations.  If you’re careful about detecting repeated or skipped offset ranges, rolling back the transaction prevents duplicated or lost messages from affecting results.  This gives the equivalent of exactly-once semantics.  It is also possible to use this tactic even for outputs that result from aggregations, which are typically hard to make idempotent.\n// The details depend on your data store, but the general idea looks like this\n// begin from the offsets committed to the database\nval\nfromOffsets\n=\nselectOffsetsFromYourDatabase\n.\nmap\n{\nresultSet\n=>\nnew\nTopicPartition\n(\nresultSet\n.\nstring\n(\n\"topic\"\n),\nresultSet\n.\nint\n(\n\"partition\"\n))\n->\nresultSet\n.\nlong\n(\n\"offset\"\n)\n}.\ntoMap\nval\nstream\n=\nKafkaUtils\n.\ncreateDirectStream\n[\nString\n,\nString\n](\nstreamingContext\n,\nPreferConsistent\n,\nAssign\n[\nString\n,\nString\n](\nfromOffsets\n.\nkeys\n.\ntoList\n,\nkafkaParams\n,\nfromOffsets\n)\n)\nstream\n.\nforeachRDD\n{\nrdd\n=>\nval\noffsetRanges\n=\nrdd\n.\nasInstanceOf\n[\nHasOffsetRanges\n].\noffsetRanges\nval\nresults\n=\nyourCalculation\n(\nrdd\n)\n// begin your transaction\n// update results\n// update offsets where the end of existing offsets matches the beginning of this batch of offsets\n// assert that offsets were updated correctly\n// end your transaction\n}\n// The details depend on your data store, but the general idea looks like this\n// begin from the offsets committed to the database\nMap\n<\nTopicPartition\n,\nLong\n>\nfromOffsets\n=\nnew\nHashMap\n<>();\nfor\n(\nresultSet\n:\nselectOffsetsFromYourDatabase\n)\nfromOffsets\n.\nput\n(\nnew\nTopicPartition\n(\nresultSet\n.\nstring\n(\n\"topic\"\n),\nresultSet\n.\nint\n(\n\"partition\"\n)),\nresultSet\n.\nlong\n(\n\"offset\"\n));\n}\nJavaInputDStream\n<\nConsumerRecord\n<\nString\n,\nString\n>>\nstream\n=\nKafkaUtils\n.\ncreateDirectStream\n(\nstreamingContext\n,\nLocationStrategies\n.\nPreferConsistent\n(),\nConsumerStrategies\n.<\nString\n,\nString\n>\nAssign\n(\nfromOffsets\n.\nkeySet\n(),\nkafkaParams\n,\nfromOffsets\n)\n);\nstream\n.\nforeachRDD\n(\nrdd\n->\n{\nOffsetRange\n[]\noffsetRanges\n=\n((\nHasOffsetRanges\n)\nrdd\n.\nrdd\n()).\noffsetRanges\n();\nObject\nresults\n=\nyourCalculation\n(\nrdd\n);\n// begin your transaction\n// update results\n// update offsets where the end of existing offsets matches the beginning of this batch of offsets\n// assert that offsets were updated correctly\n// end your transaction\n});\nSSL / TLS\nThe new Kafka consumer\nsupports SSL\n.  To enable it, set kafkaParams appropriately before passing to\ncreateDirectStream\n/\ncreateRDD\n.  Note that this only applies to communication between Spark and Kafka brokers; you are still responsible for separately\nsecuring\nSpark inter-node communication.\nval\nkafkaParams\n=\nMap\n[\nString\n,\nObject\n](\n// the usual params, make sure to change the port in bootstrap.servers if 9092 is not TLS\n\"security.protocol\"\n->\n\"SSL\"\n,\n\"ssl.truststore.location\"\n->\n\"/some-directory/kafka.client.truststore.jks\"\n,\n\"ssl.truststore.password\"\n->\n\"test1234\"\n,\n\"ssl.keystore.location\"\n->\n\"/some-directory/kafka.client.keystore.jks\"\n,\n\"ssl.keystore.password\"\n->\n\"test1234\"\n,\n\"ssl.key.password\"\n->\n\"test1234\"\n)\nMap\n<\nString\n,\nObject\n>\nkafkaParams\n=\nnew\nHashMap\n<\nString\n,\nObject\n>();\n// the usual params, make sure to change the port in bootstrap.servers if 9092 is not TLS\nkafkaParams\n.\nput\n(\n\"security.protocol\"\n,\n\"SSL\"\n);\nkafkaParams\n.\nput\n(\n\"ssl.truststore.location\"\n,\n\"/some-directory/kafka.client.truststore.jks\"\n);\nkafkaParams\n.\nput\n(\n\"ssl.truststore.password\"\n,\n\"test1234\"\n);\nkafkaParams\n.\nput\n(\n\"ssl.keystore.location\"\n,\n\"/some-directory/kafka.client.keystore.jks\"\n);\nkafkaParams\n.\nput\n(\n\"ssl.keystore.password\"\n,\n\"test1234\"\n);\nkafkaParams\n.\nput\n(\n\"ssl.key.password\"\n,\n\"test1234\"\n);\nDeploying\nAs with any Spark applications,\nspark-submit\nis used to launch your application.\nFor Scala and Java applications, if you are using SBT or Maven for project management, then package\nspark-streaming-kafka-0-10_2.13\nand its dependencies into the application JAR. Make sure\nspark-core_2.13\nand\nspark-streaming_2.13\nare marked as\nprovided\ndependencies as those are already present in a Spark installation. Then use\nspark-submit\nto launch your application (see\nDeploying section\nin the main programming guide).\nSecurity\nSee\nStructured Streaming Security\n.\nAdditional Caveats\nKafka native sink is not available so delegation token used only on consumer side."}
{"url": "https://spark.apache.org/docs/latest/streaming-kinesis-integration.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark Streaming + Kinesis Integration\nAmazon Kinesis\nis a fully managed service for real-time processing of streaming data at massive scale.\nThe Kinesis receiver creates an input DStream using the Kinesis Client Library (KCL) provided by Amazon under the Amazon Software License (ASL).\nThe KCL builds on top of the Apache 2.0 licensed AWS Java SDK and provides load-balancing, fault-tolerance, checkpointing through the concepts of Workers, Checkpoints, and Shard Leases.\nHere we explain how to configure Spark Streaming to receive data from Kinesis.\nConfiguring Kinesis\nA Kinesis stream can be set up at one of the valid Kinesis endpoints with 1 or more shards per the following\nguide\n.\nConfiguring Spark Streaming Application\nLinking:\nFor Scala/Java applications using SBT/Maven project definitions, link your streaming application against the following artifact (see\nLinking section\nin the main programming guide for further information).\ngroupId = org.apache.spark\n artifactId = spark-streaming-kinesis-asl_2.13\n version = 4.0.0\nFor Python applications, you will have to add this above library and its dependencies when deploying your application. See the\nDeploying\nsubsection below.\nNote that by linking to this library, you will include\nASL\n-licensed code in your application.\nProgramming:\nIn the streaming application code, import\nKinesisInputDStream\nand create the input DStream of byte array as follows:\nfrom\npyspark.streaming.kinesis\nimport\nKinesisUtils\n,\nInitialPositionInStream\nkinesisStream\n=\nKinesisUtils\n.\ncreateStream\n(\nstreamingContext\n,\n[\nKinesis\napp\nname\n],\n[\nKinesis\nstream\nname\n],\n[\nendpoint\nURL\n],\n[\nregion\nname\n],\n[\ninitial\nposition\n],\n[\ncheckpoint\ninterval\n],\n[\nmetricsLevel\n.\nDETAILED\n],\nStorageLevel\n.\nMEMORY_AND_DISK_2\n)\nSee the\nAPI docs\nand the\nexample\n. Refer to the\nRunning the Example\nsubsection for instructions to run the example.\nCloudWatch metrics level and dimensions. See\nthe AWS documentation about monitoring KCL\nfor details. Default is\nMetricsLevel.DETAILED\n.\nimport\norg.apache.spark.storage.StorageLevel\nimport\norg.apache.spark.streaming.kinesis.KinesisInputDStream\nimport\norg.apache.spark.streaming.\n{\nSeconds\n,\nStreamingContext\n}\nimport\norg.apache.spark.streaming.kinesis.KinesisInitialPositions\nval\nkinesisStream\n=\nKinesisInputDStream\n.\nbuilder\n.\nstreamingContext\n(\nstreamingContext\n)\n.\nendpointUrl\n([\nendpoint\nURL\n])\n.\nregionName\n([\nregion\nname\n])\n.\nstreamName\n([\nstreamName\n])\n.\ninitialPosition\n([\ninitial\nposition\n])\n.\ncheckpointAppName\n([\nKinesis\napp\nname\n])\n.\ncheckpointInterval\n([\ncheckpoint\ninterval\n])\n.\nmetricsLevel\n([\nmetricsLevel.DETAILED\n])\n.\nstorageLevel\n(\nStorageLevel\n.\nMEMORY_AND_DISK_2\n)\n.\nbuild\n()\nSee the\nAPI docs\nand the\nexample\n. Refer to the\nRunning the Example\nsubsection for instructions on how to run the example.\nimport\norg.apache.spark.storage.StorageLevel\n;\nimport\norg.apache.spark.streaming.kinesis.KinesisInputDStream\n;\nimport\norg.apache.spark.streaming.Seconds\n;\nimport\norg.apache.spark.streaming.StreamingContext\n;\nimport\norg.apache.spark.streaming.kinesis.KinesisInitialPositions\n;\nKinesisInputDStream\n<\nbyte\n[]>\nkinesisStream\n=\nKinesisInputDStream\n.\nbuilder\n()\n.\nstreamingContext\n(\nstreamingContext\n)\n.\nendpointUrl\n([\nendpoint\nURL\n])\n.\nregionName\n([\nregion\nname\n])\n.\nstreamName\n([\nstreamName\n])\n.\ninitialPosition\n([\ninitial\nposition\n])\n.\ncheckpointAppName\n([\nKinesis\napp\nname\n])\n.\ncheckpointInterval\n([\ncheckpoint\ninterval\n])\n.\nmetricsLevel\n([\nmetricsLevel\n.\nDETAILED\n])\n.\nstorageLevel\n(\nStorageLevel\n.\nMEMORY_AND_DISK_2\n)\n.\nbuild\n();\nSee the\nAPI docs\nand the\nexample\n. Refer to the\nRunning the Example\nsubsection for instructions to run the example.\nYou may also provide the following settings. This is currently only supported in Scala and Java.\nA “message handler function” that takes a Kinesis\nRecord\nand returns a generic object\nT\n, in case you would like to use other data included in a\nRecord\nsuch as partition key.\nimport\ncollection.JavaConverters._\nimport\norg.apache.spark.storage.StorageLevel\nimport\norg.apache.spark.streaming.kinesis.KinesisInputDStream\nimport\norg.apache.spark.streaming.\n{\nSeconds\n,\nStreamingContext\n}\nimport\norg.apache.spark.streaming.kinesis.KinesisInitialPositions\nimport\ncom.amazonaws.services.kinesis.clientlibrary.lib.worker.KinesisClientLibConfiguration\nimport\ncom.amazonaws.services.kinesis.metrics.interfaces.MetricsLevel\nval\nkinesisStream\n=\nKinesisInputDStream\n.\nbuilder\n.\nstreamingContext\n(\nstreamingContext\n)\n.\nendpointUrl\n([\nendpoint\nURL\n])\n.\nregionName\n([\nregion\nname\n])\n.\nstreamName\n([\nstreamName\n])\n.\ninitialPosition\n([\ninitial\nposition\n])\n.\ncheckpointAppName\n([\nKinesis\napp\nname\n])\n.\ncheckpointInterval\n([\ncheckpoint\ninterval\n])\n.\nstorageLevel\n(\nStorageLevel\n.\nMEMORY_AND_DISK_2\n)\n.\nmetricsLevel\n(\nMetricsLevel\n.\nDETAILED\n)\n.\nmetricsEnabledDimensions\n(\nKinesisClientLibConfiguration\n.\nDEFAULT_METRICS_ENABLED_DIMENSIONS\n.\nasScala\n.\ntoSet\n)\n.\nbuildWithMessageHandler\n([\nmessage\nhandler\n])\nimport\norg.apache.spark.storage.StorageLevel\n;\nimport\norg.apache.spark.streaming.kinesis.KinesisInputDStream\n;\nimport\norg.apache.spark.streaming.Seconds\n;\nimport\norg.apache.spark.streaming.StreamingContext\n;\nimport\norg.apache.spark.streaming.kinesis.KinesisInitialPositions\n;\nimport\ncom.amazonaws.services.kinesis.clientlibrary.lib.worker.KinesisClientLibConfiguration\n;\nimport\ncom.amazonaws.services.kinesis.metrics.interfaces.MetricsLevel\n;\nimport\nscala.collection.JavaConverters\n;\nKinesisInputDStream\n<\nbyte\n[]>\nkinesisStream\n=\nKinesisInputDStream\n.\nbuilder\n()\n.\nstreamingContext\n(\nstreamingContext\n)\n.\nendpointUrl\n([\nendpoint\nURL\n])\n.\nregionName\n([\nregion\nname\n])\n.\nstreamName\n([\nstreamName\n])\n.\ninitialPosition\n([\ninitial\nposition\n])\n.\ncheckpointAppName\n([\nKinesis\napp\nname\n])\n.\ncheckpointInterval\n([\ncheckpoint\ninterval\n])\n.\nstorageLevel\n(\nStorageLevel\n.\nMEMORY_AND_DISK_2\n)\n.\nmetricsLevel\n(\nMetricsLevel\n.\nDETAILED\n)\n.\nmetricsEnabledDimensions\n(\nJavaConverters\n.\nasScalaSetConverter\n(\nKinesisClientLibConfiguration\n.\nDEFAULT_METRICS_ENABLED_DIMENSIONS\n)\n.\nasScala\n().\ntoSet\n()\n)\n.\nbuildWithMessageHandler\n([\nmessage\nhandler\n]);\nstreamingContext\n: StreamingContext containing an application name used by Kinesis to tie this Kinesis application to the Kinesis stream\n[Kinesis app name]\n: The application name that will be used to checkpoint the Kinesis\n  sequence numbers in DynamoDB table.\nThe application name must be unique for a given account and region.\nIf the table exists but has incorrect checkpoint information (for a different stream, or\n  old expired sequenced numbers), then there may be temporary errors.\n[Kinesis stream name]\n: The Kinesis stream that this streaming application will pull data from.\n[endpoint URL]\n: Valid Kinesis endpoints URL can be found\nhere\n.\n[region name]\n: Valid Kinesis region names can be found\nhere\n.\n[checkpoint interval]\n: The interval (e.g., Duration(2000) = 2 seconds) at which the Kinesis Client Library saves its position in the stream.  For starters, set it to the same as the batch interval of the streaming application.\n[initial position]\n: Can be either\nKinesisInitialPositions.TrimHorizon\nor\nKinesisInitialPositions.Latest\nor\nKinesisInitialPositions.AtTimestamp\n(see\nKinesis Checkpointing\nsection and\nAmazon Kinesis API documentation\nfor more details).\n[message handler]\n: A function that takes a Kinesis\nRecord\nand outputs generic\nT\n.\nIn other versions of the API, you can also specify the AWS access key and secret key directly.\nDeploying:\nAs with any Spark applications,\nspark-submit\nis used to launch your application. However, the details are slightly different for Scala/Java applications and Python applications.\nFor Scala and Java applications, if you are using SBT or Maven for project management, then package\nspark-streaming-kinesis-asl_2.13\nand its dependencies into the application JAR. Make sure\nspark-core_2.13\nand\nspark-streaming_2.13\nare marked as\nprovided\ndependencies as those are already present in a Spark installation. Then use\nspark-submit\nto launch your application (see\nDeploying section\nin the main programming guide).\nFor Python applications which lack SBT/Maven project management,\nspark-streaming-kinesis-asl_2.13\nand its dependencies can be directly added to\nspark-submit\nusing\n--packages\n(see\nApplication Submission Guide\n). That is,\n./bin/spark-submit --packages org.apache.spark:spark-streaming-kinesis-asl_2.13:4.0.0 ...\nAlternatively, you can also download the JAR of the Maven artifact\nspark-streaming-kinesis-asl-assembly\nfrom the\nMaven repository\nand add it to\nspark-submit\nwith\n--jars\n.\nPoints to remember at runtime:\nKinesis data processing is ordered per partition and occurs at-least once per message.\nMultiple applications can read from the same Kinesis stream.  Kinesis will maintain the application-specific shard and checkpoint info in DynamoDB.\nA single Kinesis stream shard is processed by one input DStream at a time.\nA single Kinesis input DStream can read from multiple shards of a Kinesis stream by creating multiple KinesisRecordProcessor threads.\nMultiple input DStreams running in separate processes/instances can read from a Kinesis stream.\nYou never need more Kinesis input DStreams than the number of Kinesis stream shards as each input DStream will create at least one KinesisRecordProcessor thread that handles a single shard.\nHorizontal scaling is achieved by adding/removing  Kinesis input DStreams (within a single process or across multiple processes/instances) - up to the total number of Kinesis stream shards per the previous point.\nThe Kinesis input DStream will balance the load between all DStreams - even across processes/instances.\nThe Kinesis input DStream will balance the load during re-shard events (merging and splitting) due to changes in load.\nAs a best practice, it’s recommended that you avoid re-shard jitter by over-provisioning when possible.\nEach Kinesis input DStream maintains its own checkpoint info.  See the Kinesis Checkpointing section for more details.\nThere is no correlation between the number of Kinesis stream shards and the number of RDD partitions/shards created across the Spark cluster during input DStream processing.  These are 2 independent partitioning schemes.\nRunning the Example\nTo run the example,\nDownload a Spark binary from the\ndownload site\n.\nSet up Kinesis stream (see earlier section) within AWS. Note the name of the Kinesis stream and the endpoint URL corresponding to the region where the stream was created.\nSet up the environment variables\nAWS_ACCESS_KEY_ID\nand\nAWS_SECRET_ACCESS_KEY\nwith your AWS credentials.\nIn the Spark root directory, run the example as\n./bin/spark-submit\n--jars\n'connector/kinesis-asl-assembly/target/spark-streaming-kinesis-asl-assembly_*.jar'\n\\\nconnector/kinesis-asl/src/main/python/examples/streaming/kinesis_wordcount_asl.py\n\\\n[\nKinesis app name]\n[\nKinesis stream name]\n[\nendpoint URL]\n[\nregion name]\n./bin/run-example\n--packages\norg.apache.spark:spark-streaming-kinesis-asl_2.13:4.0.0 streaming.KinesisWordCountASL\n[\nKinesis app name]\n[\nKinesis stream name]\n[\nendpoint URL]\n./bin/run-example\n--packages\norg.apache.spark:spark-streaming-kinesis-asl_2.13:4.0.0 streaming.JavaKinesisWordCountASL\n[\nKinesis app name]\n[\nKinesis stream name]\n[\nendpoint URL]\nThis will wait for data to be received from the Kinesis stream.\nTo generate random string data to put onto the Kinesis stream, in another terminal, run the associated Kinesis data producer.\n./bin/run-example streaming.KinesisWordProducerASL\n[\nKinesis stream name]\n[\nendpoint URL] 1000 10\nThis will push 1000 lines per second of 10 random numbers per line to the Kinesis stream.  This data should then be received and processed by the running example.\nRecord De-aggregation\nWhen data is generated using the\nKinesis Producer Library (KPL)\n, messages may be aggregated for cost savings. Spark Streaming will automatically\nde-aggregate records during consumption.\nKinesis Checkpointing\nEach Kinesis input DStream periodically stores the current position of the stream in the backing DynamoDB table.  This allows the system to recover from failures and continue processing where the DStream left off.\nCheckpointing too frequently will cause excess load on the AWS checkpoint storage layer and may lead to AWS throttling.  The provided example handles this throttling with a random-backoff-retry strategy.\nIf no Kinesis checkpoint info exists when the input DStream starts, it will start either from the oldest record available (\nKinesisInitialPositions.TrimHorizon\n), or from the latest tip (\nKinesisInitialPositions.Latest\n), or (except Python) from the position denoted by the provided UTC timestamp (\nKinesisInitialPositions.AtTimestamp(Date timestamp)\n).  This is configurable.\nKinesisInitialPositions.Latest\ncould lead to missed records if data is added to the stream while no input DStreams are running (and no checkpoint info is being stored).\nKinesisInitialPositions.TrimHorizon\nmay lead to duplicate processing of records where the impact is dependent on checkpoint frequency and processing idempotency.\nKinesis retry configuration\nspark.streaming.kinesis.retry.waitTime\n: Wait time between Kinesis retries as a duration string. When reading from Amazon Kinesis, users may hit\nProvisionedThroughputExceededException\n’s, when consuming faster than 5 transactions/second or, exceeding the maximum read rate of 2 MiB/second. This configuration can be tweaked to increase the sleep between fetches when a fetch fails to reduce these exceptions. Default is “100ms”.\nspark.streaming.kinesis.retry.maxAttempts\n: Max number of retries for Kinesis fetches. This config can also be used to tackle the Kinesis\nProvisionedThroughputExceededException\n’s in scenarios mentioned above. It can be increased to have more number of retries for Kinesis reads. Default is 3."}
{"url": "https://spark.apache.org/docs/latest/mllib-linear-methods.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nLinear Methods - RDD-based API\nMathematical formulation\nLoss functions\nRegularizers\nOptimization\nClassification\nLinear Support Vector Machines (SVMs)\nLogistic regression\nRegression\nLinear least squares, Lasso, and ridge regression\nStreaming linear regression\nImplementation (developer)\n\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\wv}{\\mathbf{w}}\n\\newcommand{\\av}{\\mathbf{\\alpha}}\n\\newcommand{\\bv}{\\mathbf{b}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\id}{\\mathbf{I}}\n\\newcommand{\\ind}{\\mathbf{1}}\n\\newcommand{\\0}{\\mathbf{0}}\n\\newcommand{\\unit}{\\mathbf{e}}\n\\newcommand{\\one}{\\mathbf{1}}\n\\newcommand{\\zero}{\\mathbf{0}}\n\\]\nMathematical formulation\nMany standard\nmachine learning\nmethods can be formulated as a convex optimization problem, i.e.\nthe task of finding a minimizer of a convex function\n$f$\nthat depends on a variable vector\n$\\wv$\n(called\nweights\nin the code), which has\n$d$\nentries.\nFormally, we can write this as the optimization problem\n$\\min_{\\wv \\in\\R^d} \\; f(\\wv)$\n, where\nthe objective function is of the form\n\\begin{equation}\n    f(\\wv) := \\lambda\\, R(\\wv) +\n    \\frac1n \\sum_{i=1}^n L(\\wv;\\x_i,y_i)\n    \\label{eq:regPrimal}\n    \\ .\n\\end{equation}\nHere the vectors\n$\\x_i\\in\\R^d$\nare the training data examples, for\n$1\\le i\\le n$\n, and\n$y_i\\in\\R$\nare their corresponding labels, which we want to predict.\nWe call the method\nlinear\nif $L(\\wv; \\x, y)$ can be expressed as a function of $\\wv^T x$ and $y$.\nSeveral of\nspark.mllib\n’s classification and regression algorithms fall into this category,\nand are discussed here.\nThe objective function\n$f$\nhas two parts:\nthe regularizer that controls the complexity of the model,\nand the loss that measures the error of the model on the training data.\nThe loss function\n$L(\\wv;.)$\nis typically a convex function in\n$\\wv$\n.  The\nfixed regularization parameter\n$\\lambda \\ge 0$\n(\nregParam\nin the code)\ndefines the trade-off between the two goals of minimizing the loss (i.e.,\ntraining error) and minimizing model complexity (i.e., to avoid overfitting).\nLoss functions\nThe following table summarizes the loss functions and their gradients or sub-gradients for the\nmethods\nspark.mllib\nsupports:\nloss function $L(\\wv; \\x, y)$\ngradient or sub-gradient\nhinge loss\n$\\max \\{0, 1-y \\wv^T \\x \\}, \\quad y \\in \\{-1, +1\\}$\n$\\begin{cases}-y \\cdot \\x & \\text{if $y \\wv^T \\x <1$}, \\\\ 0 &\n\\text{otherwise}.\\end{cases}$\nlogistic loss\n$\\log(1+\\exp( -y \\wv^T \\x)), \\quad y \\in \\{-1, +1\\}$\n$-y \\left(1-\\frac1{1+\\exp(-y \\wv^T \\x)} \\right) \\cdot \\x$\nsquared loss\n$\\frac{1}{2} (\\wv^T \\x - y)^2, \\quad y \\in \\R$\n$(\\wv^T \\x - y) \\cdot \\x$\nNote that, in the mathematical formulation above, a binary label $y$ is denoted as either\n$+1$ (positive) or $-1$ (negative), which is convenient for the formulation.\nHowever\n, the negative label is represented by $0$ in\nspark.mllib\ninstead of $-1$, to be consistent with\nmulticlass labeling.\nRegularizers\nThe purpose of the\nregularizer\nis to\nencourage simple models and avoid overfitting.  We support the following\nregularizers in\nspark.mllib\n:\nregularizer $R(\\wv)$\ngradient or sub-gradient\nzero (unregularized)\n0\n$\\0$\nL2\n$\\frac{1}{2}\\|\\wv\\|_2^2$\n$\\wv$\nL1\n$\\|\\wv\\|_1$\n$\\mathrm{sign}(\\wv)$\nelastic net\n$\\alpha \\|\\wv\\|_1 + (1-\\alpha)\\frac{1}{2}\\|\\wv\\|_2^2$\n$\\alpha \\mathrm{sign}(\\wv) + (1-\\alpha) \\wv$\nHere\n$\\mathrm{sign}(\\wv)$\nis the vector consisting of the signs (\n$\\pm1$\n) of all the entries\nof\n$\\wv$\n.\nL2-regularized problems are generally easier to solve than L1-regularized due to smoothness.\nHowever, L1 regularization can help promote sparsity in weights leading to smaller and more interpretable models, the latter of which can be useful for feature selection.\nElastic net\nis a combination of L1 and L2 regularization. It is not recommended to train models without any regularization,\nespecially when the number of training examples is small.\nOptimization\nUnder the hood, linear methods use convex optimization methods to optimize the objective functions.\nspark.mllib\nuses two methods, SGD and L-BFGS, described in the\noptimization section\n.\nCurrently, most algorithm APIs support Stochastic Gradient Descent (SGD), and a few support L-BFGS.\nRefer to\nthis optimization section\nfor guidelines on choosing between optimization methods.\nClassification\nClassification\naims to divide items into\ncategories.\nThe most common classification type is\nbinary classification\n, where there are two\ncategories, usually named positive and negative.\nIf there are more than two categories, it is called\nmulticlass classification\n.\nspark.mllib\nsupports two linear methods for classification: linear Support Vector Machines (SVMs)\nand logistic regression.\nLinear SVMs supports only binary classification, while logistic regression supports both binary and\nmulticlass classification problems.\nFor both methods,\nspark.mllib\nsupports L1 and L2 regularized variants.\nThe training data set is represented by an RDD of\nLabeledPoint\nin MLlib,\nwhere labels are class indices starting from zero: $0, 1, 2, \\ldots$.\nLinear Support Vector Machines (SVMs)\nThe\nlinear SVM\nis a standard method for large-scale classification tasks. It is a linear method as described above in equation\n$\\eqref{eq:regPrimal}$\n, with the loss function in the formulation given by the hinge loss:\n\\[\nL(\\wv;\\x,y) := \\max \\{0, 1-y \\wv^T \\x \\}.\n\\]\nBy default, linear SVMs are trained with an L2 regularization.\nWe also support alternative L1 regularization. In this case,\nthe problem becomes a\nlinear program\n.\nThe linear SVMs algorithm outputs an SVM model. Given a new data point,\ndenoted by $\\x$, the model makes predictions based on the value of $\\wv^T \\x$.\nBy the default, if $\\wv^T \\x \\geq 0$ then the outcome is positive, and negative\notherwise.\nExamples\nThe following example shows how to load a sample dataset, build SVM model,\nand make predictions with the resulting model to compute the training error.\nRefer to the\nSVMWithSGD\nPython docs\nand\nSVMModel\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.classification\nimport\nSVMWithSGD\n,\nSVMModel\nfrom\npyspark.mllib.regression\nimport\nLabeledPoint\n# Load and parse the data\ndef\nparsePoint\n(\nline\n):\nvalues\n=\n[\nfloat\n(\nx\n)\nfor\nx\nin\nline\n.\nsplit\n(\n'\n'\n)]\nreturn\nLabeledPoint\n(\nvalues\n[\n0\n],\nvalues\n[\n1\n:])\ndata\n=\nsc\n.\ntextFile\n(\n\"\ndata/mllib/sample_svm_data.txt\n\"\n)\nparsedData\n=\ndata\n.\nmap\n(\nparsePoint\n)\n# Build the model\nmodel\n=\nSVMWithSGD\n.\ntrain\n(\nparsedData\n,\niterations\n=\n100\n)\n# Evaluating the model on training data\nlabelsAndPreds\n=\nparsedData\n.\nmap\n(\nlambda\np\n:\n(\np\n.\nlabel\n,\nmodel\n.\npredict\n(\np\n.\nfeatures\n)))\ntrainErr\n=\nlabelsAndPreds\n.\nfilter\n(\nlambda\nlp\n:\nlp\n[\n0\n]\n!=\nlp\n[\n1\n]).\ncount\n()\n/\nfloat\n(\nparsedData\n.\ncount\n())\nprint\n(\n\"\nTraining Error =\n\"\n+\nstr\n(\ntrainErr\n))\n# Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"\ntarget/tmp/pythonSVMWithSGDModel\n\"\n)\nsameModel\n=\nSVMModel\n.\nload\n(\nsc\n,\n\"\ntarget/tmp/pythonSVMWithSGDModel\n\"\n)\nFind full example code at \"examples/src/main/python/mllib/svm_with_sgd_example.py\" in the Spark repo.\nThe following code snippet illustrates how to load a sample dataset, execute a\ntraining algorithm on this training data using a static method in the algorithm\nobject, and make predictions with the resulting model to compute the training\nerror.\nRefer to the\nSVMWithSGD\nScala docs\nand\nSVMModel\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.classification.\n{\nSVMModel\n,\nSVMWithSGD\n}\nimport\norg.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nimport\norg.apache.spark.mllib.util.MLUtils\n// Load training data in LIBSVM format.\nval\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Split data into training (60%) and test (40%).\nval\nsplits\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.6\n,\n0.4\n),\nseed\n=\n11L\n)\nval\ntraining\n=\nsplits\n(\n0\n).\ncache\n()\nval\ntest\n=\nsplits\n(\n1\n)\n// Run training algorithm to build the model\nval\nnumIterations\n=\n100\nval\nmodel\n=\nSVMWithSGD\n.\ntrain\n(\ntraining\n,\nnumIterations\n)\n// Clear the default threshold.\nmodel\n.\nclearThreshold\n()\n// Compute raw scores on the test set.\nval\nscoreAndLabels\n=\ntest\n.\nmap\n{\npoint\n=>\nval\nscore\n=\nmodel\n.\npredict\n(\npoint\n.\nfeatures\n)\n(\nscore\n,\npoint\n.\nlabel\n)\n}\n// Get evaluation metrics.\nval\nmetrics\n=\nnew\nBinaryClassificationMetrics\n(\nscoreAndLabels\n)\nval\nauROC\n=\nmetrics\n.\nareaUnderROC\n()\nprintln\n(\ns\n\"Area under ROC = $auROC\"\n)\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/scalaSVMWithSGDModel\"\n)\nval\nsameModel\n=\nSVMModel\n.\nload\n(\nsc\n,\n\"target/tmp/scalaSVMWithSGDModel\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\" in the Spark repo.\nThe\nSVMWithSGD.train()\nmethod by default performs L2 regularization with the\nregularization parameter set to 1.0. If we want to configure this algorithm, we\ncan customize\nSVMWithSGD\nfurther by creating a new object directly and\ncalling setter methods. All other\nspark.mllib\nalgorithms support customization in\nthis way as well. For example, the following code produces an L1 regularized\nvariant of SVMs with regularization parameter set to 0.1, and runs the training\nalgorithm for 200 iterations.\nimport\norg.apache.spark.mllib.optimization.L1Updater\nval\nsvmAlg\n=\nnew\nSVMWithSGD\n()\nsvmAlg\n.\noptimizer\n.\nsetNumIterations\n(\n200\n)\n.\nsetRegParam\n(\n0.1\n)\n.\nsetUpdater\n(\nnew\nL1Updater\n)\nval\nmodelL1\n=\nsvmAlg\n.\nrun\n(\ntraining\n)\nAll of MLlib’s methods use Java-friendly types, so you can import and call them there the same\nway you do in Scala. The only caveat is that the methods take Scala RDD objects, while the\nSpark Java API uses a separate\nJavaRDD\nclass. You can convert a Java RDD to a Scala one by\ncalling\n.rdd()\non your\nJavaRDD\nobject. A self-contained application example\nthat is equivalent to the provided example in Scala is given below:\nRefer to the\nSVMWithSGD\nJava docs\nand\nSVMModel\nJava docs\nfor details on the API.\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.mllib.classification.SVMModel\n;\nimport\norg.apache.spark.mllib.classification.SVMWithSGD\n;\nimport\norg.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n;\nimport\norg.apache.spark.mllib.regression.LabeledPoint\n;\nimport\norg.apache.spark.mllib.util.MLUtils\n;\nString\npath\n=\n\"data/mllib/sample_libsvm_data.txt\"\n;\nJavaRDD\n<\nLabeledPoint\n>\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\npath\n).\ntoJavaRDD\n();\n// Split initial RDD into two... [60% training data, 40% testing data].\nJavaRDD\n<\nLabeledPoint\n>\ntraining\n=\ndata\n.\nsample\n(\nfalse\n,\n0.6\n,\n11L\n);\ntraining\n.\ncache\n();\nJavaRDD\n<\nLabeledPoint\n>\ntest\n=\ndata\n.\nsubtract\n(\ntraining\n);\n// Run training algorithm to build the model.\nint\nnumIterations\n=\n100\n;\nSVMModel\nmodel\n=\nSVMWithSGD\n.\ntrain\n(\ntraining\n.\nrdd\n(),\nnumIterations\n);\n// Clear the default threshold.\nmodel\n.\nclearThreshold\n();\n// Compute raw scores on the test set.\nJavaRDD\n<\nTuple2\n<\nObject\n,\nObject\n>>\nscoreAndLabels\n=\ntest\n.\nmap\n(\np\n->\nnew\nTuple2\n<>(\nmodel\n.\npredict\n(\np\n.\nfeatures\n()),\np\n.\nlabel\n()));\n// Get evaluation metrics.\nBinaryClassificationMetrics\nmetrics\n=\nnew\nBinaryClassificationMetrics\n(\nJavaRDD\n.\ntoRDD\n(\nscoreAndLabels\n));\ndouble\nauROC\n=\nmetrics\n.\nareaUnderROC\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Area under ROC = \"\n+\nauROC\n);\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/javaSVMWithSGDModel\"\n);\nSVMModel\nsameModel\n=\nSVMModel\n.\nload\n(\nsc\n,\n\"target/tmp/javaSVMWithSGDModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\" in the Spark repo.\nThe\nSVMWithSGD.train()\nmethod by default performs L2 regularization with the\nregularization parameter set to 1.0. If we want to configure this algorithm, we\ncan customize\nSVMWithSGD\nfurther by creating a new object directly and\ncalling setter methods. All other\nspark.mllib\nalgorithms support customization in\nthis way as well. For example, the following code produces an L1 regularized\nvariant of SVMs with regularization parameter set to 0.1, and runs the training\nalgorithm for 200 iterations.\nimport\norg.apache.spark.mllib.optimization.L1Updater\n;\nSVMWithSGD\nsvmAlg\n=\nnew\nSVMWithSGD\n();\nsvmAlg\n.\noptimizer\n()\n.\nsetNumIterations\n(\n200\n)\n.\nsetRegParam\n(\n0.1\n)\n.\nsetUpdater\n(\nnew\nL1Updater\n());\nSVMModel\nmodelL1\n=\nsvmAlg\n.\nrun\n(\ntraining\n.\nrdd\n());\nIn order to run the above application, follow the instructions\nprovided in the\nSelf-Contained\nApplications\nsection of the Spark\nquick-start guide. Be sure to also include\nspark-mllib\nto your build file as\na dependency.\nLogistic regression\nLogistic regression\nis widely used to predict a\nbinary response. It is a linear method as described above in equation\n$\\eqref{eq:regPrimal}$\n,\nwith the loss function in the formulation given by the logistic loss:\n\\[\nL(\\wv;\\x,y) :=  \\log(1+\\exp( -y \\wv^T \\x)).\n\\]\nFor binary classification problems, the algorithm outputs a binary logistic regression model.\nGiven a new data point, denoted by $\\x$, the model makes predictions by\napplying the logistic function\n\\[\n\\mathrm{f}(z) = \\frac{1}{1 + e^{-z}}\n\\]\nwhere $z = \\wv^T \\x$.\nBy default, if $\\mathrm{f}(\\wv^T x) > 0.5$, the outcome is positive, or\nnegative otherwise, though unlike linear SVMs, the raw output of the logistic regression\nmodel, $\\mathrm{f}(z)$, has a probabilistic interpretation (i.e., the probability\nthat $\\x$ is positive).\nBinary logistic regression can be generalized into\nmultinomial logistic regression\nto\ntrain and predict multiclass classification problems.\nFor example, for $K$ possible outcomes, one of the outcomes can be chosen as a “pivot”, and the\nother $K - 1$ outcomes can be separately regressed against the pivot outcome.\nIn\nspark.mllib\n, the first class $0$ is chosen as the “pivot” class.\nSee Section 4.4 of\nThe Elements of Statistical Learning\nfor\nreferences.\nHere is a\ndetailed mathematical derivation\n.\nFor multiclass classification problems, the algorithm will output a multinomial logistic regression\nmodel, which contains $K - 1$ binary logistic regression models regressed against the first class.\nGiven a new data points, $K - 1$ models will be run, and the class with largest probability will be\nchosen as the predicted class.\nWe implemented two algorithms to solve logistic regression: mini-batch gradient descent and L-BFGS.\nWe recommend L-BFGS over mini-batch gradient descent for faster convergence.\nExamples\nThe following example shows how to load a sample dataset, build Logistic Regression model,\nand make predictions with the resulting model to compute the training error.\nNote that the Python API does not yet support multiclass classification and model save/load but\nwill in the future.\nRefer to the\nLogisticRegressionWithLBFGS\nPython docs\nand\nLogisticRegressionModel\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.classification\nimport\nLogisticRegressionWithLBFGS\n,\nLogisticRegressionModel\nfrom\npyspark.mllib.regression\nimport\nLabeledPoint\n# Load and parse the data\ndef\nparsePoint\n(\nline\n):\nvalues\n=\n[\nfloat\n(\nx\n)\nfor\nx\nin\nline\n.\nsplit\n(\n'\n'\n)]\nreturn\nLabeledPoint\n(\nvalues\n[\n0\n],\nvalues\n[\n1\n:])\ndata\n=\nsc\n.\ntextFile\n(\n\"\ndata/mllib/sample_svm_data.txt\n\"\n)\nparsedData\n=\ndata\n.\nmap\n(\nparsePoint\n)\n# Build the model\nmodel\n=\nLogisticRegressionWithLBFGS\n.\ntrain\n(\nparsedData\n)\n# Evaluating the model on training data\nlabelsAndPreds\n=\nparsedData\n.\nmap\n(\nlambda\np\n:\n(\np\n.\nlabel\n,\nmodel\n.\npredict\n(\np\n.\nfeatures\n)))\ntrainErr\n=\nlabelsAndPreds\n.\nfilter\n(\nlambda\nlp\n:\nlp\n[\n0\n]\n!=\nlp\n[\n1\n]).\ncount\n()\n/\nfloat\n(\nparsedData\n.\ncount\n())\nprint\n(\n\"\nTraining Error =\n\"\n+\nstr\n(\ntrainErr\n))\n# Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"\ntarget/tmp/pythonLogisticRegressionWithLBFGSModel\n\"\n)\nsameModel\n=\nLogisticRegressionModel\n.\nload\n(\nsc\n,\n\"\ntarget/tmp/pythonLogisticRegressionWithLBFGSModel\n\"\n)\nFind full example code at \"examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\" in the Spark repo.\nThe following code illustrates how to load a sample multiclass dataset, split it into train and\ntest, and use\nLogisticRegressionWithLBFGS\nto fit a logistic regression model.\nThen the model is evaluated against the test dataset and saved to disk.\nRefer to the\nLogisticRegressionWithLBFGS\nScala docs\nand\nLogisticRegressionModel\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.classification.\n{\nLogisticRegressionModel\n,\nLogisticRegressionWithLBFGS\n}\nimport\norg.apache.spark.mllib.evaluation.MulticlassMetrics\nimport\norg.apache.spark.mllib.regression.LabeledPoint\nimport\norg.apache.spark.mllib.util.MLUtils\n// Load training data in LIBSVM format.\nval\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Split data into training (60%) and test (40%).\nval\nsplits\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.6\n,\n0.4\n),\nseed\n=\n11L\n)\nval\ntraining\n=\nsplits\n(\n0\n).\ncache\n()\nval\ntest\n=\nsplits\n(\n1\n)\n// Run training algorithm to build the model\nval\nmodel\n=\nnew\nLogisticRegressionWithLBFGS\n()\n.\nsetNumClasses\n(\n10\n)\n.\nrun\n(\ntraining\n)\n// Compute raw scores on the test set.\nval\npredictionAndLabels\n=\ntest\n.\nmap\n{\ncase\nLabeledPoint\n(\nlabel\n,\nfeatures\n)\n=>\nval\nprediction\n=\nmodel\n.\npredict\n(\nfeatures\n)\n(\nprediction\n,\nlabel\n)\n}\n// Get evaluation metrics.\nval\nmetrics\n=\nnew\nMulticlassMetrics\n(\npredictionAndLabels\n)\nval\naccuracy\n=\nmetrics\n.\naccuracy\nprintln\n(\ns\n\"Accuracy = $accuracy\"\n)\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/scalaLogisticRegressionWithLBFGSModel\"\n)\nval\nsameModel\n=\nLogisticRegressionModel\n.\nload\n(\nsc\n,\n\"target/tmp/scalaLogisticRegressionWithLBFGSModel\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\" in the Spark repo.\nThe following code illustrates how to load a sample multiclass dataset, split it into train and\ntest, and use\nLogisticRegressionWithLBFGS\nto fit a logistic regression model.\nThen the model is evaluated against the test dataset and saved to disk.\nRefer to the\nLogisticRegressionWithLBFGS\nJava docs\nand\nLogisticRegressionModel\nJava docs\nfor details on the API.\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.api.java.JavaPairRDD\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.mllib.classification.LogisticRegressionModel\n;\nimport\norg.apache.spark.mllib.classification.LogisticRegressionWithLBFGS\n;\nimport\norg.apache.spark.mllib.evaluation.MulticlassMetrics\n;\nimport\norg.apache.spark.mllib.regression.LabeledPoint\n;\nimport\norg.apache.spark.mllib.util.MLUtils\n;\nString\npath\n=\n\"data/mllib/sample_libsvm_data.txt\"\n;\nJavaRDD\n<\nLabeledPoint\n>\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\npath\n).\ntoJavaRDD\n();\n// Split initial RDD into two... [60% training data, 40% testing data].\nJavaRDD\n<\nLabeledPoint\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]\n{\n0.6\n,\n0.4\n},\n11L\n);\nJavaRDD\n<\nLabeledPoint\n>\ntraining\n=\nsplits\n[\n0\n].\ncache\n();\nJavaRDD\n<\nLabeledPoint\n>\ntest\n=\nsplits\n[\n1\n];\n// Run training algorithm to build the model.\nLogisticRegressionModel\nmodel\n=\nnew\nLogisticRegressionWithLBFGS\n()\n.\nsetNumClasses\n(\n10\n)\n.\nrun\n(\ntraining\n.\nrdd\n());\n// Compute raw scores on the test set.\nJavaPairRDD\n<\nObject\n,\nObject\n>\npredictionAndLabels\n=\ntest\n.\nmapToPair\n(\np\n->\nnew\nTuple2\n<>(\nmodel\n.\npredict\n(\np\n.\nfeatures\n()),\np\n.\nlabel\n()));\n// Get evaluation metrics.\nMulticlassMetrics\nmetrics\n=\nnew\nMulticlassMetrics\n(\npredictionAndLabels\n.\nrdd\n());\ndouble\naccuracy\n=\nmetrics\n.\naccuracy\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Accuracy = \"\n+\naccuracy\n);\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/javaLogisticRegressionWithLBFGSModel\"\n);\nLogisticRegressionModel\nsameModel\n=\nLogisticRegressionModel\n.\nload\n(\nsc\n,\n\"target/tmp/javaLogisticRegressionWithLBFGSModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\" in the Spark repo.\nRegression\nLinear least squares, Lasso, and ridge regression\nLinear least squares is the most common formulation for regression problems.\nIt is a linear method as described above in equation\n$\\eqref{eq:regPrimal}$\n, with the loss\nfunction in the formulation given by the squared loss:\n\\[\nL(\\wv;\\x,y) :=  \\frac{1}{2} (\\wv^T \\x - y)^2.\n\\]\nVarious related regression methods are derived by using different types of regularization:\nordinary least squares\nor\nlinear least squares\nuses\n no regularization;\nridge regression\nuses L2\nregularization; and\nLasso\nuses L1\nregularization.  For all of these models, the average loss or training error, $\\frac{1}{n} \\sum_{i=1}^n (\\wv^T x_i - y_i)^2$, is\nknown as the\nmean squared error\n.\nStreaming linear regression\nWhen data arrive in a streaming fashion, it is useful to fit regression models online,\nupdating the parameters of the model as new data arrives.\nspark.mllib\ncurrently supports\nstreaming linear regression using ordinary least squares. The fitting is similar\nto that performed offline, except fitting occurs on each batch of data, so that\nthe model continually updates to reflect the data from the stream.\nExamples\nThe following example demonstrates how to load training and testing data from two different\ninput streams of text files, parse the streams as labeled points, fit a linear regression model\nonline to the first stream, and make predictions on the second stream.\nFirst, we import the necessary classes for parsing our input data and creating the model.\nThen we make input streams for training and testing data. We assume a StreamingContext\nssc\nhas already been created, see\nSpark Streaming Programming Guide\nfor more info. For this example, we use labeled points in training and testing streams,\nbut in practice you will likely want to use unlabeled vectors for test data.\nWe create our model by initializing the weights to 0.\nNow we register the streams for training and testing and start the job.\nWe can now save text files with data to the training or testing folders.\nEach line should be a data point formatted as\n(y,[x1,x2,x3])\nwhere\ny\nis the label\nand\nx1,x2,x3\nare the features. Anytime a text file is placed in\nsys.argv[1]\nthe model will update. Anytime a text file is placed in\nsys.argv[2]\nyou will see predictions.\nAs you feed more data to the training directory, the predictions\nwill get better!\nHere a complete example:\nimport\nsys\nfrom\npyspark.mllib.linalg\nimport\nVectors\nfrom\npyspark.mllib.regression\nimport\nLabeledPoint\nfrom\npyspark.mllib.regression\nimport\nStreamingLinearRegressionWithSGD\ndef\nparse\n(\nlp\n):\nlabel\n=\nfloat\n(\nlp\n[\nlp\n.\nfind\n(\n'\n(\n'\n)\n+\n1\n:\nlp\n.\nfind\n(\n'\n,\n'\n)])\nvec\n=\nVectors\n.\ndense\n(\nlp\n[\nlp\n.\nfind\n(\n'\n[\n'\n)\n+\n1\n:\nlp\n.\nfind\n(\n'\n]\n'\n)].\nsplit\n(\n'\n,\n'\n))\nreturn\nLabeledPoint\n(\nlabel\n,\nvec\n)\ntrainingData\n=\nssc\n.\ntextFileStream\n(\nsys\n.\nargv\n[\n1\n]).\nmap\n(\nparse\n).\ncache\n()\ntestData\n=\nssc\n.\ntextFileStream\n(\nsys\n.\nargv\n[\n2\n]).\nmap\n(\nparse\n)\nnumFeatures\n=\n3\nmodel\n=\nStreamingLinearRegressionWithSGD\n()\nmodel\n.\nsetInitialWeights\n([\n0.0\n,\n0.0\n,\n0.0\n])\nmodel\n.\ntrainOn\n(\ntrainingData\n)\nprint\n(\nmodel\n.\npredictOnValues\n(\ntestData\n.\nmap\n(\nlambda\nlp\n:\n(\nlp\n.\nlabel\n,\nlp\n.\nfeatures\n))))\nssc\n.\nstart\n()\nssc\n.\nawaitTermination\n()\nFind full example code at \"examples/src/main/python/mllib/streaming_linear_regression_example.py\" in the Spark repo.\nFirst, we import the necessary classes for parsing our input data and creating the model.\nThen we make input streams for training and testing data. We assume a StreamingContext\nssc\nhas already been created, see\nSpark Streaming Programming Guide\nfor more info. For this example, we use labeled points in training and testing streams,\nbut in practice you will likely want to use unlabeled vectors for test data.\nWe create our model by initializing the weights to zero and register the streams for training and\ntesting then start the job. Printing predictions alongside true labels lets us easily see the\nresult.\nFinally, we can save text files with data to the training or testing folders.\nEach line should be a data point formatted as\n(y,[x1,x2,x3])\nwhere\ny\nis the label\nand\nx1,x2,x3\nare the features. Anytime a text file is placed in\nargs(0)\nthe model will update. Anytime a text file is placed in\nargs(1)\nyou will see predictions.\nAs you feed more data to the training directory, the predictions\nwill get better!\nHere is a complete example:\nimport\norg.apache.spark.mllib.linalg.Vectors\nimport\norg.apache.spark.mllib.regression.LabeledPoint\nimport\norg.apache.spark.mllib.regression.StreamingLinearRegressionWithSGD\nval\ntrainingData\n=\nssc\n.\ntextFileStream\n(\nargs\n(\n0\n)).\nmap\n(\nLabeledPoint\n.\nparse\n).\ncache\n()\nval\ntestData\n=\nssc\n.\ntextFileStream\n(\nargs\n(\n1\n)).\nmap\n(\nLabeledPoint\n.\nparse\n)\nval\nnumFeatures\n=\n3\nval\nmodel\n=\nnew\nStreamingLinearRegressionWithSGD\n()\n.\nsetInitialWeights\n(\nVectors\n.\nzeros\n(\nnumFeatures\n))\nmodel\n.\ntrainOn\n(\ntrainingData\n)\nmodel\n.\npredictOnValues\n(\ntestData\n.\nmap\n(\nlp\n=>\n(\nlp\n.\nlabel\n,\nlp\n.\nfeatures\n))).\nprint\n()\nssc\n.\nstart\n()\nssc\n.\nawaitTermination\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\" in the Spark repo.\nImplementation (developer)\nBehind the scene,\nspark.mllib\nimplements a simple distributed version of stochastic gradient descent\n(SGD), building on the underlying gradient descent primitive (as described in the\noptimization\nsection).  All provided algorithms take as input a\nregularization parameter (\nregParam\n) along with various parameters associated with stochastic\ngradient descent (\nstepSize\n,\nnumIterations\n,\nminiBatchFraction\n).  For each of them, we support\nall three possible regularizations (none, L1 or L2).\nFor Logistic Regression,\nL-BFGS\nversion is implemented under\nLogisticRegressionWithLBFGS\n, and this\nversion supports both binary and multinomial Logistic Regression while SGD version only supports\nbinary Logistic Regression. However, L-BFGS version doesn’t support L1 regularization but SGD one\nsupports L1 regularization. When L1 regularization is not required, L-BFGS version is strongly\nrecommended since it converges faster and more accurately compared to SGD by approximating the\ninverse Hessian matrix using quasi-Newton method.\nAlgorithms are all implemented in Scala:\nSVMWithSGD\nLogisticRegressionWithLBFGS\nLogisticRegressionWithSGD\nLinearRegressionWithSGD\nRidgeRegressionWithSGD\nLassoWithSGD"}
{"url": "https://spark.apache.org/docs/latest/core-migration-guide.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMigration Guide: Spark Core\nUpgrading from Core 3.5 to 4.0\nUpgrading from Core 3.5.3 to 3.5.4\nUpgrading from Core 3.4 to 3.5\nUpgrading from Core 3.3 to 3.4\nUpgrading from Core 3.2 to 3.3\nUpgrading from Core 3.1 to 3.2\nUpgrading from Core 3.0 to 3.1\nUpgrading from Core 2.4 to 3.0\nUpgrading from Core 3.5 to 4.0\nSince Spark 4.0, Spark migrated all its internal reference of servlet API from\njavax\nto\njakarta\nSince Spark 4.0, Spark will roll event logs to archive them incrementally. To restore the behavior before Spark 4.0, you can set\nspark.eventLog.rolling.enabled\nto\nfalse\n.\nSince Spark 4.0, Spark will compress event logs. To restore the behavior before Spark 4.0, you can set\nspark.eventLog.compress\nto\nfalse\n.\nSince Spark 4.0, Spark workers will clean up worker and stopped application directories periodically. To restore the behavior before Spark 4.0, you can set\nspark.worker.cleanup.enabled\nto\nfalse\n.\nSince Spark 4.0,\nspark.shuffle.service.db.backend\nis set to\nROCKSDB\nby default which means Spark will use RocksDB store for shuffle service. To restore the behavior before Spark 4.0, you can set\nspark.shuffle.service.db.backend\nto\nLEVELDB\n.\nIn Spark 4.0, support for Apache Mesos as a resource manager was removed.\nSince Spark 4.0, Spark will allocate executor pods with a batch size of\n10\n. To restore the legacy behavior, you can set\nspark.kubernetes.allocation.batch.size\nto\n5\n.\nSince Spark 4.0, Spark uses\nReadWriteOncePod\ninstead of\nReadWriteOnce\naccess mode in persistence volume claims. To restore the legacy behavior, you can set\nspark.kubernetes.legacy.useReadWriteOnceAccessMode\nto\ntrue\n.\nSince Spark 4.0, Spark reports its executor pod status by checking all containers of that pod. To restore the legacy behavior, you can set\nspark.kubernetes.executor.checkAllContainers\nto\nfalse\n.\nSince Spark 4.0, Spark uses\n~/.ivy2.5.2\nas Ivy user directory by default to isolate the existing systems from Apache Ivy’s incompatibility. To restore the legacy behavior, you can set\nspark.jars.ivy\nto\n~/.ivy2\n.\nSince Spark 4.0, Spark uses the external shuffle service for deleting shuffle blocks for deallocated executors when the shuffle is no longer needed. To restore the legacy behavior, you can set\nspark.shuffle.service.removeShuffle\nto\nfalse\n.\nSince Spark 4.0, the MDC (Mapped Diagnostic Context) key for Spark task names in Spark logs has been changed from\nmdc.taskName\nto\ntask_name\n. To use the key\nmdc.taskName\n, you can set\nspark.log.legacyTaskNameMdc.enabled\nto\ntrue\n.\nSince Spark 4.0, Spark performs speculative executions less aggressively with\nspark.speculation.multiplier=3\nand\nspark.speculation.quantile=0.9\n. To restore the legacy behavior, you can set\nspark.speculation.multiplier=1.5\nand\nspark.speculation.quantile=0.75\n.\nSince Spark 4.0,\nspark.shuffle.unsafe.file.output.buffer\nis deprecated though still works. Use\nspark.shuffle.localDisk.file.output.buffer\ninstead.\nSince Spark 4.0, when reading files hits\norg.apache.hadoop.security.AccessControlException\nand\norg.apache.hadoop.hdfs.BlockMissingException\n, the exception will be thrown and fail the task, even if\nspark.files.ignoreCorruptFiles\nis set to\ntrue\n.\nUpgrading from Core 3.5.3 to 3.5.4\nSince Spark 3.5.4, when reading files hits\norg.apache.hadoop.security.AccessControlException\nand\norg.apache.hadoop.hdfs.BlockMissingException\n, the exception will be thrown and fail the task, even if\nspark.files.ignoreCorruptFiles\nis set to\ntrue\n.\nUpgrading from Core 3.4 to 3.5\nSince Spark 3.5,\nspark.yarn.executor.failuresValidityInterval\nis deprecated. Use\nspark.executor.failuresValidityInterval\ninstead.\nSince Spark 3.5,\nspark.yarn.max.executor.failures\nis deprecated. Use\nspark.executor.maxNumFailures\ninstead.\nUpgrading from Core 3.3 to 3.4\nSince Spark 3.4, Spark driver will own\nPersistentVolumeClaim\ns and try to reuse if they are not assigned to live executors. To restore the behavior before Spark 3.4, you can set\nspark.kubernetes.driver.ownPersistentVolumeClaim\nto\nfalse\nand\nspark.kubernetes.driver.reusePersistentVolumeClaim\nto\nfalse\n.\nSince Spark 3.4, Spark driver will track shuffle data when dynamic allocation is enabled without shuffle service. To restore the behavior before Spark 3.4, you can set\nspark.dynamicAllocation.shuffleTracking.enabled\nto\nfalse\n.\nSince Spark 3.4, Spark will try to decommission cached RDD and shuffle blocks if both\nspark.decommission.enabled\nand\nspark.storage.decommission.enabled\nare true. To restore the behavior before Spark 3.4, you can set both\nspark.storage.decommission.rddBlocks.enabled\nand\nspark.storage.decommission.shuffleBlocks.enabled\nto\nfalse\n.\nSince Spark 3.4, Spark will use RocksDB store if\nspark.history.store.hybridStore.enabled\nis true. To restore the behavior before Spark 3.4, you can set\nspark.history.store.hybridStore.diskBackend\nto\nLEVELDB\n.\nUpgrading from Core 3.2 to 3.3\nSince Spark 3.3, Spark migrates its log4j dependency from 1.x to 2.x because log4j 1.x has reached end of life and is no longer supported by the community. Vulnerabilities reported after August 2015 against log4j 1.x were not checked and will not be fixed. Users should rewrite original log4j properties files using log4j2 syntax (XML, JSON, YAML, or properties format). Spark rewrites the\nconf/log4j.properties.template\nwhich is included in Spark distribution, to\nconf/log4j2.properties.template\nwith log4j2 properties format.\nUpgrading from Core 3.1 to 3.2\nSince Spark 3.2,\nspark.scheduler.allocation.file\nsupports read remote file using hadoop filesystem which means if the path has no scheme Spark will respect hadoop configuration to read it. To restore the behavior before Spark 3.2, you can specify the local scheme for\nspark.scheduler.allocation.file\ne.g.\nfile:///path/to/file\n.\nSince Spark 3.2,\nspark.hadoopRDD.ignoreEmptySplits\nis set to\ntrue\nby default which means Spark will not create empty partitions for empty input splits. To restore the behavior before Spark 3.2, you can set\nspark.hadoopRDD.ignoreEmptySplits\nto\nfalse\n.\nSince Spark 3.2,\nspark.eventLog.compression.codec\nis set to\nzstd\nby default which means Spark will not fallback to use\nspark.io.compression.codec\nanymore.\nSince Spark 3.2,\nspark.storage.replication.proactive\nis enabled by default which means Spark tries to replenish in case of the loss of cached RDD block replicas due to executor failures. To restore the behavior before Spark 3.2, you can set\nspark.storage.replication.proactive\nto\nfalse\n.\nIn Spark 3.2,\nspark.launcher.childConectionTimeout\nis deprecated (typo) though still works. Use\nspark.launcher.childConnectionTimeout\ninstead.\nIn Spark 3.2, support for Apache Mesos as a resource manager is deprecated and will be removed in a future version.\nIn Spark 3.2, Spark will delete K8s driver service resource when the application terminates by itself. To restore the behavior before Spark 3.2, you can set\nspark.kubernetes.driver.service.deleteOnTermination\nto\nfalse\n.\nUpgrading from Core 3.0 to 3.1\nIn Spark 3.0 and below,\nSparkContext\ncan be created in executors. Since Spark 3.1, an exception will be thrown when creating\nSparkContext\nin executors. You can allow it by setting the configuration\nspark.executor.allowSparkContext\nwhen creating\nSparkContext\nin executors.\nIn Spark 3.0 and below, Spark propagated the Hadoop classpath from\nyarn.application.classpath\nand\nmapreduce.application.classpath\ninto the Spark application submitted to YARN when Spark distribution is with the built-in Hadoop. Since Spark 3.1, it does not propagate anymore when the Spark distribution is with the built-in Hadoop in order to prevent the failure from the different transitive dependencies picked up from the Hadoop cluster such as Guava and Jackson. To restore the behavior before Spark 3.1, you can set\nspark.yarn.populateHadoopClasspath\nto\ntrue\n.\nUpgrading from Core 2.4 to 3.0\nThe\norg.apache.spark.ExecutorPlugin\ninterface and related configuration has been replaced with\norg.apache.spark.api.plugin.SparkPlugin\n, which adds new functionality. Plugins using the old\ninterface must be modified to extend the new interfaces. Check the\nMonitoring\nguide for more details.\nDeprecated method\nTaskContext.isRunningLocally\nhas been removed. Local execution was removed and it always has returned\nfalse\n.\nDeprecated method\nshuffleBytesWritten\n,\nshuffleWriteTime\nand\nshuffleRecordsWritten\nin\nShuffleWriteMetrics\nhave been removed. Instead, use\nbytesWritten\n,\nwriteTime\nand\nrecordsWritten\nrespectively.\nDeprecated method\nAccumulableInfo.apply\nhave been removed because creating\nAccumulableInfo\nis disallowed.\nDeprecated accumulator v1 APIs have been removed and please use v2 APIs instead.\nEvent log file will be written as UTF-8 encoding, and Spark History Server will replay event log files as UTF-8 encoding. Previously Spark wrote the event log file as default charset of driver JVM process, so Spark History Server of Spark 2.x is needed to read the old event log files in case of incompatible encoding.\nA new protocol for fetching shuffle blocks is used. It’s recommended that external shuffle services be upgraded when running Spark 3.0 apps. You can still use old external shuffle services by setting the configuration\nspark.shuffle.useOldFetchProtocol\nto\ntrue\n. Otherwise, Spark may run into errors with messages like\nIllegalArgumentException: Unexpected message type: <number>\n.\nSPARK_WORKER_INSTANCES\nis deprecated in Standalone mode. It’s recommended to launch multiple executors in one worker and launch one worker per node instead of launching multiple workers per node and launching one executor per worker."}
{"url": "https://spark.apache.org/docs/latest/streaming/ss-migration-guide.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nStructured Streaming Programming Guide\nOverview\nGetting Started\nAPIs on DataFrames and Datasets\nPerformance Tips\nAdditional Information\nMigration Guide: Structured Streaming\nNote that this migration guide describes the items specific to Structured Streaming.\nMany items of SQL migration can be applied when migrating Structured Streaming to higher versions.\nPlease refer\nMigration Guide: SQL, Datasets and DataFrame\n.\nUpgrading from Structured Streaming 3.5 to 4.0\nSince Spark 4.0, Spark falls back to single batch execution if any source in the query does not support\nTrigger.AvailableNow\n. This is to avoid any possible correctness, duplication, and dataloss issue due to incompatibility between source and wrapper implementation. (See\nSPARK-45178\nfor more details.)\nSince Spark 4.0, new configuration\nspark.sql.streaming.ratioExtraSpaceAllowedInCheckpoint\n(default:\n0.3\n) controls the amount of additional space allowed in the checkpoint directory to store stale version files for batch deletion inside maintenance task. This is to amortize the cost of listing in cloud store. Setting this to\n0\ndefaults to the old behavior. (See\nSPARK-48931\nfor more details.)\nSince Spark 4.0, when relative path is used to output data in\nDataStreamWriter\nthe resolution to absolute path is done in the Spark Driver and is not deferred to Spark Executor. This is to make Structured Streaming behavior similar to DataFrame API (\nDataFrameWriter\n). (See\nSPARK-50854\nfor more details.)\nSince Spark 4.0, the deprecated config\nspark.databricks.sql.optimizer.pruneFiltersCanPruneStreamingSubplan\nhas been removed. (See\nSPARK-51187\nfor more details.)\nUpgrading from Structured Streaming 3.3 to 3.4\nSince Spark 3.4,\nTrigger.Once\nis deprecated, and users are encouraged to migrate from\nTrigger.Once\nto\nTrigger.AvailableNow\n. Please refer\nSPARK-39805\nfor more details.\nSince Spark 3.4, the default value of configuration for Kafka offset fetching (\nspark.sql.streaming.kafka.useDeprecatedOffsetFetching\n) is changed from\ntrue\nto\nfalse\n. The default no longer relies consumer group based scheduling, which affect the required ACL. For further details please see\nStructured Streaming Kafka Integration\n.\nUpgrading from Structured Streaming 3.2 to 3.3\nSince Spark 3.3, all stateful operators require hash partitioning with exact grouping keys. In previous versions, all stateful operators except stream-stream join require loose partitioning criteria which opens the possibility on correctness issue. (See\nSPARK-38204\nfor more details.) To ensure backward compatibility, we retain the old behavior with the checkpoint built from older versions.\nUpgrading from Structured Streaming 3.0 to 3.1\nIn Spark 3.0 and before, for the queries that have stateful operation which can emit rows older than the current watermark plus allowed late record delay, which are “late rows” in downstream stateful operations and these rows can be discarded, Spark only prints a warning message. Since Spark 3.1, Spark will check for such queries with possible correctness issue and throw AnalysisException for it by default. For the users who understand the possible risk of correctness issue and still decide to run the query, please disable this check by setting the config\nspark.sql.streaming.statefulOperator.checkCorrectness.enabled\nto false.\nIn Spark 3.0 and before Spark uses\nKafkaConsumer\nfor offset fetching which could cause infinite wait in the driver.\nIn Spark 3.1 a new configuration option added\nspark.sql.streaming.kafka.useDeprecatedOffsetFetching\n(default:\ntrue\n)\nwhich could be set to\nfalse\nallowing Spark to use new offset fetching mechanism using\nAdminClient\n.\nFor further details please see\nStructured Streaming Kafka Integration\n.\nUpgrading from Structured Streaming 2.4 to 3.0\nIn Spark 3.0, Structured Streaming forces the source schema into nullable when file-based datasources such as text, json, csv, parquet and orc are used via\nspark.readStream(...)\n. Previously, it respected the nullability in source schema; however, it caused issues tricky to debug with NPE. To restore the previous behavior, set\nspark.sql.streaming.fileSource.schema.forceNullable\nto\nfalse\n.\nSpark 3.0 fixes the correctness issue on Stream-stream outer join, which changes the schema of state. (See\nSPARK-26154\nfor more details). If you start your query from checkpoint constructed from Spark 2.x which uses stream-stream outer join, Spark 3.0 fails the query. To recalculate outputs, discard the checkpoint and replay previous inputs.\nIn Spark 3.0, the deprecated class\norg.apache.spark.sql.streaming.ProcessingTime\nhas been removed. Use\norg.apache.spark.sql.streaming.Trigger.ProcessingTime\ninstead. Likewise,\norg.apache.spark.sql.execution.streaming.continuous.ContinuousTrigger\nhas been removed in favor of\nTrigger.Continuous\n, and\norg.apache.spark.sql.execution.streaming.OneTimeTrigger\nhas been hidden in favor of\nTrigger.Once\n."}
{"url": "https://spark.apache.org/docs/latest/pyspark-migration-guide.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMigration Guide: PySpark (Python on Spark)\nThe migration guide is now archived on\nthis page\n."}
{"url": "https://spark.apache.org/docs/latest/app-dev-spark-connect.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nApplication Development with Spark Connect\nSpark Connect Overview\nIn Apache Spark 3.4, Spark Connect introduced a decoupled client-server\narchitecture that allows remote connectivity to Spark clusters using the\nDataFrame API and unresolved logical plans as the protocol. The separation\nbetween client and server allows Spark and its open ecosystem to be\nleveraged from everywhere. It can be embedded in modern data applications,\nin IDEs, Notebooks and programming languages.\nTo learn more about Spark Connect, see\nSpark Connect Overview\n.\nRedefining Spark Applications using Spark Connect\nWith its decoupled client-server architecture, Spark Connect simplifies how Spark Applications are\ndeveloped.\nThe notion of Spark Client Applications and Spark Server Libraries are introduced as follows:\nSpark Client Applications\nare regular Spark applications that use Spark and its rich ecosystem for\ndistributed data processing. Examples include ETL pipelines, data preparation, and model training\nand inference.\nSpark Server Libraries\nbuild on, extend, and complement Spark’s functionality, e.g.\nMLlib\n(distributed ML libraries that use Spark’s powerful distributed processing). Spark Connect\ncan be extended to expose client-side interfaces for Spark Server Libraries.\nWith Spark 3.4 and Spark Connect, the development of Spark Client Applications is simplified, and\nclear extension points and guidelines are provided on how to build Spark Server Libraries, making\nit easy for both types of applications to evolve alongside Spark. As illustrated in Fig.1, Spark\nClient applications connect to Spark using the Spark Connect API, which is essentially the\nDataFrame API and fully declarative.\nSpark Server Libraries extend Spark. They typically provide additional server-side logic integrated\nwith Spark, which is exposed to client applications as part of the Spark Connect API, using Spark\nConnect extension points. For example, the\nSpark Server Library\nconsists of custom\nservice-side logic (as indicated by the blue box labeled\nCustom Library Plugin\n), which is exposed\nto the client via the blue box as part of the Spark Connect API. The client uses this API, e.g.,\nalongside PySpark or the Spark Scala client, making it easy for Spark client applications to work\nwith the custom logic/library.\nSpark API Mode: Spark Client and Spark Classic\nSpark provides the API mode,\nspark.api.mode\nconfiguration, enabling Spark Classic applications\nto seamlessly switch to Spark Connect. Depending on the value of\nspark.api.mode\n, the application\ncan run in either Spark Classic or Spark Connect mode. Here is an example:\nfrom\npyspark.sql\nimport\nSparkSession\nSparkSession\n.\nbuilder\n.\nconfig\n(\n\"\nspark.api.mode\n\"\n,\n\"\nconnect\n\"\n).\nmaster\n(\n\"\n...\n\"\n).\ngetOrCreate\n()\nYou can also apply this configuration to both Scala and PySpark applications when submitting yours:\nspark-submit\n--master\n\"...\"\n--conf\nspark.api.mode\n=\nconnect\nAdditionally, Spark Connect offers convenient options for local testing. By setting\nspark.remote\nto\nlocal[...]\nor\nlocal-cluster[...]\n, you can start a local Spark Connect server and access a Spark\nConnect session.\nThis is similar to using\n--conf spark.api.mode=connect\nwith\n--master ...\n. However, note that\nspark.remote\nand\n--remote\nare limited to\nlocal*\nvalues, while\n--conf spark.api.mode=connect\nwith\n--master ...\nsupports additional cluster URLs, such as spark://, for broader compatibility with\nSpark Classic.\nSpark Client Applications\nSpark Client Applications are the\nregular Spark applications\nthat Spark users develop today, e.g.,\nETL pipelines, data preparation, or model training or inference. These are typically built using\nSparks declarative DataFrame and DataSet APIs. With Spark Connect, the core behaviour remains the\nsame, but there are a few differences:\nLower-level, non-declarative APIs (RDDs) can no longer be directly used from Spark Client\napplications. Alternatives for missing RDD functionality are provided as part of the higher-level\nDataFrame API.\nClient applications no longer have direct access to the Spark driver JVM; they are fully\nseparated from the server.\nClient applications based on Spark Connect can be submitted in the same way as any previous job.\nIn addition, Spark Client Applications based on Spark Connect have several benefits compared to\nclassic Spark applications using earlier Spark versions (3.4 and below):\nUpgradability\n: Upgrading to new Spark Server versions is seamless, as the Spark Connect API\nabstracts any changes/improvements on the server side. Client- and server APIs are cleanly\nseparated.\nSimplicity\n: The number of APIs exposed to the user is reduced from 3 to 2. The Spark Connect API\nis fully declarative and consequently easy to learn for new users familiar with SQL.\nStability\n: When using Spark Connect, the client applications no longer run on the Spark driver\nand, therefore don’t cause and are not affected by any instability on the server.\nRemote connectivity\n: The decoupled architecture allows remote connectivity to Spark beyond SQL\nand JDBC: any application can now interactively use Spark “as a service”.\nBackwards compatibility\n: The Spark Connect API is code-compatible with earlier Spark versions,\nexcept for the usage of RDDs, for which a list of alternative APIs is provided in Spark Connect.\nSpark Server Libraries\nUntil Spark 3.4, extensions to Spark (e.g.,\nSpark ML\nor\nSpark-NLP\n) were built and deployed like Spark\nClient Applications. With Spark 3.4 and Spark Connect,  explicit extension points are offered to\nextend Spark via Spark Server Libraries. These extension points provide functionality that can be\nexposed to a client, which differs from existing extension points in Spark such as\nSparkSession extensions\nor\nSpark Plugins\n.\nGetting Started: Extending Spark with Spark Server Libraries\nSpark Connect is available and supports PySpark and Scala\napplications. We will walk through how to run an Apache Spark server with Spark\nConnect and connect to it from a client application using the Spark Connect client\nlibrary.\nA Spark Server Library consists of the following components, illustrated in Fig. 2:\nThe Spark Connect protocol extension (blue box\nProto\nAPI)\nA Spark Connect Plugin.\nThe application logic that extends Spark.\nThe client package that exposes the Spark Server Library application logic to the Spark Client\nApplication, alongside PySpark or the Scala Spark Client.\n(1) Spark Connect Protocol Extension\nTo extend Spark with a new Spark Server Library, developers can extend the three main operation\ntypes in the Spark Connect protocol:\nRelation\n,\nExpression\n, and\nCommand\n.\nmessage\nRelation\n{\noneof\nrel_type\n{\nRead\nread\n=\n1\n;\n// ...\ngoogle.protobuf.Any\nextension\n=\n998\n;\n}\n}\nmessage\nExpression\n{\noneof\nexpr_type\n{\nLiteral\nliteral\n=\n1\n;\n// ...\ngoogle.protobuf.Any\nextension\n=\n999\n;\n}\n}\nmessage\nCommand\n{\noneof\ncommand_type\n{\nWriteCommand\nwrite_command\n=\n1\n;\n// ...\ngoogle.protobuf.Any\nextension\n=\n999\n;\n}\n}\nTheir extension fields allow serializing arbitrary protobuf messages as part of the Spark Connect\nprotocol. These messages represent the parameters or state of the extension implementation.\nTo build a custom expression type, the developer first defines the custom protobuf definition\nof the expression.\nmessage\nExamplePluginExpression\n{\nExpression\nchild\n=\n1\n;\nstring\ncustom_field\n=\n2\n;\n}\n(2) Spark Connect Plugin implementation with (3) custom application logic\nAs a next step, the developer implements the\nExpressionPlugin\nclass of Spark Connect with custom\napplication logic based on the input parameters of the protobuf message.\nclass\nExampleExpressionPlugin\nextends\nExpressionPlugin\n{\noverride\ndef\ntransform\n(\nrelation\n:\nprotobuf.Any\n,\nplanner\n:\nSparkConnectPlanner\n)\n:\nOption\n[\nExpression\n]\n=\n{\n// Check if the serialized value of protobuf.Any matches the type\n// of our example expression.\nif\n(\n!\nrelation.is\n(\nclassOf\n[\nproto.ExamplePluginExpression\n]))\n{\nreturn\nNone\n}\nval\nexp\n=\nrelation.unpack\n(\nclassOf\n[\nproto.ExamplePluginExpression\n])\nSome\n(\nAlias\n(\nplanner.transformExpression\n(\nexp.getChild\n),\nexp.getCustomField\n)(\nexplicitMetadata\n=\nNone\n))\n}\n}\nOnce the application logic is developed, the code must be packaged as a jar and Spark must be\nconfigured to pick up the additional logic. The relevant Spark configuration options are:\nspark.jars\nwhich define the location of the Jar file containing the application logic built for\nthe custom expression.\nspark.connect.extensions.expression.classes\nspecifying the full class name\nof each expression extension loaded by Spark. Based on these configuration options, Spark will\nload the values at startup and make them available for processing.\n(4) Spark Server Library Client Package\nOnce the server component is deployed, any client can use it with the right protobuf messages.\nIn the example above, the following message payload sent to the Spark Connect endpoint would be\nenough to trigger the extension mechanism.\n{\n\"project\"\n:\n{\n\"input\"\n:\n{\n\"sql\"\n:\n{\n\"query\"\n:\n\"select * from samples.nyctaxi.trips\"\n}\n},\n\"expressions\"\n:\n[\n{\n\"extension\"\n:\n{\n\"typeUrl\"\n:\n\"type.googleapis.com/spark.connect.ExamplePluginExpression\"\n,\n\"value\"\n:\n\"\n\\n\\0\n06\n\\0\n22\n\\0\n04\n\\n\\0\n02id\n\\0\n22\n\\0\n06testval\"\n}\n}\n]\n}\n}\nTo make the example available in Python, the application developer provides a Python library that\nwraps the new expression and embeds it into PySpark. The easiest way to provide a function for any\nexpression is to take a PySpark column instance as an argument and return a new Column instance\nwith the expression applied.\nfrom\npyspark.sql.connect.column\nimport\nExpression\nimport\npyspark.sql.connect.proto\nas\nproto\nfrom\nmyxample.proto\nimport\nExamplePluginExpression\n# Internal class that satisfies the interface by the Python client\n# of Spark Connect to generate the protobuf representation from\n# an instance of the expression.\nclass\nExampleExpression\n(\nExpression\n):\ndef\nto_plan\n(\nself\n,\nsession\n)\n->\nproto\n.\nExpression\n:\nfun\n=\nproto\n.\nExpression\n()\nplugin\n=\nExamplePluginExpression\n()\nplugin\n.\nchild\n.\nliteral\n.\nlong\n=\n10\nplugin\n.\ncustom_field\n=\n\"\nexample\n\"\nfun\n.\nextension\n.\nPack\n(\nplugin\n)\nreturn\nfun\n# Defining the function to be used from the consumers.\ndef\nexample_expression\n(\ncol\n:\nColumn\n)\n->\nColumn\n:\nreturn\nColumn\n(\nExampleExpression\n())\n# Using the expression in the Spark Connect client code.\ndf\n=\nspark\n.\nread\n.\ntable\n(\n\"\nsamples.nyctaxi.trips\n\"\n)\ndf\n.\nselect\n(\nexample_expression\n(\ndf\n[\n\"\nfare_amount\n\"\n])).\ncollect\n()"}
{"url": "https://spark.apache.org/docs/latest/ml-advanced", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nAdvanced topics\nOptimization of linear methods (developer)\nLimited-memory BFGS (L-BFGS)\nNormal equation solver for weighted least squares\nIteratively reweighted least squares (IRLS)\n\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}} \n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\wv}{\\mathbf{w}}\n\\newcommand{\\av}{\\mathbf{\\alpha}}\n\\newcommand{\\bv}{\\mathbf{b}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\id}{\\mathbf{I}} \n\\newcommand{\\ind}{\\mathbf{1}} \n\\newcommand{\\0}{\\mathbf{0}} \n\\newcommand{\\unit}{\\mathbf{e}} \n\\newcommand{\\one}{\\mathbf{1}} \n\\newcommand{\\zero}{\\mathbf{0}}\n\\]\nOptimization of linear methods (developer)\nLimited-memory BFGS (L-BFGS)\nL-BFGS\nis an optimization \nalgorithm in the family of quasi-Newton methods to solve the optimization problems of the form\n$\\min_{\\wv \\in\\R^d} \\; f(\\wv)$\n. The L-BFGS method approximates the objective function locally as a \nquadratic without evaluating the second partial derivatives of the objective function to construct the \nHessian matrix. The Hessian matrix is approximated by previous gradient evaluations, so there is no \nvertical scalability issue (the number of training features) unlike computing the Hessian matrix \nexplicitly in Newton’s method. As a result, L-BFGS often achieves faster convergence compared with \nother first-order optimizations.\nOrthant-Wise Limited-memory\nQuasi-Newton\n(OWL-QN) is an extension of L-BFGS that can effectively handle L1 and elastic net regularization.\nL-BFGS is used as a solver for\nLinearRegression\n,\nLogisticRegression\n,\nAFTSurvivalRegression\nand\nMultilayerPerceptronClassifier\n.\nMLlib L-BFGS solver calls the corresponding implementation in\nbreeze\n.\nNormal equation solver for weighted least squares\nMLlib implements normal equation solver for\nweighted least squares\nby\nWeightedLeastSquares\n.\nGiven $n$ weighted observations $(w_i, a_i, b_i)$:\n$w_i$ the weight of i-th observation\n$a_i$ the features vector of i-th observation\n$b_i$ the label of i-th observation\nThe number of features for each observation is $m$. We use the following weighted least squares formulation:\n\\[   \n\\min_{\\mathbf{x}}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(\\mathbf{a}_i^T \\mathbf{x} -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{\\lambda}{\\delta}\\left[\\frac{1}{2}(1 - \\alpha)\\sum_{j=1}^m(\\sigma_j x_j)^2 + \\alpha\\sum_{j=1}^m |\\sigma_j x_j|\\right]\n\\]\nwhere $\\lambda$ is the regularization parameter, $\\alpha$ is the elastic-net mixing parameter, $\\delta$ is the population standard deviation of the label\nand $\\sigma_j$ is the population standard deviation of the j-th feature column.\nThis objective function requires only one pass over the data to collect the statistics necessary to solve it. For an\n$n \\times m$ data matrix, these statistics require only $O(m^2)$ storage and so can be stored on a single machine when $m$ (the number of features) is\nrelatively small. We can then solve the normal equations on a single machine using local methods like direct Cholesky factorization or iterative optimization programs.\nSpark MLlib currently supports two types of solvers for the normal equations: Cholesky factorization and Quasi-Newton methods (L-BFGS/OWL-QN). Cholesky factorization\ndepends on a positive definite covariance matrix (i.e. columns of the data matrix must be linearly independent) and will fail if this condition is violated. Quasi-Newton methods\nare still capable of providing a reasonable solution even when the covariance matrix is not positive definite, so the normal equation solver can also fall back to \nQuasi-Newton methods in this case. This fallback is currently always enabled for the\nLinearRegression\nand\nGeneralizedLinearRegression\nestimators.\nWeightedLeastSquares\nsupports L1, L2, and elastic-net regularization and provides options to enable or disable regularization and standardization. In the case where no \nL1 regularization is applied (i.e. $\\alpha = 0$), there exists an analytical solution and either Cholesky or Quasi-Newton solver may be used. When $\\alpha > 0$ no analytical \nsolution exists and we instead use the Quasi-Newton solver to find the coefficients iteratively.\nIn order to make the normal equation approach efficient,\nWeightedLeastSquares\nrequires that the number of features is no more than 4096. For larger problems, use L-BFGS instead.\nIteratively reweighted least squares (IRLS)\nMLlib implements\niteratively reweighted least squares (IRLS)\nby\nIterativelyReweightedLeastSquares\n.\nIt can be used to find the maximum likelihood estimates of a generalized linear model (GLM), find M-estimator in robust regression and other optimization problems.\nRefer to\nIteratively Reweighted Least Squares for Maximum Likelihood Estimation, and some Robust and Resistant Alternatives\nfor more information.\nIt solves certain optimization problems iteratively through the following procedure:\nlinearize the objective at current solution and update corresponding weight.\nsolve a weighted least squares (WLS) problem by WeightedLeastSquares.\nrepeat above steps until convergence.\nSince it involves solving a weighted least squares (WLS) problem by\nWeightedLeastSquares\nin each iteration,\nit also requires the number of features to be no more than 4096.\nCurrently IRLS is used as the default solver of\nGeneralizedLinearRegression\n."}
{"url": "https://spark.apache.org/docs/latest/mllib-decision-tree.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nDecision Trees - RDD-based API\nBasic algorithm\nNode impurity and information gain\nSplit candidates\nStopping rule\nUsage tips\nProblem specification parameters\nStopping criteria\nTunable parameters\nCaching and checkpointing\nScaling\nExamples\nClassification\nRegression\nDecision trees\nand their ensembles are popular methods for the machine learning tasks of\nclassification and regression. Decision trees are widely used since they are easy to interpret,\nhandle categorical features, extend to the multiclass classification setting, do not require\nfeature scaling, and are able to capture non-linearities and feature interactions. Tree ensemble\nalgorithms such as random forests and boosting are among the top performers for classification and\nregression tasks.\nspark.mllib\nsupports decision trees for binary and multiclass classification and for regression,\nusing both continuous and categorical features. The implementation partitions data by rows,\nallowing distributed training with millions of instances.\nEnsembles of trees (Random Forests and Gradient-Boosted Trees) are described in the\nEnsembles guide\n.\nBasic algorithm\nThe decision tree is a greedy algorithm that performs a recursive binary partitioning of the feature\nspace.  The tree predicts the same label for each bottommost (leaf) partition.\nEach partition is chosen greedily by selecting the\nbest split\nfrom a set of possible splits,\nin order to maximize the information gain at a tree node. In other words, the split chosen at each\ntree node is chosen from the set\n$\\underset{s}{\\operatorname{argmax}} IG(D,s)$\nwhere\n$IG(D,s)$\nis the information gain when a split\n$s$\nis applied to a dataset\n$D$\n.\nNode impurity and information gain\nThe\nnode impurity\nis a measure of the homogeneity of the labels at the node. The current\nimplementation provides two impurity measures for classification (Gini impurity and entropy) and one\nimpurity measure for regression (variance).\nImpurity\nTask\nFormula\nDescription\nGini impurity\nClassification\n$\\sum_{i=1}^{C} f_i(1-f_i)$\n$f_i$ is the frequency of label $i$ at a node and $C$ is the number of unique labels.\nEntropy\nClassification\n$\\sum_{i=1}^{C} -f_ilog(f_i)$\n$f_i$ is the frequency of label $i$ at a node and $C$ is the number of unique labels.\nVariance\nRegression\n$\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\mu)^2$\n$y_i$ is label for an instance,\n      $N$ is the number of instances and $\\mu$ is the mean given by $\\frac{1}{N} \\sum_{i=1}^N y_i$.\nThe\ninformation gain\nis the difference between the parent node impurity and the weighted sum of\nthe two child node impurities. Assuming that a split $s$ partitions the dataset\n$D$\nof size\n$N$\ninto two datasets\n$D_{left}$\nand\n$D_{right}$\nof sizes\n$N_{left}$\nand\n$N_{right}$\n,\nrespectively, the information gain is:\n$IG(D,s) = Impurity(D) - \\frac{N_{left}}{N} Impurity(D_{left}) - \\frac{N_{right}}{N} Impurity(D_{right})$\nSplit candidates\nContinuous features\nFor small datasets in single-machine implementations, the split candidates for each continuous\nfeature are typically the unique values for the feature. Some implementations sort the feature\nvalues and then use the ordered unique values as split candidates for faster tree calculations.\nSorting feature values is expensive for large distributed datasets.\nThis implementation computes an approximate set of split candidates by performing a quantile\ncalculation over a sampled fraction of the data.\nThe ordered splits create “bins” and the maximum number of such\nbins can be specified using the\nmaxBins\nparameter.\nNote that the number of bins cannot be greater than the number of instances\n$N$\n(a rare scenario\nsince the default\nmaxBins\nvalue is 32). The tree algorithm automatically reduces the number of\nbins if the condition is not satisfied.\nCategorical features\nFor a categorical feature with\n$M$\npossible values (categories), one could come up with\n$2^{M-1}-1$\nsplit candidates. For binary (0/1) classification and regression,\nwe can reduce the number of split candidates to\n$M-1$\nby ordering the\ncategorical feature values by the average label. (See Section 9.2.4 in\nElements of Statistical Machine Learning\nfor\ndetails.) For example, for a binary classification problem with one categorical feature with three\ncategories A, B and C whose corresponding proportions of label 1 are 0.2, 0.6 and 0.4, the categorical\nfeatures are ordered as A, C, B. The two split candidates are A | C, B\nand A , C | B where | denotes the split.\nIn multiclass classification, all\n$2^{M-1}-1$\npossible splits are used whenever possible.\nWhen\n$2^{M-1}-1$\nis greater than the\nmaxBins\nparameter, we use a (heuristic) method\nsimilar to the method used for binary classification and regression.\nThe\n$M$\ncategorical feature values are ordered by impurity,\nand the resulting\n$M-1$\nsplit candidates are considered.\nStopping rule\nThe recursive tree construction is stopped at a node when one of the following conditions is met:\nThe node depth is equal to the\nmaxDepth\ntraining parameter.\nNo split candidate leads to an information gain greater than\nminInfoGain\n.\nNo split candidate produces child nodes which each have at least\nminInstancesPerNode\ntraining instances.\nUsage tips\nWe include a few guidelines for using decision trees by discussing the various parameters.\nThe parameters are listed below roughly in order of descending importance.  New users should mainly consider the “Problem specification parameters” section and the\nmaxDepth\nparameter.\nProblem specification parameters\nThese parameters describe the problem you want to solve and your dataset.\nThey should be specified and do not require tuning.\nalgo\n: Type of decision tree, either\nClassification\nor\nRegression\n.\nnumClasses\n: Number of classes (for\nClassification\nonly).\ncategoricalFeaturesInfo\n: Specifies which features are categorical and how many categorical values each of those features can take.  This is given as a map from feature indices to feature arity (number of categories).  Any features not in this map are treated as continuous.\nFor example,\nMap(0 -> 2, 4 -> 10)\nspecifies that feature\n0\nis binary (taking values\n0\nor\n1\n) and that feature\n4\nhas 10 categories (values\n{0, 1, ..., 9}\n).  Note that feature indices are 0-based: features\n0\nand\n4\nare the 1st and 5th elements of an instance’s feature vector.\nNote that you do not have to specify\ncategoricalFeaturesInfo\n.  The algorithm will still run and may get reasonable results.  However, performance should be better if categorical features are properly designated.\nStopping criteria\nThese parameters determine when the tree stops building (adding new nodes).\nWhen tuning these parameters, be careful to validate on held-out test data to avoid overfitting.\nmaxDepth\n: Maximum depth of a tree.  Deeper trees are more expressive (potentially allowing higher accuracy), but they are also more costly to train and are more likely to overfit.\nminInstancesPerNode\n: For a node to be split further, each of its children must receive at least this number of training instances.  This is commonly used with\nRandomForest\nsince those are often trained deeper than individual trees.\nminInfoGain\n: For a node to be split further, the split must improve at least this much (in terms of information gain).\nTunable parameters\nThese parameters may be tuned.  Be careful to validate on held-out test data when tuning in order to avoid overfitting.\nmaxBins\n: Number of bins used when discretizing continuous features.\nIncreasing\nmaxBins\nallows the algorithm to consider more split candidates and make fine-grained split decisions.  However, it also increases computation and communication.\nNote that the\nmaxBins\nparameter must be at least the maximum number of categories\n$M$\nfor any categorical feature.\nmaxMemoryInMB\n: Amount of memory to be used for collecting sufficient statistics.\nThe default value is conservatively chosen to be 256 MiB to allow the decision algorithm to work in most scenarios.  Increasing\nmaxMemoryInMB\ncan lead to faster training (if the memory is available) by allowing fewer passes over the data.  However, there may be decreasing returns as\nmaxMemoryInMB\ngrows since the amount of communication on each iteration can be proportional to\nmaxMemoryInMB\n.\nImplementation details\n: For faster processing, the decision tree algorithm collects statistics about groups of nodes to split (rather than 1 node at a time).  The number of nodes which can be handled in one group is determined by the memory requirements (which vary per features).  The\nmaxMemoryInMB\nparameter specifies the memory limit in terms of megabytes which each worker can use for these statistics.\nsubsamplingRate\n: Fraction of the training data used for learning the decision tree.  This parameter is most relevant for training ensembles of trees (using\nRandomForest\nand\nGradientBoostedTrees\n), where it can be useful to subsample the original data.  For training a single decision tree, this parameter is less useful since the number of training instances is generally not the main constraint.\nimpurity\n: Impurity measure (discussed above) used to choose between candidate splits.  This measure must match the\nalgo\nparameter.\nCaching and checkpointing\nMLlib 1.2 adds several features for scaling up to larger (deeper) trees and tree ensembles.  When\nmaxDepth\nis set to be large, it can be useful to turn on node ID caching and checkpointing.  These parameters are also useful for\nRandomForest\nwhen\nnumTrees\nis set to be large.\nuseNodeIdCache\n: If this is set to true, the algorithm will avoid passing the current model (tree or trees) to executors on each iteration.\nThis can be useful with deep trees (speeding up computation on workers) and for large Random Forests (reducing communication on each iteration).\nImplementation details\n: By default, the algorithm communicates the current model to executors so that executors can match training instances with tree nodes.  When this setting is turned on, then the algorithm will instead cache this information.\nNode ID caching generates a sequence of RDDs (1 per iteration).  This long lineage can cause performance problems, but checkpointing intermediate RDDs can alleviate those problems.\nNote that checkpointing is only applicable when\nuseNodeIdCache\nis set to true.\ncheckpointDir\n: Directory for checkpointing node ID cache RDDs.\ncheckpointInterval\n: Frequency for checkpointing node ID cache RDDs.  Setting this too low will cause extra overhead from writing to HDFS; setting this too high can cause problems if executors fail and the RDD needs to be recomputed.\nScaling\nComputation scales approximately linearly in the number of training instances,\nin the number of features, and in the\nmaxBins\nparameter.\nCommunication scales approximately linearly in the number of features and in\nmaxBins\n.\nThe implemented algorithm reads both sparse and dense data. However, it is not optimized for sparse input.\nExamples\nClassification\nThe example below demonstrates how to load a\nLIBSVM data file\n,\nparse it as an RDD of\nLabeledPoint\nand then\nperform classification using a decision tree with Gini impurity as an impurity measure and a\nmaximum tree depth of 5. The test error is calculated to measure the algorithm accuracy.\nRefer to the\nDecisionTree\nPython docs\nand\nDecisionTreeModel\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.tree\nimport\nDecisionTree\n,\nDecisionTreeModel\nfrom\npyspark.mllib.util\nimport\nMLUtils\n# Load and parse the data file into an RDD of LabeledPoint.\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n'\ndata/mllib/sample_libsvm_data.txt\n'\n)\n# Split the data into training and test sets (30% held out for testing)\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n([\n0.7\n,\n0.3\n])\n# Train a DecisionTree model.\n#  Empty categoricalFeaturesInfo indicates all features are continuous.\nmodel\n=\nDecisionTree\n.\ntrainClassifier\n(\ntrainingData\n,\nnumClasses\n=\n2\n,\ncategoricalFeaturesInfo\n=\n{},\nimpurity\n=\n'\ngini\n'\n,\nmaxDepth\n=\n5\n,\nmaxBins\n=\n32\n)\n# Evaluate model on test instances and compute test error\npredictions\n=\nmodel\n.\npredict\n(\ntestData\n.\nmap\n(\nlambda\nx\n:\nx\n.\nfeatures\n))\nlabelsAndPredictions\n=\ntestData\n.\nmap\n(\nlambda\nlp\n:\nlp\n.\nlabel\n).\nzip\n(\npredictions\n)\ntestErr\n=\nlabelsAndPredictions\n.\nfilter\n(\nlambda\nlp\n:\nlp\n[\n0\n]\n!=\nlp\n[\n1\n]).\ncount\n()\n/\nfloat\n(\ntestData\n.\ncount\n())\nprint\n(\n'\nTest Error =\n'\n+\nstr\n(\ntestErr\n))\nprint\n(\n'\nLearned classification tree model:\n'\n)\nprint\n(\nmodel\n.\ntoDebugString\n())\n# Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"\ntarget/tmp/myDecisionTreeClassificationModel\n\"\n)\nsameModel\n=\nDecisionTreeModel\n.\nload\n(\nsc\n,\n\"\ntarget/tmp/myDecisionTreeClassificationModel\n\"\n)\nFind full example code at \"examples/src/main/python/mllib/decision_tree_classification_example.py\" in the Spark repo.\nRefer to the\nDecisionTree\nScala docs\nand\nDecisionTreeModel\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.tree.DecisionTree\nimport\norg.apache.spark.mllib.tree.model.DecisionTreeModel\nimport\norg.apache.spark.mllib.util.MLUtils\n// Load and parse the data file.\nval\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Split the data into training and test sets (30% held out for testing)\nval\nsplits\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.7\n,\n0.3\n))\nval\n(\ntrainingData\n,\ntestData\n)\n=\n(\nsplits\n(\n0\n),\nsplits\n(\n1\n))\n// Train a DecisionTree model.\n//  Empty categoricalFeaturesInfo indicates all features are continuous.\nval\nnumClasses\n=\n2\nval\ncategoricalFeaturesInfo\n=\nMap\n[\nInt\n,\nInt\n]()\nval\nimpurity\n=\n\"gini\"\nval\nmaxDepth\n=\n5\nval\nmaxBins\n=\n32\nval\nmodel\n=\nDecisionTree\n.\ntrainClassifier\n(\ntrainingData\n,\nnumClasses\n,\ncategoricalFeaturesInfo\n,\nimpurity\n,\nmaxDepth\n,\nmaxBins\n)\n// Evaluate model on test instances and compute test error\nval\nlabelAndPreds\n=\ntestData\n.\nmap\n{\npoint\n=>\nval\nprediction\n=\nmodel\n.\npredict\n(\npoint\n.\nfeatures\n)\n(\npoint\n.\nlabel\n,\nprediction\n)\n}\nval\ntestErr\n=\nlabelAndPreds\n.\nfilter\n(\nr\n=>\nr\n.\n_1\n!=\nr\n.\n_2\n).\ncount\n().\ntoDouble\n/\ntestData\n.\ncount\n()\nprintln\n(\ns\n\"Test Error = $testErr\"\n)\nprintln\n(\ns\n\"Learned classification tree model:\\n ${model.toDebugString}\"\n)\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/myDecisionTreeClassificationModel\"\n)\nval\nsameModel\n=\nDecisionTreeModel\n.\nload\n(\nsc\n,\n\"target/tmp/myDecisionTreeClassificationModel\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\" in the Spark repo.\nRefer to the\nDecisionTree\nJava docs\nand\nDecisionTreeModel\nJava docs\nfor details on the API.\nimport\njava.util.HashMap\n;\nimport\njava.util.Map\n;\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.SparkConf\n;\nimport\norg.apache.spark.api.java.JavaPairRDD\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.api.java.JavaSparkContext\n;\nimport\norg.apache.spark.mllib.regression.LabeledPoint\n;\nimport\norg.apache.spark.mllib.tree.DecisionTree\n;\nimport\norg.apache.spark.mllib.tree.model.DecisionTreeModel\n;\nimport\norg.apache.spark.mllib.util.MLUtils\n;\nSparkConf\nsparkConf\n=\nnew\nSparkConf\n().\nsetAppName\n(\n\"JavaDecisionTreeClassificationExample\"\n);\nJavaSparkContext\njsc\n=\nnew\nJavaSparkContext\n(\nsparkConf\n);\n// Load and parse the data file.\nString\ndatapath\n=\n\"data/mllib/sample_libsvm_data.txt\"\n;\nJavaRDD\n<\nLabeledPoint\n>\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\njsc\n.\nsc\n(),\ndatapath\n).\ntoJavaRDD\n();\n// Split the data into training and test sets (30% held out for testing)\nJavaRDD\n<\nLabeledPoint\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]{\n0.7\n,\n0.3\n});\nJavaRDD\n<\nLabeledPoint\n>\ntrainingData\n=\nsplits\n[\n0\n];\nJavaRDD\n<\nLabeledPoint\n>\ntestData\n=\nsplits\n[\n1\n];\n// Set parameters.\n//  Empty categoricalFeaturesInfo indicates all features are continuous.\nint\nnumClasses\n=\n2\n;\nMap\n<\nInteger\n,\nInteger\n>\ncategoricalFeaturesInfo\n=\nnew\nHashMap\n<>();\nString\nimpurity\n=\n\"gini\"\n;\nint\nmaxDepth\n=\n5\n;\nint\nmaxBins\n=\n32\n;\n// Train a DecisionTree model for classification.\nDecisionTreeModel\nmodel\n=\nDecisionTree\n.\ntrainClassifier\n(\ntrainingData\n,\nnumClasses\n,\ncategoricalFeaturesInfo\n,\nimpurity\n,\nmaxDepth\n,\nmaxBins\n);\n// Evaluate model on test instances and compute test error\nJavaPairRDD\n<\nDouble\n,\nDouble\n>\npredictionAndLabel\n=\ntestData\n.\nmapToPair\n(\np\n->\nnew\nTuple2\n<>(\nmodel\n.\npredict\n(\np\n.\nfeatures\n()),\np\n.\nlabel\n()));\ndouble\ntestErr\n=\npredictionAndLabel\n.\nfilter\n(\npl\n->\n!\npl\n.\n_1\n().\nequals\n(\npl\n.\n_2\n())).\ncount\n()\n/\n(\ndouble\n)\ntestData\n.\ncount\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Test Error: \"\n+\ntestErr\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Learned classification tree model:\\n\"\n+\nmodel\n.\ntoDebugString\n());\n// Save and load model\nmodel\n.\nsave\n(\njsc\n.\nsc\n(),\n\"target/tmp/myDecisionTreeClassificationModel\"\n);\nDecisionTreeModel\nsameModel\n=\nDecisionTreeModel\n.\nload\n(\njsc\n.\nsc\n(),\n\"target/tmp/myDecisionTreeClassificationModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\" in the Spark repo.\nRegression\nThe example below demonstrates how to load a\nLIBSVM data file\n,\nparse it as an RDD of\nLabeledPoint\nand then\nperform regression using a decision tree with variance as an impurity measure and a maximum tree\ndepth of 5. The Mean Squared Error (MSE) is computed at the end to evaluate\ngoodness of fit\n.\nRefer to the\nDecisionTree\nPython docs\nand\nDecisionTreeModel\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.tree\nimport\nDecisionTree\n,\nDecisionTreeModel\nfrom\npyspark.mllib.util\nimport\nMLUtils\n# Load and parse the data file into an RDD of LabeledPoint.\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n'\ndata/mllib/sample_libsvm_data.txt\n'\n)\n# Split the data into training and test sets (30% held out for testing)\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n([\n0.7\n,\n0.3\n])\n# Train a DecisionTree model.\n#  Empty categoricalFeaturesInfo indicates all features are continuous.\nmodel\n=\nDecisionTree\n.\ntrainRegressor\n(\ntrainingData\n,\ncategoricalFeaturesInfo\n=\n{},\nimpurity\n=\n'\nvariance\n'\n,\nmaxDepth\n=\n5\n,\nmaxBins\n=\n32\n)\n# Evaluate model on test instances and compute test error\npredictions\n=\nmodel\n.\npredict\n(\ntestData\n.\nmap\n(\nlambda\nx\n:\nx\n.\nfeatures\n))\nlabelsAndPredictions\n=\ntestData\n.\nmap\n(\nlambda\nlp\n:\nlp\n.\nlabel\n).\nzip\n(\npredictions\n)\ntestMSE\n=\nlabelsAndPredictions\n.\nmap\n(\nlambda\nlp\n:\n(\nlp\n[\n0\n]\n-\nlp\n[\n1\n])\n*\n(\nlp\n[\n0\n]\n-\nlp\n[\n1\n])).\nsum\n()\n/\n\\\nfloat\n(\ntestData\n.\ncount\n())\nprint\n(\n'\nTest Mean Squared Error =\n'\n+\nstr\n(\ntestMSE\n))\nprint\n(\n'\nLearned regression tree model:\n'\n)\nprint\n(\nmodel\n.\ntoDebugString\n())\n# Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"\ntarget/tmp/myDecisionTreeRegressionModel\n\"\n)\nsameModel\n=\nDecisionTreeModel\n.\nload\n(\nsc\n,\n\"\ntarget/tmp/myDecisionTreeRegressionModel\n\"\n)\nFind full example code at \"examples/src/main/python/mllib/decision_tree_regression_example.py\" in the Spark repo.\nRefer to the\nDecisionTree\nScala docs\nand\nDecisionTreeModel\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.tree.DecisionTree\nimport\norg.apache.spark.mllib.tree.model.DecisionTreeModel\nimport\norg.apache.spark.mllib.util.MLUtils\n// Load and parse the data file.\nval\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Split the data into training and test sets (30% held out for testing)\nval\nsplits\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.7\n,\n0.3\n))\nval\n(\ntrainingData\n,\ntestData\n)\n=\n(\nsplits\n(\n0\n),\nsplits\n(\n1\n))\n// Train a DecisionTree model.\n//  Empty categoricalFeaturesInfo indicates all features are continuous.\nval\ncategoricalFeaturesInfo\n=\nMap\n[\nInt\n,\nInt\n]()\nval\nimpurity\n=\n\"variance\"\nval\nmaxDepth\n=\n5\nval\nmaxBins\n=\n32\nval\nmodel\n=\nDecisionTree\n.\ntrainRegressor\n(\ntrainingData\n,\ncategoricalFeaturesInfo\n,\nimpurity\n,\nmaxDepth\n,\nmaxBins\n)\n// Evaluate model on test instances and compute test error\nval\nlabelsAndPredictions\n=\ntestData\n.\nmap\n{\npoint\n=>\nval\nprediction\n=\nmodel\n.\npredict\n(\npoint\n.\nfeatures\n)\n(\npoint\n.\nlabel\n,\nprediction\n)\n}\nval\ntestMSE\n=\nlabelsAndPredictions\n.\nmap\n{\ncase\n(\nv\n,\np\n)\n=>\nmath\n.\npow\n(\nv\n-\np\n,\n2\n)\n}.\nmean\n()\nprintln\n(\ns\n\"Test Mean Squared Error = $testMSE\"\n)\nprintln\n(\ns\n\"Learned regression tree model:\\n ${model.toDebugString}\"\n)\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/myDecisionTreeRegressionModel\"\n)\nval\nsameModel\n=\nDecisionTreeModel\n.\nload\n(\nsc\n,\n\"target/tmp/myDecisionTreeRegressionModel\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\" in the Spark repo.\nRefer to the\nDecisionTree\nJava docs\nand\nDecisionTreeModel\nJava docs\nfor details on the API.\nimport\njava.util.HashMap\n;\nimport\njava.util.Map\n;\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.SparkConf\n;\nimport\norg.apache.spark.api.java.JavaPairRDD\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.api.java.JavaSparkContext\n;\nimport\norg.apache.spark.mllib.regression.LabeledPoint\n;\nimport\norg.apache.spark.mllib.tree.DecisionTree\n;\nimport\norg.apache.spark.mllib.tree.model.DecisionTreeModel\n;\nimport\norg.apache.spark.mllib.util.MLUtils\n;\nSparkConf\nsparkConf\n=\nnew\nSparkConf\n().\nsetAppName\n(\n\"JavaDecisionTreeRegressionExample\"\n);\nJavaSparkContext\njsc\n=\nnew\nJavaSparkContext\n(\nsparkConf\n);\n// Load and parse the data file.\nString\ndatapath\n=\n\"data/mllib/sample_libsvm_data.txt\"\n;\nJavaRDD\n<\nLabeledPoint\n>\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\njsc\n.\nsc\n(),\ndatapath\n).\ntoJavaRDD\n();\n// Split the data into training and test sets (30% held out for testing)\nJavaRDD\n<\nLabeledPoint\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]{\n0.7\n,\n0.3\n});\nJavaRDD\n<\nLabeledPoint\n>\ntrainingData\n=\nsplits\n[\n0\n];\nJavaRDD\n<\nLabeledPoint\n>\ntestData\n=\nsplits\n[\n1\n];\n// Set parameters.\n// Empty categoricalFeaturesInfo indicates all features are continuous.\nMap\n<\nInteger\n,\nInteger\n>\ncategoricalFeaturesInfo\n=\nnew\nHashMap\n<>();\nString\nimpurity\n=\n\"variance\"\n;\nint\nmaxDepth\n=\n5\n;\nint\nmaxBins\n=\n32\n;\n// Train a DecisionTree model.\nDecisionTreeModel\nmodel\n=\nDecisionTree\n.\ntrainRegressor\n(\ntrainingData\n,\ncategoricalFeaturesInfo\n,\nimpurity\n,\nmaxDepth\n,\nmaxBins\n);\n// Evaluate model on test instances and compute test error\nJavaPairRDD\n<\nDouble\n,\nDouble\n>\npredictionAndLabel\n=\ntestData\n.\nmapToPair\n(\np\n->\nnew\nTuple2\n<>(\nmodel\n.\npredict\n(\np\n.\nfeatures\n()),\np\n.\nlabel\n()));\ndouble\ntestMSE\n=\npredictionAndLabel\n.\nmapToDouble\n(\npl\n->\n{\ndouble\ndiff\n=\npl\n.\n_1\n()\n-\npl\n.\n_2\n();\nreturn\ndiff\n*\ndiff\n;\n}).\nmean\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Test Mean Squared Error: \"\n+\ntestMSE\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Learned regression tree model:\\n\"\n+\nmodel\n.\ntoDebugString\n());\n// Save and load model\nmodel\n.\nsave\n(\njsc\n.\nsc\n(),\n\"target/tmp/myDecisionTreeRegressionModel\"\n);\nDecisionTreeModel\nsameModel\n=\nDecisionTreeModel\n.\nload\n(\njsc\n.\nsc\n(),\n\"target/tmp/myDecisionTreeRegressionModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\" in the Spark repo."}
{"url": "https://spark.apache.org/docs/latest/mllib-ensembles.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nEnsembles - RDD-based API\nGradient-Boosted Trees vs. Random Forests\nRandom Forests\nBasic algorithm\nTraining\nPrediction\nUsage tips\nExamples\nClassification\nRegression\nGradient-Boosted Trees (GBTs)\nBasic algorithm\nLosses\nUsage tips\nValidation while training\nExamples\nClassification\nRegression\nAn\nensemble method\nis a learning algorithm which creates a model composed of a set of other base models.\nspark.mllib\nsupports two major ensemble algorithms:\nGradientBoostedTrees\nand\nRandomForest\n.\nBoth use\ndecision trees\nas their base models.\nGradient-Boosted Trees vs. Random Forests\nBoth\nGradient-Boosted Trees (GBTs)\nand\nRandom Forests\nare algorithms for learning ensembles of trees, but the training processes are different.  There are several practical trade-offs:\nGBTs train one tree at a time, so they can take longer to train than random forests.  Random Forests can train multiple trees in parallel.\nOn the other hand, it is often reasonable to use smaller (shallower) trees with GBTs than with Random Forests, and training smaller trees takes less time.\nRandom Forests can be less prone to overfitting.  Training more trees in a Random Forest reduces the likelihood of overfitting, but training more trees with GBTs increases the likelihood of overfitting.  (In statistical language, Random Forests reduce variance by using more trees, whereas GBTs reduce bias by using more trees.)\nRandom Forests can be easier to tune since performance improves monotonically with the number of trees (whereas performance can start to decrease for GBTs if the number of trees grows too large).\nIn short, both algorithms can be effective, and the choice should be based on the particular dataset.\nRandom Forests\nRandom forests\nare ensembles of\ndecision trees\n.\nRandom forests are one of the most successful machine learning models for classification and\nregression.  They combine many decision trees in order to reduce the risk of overfitting.\nLike decision trees, random forests handle categorical features,\nextend to the multiclass classification setting, do not require\nfeature scaling, and are able to capture non-linearities and feature interactions.\nspark.mllib\nsupports random forests for binary and multiclass classification and for regression,\nusing both continuous and categorical features.\nspark.mllib\nimplements random forests using the existing\ndecision tree\nimplementation.  Please see the decision tree guide for more information on trees.\nBasic algorithm\nRandom forests train a set of decision trees separately, so the training can be done in parallel.\nThe algorithm injects randomness into the training process so that each decision tree is a bit\ndifferent.  Combining the predictions from each tree reduces the variance of the predictions,\nimproving the performance on test data.\nTraining\nThe randomness injected into the training process includes:\nSubsampling the original dataset on each iteration to get a different training set (a.k.a. bootstrapping).\nConsidering different random subsets of features to split on at each tree node.\nApart from these randomizations, decision tree training is done in the same way as for individual decision trees.\nPrediction\nTo make a prediction on a new instance, a random forest must aggregate the predictions from its set of decision trees.  This aggregation is done differently for classification and regression.\nClassification\n: Majority vote. Each tree’s prediction is counted as a vote for one class.  The label is predicted to be the class which receives the most votes.\nRegression\n: Averaging. Each tree predicts a real value.  The label is predicted to be the average of the tree predictions.\nUsage tips\nWe include a few guidelines for using random forests by discussing the various parameters.\nWe omit some decision tree parameters since those are covered in the\ndecision tree guide\n.\nThe first two parameters we mention are the most important, and tuning them can often improve performance:\nnumTrees\n: Number of trees in the forest.\nIncreasing the number of trees will decrease the variance in predictions, improving the model’s test-time accuracy.\nTraining time increases roughly linearly in the number of trees.\nmaxDepth\n: Maximum depth of each tree in the forest.\nIncreasing the depth makes the model more expressive and powerful.  However, deep trees take longer to train and are also more prone to overfitting.\nIn general, it is acceptable to train deeper trees when using random forests than when using a single decision tree.  One tree is more likely to overfit than a random forest (because of the variance reduction from averaging multiple trees in the forest).\nThe next two parameters generally do not require tuning.  However, they can be tuned to speed up training.\nsubsamplingRate\n: This parameter specifies the size of the dataset used for training each tree in the forest, as a fraction of the size of the original dataset.  The default (1.0) is recommended, but decreasing this fraction can speed up training.\nfeatureSubsetStrategy\n: Number of features to use as candidates for splitting at each tree node.  The number is specified as a fraction or function of the total number of features.  Decreasing this number will speed up training, but can sometimes impact performance if too low.\nExamples\nClassification\nThe example below demonstrates how to load a\nLIBSVM data file\n,\nparse it as an RDD of\nLabeledPoint\nand then\nperform classification using a Random Forest.\nThe test error is calculated to measure the algorithm accuracy.\nRefer to the\nRandomForest\nPython docs\nand\nRandomForest\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.tree\nimport\nRandomForest\n,\nRandomForestModel\nfrom\npyspark.mllib.util\nimport\nMLUtils\n# Load and parse the data file into an RDD of LabeledPoint.\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n'\ndata/mllib/sample_libsvm_data.txt\n'\n)\n# Split the data into training and test sets (30% held out for testing)\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n([\n0.7\n,\n0.3\n])\n# Train a RandomForest model.\n#  Empty categoricalFeaturesInfo indicates all features are continuous.\n#  Note: Use larger numTrees in practice.\n#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\nmodel\n=\nRandomForest\n.\ntrainClassifier\n(\ntrainingData\n,\nnumClasses\n=\n2\n,\ncategoricalFeaturesInfo\n=\n{},\nnumTrees\n=\n3\n,\nfeatureSubsetStrategy\n=\n\"\nauto\n\"\n,\nimpurity\n=\n'\ngini\n'\n,\nmaxDepth\n=\n4\n,\nmaxBins\n=\n32\n)\n# Evaluate model on test instances and compute test error\npredictions\n=\nmodel\n.\npredict\n(\ntestData\n.\nmap\n(\nlambda\nx\n:\nx\n.\nfeatures\n))\nlabelsAndPredictions\n=\ntestData\n.\nmap\n(\nlambda\nlp\n:\nlp\n.\nlabel\n).\nzip\n(\npredictions\n)\ntestErr\n=\nlabelsAndPredictions\n.\nfilter\n(\nlambda\nlp\n:\nlp\n[\n0\n]\n!=\nlp\n[\n1\n]).\ncount\n()\n/\nfloat\n(\ntestData\n.\ncount\n())\nprint\n(\n'\nTest Error =\n'\n+\nstr\n(\ntestErr\n))\nprint\n(\n'\nLearned classification forest model:\n'\n)\nprint\n(\nmodel\n.\ntoDebugString\n())\n# Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"\ntarget/tmp/myRandomForestClassificationModel\n\"\n)\nsameModel\n=\nRandomForestModel\n.\nload\n(\nsc\n,\n\"\ntarget/tmp/myRandomForestClassificationModel\n\"\n)\nFind full example code at \"examples/src/main/python/mllib/random_forest_classification_example.py\" in the Spark repo.\nRefer to the\nRandomForest\nScala docs\nand\nRandomForestModel\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.tree.RandomForest\nimport\norg.apache.spark.mllib.tree.model.RandomForestModel\nimport\norg.apache.spark.mllib.util.MLUtils\n// Load and parse the data file.\nval\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Split the data into training and test sets (30% held out for testing)\nval\nsplits\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.7\n,\n0.3\n))\nval\n(\ntrainingData\n,\ntestData\n)\n=\n(\nsplits\n(\n0\n),\nsplits\n(\n1\n))\n// Train a RandomForest model.\n// Empty categoricalFeaturesInfo indicates all features are continuous.\nval\nnumClasses\n=\n2\nval\ncategoricalFeaturesInfo\n=\nMap\n[\nInt\n,\nInt\n]()\nval\nnumTrees\n=\n3\n// Use more in practice.\nval\nfeatureSubsetStrategy\n=\n\"auto\"\n// Let the algorithm choose.\nval\nimpurity\n=\n\"gini\"\nval\nmaxDepth\n=\n4\nval\nmaxBins\n=\n32\nval\nmodel\n=\nRandomForest\n.\ntrainClassifier\n(\ntrainingData\n,\nnumClasses\n,\ncategoricalFeaturesInfo\n,\nnumTrees\n,\nfeatureSubsetStrategy\n,\nimpurity\n,\nmaxDepth\n,\nmaxBins\n)\n// Evaluate model on test instances and compute test error\nval\nlabelAndPreds\n=\ntestData\n.\nmap\n{\npoint\n=>\nval\nprediction\n=\nmodel\n.\npredict\n(\npoint\n.\nfeatures\n)\n(\npoint\n.\nlabel\n,\nprediction\n)\n}\nval\ntestErr\n=\nlabelAndPreds\n.\nfilter\n(\nr\n=>\nr\n.\n_1\n!=\nr\n.\n_2\n).\ncount\n().\ntoDouble\n/\ntestData\n.\ncount\n()\nprintln\n(\ns\n\"Test Error = $testErr\"\n)\nprintln\n(\ns\n\"Learned classification forest model:\\n ${model.toDebugString}\"\n)\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/myRandomForestClassificationModel\"\n)\nval\nsameModel\n=\nRandomForestModel\n.\nload\n(\nsc\n,\n\"target/tmp/myRandomForestClassificationModel\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\" in the Spark repo.\nRefer to the\nRandomForest\nJava docs\nand\nRandomForestModel\nJava docs\nfor details on the API.\nimport\njava.util.HashMap\n;\nimport\njava.util.Map\n;\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.SparkConf\n;\nimport\norg.apache.spark.api.java.JavaPairRDD\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.api.java.JavaSparkContext\n;\nimport\norg.apache.spark.mllib.regression.LabeledPoint\n;\nimport\norg.apache.spark.mllib.tree.RandomForest\n;\nimport\norg.apache.spark.mllib.tree.model.RandomForestModel\n;\nimport\norg.apache.spark.mllib.util.MLUtils\n;\nSparkConf\nsparkConf\n=\nnew\nSparkConf\n().\nsetAppName\n(\n\"JavaRandomForestClassificationExample\"\n);\nJavaSparkContext\njsc\n=\nnew\nJavaSparkContext\n(\nsparkConf\n);\n// Load and parse the data file.\nString\ndatapath\n=\n\"data/mllib/sample_libsvm_data.txt\"\n;\nJavaRDD\n<\nLabeledPoint\n>\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\njsc\n.\nsc\n(),\ndatapath\n).\ntoJavaRDD\n();\n// Split the data into training and test sets (30% held out for testing)\nJavaRDD\n<\nLabeledPoint\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]{\n0.7\n,\n0.3\n});\nJavaRDD\n<\nLabeledPoint\n>\ntrainingData\n=\nsplits\n[\n0\n];\nJavaRDD\n<\nLabeledPoint\n>\ntestData\n=\nsplits\n[\n1\n];\n// Train a RandomForest model.\n// Empty categoricalFeaturesInfo indicates all features are continuous.\nint\nnumClasses\n=\n2\n;\nMap\n<\nInteger\n,\nInteger\n>\ncategoricalFeaturesInfo\n=\nnew\nHashMap\n<>();\nint\nnumTrees\n=\n3\n;\n// Use more in practice.\nString\nfeatureSubsetStrategy\n=\n\"auto\"\n;\n// Let the algorithm choose.\nString\nimpurity\n=\n\"gini\"\n;\nint\nmaxDepth\n=\n5\n;\nint\nmaxBins\n=\n32\n;\nint\nseed\n=\n12345\n;\nRandomForestModel\nmodel\n=\nRandomForest\n.\ntrainClassifier\n(\ntrainingData\n,\nnumClasses\n,\ncategoricalFeaturesInfo\n,\nnumTrees\n,\nfeatureSubsetStrategy\n,\nimpurity\n,\nmaxDepth\n,\nmaxBins\n,\nseed\n);\n// Evaluate model on test instances and compute test error\nJavaPairRDD\n<\nDouble\n,\nDouble\n>\npredictionAndLabel\n=\ntestData\n.\nmapToPair\n(\np\n->\nnew\nTuple2\n<>(\nmodel\n.\npredict\n(\np\n.\nfeatures\n()),\np\n.\nlabel\n()));\ndouble\ntestErr\n=\npredictionAndLabel\n.\nfilter\n(\npl\n->\n!\npl\n.\n_1\n().\nequals\n(\npl\n.\n_2\n())).\ncount\n()\n/\n(\ndouble\n)\ntestData\n.\ncount\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Test Error: \"\n+\ntestErr\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Learned classification forest model:\\n\"\n+\nmodel\n.\ntoDebugString\n());\n// Save and load model\nmodel\n.\nsave\n(\njsc\n.\nsc\n(),\n\"target/tmp/myRandomForestClassificationModel\"\n);\nRandomForestModel\nsameModel\n=\nRandomForestModel\n.\nload\n(\njsc\n.\nsc\n(),\n\"target/tmp/myRandomForestClassificationModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\" in the Spark repo.\nRegression\nThe example below demonstrates how to load a\nLIBSVM data file\n,\nparse it as an RDD of\nLabeledPoint\nand then\nperform regression using a Random Forest.\nThe Mean Squared Error (MSE) is computed at the end to evaluate\ngoodness of fit\n.\nRefer to the\nRandomForest\nPython docs\nand\nRandomForest\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.tree\nimport\nRandomForest\n,\nRandomForestModel\nfrom\npyspark.mllib.util\nimport\nMLUtils\n# Load and parse the data file into an RDD of LabeledPoint.\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n'\ndata/mllib/sample_libsvm_data.txt\n'\n)\n# Split the data into training and test sets (30% held out for testing)\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n([\n0.7\n,\n0.3\n])\n# Train a RandomForest model.\n#  Empty categoricalFeaturesInfo indicates all features are continuous.\n#  Note: Use larger numTrees in practice.\n#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\nmodel\n=\nRandomForest\n.\ntrainRegressor\n(\ntrainingData\n,\ncategoricalFeaturesInfo\n=\n{},\nnumTrees\n=\n3\n,\nfeatureSubsetStrategy\n=\n\"\nauto\n\"\n,\nimpurity\n=\n'\nvariance\n'\n,\nmaxDepth\n=\n4\n,\nmaxBins\n=\n32\n)\n# Evaluate model on test instances and compute test error\npredictions\n=\nmodel\n.\npredict\n(\ntestData\n.\nmap\n(\nlambda\nx\n:\nx\n.\nfeatures\n))\nlabelsAndPredictions\n=\ntestData\n.\nmap\n(\nlambda\nlp\n:\nlp\n.\nlabel\n).\nzip\n(\npredictions\n)\ntestMSE\n=\nlabelsAndPredictions\n.\nmap\n(\nlambda\nlp\n:\n(\nlp\n[\n0\n]\n-\nlp\n[\n1\n])\n*\n(\nlp\n[\n0\n]\n-\nlp\n[\n1\n])).\nsum\n()\n/\n\\\nfloat\n(\ntestData\n.\ncount\n())\nprint\n(\n'\nTest Mean Squared Error =\n'\n+\nstr\n(\ntestMSE\n))\nprint\n(\n'\nLearned regression forest model:\n'\n)\nprint\n(\nmodel\n.\ntoDebugString\n())\n# Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"\ntarget/tmp/myRandomForestRegressionModel\n\"\n)\nsameModel\n=\nRandomForestModel\n.\nload\n(\nsc\n,\n\"\ntarget/tmp/myRandomForestRegressionModel\n\"\n)\nFind full example code at \"examples/src/main/python/mllib/random_forest_regression_example.py\" in the Spark repo.\nRefer to the\nRandomForest\nScala docs\nand\nRandomForestModel\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.tree.RandomForest\nimport\norg.apache.spark.mllib.tree.model.RandomForestModel\nimport\norg.apache.spark.mllib.util.MLUtils\n// Load and parse the data file.\nval\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Split the data into training and test sets (30% held out for testing)\nval\nsplits\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.7\n,\n0.3\n))\nval\n(\ntrainingData\n,\ntestData\n)\n=\n(\nsplits\n(\n0\n),\nsplits\n(\n1\n))\n// Train a RandomForest model.\n// Empty categoricalFeaturesInfo indicates all features are continuous.\nval\nnumClasses\n=\n2\nval\ncategoricalFeaturesInfo\n=\nMap\n[\nInt\n,\nInt\n]()\nval\nnumTrees\n=\n3\n// Use more in practice.\nval\nfeatureSubsetStrategy\n=\n\"auto\"\n// Let the algorithm choose.\nval\nimpurity\n=\n\"variance\"\nval\nmaxDepth\n=\n4\nval\nmaxBins\n=\n32\nval\nmodel\n=\nRandomForest\n.\ntrainRegressor\n(\ntrainingData\n,\ncategoricalFeaturesInfo\n,\nnumTrees\n,\nfeatureSubsetStrategy\n,\nimpurity\n,\nmaxDepth\n,\nmaxBins\n)\n// Evaluate model on test instances and compute test error\nval\nlabelsAndPredictions\n=\ntestData\n.\nmap\n{\npoint\n=>\nval\nprediction\n=\nmodel\n.\npredict\n(\npoint\n.\nfeatures\n)\n(\npoint\n.\nlabel\n,\nprediction\n)\n}\nval\ntestMSE\n=\nlabelsAndPredictions\n.\nmap\n{\ncase\n(\nv\n,\np\n)\n=>\nmath\n.\npow\n((\nv\n-\np\n),\n2\n)}.\nmean\n()\nprintln\n(\ns\n\"Test Mean Squared Error = $testMSE\"\n)\nprintln\n(\ns\n\"Learned regression forest model:\\n ${model.toDebugString}\"\n)\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/myRandomForestRegressionModel\"\n)\nval\nsameModel\n=\nRandomForestModel\n.\nload\n(\nsc\n,\n\"target/tmp/myRandomForestRegressionModel\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\" in the Spark repo.\nRefer to the\nRandomForest\nJava docs\nand\nRandomForestModel\nJava docs\nfor details on the API.\nimport\njava.util.HashMap\n;\nimport\njava.util.Map\n;\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.api.java.JavaPairRDD\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.api.java.JavaSparkContext\n;\nimport\norg.apache.spark.mllib.regression.LabeledPoint\n;\nimport\norg.apache.spark.mllib.tree.RandomForest\n;\nimport\norg.apache.spark.mllib.tree.model.RandomForestModel\n;\nimport\norg.apache.spark.mllib.util.MLUtils\n;\nimport\norg.apache.spark.SparkConf\n;\nSparkConf\nsparkConf\n=\nnew\nSparkConf\n().\nsetAppName\n(\n\"JavaRandomForestRegressionExample\"\n);\nJavaSparkContext\njsc\n=\nnew\nJavaSparkContext\n(\nsparkConf\n);\n// Load and parse the data file.\nString\ndatapath\n=\n\"data/mllib/sample_libsvm_data.txt\"\n;\nJavaRDD\n<\nLabeledPoint\n>\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\njsc\n.\nsc\n(),\ndatapath\n).\ntoJavaRDD\n();\n// Split the data into training and test sets (30% held out for testing)\nJavaRDD\n<\nLabeledPoint\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]{\n0.7\n,\n0.3\n});\nJavaRDD\n<\nLabeledPoint\n>\ntrainingData\n=\nsplits\n[\n0\n];\nJavaRDD\n<\nLabeledPoint\n>\ntestData\n=\nsplits\n[\n1\n];\n// Set parameters.\n// Empty categoricalFeaturesInfo indicates all features are continuous.\nMap\n<\nInteger\n,\nInteger\n>\ncategoricalFeaturesInfo\n=\nnew\nHashMap\n<>();\nint\nnumTrees\n=\n3\n;\n// Use more in practice.\nString\nfeatureSubsetStrategy\n=\n\"auto\"\n;\n// Let the algorithm choose.\nString\nimpurity\n=\n\"variance\"\n;\nint\nmaxDepth\n=\n4\n;\nint\nmaxBins\n=\n32\n;\nint\nseed\n=\n12345\n;\n// Train a RandomForest model.\nRandomForestModel\nmodel\n=\nRandomForest\n.\ntrainRegressor\n(\ntrainingData\n,\ncategoricalFeaturesInfo\n,\nnumTrees\n,\nfeatureSubsetStrategy\n,\nimpurity\n,\nmaxDepth\n,\nmaxBins\n,\nseed\n);\n// Evaluate model on test instances and compute test error\nJavaPairRDD\n<\nDouble\n,\nDouble\n>\npredictionAndLabel\n=\ntestData\n.\nmapToPair\n(\np\n->\nnew\nTuple2\n<>(\nmodel\n.\npredict\n(\np\n.\nfeatures\n()),\np\n.\nlabel\n()));\ndouble\ntestMSE\n=\npredictionAndLabel\n.\nmapToDouble\n(\npl\n->\n{\ndouble\ndiff\n=\npl\n.\n_1\n()\n-\npl\n.\n_2\n();\nreturn\ndiff\n*\ndiff\n;\n}).\nmean\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Test Mean Squared Error: \"\n+\ntestMSE\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Learned regression forest model:\\n\"\n+\nmodel\n.\ntoDebugString\n());\n// Save and load model\nmodel\n.\nsave\n(\njsc\n.\nsc\n(),\n\"target/tmp/myRandomForestRegressionModel\"\n);\nRandomForestModel\nsameModel\n=\nRandomForestModel\n.\nload\n(\njsc\n.\nsc\n(),\n\"target/tmp/myRandomForestRegressionModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\" in the Spark repo.\nGradient-Boosted Trees (GBTs)\nGradient-Boosted Trees (GBTs)\nare ensembles of\ndecision trees\n.\nGBTs iteratively train decision trees in order to minimize a loss function.\nLike decision trees, GBTs handle categorical features,\nextend to the multiclass classification setting, do not require\nfeature scaling, and are able to capture non-linearities and feature interactions.\nspark.mllib\nsupports GBTs for binary classification and for regression,\nusing both continuous and categorical features.\nspark.mllib\nimplements GBTs using the existing\ndecision tree\nimplementation.  Please see the decision tree guide for more information on trees.\nNote\n: GBTs do not yet support multiclass classification.  For multiclass problems, please use\ndecision trees\nor\nRandom Forests\n.\nBasic algorithm\nGradient boosting iteratively trains a sequence of decision trees.\nOn each iteration, the algorithm uses the current ensemble to predict the label of each training instance and then compares the prediction with the true label.  The dataset is re-labeled to put more emphasis on training instances with poor predictions.  Thus, in the next iteration, the decision tree will help correct for previous mistakes.\nThe specific mechanism for re-labeling instances is defined by a loss function (discussed below).  With each iteration, GBTs further reduce this loss function on the training data.\nLosses\nThe table below lists the losses currently supported by GBTs in\nspark.mllib\n.\nNote that each loss is applicable to one of classification or regression, not both.\nNotation: $N$ = number of instances. $y_i$ = label of instance $i$.  $x_i$ = features of instance $i$.  $F(x_i)$ = model’s predicted label for instance $i$.\nLoss\nTask\nFormula\nDescription\nLog Loss\nClassification\n$2 \\sum_{i=1}^{N} \\log(1+\\exp(-2 y_i F(x_i)))$\nTwice binomial negative log likelihood.\nSquared Error\nRegression\n$\\sum_{i=1}^{N} (y_i - F(x_i))^2$\nAlso called L2 loss.  Default loss for regression tasks.\nAbsolute Error\nRegression\n$\\sum_{i=1}^{N} |y_i - F(x_i)|$\nAlso called L1 loss.  Can be more robust to outliers than Squared Error.\nUsage tips\nWe include a few guidelines for using GBTs by discussing the various parameters.\nWe omit some decision tree parameters since those are covered in the\ndecision tree guide\n.\nloss\n: See the section above for information on losses and their applicability to tasks (classification vs. regression).  Different losses can give significantly different results, depending on the dataset.\nnumIterations\n: This sets the number of trees in the ensemble.  Each iteration produces one tree.  Increasing this number makes the model more expressive, improving training data accuracy.  However, test-time accuracy may suffer if this is too large.\nlearningRate\n: This parameter should not need to be tuned.  If the algorithm behavior seems unstable, decreasing this value may improve stability.\nalgo\n: The algorithm or task (classification vs. regression) is set using the tree [Strategy] parameter.\nValidation while training\nGradient boosting can overfit when trained with more trees. In order to prevent overfitting, it is useful to validate while\ntraining. The method runWithValidation has been provided to make use of this option. It takes a pair of RDD’s as arguments, the\nfirst one being the training dataset and the second being the validation dataset.\nThe training is stopped when the improvement in the validation error is not more than a certain tolerance\n(supplied by the\nvalidationTol\nargument in\nBoostingStrategy\n). In practice, the validation error\ndecreases initially and later increases. There might be cases in which the validation error does not change monotonically,\nand the user is advised to set a large enough negative tolerance and examine the validation curve using\nevaluateEachIteration\n(which gives the error or loss per iteration) to tune the number of iterations.\nExamples\nClassification\nThe example below demonstrates how to load a\nLIBSVM data file\n,\nparse it as an RDD of\nLabeledPoint\nand then\nperform classification using Gradient-Boosted Trees with log loss.\nThe test error is calculated to measure the algorithm accuracy.\nRefer to the\nGradientBoostedTrees\nPython docs\nand\nGradientBoostedTreesModel\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.tree\nimport\nGradientBoostedTrees\n,\nGradientBoostedTreesModel\nfrom\npyspark.mllib.util\nimport\nMLUtils\n# Load and parse the data file.\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\n# Split the data into training and test sets (30% held out for testing)\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n([\n0.7\n,\n0.3\n])\n# Train a GradientBoostedTrees model.\n#  Notes: (a) Empty categoricalFeaturesInfo indicates all features are continuous.\n#         (b) Use more iterations in practice.\nmodel\n=\nGradientBoostedTrees\n.\ntrainClassifier\n(\ntrainingData\n,\ncategoricalFeaturesInfo\n=\n{},\nnumIterations\n=\n3\n)\n# Evaluate model on test instances and compute test error\npredictions\n=\nmodel\n.\npredict\n(\ntestData\n.\nmap\n(\nlambda\nx\n:\nx\n.\nfeatures\n))\nlabelsAndPredictions\n=\ntestData\n.\nmap\n(\nlambda\nlp\n:\nlp\n.\nlabel\n).\nzip\n(\npredictions\n)\ntestErr\n=\nlabelsAndPredictions\n.\nfilter\n(\nlambda\nlp\n:\nlp\n[\n0\n]\n!=\nlp\n[\n1\n]).\ncount\n()\n/\nfloat\n(\ntestData\n.\ncount\n())\nprint\n(\n'\nTest Error =\n'\n+\nstr\n(\ntestErr\n))\nprint\n(\n'\nLearned classification GBT model:\n'\n)\nprint\n(\nmodel\n.\ntoDebugString\n())\n# Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"\ntarget/tmp/myGradientBoostingClassificationModel\n\"\n)\nsameModel\n=\nGradientBoostedTreesModel\n.\nload\n(\nsc\n,\n\"\ntarget/tmp/myGradientBoostingClassificationModel\n\"\n)\nFind full example code at \"examples/src/main/python/mllib/gradient_boosting_classification_example.py\" in the Spark repo.\nRefer to the\nGradientBoostedTrees\nScala docs\nand\nGradientBoostedTreesModel\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.tree.GradientBoostedTrees\nimport\norg.apache.spark.mllib.tree.configuration.BoostingStrategy\nimport\norg.apache.spark.mllib.tree.model.GradientBoostedTreesModel\nimport\norg.apache.spark.mllib.util.MLUtils\n// Load and parse the data file.\nval\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Split the data into training and test sets (30% held out for testing)\nval\nsplits\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.7\n,\n0.3\n))\nval\n(\ntrainingData\n,\ntestData\n)\n=\n(\nsplits\n(\n0\n),\nsplits\n(\n1\n))\n// Train a GradientBoostedTrees model.\n// The defaultParams for Classification use LogLoss by default.\nval\nboostingStrategy\n=\nBoostingStrategy\n.\ndefaultParams\n(\n\"Classification\"\n)\nboostingStrategy\n.\nnumIterations\n=\n3\n// Note: Use more iterations in practice.\nboostingStrategy\n.\ntreeStrategy\n.\nnumClasses\n=\n2\nboostingStrategy\n.\ntreeStrategy\n.\nmaxDepth\n=\n5\n// Empty categoricalFeaturesInfo indicates all features are continuous.\nboostingStrategy\n.\ntreeStrategy\n.\ncategoricalFeaturesInfo\n=\nMap\n[\nInt\n,\nInt\n]()\nval\nmodel\n=\nGradientBoostedTrees\n.\ntrain\n(\ntrainingData\n,\nboostingStrategy\n)\n// Evaluate model on test instances and compute test error\nval\nlabelAndPreds\n=\ntestData\n.\nmap\n{\npoint\n=>\nval\nprediction\n=\nmodel\n.\npredict\n(\npoint\n.\nfeatures\n)\n(\npoint\n.\nlabel\n,\nprediction\n)\n}\nval\ntestErr\n=\nlabelAndPreds\n.\nfilter\n(\nr\n=>\nr\n.\n_1\n!=\nr\n.\n_2\n).\ncount\n().\ntoDouble\n/\ntestData\n.\ncount\n()\nprintln\n(\ns\n\"Test Error = $testErr\"\n)\nprintln\n(\ns\n\"Learned classification GBT model:\\n ${model.toDebugString}\"\n)\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/myGradientBoostingClassificationModel\"\n)\nval\nsameModel\n=\nGradientBoostedTreesModel\n.\nload\n(\nsc\n,\n\"target/tmp/myGradientBoostingClassificationModel\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\" in the Spark repo.\nRefer to the\nGradientBoostedTrees\nJava docs\nand\nGradientBoostedTreesModel\nJava docs\nfor details on the API.\nimport\njava.util.HashMap\n;\nimport\njava.util.Map\n;\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.SparkConf\n;\nimport\norg.apache.spark.api.java.JavaPairRDD\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.api.java.JavaSparkContext\n;\nimport\norg.apache.spark.mllib.regression.LabeledPoint\n;\nimport\norg.apache.spark.mllib.tree.GradientBoostedTrees\n;\nimport\norg.apache.spark.mllib.tree.configuration.BoostingStrategy\n;\nimport\norg.apache.spark.mllib.tree.model.GradientBoostedTreesModel\n;\nimport\norg.apache.spark.mllib.util.MLUtils\n;\nSparkConf\nsparkConf\n=\nnew\nSparkConf\n()\n.\nsetAppName\n(\n\"JavaGradientBoostedTreesClassificationExample\"\n);\nJavaSparkContext\njsc\n=\nnew\nJavaSparkContext\n(\nsparkConf\n);\n// Load and parse the data file.\nString\ndatapath\n=\n\"data/mllib/sample_libsvm_data.txt\"\n;\nJavaRDD\n<\nLabeledPoint\n>\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\njsc\n.\nsc\n(),\ndatapath\n).\ntoJavaRDD\n();\n// Split the data into training and test sets (30% held out for testing)\nJavaRDD\n<\nLabeledPoint\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]{\n0.7\n,\n0.3\n});\nJavaRDD\n<\nLabeledPoint\n>\ntrainingData\n=\nsplits\n[\n0\n];\nJavaRDD\n<\nLabeledPoint\n>\ntestData\n=\nsplits\n[\n1\n];\n// Train a GradientBoostedTrees model.\n// The defaultParams for Classification use LogLoss by default.\nBoostingStrategy\nboostingStrategy\n=\nBoostingStrategy\n.\ndefaultParams\n(\n\"Classification\"\n);\nboostingStrategy\n.\nsetNumIterations\n(\n3\n);\n// Note: Use more iterations in practice.\nboostingStrategy\n.\ngetTreeStrategy\n().\nsetNumClasses\n(\n2\n);\nboostingStrategy\n.\ngetTreeStrategy\n().\nsetMaxDepth\n(\n5\n);\n// Empty categoricalFeaturesInfo indicates all features are continuous.\nMap\n<\nInteger\n,\nInteger\n>\ncategoricalFeaturesInfo\n=\nnew\nHashMap\n<>();\nboostingStrategy\n.\ntreeStrategy\n().\nsetCategoricalFeaturesInfo\n(\ncategoricalFeaturesInfo\n);\nGradientBoostedTreesModel\nmodel\n=\nGradientBoostedTrees\n.\ntrain\n(\ntrainingData\n,\nboostingStrategy\n);\n// Evaluate model on test instances and compute test error\nJavaPairRDD\n<\nDouble\n,\nDouble\n>\npredictionAndLabel\n=\ntestData\n.\nmapToPair\n(\np\n->\nnew\nTuple2\n<>(\nmodel\n.\npredict\n(\np\n.\nfeatures\n()),\np\n.\nlabel\n()));\ndouble\ntestErr\n=\npredictionAndLabel\n.\nfilter\n(\npl\n->\n!\npl\n.\n_1\n().\nequals\n(\npl\n.\n_2\n())).\ncount\n()\n/\n(\ndouble\n)\ntestData\n.\ncount\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Test Error: \"\n+\ntestErr\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Learned classification GBT model:\\n\"\n+\nmodel\n.\ntoDebugString\n());\n// Save and load model\nmodel\n.\nsave\n(\njsc\n.\nsc\n(),\n\"target/tmp/myGradientBoostingClassificationModel\"\n);\nGradientBoostedTreesModel\nsameModel\n=\nGradientBoostedTreesModel\n.\nload\n(\njsc\n.\nsc\n(),\n\"target/tmp/myGradientBoostingClassificationModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\" in the Spark repo.\nRegression\nThe example below demonstrates how to load a\nLIBSVM data file\n,\nparse it as an RDD of\nLabeledPoint\nand then\nperform regression using Gradient-Boosted Trees with Squared Error as the loss.\nThe Mean Squared Error (MSE) is computed at the end to evaluate\ngoodness of fit\n.\nRefer to the\nGradientBoostedTrees\nPython docs\nand\nGradientBoostedTreesModel\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.tree\nimport\nGradientBoostedTrees\n,\nGradientBoostedTreesModel\nfrom\npyspark.mllib.util\nimport\nMLUtils\n# Load and parse the data file.\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\n# Split the data into training and test sets (30% held out for testing)\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n([\n0.7\n,\n0.3\n])\n# Train a GradientBoostedTrees model.\n#  Notes: (a) Empty categoricalFeaturesInfo indicates all features are continuous.\n#         (b) Use more iterations in practice.\nmodel\n=\nGradientBoostedTrees\n.\ntrainRegressor\n(\ntrainingData\n,\ncategoricalFeaturesInfo\n=\n{},\nnumIterations\n=\n3\n)\n# Evaluate model on test instances and compute test error\npredictions\n=\nmodel\n.\npredict\n(\ntestData\n.\nmap\n(\nlambda\nx\n:\nx\n.\nfeatures\n))\nlabelsAndPredictions\n=\ntestData\n.\nmap\n(\nlambda\nlp\n:\nlp\n.\nlabel\n).\nzip\n(\npredictions\n)\ntestMSE\n=\nlabelsAndPredictions\n.\nmap\n(\nlambda\nlp\n:\n(\nlp\n[\n0\n]\n-\nlp\n[\n1\n])\n*\n(\nlp\n[\n0\n]\n-\nlp\n[\n1\n])).\nsum\n()\n/\n\\\nfloat\n(\ntestData\n.\ncount\n())\nprint\n(\n'\nTest Mean Squared Error =\n'\n+\nstr\n(\ntestMSE\n))\nprint\n(\n'\nLearned regression GBT model:\n'\n)\nprint\n(\nmodel\n.\ntoDebugString\n())\n# Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"\ntarget/tmp/myGradientBoostingRegressionModel\n\"\n)\nsameModel\n=\nGradientBoostedTreesModel\n.\nload\n(\nsc\n,\n\"\ntarget/tmp/myGradientBoostingRegressionModel\n\"\n)\nFind full example code at \"examples/src/main/python/mllib/gradient_boosting_regression_example.py\" in the Spark repo.\nRefer to the\nGradientBoostedTrees\nScala docs\nand\nGradientBoostedTreesModel\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.tree.GradientBoostedTrees\nimport\norg.apache.spark.mllib.tree.configuration.BoostingStrategy\nimport\norg.apache.spark.mllib.tree.model.GradientBoostedTreesModel\nimport\norg.apache.spark.mllib.util.MLUtils\n// Load and parse the data file.\nval\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Split the data into training and test sets (30% held out for testing)\nval\nsplits\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.7\n,\n0.3\n))\nval\n(\ntrainingData\n,\ntestData\n)\n=\n(\nsplits\n(\n0\n),\nsplits\n(\n1\n))\n// Train a GradientBoostedTrees model.\n// The defaultParams for Regression use SquaredError by default.\nval\nboostingStrategy\n=\nBoostingStrategy\n.\ndefaultParams\n(\n\"Regression\"\n)\nboostingStrategy\n.\nnumIterations\n=\n3\n// Note: Use more iterations in practice.\nboostingStrategy\n.\ntreeStrategy\n.\nmaxDepth\n=\n5\n// Empty categoricalFeaturesInfo indicates all features are continuous.\nboostingStrategy\n.\ntreeStrategy\n.\ncategoricalFeaturesInfo\n=\nMap\n[\nInt\n,\nInt\n]()\nval\nmodel\n=\nGradientBoostedTrees\n.\ntrain\n(\ntrainingData\n,\nboostingStrategy\n)\n// Evaluate model on test instances and compute test error\nval\nlabelsAndPredictions\n=\ntestData\n.\nmap\n{\npoint\n=>\nval\nprediction\n=\nmodel\n.\npredict\n(\npoint\n.\nfeatures\n)\n(\npoint\n.\nlabel\n,\nprediction\n)\n}\nval\ntestMSE\n=\nlabelsAndPredictions\n.\nmap\n{\ncase\n(\nv\n,\np\n)\n=>\nmath\n.\npow\n((\nv\n-\np\n),\n2\n)}.\nmean\n()\nprintln\n(\ns\n\"Test Mean Squared Error = $testMSE\"\n)\nprintln\n(\ns\n\"Learned regression GBT model:\\n ${model.toDebugString}\"\n)\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/myGradientBoostingRegressionModel\"\n)\nval\nsameModel\n=\nGradientBoostedTreesModel\n.\nload\n(\nsc\n,\n\"target/tmp/myGradientBoostingRegressionModel\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\" in the Spark repo.\nRefer to the\nGradientBoostedTrees\nJava docs\nand\nGradientBoostedTreesModel\nJava docs\nfor details on the API.\nimport\njava.util.HashMap\n;\nimport\njava.util.Map\n;\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.SparkConf\n;\nimport\norg.apache.spark.api.java.JavaPairRDD\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.api.java.JavaSparkContext\n;\nimport\norg.apache.spark.mllib.regression.LabeledPoint\n;\nimport\norg.apache.spark.mllib.tree.GradientBoostedTrees\n;\nimport\norg.apache.spark.mllib.tree.configuration.BoostingStrategy\n;\nimport\norg.apache.spark.mllib.tree.model.GradientBoostedTreesModel\n;\nimport\norg.apache.spark.mllib.util.MLUtils\n;\nSparkConf\nsparkConf\n=\nnew\nSparkConf\n()\n.\nsetAppName\n(\n\"JavaGradientBoostedTreesRegressionExample\"\n);\nJavaSparkContext\njsc\n=\nnew\nJavaSparkContext\n(\nsparkConf\n);\n// Load and parse the data file.\nString\ndatapath\n=\n\"data/mllib/sample_libsvm_data.txt\"\n;\nJavaRDD\n<\nLabeledPoint\n>\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\njsc\n.\nsc\n(),\ndatapath\n).\ntoJavaRDD\n();\n// Split the data into training and test sets (30% held out for testing)\nJavaRDD\n<\nLabeledPoint\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]{\n0.7\n,\n0.3\n});\nJavaRDD\n<\nLabeledPoint\n>\ntrainingData\n=\nsplits\n[\n0\n];\nJavaRDD\n<\nLabeledPoint\n>\ntestData\n=\nsplits\n[\n1\n];\n// Train a GradientBoostedTrees model.\n// The defaultParams for Regression use SquaredError by default.\nBoostingStrategy\nboostingStrategy\n=\nBoostingStrategy\n.\ndefaultParams\n(\n\"Regression\"\n);\nboostingStrategy\n.\nsetNumIterations\n(\n3\n);\n// Note: Use more iterations in practice.\nboostingStrategy\n.\ngetTreeStrategy\n().\nsetMaxDepth\n(\n5\n);\n// Empty categoricalFeaturesInfo indicates all features are continuous.\nMap\n<\nInteger\n,\nInteger\n>\ncategoricalFeaturesInfo\n=\nnew\nHashMap\n<>();\nboostingStrategy\n.\ntreeStrategy\n().\nsetCategoricalFeaturesInfo\n(\ncategoricalFeaturesInfo\n);\nGradientBoostedTreesModel\nmodel\n=\nGradientBoostedTrees\n.\ntrain\n(\ntrainingData\n,\nboostingStrategy\n);\n// Evaluate model on test instances and compute test error\nJavaPairRDD\n<\nDouble\n,\nDouble\n>\npredictionAndLabel\n=\ntestData\n.\nmapToPair\n(\np\n->\nnew\nTuple2\n<>(\nmodel\n.\npredict\n(\np\n.\nfeatures\n()),\np\n.\nlabel\n()));\ndouble\ntestMSE\n=\npredictionAndLabel\n.\nmapToDouble\n(\npl\n->\n{\ndouble\ndiff\n=\npl\n.\n_1\n()\n-\npl\n.\n_2\n();\nreturn\ndiff\n*\ndiff\n;\n}).\nmean\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Test Mean Squared Error: \"\n+\ntestMSE\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Learned regression GBT model:\\n\"\n+\nmodel\n.\ntoDebugString\n());\n// Save and load model\nmodel\n.\nsave\n(\njsc\n.\nsc\n(),\n\"target/tmp/myGradientBoostingRegressionModel\"\n);\nGradientBoostedTreesModel\nsameModel\n=\nGradientBoostedTreesModel\n.\nload\n(\njsc\n.\nsc\n(),\n\"target/tmp/myGradientBoostingRegressionModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\" in the Spark repo."}
{"url": "https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nStructured Streaming + Kafka Integration Guide (Kafka broker version 0.10.0 or higher)\nThis page has moved\nhere\n."}
{"url": "https://spark.apache.org/docs/latest/streaming/structured-streaming-kafka-integration.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nStructured Streaming Programming Guide\nOverview\nGetting Started\nAPIs on DataFrames and Datasets\nPerformance Tips\nAdditional Information\nStructured Streaming + Kafka Integration Guide (Kafka broker version 0.10.0 or higher)\nStructured Streaming integration for Kafka 0.10 to read data from and write data to Kafka.\nLinking\nFor Scala/Java applications using SBT/Maven project definitions, link your application with the following artifact:\ngroupId = org.apache.spark\nartifactId = spark-sql-kafka-0-10_2.13\nversion = 4.0.0\nPlease note that to use the headers functionality, your Kafka client version should be version 0.11.0.0 or up.\nFor Python applications, you need to add this above library and its dependencies when deploying your\napplication. See the\nDeploying\nsubsection below.\nFor experimenting on\nspark-shell\n, you need to add this above library and its dependencies too when invoking\nspark-shell\n. Also, see the\nDeploying\nsubsection below.\nReading Data from Kafka\nCreating a Kafka Source for Streaming Queries\n# Subscribe to 1 topic\ndf\n=\nspark\n\\\n.\nreadStream\n\\\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\noption\n(\n\"\nsubscribe\n\"\n,\n\"\ntopic1\n\"\n)\n\\\n.\nload\n()\ndf\n.\nselectExpr\n(\n\"\nCAST(key AS STRING)\n\"\n,\n\"\nCAST(value AS STRING)\n\"\n)\n# Subscribe to 1 topic, with headers\ndf\n=\nspark\n\\\n.\nreadStream\n\\\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\noption\n(\n\"\nsubscribe\n\"\n,\n\"\ntopic1\n\"\n)\n\\\n.\noption\n(\n\"\nincludeHeaders\n\"\n,\n\"\ntrue\n\"\n)\n\\\n.\nload\n()\ndf\n.\nselectExpr\n(\n\"\nCAST(key AS STRING)\n\"\n,\n\"\nCAST(value AS STRING)\n\"\n,\n\"\nheaders\n\"\n)\n# Subscribe to multiple topics\ndf\n=\nspark\n\\\n.\nreadStream\n\\\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\noption\n(\n\"\nsubscribe\n\"\n,\n\"\ntopic1,topic2\n\"\n)\n\\\n.\nload\n()\ndf\n.\nselectExpr\n(\n\"\nCAST(key AS STRING)\n\"\n,\n\"\nCAST(value AS STRING)\n\"\n)\n# Subscribe to a pattern\ndf\n=\nspark\n\\\n.\nreadStream\n\\\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\noption\n(\n\"\nsubscribePattern\n\"\n,\n\"\ntopic.*\n\"\n)\n\\\n.\nload\n()\ndf\n.\nselectExpr\n(\n\"\nCAST(key AS STRING)\n\"\n,\n\"\nCAST(value AS STRING)\n\"\n)\n// Subscribe to 1 topic\nval\ndf\n=\nspark\n.\nreadStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribe\"\n,\n\"topic1\"\n)\n.\nload\n()\ndf\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n)\n.\nas\n[(\nString\n,\nString\n)]\n// Subscribe to 1 topic, with headers\nval\ndf\n=\nspark\n.\nreadStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribe\"\n,\n\"topic1\"\n)\n.\noption\n(\n\"includeHeaders\"\n,\n\"true\"\n)\n.\nload\n()\ndf\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n,\n\"headers\"\n)\n.\nas\n[(\nString\n,\nString\n,\nArray\n[(\nString\n,\nArray\n[\nByte\n])])]\n// Subscribe to multiple topics\nval\ndf\n=\nspark\n.\nreadStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribe\"\n,\n\"topic1,topic2\"\n)\n.\nload\n()\ndf\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n)\n.\nas\n[(\nString\n,\nString\n)]\n// Subscribe to a pattern\nval\ndf\n=\nspark\n.\nreadStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribePattern\"\n,\n\"topic.*\"\n)\n.\nload\n()\ndf\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n)\n.\nas\n[(\nString\n,\nString\n)]\n// Subscribe to 1 topic\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\nreadStream\n()\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribe\"\n,\n\"topic1\"\n)\n.\nload\n();\ndf\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n);\n// Subscribe to 1 topic, with headers\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\nreadStream\n()\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribe\"\n,\n\"topic1\"\n)\n.\noption\n(\n\"includeHeaders\"\n,\n\"true\"\n)\n.\nload\n()\ndf\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n,\n\"headers\"\n);\n// Subscribe to multiple topics\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\nreadStream\n()\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribe\"\n,\n\"topic1,topic2\"\n)\n.\nload\n();\ndf\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n);\n// Subscribe to a pattern\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\nreadStream\n()\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribePattern\"\n,\n\"topic.*\"\n)\n.\nload\n();\ndf\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n);\nCreating a Kafka Source for Batch Queries\nIf you have a use case that is better suited to batch processing,\nyou can create a Dataset/DataFrame for a defined range of offsets.\n# Subscribe to 1 topic defaults to the earliest and latest offsets\ndf\n=\nspark\n\\\n.\nread\n\\\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\noption\n(\n\"\nsubscribe\n\"\n,\n\"\ntopic1\n\"\n)\n\\\n.\nload\n()\ndf\n.\nselectExpr\n(\n\"\nCAST(key AS STRING)\n\"\n,\n\"\nCAST(value AS STRING)\n\"\n)\n# Subscribe to multiple topics, specifying explicit Kafka offsets\ndf\n=\nspark\n\\\n.\nread\n\\\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\noption\n(\n\"\nsubscribe\n\"\n,\n\"\ntopic1,topic2\n\"\n)\n\\\n.\noption\n(\n\"\nstartingOffsets\n\"\n,\n\"\"\"\n{\n\"\ntopic1\n\"\n:{\n\"\n0\n\"\n:23,\n\"\n1\n\"\n:-2},\n\"\ntopic2\n\"\n:{\n\"\n0\n\"\n:-2}}\n\"\"\"\n)\n\\\n.\noption\n(\n\"\nendingOffsets\n\"\n,\n\"\"\"\n{\n\"\ntopic1\n\"\n:{\n\"\n0\n\"\n:50,\n\"\n1\n\"\n:-1},\n\"\ntopic2\n\"\n:{\n\"\n0\n\"\n:-1}}\n\"\"\"\n)\n\\\n.\nload\n()\ndf\n.\nselectExpr\n(\n\"\nCAST(key AS STRING)\n\"\n,\n\"\nCAST(value AS STRING)\n\"\n)\n# Subscribe to a pattern, at the earliest and latest offsets\ndf\n=\nspark\n\\\n.\nread\n\\\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\noption\n(\n\"\nsubscribePattern\n\"\n,\n\"\ntopic.*\n\"\n)\n\\\n.\noption\n(\n\"\nstartingOffsets\n\"\n,\n\"\nearliest\n\"\n)\n\\\n.\noption\n(\n\"\nendingOffsets\n\"\n,\n\"\nlatest\n\"\n)\n\\\n.\nload\n()\ndf\n.\nselectExpr\n(\n\"\nCAST(key AS STRING)\n\"\n,\n\"\nCAST(value AS STRING)\n\"\n)\n// Subscribe to 1 topic defaults to the earliest and latest offsets\nval\ndf\n=\nspark\n.\nread\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribe\"\n,\n\"topic1\"\n)\n.\nload\n()\ndf\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n)\n.\nas\n[(\nString\n,\nString\n)]\n// Subscribe to multiple topics, specifying explicit Kafka offsets\nval\ndf\n=\nspark\n.\nread\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribe\"\n,\n\"topic1,topic2\"\n)\n.\noption\n(\n\"startingOffsets\"\n,\n\"\"\"{\"topic1\":{\"0\":23,\"1\":-2},\"topic2\":{\"0\":-2}}\"\"\"\n)\n.\noption\n(\n\"endingOffsets\"\n,\n\"\"\"{\"topic1\":{\"0\":50,\"1\":-1},\"topic2\":{\"0\":-1}}\"\"\"\n)\n.\nload\n()\ndf\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n)\n.\nas\n[(\nString\n,\nString\n)]\n// Subscribe to a pattern, at the earliest and latest offsets\nval\ndf\n=\nspark\n.\nread\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribePattern\"\n,\n\"topic.*\"\n)\n.\noption\n(\n\"startingOffsets\"\n,\n\"earliest\"\n)\n.\noption\n(\n\"endingOffsets\"\n,\n\"latest\"\n)\n.\nload\n()\ndf\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n)\n.\nas\n[(\nString\n,\nString\n)]\n// Subscribe to 1 topic defaults to the earliest and latest offsets\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\nread\n()\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribe\"\n,\n\"topic1\"\n)\n.\nload\n();\ndf\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n);\n// Subscribe to multiple topics, specifying explicit Kafka offsets\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\nread\n()\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribe\"\n,\n\"topic1,topic2\"\n)\n.\noption\n(\n\"startingOffsets\"\n,\n\"{\\\"topic1\\\":{\\\"0\\\":23,\\\"1\\\":-2},\\\"topic2\\\":{\\\"0\\\":-2}}\"\n)\n.\noption\n(\n\"endingOffsets\"\n,\n\"{\\\"topic1\\\":{\\\"0\\\":50,\\\"1\\\":-1},\\\"topic2\\\":{\\\"0\\\":-1}}\"\n)\n.\nload\n();\ndf\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n);\n// Subscribe to a pattern, at the earliest and latest offsets\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\nread\n()\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribePattern\"\n,\n\"topic.*\"\n)\n.\noption\n(\n\"startingOffsets\"\n,\n\"earliest\"\n)\n.\noption\n(\n\"endingOffsets\"\n,\n\"latest\"\n)\n.\nload\n();\ndf\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n);\nEach row in the source has the following schema:\nColumn\nType\nkey\nbinary\nvalue\nbinary\ntopic\nstring\npartition\nint\noffset\nlong\ntimestamp\ntimestamp\ntimestampType\nint\nheaders (optional)\narray\nThe following options must be set for the Kafka source\nfor both batch and streaming queries.\nOption\nvalue\nmeaning\nassign\njson string {\"topicA\":[0,1],\"topicB\":[2,4]}\nSpecific TopicPartitions to consume.\n  Only one of \"assign\", \"subscribe\" or \"subscribePattern\"\n  options can be specified for Kafka source.\nsubscribe\nA comma-separated list of topics\nThe topic list to subscribe.\n  Only one of \"assign\", \"subscribe\" or \"subscribePattern\"\n  options can be specified for Kafka source.\nsubscribePattern\nJava regex string\nThe pattern used to subscribe to topic(s).\n  Only one of \"assign, \"subscribe\" or \"subscribePattern\"\n  options can be specified for Kafka source.\nkafka.bootstrap.servers\nA comma-separated list of host:port\nThe Kafka \"bootstrap.servers\" configuration.\nThe following configurations are optional:\nOption\nvalue\ndefault\nquery type\nmeaning\nstartingTimestamp\ntimestamp string e.g. \"1000\"\nnone (next preference is\nstartingOffsetsByTimestamp\n)\nstreaming and batch\nThe start point of timestamp when a query is started, a string specifying a starting timestamp for\n  all partitions in topics being subscribed. Please refer the details on timestamp offset options below. If Kafka doesn't return the matched offset,\n  the behavior will follow to the value of the option\nstartingOffsetsByTimestampStrategy\nNote1:\nstartingTimestamp\ntakes precedence over\nstartingOffsetsByTimestamp\nand\nstartingOffsets\n.\nNote2: For streaming queries, this only applies when a new query is started, and that resuming will\n  always pick up from where the query left off. Newly discovered partitions during a query will start at\n  earliest.\nstartingOffsetsByTimestamp\njson string\n  \"\"\" {\"topicA\":{\"0\": 1000, \"1\": 1000}, \"topicB\": {\"0\": 2000, \"1\": 2000}} \"\"\"\nnone (next preference is\nstartingOffsets\n)\nstreaming and batch\nThe start point of timestamp when a query is started, a json string specifying a starting timestamp for\n  each TopicPartition. Please refer the details on timestamp offset options below. If Kafka doesn't return the matched offset,\n  the behavior will follow to the value of the option\nstartingOffsetsByTimestampStrategy\nNote1:\nstartingOffsetsByTimestamp\ntakes precedence over\nstartingOffsets\n.\nNote2: For streaming queries, this only applies when a new query is started, and that resuming will\n  always pick up from where the query left off. Newly discovered partitions during a query will start at\n  earliest.\nstartingOffsets\n\"earliest\", \"latest\" (streaming only), or json string\n  \"\"\" {\"topicA\":{\"0\":23,\"1\":-1},\"topicB\":{\"0\":-2}} \"\"\"\n\"latest\" for streaming, \"earliest\" for batch\nstreaming and batch\nThe start point when a query is started, either \"earliest\" which is from the earliest offsets,\n  \"latest\" which is just from the latest offsets, or a json string specifying a starting offset for\n  each TopicPartition.  In the json, -2 as an offset can be used to refer to earliest, -1 to latest.\n  Note: For batch queries, latest (either implicitly or by using -1 in json) is not allowed.\n  For streaming queries, this only applies when a new query is started, and that resuming will\n  always pick up from where the query left off. Newly discovered partitions during a query will start at\n  earliest.\nendingTimestamp\ntimestamp string e.g. \"1000\"\nnone (next preference is\nendingOffsetsByTimestamp\n)\nbatch query\nThe end point when a batch query is ended, a json string specifying an ending timestamp for\n  all partitions in topics being subscribed. Please refer the details on timestamp offset options below.\n  If Kafka doesn't return the matched offset, the offset will be set to latest.\nNote:\nendingTimestamp\ntakes precedence over\nendingOffsetsByTimestamp\nand\nendingOffsets\n.\nendingOffsetsByTimestamp\njson string\n  \"\"\" {\"topicA\":{\"0\": 1000, \"1\": 1000}, \"topicB\": {\"0\": 2000, \"1\": 2000}} \"\"\"\nnone (next preference is\nendingOffsets\n)\nbatch query\nThe end point when a batch query is ended, a json string specifying an ending timestamp for each TopicPartition.\n  Please refer the details on timestamp offset options below. If Kafka doesn't return the matched offset,\n  the offset will be set to latest.\nNote:\nendingOffsetsByTimestamp\ntakes precedence over\nendingOffsets\n.\nendingOffsets\nlatest or json string\n  {\"topicA\":{\"0\":23,\"1\":-1},\"topicB\":{\"0\":-1}}\nlatest\nbatch query\nThe end point when a batch query is ended, either \"latest\" which is just referred to the\n  latest, or a json string specifying an ending offset for each TopicPartition.  In the json, -1\n  as an offset can be used to refer to latest, and -2 (earliest) as an offset is not allowed.\nfailOnDataLoss\ntrue or false\ntrue\nstreaming and batch\nWhether to fail the query when it's possible that data is lost (e.g., topics are deleted, or\n  offsets are out of range). This may be a false alarm. You can disable it when it doesn't work\n  as you expected.\nkafkaConsumer.pollTimeoutMs\nlong\n120000\nstreaming and batch\nThe timeout in milliseconds to poll data from Kafka in executors. When not defined it falls\n  back to\nspark.network.timeout\n.\nfetchOffset.numRetries\nint\n3\nstreaming and batch\nNumber of times to retry before giving up fetching Kafka offsets.\nfetchOffset.retryIntervalMs\nlong\n10\nstreaming and batch\nmilliseconds to wait before retrying to fetch Kafka offsets\nmaxOffsetsPerTrigger\nlong\nnone\nstreaming query\nRate limit on maximum number of offsets processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume.\nminOffsetsPerTrigger\nlong\nnone\nstreaming query\nMinimum number of offsets to be processed per trigger interval. The specified total number of\n  offsets will be proportionally split across topicPartitions of different volume. Note, if the\n  maxTriggerDelay is exceeded, a trigger will be fired even if the number of available offsets\n  doesn't reach minOffsetsPerTrigger.\nmaxTriggerDelay\ntime with units\n15m\nstreaming query\nMaximum amount of time for which trigger can be delayed between two triggers provided some\n  data is available from the source. This option is only applicable if minOffsetsPerTrigger is set.\nminPartitions\nint\nnone\nstreaming and batch\nDesired minimum number of partitions to read from Kafka.\n  By default, Spark has a 1-1 mapping of topicPartitions to Spark partitions consuming from Kafka.\n  If you set this option to a value greater than your topicPartitions, Spark will divvy up large\n  Kafka partitions to smaller pieces. Please note that this configuration is like a\nhint\n: the\n  number of Spark tasks will be\napproximately\nminPartitions\n. It can be less or more depending on\n  rounding errors or Kafka partitions that didn't receive any new data.\nmaxRecordsPerPartition\nlong\nnone\nstreaming and batch\nLimit maximum number of records present in a partition.\n  By default, Spark has a 1-1 mapping of topicPartitions to Spark partitions consuming from Kafka.\n  If you set this option, Spark will divvy up Kafka partitions to smaller pieces so that each partition\n  has upto\nmaxRecordsPerPartition\nrecords. When both\nminPartitions\nand\nmaxRecordsPerPartition\nare set, number of partitions will be\napproximately\nmax of\n(recordsPerPartition / maxRecordsPerPartition)\nand\nminPartitions\n. In such case spark\n  will divvy up partitions based on\nmaxRecordsPerPartition\nand if the final partition count is less than\nminPartitions\nit will divvy up partitions again based on\nminPartitions\n.\ngroupIdPrefix\nstring\nspark-kafka-source\nstreaming and batch\nPrefix of consumer group identifiers (\ngroup.id\n) that are generated by structured streaming\n  queries. If \"kafka.group.id\" is set, this option will be ignored.\nkafka.group.id\nstring\nnone\nstreaming and batch\nThe Kafka group id to use in Kafka consumer while reading from Kafka. Use this with caution.\n  By default, each query generates a unique group id for reading data. This ensures that each Kafka\n  source has its own consumer group that does not face interference from any other consumer, and\n  therefore can read all of the partitions of its subscribed topics. In some scenarios (for example,\n  Kafka group-based authorization), you may want to use a specific authorized group id to read data.\n  You can optionally set the group id. However, do this with extreme caution as it can cause\n  unexpected behavior. Concurrently running queries (both, batch and streaming) or sources with the\n  same group id are likely interfere with each other causing each query to read only part of the\n  data. This may also occur when queries are started/restarted in quick succession. To minimize such\n  issues, set the Kafka consumer session timeout (by setting option \"kafka.session.timeout.ms\") to\n  be very small. When this is set, option \"groupIdPrefix\" will be ignored.\nincludeHeaders\nboolean\nfalse\nstreaming and batch\nWhether to include the Kafka headers in the row.\nstartingOffsetsByTimestampStrategy\n\"error\" or \"latest\"\n\"error\"\nstreaming and batch\nThe strategy will be used when the specified starting offset by timestamp (either global or per partition) doesn't match with the offset Kafka returned. Here's the strategy name and corresponding descriptions:\n\"error\": fail the query and end users have to deal with workarounds requiring manual steps.\n\"latest\": assigns the latest offset for these partitions, so that Spark can read newer records from these partitions in further micro-batches.\nDetails on timestamp offset options\nThe returned offset for each partition is the earliest offset whose timestamp is greater than or equal to the given timestamp in the corresponding partition.\nThe behavior varies across options if Kafka doesn’t return the matched offset - check the description of each option.\nSpark simply passes the timestamp information to\nKafkaConsumer.offsetsForTimes\n, and doesn’t interpret or reason about the value.\nFor more details on\nKafkaConsumer.offsetsForTimes\n, please refer\njavadoc\nfor details.\nAlso, the meaning of\ntimestamp\nhere can be vary according to Kafka configuration (\nlog.message.timestamp.type\n): please refer\nKafka documentation\nfor further details.\nTimestamp offset options require Kafka 0.10.1.0 or higher.\nOffset fetching\nIn Spark 3.0 and before Spark uses\nKafkaConsumer\nfor offset fetching which could cause infinite wait in the driver.\nIn Spark 3.1 a new configuration option added\nspark.sql.streaming.kafka.useDeprecatedOffsetFetching\n(default:\nfalse\n)\nwhich allows Spark to use new offset fetching mechanism using\nAdminClient\n. (Set this to\ntrue\nto use old offset fetching with\nKafkaConsumer\n.)\nWhen the new mechanism used the following applies.\nFirst of all the new approach supports Kafka brokers\n0.11.0.0+\n.\nIn Spark 3.0 and below, secure Kafka processing needed the following ACLs from driver perspective:\nTopic resource describe operation\nTopic resource read operation\nGroup resource read operation\nSince Spark 3.1, offsets can be obtained with\nAdminClient\ninstead of\nKafkaConsumer\nand for that the following ACLs needed from driver perspective:\nTopic resource describe operation\nSince\nAdminClient\nin driver is not connecting to consumer group,\ngroup.id\nbased authorization will not work anymore (executors never done group based authorization).\nWorth to mention executor side is behaving the exact same way like before (group prefix and override works).\nConsumer Caching\nIt’s time-consuming to initialize Kafka consumers, especially in streaming scenarios where processing time is a key factor.\nBecause of this, Spark pools Kafka consumers on executors, by leveraging Apache Commons Pool.\nThe caching key is built up from the following information:\nTopic name\nTopic partition\nGroup ID\nThe following properties are available to configure the consumer pool:\nProperty Name\nDefault\nMeaning\nSince Version\nspark.kafka.consumer.cache.capacity\n64\nThe maximum number of consumers cached. Please note that it's a soft limit.\n3.0.0\nspark.kafka.consumer.cache.timeout\n5m (5 minutes)\nThe minimum amount of time a consumer may sit idle in the pool before it is eligible for eviction by the evictor.\n3.0.0\nspark.kafka.consumer.cache.evictorThreadRunInterval\n1m (1 minute)\nThe interval of time between runs of the idle evictor thread for consumer pool. When non-positive, no idle evictor thread will be run.\n3.0.0\nspark.kafka.consumer.cache.jmx.enable\nfalse\nEnable or disable JMX for pools created with this configuration instance. Statistics of the pool are available via JMX instance.\n  The prefix of JMX name is set to \"kafka010-cached-simple-kafka-consumer-pool\".\n3.0.0\nThe size of the pool is limited by\nspark.kafka.consumer.cache.capacity\n,\nbut it works as “soft-limit” to not block Spark tasks.\nIdle eviction thread periodically removes consumers which are not used longer than given timeout.\nIf this threshold is reached when borrowing, it tries to remove the least-used entry that is currently not in use.\nIf it cannot be removed, then the pool will keep growing. In the worst case, the pool will grow to\nthe max number of concurrent tasks that can run in the executor (that is, number of task slots).\nIf a task fails for any reason, the new task is executed with a newly created Kafka consumer for safety reasons.\nAt the same time, we invalidate all consumers in pool which have same caching key, to remove consumer which was used\nin failed execution. Consumers which any other tasks are using will not be closed, but will be invalidated as well\nwhen they are returned into pool.\nAlong with consumers, Spark pools the records fetched from Kafka separately, to let Kafka consumers stateless in point\nof Spark’s view, and maximize the efficiency of pooling. It leverages same cache key with Kafka consumers pool.\nNote that it doesn’t leverage Apache Commons Pool due to the difference of characteristics.\nThe following properties are available to configure the fetched data pool:\nProperty Name\nDefault\nMeaning\nSince Version\nspark.kafka.consumer.fetchedData.cache.timeout\n5m (5 minutes)\nThe minimum amount of time a fetched data may sit idle in the pool before it is eligible for eviction by the evictor.\n3.0.0\nspark.kafka.consumer.fetchedData.cache.evictorThreadRunInterval\n1m (1 minute)\nThe interval of time between runs of the idle evictor thread for fetched data pool. When non-positive, no idle evictor thread will be run.\n3.0.0\nWriting Data to Kafka\nHere, we describe the support for writing Streaming Queries and Batch Queries to Apache Kafka. Take note that\nApache Kafka only supports at least once write semantics. Consequently, when writing—either Streaming Queries\nor Batch Queries—to Kafka, some records may be duplicated; this can happen, for example, if Kafka needs\nto retry a message that was not acknowledged by a Broker, even though that Broker received and wrote the message record.\nStructured Streaming cannot prevent such duplicates from occurring due to these Kafka write semantics. However,\nif writing the query is successful, then you can assume that the query output was written at least once. A possible\nsolution to remove duplicates when reading the written data could be to introduce a primary (unique) key\nthat can be used to perform de-duplication when reading.\nThe Dataframe being written to Kafka should have the following columns in schema:\nColumn\nType\nkey (optional)\nstring or binary\nvalue (required)\nstring or binary\nheaders (optional)\narray\ntopic (*optional)\nstring\npartition (optional)\nint\n* The topic column is required if the “topic” configuration option is not specified.\nThe value column is the only required option. If a key column is not specified then\na\nnull\nvalued key column will be automatically added (see Kafka semantics on\nhow\nnull\nvalued key values are handled). If a topic column exists then its value\nis used as the topic when writing the given row to Kafka, unless the “topic” configuration\noption is set i.e., the “topic” configuration option overrides the topic column.\nIf a “partition” column is not specified (or its value is\nnull\n)\nthen the partition is calculated by the Kafka producer.\nA Kafka partitioner can be specified in Spark by setting the\nkafka.partitioner.class\noption. If not present, Kafka default partitioner\nwill be used.\nThe following options must be set for the Kafka sink\nfor both batch and streaming queries.\nOption\nvalue\nmeaning\nkafka.bootstrap.servers\nA comma-separated list of host:port\nThe Kafka \"bootstrap.servers\" configuration.\nThe following configurations are optional:\nOption\nvalue\ndefault\nquery type\nmeaning\ntopic\nstring\nnone\nstreaming and batch\nSets the topic that all rows will be written to in Kafka. This option overrides any\n  topic column that may exist in the data.\nincludeHeaders\nboolean\nfalse\nstreaming and batch\nWhether to include the Kafka headers in the row.\nCreating a Kafka Sink for Streaming Queries\n# Write key-value data from a DataFrame to a specific Kafka topic specified in an option\nds\n=\ndf\n\\\n.\nselectExpr\n(\n\"\nCAST(key AS STRING)\n\"\n,\n\"\nCAST(value AS STRING)\n\"\n)\n\\\n.\nwriteStream\n\\\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\noption\n(\n\"\ntopic\n\"\n,\n\"\ntopic1\n\"\n)\n\\\n.\nstart\n()\n# Write key-value data from a DataFrame to Kafka using a topic specified in the data\nds\n=\ndf\n\\\n.\nselectExpr\n(\n\"\ntopic\n\"\n,\n\"\nCAST(key AS STRING)\n\"\n,\n\"\nCAST(value AS STRING)\n\"\n)\n\\\n.\nwriteStream\n\\\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\nstart\n()\n// Write key-value data from a DataFrame to a specific Kafka topic specified in an option\nval\nds\n=\ndf\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n)\n.\nwriteStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"topic\"\n,\n\"topic1\"\n)\n.\nstart\n()\n// Write key-value data from a DataFrame to Kafka using a topic specified in the data\nval\nds\n=\ndf\n.\nselectExpr\n(\n\"topic\"\n,\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n)\n.\nwriteStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\nstart\n()\n// Write key-value data from a DataFrame to a specific Kafka topic specified in an option\nStreamingQuery\nds\n=\ndf\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n)\n.\nwriteStream\n()\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"topic\"\n,\n\"topic1\"\n)\n.\nstart\n();\n// Write key-value data from a DataFrame to Kafka using a topic specified in the data\nStreamingQuery\nds\n=\ndf\n.\nselectExpr\n(\n\"topic\"\n,\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n)\n.\nwriteStream\n()\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\nstart\n();\nWriting the output of Batch Queries to Kafka\n# Write key-value data from a DataFrame to a specific Kafka topic specified in an option\ndf\n.\nselectExpr\n(\n\"\nCAST(key AS STRING)\n\"\n,\n\"\nCAST(value AS STRING)\n\"\n)\n\\\n.\nwrite\n\\\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\noption\n(\n\"\ntopic\n\"\n,\n\"\ntopic1\n\"\n)\n\\\n.\nsave\n()\n# Write key-value data from a DataFrame to Kafka using a topic specified in the data\ndf\n.\nselectExpr\n(\n\"\ntopic\n\"\n,\n\"\nCAST(key AS STRING)\n\"\n,\n\"\nCAST(value AS STRING)\n\"\n)\n\\\n.\nwrite\n\\\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\nsave\n()\n// Write key-value data from a DataFrame to a specific Kafka topic specified in an option\ndf\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n)\n.\nwrite\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"topic\"\n,\n\"topic1\"\n)\n.\nsave\n()\n// Write key-value data from a DataFrame to Kafka using a topic specified in the data\ndf\n.\nselectExpr\n(\n\"topic\"\n,\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n)\n.\nwrite\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\nsave\n()\n// Write key-value data from a DataFrame to a specific Kafka topic specified in an option\ndf\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n)\n.\nwrite\n()\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"topic\"\n,\n\"topic1\"\n)\n.\nsave\n();\n// Write key-value data from a DataFrame to Kafka using a topic specified in the data\ndf\n.\nselectExpr\n(\n\"topic\"\n,\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n)\n.\nwrite\n()\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\nsave\n();\nProducer Caching\nGiven Kafka producer instance is designed to be thread-safe, Spark initializes a Kafka producer instance and co-use across tasks for same caching key.\nThe caching key is built up from the following information:\nKafka producer configuration\nThis includes configuration for authorization, which Spark will automatically include when delegation token is being used. Even we take authorization into account, you can expect same Kafka producer instance will be used among same Kafka producer configuration.\nIt will use different Kafka producer when delegation token is renewed; Kafka producer instance for old delegation token will be evicted according to the cache policy.\nThe following properties are available to configure the producer pool:\nProperty Name\nDefault\nMeaning\nSince Version\nspark.kafka.producer.cache.timeout\n10m (10 minutes)\nThe minimum amount of time a producer may sit idle in the pool before it is eligible for eviction by the evictor.\n2.2.1\nspark.kafka.producer.cache.evictorThreadRunInterval\n1m (1 minute)\nThe interval of time between runs of the idle evictor thread for producer pool. When non-positive, no idle evictor thread will be run.\n3.0.0\nIdle eviction thread periodically removes producers which are not used longer than given timeout. Note that the producer is shared and used concurrently, so the last used timestamp is determined by the moment the producer instance is returned and reference count is 0.\nKafka Specific Configurations\nKafka’s own configurations can be set via\nDataStreamReader.option\nwith\nkafka.\nprefix, e.g,\nstream.option(\"kafka.bootstrap.servers\", \"host:port\")\n. For possible kafka parameters, see\nKafka consumer config docs\nfor\nparameters related to reading data, and\nKafka producer config docs\nfor parameters related to writing data.\nNote that the following Kafka params cannot be set and the Kafka source or sink will throw an exception:\ngroup.id\n: Kafka source will create a unique group id for each query automatically. The user can\nset the prefix of the automatically generated group.id’s via the optional source option\ngroupIdPrefix\n,\ndefault value is “spark-kafka-source”. You can also set “kafka.group.id” to force Spark to use a special\ngroup id, however, please read warnings for this option and use it with caution.\nauto.offset.reset\n: Set the source option\nstartingOffsets\nto specify\n where to start instead. Structured Streaming manages which offsets are consumed internally, rather\n than rely on the kafka Consumer to do it. This will ensure that no data is missed when new\n topics/partitions are dynamically subscribed. Note that\nstartingOffsets\nonly applies when a new\n streaming query is started, and that resuming will always pick up from where the query left off. Note\n that when the offsets consumed by a streaming application no longer exist in Kafka (e.g., topics are deleted,\n offsets are out of range, or offsets are removed after retention period), the offsets will not be reset\n and the streaming application will see data loss. In extreme cases, for example the throughput of the\n streaming application cannot catch up the retention speed of Kafka, the input rows of a batch might be\n gradually reduced until zero when the offset ranges of the batch are completely not in Kafka. Enabling\nfailOnDataLoss\noption can ask Structured Streaming to fail the query for such cases.\nkey.deserializer\n: Keys are always deserialized as byte arrays with ByteArrayDeserializer. Use\n DataFrame operations to explicitly deserialize the keys.\nvalue.deserializer\n: Values are always deserialized as byte arrays with ByteArrayDeserializer.\n Use DataFrame operations to explicitly deserialize the values.\nkey.serializer\n: Keys are always serialized with ByteArraySerializer or StringSerializer. Use\nDataFrame operations to explicitly serialize the keys into either strings or byte arrays.\nvalue.serializer\n: values are always serialized with ByteArraySerializer or StringSerializer. Use\nDataFrame operations to explicitly serialize the values into either strings or byte arrays.\nenable.auto.commit\n: Kafka source doesn’t commit any offset.\ninterceptor.classes\n: Kafka source always read keys and values as byte arrays. It’s not safe to\n use ConsumerInterceptor as it may break the query.\nDeploying\nAs with any Spark applications,\nspark-submit\nis used to launch your application.\nspark-sql-kafka-0-10_2.13\nand its dependencies can be directly added to\nspark-submit\nusing\n--packages\n, such as,\n./bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0 ...\nFor experimenting on\nspark-shell\n, you can also use\n--packages\nto add\nspark-sql-kafka-0-10_2.13\nand its dependencies directly,\n./bin/spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0 ...\nSee\nApplication Submission Guide\nfor more details about submitting\napplications with external dependencies.\nSecurity\nKafka 0.9.0.0 introduced several features that increases security in a cluster. For detailed\ndescription about these possibilities, see\nKafka security docs\n.\nIt’s worth noting that security is optional and turned off by default.\nSpark supports the following ways to authenticate against Kafka cluster:\nDelegation token (introduced in Kafka broker 1.1.0)\nJAAS login configuration\nDelegation token\nThis way the application can be configured via Spark parameters and may not need JAAS login\nconfiguration (Spark can use Kafka’s dynamic JAAS configuration feature). For further information\nabout delegation tokens, see\nKafka delegation token docs\n.\nThe process is initiated by Spark’s Kafka delegation token provider. When\nspark.kafka.clusters.${cluster}.auth.bootstrap.servers\nis set,\nSpark considers the following log in options, in order of preference:\nJAAS login configuration\n, please see example below.\nKeytab file\n, such as,\n./bin/spark-submit \\\n    --keytab <KEYTAB_FILE> \\\n    --principal <PRINCIPAL> \\\n    --conf spark.kafka.clusters.${cluster}.auth.bootstrap.servers=<KAFKA_SERVERS> \\\n    ...\nKerberos credential cache\n, such as,\n./bin/spark-submit \\\n    --conf spark.kafka.clusters.${cluster}.auth.bootstrap.servers=<KAFKA_SERVERS> \\\n    ...\nThe Kafka delegation token provider can be turned off by setting\nspark.security.credentials.kafka.enabled\nto\nfalse\n(default:\ntrue\n).\nSpark can be configured to use the following authentication protocols to obtain token (it must match with\nKafka broker configuration):\nSASL SSL (default)\nSSL\nSASL PLAINTEXT (for testing)\nAfter obtaining delegation token successfully, Spark distributes it across nodes and renews it accordingly.\nDelegation token uses\nSCRAM\nlogin module for authentication and because of that the appropriate\nspark.kafka.clusters.${cluster}.sasl.token.mechanism\n(default:\nSCRAM-SHA-512\n) has to be configured. Also, this parameter\nmust match with Kafka broker configuration.\nWhen delegation token is available on an executor Spark considers the following log in options, in order of preference:\nJAAS login configuration\n, please see example below.\nDelegation token\n, please see\nspark.kafka.clusters.${cluster}.target.bootstrap.servers.regex\nparameter for further details.\nWhen none of the above applies then unsecure connection assumed.\nConfiguration\nDelegation tokens can be obtained from multiple clusters and\n${cluster}\nis an arbitrary unique identifier which helps to group different configurations.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.kafka.clusters.${cluster}.auth.bootstrap.servers\nNone\nA list of coma separated host/port pairs to use for establishing the initial connection\n      to the Kafka cluster. For further details please see Kafka documentation. Only used to obtain delegation token.\n3.0.0\nspark.kafka.clusters.${cluster}.target.bootstrap.servers.regex\n.*\nRegular expression to match against the\nbootstrap.servers\nconfig for sources and sinks in the application.\n      If a server address matches this regex, the delegation token obtained from the respective bootstrap servers will be used when connecting.\n      If multiple clusters match the address, an exception will be thrown and the query won't be started.\n      Kafka's secure and unsecure listeners are bound to different ports. When both used the secure listener port has to be part of the regular expression.\n3.0.0\nspark.kafka.clusters.${cluster}.security.protocol\nSASL_SSL\nProtocol used to communicate with brokers. For further details please see Kafka documentation. Protocol is applied on all the sources and sinks as default where\nbootstrap.servers\nconfig matches (for further details please see\nspark.kafka.clusters.${cluster}.target.bootstrap.servers.regex\n),\n      and can be overridden by setting\nkafka.security.protocol\non the source or sink.\n3.0.0\nspark.kafka.clusters.${cluster}.sasl.kerberos.service.name\nkafka\nThe Kerberos principal name that Kafka runs as. This can be defined either in Kafka's JAAS config or in Kafka's config.\n      For further details please see Kafka documentation. Only used to obtain delegation token.\n3.0.0\nspark.kafka.clusters.${cluster}.ssl.truststore.type\nNone\nThe file format of the trust store file. For further details please see Kafka documentation. Only used to obtain delegation token.\n3.2.0\nspark.kafka.clusters.${cluster}.ssl.truststore.location\nNone\nThe location of the trust store file. For further details please see Kafka documentation. Only used to obtain delegation token.\n3.0.0\nspark.kafka.clusters.${cluster}.ssl.truststore.password\nNone\nThe store password for the trust store file. This is optional and only needed if\nspark.kafka.clusters.${cluster}.ssl.truststore.location\nis configured.\n      For further details please see Kafka documentation. Only used to obtain delegation token.\n3.0.0\nspark.kafka.clusters.${cluster}.ssl.keystore.type\nNone\nThe file format of the key store file. This is optional for client.\n      For further details please see Kafka documentation. Only used to obtain delegation token.\n3.2.0\nspark.kafka.clusters.${cluster}.ssl.keystore.location\nNone\nThe location of the key store file. This is optional for client and can be used for two-way authentication for client.\n      For further details please see Kafka documentation. Only used to obtain delegation token.\n3.0.0\nspark.kafka.clusters.${cluster}.ssl.keystore.password\nNone\nThe store password for the key store file. This is optional and only needed if\nspark.kafka.clusters.${cluster}.ssl.keystore.location\nis configured.\n      For further details please see Kafka documentation. Only used to obtain delegation token.\n3.0.0\nspark.kafka.clusters.${cluster}.ssl.key.password\nNone\nThe password of the private key in the key store file. This is optional for client.\n      For further details please see Kafka documentation. Only used to obtain delegation token.\n3.0.0\nspark.kafka.clusters.${cluster}.sasl.token.mechanism\nSCRAM-SHA-512\nSASL mechanism used for client connections with delegation token. Because SCRAM login module used for authentication a compatible mechanism has to be set here.\n      For further details please see Kafka documentation (\nsasl.mechanism\n). Only used to authenticate against Kafka broker with delegation token.\n3.0.0\nKafka Specific Configurations\nKafka’s own configurations can be set with\nkafka.\nprefix, e.g,\n--conf spark.kafka.clusters.${cluster}.kafka.retries=1\n.\nFor possible Kafka parameters, see\nKafka adminclient config docs\n.\nCaveats\nObtaining delegation token for proxy user is not yet supported (\nKAFKA-6945\n).\nJAAS login configuration\nJAAS login configuration must placed on all nodes where Spark tries to access Kafka cluster.\nThis provides the possibility to apply any custom authentication logic with a higher cost to maintain.\nThis can be done several ways. One possibility is to provide additional JVM parameters, such as,\n./bin/spark-submit \\\n    --driver-java-options \"-Djava.security.auth.login.config=/path/to/custom_jaas.conf\" \\\n    --conf spark.executor.extraJavaOptions=-Djava.security.auth.login.config=/path/to/custom_jaas.conf \\\n    ..."}
{"url": "https://spark.apache.org/docs/latest/ml-guide", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nMachine Learning Library (MLlib) Guide\nMLlib is Spark’s machine learning (ML) library.\nIts goal is to make practical machine learning scalable and easy.\nAt a high level, it provides tools such as:\nML Algorithms: common learning algorithms such as classification, regression, clustering, and collaborative filtering\nFeaturization: feature extraction, transformation, dimensionality reduction, and selection\nPipelines: tools for constructing, evaluating, and tuning ML Pipelines\nPersistence: saving and load algorithms, models, and Pipelines\nUtilities: linear algebra, statistics, data handling, etc.\nAnnouncement: DataFrame-based API is primary API\nThe MLlib RDD-based API is now in maintenance mode.\nAs of Spark 2.0, the\nRDD\n-based APIs in the\nspark.mllib\npackage have entered maintenance mode.\nThe primary Machine Learning API for Spark is now the\nDataFrame\n-based API in the\nspark.ml\npackage.\nWhat are the implications?\nMLlib will still support the RDD-based API in\nspark.mllib\nwith bug fixes.\nMLlib will not add new features to the RDD-based API.\nIn the Spark 2.x releases, MLlib will add features to the DataFrames-based API to reach feature parity with the RDD-based API.\nWhy is MLlib switching to the DataFrame-based API?\nDataFrames provide a more user-friendly API than RDDs.  The many benefits of DataFrames include Spark Datasources, SQL/DataFrame queries, Tungsten and Catalyst optimizations, and uniform APIs across languages.\nThe DataFrame-based API for MLlib provides a uniform API across ML algorithms and across multiple languages.\nDataFrames facilitate practical ML Pipelines, particularly feature transformations.  See the\nPipelines guide\nfor details.\nWhat is “Spark ML”?\n“Spark ML” is not an official name but occasionally used to refer to the MLlib DataFrame-based API.\nThis is majorly due to the\norg.apache.spark.ml\nScala package name used by the DataFrame-based API, \nand the “Spark ML Pipelines” term we used initially to emphasize the pipeline concept.\nIs MLlib deprecated?\nNo. MLlib includes both the RDD-based API and the DataFrame-based API.\nThe RDD-based API is now in maintenance mode.\nBut neither API is deprecated, nor MLlib as a whole.\nDependencies\nMLlib uses linear algebra packages\nBreeze\nand\ndev.ludovic.netlib\nfor optimised numerical processing\n1\n. Those packages may call native acceleration libraries such as\nIntel MKL\nor\nOpenBLAS\nif they are available as system libraries or in runtime library paths.\nHowever, native acceleration libraries can’t be distributed with Spark. See\nMLlib Linear Algebra Acceleration Guide\nfor how to enable accelerated linear algebra processing. If accelerated native libraries are not enabled, you will see a warning message like below and a pure JVM implementation will be used instead:\nWARNING: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\nTo use MLlib in Python, you will need\nNumPy\nversion 1.4 or newer.\nHighlights in 3.0\nThe list below highlights some of the new features and enhancements added to MLlib in the\n3.0\nrelease of Spark:\nMultiple columns support was added to\nBinarizer\n(\nSPARK-23578\n),\nStringIndexer\n(\nSPARK-11215\n),\nStopWordsRemover\n(\nSPARK-29808\n) and PySpark\nQuantileDiscretizer\n(\nSPARK-22796\n).\nTree-Based Feature Transformation was added\n(\nSPARK-13677\n).\nTwo new evaluators\nMultilabelClassificationEvaluator\n(\nSPARK-16692\n) and\nRankingEvaluator\n(\nSPARK-28045\n) were added.\nSample weights support was added in\nDecisionTreeClassifier/Regressor\n(\nSPARK-19591\n),\nRandomForestClassifier/Regressor\n(\nSPARK-9478\n),\nGBTClassifier/Regressor\n(\nSPARK-9612\n),\nMulticlassClassificationEvaluator\n(\nSPARK-24101\n),\nRegressionEvaluator\n(\nSPARK-24102\n),\nBinaryClassificationEvaluator\n(\nSPARK-24103\n),\nBisectingKMeans\n(\nSPARK-30351\n),\nKMeans\n(\nSPARK-29967\n) and\nGaussianMixture\n(\nSPARK-30102\n).\nR API for\nPowerIterationClustering\nwas added\n(\nSPARK-19827\n).\nAdded Spark ML listener for tracking ML pipeline status\n(\nSPARK-23674\n).\nFit with validation set was added to Gradient Boosted Trees in Python\n(\nSPARK-24333\n).\nRobustScaler\ntransformer was added\n(\nSPARK-28399\n).\nFactorization Machines\nclassifier and regressor were added\n(\nSPARK-29224\n).\nGaussian Naive Bayes Classifier (\nSPARK-16872\n) and Complement Naive Bayes Classifier (\nSPARK-29942\n) were added.\nML function parity between Scala and Python\n(\nSPARK-28958\n).\npredictRaw\nis made public in all the Classification models.\npredictProbability\nis made public in all the Classification models except\nLinearSVCModel\n(\nSPARK-30358\n).\nMigration Guide\nThe migration guide is now archived\non this page\n.\nTo learn more about the benefits and background of system optimised natives, you may wish to\nwatch Sam Halliday’s ScalaX talk on\nHigh Performance Linear Algebra in Scala\n.\n↩"}
{"url": "https://spark.apache.org/docs/latest/sql-ref.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nANSI Compliance\nData Types\nDatetime Pattern\nNumber Pattern\nOperators\nFunctions\nIdentifiers\nIDENTIFIER clause\nLiterals\nNull Semantics\nSQL Syntax\nError Conditions\nSQL Reference\nSpark SQL is Apache Spark’s module for working with structured data. This guide is a reference for Structured Query Language (SQL) and includes syntax, semantics, keywords, and examples for common SQL usage. It contains information for the following topics:\nANSI Compliance\nData Types\nDatetime Pattern\nNumber Pattern\nOperators\nFunctions\nBuilt-in Functions\nScalar User-Defined Functions (UDFs)\nUser-Defined Aggregate Functions (UDAFs)\nIntegration with Hive UDFs/UDAFs/UDTFs\nFunction Invocation\nIdentifiers\nIDENTIFIER clause\nLiterals\nNull Semantics\nSQL Syntax\nDDL Statements\nDML Statements\nData Retrieval Statements\nAuxiliary Statements\nPipe Syntax"}
{"url": "https://spark.apache.org/docs/latest/sparkr-migration-guide.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMigration Guide: SparkR (R on Spark)\nUpgrading from SparkR 3.5 to 4.0\nUpgrading from SparkR 3.1 to 3.2\nUpgrading from SparkR 2.4 to 3.0\nUpgrading from SparkR 2.3 to 2.4\nUpgrading from SparkR 2.3 to 2.3.1 and above\nUpgrading from SparkR 2.2 to 2.3\nUpgrading from SparkR 2.1 to 2.2\nUpgrading from SparkR 2.0 to 3.1\nUpgrading from SparkR 1.6 to 2.0\nUpgrading from SparkR 1.5 to 1.6\nNote that this migration guide describes the items specific to SparkR.\nMany items of SQL migration can be applied when migrating SparkR to higher versions.\nPlease refer\nMigration Guide: SQL, Datasets and DataFrame\n.\nUpgrading from SparkR 3.5 to 4.0\nIn Spark 4.0, SparkR is deprecated and will be removed in a future version.\nUpgrading from SparkR 3.1 to 3.2\nPreviously, SparkR automatically downloaded and installed the Spark distribution in user’s cache directory to complete SparkR installation when SparkR runs in a plain R shell or Rscript, and the Spark distribution cannot be found. Now, it asks if users want to download and install or not. To restore the previous behavior, set\nSPARKR_ASK_INSTALLATION\nenvironment variable to\nFALSE\n.\nUpgrading from SparkR 2.4 to 3.0\nThe deprecated methods\nparquetFile\n,\nsaveAsParquetFile\n,\njsonFile\n,\njsonRDD\nhave been removed. Use\nread.parquet\n,\nwrite.parquet\n,\nread.json\ninstead.\nUpgrading from SparkR 2.3 to 2.4\nPreviously, we don’t check the validity of the size of the last layer in\nspark.mlp\n. For example, if the training data only has two labels, a\nlayers\nparam like\nc(1, 3)\ndoesn’t cause an error previously, now it does.\nUpgrading from SparkR 2.3 to 2.3.1 and above\nIn SparkR 2.3.0 and earlier, the\nstart\nparameter of\nsubstr\nmethod was wrongly subtracted by one and considered as 0-based. This can lead to inconsistent substring results and also does not match with the behaviour with\nsubstr\nin R. In version 2.3.1 and later, it has been fixed so the\nstart\nparameter of\nsubstr\nmethod is now 1-based. As an example,\nsubstr(lit('abcdef'), 2, 4))\nwould result to\nabc\nin SparkR 2.3.0, and the result would be\nbcd\nin SparkR 2.3.1.\nUpgrading from SparkR 2.2 to 2.3\nThe\nstringsAsFactors\nparameter was previously ignored with\ncollect\n, for example, in\ncollect(createDataFrame(iris), stringsAsFactors = TRUE))\n. It has been corrected.\nFor\nsummary\n, option for statistics to compute has been added. Its output is changed from that from\ndescribe\n.\nA warning can be raised if versions of SparkR package and the Spark JVM do not match.\nUpgrading from SparkR 2.1 to 2.2\nA\nnumPartitions\nparameter has been added to\ncreateDataFrame\nand\nas.DataFrame\n. When splitting the data, the partition position calculation has been made to match the one in Scala.\nThe method\ncreateExternalTable\nhas been deprecated to be replaced by\ncreateTable\n. Either methods can be called to create external or managed table. Additional catalog methods have also been added.\nBy default, derby.log is now saved to\ntempdir()\n. This will be created when instantiating the SparkSession with\nenableHiveSupport\nset to\nTRUE\n.\nspark.lda\nwas not setting the optimizer correctly. It has been corrected.\nSeveral model summary outputs are updated to have\ncoefficients\nas\nmatrix\n. This includes\nspark.logit\n,\nspark.kmeans\n,\nspark.glm\n. Model summary outputs for\nspark.gaussianMixture\nhave added log-likelihood as\nloglik\n.\nUpgrading from SparkR 2.0 to 3.1\njoin\nno longer performs Cartesian Product by default, use\ncrossJoin\ninstead.\nUpgrading from SparkR 1.6 to 2.0\nThe method\ntable\nhas been removed and replaced by\ntableToDF\n.\nThe class\nDataFrame\nhas been renamed to\nSparkDataFrame\nto avoid name conflicts.\nSpark’s\nSQLContext\nand\nHiveContext\nhave been deprecated to be replaced by\nSparkSession\n. Instead of\nsparkR.init()\n, call\nsparkR.session()\nin its place to instantiate the SparkSession. Once that is done, that currently active SparkSession will be used for SparkDataFrame operations.\nThe parameter\nsparkExecutorEnv\nis not supported by\nsparkR.session\n. To set environment for the executors, set Spark config properties with the prefix “spark.executorEnv.VAR_NAME”, for example, “spark.executorEnv.PATH”\nThe\nsqlContext\nparameter is no longer required for these functions:\ncreateDataFrame\n,\nas.DataFrame\n,\nread.json\n,\njsonFile\n,\nread.parquet\n,\nparquetFile\n,\nread.text\n,\nsql\n,\ntables\n,\ntableNames\n,\ncacheTable\n,\nuncacheTable\n,\nclearCache\n,\ndropTempTable\n,\nread.df\n,\nloadDF\n,\ncreateExternalTable\n.\nThe method\nregisterTempTable\nhas been deprecated to be replaced by\ncreateOrReplaceTempView\n.\nThe method\ndropTempTable\nhas been deprecated to be replaced by\ndropTempView\n.\nThe\nsc\nSparkContext parameter is no longer required for these functions:\nsetJobGroup\n,\nclearJobGroup\n,\ncancelJobGroup\nUpgrading from SparkR 1.5 to 1.6\nBefore Spark 1.6.0, the default mode for writes was\nappend\n. It was changed in Spark 1.6.0 to\nerror\nto match the Scala API.\nSparkSQL converts\nNA\nin R to\nnull\nand vice-versa.\nSince 1.6.1, withColumn method in SparkR supports adding a new column to or replacing existing columns\nof the same name of a DataFrame."}
{"url": "https://spark.apache.org/docs/latest/sql-data-sources.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTroubleshooting\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nData Sources\nSpark SQL supports operating on a variety of data sources through the DataFrame interface.\nA DataFrame can be operated on using relational transformations and can also be used to create a temporary view.\nRegistering a DataFrame as a temporary view allows you to run SQL queries over its data. This section\ndescribes the general methods for loading and saving data using the Spark Data Sources and then\ngoes into specific options that are available for the built-in data sources.\nGeneric Load/Save Functions\nManually Specifying Options\nRun SQL on files directly\nSave Modes\nSaving to Persistent Tables\nBucketing, Sorting and Partitioning\nGeneric File Source Options\nIgnore Corrupt Files\nIgnore Missing Files\nPath Glob Filter\nRecursive File Lookup\nParquet Files\nLoading Data Programmatically\nPartition Discovery\nSchema Merging\nHive metastore Parquet table conversion\nConfiguration\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nSpecifying storage format for Hive tables\nInteracting with Different Versions of Hive Metastore\nJDBC To Other Databases\nAvro Files\nDeploying\nLoad and Save Functions\nto_avro() and from_avro()\nData Source Option\nConfiguration\nCompatibility with Databricks spark-avro\nSupported types for Avro -> Spark SQL conversion\nSupported types for Spark SQL -> Avro conversion\nProtobuf data\nDeploying\nto_protobuf() and from_protobuf()\nSupported types for Protobuf -> Spark SQL conversion\nSupported types for Spark SQL -> Protobuf conversion\nHandling circular references protobuf fields\nWhole Binary Files\nTroubleshooting"}
{"url": "https://spark.apache.org/docs/latest/streaming/additional-information.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nStructured Streaming Programming Guide\nOverview\nGetting Started\nAPIs on DataFrames and Datasets\nPerformance Tips\nAdditional Information\nMiscellaneous Notes\nRelated Resources\nMigration Guide\nStructured Streaming Programming Guide\nMiscellaneous Notes\nSeveral configurations are not modifiable after the query has run. To change them, discard the checkpoint and start a new query. These configurations include:\nspark.sql.shuffle.partitions\nThis is due to the physical partitioning of state: state is partitioned via applying hash function to key, hence the number of partitions for state should be unchanged.\nIf you want to run fewer tasks for stateful operations,\ncoalesce\nwould help with avoiding unnecessary repartitioning.\nAfter\ncoalesce\n, the number of (reduced) tasks will be kept unless another shuffle happens.\nspark.sql.streaming.stateStore.providerClass\n: To read the previous state of the query properly, the class of state store provider should be unchanged.\nspark.sql.streaming.multipleWatermarkPolicy\n: Modification of this would lead inconsistent watermark value when query contains multiple watermarks, hence the policy should be unchanged.\nRelated Resources\nFurther Reading\nSee and run the\nPython\n/\nScala\n/\nJava\n/\nR\nexamples.\nInstructions\non how to run Spark examples\nRead about integrating with Kafka in the\nStructured Streaming Kafka Integration Guide\nRead more details about using DataFrames/Datasets in the\nSpark SQL Programming Guide\nThird-party Blog Posts\nReal-time Streaming ETL with Structured Streaming in Apache Spark 2.1 (Databricks Blog)\nReal-Time End-to-End Integration with Apache Kafka in Apache Spark’s Structured Streaming (Databricks Blog)\nEvent-time Aggregation and Watermarking in Apache Spark’s Structured Streaming (Databricks Blog)\nTalks\nSpark Summit Europe 2017\nEasy, Scalable, Fault-tolerant Stream Processing with Structured Streaming in Apache Spark -\nPart 1 slides/video\n,\nPart 2 slides/video\nDeep Dive into Stateful Stream Processing in Structured Streaming -\nslides/video\nSpark Summit 2016\nA Deep Dive into Structured Streaming -\nslides/video\nMigration Guide\nThe migration guide is now archived\non this page\n."}
{"url": "https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nPySpark Usage Guide for Pandas with Apache Arrow\nThe Arrow usage guide is now archived on\nthis page\n."}
{"url": "https://spark.apache.org/docs/latest/mllib-guide.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nMLlib: RDD-based API\nThis page documents sections of the MLlib guide for the RDD-based API (the\nspark.mllib\npackage).\nPlease see the\nMLlib Main Guide\nfor the DataFrame-based API (the\nspark.ml\npackage),\nwhich is now the primary API for MLlib.\nData types\nBasic statistics\nsummary statistics\ncorrelations\nstratified sampling\nhypothesis testing\nstreaming significance testing\nrandom data generation\nClassification and regression\nlinear models (SVMs, logistic regression, linear regression)\nnaive Bayes\ndecision trees\nensembles of trees (Random Forests and Gradient-Boosted Trees)\nisotonic regression\nCollaborative filtering\nalternating least squares (ALS)\nClustering\nk-means\nGaussian mixture\npower iteration clustering (PIC)\nlatent Dirichlet allocation (LDA)\nbisecting k-means\nstreaming k-means\nDimensionality reduction\nsingular value decomposition (SVD)\nprincipal component analysis (PCA)\nFeature extraction and transformation\nFrequent pattern mining\nFP-growth\nassociation rules\nPrefixSpan\nEvaluation metrics\nPMML model export\nOptimization (developer)\nstochastic gradient descent\nlimited-memory BFGS (L-BFGS)"}
{"url": "https://spark.apache.org/docs/latest/mllib-pmml-model-export.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nPMML model export - RDD-based API\nspark.mllib supported models\nExamples\nspark.mllib supported models\nspark.mllib\nsupports model export to Predictive Model Markup Language (\nPMML\n).\nThe table below outlines the\nspark.mllib\nmodels that can be exported to PMML and their equivalent PMML model.\nspark.mllib model\nPMML model\nKMeansModel\nClusteringModel\nLinearRegressionModel\nRegressionModel (functionName=\"regression\")\nRidgeRegressionModel\nRegressionModel (functionName=\"regression\")\nLassoModel\nRegressionModel (functionName=\"regression\")\nSVMModel\nRegressionModel (functionName=\"classification\" normalizationMethod=\"none\")\nBinary LogisticRegressionModel\nRegressionModel (functionName=\"classification\" normalizationMethod=\"logit\")\nExamples\nTo export a supported\nmodel\n(see table above) to PMML, simply call\nmodel.toPMML\n.\nAs well as exporting the PMML model to a String (\nmodel.toPMML\nas in the example above), you can export the PMML model to other formats.\nRefer to the\nKMeans\nScala docs\nand\nVectors\nScala docs\nfor details on the API.\nHere a complete example of building a KMeansModel and print it out in PMML format:\nimport\norg.apache.spark.mllib.clustering.KMeans\nimport\norg.apache.spark.mllib.linalg.Vectors\n// Load and parse the data\nval\ndata\n=\nsc\n.\ntextFile\n(\n\"data/mllib/kmeans_data.txt\"\n)\nval\nparsedData\n=\ndata\n.\nmap\n(\ns\n=>\nVectors\n.\ndense\n(\ns\n.\nsplit\n(\n' '\n).\nmap\n(\n_\n.\ntoDouble\n))).\ncache\n()\n// Cluster the data into two classes using KMeans\nval\nnumClusters\n=\n2\nval\nnumIterations\n=\n20\nval\nclusters\n=\nKMeans\n.\ntrain\n(\nparsedData\n,\nnumClusters\n,\nnumIterations\n)\n// Export to PMML to a String in PMML format\nprintln\n(\ns\n\"PMML Model:\\n ${clusters.toPMML()}\"\n)\n// Export the model to a local file in PMML format\nclusters\n.\ntoPMML\n(\n\"/tmp/kmeans.xml\"\n)\n// Export the model to a directory on a distributed file system in PMML format\nclusters\n.\ntoPMML\n(\nsc\n,\n\"/tmp/kmeans\"\n)\n// Export the model to the OutputStream in PMML format\nclusters\n.\ntoPMML\n(\nSystem\n.\nout\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\" in the Spark repo.\nFor unsupported models, either you will not find a\n.toPMML\nmethod or an\nIllegalArgumentException\nwill be thrown."}
{"url": "https://spark.apache.org/docs/latest/sql-distributed-sql-engine.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nPerformance Tuning\nDistributed SQL Engine\nRunning the Thrift JDBC/ODBC server\nRunning the Spark SQL CLI\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nDistributed SQL Engine\nRunning the Thrift JDBC/ODBC server\nRunning the Spark SQL CLI\nSpark SQL can also act as a distributed query engine using its JDBC/ODBC or command-line interface.\nIn this mode, end-users or applications can interact with Spark SQL directly to run SQL queries,\nwithout the need to write any code.\nRunning the Thrift JDBC/ODBC server\nThe Thrift JDBC/ODBC server implemented here corresponds to the\nHiveServer2\nin built-in Hive. You can test the JDBC server with the beeline script that comes with either Spark or compatible Hive.\nTo start the JDBC/ODBC server, run the following in the Spark directory:\n./sbin/start-thriftserver.sh\nThis script accepts all\nbin/spark-submit\ncommand line options, plus a\n--hiveconf\noption to\nspecify Hive properties. You may run\n./sbin/start-thriftserver.sh --help\nfor a complete list of\nall available options. By default, the server listens on localhost:10000. You may override this\nbehaviour via either environment variables, i.e.:\nexport\nHIVE_SERVER2_THRIFT_PORT\n=\n<listening-port>\nexport\nHIVE_SERVER2_THRIFT_BIND_HOST\n=\n<listening-host>\n./sbin/start-thriftserver.sh\n\\\n--master\n<master-uri>\n\\\n...\nor system properties:\n./sbin/start-thriftserver.sh\n\\\n--hiveconf\nhive.server2.thrift.port\n=\n<listening-port>\n\\\n--hiveconf\nhive.server2.thrift.bind.host\n=\n<listening-host>\n\\\n--master\n<master-uri>\n  ...\nNow you can use beeline to test the Thrift JDBC/ODBC server:\n./bin/beeline\nConnect to the JDBC/ODBC server in beeline with:\nbeeline> !connect jdbc:hive2://localhost:10000\nBeeline will ask you for a username and password. In non-secure mode, simply enter the username on\nyour machine and a blank password. For secure mode, please follow the instructions given in the\nbeeline documentation\n.\nConfiguration of Hive is done by placing your\nhive-site.xml\n,\ncore-site.xml\nand\nhdfs-site.xml\nfiles in\nconf/\n.\nYou may also use the beeline script that comes with Hive.\nThrift JDBC server also supports sending thrift RPC messages over HTTP transport.\nUse the following setting to enable HTTP mode as system property or in\nhive-site.xml\nfile in\nconf/\n:\nhive.server2.transport.mode - Set this to value: http\nhive.server2.thrift.http.port - HTTP port number to listen on; default is 10001\nhive.server2.http.endpoint - HTTP endpoint; default is cliservice\nTo test, use beeline to connect to the JDBC/ODBC server in http mode with:\nbeeline> !connect jdbc:hive2://<host>:<port>/<database>;transportMode=http;httpPath=<http_endpoint>\nIf you closed a session and do CTAS, you must set\nfs.%s.impl.disable.cache\nto true in\nhive-site.xml\n.\nSee more details in\n[SPARK-21067]\n.\nRunning the Spark SQL CLI\nTo use the Spark SQL command line interface (CLI) from the shell:\n./bin/spark-sql\nFor details, please refer to\nSpark SQL CLI"}
{"url": "https://spark.apache.org/docs/latest/mllib-classification-regression.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nLinear models (SVMs, logistic regression, linear regression)\nNaive Bayes\ndecision trees\nensembles of trees (Random Forests and Gradient-Boosted Trees)\nisotonic regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nClassification and Regression - RDD-based API\nThe\nspark.mllib\npackage supports various methods for\nbinary classification\n,\nmulticlass\nclassification\n, and\nregression analysis\n. The table below outlines\nthe supported algorithms for each type of problem.\nProblem Type\nSupported Methods\nBinary Classification\nlinear SVMs, logistic regression, decision trees, random forests, gradient-boosted trees, naive Bayes\nMulticlass Classification\nlogistic regression, decision trees, random forests, naive Bayes\nRegression\nlinear least squares, Lasso, ridge regression, decision trees, random forests, gradient-boosted trees, isotonic regression\nMore details for these methods can be found here:\nLinear models\nclassification (SVMs, logistic regression)\nlinear regression (least squares, Lasso, ridge)\nDecision trees\nEnsembles of decision trees\nrandom forests\ngradient-boosted trees\nNaive Bayes\nIsotonic regression"}
{"url": "https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nalternating least squares (ALS)\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nCollaborative Filtering - RDD-based API\nCollaborative filtering\nExplicit vs. implicit feedback\nScaling of the regularization parameter\nExamples\nTutorial\nCollaborative filtering\nCollaborative filtering\nis commonly used for recommender systems.  These techniques aim to fill in the\nmissing entries of a user-item association matrix.\nspark.mllib\ncurrently supports\nmodel-based collaborative filtering, in which users and products are described\nby a small set of latent factors that can be used to predict missing entries.\nspark.mllib\nuses the\nalternating least squares\n(ALS)\nalgorithm to learn these latent factors. The implementation in\nspark.mllib\nhas the\nfollowing parameters:\nnumBlocks\nis the number of blocks used to parallelize computation (set to -1 to auto-configure).\nrank\nis the number of features to use (also referred to as the number of latent factors).\niterations\nis the number of iterations of ALS to run. ALS typically converges to a reasonable\nsolution in 20 iterations or less.\nlambda\nspecifies the regularization parameter in ALS.\nimplicitPrefs\nspecifies whether to use the\nexplicit feedback\nALS variant or one adapted for\nimplicit feedback\ndata.\nalpha\nis a parameter applicable to the implicit feedback variant of ALS that governs the\nbaseline\nconfidence in preference observations.\nExplicit vs. implicit feedback\nThe standard approach to matrix factorization-based collaborative filtering treats\nthe entries in the user-item matrix as\nexplicit\npreferences given by the user to the item,\nfor example, users giving ratings to movies.\nIt is common in many real-world use cases to only have access to\nimplicit feedback\n(e.g. views,\nclicks, purchases, likes, shares etc.). The approach used in\nspark.mllib\nto deal with such data is taken\nfrom\nCollaborative Filtering for Implicit Feedback Datasets\n.\nEssentially, instead of trying to model the matrix of ratings directly, this approach treats the data\nas numbers representing the\nstrength\nin observations of user actions (such as the number of clicks,\nor the cumulative duration someone spent viewing a movie). Those numbers are then related to the level of\nconfidence in observed user preferences, rather than explicit ratings given to items. The model\nthen tries to find latent factors that can be used to predict the expected preference of a user for\nan item.\nScaling of the regularization parameter\nSince v1.1, we scale the regularization parameter\nlambda\nin solving each least squares problem by\nthe number of ratings the user generated in updating user factors,\nor the number of ratings the product received in updating product factors.\nThis approach is named “ALS-WR” and discussed in the paper\n“\nLarge-Scale Parallel Collaborative Filtering for the Netflix Prize\n”.\nIt makes\nlambda\nless dependent on the scale of the dataset, so we can apply the\nbest parameter learned from a sampled subset to the full dataset and expect similar performance.\nExamples\nIn the following example we load rating data. Each row consists of a user, a product and a rating.\nWe use the default ALS.train() method which assumes ratings are explicit. We evaluate the\nrecommendation by measuring the Mean Squared Error of rating prediction.\nRefer to the\nALS\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.recommendation\nimport\nALS\n,\nMatrixFactorizationModel\n,\nRating\n# Load and parse the data\ndata\n=\nsc\n.\ntextFile\n(\n\"\ndata/mllib/als/test.data\n\"\n)\nratings\n=\ndata\n.\nmap\n(\nlambda\nl\n:\nl\n.\nsplit\n(\n'\n,\n'\n))\n\\\n.\nmap\n(\nlambda\nl\n:\nRating\n(\nint\n(\nl\n[\n0\n]),\nint\n(\nl\n[\n1\n]),\nfloat\n(\nl\n[\n2\n])))\n# Build the recommendation model using Alternating Least Squares\nrank\n=\n10\nnumIterations\n=\n10\nmodel\n=\nALS\n.\ntrain\n(\nratings\n,\nrank\n,\nnumIterations\n)\n# Evaluate the model on training data\ntestdata\n=\nratings\n.\nmap\n(\nlambda\np\n:\n(\np\n[\n0\n],\np\n[\n1\n]))\npredictions\n=\nmodel\n.\npredictAll\n(\ntestdata\n).\nmap\n(\nlambda\nr\n:\n((\nr\n[\n0\n],\nr\n[\n1\n]),\nr\n[\n2\n]))\nratesAndPreds\n=\nratings\n.\nmap\n(\nlambda\nr\n:\n((\nr\n[\n0\n],\nr\n[\n1\n]),\nr\n[\n2\n])).\njoin\n(\npredictions\n)\nMSE\n=\nratesAndPreds\n.\nmap\n(\nlambda\nr\n:\n(\nr\n[\n1\n][\n0\n]\n-\nr\n[\n1\n][\n1\n])\n**\n2\n).\nmean\n()\nprint\n(\n\"\nMean Squared Error =\n\"\n+\nstr\n(\nMSE\n))\n# Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"\ntarget/tmp/myCollaborativeFilter\n\"\n)\nsameModel\n=\nMatrixFactorizationModel\n.\nload\n(\nsc\n,\n\"\ntarget/tmp/myCollaborativeFilter\n\"\n)\nFind full example code at \"examples/src/main/python/mllib/recommendation_example.py\" in the Spark repo.\nIf the rating matrix is derived from other source of information (i.e. it is inferred from other\nsignals), you can use the trainImplicit method to get better results.\n# Build the recommendation model using Alternating Least Squares based on implicit ratings\nmodel\n=\nALS\n.\ntrainImplicit\n(\nratings\n,\nrank\n,\nnumIterations\n,\nalpha\n=\n0.01\n)\nIn the following example, we load rating data. Each row consists of a user, a product and a rating.\nWe use the default\nALS.train()\nmethod which assumes ratings are explicit. We evaluate the\nrecommendation model by measuring the Mean Squared Error of rating prediction.\nRefer to the\nALS\nScala docs\nfor more details on the API.\nimport\norg.apache.spark.mllib.recommendation.ALS\nimport\norg.apache.spark.mllib.recommendation.MatrixFactorizationModel\nimport\norg.apache.spark.mllib.recommendation.Rating\n// Load and parse the data\nval\ndata\n=\nsc\n.\ntextFile\n(\n\"data/mllib/als/test.data\"\n)\nval\nratings\n=\ndata\n.\nmap\n(\n_\n.\nsplit\n(\n','\n)\nmatch\n{\ncase\nArray\n(\nuser\n,\nitem\n,\nrate\n)\n=>\nRating\n(\nuser\n.\ntoInt\n,\nitem\n.\ntoInt\n,\nrate\n.\ntoDouble\n)\n})\n// Build the recommendation model using ALS\nval\nrank\n=\n10\nval\nnumIterations\n=\n10\nval\nmodel\n=\nALS\n.\ntrain\n(\nratings\n,\nrank\n,\nnumIterations\n,\n0.01\n)\n// Evaluate the model on rating data\nval\nusersProducts\n=\nratings\n.\nmap\n{\ncase\nRating\n(\nuser\n,\nproduct\n,\nrate\n)\n=>\n(\nuser\n,\nproduct\n)\n}\nval\npredictions\n=\nmodel\n.\npredict\n(\nusersProducts\n).\nmap\n{\ncase\nRating\n(\nuser\n,\nproduct\n,\nrate\n)\n=>\n((\nuser\n,\nproduct\n),\nrate\n)\n}\nval\nratesAndPreds\n=\nratings\n.\nmap\n{\ncase\nRating\n(\nuser\n,\nproduct\n,\nrate\n)\n=>\n((\nuser\n,\nproduct\n),\nrate\n)\n}.\njoin\n(\npredictions\n)\nval\nMSE\n=\nratesAndPreds\n.\nmap\n{\ncase\n((\nuser\n,\nproduct\n),\n(\nr1\n,\nr2\n))\n=>\nval\nerr\n=\n(\nr1\n-\nr2\n)\nerr\n*\nerr\n}.\nmean\n()\nprintln\n(\ns\n\"Mean Squared Error = $MSE\"\n)\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/myCollaborativeFilter\"\n)\nval\nsameModel\n=\nMatrixFactorizationModel\n.\nload\n(\nsc\n,\n\"target/tmp/myCollaborativeFilter\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\" in the Spark repo.\nIf the rating matrix is derived from another source of information (i.e. it is inferred from\nother signals), you can use the\ntrainImplicit\nmethod to get better results.\nval\nalpha\n=\n0.01\nval\nlambda\n=\n0.01\nval\nmodel\n=\nALS\n.\ntrainImplicit\n(\nratings\n,\nrank\n,\nnumIterations\n,\nlambda\n,\nalpha\n)\nAll of MLlib’s methods use Java-friendly types, so you can import and call them there the same\nway you do in Scala. The only caveat is that the methods take Scala RDD objects, while the\nSpark Java API uses a separate\nJavaRDD\nclass. You can convert a Java RDD to a Scala one by\ncalling\n.rdd()\non your\nJavaRDD\nobject. A self-contained application example\nthat is equivalent to the provided example in Scala is given below:\nRefer to the\nALS\nJava docs\nfor more details on the API.\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.api.java.*\n;\nimport\norg.apache.spark.mllib.recommendation.ALS\n;\nimport\norg.apache.spark.mllib.recommendation.MatrixFactorizationModel\n;\nimport\norg.apache.spark.mllib.recommendation.Rating\n;\nimport\norg.apache.spark.SparkConf\n;\nSparkConf\nconf\n=\nnew\nSparkConf\n().\nsetAppName\n(\n\"Java Collaborative Filtering Example\"\n);\nJavaSparkContext\njsc\n=\nnew\nJavaSparkContext\n(\nconf\n);\n// Load and parse the data\nString\npath\n=\n\"data/mllib/als/test.data\"\n;\nJavaRDD\n<\nString\n>\ndata\n=\njsc\n.\ntextFile\n(\npath\n);\nJavaRDD\n<\nRating\n>\nratings\n=\ndata\n.\nmap\n(\ns\n->\n{\nString\n[]\nsarray\n=\ns\n.\nsplit\n(\n\",\"\n);\nreturn\nnew\nRating\n(\nInteger\n.\nparseInt\n(\nsarray\n[\n0\n]),\nInteger\n.\nparseInt\n(\nsarray\n[\n1\n]),\nDouble\n.\nparseDouble\n(\nsarray\n[\n2\n]));\n});\n// Build the recommendation model using ALS\nint\nrank\n=\n10\n;\nint\nnumIterations\n=\n10\n;\nMatrixFactorizationModel\nmodel\n=\nALS\n.\ntrain\n(\nJavaRDD\n.\ntoRDD\n(\nratings\n),\nrank\n,\nnumIterations\n,\n0.01\n);\n// Evaluate the model on rating data\nJavaRDD\n<\nTuple2\n<\nObject\n,\nObject\n>>\nuserProducts\n=\nratings\n.\nmap\n(\nr\n->\nnew\nTuple2\n<>(\nr\n.\nuser\n(),\nr\n.\nproduct\n()));\nJavaPairRDD\n<\nTuple2\n<\nInteger\n,\nInteger\n>,\nDouble\n>\npredictions\n=\nJavaPairRDD\n.\nfromJavaRDD\n(\nmodel\n.\npredict\n(\nJavaRDD\n.\ntoRDD\n(\nuserProducts\n)).\ntoJavaRDD\n()\n.\nmap\n(\nr\n->\nnew\nTuple2\n<>(\nnew\nTuple2\n<>(\nr\n.\nuser\n(),\nr\n.\nproduct\n()),\nr\n.\nrating\n()))\n);\nJavaRDD\n<\nTuple2\n<\nDouble\n,\nDouble\n>>\nratesAndPreds\n=\nJavaPairRDD\n.\nfromJavaRDD\n(\nratings\n.\nmap\n(\nr\n->\nnew\nTuple2\n<>(\nnew\nTuple2\n<>(\nr\n.\nuser\n(),\nr\n.\nproduct\n()),\nr\n.\nrating\n())))\n.\njoin\n(\npredictions\n).\nvalues\n();\ndouble\nMSE\n=\nratesAndPreds\n.\nmapToDouble\n(\npair\n->\n{\ndouble\nerr\n=\npair\n.\n_1\n()\n-\npair\n.\n_2\n();\nreturn\nerr\n*\nerr\n;\n}).\nmean\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Mean Squared Error = \"\n+\nMSE\n);\n// Save and load model\nmodel\n.\nsave\n(\njsc\n.\nsc\n(),\n\"target/tmp/myCollaborativeFilter\"\n);\nMatrixFactorizationModel\nsameModel\n=\nMatrixFactorizationModel\n.\nload\n(\njsc\n.\nsc\n(),\n\"target/tmp/myCollaborativeFilter\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\" in the Spark repo.\nIn order to run the above application, follow the instructions\nprovided in the\nSelf-Contained Applications\nsection of the Spark\nQuick Start guide. Be sure to also include\nspark-mllib\nto your build file as\na dependency.\nTutorial\nThe\ntraining exercises\nfrom the Spark Summit 2014 include a hands-on tutorial for\npersonalized movie recommendation with\nspark.mllib\n."}
{"url": "https://spark.apache.org/docs/latest/mllib-frequent-pattern-mining.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nFP-growth\nassociation rules\nPrefixSpan\nEvaluation metrics\nPMML model export\nOptimization (developer)\nFrequent Pattern Mining - RDD-based API\nMining frequent items, itemsets, subsequences, or other substructures is usually among the\nfirst steps to analyze a large-scale dataset, which has been an active research topic in\ndata mining for years.\nWe refer users to Wikipedia’s\nassociation rule learning\nfor more information.\nspark.mllib\nprovides a parallel implementation of FP-growth,\na popular algorithm to mining frequent itemsets.\nFP-growth\nThe FP-growth algorithm is described in the paper\nHan et al., Mining frequent patterns without candidate generation\n,\nwhere “FP” stands for frequent pattern.\nGiven a dataset of transactions, the first step of FP-growth is to calculate item frequencies and identify frequent items.\nDifferent from\nApriori-like\nalgorithms designed for the same purpose,\nthe second step of FP-growth uses a suffix tree (FP-tree) structure to encode transactions without generating candidate sets\nexplicitly, which are usually expensive to generate.\nAfter the second step, the frequent itemsets can be extracted from the FP-tree.\nIn\nspark.mllib\n, we implemented a parallel version of FP-growth called PFP,\nas described in\nLi et al., PFP: Parallel FP-growth for query recommendation\n.\nPFP distributes the work of growing FP-trees based on the suffixes of transactions,\nand hence more scalable than a single-machine implementation.\nWe refer users to the papers for more details.\nspark.mllib\n’s FP-growth implementation takes the following (hyper-)parameters:\nminSupport\n: the minimum support for an itemset to be identified as frequent.\nFor example, if an item appears 3 out of 5 transactions, it has a support of 3/5=0.6.\nnumPartitions\n: the number of partitions used to distribute the work.\nExamples\nFPGrowth\nimplements the\nFP-growth algorithm.\nIt takes an\nRDD\nof transactions, where each transaction is a\nList\nof items of a generic type.\nCalling\nFPGrowth.train\nwith transactions returns an\nFPGrowthModel\nthat stores the frequent itemsets with their frequencies.\nRefer to the\nFPGrowth\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.fpm\nimport\nFPGrowth\ndata\n=\nsc\n.\ntextFile\n(\n\"\ndata/mllib/sample_fpgrowth.txt\n\"\n)\ntransactions\n=\ndata\n.\nmap\n(\nlambda\nline\n:\nline\n.\nstrip\n().\nsplit\n(\n'\n'\n))\nmodel\n=\nFPGrowth\n.\ntrain\n(\ntransactions\n,\nminSupport\n=\n0.2\n,\nnumPartitions\n=\n10\n)\nresult\n=\nmodel\n.\nfreqItemsets\n().\ncollect\n()\nfor\nfi\nin\nresult\n:\nprint\n(\nfi\n)\nFind full example code at \"examples/src/main/python/mllib/fpgrowth_example.py\" in the Spark repo.\nFPGrowth\nimplements the\nFP-growth algorithm.\nIt takes an\nRDD\nof transactions, where each transaction is an\nArray\nof items of a generic type.\nCalling\nFPGrowth.run\nwith transactions returns an\nFPGrowthModel\nthat stores the frequent itemsets with their frequencies.  The following\nexample illustrates how to mine frequent itemsets and association rules\n(see\nAssociation\nRules\nfor\ndetails) from\ntransactions\n.\nRefer to the\nFPGrowth\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.fpm.FPGrowth\nimport\norg.apache.spark.rdd.RDD\nval\ndata\n=\nsc\n.\ntextFile\n(\n\"data/mllib/sample_fpgrowth.txt\"\n)\nval\ntransactions\n:\nRDD\n[\nArray\n[\nString\n]]\n=\ndata\n.\nmap\n(\ns\n=>\ns\n.\ntrim\n.\nsplit\n(\n' '\n))\nval\nfpg\n=\nnew\nFPGrowth\n()\n.\nsetMinSupport\n(\n0.2\n)\n.\nsetNumPartitions\n(\n10\n)\nval\nmodel\n=\nfpg\n.\nrun\n(\ntransactions\n)\nmodel\n.\nfreqItemsets\n.\ncollect\n().\nforeach\n{\nitemset\n=>\nprintln\n(\ns\n\"${itemset.items.mkString(\"\n[\n\"\n,\n\"\n,\n\"\n,\n\"\n]\n\")},${itemset.freq}\"\n)\n}\nval\nminConfidence\n=\n0.8\nmodel\n.\ngenerateAssociationRules\n(\nminConfidence\n).\ncollect\n().\nforeach\n{\nrule\n=>\nprintln\n(\ns\n\"${rule.antecedent.mkString(\"\n[\n\"\n,\n\"\n,\n\"\n,\n\"\n]\n\")}=> \"\n+\ns\n\"${rule.consequent .mkString(\"\n[\n\"\n,\n\"\n,\n\"\n,\n\"\n]\n\")},${rule.confidence}\"\n)\n}\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\" in the Spark repo.\nFPGrowth\nimplements the\nFP-growth algorithm.\nIt takes a\nJavaRDD\nof transactions, where each transaction is an\nIterable\nof items of a generic type.\nCalling\nFPGrowth.run\nwith transactions returns an\nFPGrowthModel\nthat stores the frequent itemsets with their frequencies.  The following\nexample illustrates how to mine frequent itemsets and association rules\n(see\nAssociation\nRules\nfor\ndetails) from\ntransactions\n.\nRefer to the\nFPGrowth\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.api.java.JavaSparkContext\n;\nimport\norg.apache.spark.mllib.fpm.AssociationRules\n;\nimport\norg.apache.spark.mllib.fpm.FPGrowth\n;\nimport\norg.apache.spark.mllib.fpm.FPGrowthModel\n;\nJavaRDD\n<\nString\n>\ndata\n=\nsc\n.\ntextFile\n(\n\"data/mllib/sample_fpgrowth.txt\"\n);\nJavaRDD\n<\nList\n<\nString\n>>\ntransactions\n=\ndata\n.\nmap\n(\nline\n->\nArrays\n.\nasList\n(\nline\n.\nsplit\n(\n\" \"\n)));\nFPGrowth\nfpg\n=\nnew\nFPGrowth\n()\n.\nsetMinSupport\n(\n0.2\n)\n.\nsetNumPartitions\n(\n10\n);\nFPGrowthModel\n<\nString\n>\nmodel\n=\nfpg\n.\nrun\n(\ntransactions\n);\nfor\n(\nFPGrowth\n.\nFreqItemset\n<\nString\n>\nitemset:\nmodel\n.\nfreqItemsets\n().\ntoJavaRDD\n().\ncollect\n())\n{\nSystem\n.\nout\n.\nprintln\n(\n\"[\"\n+\nitemset\n.\njavaItems\n()\n+\n\"], \"\n+\nitemset\n.\nfreq\n());\n}\ndouble\nminConfidence\n=\n0.8\n;\nfor\n(\nAssociationRules\n.\nRule\n<\nString\n>\nrule\n:\nmodel\n.\ngenerateAssociationRules\n(\nminConfidence\n).\ntoJavaRDD\n().\ncollect\n())\n{\nSystem\n.\nout\n.\nprintln\n(\nrule\n.\njavaAntecedent\n()\n+\n\" => \"\n+\nrule\n.\njavaConsequent\n()\n+\n\", \"\n+\nrule\n.\nconfidence\n());\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\" in the Spark repo.\nAssociation Rules\nAssociationRules\nimplements a parallel rule generation algorithm for constructing rules\nthat have a single item as the consequent.\nRefer to the\nAssociationRules\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.fpm.AssociationRules\nimport\norg.apache.spark.mllib.fpm.FPGrowth.FreqItemset\nval\nfreqItemsets\n=\nsc\n.\nparallelize\n(\nSeq\n(\nnew\nFreqItemset\n(\nArray\n(\n\"a\"\n),\n15L\n),\nnew\nFreqItemset\n(\nArray\n(\n\"b\"\n),\n35L\n),\nnew\nFreqItemset\n(\nArray\n(\n\"a\"\n,\n\"b\"\n),\n12L\n)\n))\nval\nar\n=\nnew\nAssociationRules\n()\n.\nsetMinConfidence\n(\n0.8\n)\nval\nresults\n=\nar\n.\nrun\n(\nfreqItemsets\n)\nresults\n.\ncollect\n().\nforeach\n{\nrule\n=>\nprintln\n(\ns\n\"[${rule.antecedent.mkString(\"\n,\n\")}=>${rule.consequent.mkString(\"\n,\n\")} ]\"\n+\ns\n\" ${rule.confidence}\"\n)\n}\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\" in the Spark repo.\nAssociationRules\nimplements a parallel rule generation algorithm for constructing rules\nthat have a single item as the consequent.\nRefer to the\nAssociationRules\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.api.java.JavaSparkContext\n;\nimport\norg.apache.spark.mllib.fpm.AssociationRules\n;\nimport\norg.apache.spark.mllib.fpm.FPGrowth\n;\nimport\norg.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n;\nJavaRDD\n<\nFPGrowth\n.\nFreqItemset\n<\nString\n>>\nfreqItemsets\n=\nsc\n.\nparallelize\n(\nArrays\n.\nasList\n(\nnew\nFreqItemset\n<>(\nnew\nString\n[]\n{\n\"a\"\n},\n15L\n),\nnew\nFreqItemset\n<>(\nnew\nString\n[]\n{\n\"b\"\n},\n35L\n),\nnew\nFreqItemset\n<>(\nnew\nString\n[]\n{\n\"a\"\n,\n\"b\"\n},\n12L\n)\n));\nAssociationRules\narules\n=\nnew\nAssociationRules\n()\n.\nsetMinConfidence\n(\n0.8\n);\nJavaRDD\n<\nAssociationRules\n.\nRule\n<\nString\n>>\nresults\n=\narules\n.\nrun\n(\nfreqItemsets\n);\nfor\n(\nAssociationRules\n.\nRule\n<\nString\n>\nrule\n:\nresults\n.\ncollect\n())\n{\nSystem\n.\nout\n.\nprintln\n(\nrule\n.\njavaAntecedent\n()\n+\n\" => \"\n+\nrule\n.\njavaConsequent\n()\n+\n\", \"\n+\nrule\n.\nconfidence\n());\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\" in the Spark repo.\nPrefixSpan\nPrefixSpan is a sequential pattern mining algorithm described in\nPei et al., Mining Sequential Patterns by Pattern-Growth: The\nPrefixSpan Approach\n. We refer\nthe reader to the referenced paper for formalizing the sequential\npattern mining problem.\nspark.mllib\n’s PrefixSpan implementation takes the following parameters:\nminSupport\n: the minimum support required to be considered a frequent\nsequential pattern.\nmaxPatternLength\n: the maximum length of a frequent sequential\npattern. Any frequent pattern exceeding this length will not be\nincluded in the results.\nmaxLocalProjDBSize\n: the maximum number of items allowed in a\nprefix-projected database before local iterative processing of the\nprojected database begins. This parameter should be tuned with respect\nto the size of your executors.\nExamples\nThe following example illustrates PrefixSpan running on the sequences\n(using same notation as Pei et al):\n<(12)3>\n  <1(32)(12)>\n  <(12)5>\n  <6>\nPrefixSpan\nimplements the\nPrefixSpan algorithm.\nCalling\nPrefixSpan.run\nreturns a\nPrefixSpanModel\nthat stores the frequent sequences with their frequencies.\nRefer to the\nPrefixSpan\nScala docs\nand\nPrefixSpanModel\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.fpm.PrefixSpan\nval\nsequences\n=\nsc\n.\nparallelize\n(\nSeq\n(\nArray\n(\nArray\n(\n1\n,\n2\n),\nArray\n(\n3\n)),\nArray\n(\nArray\n(\n1\n),\nArray\n(\n3\n,\n2\n),\nArray\n(\n1\n,\n2\n)),\nArray\n(\nArray\n(\n1\n,\n2\n),\nArray\n(\n5\n)),\nArray\n(\nArray\n(\n6\n))\n),\n2\n).\ncache\n()\nval\nprefixSpan\n=\nnew\nPrefixSpan\n()\n.\nsetMinSupport\n(\n0.5\n)\n.\nsetMaxPatternLength\n(\n5\n)\nval\nmodel\n=\nprefixSpan\n.\nrun\n(\nsequences\n)\nmodel\n.\nfreqSequences\n.\ncollect\n().\nforeach\n{\nfreqSequence\n=>\nprintln\n(\ns\n\"${freqSequence.sequence.map(_.mkString(\"\n[\n\"\n,\n\"\n,\n\"\n,\n\"\n]\n\")).mkString(\"\n[\n\"\n,\n\"\n,\n\"\n,\n\"\n]\n\")},\"\n+\ns\n\" ${freqSequence.freq}\"\n)\n}\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\" in the Spark repo.\nPrefixSpan\nimplements the\nPrefixSpan algorithm.\nCalling\nPrefixSpan.run\nreturns a\nPrefixSpanModel\nthat stores the frequent sequences with their frequencies.\nRefer to the\nPrefixSpan\nJava docs\nand\nPrefixSpanModel\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.mllib.fpm.PrefixSpan\n;\nimport\norg.apache.spark.mllib.fpm.PrefixSpanModel\n;\nJavaRDD\n<\nList\n<\nList\n<\nInteger\n>>>\nsequences\n=\nsc\n.\nparallelize\n(\nArrays\n.\nasList\n(\nArrays\n.\nasList\n(\nArrays\n.\nasList\n(\n1\n,\n2\n),\nArrays\n.\nasList\n(\n3\n)),\nArrays\n.\nasList\n(\nArrays\n.\nasList\n(\n1\n),\nArrays\n.\nasList\n(\n3\n,\n2\n),\nArrays\n.\nasList\n(\n1\n,\n2\n)),\nArrays\n.\nasList\n(\nArrays\n.\nasList\n(\n1\n,\n2\n),\nArrays\n.\nasList\n(\n5\n)),\nArrays\n.\nasList\n(\nArrays\n.\nasList\n(\n6\n))\n),\n2\n);\nPrefixSpan\nprefixSpan\n=\nnew\nPrefixSpan\n()\n.\nsetMinSupport\n(\n0.5\n)\n.\nsetMaxPatternLength\n(\n5\n);\nPrefixSpanModel\n<\nInteger\n>\nmodel\n=\nprefixSpan\n.\nrun\n(\nsequences\n);\nfor\n(\nPrefixSpan\n.\nFreqSequence\n<\nInteger\n>\nfreqSeq:\nmodel\n.\nfreqSequences\n().\ntoJavaRDD\n().\ncollect\n())\n{\nSystem\n.\nout\n.\nprintln\n(\nfreqSeq\n.\njavaSequence\n()\n+\n\", \"\n+\nfreqSeq\n.\nfreq\n());\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\" in the Spark repo."}
{"url": "https://spark.apache.org/docs/latest/mllib-optimization.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nstochastic gradient descent\nlimited-memory BFGS (L-BFGS)\nOptimization - RDD-based API\nMathematical description\nGradient descent\nStochastic gradient descent (SGD)\nUpdate schemes for distributed SGD\nLimited-memory BFGS (L-BFGS)\nChoosing an Optimization Method\nImplementation in MLlib\nGradient descent and stochastic gradient descent\nL-BFGS\nDeveloper’s notes\n\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}} \n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\wv}{\\mathbf{w}}\n\\newcommand{\\av}{\\mathbf{\\alpha}}\n\\newcommand{\\bv}{\\mathbf{b}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\id}{\\mathbf{I}} \n\\newcommand{\\ind}{\\mathbf{1}} \n\\newcommand{\\0}{\\mathbf{0}} \n\\newcommand{\\unit}{\\mathbf{e}} \n\\newcommand{\\one}{\\mathbf{1}} \n\\newcommand{\\zero}{\\mathbf{0}}\n\\]\nMathematical description\nGradient descent\nThe simplest method to solve optimization problems of the form\n$\\min_{\\wv \\in\\R^d} \\; f(\\wv)$\nis\ngradient descent\n.\nSuch first-order optimization methods (including gradient descent and stochastic variants\nthereof) are well-suited for large-scale and distributed computation.\nGradient descent methods aim to find a local minimum of a function by iteratively taking steps in\nthe direction of steepest descent, which is the negative of the derivative (called the\ngradient\n) of the function at the current point, i.e., at\nthe current parameter value.\nIf the objective function\n$f$\nis not differentiable at all arguments, but still convex, then a\nsub-gradient\nis the natural generalization of the gradient, and assumes the role of the step direction.\nIn any case, computing a gradient or sub-gradient of\n$f$\nis expensive — it requires a full\npass through the complete dataset, in order to compute the contributions from all loss terms.\nStochastic gradient descent (SGD)\nOptimization problems whose objective function\n$f$\nis written as a sum are particularly\nsuitable to be solved using\nstochastic gradient descent (SGD)\n. \nIn our case, for the optimization formulations commonly used in\nsupervised machine learning\n,\n\\begin{equation}\n    f(\\wv) := \n    \\lambda\\, R(\\wv) +\n    \\frac1n \\sum_{i=1}^n L(\\wv;\\x_i,y_i) \n    \\label{eq:regPrimal}\n    \\ .\n\\end{equation}\nthis is especially natural, because the loss is written as an average of the individual losses\ncoming from each datapoint.\nA stochastic subgradient is a randomized choice of a vector, such that in expectation, we obtain\na true subgradient of the original objective function.\nPicking one datapoint\n$i\\in[1..n]$\nuniformly at random, we obtain a stochastic subgradient of\n$\\eqref{eq:regPrimal}$\n, with respect to\n$\\wv$\nas follows:\n\\[\nf'_{\\wv,i} := L'_{\\wv,i} + \\lambda\\, R'_\\wv \\ ,\n\\]\nwhere\n$L'_{\\wv,i} \\in \\R^d$\nis a subgradient of the part of the loss function determined by the\n$i$\n-th datapoint, that is\n$L'_{\\wv,i} \\in \\frac{\\partial}{\\partial \\wv}  L(\\wv;\\x_i,y_i)$\n.\nFurthermore,\n$R'_\\wv$\nis a subgradient of the regularizer\n$R(\\wv)$\n, i.e.\n$R'_\\wv \\in\n\\frac{\\partial}{\\partial \\wv} R(\\wv)$\n. The term\n$R'_\\wv$\ndoes not depend on which random\ndatapoint is picked.\nClearly, in expectation over the random choice of\n$i\\in[1..n]$\n, we have that\n$f'_{\\wv,i}$\nis\na subgradient of the original objective\n$f$\n, meaning that\n$\\E\\left[f'_{\\wv,i}\\right] \\in\n\\frac{\\partial}{\\partial \\wv} f(\\wv)$\n.\nRunning SGD now simply becomes walking in the direction of the negative stochastic subgradient\n$f'_{\\wv,i}$\n, that is\n\\begin{equation}\\label{eq:SGDupdate}\n\\wv^{(t+1)} := \\wv^{(t)}  - \\gamma \\; f'_{\\wv,i} \\ .\n\\end{equation}\nStep-size.\nThe parameter\n$\\gamma$\nis the step-size, which in the default implementation is chosen\ndecreasing with the square root of the iteration counter, i.e.\n$\\gamma := \\frac{s}{\\sqrt{t}}$\nin the\n$t$\n-th iteration, with the input parameter\n$s=$ stepSize\n. Note that selecting the best\nstep-size for SGD methods can often be delicate in practice and is a topic of active research.\nGradients.\nA table of (sub)gradients of the machine learning methods implemented in\nspark.mllib\n, is available in\nthe\nclassification and regression\nsection.\nProximal Updates.\nAs an alternative to just use the subgradient\n$R'(\\wv)$\nof the regularizer in the step\ndirection, an improved update for some cases can be obtained by using the proximal operator\ninstead.\nFor the L1-regularizer, the proximal operator is given by soft thresholding, as implemented in\nL1Updater\n.\nUpdate schemes for distributed SGD\nThe SGD implementation in\nGradientDescent\nuses\na simple (distributed) sampling of the data examples.\nWe recall that the loss part of the optimization problem\n$\\eqref{eq:regPrimal}$\nis\n$\\frac1n \\sum_{i=1}^n L(\\wv;\\x_i,y_i)$\n, and therefore\n$\\frac1n \\sum_{i=1}^n L'_{\\wv,i}$\nwould\nbe the true (sub)gradient.\nSince this would require access to the full data set, the parameter\nminiBatchFraction\nspecifies\nwhich fraction of the full data to use instead.\nThe average of the gradients over this subset, i.e.\n\\[\n\\frac1{|S|} \\sum_{i\\in S} L'_{\\wv,i} \\ ,\n\\]\nis a stochastic gradient. Here\n$S$\nis the sampled subset of size\n$|S|=$ miniBatchFraction\n$\\cdot n$\n.\nIn each iteration, the sampling over the distributed dataset\n(\nRDD\n), as well as the\ncomputation of the sum of the partial results from each worker machine is performed by the\nstandard spark routines.\nIf the fraction of points\nminiBatchFraction\nis set to 1 (default), then the resulting step in\neach iteration is exact (sub)gradient descent. In this case, there is no randomness and no\nvariance in the used step directions.\nOn the other extreme, if\nminiBatchFraction\nis chosen very small, such that only a single point\nis sampled, i.e.\n$|S|=$ miniBatchFraction $\\cdot n = 1$\n, then the algorithm is equivalent to\nstandard SGD. In that case, the step direction depends from the uniformly random sampling of the\npoint.\nLimited-memory BFGS (L-BFGS)\nL-BFGS\nis an optimization \nalgorithm in the family of quasi-Newton methods to solve the optimization problems of the form\n$\\min_{\\wv \\in\\R^d} \\; f(\\wv)$\n. The L-BFGS method approximates the objective function locally as a \nquadratic without evaluating the second partial derivatives of the objective function to construct the \nHessian matrix. The Hessian matrix is approximated by previous gradient evaluations, so there is no \nvertical scalability issue (the number of training features) when computing the Hessian matrix \nexplicitly in Newton’s method. As a result, L-BFGS often achieves more rapid convergence compared with\nother first-order optimization.\nChoosing an Optimization Method\nLinear methods\nuse optimization internally, and some linear methods in\nspark.mllib\nsupport both SGD and L-BFGS.\nDifferent optimization methods can have different convergence guarantees depending on the properties of the objective function, and we cannot cover the literature here.\nIn general, when L-BFGS is available, we recommend using it instead of SGD since L-BFGS tends to converge faster (in fewer iterations).\nImplementation in MLlib\nGradient descent and stochastic gradient descent\nGradient descent methods including stochastic subgradient descent (SGD) as\nincluded as a low-level primitive in\nMLlib\n, upon which various ML algorithms \nare developed, see the\nlinear methods\nsection for example.\nThe SGD class\nGradientDescent\nsets the following parameters:\nGradient\nis a class that computes the stochastic gradient of the function\nbeing optimized, i.e., with respect to a single training example, at the\ncurrent parameter value. MLlib includes gradient classes for common loss\nfunctions, e.g., hinge, logistic, least-squares.  The gradient class takes as\ninput a training example, its label, and the current parameter value.\nUpdater\nis a class that performs the actual gradient descent step, i.e. \nupdating the weights in each iteration, for a given gradient of the loss part.\nThe updater is also responsible to perform the update from the regularization \npart. MLlib includes updaters for cases without regularization, as well as\nL1 and L2 regularizers.\nstepSize\nis a scalar value denoting the initial step size for gradient\ndescent. All updaters in MLlib use a step size at the t-th step equal to\nstepSize $/ \\sqrt{t}$\n.\nnumIterations\nis the number of iterations to run.\nregParam\nis the regularization parameter when using L1 or L2 regularization.\nminiBatchFraction\nis the fraction of the total data that is sampled in \neach iteration, to compute the gradient direction.\nSampling still requires a pass over the entire RDD, so decreasing\nminiBatchFraction\nmay not speed up optimization much.  Users will see the greatest speedup when the gradient is expensive to compute, for only the chosen samples are used for computing the gradient.\nL-BFGS\nL-BFGS is currently only a low-level optimization primitive in\nMLlib\n. If you want to use L-BFGS in various \nML algorithms such as Linear Regression, and Logistic Regression, you have to pass the gradient of objective\nfunction, and updater into optimizer yourself instead of using the training APIs like\nLogisticRegressionWithSGD\n.\nSee the example below. It will be addressed in the next release.\nThe L1 regularization by using\nL1Updater\nwill not work since the \nsoft-thresholding logic in L1Updater is designed for gradient descent. See the developer’s note.\nThe L-BFGS method\nLBFGS.runLBFGS\nhas the following parameters:\nGradient\nis a class that computes the gradient of the objective function\nbeing optimized, i.e., with respect to a single training example, at the\ncurrent parameter value. MLlib includes gradient classes for common loss\nfunctions, e.g., hinge, logistic, least-squares.  The gradient class takes as\ninput a training example, its label, and the current parameter value.\nUpdater\nis a class that computes the gradient and loss of objective function \nof the regularization part for L-BFGS. MLlib includes updaters for cases without \nregularization, as well as L2 regularizer.\nnumCorrections\nis the number of corrections used in the L-BFGS update. 10 is \nrecommended.\nmaxNumIterations\nis the maximal number of iterations that L-BFGS can be run.\nregParam\nis the regularization parameter when using regularization.\nconvergenceTol\ncontrols how much relative change is still allowed when L-BFGS\nis considered to converge. This must be nonnegative. Lower values are less tolerant and\ntherefore generally cause more iterations to be run. This value looks at both average\nimprovement and the norm of gradient inside\nBreeze LBFGS\n.\nThe\nreturn\nis a tuple containing two elements. The first element is a column matrix\ncontaining weights for every feature, and the second element is an array containing \nthe loss computed for every iteration.\nHere is an example to train binary logistic regression with L2 regularization using\nL-BFGS optimizer.\nRefer to the\nLBFGS\nScala docs\nand\nSquaredL2Updater\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.classification.LogisticRegressionModel\nimport\norg.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nimport\norg.apache.spark.mllib.linalg.Vectors\nimport\norg.apache.spark.mllib.optimization.\n{\nLBFGS\n,\nLogisticGradient\n,\nSquaredL2Updater\n}\nimport\norg.apache.spark.mllib.util.MLUtils\nval\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_libsvm_data.txt\"\n)\nval\nnumFeatures\n=\ndata\n.\ntake\n(\n1\n)(\n0\n).\nfeatures\n.\nsize\n// Split data into training (60%) and test (40%).\nval\nsplits\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.6\n,\n0.4\n),\nseed\n=\n11L\n)\n// Append 1 into the training data as intercept.\nval\ntraining\n=\nsplits\n(\n0\n).\nmap\n(\nx\n=>\n(\nx\n.\nlabel\n,\nMLUtils\n.\nappendBias\n(\nx\n.\nfeatures\n))).\ncache\n()\nval\ntest\n=\nsplits\n(\n1\n)\n// Run training algorithm to build the model\nval\nnumCorrections\n=\n10\nval\nconvergenceTol\n=\n1\ne\n-\n4\nval\nmaxNumIterations\n=\n20\nval\nregParam\n=\n0.1\nval\ninitialWeightsWithIntercept\n=\nVectors\n.\ndense\n(\nnew\nArray\n[\nDouble\n](\nnumFeatures\n+\n1\n))\nval\n(\nweightsWithIntercept\n,\nloss\n)\n=\nLBFGS\n.\nrunLBFGS\n(\ntraining\n,\nnew\nLogisticGradient\n(),\nnew\nSquaredL2Updater\n(),\nnumCorrections\n,\nconvergenceTol\n,\nmaxNumIterations\n,\nregParam\n,\ninitialWeightsWithIntercept\n)\nval\nmodel\n=\nnew\nLogisticRegressionModel\n(\nVectors\n.\ndense\n(\nweightsWithIntercept\n.\ntoArray\n.\nslice\n(\n0\n,\nweightsWithIntercept\n.\nsize\n-\n1\n)),\nweightsWithIntercept\n(\nweightsWithIntercept\n.\nsize\n-\n1\n))\n// Clear the default threshold.\nmodel\n.\nclearThreshold\n()\n// Compute raw scores on the test set.\nval\nscoreAndLabels\n=\ntest\n.\nmap\n{\npoint\n=>\nval\nscore\n=\nmodel\n.\npredict\n(\npoint\n.\nfeatures\n)\n(\nscore\n,\npoint\n.\nlabel\n)\n}\n// Get evaluation metrics.\nval\nmetrics\n=\nnew\nBinaryClassificationMetrics\n(\nscoreAndLabels\n)\nval\nauROC\n=\nmetrics\n.\nareaUnderROC\n()\nprintln\n(\n\"Loss of each step in training process\"\n)\nloss\n.\nforeach\n(\nprintln\n)\nprintln\n(\ns\n\"Area under ROC = $auROC\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\" in the Spark repo.\nRefer to the\nLBFGS\nJava docs\nand\nSquaredL2Updater\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.api.java.*\n;\nimport\norg.apache.spark.mllib.classification.LogisticRegressionModel\n;\nimport\norg.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n;\nimport\norg.apache.spark.mllib.linalg.Vector\n;\nimport\norg.apache.spark.mllib.linalg.Vectors\n;\nimport\norg.apache.spark.mllib.optimization.*\n;\nimport\norg.apache.spark.mllib.regression.LabeledPoint\n;\nimport\norg.apache.spark.mllib.util.MLUtils\n;\nimport\norg.apache.spark.SparkConf\n;\nimport\norg.apache.spark.SparkContext\n;\nString\npath\n=\n\"data/mllib/sample_libsvm_data.txt\"\n;\nJavaRDD\n<\nLabeledPoint\n>\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\npath\n).\ntoJavaRDD\n();\nint\nnumFeatures\n=\ndata\n.\ntake\n(\n1\n).\nget\n(\n0\n).\nfeatures\n().\nsize\n();\n// Split initial RDD into two... [60% training data, 40% testing data].\nJavaRDD\n<\nLabeledPoint\n>\ntrainingInit\n=\ndata\n.\nsample\n(\nfalse\n,\n0.6\n,\n11L\n);\nJavaRDD\n<\nLabeledPoint\n>\ntest\n=\ndata\n.\nsubtract\n(\ntrainingInit\n);\n// Append 1 into the training data as intercept.\nJavaPairRDD\n<\nObject\n,\nVector\n>\ntraining\n=\ndata\n.\nmapToPair\n(\np\n->\nnew\nTuple2\n<>(\np\n.\nlabel\n(),\nMLUtils\n.\nappendBias\n(\np\n.\nfeatures\n())));\ntraining\n.\ncache\n();\n// Run training algorithm to build the model.\nint\nnumCorrections\n=\n10\n;\ndouble\nconvergenceTol\n=\n1\ne\n-\n4\n;\nint\nmaxNumIterations\n=\n20\n;\ndouble\nregParam\n=\n0.1\n;\nVector\ninitialWeightsWithIntercept\n=\nVectors\n.\ndense\n(\nnew\ndouble\n[\nnumFeatures\n+\n1\n]);\nTuple2\n<\nVector\n,\ndouble\n[]>\nresult\n=\nLBFGS\n.\nrunLBFGS\n(\ntraining\n.\nrdd\n(),\nnew\nLogisticGradient\n(),\nnew\nSquaredL2Updater\n(),\nnumCorrections\n,\nconvergenceTol\n,\nmaxNumIterations\n,\nregParam\n,\ninitialWeightsWithIntercept\n);\nVector\nweightsWithIntercept\n=\nresult\n.\n_1\n();\ndouble\n[]\nloss\n=\nresult\n.\n_2\n();\nLogisticRegressionModel\nmodel\n=\nnew\nLogisticRegressionModel\n(\nVectors\n.\ndense\n(\nArrays\n.\ncopyOf\n(\nweightsWithIntercept\n.\ntoArray\n(),\nweightsWithIntercept\n.\nsize\n()\n-\n1\n)),\n(\nweightsWithIntercept\n.\ntoArray\n())[\nweightsWithIntercept\n.\nsize\n()\n-\n1\n]);\n// Clear the default threshold.\nmodel\n.\nclearThreshold\n();\n// Compute raw scores on the test set.\nJavaPairRDD\n<\nObject\n,\nObject\n>\nscoreAndLabels\n=\ntest\n.\nmapToPair\n(\np\n->\nnew\nTuple2\n<>(\nmodel\n.\npredict\n(\np\n.\nfeatures\n()),\np\n.\nlabel\n()));\n// Get evaluation metrics.\nBinaryClassificationMetrics\nmetrics\n=\nnew\nBinaryClassificationMetrics\n(\nscoreAndLabels\n.\nrdd\n());\ndouble\nauROC\n=\nmetrics\n.\nareaUnderROC\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Loss of each step in training process\"\n);\nfor\n(\ndouble\nl\n:\nloss\n)\n{\nSystem\n.\nout\n.\nprintln\n(\nl\n);\n}\nSystem\n.\nout\n.\nprintln\n(\n\"Area under ROC = \"\n+\nauROC\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\" in the Spark repo.\nDeveloper’s notes\nSince the Hessian is constructed approximately from previous gradient evaluations, \nthe objective function can not be changed during the optimization process. \nAs a result, Stochastic L-BFGS will not work naively by just using miniBatch; \ntherefore, we don’t provide this until we have better understanding.\nUpdater\nis a class originally designed for gradient decent which computes \nthe actual gradient descent step. However, we’re able to take the gradient and \nloss of objective function of regularization for L-BFGS by ignoring the part of logic\nonly for gradient decent such as adaptive step size stuff. We will refactorize\nthis into regularizer to replace updater to separate the logic between \nregularization and step update later."}
{"url": "https://spark.apache.org/docs/latest/ml-linalg-guide.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nMLlib Linear Algebra Acceleration Guide\nIntroduction\nThis guide provides necessary information to enable accelerated linear algebra processing for Spark MLlib.\nSpark MLlib defines Vector and Matrix as basic data types for machine learning algorithms. On top of them,\nBLAS\nand\nLAPACK\noperations are implemented and supported by\ndev.ludovic.netlib\n(the algorithms may also call\nBreeze\n).\ndev.ludovic.netlib\ncan use optimized native linear algebra libraries (referred to as “native libraries” or “BLAS libraries” hereafter) for faster numerical processing.\nIntel MKL\nand\nOpenBLAS\nare two popular ones.\nThe official released Spark binaries don’t contain these native libraries.\nThe following sections describe how to install native libraries, configure them properly, and how to point\ndev.ludovic.netlib\nto these native libraries.\nInstall native linear algebra libraries\nIntel MKL and OpenBLAS are two popular native linear algebra libraries. You can choose one of them based on your preference. We provide basic instructions as below.\nIntel MKL\nDownload and install Intel MKL. The installation should be done on all nodes of the cluster. We assume the installation location is $MKLROOT (e.g. /opt/intel/mkl).\nCreate soft links to\nlibmkl_rt.so\nwith specific names in system library search paths. For instance, make sure\n/usr/local/lib\nis in system library search paths and run the following commands:\n$ ln -sf $MKLROOT/lib/intel64/libmkl_rt.so /usr/local/lib/libblas.so.3\n$ ln -sf $MKLROOT/lib/intel64/libmkl_rt.so /usr/local/lib/liblapack.so.3\nOpenBLAS\nThe installation should be done on all nodes of the cluster. Generic version of OpenBLAS are available with most distributions. You can install it with a distribution package manager like\napt\nor\nyum\n.\nFor Debian / Ubuntu:\nsudo apt-get install libopenblas-base\nsudo update-alternatives --config libblas.so.3\nFor CentOS / RHEL:\nsudo yum install openblas\nCheck if native libraries are enabled for MLlib\nTo verify native libraries are properly loaded, start\nspark-shell\nand run the following code:\nscala> import dev.ludovic.netlib.blas.NativeBLAS\nscala> NativeBLAS.getInstance()\nIf they are correctly loaded, it should print\ndev.ludovic.netlib.blas.NativeBLAS = dev.ludovic.netlib.blas.JNIBLAS@...\n. Otherwise the warnings should be printed:\nWARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n...\njava.lang.RuntimeException: Unable to load native implementation\n  at dev.ludovic.netlib.blas.InstanceBuilder.nativeBlas(InstanceBuilder.java:59)\n  at dev.ludovic.netlib.blas.NativeBLAS.getInstance(NativeBLAS.java:31)\n  ...\nYou can also point\ndev.ludovic.netlib\nto specific libraries names and paths. For example,\n-Ddev.ludovic.netlib.blas.nativeLib=libmkl_rt.so\nor\n-Ddev.ludovic.netlib.blas.nativeLibPath=$MKLROOT/lib/intel64/libmkl_rt.so\nfor Intel MKL. You have similar parameters for LAPACK and ARPACK:\n-Ddev.ludovic.netlib.lapack.nativeLib=...\n,\n-Ddev.ludovic.netlib.lapack.nativeLibPath=...\n,\n-Ddev.ludovic.netlib.arpack.nativeLib=...\n, and\n-Ddev.ludovic.netlib.arpack.nativeLibPath=...\n.\nIf native libraries are not properly configured in the system, the Java implementation (javaBLAS) will be used as fallback option.\nSpark Configuration\nThe default behavior of multi-threading in either Intel MKL or OpenBLAS may not be optimal with Spark’s execution model\n1\n.\nTherefore configuring these native libraries to use a single thread for operations may actually improve performance (see\nSPARK-21305\n). It is usually optimal to match this to the number of\nspark.task.cpus\n, which is\n1\nby default and typically left at\n1\n.\nYou can use the options in\nconfig/spark-env.sh\nto set thread number for Intel MKL or OpenBLAS:\nFor Intel MKL:\nMKL_NUM_THREADS=1\nFor OpenBLAS:\nOPENBLAS_NUM_THREADS=1\nPlease refer to the following resources to understand how to configure the number of threads for these BLAS implementations:\nIntel MKL\nor\nIntel oneMKL\nand\nOpenBLAS\n.\n↩"}
{"url": "https://spark.apache.org/docs/latest/mllib-feature-extraction.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nFeature Extraction and Transformation - RDD-based API\nTF-IDF\nWord2Vec\nModel\nExample\nStandardScaler\nModel Fitting\nExample\nNormalizer\nExample\nChiSqSelector\nModel Fitting\nExample\nElementwiseProduct\nExample\nPCA\nTF-IDF\nNote\nWe recommend using the DataFrame-based API, which is detailed in the\nML user guide on \nTF-IDF\n.\nTerm frequency-inverse document frequency (TF-IDF)\nis a feature \nvectorization method widely used in text mining to reflect the importance of a term to a document in the corpus.\nDenote a term by\n$t$\n, a document by\n$d$\n, and the corpus by\n$D$\n.\nTerm frequency\n$TF(t, d)$\nis the number of times that term\n$t$\nappears in document\n$d$\n,\nwhile document frequency\n$DF(t, D)$\nis the number of documents that contains term\n$t$\n.\nIf we only use term frequency to measure the importance, it is very easy to over-emphasize terms that\nappear very often but carry little information about the document, e.g., “a”, “the”, and “of”.\nIf a term appears very often across the corpus, it means it doesn’t carry special information about\na particular document.\nInverse document frequency is a numerical measure of how much information a term provides:\n\\[\nIDF(t, D) = \\log \\frac{|D| + 1}{DF(t, D) + 1},\n\\]\nwhere\n$|D|$\nis the total number of documents in the corpus.\nSince logarithm is used, if a term appears in all documents, its IDF value becomes 0.\nNote that a smoothing term is applied to avoid dividing by zero for terms outside the corpus.\nThe TF-IDF measure is simply the product of TF and IDF:\n\\[\nTFIDF(t, d, D) = TF(t, d) \\cdot IDF(t, D).\n\\]\nThere are several variants on the definition of term frequency and document frequency.\nIn\nspark.mllib\n, we separate TF and IDF to make them flexible.\nOur implementation of term frequency utilizes the\nhashing trick\n.\nA raw feature is mapped into an index (term) by applying a hash function.\nThen term frequencies are calculated based on the mapped indices.\nThis approach avoids the need to compute a global term-to-index map,\nwhich can be expensive for a large corpus, but it suffers from potential hash collisions,\nwhere different raw features may become the same term after hashing.\nTo reduce the chance of collision, we can increase the target feature dimension, i.e., \nthe number of buckets of the hash table.\nThe default feature dimension is\n$2^{20} = 1,048,576$\n.\nNote:\nspark.mllib\ndoesn’t provide tools for text segmentation.\nWe refer users to the\nStanford NLP Group\nand\nscalanlp/chalk\n.\nTF and IDF are implemented in\nHashingTF\nand\nIDF\n.\nHashingTF\ntakes an RDD of list as the input.\nEach record could be an iterable of strings or other types.\nRefer to the\nHashingTF\nPython docs\nfor details on the API.\nfrom\npyspark.mllib.feature\nimport\nHashingTF\n,\nIDF\n# Load documents (one per line).\ndocuments\n=\nsc\n.\ntextFile\n(\n\"\ndata/mllib/kmeans_data.txt\n\"\n).\nmap\n(\nlambda\nline\n:\nline\n.\nsplit\n(\n\"\n\"\n))\nhashingTF\n=\nHashingTF\n()\ntf\n=\nhashingTF\n.\ntransform\n(\ndocuments\n)\n# While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n# First to compute the IDF vector and second to scale the term frequencies by IDF.\ntf\n.\ncache\n()\nidf\n=\nIDF\n().\nfit\n(\ntf\n)\ntfidf\n=\nidf\n.\ntransform\n(\ntf\n)\n# spark.mllib's IDF implementation provides an option for ignoring terms\n# which occur in less than a minimum number of documents.\n# In such cases, the IDF for these terms is set to 0.\n# This feature can be used by passing the minDocFreq value to the IDF constructor.\nidfIgnore\n=\nIDF\n(\nminDocFreq\n=\n2\n).\nfit\n(\ntf\n)\ntfidfIgnore\n=\nidfIgnore\n.\ntransform\n(\ntf\n)\nFind full example code at \"examples/src/main/python/mllib/tf_idf_example.py\" in the Spark repo.\nTF and IDF are implemented in\nHashingTF\nand\nIDF\n.\nHashingTF\ntakes an\nRDD[Iterable[_]]\nas the input.\nEach record could be an iterable of strings or other types.\nRefer to the\nHashingTF\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.feature.\n{\nHashingTF\n,\nIDF\n}\nimport\norg.apache.spark.mllib.linalg.Vector\nimport\norg.apache.spark.rdd.RDD\n// Load documents (one per line).\nval\ndocuments\n:\nRDD\n[\nSeq\n[\nString\n]]\n=\nsc\n.\ntextFile\n(\n\"data/mllib/kmeans_data.txt\"\n)\n.\nmap\n(\n_\n.\nsplit\n(\n\" \"\n).\ntoSeq\n)\nval\nhashingTF\n=\nnew\nHashingTF\n()\nval\ntf\n:\nRDD\n[\nVector\n]\n=\nhashingTF\n.\ntransform\n(\ndocuments\n)\n// While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n// First to compute the IDF vector and second to scale the term frequencies by IDF.\ntf\n.\ncache\n()\nval\nidf\n=\nnew\nIDF\n().\nfit\n(\ntf\n)\nval\ntfidf\n:\nRDD\n[\nVector\n]\n=\nidf\n.\ntransform\n(\ntf\n)\n// spark.mllib IDF implementation provides an option for ignoring terms which occur in less than\n// a minimum number of documents. In such cases, the IDF for these terms is set to 0.\n// This feature can be used by passing the minDocFreq value to the IDF constructor.\nval\nidfIgnore\n=\nnew\nIDF\n(\nminDocFreq\n=\n2\n).\nfit\n(\ntf\n)\nval\ntfidfIgnore\n:\nRDD\n[\nVector\n]\n=\nidfIgnore\n.\ntransform\n(\ntf\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\" in the Spark repo.\nWord2Vec\nWord2Vec\ncomputes distributed vector representation of words.\nThe main advantage of the distributed\nrepresentations is that similar words are close in the vector space, which makes generalization to \nnovel patterns easier and model estimation more robust. Distributed vector representation is \nshowed to be useful in many natural language processing applications such as named entity \nrecognition, disambiguation, parsing, tagging and machine translation.\nModel\nIn our implementation of Word2Vec, we used skip-gram model. The training objective of skip-gram is\nto learn word vector representations that are good at predicting its context in the same sentence. \nMathematically, given a sequence of training words\n$w_1, w_2, \\dots, w_T$\n, the objective of the\nskip-gram model is to maximize the average log-likelihood\n\\[\n\\frac{1}{T} \\sum_{t = 1}^{T}\\sum_{j=-k}^{j=k} \\log p(w_{t+j} | w_t)\n\\]\nwhere $k$ is the size of the training window.\nIn the skip-gram model, every word $w$ is associated with two vectors $u_w$ and $v_w$ which are \nvector representations of $w$ as word and context respectively. The probability of correctly \npredicting word $w_i$ given word $w_j$ is determined by the softmax model, which is\n\\[\np(w_i | w_j ) = \\frac{\\exp(u_{w_i}^{\\top}v_{w_j})}{\\sum_{l=1}^{V} \\exp(u_l^{\\top}v_{w_j})}\n\\]\nwhere $V$ is the vocabulary size.\nThe skip-gram model with softmax is expensive because the cost of computing $\\log p(w_i | w_j)$\nis proportional to $V$, which can be easily in order of millions. To speed up training of Word2Vec, \nwe used hierarchical softmax, which reduced the complexity of computing of $\\log p(w_i | w_j)$ to\n$O(\\log(V))$\nExample\nThe example below demonstrates how to load a text file, parse it as an RDD of\nSeq[String]\n,\nconstruct a\nWord2Vec\ninstance and then fit a\nWord2VecModel\nwith the input data. Finally,\nwe display the top 40 synonyms of the specified word. To run the example, first download\nthe\ntext8\ndata and extract it to your preferred directory.\nHere we assume the extracted file is\ntext8\nand in same directory as you run the spark shell.\nRefer to the\nWord2Vec\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.feature\nimport\nWord2Vec\ninp\n=\nsc\n.\ntextFile\n(\n\"\ndata/mllib/sample_lda_data.txt\n\"\n).\nmap\n(\nlambda\nrow\n:\nrow\n.\nsplit\n(\n\"\n\"\n))\nword2vec\n=\nWord2Vec\n()\nmodel\n=\nword2vec\n.\nfit\n(\ninp\n)\nsynonyms\n=\nmodel\n.\nfindSynonyms\n(\n'\n1\n'\n,\n5\n)\nfor\nword\n,\ncosine_distance\nin\nsynonyms\n:\nprint\n(\n\"\n{}: {}\n\"\n.\nformat\n(\nword\n,\ncosine_distance\n))\nFind full example code at \"examples/src/main/python/mllib/word2vec_example.py\" in the Spark repo.\nRefer to the\nWord2Vec\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.feature.\n{\nWord2Vec\n,\nWord2VecModel\n}\nval\ninput\n=\nsc\n.\ntextFile\n(\n\"data/mllib/sample_lda_data.txt\"\n).\nmap\n(\nline\n=>\nline\n.\nsplit\n(\n\" \"\n).\ntoSeq\n)\nval\nword2vec\n=\nnew\nWord2Vec\n()\nval\nmodel\n=\nword2vec\n.\nfit\n(\ninput\n)\nval\nsynonyms\n=\nmodel\n.\nfindSynonyms\n(\n\"1\"\n,\n5\n)\nfor\n((\nsynonym\n,\ncosineSimilarity\n)\n<-\nsynonyms\n)\n{\nprintln\n(\ns\n\"$synonym $cosineSimilarity\"\n)\n}\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"myModelPath\"\n)\nval\nsameModel\n=\nWord2VecModel\n.\nload\n(\nsc\n,\n\"myModelPath\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\" in the Spark repo.\nStandardScaler\nStandardizes features by scaling to unit variance and/or removing the mean using column summary\nstatistics on the samples in the training set. This is a very common pre-processing step.\nFor example, RBF kernel of Support Vector Machines or the L1 and L2 regularized linear models\ntypically work better when all features have unit variance and/or zero mean.\nStandardization can improve the convergence rate during the optimization process, and also prevents\nagainst features with very large variances exerting an overly large influence during model training.\nModel Fitting\nStandardScaler\nhas the\nfollowing parameters in the constructor:\nwithMean\nFalse by default. Centers the data with mean before scaling. It will build a dense\noutput, so take care when applying to sparse input.\nwithStd\nTrue by default. Scales the data to unit standard deviation.\nWe provide a\nfit\nmethod in\nStandardScaler\nwhich can take an input of\nRDD[Vector]\n, learn the summary statistics, and then\nreturn a model which can transform the input dataset into unit standard deviation and/or zero mean features\ndepending how we configure the\nStandardScaler\n.\nThis model implements\nVectorTransformer\nwhich can apply the standardization on a\nVector\nto produce a transformed\nVector\nor on\nan\nRDD[Vector]\nto produce a transformed\nRDD[Vector]\n.\nNote that if the variance of a feature is zero, it will return default\n0.0\nvalue in the\nVector\nfor that feature.\nExample\nThe example below demonstrates how to load a dataset in libsvm format, and standardize the features\nso that the new features have unit standard deviation and/or zero mean.\nRefer to the\nStandardScaler\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.feature\nimport\nStandardScaler\nfrom\npyspark.mllib.linalg\nimport\nVectors\nfrom\npyspark.mllib.util\nimport\nMLUtils\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\nlabel\n=\ndata\n.\nmap\n(\nlambda\nx\n:\nx\n.\nlabel\n)\nfeatures\n=\ndata\n.\nmap\n(\nlambda\nx\n:\nx\n.\nfeatures\n)\nscaler1\n=\nStandardScaler\n().\nfit\n(\nfeatures\n)\nscaler2\n=\nStandardScaler\n(\nwithMean\n=\nTrue\n,\nwithStd\n=\nTrue\n).\nfit\n(\nfeatures\n)\n# data1 will be unit variance.\ndata1\n=\nlabel\n.\nzip\n(\nscaler1\n.\ntransform\n(\nfeatures\n))\n# data2 will be unit variance and zero mean.\ndata2\n=\nlabel\n.\nzip\n(\nscaler2\n.\ntransform\n(\nfeatures\n.\nmap\n(\nlambda\nx\n:\nVectors\n.\ndense\n(\nx\n.\ntoArray\n()))))\nFind full example code at \"examples/src/main/python/mllib/standard_scaler_example.py\" in the Spark repo.\nRefer to the\nStandardScaler\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.feature.\n{\nStandardScaler\n,\nStandardScalerModel\n}\nimport\norg.apache.spark.mllib.linalg.Vectors\nimport\norg.apache.spark.mllib.util.MLUtils\nval\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_libsvm_data.txt\"\n)\nval\nscaler1\n=\nnew\nStandardScaler\n().\nfit\n(\ndata\n.\nmap\n(\nx\n=>\nx\n.\nfeatures\n))\nval\nscaler2\n=\nnew\nStandardScaler\n(\nwithMean\n=\ntrue\n,\nwithStd\n=\ntrue\n).\nfit\n(\ndata\n.\nmap\n(\nx\n=>\nx\n.\nfeatures\n))\n// scaler3 is an identical model to scaler2, and will produce identical transformations\nval\nscaler3\n=\nnew\nStandardScalerModel\n(\nscaler2\n.\nstd\n,\nscaler2\n.\nmean\n)\n// data1 will be unit variance.\nval\ndata1\n=\ndata\n.\nmap\n(\nx\n=>\n(\nx\n.\nlabel\n,\nscaler1\n.\ntransform\n(\nx\n.\nfeatures\n)))\n// data2 will be unit variance and zero mean.\nval\ndata2\n=\ndata\n.\nmap\n(\nx\n=>\n(\nx\n.\nlabel\n,\nscaler2\n.\ntransform\n(\nVectors\n.\ndense\n(\nx\n.\nfeatures\n.\ntoArray\n))))\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\" in the Spark repo.\nNormalizer\nNormalizer scales individual samples to have unit $L^p$ norm. This is a common operation for text\nclassification or clustering. For example, the dot product of two $L^2$ normalized TF-IDF vectors\nis the cosine similarity of the vectors.\nNormalizer\nhas the following\nparameter in the constructor:\np\nNormalization in $L^p$ space, $p = 2$ by default.\nNormalizer\nimplements\nVectorTransformer\nwhich can apply the normalization on a\nVector\nto produce a transformed\nVector\nor on\nan\nRDD[Vector]\nto produce a transformed\nRDD[Vector]\n.\nNote that if the norm of the input is zero, it will return the input vector.\nExample\nThe example below demonstrates how to load a dataset in libsvm format, and normalizes the features\nwith $L^2$ norm, and $L^\\infty$ norm.\nRefer to the\nNormalizer\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.feature\nimport\nNormalizer\nfrom\npyspark.mllib.util\nimport\nMLUtils\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\nlabels\n=\ndata\n.\nmap\n(\nlambda\nx\n:\nx\n.\nlabel\n)\nfeatures\n=\ndata\n.\nmap\n(\nlambda\nx\n:\nx\n.\nfeatures\n)\nnormalizer1\n=\nNormalizer\n()\nnormalizer2\n=\nNormalizer\n(\np\n=\nfloat\n(\n\"\ninf\n\"\n))\n# Each sample in data1 will be normalized using $L^2$ norm.\ndata1\n=\nlabels\n.\nzip\n(\nnormalizer1\n.\ntransform\n(\nfeatures\n))\n# Each sample in data2 will be normalized using $L^\\infty$ norm.\ndata2\n=\nlabels\n.\nzip\n(\nnormalizer2\n.\ntransform\n(\nfeatures\n))\nFind full example code at \"examples/src/main/python/mllib/normalizer_example.py\" in the Spark repo.\nRefer to the\nNormalizer\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.feature.Normalizer\nimport\norg.apache.spark.mllib.util.MLUtils\nval\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_libsvm_data.txt\"\n)\nval\nnormalizer1\n=\nnew\nNormalizer\n()\nval\nnormalizer2\n=\nnew\nNormalizer\n(\np\n=\nDouble\n.\nPositiveInfinity\n)\n// Each sample in data1 will be normalized using $L^2$ norm.\nval\ndata1\n=\ndata\n.\nmap\n(\nx\n=>\n(\nx\n.\nlabel\n,\nnormalizer1\n.\ntransform\n(\nx\n.\nfeatures\n)))\n// Each sample in data2 will be normalized using $L^\\infty$ norm.\nval\ndata2\n=\ndata\n.\nmap\n(\nx\n=>\n(\nx\n.\nlabel\n,\nnormalizer2\n.\ntransform\n(\nx\n.\nfeatures\n)))\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\" in the Spark repo.\nChiSqSelector\nFeature selection\ntries to identify relevant\nfeatures for use in model construction. It reduces the size of the feature space, which can improve\nboth speed and statistical learning behavior.\nChiSqSelector\nimplements\nChi-Squared feature selection. It operates on labeled data with categorical features. ChiSqSelector uses the\nChi-Squared test of independence\nto decide which\nfeatures to choose. It supports five selection methods:\nnumTopFeatures\n,\npercentile\n,\nfpr\n,\nfdr\n,\nfwe\n:\nnumTopFeatures\nchooses a fixed number of top features according to a chi-squared test. This is akin to yielding the features with the most predictive power.\npercentile\nis similar to\nnumTopFeatures\nbut chooses a fraction of all features instead of a fixed number.\nfpr\nchooses all features whose p-values are below a threshold, thus controlling the false positive rate of selection.\nfdr\nuses the\nBenjamini-Hochberg procedure\nto choose all features whose false discovery rate is below a threshold.\nfwe\nchooses all features whose p-values are below a threshold. The threshold is scaled by 1/numFeatures, thus controlling the family-wise error rate of selection.\nBy default, the selection method is\nnumTopFeatures\n, with the default number of top features set to 50.\nThe user can choose a selection method using\nsetSelectorType\n.\nThe number of features to select can be tuned using a held-out validation set.\nModel Fitting\nThe\nfit\nmethod takes\nan input of\nRDD[LabeledPoint]\nwith categorical features, learns the summary statistics, and then\nreturns a\nChiSqSelectorModel\nwhich can transform an input dataset into the reduced feature space.\nThe\nChiSqSelectorModel\ncan be applied either to a\nVector\nto produce a reduced\nVector\n, or to\nan\nRDD[Vector]\nto produce a reduced\nRDD[Vector]\n.\nNote that the user can also construct a\nChiSqSelectorModel\nby hand by providing an array of selected feature indices (which must be sorted in ascending order).\nExample\nThe following example shows the basic use of ChiSqSelector. The data set used has a feature matrix consisting of greyscale values that vary from 0 to 255 for each feature.\nRefer to the\nChiSqSelector\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.feature.ChiSqSelector\nimport\norg.apache.spark.mllib.linalg.Vectors\nimport\norg.apache.spark.mllib.regression.LabeledPoint\nimport\norg.apache.spark.mllib.util.MLUtils\n// Load some data in libsvm format\nval\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Discretize data in 16 equal bins since ChiSqSelector requires categorical features\n// Even though features are doubles, the ChiSqSelector treats each unique value as a category\nval\ndiscretizedData\n=\ndata\n.\nmap\n{\nlp\n=>\nLabeledPoint\n(\nlp\n.\nlabel\n,\nVectors\n.\ndense\n(\nlp\n.\nfeatures\n.\ntoArray\n.\nmap\n{\nx\n=>\n(\nx\n/\n16\n).\nfloor\n}))\n}\n// Create ChiSqSelector that will select top 50 of 692 features\nval\nselector\n=\nnew\nChiSqSelector\n(\n50\n)\n// Create ChiSqSelector model (selecting features)\nval\ntransformer\n=\nselector\n.\nfit\n(\ndiscretizedData\n)\n// Filter the top 50 features from each feature vector\nval\nfilteredData\n=\ndiscretizedData\n.\nmap\n{\nlp\n=>\nLabeledPoint\n(\nlp\n.\nlabel\n,\ntransformer\n.\ntransform\n(\nlp\n.\nfeatures\n))\n}\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\" in the Spark repo.\nRefer to the\nChiSqSelector\nJava docs\nfor details on the API.\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.mllib.feature.ChiSqSelector\n;\nimport\norg.apache.spark.mllib.feature.ChiSqSelectorModel\n;\nimport\norg.apache.spark.mllib.linalg.Vectors\n;\nimport\norg.apache.spark.mllib.regression.LabeledPoint\n;\nimport\norg.apache.spark.mllib.util.MLUtils\n;\nJavaRDD\n<\nLabeledPoint\n>\npoints\n=\nMLUtils\n.\nloadLibSVMFile\n(\njsc\n.\nsc\n(),\n\"data/mllib/sample_libsvm_data.txt\"\n).\ntoJavaRDD\n().\ncache\n();\n// Discretize data in 16 equal bins since ChiSqSelector requires categorical features\n// Although features are doubles, the ChiSqSelector treats each unique value as a category\nJavaRDD\n<\nLabeledPoint\n>\ndiscretizedData\n=\npoints\n.\nmap\n(\nlp\n->\n{\ndouble\n[]\ndiscretizedFeatures\n=\nnew\ndouble\n[\nlp\n.\nfeatures\n().\nsize\n()];\nfor\n(\nint\ni\n=\n0\n;\ni\n<\nlp\n.\nfeatures\n().\nsize\n();\n++\ni\n)\n{\ndiscretizedFeatures\n[\ni\n]\n=\nMath\n.\nfloor\n(\nlp\n.\nfeatures\n().\napply\n(\ni\n)\n/\n16\n);\n}\nreturn\nnew\nLabeledPoint\n(\nlp\n.\nlabel\n(),\nVectors\n.\ndense\n(\ndiscretizedFeatures\n));\n});\n// Create ChiSqSelector that will select top 50 of 692 features\nChiSqSelector\nselector\n=\nnew\nChiSqSelector\n(\n50\n);\n// Create ChiSqSelector model (selecting features)\nChiSqSelectorModel\ntransformer\n=\nselector\n.\nfit\n(\ndiscretizedData\n.\nrdd\n());\n// Filter the top 50 features from each feature vector\nJavaRDD\n<\nLabeledPoint\n>\nfilteredData\n=\ndiscretizedData\n.\nmap\n(\nlp\n->\nnew\nLabeledPoint\n(\nlp\n.\nlabel\n(),\ntransformer\n.\ntransform\n(\nlp\n.\nfeatures\n())));\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\" in the Spark repo.\nElementwiseProduct\nElementwiseProduct\nmultiplies each input vector by a provided “weight” vector, using element-wise\nmultiplication. In other words, it scales each column of the dataset by a scalar multiplier. This\nrepresents the\nHadamard product\nbetween the input vector,\nv\nand transforming vector,\nscalingVec\n, to yield a result vector.\nDenoting the\nscalingVec\nas “\nw\n”, this transformation may be written as:\n\\[ \\begin{pmatrix}\nv_1 \\\\\n\\vdots \\\\\nv_N\n\\end{pmatrix} \\circ \\begin{pmatrix}\n                    w_1 \\\\\n                    \\vdots \\\\\n                    w_N\n                    \\end{pmatrix}\n= \\begin{pmatrix}\n  v_1 w_1 \\\\\n  \\vdots \\\\\n  v_N w_N\n  \\end{pmatrix}\n\\]\nElementwiseProduct\nhas the following parameter in the constructor:\nscalingVec\n: the transforming vector.\nElementwiseProduct\nimplements\nVectorTransformer\nwhich can apply the weighting on a\nVector\nto produce a transformed\nVector\nor on an\nRDD[Vector]\nto produce a transformed\nRDD[Vector]\n.\nExample\nThis example below demonstrates how to transform vectors using a transforming vector value.\nRefer to the\nElementwiseProduct\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.feature\nimport\nElementwiseProduct\nfrom\npyspark.mllib.linalg\nimport\nVectors\ndata\n=\nsc\n.\ntextFile\n(\n\"\ndata/mllib/kmeans_data.txt\n\"\n)\nparsedData\n=\ndata\n.\nmap\n(\nlambda\nx\n:\n[\nfloat\n(\nt\n)\nfor\nt\nin\nx\n.\nsplit\n(\n\"\n\"\n)])\n# Create weight vector.\ntransformingVector\n=\nVectors\n.\ndense\n([\n0.0\n,\n1.0\n,\n2.0\n])\ntransformer\n=\nElementwiseProduct\n(\ntransformingVector\n)\n# Batch transform\ntransformedData\n=\ntransformer\n.\ntransform\n(\nparsedData\n)\n# Single-row transform\ntransformedData2\n=\ntransformer\n.\ntransform\n(\nparsedData\n.\nfirst\n())\nFind full example code at \"examples/src/main/python/mllib/elementwise_product_example.py\" in the Spark repo.\nRefer to the\nElementwiseProduct\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.feature.ElementwiseProduct\nimport\norg.apache.spark.mllib.linalg.Vectors\n// Create some vector data; also works for sparse vectors\nval\ndata\n=\nsc\n.\nparallelize\n(\nSeq\n(\nVectors\n.\ndense\n(\n1.0\n,\n2.0\n,\n3.0\n),\nVectors\n.\ndense\n(\n4.0\n,\n5.0\n,\n6.0\n)))\nval\ntransformingVector\n=\nVectors\n.\ndense\n(\n0.0\n,\n1.0\n,\n2.0\n)\nval\ntransformer\n=\nnew\nElementwiseProduct\n(\ntransformingVector\n)\n// Batch transform and per-row transform give the same results:\nval\ntransformedData\n=\ntransformer\n.\ntransform\n(\ndata\n)\nval\ntransformedData2\n=\ndata\n.\nmap\n(\nx\n=>\ntransformer\n.\ntransform\n(\nx\n))\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\" in the Spark repo.\nRefer to the\nElementwiseProduct\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.mllib.feature.ElementwiseProduct\n;\nimport\norg.apache.spark.mllib.linalg.Vector\n;\nimport\norg.apache.spark.mllib.linalg.Vectors\n;\n// Create some vector data; also works for sparse vectors\nJavaRDD\n<\nVector\n>\ndata\n=\njsc\n.\nparallelize\n(\nArrays\n.\nasList\n(\nVectors\n.\ndense\n(\n1.0\n,\n2.0\n,\n3.0\n),\nVectors\n.\ndense\n(\n4.0\n,\n5.0\n,\n6.0\n)));\nVector\ntransformingVector\n=\nVectors\n.\ndense\n(\n0.0\n,\n1.0\n,\n2.0\n);\nElementwiseProduct\ntransformer\n=\nnew\nElementwiseProduct\n(\ntransformingVector\n);\n// Batch transform and per-row transform give the same results:\nJavaRDD\n<\nVector\n>\ntransformedData\n=\ntransformer\n.\ntransform\n(\ndata\n);\nJavaRDD\n<\nVector\n>\ntransformedData2\n=\ndata\n.\nmap\n(\ntransformer:\n:\ntransform\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\" in the Spark repo.\nPCA\nA feature transformer that projects vectors to a low-dimensional space using PCA.\nDetails you can read at\ndimensionality reduction\n."}
{"url": "https://spark.apache.org/docs/latest/mllib-data-types.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nData Types - RDD-based API\nLocal vector\nLabeled point\nLocal matrix\nDistributed matrix\nRowMatrix\nIndexedRowMatrix\nCoordinateMatrix\nBlockMatrix\nMLlib supports local vectors and matrices stored on a single machine, \nas well as distributed matrices backed by one or more RDDs.\nLocal vectors and local matrices are simple data models \nthat serve as public interfaces. The underlying linear algebra operations are provided by\nBreeze\n.\nA training example used in supervised learning is called a “labeled point” in MLlib.\nLocal vector\nA local vector has integer-typed and 0-based indices and double-typed values, stored on a single\nmachine.  MLlib supports two types of local vectors: dense and sparse.  A dense vector is backed by\na double array representing its entry values, while a sparse vector is backed by two parallel\narrays: indices and values.  For example, a vector\n(1.0, 0.0, 3.0)\ncan be represented in dense\nformat as\n[1.0, 0.0, 3.0]\nor in sparse format as\n(3, [0, 2], [1.0, 3.0])\n, where\n3\nis the size\nof the vector.\nMLlib recognizes the following types as dense vectors:\nNumPy’s\narray\nPython’s list, e.g.,\n[1, 2, 3]\nand the following as sparse vectors:\nMLlib’s\nSparseVector\n.\nSciPy’s\ncsc_matrix\nwith a single column\nWe recommend using NumPy arrays over lists for efficiency, and using the factory methods implemented\nin\nVectors\nto create sparse vectors.\nRefer to the\nVectors\nPython docs\nfor more details on the API.\nimport\nnumpy\nas\nnp\nimport\nscipy.sparse\nas\nsps\nfrom\npyspark.mllib.linalg\nimport\nVectors\n# Use a NumPy array as a dense vector.\ndv1\n=\nnp\n.\narray\n([\n1.0\n,\n0.0\n,\n3.0\n])\n# Use a Python list as a dense vector.\ndv2\n=\n[\n1.0\n,\n0.0\n,\n3.0\n]\n# Create a SparseVector.\nsv1\n=\nVectors\n.\nsparse\n(\n3\n,\n[\n0\n,\n2\n],\n[\n1.0\n,\n3.0\n])\n# Use a single-column SciPy csc_matrix as a sparse vector.\nsv2\n=\nsps\n.\ncsc_matrix\n((\nnp\n.\narray\n([\n1.0\n,\n3.0\n]),\nnp\n.\narray\n([\n0\n,\n2\n]),\nnp\n.\narray\n([\n0\n,\n2\n])),\nshape\n=\n(\n3\n,\n1\n))\nThe base class of local vectors is\nVector\n, and we provide two\nimplementations:\nDenseVector\nand\nSparseVector\n.  We recommend\nusing the factory methods implemented in\nVectors\nto create local vectors.\nRefer to the\nVector\nScala docs\nand\nVectors\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.linalg.\n{\nVector\n,\nVectors\n}\n// Create a dense vector (1.0, 0.0, 3.0).\nval\ndv\n:\nVector\n=\nVectors\n.\ndense\n(\n1.0\n,\n0.0\n,\n3.0\n)\n// Create a sparse vector (1.0, 0.0, 3.0) by specifying its indices and values corresponding to nonzero entries.\nval\nsv1\n:\nVector\n=\nVectors\n.\nsparse\n(\n3\n,\nArray\n(\n0\n,\n2\n),\nArray\n(\n1.0\n,\n3.0\n))\n// Create a sparse vector (1.0, 0.0, 3.0) by specifying its nonzero entries.\nval\nsv2\n:\nVector\n=\nVectors\n.\nsparse\n(\n3\n,\nSeq\n((\n0\n,\n1.0\n),\n(\n2\n,\n3.0\n)))\nNote:\nScala imports\nscala.collection.immutable.Vector\nby default, so you have to import\norg.apache.spark.mllib.linalg.Vector\nexplicitly to use MLlib’s\nVector\n.\nThe base class of local vectors is\nVector\n, and we provide two\nimplementations:\nDenseVector\nand\nSparseVector\n.  We recommend\nusing the factory methods implemented in\nVectors\nto create local vectors.\nRefer to the\nVector\nJava docs\nand\nVectors\nJava docs\nfor details on the API.\nimport\norg.apache.spark.mllib.linalg.Vector\n;\nimport\norg.apache.spark.mllib.linalg.Vectors\n;\n// Create a dense vector (1.0, 0.0, 3.0).\nVector\ndv\n=\nVectors\n.\ndense\n(\n1.0\n,\n0.0\n,\n3.0\n);\n// Create a sparse vector (1.0, 0.0, 3.0) by specifying its indices and values corresponding to nonzero entries.\nVector\nsv\n=\nVectors\n.\nsparse\n(\n3\n,\nnew\nint\n[]\n{\n0\n,\n2\n},\nnew\ndouble\n[]\n{\n1.0\n,\n3.0\n});\nLabeled point\nA labeled point is a local vector, either dense or sparse, associated with a label/response.\nIn MLlib, labeled points are used in supervised learning algorithms.\nWe use a double to store a label, so we can use labeled points in both regression and classification.\nFor binary classification, a label should be either\n0\n(negative) or\n1\n(positive).\nFor multiclass classification, labels should be class indices starting from zero:\n0, 1, 2, ...\n.\nA labeled point is represented by\nLabeledPoint\n.\nRefer to the\nLabeledPoint\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.linalg\nimport\nSparseVector\nfrom\npyspark.mllib.regression\nimport\nLabeledPoint\n# Create a labeled point with a positive label and a dense feature vector.\npos\n=\nLabeledPoint\n(\n1.0\n,\n[\n1.0\n,\n0.0\n,\n3.0\n])\n# Create a labeled point with a negative label and a sparse feature vector.\nneg\n=\nLabeledPoint\n(\n0.0\n,\nSparseVector\n(\n3\n,\n[\n0\n,\n2\n],\n[\n1.0\n,\n3.0\n]))\nA labeled point is represented by the case class\nLabeledPoint\n.\nRefer to the\nLabeledPoint\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.linalg.Vectors\nimport\norg.apache.spark.mllib.regression.LabeledPoint\n// Create a labeled point with a positive label and a dense feature vector.\nval\npos\n=\nLabeledPoint\n(\n1.0\n,\nVectors\n.\ndense\n(\n1.0\n,\n0.0\n,\n3.0\n))\n// Create a labeled point with a negative label and a sparse feature vector.\nval\nneg\n=\nLabeledPoint\n(\n0.0\n,\nVectors\n.\nsparse\n(\n3\n,\nArray\n(\n0\n,\n2\n),\nArray\n(\n1.0\n,\n3.0\n)))\nA labeled point is represented by\nLabeledPoint\n.\nRefer to the\nLabeledPoint\nJava docs\nfor details on the API.\nimport\norg.apache.spark.mllib.linalg.Vectors\n;\nimport\norg.apache.spark.mllib.regression.LabeledPoint\n;\n// Create a labeled point with a positive label and a dense feature vector.\nLabeledPoint\npos\n=\nnew\nLabeledPoint\n(\n1.0\n,\nVectors\n.\ndense\n(\n1.0\n,\n0.0\n,\n3.0\n));\n// Create a labeled point with a negative label and a sparse feature vector.\nLabeledPoint\nneg\n=\nnew\nLabeledPoint\n(\n0.0\n,\nVectors\n.\nsparse\n(\n3\n,\nnew\nint\n[]\n{\n0\n,\n2\n},\nnew\ndouble\n[]\n{\n1.0\n,\n3.0\n}));\nSparse data\nIt is very common in practice to have sparse training data.  MLlib supports reading training\nexamples stored in\nLIBSVM\nformat, which is the default format used by\nLIBSVM\nand\nLIBLINEAR\n.  It is a text format in which each line\nrepresents a labeled sparse feature vector using the following format:\nlabel index1:value1 index2:value2 ...\nwhere the indices are one-based and in ascending order. \nAfter loading, the feature indices are converted to zero-based.\nMLUtils.loadLibSVMFile\nreads training\nexamples stored in LIBSVM format.\nRefer to the\nMLUtils\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.util\nimport\nMLUtils\nexamples\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\nMLUtils.loadLibSVMFile\nreads training\nexamples stored in LIBSVM format.\nRefer to the\nMLUtils\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.regression.LabeledPoint\nimport\norg.apache.spark.mllib.util.MLUtils\nimport\norg.apache.spark.rdd.RDD\nval\nexamples\n:\nRDD\n[\nLabeledPoint\n]\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_libsvm_data.txt\"\n)\nMLUtils.loadLibSVMFile\nreads training\nexamples stored in LIBSVM format.\nRefer to the\nMLUtils\nJava docs\nfor details on the API.\nimport\norg.apache.spark.mllib.regression.LabeledPoint\n;\nimport\norg.apache.spark.mllib.util.MLUtils\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nJavaRDD\n<\nLabeledPoint\n>\nexamples\n=\nMLUtils\n.\nloadLibSVMFile\n(\njsc\n.\nsc\n(),\n\"data/mllib/sample_libsvm_data.txt\"\n).\ntoJavaRDD\n();\nLocal matrix\nA local matrix has integer-typed row and column indices and double-typed values, stored on a single\nmachine.  MLlib supports dense matrices, whose entry values are stored in a single double array in\ncolumn-major order, and sparse matrices, whose non-zero entry values are stored in the Compressed Sparse\nColumn (CSC) format in column-major order.  For example, the following dense matrix\n\\[ \\begin{pmatrix}\n1.0 & 2.0 \\\\\n3.0 & 4.0 \\\\\n5.0 & 6.0\n\\end{pmatrix}\n\\]\nis stored in a one-dimensional array\n[1.0, 3.0, 5.0, 2.0, 4.0, 6.0]\nwith the matrix size\n(3, 2)\n.\nThe base class of local matrices is\nMatrix\n, and we provide two\nimplementations:\nDenseMatrix\n,\nand\nSparseMatrix\n.\nWe recommend using the factory methods implemented\nin\nMatrices\nto create local\nmatrices. Remember, local matrices in MLlib are stored in column-major order.\nRefer to the\nMatrix\nPython docs\nand\nMatrices\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.linalg\nimport\nMatrix\n,\nMatrices\n# Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\ndm2\n=\nMatrices\n.\ndense\n(\n3\n,\n2\n,\n[\n1\n,\n3\n,\n5\n,\n2\n,\n4\n,\n6\n])\n# Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))\nsm\n=\nMatrices\n.\nsparse\n(\n3\n,\n2\n,\n[\n0\n,\n1\n,\n3\n],\n[\n0\n,\n2\n,\n1\n],\n[\n9\n,\n6\n,\n8\n])\nThe base class of local matrices is\nMatrix\n, and we provide two\nimplementations:\nDenseMatrix\n,\nand\nSparseMatrix\n.\nWe recommend using the factory methods implemented\nin\nMatrices\nto create local\nmatrices. Remember, local matrices in MLlib are stored in column-major order.\nRefer to the\nMatrix\nScala docs\nand\nMatrices\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.linalg.\n{\nMatrix\n,\nMatrices\n}\n// Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\nval\ndm\n:\nMatrix\n=\nMatrices\n.\ndense\n(\n3\n,\n2\n,\nArray\n(\n1.0\n,\n3.0\n,\n5.0\n,\n2.0\n,\n4.0\n,\n6.0\n))\n// Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))\nval\nsm\n:\nMatrix\n=\nMatrices\n.\nsparse\n(\n3\n,\n2\n,\nArray\n(\n0\n,\n1\n,\n3\n),\nArray\n(\n0\n,\n2\n,\n1\n),\nArray\n(\n9\n,\n6\n,\n8\n))\nThe base class of local matrices is\nMatrix\n, and we provide two\nimplementations:\nDenseMatrix\n,\nand\nSparseMatrix\n.\nWe recommend using the factory methods implemented\nin\nMatrices\nto create local\nmatrices. Remember, local matrices in MLlib are stored in column-major order.\nRefer to the\nMatrix\nJava docs\nand\nMatrices\nJava docs\nfor details on the API.\nimport\norg.apache.spark.mllib.linalg.Matrix\n;\nimport\norg.apache.spark.mllib.linalg.Matrices\n;\n// Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\nMatrix\ndm\n=\nMatrices\n.\ndense\n(\n3\n,\n2\n,\nnew\ndouble\n[]\n{\n1.0\n,\n3.0\n,\n5.0\n,\n2.0\n,\n4.0\n,\n6.0\n});\n// Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))\nMatrix\nsm\n=\nMatrices\n.\nsparse\n(\n3\n,\n2\n,\nnew\nint\n[]\n{\n0\n,\n1\n,\n3\n},\nnew\nint\n[]\n{\n0\n,\n2\n,\n1\n},\nnew\ndouble\n[]\n{\n9\n,\n6\n,\n8\n});\nDistributed matrix\nA distributed matrix has long-typed row and column indices and double-typed values, stored\ndistributively in one or more RDDs.  It is very important to choose the right format to store large\nand distributed matrices.  Converting a distributed matrix to a different format may require a\nglobal shuffle, which is quite expensive. Four types of distributed matrices have been implemented\nso far.\nThe basic type is called\nRowMatrix\n. A\nRowMatrix\nis a row-oriented distributed\nmatrix without meaningful row indices, e.g., a collection of feature vectors.\nIt is backed by an RDD of its rows, where each row is a local vector.\nWe assume that the number of columns is not huge for a\nRowMatrix\nso that a single\nlocal vector can be reasonably communicated to the driver and can also be stored /\noperated on using a single node. \nAn\nIndexedRowMatrix\nis similar to a\nRowMatrix\nbut with row indices,\nwhich can be used for identifying rows and executing joins.\nA\nCoordinateMatrix\nis a distributed matrix stored in\ncoordinate list (COO)\nformat,\nbacked by an RDD of its entries.\nA\nBlockMatrix\nis a distributed matrix backed by an RDD of\nMatrixBlock\nwhich is a tuple of\n(Int, Int, Matrix)\n.\nNote\nThe underlying RDDs of a distributed matrix must be deterministic, because we cache the matrix size.\nIn general, the use of non-deterministic RDDs can lead to errors.\nRowMatrix\nA\nRowMatrix\nis a row-oriented distributed matrix without meaningful row indices, backed by an RDD\nof its rows, where each row is a local vector.\nSince each row is represented by a local vector, the number of columns is\nlimited by the integer range but it should be much smaller in practice.\nA\nRowMatrix\ncan be \ncreated from an\nRDD\nof vectors.\nRefer to the\nRowMatrix\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.linalg.distributed\nimport\nRowMatrix\n# Create an RDD of vectors.\nrows\n=\nsc\n.\nparallelize\n([[\n1\n,\n2\n,\n3\n],\n[\n4\n,\n5\n,\n6\n],\n[\n7\n,\n8\n,\n9\n],\n[\n10\n,\n11\n,\n12\n]])\n# Create a RowMatrix from an RDD of vectors.\nmat\n=\nRowMatrix\n(\nrows\n)\n# Get its size.\nm\n=\nmat\n.\nnumRows\n()\n# 4\nn\n=\nmat\n.\nnumCols\n()\n# 3\n# Get the rows as an RDD of vectors again.\nrowsRDD\n=\nmat\n.\nrows\nA\nRowMatrix\ncan be\ncreated from an\nRDD[Vector]\ninstance.  Then we can compute its column summary statistics and decompositions.\nQR decomposition\nis of the form A = QR where Q is an orthogonal matrix and R is an upper triangular matrix.\nFor\nsingular value decomposition (SVD)\nand\nprincipal component analysis (PCA)\n, please refer to\nDimensionality reduction\n.\nRefer to the\nRowMatrix\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.linalg.Vector\nimport\norg.apache.spark.mllib.linalg.distributed.RowMatrix\nval\nrows\n:\nRDD\n[\nVector\n]\n=\n...\n// an RDD of local vectors\n// Create a RowMatrix from an RDD[Vector].\nval\nmat\n:\nRowMatrix\n=\nnew\nRowMatrix\n(\nrows\n)\n// Get its size.\nval\nm\n=\nmat\n.\nnumRows\n()\nval\nn\n=\nmat\n.\nnumCols\n()\n// QR decomposition\nval\nqrResult\n=\nmat\n.\ntallSkinnyQR\n(\ntrue\n)\nA\nRowMatrix\ncan be\ncreated from a\nJavaRDD<Vector>\ninstance.  Then we can compute its column summary statistics.\nRefer to the\nRowMatrix\nJava docs\nfor details on the API.\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.mllib.linalg.Vector\n;\nimport\norg.apache.spark.mllib.linalg.distributed.RowMatrix\n;\nJavaRDD\n<\nVector\n>\nrows\n=\n...\n// a JavaRDD of local vectors\n// Create a RowMatrix from a JavaRDD<Vector>.\nRowMatrix\nmat\n=\nnew\nRowMatrix\n(\nrows\n.\nrdd\n());\n// Get its size.\nlong\nm\n=\nmat\n.\nnumRows\n();\nlong\nn\n=\nmat\n.\nnumCols\n();\n// QR decomposition\nQRDecomposition\n<\nRowMatrix\n,\nMatrix\n>\nresult\n=\nmat\n.\ntallSkinnyQR\n(\ntrue\n);\nIndexedRowMatrix\nAn\nIndexedRowMatrix\nis similar to a\nRowMatrix\nbut with meaningful row indices.  It is backed by\nan RDD of indexed rows, so that each row is represented by its index (long-typed) and a local \nvector.\nAn\nIndexedRowMatrix\ncan be created from an\nRDD\nof\nIndexedRow\ns, where\nIndexedRow\nis a \nwrapper over\n(long, vector)\n.  An\nIndexedRowMatrix\ncan be converted to a\nRowMatrix\nby dropping\nits row indices.\nRefer to the\nIndexedRowMatrix\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.linalg.distributed\nimport\nIndexedRow\n,\nIndexedRowMatrix\n# Create an RDD of indexed rows.\n#   - This can be done explicitly with the IndexedRow class:\nindexedRows\n=\nsc\n.\nparallelize\n([\nIndexedRow\n(\n0\n,\n[\n1\n,\n2\n,\n3\n]),\nIndexedRow\n(\n1\n,\n[\n4\n,\n5\n,\n6\n]),\nIndexedRow\n(\n2\n,\n[\n7\n,\n8\n,\n9\n]),\nIndexedRow\n(\n3\n,\n[\n10\n,\n11\n,\n12\n])])\n#   - or by using (long, vector) tuples:\nindexedRows\n=\nsc\n.\nparallelize\n([(\n0\n,\n[\n1\n,\n2\n,\n3\n]),\n(\n1\n,\n[\n4\n,\n5\n,\n6\n]),\n(\n2\n,\n[\n7\n,\n8\n,\n9\n]),\n(\n3\n,\n[\n10\n,\n11\n,\n12\n])])\n# Create an IndexedRowMatrix from an RDD of IndexedRows.\nmat\n=\nIndexedRowMatrix\n(\nindexedRows\n)\n# Get its size.\nm\n=\nmat\n.\nnumRows\n()\n# 4\nn\n=\nmat\n.\nnumCols\n()\n# 3\n# Get the rows as an RDD of IndexedRows.\nrowsRDD\n=\nmat\n.\nrows\n# Convert to a RowMatrix by dropping the row indices.\nrowMat\n=\nmat\n.\ntoRowMatrix\n()\nAn\nIndexedRowMatrix\ncan be created from an\nRDD[IndexedRow]\ninstance, where\nIndexedRow\nis a\nwrapper over\n(Long, Vector)\n.  An\nIndexedRowMatrix\ncan be converted to a\nRowMatrix\nby dropping\nits row indices.\nRefer to the\nIndexedRowMatrix\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.linalg.distributed.\n{\nIndexedRow\n,\nIndexedRowMatrix\n,\nRowMatrix\n}\nval\nrows\n:\nRDD\n[\nIndexedRow\n]\n=\n...\n// an RDD of indexed rows\n// Create an IndexedRowMatrix from an RDD[IndexedRow].\nval\nmat\n:\nIndexedRowMatrix\n=\nnew\nIndexedRowMatrix\n(\nrows\n)\n// Get its size.\nval\nm\n=\nmat\n.\nnumRows\n()\nval\nn\n=\nmat\n.\nnumCols\n()\n// Drop its row indices.\nval\nrowMat\n:\nRowMatrix\n=\nmat\n.\ntoRowMatrix\n()\nAn\nIndexedRowMatrix\ncan be created from a\nJavaRDD<IndexedRow>\ninstance, where\nIndexedRow\nis a\nwrapper over\n(long, Vector)\n.  An\nIndexedRowMatrix\ncan be converted to a\nRowMatrix\nby dropping\nits row indices.\nRefer to the\nIndexedRowMatrix\nJava docs\nfor details on the API.\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.mllib.linalg.distributed.IndexedRow\n;\nimport\norg.apache.spark.mllib.linalg.distributed.IndexedRowMatrix\n;\nimport\norg.apache.spark.mllib.linalg.distributed.RowMatrix\n;\nJavaRDD\n<\nIndexedRow\n>\nrows\n=\n...\n// a JavaRDD of indexed rows\n// Create an IndexedRowMatrix from a JavaRDD<IndexedRow>.\nIndexedRowMatrix\nmat\n=\nnew\nIndexedRowMatrix\n(\nrows\n.\nrdd\n());\n// Get its size.\nlong\nm\n=\nmat\n.\nnumRows\n();\nlong\nn\n=\nmat\n.\nnumCols\n();\n// Drop its row indices.\nRowMatrix\nrowMat\n=\nmat\n.\ntoRowMatrix\n();\nCoordinateMatrix\nA\nCoordinateMatrix\nis a distributed matrix backed by an RDD of its entries.  Each entry is a tuple\nof\n(i: Long, j: Long, value: Double)\n, where\ni\nis the row index,\nj\nis the column index, and\nvalue\nis the entry value.  A\nCoordinateMatrix\nshould be used only when both\ndimensions of the matrix are huge and the matrix is very sparse.\nA\nCoordinateMatrix\ncan be created from an\nRDD\nof\nMatrixEntry\nentries, where\nMatrixEntry\nis a \nwrapper over\n(long, long, float)\n.  A\nCoordinateMatrix\ncan be converted to a\nRowMatrix\nby \ncalling\ntoRowMatrix\n, or to an\nIndexedRowMatrix\nwith sparse rows by calling\ntoIndexedRowMatrix\n.\nRefer to the\nCoordinateMatrix\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.linalg.distributed\nimport\nCoordinateMatrix\n,\nMatrixEntry\n# Create an RDD of coordinate entries.\n#   - This can be done explicitly with the MatrixEntry class:\nentries\n=\nsc\n.\nparallelize\n([\nMatrixEntry\n(\n0\n,\n0\n,\n1.2\n),\nMatrixEntry\n(\n1\n,\n0\n,\n2.1\n),\nMatrixEntry\n(\n2\n,\n1\n,\n3.7\n)])\n#   - or using (long, long, float) tuples:\nentries\n=\nsc\n.\nparallelize\n([(\n0\n,\n0\n,\n1.2\n),\n(\n1\n,\n0\n,\n2.1\n),\n(\n2\n,\n1\n,\n3.7\n)])\n# Create a CoordinateMatrix from an RDD of MatrixEntries.\nmat\n=\nCoordinateMatrix\n(\nentries\n)\n# Get its size.\nm\n=\nmat\n.\nnumRows\n()\n# 3\nn\n=\nmat\n.\nnumCols\n()\n# 2\n# Get the entries as an RDD of MatrixEntries.\nentriesRDD\n=\nmat\n.\nentries\n# Convert to a RowMatrix.\nrowMat\n=\nmat\n.\ntoRowMatrix\n()\n# Convert to an IndexedRowMatrix.\nindexedRowMat\n=\nmat\n.\ntoIndexedRowMatrix\n()\n# Convert to a BlockMatrix.\nblockMat\n=\nmat\n.\ntoBlockMatrix\n()\nA\nCoordinateMatrix\ncan be created from an\nRDD[MatrixEntry]\ninstance, where\nMatrixEntry\nis a\nwrapper over\n(Long, Long, Double)\n.  A\nCoordinateMatrix\ncan be converted to an\nIndexedRowMatrix\nwith sparse rows by calling\ntoIndexedRowMatrix\n.  Other computations for\nCoordinateMatrix\nare not currently supported.\nRefer to the\nCoordinateMatrix\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.linalg.distributed.\n{\nCoordinateMatrix\n,\nMatrixEntry\n}\nval\nentries\n:\nRDD\n[\nMatrixEntry\n]\n=\n...\n// an RDD of matrix entries\n// Create a CoordinateMatrix from an RDD[MatrixEntry].\nval\nmat\n:\nCoordinateMatrix\n=\nnew\nCoordinateMatrix\n(\nentries\n)\n// Get its size.\nval\nm\n=\nmat\n.\nnumRows\n()\nval\nn\n=\nmat\n.\nnumCols\n()\n// Convert it to an IndexRowMatrix whose rows are sparse vectors.\nval\nindexedRowMatrix\n=\nmat\n.\ntoIndexedRowMatrix\n()\nA\nCoordinateMatrix\ncan be created from a\nJavaRDD<MatrixEntry>\ninstance, where\nMatrixEntry\nis a\nwrapper over\n(long, long, double)\n.  A\nCoordinateMatrix\ncan be converted to an\nIndexedRowMatrix\nwith sparse rows by calling\ntoIndexedRowMatrix\n. Other computations for\nCoordinateMatrix\nare not currently supported.\nRefer to the\nCoordinateMatrix\nJava docs\nfor details on the API.\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.mllib.linalg.distributed.CoordinateMatrix\n;\nimport\norg.apache.spark.mllib.linalg.distributed.IndexedRowMatrix\n;\nimport\norg.apache.spark.mllib.linalg.distributed.MatrixEntry\n;\nJavaRDD\n<\nMatrixEntry\n>\nentries\n=\n...\n// a JavaRDD of matrix entries\n// Create a CoordinateMatrix from a JavaRDD<MatrixEntry>.\nCoordinateMatrix\nmat\n=\nnew\nCoordinateMatrix\n(\nentries\n.\nrdd\n());\n// Get its size.\nlong\nm\n=\nmat\n.\nnumRows\n();\nlong\nn\n=\nmat\n.\nnumCols\n();\n// Convert it to an IndexRowMatrix whose rows are sparse vectors.\nIndexedRowMatrix\nindexedRowMatrix\n=\nmat\n.\ntoIndexedRowMatrix\n();\nBlockMatrix\nA\nBlockMatrix\nis a distributed matrix backed by an RDD of\nMatrixBlock\ns, where a\nMatrixBlock\nis\na tuple of\n((Int, Int), Matrix)\n, where the\n(Int, Int)\nis the index of the block, and\nMatrix\nis\nthe sub-matrix at the given index with size\nrowsPerBlock\nx\ncolsPerBlock\n.\nBlockMatrix\nsupports methods such as\nadd\nand\nmultiply\nwith another\nBlockMatrix\n.\nBlockMatrix\nalso has a helper function\nvalidate\nwhich can be used to check whether the\nBlockMatrix\nis set up properly.\nA\nBlockMatrix\ncan be created from an\nRDD\nof sub-matrix blocks, where a sub-matrix block is a\n((blockRowIndex, blockColIndex), sub-matrix)\ntuple.\nRefer to the\nBlockMatrix\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.linalg\nimport\nMatrices\nfrom\npyspark.mllib.linalg.distributed\nimport\nBlockMatrix\n# Create an RDD of sub-matrix blocks.\nblocks\n=\nsc\n.\nparallelize\n([((\n0\n,\n0\n),\nMatrices\n.\ndense\n(\n3\n,\n2\n,\n[\n1\n,\n2\n,\n3\n,\n4\n,\n5\n,\n6\n])),\n((\n1\n,\n0\n),\nMatrices\n.\ndense\n(\n3\n,\n2\n,\n[\n7\n,\n8\n,\n9\n,\n10\n,\n11\n,\n12\n]))])\n# Create a BlockMatrix from an RDD of sub-matrix blocks.\nmat\n=\nBlockMatrix\n(\nblocks\n,\n3\n,\n2\n)\n# Get its size.\nm\n=\nmat\n.\nnumRows\n()\n# 6\nn\n=\nmat\n.\nnumCols\n()\n# 2\n# Get the blocks as an RDD of sub-matrix blocks.\nblocksRDD\n=\nmat\n.\nblocks\n# Convert to a LocalMatrix.\nlocalMat\n=\nmat\n.\ntoLocalMatrix\n()\n# Convert to an IndexedRowMatrix.\nindexedRowMat\n=\nmat\n.\ntoIndexedRowMatrix\n()\n# Convert to a CoordinateMatrix.\ncoordinateMat\n=\nmat\n.\ntoCoordinateMatrix\n()\nA\nBlockMatrix\ncan be\nmost easily created from an\nIndexedRowMatrix\nor\nCoordinateMatrix\nby calling\ntoBlockMatrix\n.\ntoBlockMatrix\ncreates blocks of size 1024 x 1024 by default.\nUsers may change the block size by supplying the values through\ntoBlockMatrix(rowsPerBlock, colsPerBlock)\n.\nRefer to the\nBlockMatrix\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.linalg.distributed.\n{\nBlockMatrix\n,\nCoordinateMatrix\n,\nMatrixEntry\n}\nval\nentries\n:\nRDD\n[\nMatrixEntry\n]\n=\n...\n// an RDD of (i, j, v) matrix entries\n// Create a CoordinateMatrix from an RDD[MatrixEntry].\nval\ncoordMat\n:\nCoordinateMatrix\n=\nnew\nCoordinateMatrix\n(\nentries\n)\n// Transform the CoordinateMatrix to a BlockMatrix\nval\nmatA\n:\nBlockMatrix\n=\ncoordMat\n.\ntoBlockMatrix\n().\ncache\n()\n// Validate whether the BlockMatrix is set up properly. Throws an Exception when it is not valid.\n// Nothing happens if it is valid.\nmatA\n.\nvalidate\n()\n// Calculate A^T A.\nval\nata\n=\nmatA\n.\ntranspose\n.\nmultiply\n(\nmatA\n)\nA\nBlockMatrix\ncan be\nmost easily created from an\nIndexedRowMatrix\nor\nCoordinateMatrix\nby calling\ntoBlockMatrix\n.\ntoBlockMatrix\ncreates blocks of size 1024 x 1024 by default.\nUsers may change the block size by supplying the values through\ntoBlockMatrix(rowsPerBlock, colsPerBlock)\n.\nRefer to the\nBlockMatrix\nJava docs\nfor details on the API.\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.mllib.linalg.distributed.BlockMatrix\n;\nimport\norg.apache.spark.mllib.linalg.distributed.CoordinateMatrix\n;\nimport\norg.apache.spark.mllib.linalg.distributed.IndexedRowMatrix\n;\nJavaRDD\n<\nMatrixEntry\n>\nentries\n=\n...\n// a JavaRDD of (i, j, v) Matrix Entries\n// Create a CoordinateMatrix from a JavaRDD<MatrixEntry>.\nCoordinateMatrix\ncoordMat\n=\nnew\nCoordinateMatrix\n(\nentries\n.\nrdd\n());\n// Transform the CoordinateMatrix to a BlockMatrix\nBlockMatrix\nmatA\n=\ncoordMat\n.\ntoBlockMatrix\n().\ncache\n();\n// Validate whether the BlockMatrix is set up properly. Throws an Exception when it is not valid.\n// Nothing happens if it is valid.\nmatA\n.\nvalidate\n();\n// Calculate A^T A.\nBlockMatrix\nata\n=\nmatA\n.\ntranspose\n().\nmultiply\n(\nmatA\n);"}
{"url": "https://spark.apache.org/docs/latest/mllib-clustering.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nk-means\nGaussian mixture\npower iteration clustering (PIC)\nlatent Dirichlet allocation (LDA)\nstreaming k-means\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nClustering - RDD-based API\nClustering\nis an unsupervised learning problem whereby we aim to group subsets\nof entities with one another based on some notion of similarity.  Clustering is\noften used for exploratory analysis and/or as a component of a hierarchical\nsupervised learning\npipeline (in which distinct classifiers or regression\nmodels are trained for each cluster).\nThe\nspark.mllib\npackage supports the following models:\nK-means\nGaussian mixture\nPower iteration clustering (PIC)\nLatent Dirichlet allocation (LDA)\nBisecting k-means\nStreaming k-means\nK-means\nK-means\nis one of the\nmost commonly used clustering algorithms that clusters the data points into a\npredefined number of clusters. The\nspark.mllib\nimplementation includes a parallelized\nvariant of the\nk-means++\nmethod\ncalled\nkmeans||\n.\nThe implementation in\nspark.mllib\nhas the following parameters:\nk\nis the number of desired clusters. Note that it is possible for fewer than k clusters to be returned, for example, if there are fewer than k distinct points to cluster.\nmaxIterations\nis the maximum number of iterations to run.\ninitializationMode\nspecifies either random initialization or\ninitialization via k-means||.\nruns\nThis param has no effect since Spark 2.0.0.\ninitializationSteps\ndetermines the number of steps in the k-means|| algorithm.\nepsilon\ndetermines the distance threshold within which we consider k-means to have converged.\ninitialModel\nis an optional set of cluster centers used for initialization. If this parameter is supplied, only one run is performed.\nExamples\nThe following examples can be tested in the PySpark shell.\nIn the following example after loading and parsing data, we use the KMeans object to cluster the\ndata into two clusters. The number of desired clusters is passed to the algorithm. We then compute\nWithin Set Sum of Squared Error (WSSSE). You can reduce this error measure by increasing\nk\n. In\nfact the optimal\nk\nis usually one where there is an “elbow” in the WSSSE graph.\nRefer to the\nKMeans\nPython docs\nand\nKMeansModel\nPython docs\nfor more details on the API.\nfrom\nnumpy\nimport\narray\nfrom\nmath\nimport\nsqrt\nfrom\npyspark.mllib.clustering\nimport\nKMeans\n,\nKMeansModel\n# Load and parse the data\ndata\n=\nsc\n.\ntextFile\n(\n\"\ndata/mllib/kmeans_data.txt\n\"\n)\nparsedData\n=\ndata\n.\nmap\n(\nlambda\nline\n:\narray\n([\nfloat\n(\nx\n)\nfor\nx\nin\nline\n.\nsplit\n(\n'\n'\n)]))\n# Build the model (cluster the data)\nclusters\n=\nKMeans\n.\ntrain\n(\nparsedData\n,\n2\n,\nmaxIterations\n=\n10\n,\ninitializationMode\n=\n\"\nrandom\n\"\n)\n# Evaluate clustering by computing Within Set Sum of Squared Errors\ndef\nerror\n(\npoint\n):\ncenter\n=\nclusters\n.\ncenters\n[\nclusters\n.\npredict\n(\npoint\n)]\nreturn\nsqrt\n(\nsum\n([\nx\n**\n2\nfor\nx\nin\n(\npoint\n-\ncenter\n)]))\nWSSSE\n=\nparsedData\n.\nmap\n(\nlambda\npoint\n:\nerror\n(\npoint\n)).\nreduce\n(\nlambda\nx\n,\ny\n:\nx\n+\ny\n)\nprint\n(\n\"\nWithin Set Sum of Squared Error =\n\"\n+\nstr\n(\nWSSSE\n))\n# Save and load model\nclusters\n.\nsave\n(\nsc\n,\n\"\ntarget/org/apache/spark/PythonKMeansExample/KMeansModel\n\"\n)\nsameModel\n=\nKMeansModel\n.\nload\n(\nsc\n,\n\"\ntarget/org/apache/spark/PythonKMeansExample/KMeansModel\n\"\n)\nFind full example code at \"examples/src/main/python/mllib/k_means_example.py\" in the Spark repo.\nThe following code snippets can be executed in\nspark-shell\n.\nIn the following example after loading and parsing data, we use the\nKMeans\nobject to cluster the data\ninto two clusters. The number of desired clusters is passed to the algorithm. We then compute Within\nSet Sum of Squared Error (WSSSE). You can reduce this error measure by increasing\nk\n. In fact, the\noptimal\nk\nis usually one where there is an “elbow” in the WSSSE graph.\nRefer to the\nKMeans\nScala docs\nand\nKMeansModel\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.clustering.\n{\nKMeans\n,\nKMeansModel\n}\nimport\norg.apache.spark.mllib.linalg.Vectors\n// Load and parse the data\nval\ndata\n=\nsc\n.\ntextFile\n(\n\"data/mllib/kmeans_data.txt\"\n)\nval\nparsedData\n=\ndata\n.\nmap\n(\ns\n=>\nVectors\n.\ndense\n(\ns\n.\nsplit\n(\n' '\n).\nmap\n(\n_\n.\ntoDouble\n))).\ncache\n()\n// Cluster the data into two classes using KMeans\nval\nnumClusters\n=\n2\nval\nnumIterations\n=\n20\nval\nclusters\n=\nKMeans\n.\ntrain\n(\nparsedData\n,\nnumClusters\n,\nnumIterations\n)\n// Evaluate clustering by computing Within Set Sum of Squared Errors\nval\nWSSSE\n=\nclusters\n.\ncomputeCost\n(\nparsedData\n)\nprintln\n(\ns\n\"Within Set Sum of Squared Errors = $WSSSE\"\n)\n// Save and load model\nclusters\n.\nsave\n(\nsc\n,\n\"target/org/apache/spark/KMeansExample/KMeansModel\"\n)\nval\nsameModel\n=\nKMeansModel\n.\nload\n(\nsc\n,\n\"target/org/apache/spark/KMeansExample/KMeansModel\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\" in the Spark repo.\nAll of MLlib’s methods use Java-friendly types, so you can import and call them there the same\nway you do in Scala. The only caveat is that the methods take Scala RDD objects, while the\nSpark Java API uses a separate\nJavaRDD\nclass. You can convert a Java RDD to a Scala one by\ncalling\n.rdd()\non your\nJavaRDD\nobject. A self-contained application example\nthat is equivalent to the provided example in Scala is given below:\nRefer to the\nKMeans\nJava docs\nand\nKMeansModel\nJava docs\nfor details on the API.\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.mllib.clustering.KMeans\n;\nimport\norg.apache.spark.mllib.clustering.KMeansModel\n;\nimport\norg.apache.spark.mllib.linalg.Vector\n;\nimport\norg.apache.spark.mllib.linalg.Vectors\n;\n// Load and parse data\nString\npath\n=\n\"data/mllib/kmeans_data.txt\"\n;\nJavaRDD\n<\nString\n>\ndata\n=\njsc\n.\ntextFile\n(\npath\n);\nJavaRDD\n<\nVector\n>\nparsedData\n=\ndata\n.\nmap\n(\ns\n->\n{\nString\n[]\nsarray\n=\ns\n.\nsplit\n(\n\" \"\n);\ndouble\n[]\nvalues\n=\nnew\ndouble\n[\nsarray\n.\nlength\n];\nfor\n(\nint\ni\n=\n0\n;\ni\n<\nsarray\n.\nlength\n;\ni\n++)\n{\nvalues\n[\ni\n]\n=\nDouble\n.\nparseDouble\n(\nsarray\n[\ni\n]);\n}\nreturn\nVectors\n.\ndense\n(\nvalues\n);\n});\nparsedData\n.\ncache\n();\n// Cluster the data into two classes using KMeans\nint\nnumClusters\n=\n2\n;\nint\nnumIterations\n=\n20\n;\nKMeansModel\nclusters\n=\nKMeans\n.\ntrain\n(\nparsedData\n.\nrdd\n(),\nnumClusters\n,\nnumIterations\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Cluster centers:\"\n);\nfor\n(\nVector\ncenter:\nclusters\n.\nclusterCenters\n())\n{\nSystem\n.\nout\n.\nprintln\n(\n\" \"\n+\ncenter\n);\n}\ndouble\ncost\n=\nclusters\n.\ncomputeCost\n(\nparsedData\n.\nrdd\n());\nSystem\n.\nout\n.\nprintln\n(\n\"Cost: \"\n+\ncost\n);\n// Evaluate clustering by computing Within Set Sum of Squared Errors\ndouble\nWSSSE\n=\nclusters\n.\ncomputeCost\n(\nparsedData\n.\nrdd\n());\nSystem\n.\nout\n.\nprintln\n(\n\"Within Set Sum of Squared Errors = \"\n+\nWSSSE\n);\n// Save and load model\nclusters\n.\nsave\n(\njsc\n.\nsc\n(),\n\"target/org/apache/spark/JavaKMeansExample/KMeansModel\"\n);\nKMeansModel\nsameModel\n=\nKMeansModel\n.\nload\n(\njsc\n.\nsc\n(),\n\"target/org/apache/spark/JavaKMeansExample/KMeansModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\" in the Spark repo.\nGaussian mixture\nA\nGaussian Mixture Model\nrepresents a composite distribution whereby points are drawn from one of\nk\nGaussian sub-distributions,\neach with its own probability.  The\nspark.mllib\nimplementation uses the\nexpectation-maximization\nalgorithm to induce the maximum-likelihood model given a set of samples.  The implementation\nhas the following parameters:\nk\nis the number of desired clusters.\nconvergenceTol\nis the maximum change in log-likelihood at which we consider convergence achieved.\nmaxIterations\nis the maximum number of iterations to perform without reaching convergence.\ninitialModel\nis an optional starting point from which to start the EM algorithm. If this parameter is omitted, a random starting point will be constructed from the data.\nExamples\nIn the following example after loading and parsing data, we use a\nGaussianMixture\nobject to cluster the data into two clusters. The number of desired clusters is passed\nto the algorithm. We then output the parameters of the mixture model.\nRefer to the\nGaussianMixture\nPython docs\nand\nGaussianMixtureModel\nPython docs\nfor more details on the API.\nfrom\nnumpy\nimport\narray\nfrom\npyspark.mllib.clustering\nimport\nGaussianMixture\n,\nGaussianMixtureModel\n# Load and parse the data\ndata\n=\nsc\n.\ntextFile\n(\n\"\ndata/mllib/gmm_data.txt\n\"\n)\nparsedData\n=\ndata\n.\nmap\n(\nlambda\nline\n:\narray\n([\nfloat\n(\nx\n)\nfor\nx\nin\nline\n.\nstrip\n().\nsplit\n(\n'\n'\n)]))\n# Build the model (cluster the data)\ngmm\n=\nGaussianMixture\n.\ntrain\n(\nparsedData\n,\n2\n)\n# Save and load model\ngmm\n.\nsave\n(\nsc\n,\n\"\ntarget/org/apache/spark/PythonGaussianMixtureExample/GaussianMixtureModel\n\"\n)\nsameModel\n=\nGaussianMixtureModel\n\\\n.\nload\n(\nsc\n,\n\"\ntarget/org/apache/spark/PythonGaussianMixtureExample/GaussianMixtureModel\n\"\n)\n# output parameters of model\nfor\ni\nin\nrange\n(\n2\n):\nprint\n(\n\"\nweight =\n\"\n,\ngmm\n.\nweights\n[\ni\n],\n\"\nmu =\n\"\n,\ngmm\n.\ngaussians\n[\ni\n].\nmu\n,\n\"\nsigma =\n\"\n,\ngmm\n.\ngaussians\n[\ni\n].\nsigma\n.\ntoArray\n())\nFind full example code at \"examples/src/main/python/mllib/gaussian_mixture_example.py\" in the Spark repo.\nIn the following example after loading and parsing data, we use a\nGaussianMixture\nobject to cluster the data into two clusters. The number of desired clusters is passed\nto the algorithm. We then output the parameters of the mixture model.\nRefer to the\nGaussianMixture\nScala docs\nand\nGaussianMixtureModel\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.clustering.\n{\nGaussianMixture\n,\nGaussianMixtureModel\n}\nimport\norg.apache.spark.mllib.linalg.Vectors\n// Load and parse the data\nval\ndata\n=\nsc\n.\ntextFile\n(\n\"data/mllib/gmm_data.txt\"\n)\nval\nparsedData\n=\ndata\n.\nmap\n(\ns\n=>\nVectors\n.\ndense\n(\ns\n.\ntrim\n.\nsplit\n(\n' '\n).\nmap\n(\n_\n.\ntoDouble\n))).\ncache\n()\n// Cluster the data into two classes using GaussianMixture\nval\ngmm\n=\nnew\nGaussianMixture\n().\nsetK\n(\n2\n).\nrun\n(\nparsedData\n)\n// Save and load model\ngmm\n.\nsave\n(\nsc\n,\n\"target/org/apache/spark/GaussianMixtureExample/GaussianMixtureModel\"\n)\nval\nsameModel\n=\nGaussianMixtureModel\n.\nload\n(\nsc\n,\n\"target/org/apache/spark/GaussianMixtureExample/GaussianMixtureModel\"\n)\n// output parameters of max-likelihood model\nfor\n(\ni\n<-\n0\nuntil\ngmm\n.\nk\n)\n{\nprintln\n(\n\"weight=%f\\nmu=%s\\nsigma=\\n%s\\n\"\nformat\n(\ngmm\n.\nweights\n(\ni\n),\ngmm\n.\ngaussians\n(\ni\n).\nmu\n,\ngmm\n.\ngaussians\n(\ni\n).\nsigma\n))\n}\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\" in the Spark repo.\nAll of MLlib’s methods use Java-friendly types, so you can import and call them there the same\nway you do in Scala. The only caveat is that the methods take Scala RDD objects, while the\nSpark Java API uses a separate\nJavaRDD\nclass. You can convert a Java RDD to a Scala one by\ncalling\n.rdd()\non your\nJavaRDD\nobject. A self-contained application example\nthat is equivalent to the provided example in Scala is given below:\nRefer to the\nGaussianMixture\nJava docs\nand\nGaussianMixtureModel\nJava docs\nfor details on the API.\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.mllib.clustering.GaussianMixture\n;\nimport\norg.apache.spark.mllib.clustering.GaussianMixtureModel\n;\nimport\norg.apache.spark.mllib.linalg.Vector\n;\nimport\norg.apache.spark.mllib.linalg.Vectors\n;\n// Load and parse data\nString\npath\n=\n\"data/mllib/gmm_data.txt\"\n;\nJavaRDD\n<\nString\n>\ndata\n=\njsc\n.\ntextFile\n(\npath\n);\nJavaRDD\n<\nVector\n>\nparsedData\n=\ndata\n.\nmap\n(\ns\n->\n{\nString\n[]\nsarray\n=\ns\n.\ntrim\n().\nsplit\n(\n\" \"\n);\ndouble\n[]\nvalues\n=\nnew\ndouble\n[\nsarray\n.\nlength\n];\nfor\n(\nint\ni\n=\n0\n;\ni\n<\nsarray\n.\nlength\n;\ni\n++)\n{\nvalues\n[\ni\n]\n=\nDouble\n.\nparseDouble\n(\nsarray\n[\ni\n]);\n}\nreturn\nVectors\n.\ndense\n(\nvalues\n);\n});\nparsedData\n.\ncache\n();\n// Cluster the data into two classes using GaussianMixture\nGaussianMixtureModel\ngmm\n=\nnew\nGaussianMixture\n().\nsetK\n(\n2\n).\nrun\n(\nparsedData\n.\nrdd\n());\n// Save and load GaussianMixtureModel\ngmm\n.\nsave\n(\njsc\n.\nsc\n(),\n\"target/org/apache/spark/JavaGaussianMixtureExample/GaussianMixtureModel\"\n);\nGaussianMixtureModel\nsameModel\n=\nGaussianMixtureModel\n.\nload\n(\njsc\n.\nsc\n(),\n\"target/org.apache.spark.JavaGaussianMixtureExample/GaussianMixtureModel\"\n);\n// Output the parameters of the mixture model\nfor\n(\nint\nj\n=\n0\n;\nj\n<\ngmm\n.\nk\n();\nj\n++)\n{\nSystem\n.\nout\n.\nprintf\n(\n\"weight=%f\\nmu=%s\\nsigma=\\n%s\\n\"\n,\ngmm\n.\nweights\n()[\nj\n],\ngmm\n.\ngaussians\n()[\nj\n].\nmu\n(),\ngmm\n.\ngaussians\n()[\nj\n].\nsigma\n());\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\" in the Spark repo.\nPower iteration clustering (PIC)\nPower iteration clustering (PIC) is a scalable and efficient algorithm for clustering vertices of a\ngraph given pairwise similarities as edge properties,\ndescribed in\nLin and Cohen, Power Iteration Clustering\n.\nIt computes a pseudo-eigenvector of the normalized affinity matrix of the graph via\npower iteration\nand uses it to cluster vertices.\nspark.mllib\nincludes an implementation of PIC using GraphX as its backend.\nIt takes an\nRDD\nof\n(srcId, dstId, similarity)\ntuples and outputs a model with the clustering assignments.\nThe similarities must be nonnegative.\nPIC assumes that the similarity measure is symmetric.\nA pair\n(srcId, dstId)\nregardless of the ordering should appear at most once in the input data.\nIf a pair is missing from input, their similarity is treated as zero.\nspark.mllib\n’s PIC implementation takes the following (hyper-)parameters:\nk\n: number of clusters\nmaxIterations\n: maximum number of power iterations\ninitializationMode\n: initialization model. This can be either “random”, which is the default,\nto use a random vector as vertex properties, or “degree” to use normalized sum similarities.\nExamples\nIn the following, we show code snippets to demonstrate how to use PIC in\nspark.mllib\n.\nPowerIterationClustering\nimplements the PIC algorithm.\nIt takes an\nRDD\nof\n(srcId: Long, dstId: Long, similarity: Double)\ntuples representing the\naffinity matrix.\nCalling\nPowerIterationClustering.run\nreturns a\nPowerIterationClusteringModel\n,\nwhich contains the computed clustering assignments.\nRefer to the\nPowerIterationClustering\nPython docs\nand\nPowerIterationClusteringModel\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.clustering\nimport\nPowerIterationClustering\n,\nPowerIterationClusteringModel\n# Load and parse the data\ndata\n=\nsc\n.\ntextFile\n(\n\"\ndata/mllib/pic_data.txt\n\"\n)\nsimilarities\n=\ndata\n.\nmap\n(\nlambda\nline\n:\ntuple\n([\nfloat\n(\nx\n)\nfor\nx\nin\nline\n.\nsplit\n(\n'\n'\n)]))\n# Cluster the data into two classes using PowerIterationClustering\nmodel\n=\nPowerIterationClustering\n.\ntrain\n(\nsimilarities\n,\n2\n,\n10\n)\nmodel\n.\nassignments\n().\nforeach\n(\nlambda\nx\n:\nprint\n(\nstr\n(\nx\n.\nid\n)\n+\n\"\n->\n\"\n+\nstr\n(\nx\n.\ncluster\n)))\n# Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"\ntarget/org/apache/spark/PythonPowerIterationClusteringExample/PICModel\n\"\n)\nsameModel\n=\nPowerIterationClusteringModel\n\\\n.\nload\n(\nsc\n,\n\"\ntarget/org/apache/spark/PythonPowerIterationClusteringExample/PICModel\n\"\n)\nFind full example code at \"examples/src/main/python/mllib/power_iteration_clustering_example.py\" in the Spark repo.\nPowerIterationClustering\nimplements the PIC algorithm.\nIt takes an\nRDD\nof\n(srcId: Long, dstId: Long, similarity: Double)\ntuples representing the\naffinity matrix.\nCalling\nPowerIterationClustering.run\nreturns a\nPowerIterationClusteringModel\n,\nwhich contains the computed clustering assignments.\nRefer to the\nPowerIterationClustering\nScala docs\nand\nPowerIterationClusteringModel\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.clustering.PowerIterationClustering\nval\ncirclesRdd\n=\ngenerateCirclesRdd\n(\nsc\n,\nparams\n.\nk\n,\nparams\n.\nnumPoints\n)\nval\nmodel\n=\nnew\nPowerIterationClustering\n()\n.\nsetK\n(\nparams\n.\nk\n)\n.\nsetMaxIterations\n(\nparams\n.\nmaxIterations\n)\n.\nsetInitializationMode\n(\n\"degree\"\n)\n.\nrun\n(\ncirclesRdd\n)\nval\nclusters\n=\nmodel\n.\nassignments\n.\ncollect\n().\ngroupBy\n(\n_\n.\ncluster\n).\ntransform\n((\n_\n,\nv\n)\n=>\nv\n.\nmap\n(\n_\n.\nid\n))\nval\nassignments\n=\nclusters\n.\ntoList\n.\nsortBy\n{\ncase\n(\nk\n,\nv\n)\n=>\nv\n.\nlength\n}\nval\nassignmentsStr\n=\nassignments\n.\nmap\n{\ncase\n(\nk\n,\nv\n)\n=>\ns\n\"$k -> ${v.sorted.mkString(\"\n[\n\"\n,\n\"\n,\n\"\n,\n\"\n]\n\")}\"\n}.\nmkString\n(\n\", \"\n)\nval\nsizesStr\n=\nassignments\n.\nmap\n{\n_\n.\n_2\n.\nlength\n}.\nsorted\n.\nmkString\n(\n\"(\"\n,\n\",\"\n,\n\")\"\n)\nprintln\n(\ns\n\"Cluster assignments: $assignmentsStr\\ncluster sizes: $sizesStr\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\" in the Spark repo.\nPowerIterationClustering\nimplements the PIC algorithm.\nIt takes a\nJavaRDD\nof\n(srcId: Long, dstId: Long, similarity: Double)\ntuples representing the\naffinity matrix.\nCalling\nPowerIterationClustering.run\nreturns a\nPowerIterationClusteringModel\nwhich contains the computed clustering assignments.\nRefer to the\nPowerIterationClustering\nJava docs\nand\nPowerIterationClusteringModel\nJava docs\nfor details on the API.\nimport\norg.apache.spark.mllib.clustering.PowerIterationClustering\n;\nimport\norg.apache.spark.mllib.clustering.PowerIterationClusteringModel\n;\nJavaRDD\n<\nTuple3\n<\nLong\n,\nLong\n,\nDouble\n>>\nsimilarities\n=\nsc\n.\nparallelize\n(\nArrays\n.\nasList\n(\nnew\nTuple3\n<>(\n0L\n,\n1L\n,\n0.9\n),\nnew\nTuple3\n<>(\n1L\n,\n2L\n,\n0.9\n),\nnew\nTuple3\n<>(\n2L\n,\n3L\n,\n0.9\n),\nnew\nTuple3\n<>(\n3L\n,\n4L\n,\n0.1\n),\nnew\nTuple3\n<>(\n4L\n,\n5L\n,\n0.9\n)));\nPowerIterationClustering\npic\n=\nnew\nPowerIterationClustering\n()\n.\nsetK\n(\n2\n)\n.\nsetMaxIterations\n(\n10\n);\nPowerIterationClusteringModel\nmodel\n=\npic\n.\nrun\n(\nsimilarities\n);\nfor\n(\nPowerIterationClustering\n.\nAssignment\na:\nmodel\n.\nassignments\n().\ntoJavaRDD\n().\ncollect\n())\n{\nSystem\n.\nout\n.\nprintln\n(\na\n.\nid\n()\n+\n\" -> \"\n+\na\n.\ncluster\n());\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\" in the Spark repo.\nLatent Dirichlet allocation (LDA)\nLatent Dirichlet allocation (LDA)\nis a topic model which infers topics from a collection of text documents.\nLDA can be thought of as a clustering algorithm as follows:\nTopics correspond to cluster centers, and documents correspond to\nexamples (rows) in a dataset.\nTopics and documents both exist in a feature space, where feature\nvectors are vectors of word counts (bag of words).\nRather than estimating a clustering using a traditional distance, LDA\nuses a function based on a statistical model of how text documents are\ngenerated.\nLDA supports different inference algorithms via\nsetOptimizer\nfunction.\nEMLDAOptimizer\nlearns clustering using\nexpectation-maximization\non the likelihood function and yields comprehensive results, while\nOnlineLDAOptimizer\nuses iterative mini-batch sampling for\nonline\nvariational\ninference\nand is generally memory friendly.\nLDA takes in a collection of documents as vectors of word counts and the\nfollowing parameters (set using the builder pattern):\nk\n: Number of topics (i.e., cluster centers)\noptimizer\n: Optimizer to use for learning the LDA model, either\nEMLDAOptimizer\nor\nOnlineLDAOptimizer\ndocConcentration\n: Dirichlet parameter for prior over documents’\ndistributions over topics. Larger values encourage smoother inferred\ndistributions.\ntopicConcentration\n: Dirichlet parameter for prior over topics’\ndistributions over terms (words). Larger values encourage smoother\ninferred distributions.\nmaxIterations\n: Limit on the number of iterations.\ncheckpointInterval\n: If using checkpointing (set in the Spark\nconfiguration), this parameter specifies the frequency with which\ncheckpoints will be created.  If\nmaxIterations\nis large, using\ncheckpointing can help reduce shuffle file sizes on disk and help with\nfailure recovery.\nAll of\nspark.mllib\n’s LDA models support:\ndescribeTopics\n: Returns topics as arrays of most important terms and\nterm weights\ntopicsMatrix\n: Returns a\nvocabSize\nby\nk\nmatrix where each column\nis a topic\nNote\n: LDA is still an experimental feature under active development.\nAs a result, certain features are only available in one of the two\noptimizers / models generated by the optimizer. Currently, a distributed\nmodel can be converted into a local model, but not vice-versa.\nThe following discussion will describe each optimizer/model pair\nseparately.\nExpectation Maximization\nImplemented in\nEMLDAOptimizer\nand\nDistributedLDAModel\n.\nFor the parameters provided to\nLDA\n:\ndocConcentration\n: Only symmetric priors are supported, so all values\nin the provided\nk\n-dimensional vector must be identical. All values\nmust also be $> 1.0$. Providing\nVector(-1)\nresults in default behavior\n(uniform\nk\ndimensional vector with value $(50 / k) + 1$\ntopicConcentration\n: Only symmetric priors supported. Values must be\n$> 1.0$. Providing\n-1\nresults in defaulting to a value of $0.1 + 1$.\nmaxIterations\n: The maximum number of EM iterations.\nNote\n: It is important to do enough iterations.  In early iterations, EM often has useless topics,\nbut those topics improve dramatically after more iterations.  Using at least 20 and possibly\n50-100 iterations is often reasonable, depending on your dataset.\nEMLDAOptimizer\nproduces a\nDistributedLDAModel\n, which stores not only\nthe inferred topics but also the full training corpus and topic\ndistributions for each document in the training corpus. A\nDistributedLDAModel\nsupports:\ntopTopicsPerDocument\n: The top topics and their weights for\n each document in the training corpus\ntopDocumentsPerTopic\n: The top documents for each topic and\n the corresponding weight of the topic in the documents.\nlogPrior\n: log probability of the estimated topics and\n document-topic distributions given the hyperparameters\ndocConcentration\nand\ntopicConcentration\nlogLikelihood\n: log likelihood of the training corpus, given the\n inferred topics and document-topic distributions\nOnline Variational Bayes\nImplemented in\nOnlineLDAOptimizer\nand\nLocalLDAModel\n.\nFor the parameters provided to\nLDA\n:\ndocConcentration\n: Asymmetric priors can be used by passing in a\nvector with values equal to the Dirichlet parameter in each of the\nk\ndimensions. Values should be $>= 0$. Providing\nVector(-1)\nresults in\ndefault behavior (uniform\nk\ndimensional vector with value $(1.0 / k)$)\ntopicConcentration\n: Only symmetric priors supported. Values must be\n$>= 0$. Providing\n-1\nresults in defaulting to a value of $(1.0 / k)$.\nmaxIterations\n: Maximum number of minibatches to submit.\nIn addition,\nOnlineLDAOptimizer\naccepts the following parameters:\nminiBatchFraction\n: Fraction of corpus sampled and used at each\niteration\noptimizeDocConcentration\n: If set to true, performs maximum-likelihood\nestimation of the hyperparameter\ndocConcentration\n(aka\nalpha\n)\nafter each minibatch and sets the optimized\ndocConcentration\nin the\nreturned\nLocalLDAModel\ntau0\nand\nkappa\n: Used for learning-rate decay, which is computed by\n$(\\tau_0 + iter)^{-\\kappa}$ where $iter$ is the current number of iterations.\nOnlineLDAOptimizer\nproduces a\nLocalLDAModel\n, which only stores the\ninferred topics. A\nLocalLDAModel\nsupports:\nlogLikelihood(documents)\n: Calculates a lower bound on the provided\ndocuments\ngiven the inferred topics.\nlogPerplexity(documents)\n: Calculates an upper bound on the\nperplexity of the provided\ndocuments\ngiven the inferred topics.\nExamples\nIn the following example, we load word count vectors representing a corpus of documents.\nWe then use\nLDA\nto infer three topics from the documents. The number of desired clusters is passed\nto the algorithm. We then output the topics, represented as probability distributions over words.\nRefer to the\nLDA\nPython docs\nand\nLDAModel\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.clustering\nimport\nLDA\n,\nLDAModel\nfrom\npyspark.mllib.linalg\nimport\nVectors\n# Load and parse the data\ndata\n=\nsc\n.\ntextFile\n(\n\"\ndata/mllib/sample_lda_data.txt\n\"\n)\nparsedData\n=\ndata\n.\nmap\n(\nlambda\nline\n:\nVectors\n.\ndense\n([\nfloat\n(\nx\n)\nfor\nx\nin\nline\n.\nstrip\n().\nsplit\n(\n'\n'\n)]))\n# Index documents with unique IDs\ncorpus\n=\nparsedData\n.\nzipWithIndex\n().\nmap\n(\nlambda\nx\n:\n[\nx\n[\n1\n],\nx\n[\n0\n]]).\ncache\n()\n# Cluster the documents into three topics using LDA\nldaModel\n=\nLDA\n.\ntrain\n(\ncorpus\n,\nk\n=\n3\n)\n# Output topics. Each is a distribution over words (matching word count vectors)\nprint\n(\n\"\nLearned topics (as distributions over vocab of\n\"\n+\nstr\n(\nldaModel\n.\nvocabSize\n())\n+\n\"\nwords):\n\"\n)\ntopics\n=\nldaModel\n.\ntopicsMatrix\n()\nfor\ntopic\nin\nrange\n(\n3\n):\nprint\n(\n\"\nTopic\n\"\n+\nstr\n(\ntopic\n)\n+\n\"\n:\n\"\n)\nfor\nword\nin\nrange\n(\n0\n,\nldaModel\n.\nvocabSize\n()):\nprint\n(\n\"\n\"\n+\nstr\n(\ntopics\n[\nword\n][\ntopic\n]))\n# Save and load model\nldaModel\n.\nsave\n(\nsc\n,\n\"\ntarget/org/apache/spark/PythonLatentDirichletAllocationExample/LDAModel\n\"\n)\nsameModel\n=\nLDAModel\n\\\n.\nload\n(\nsc\n,\n\"\ntarget/org/apache/spark/PythonLatentDirichletAllocationExample/LDAModel\n\"\n)\nFind full example code at \"examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\" in the Spark repo.\nRefer to the\nLDA\nScala docs\nand\nDistributedLDAModel\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.clustering.\n{\nDistributedLDAModel\n,\nLDA\n}\nimport\norg.apache.spark.mllib.linalg.Vectors\n// Load and parse the data\nval\ndata\n=\nsc\n.\ntextFile\n(\n\"data/mllib/sample_lda_data.txt\"\n)\nval\nparsedData\n=\ndata\n.\nmap\n(\ns\n=>\nVectors\n.\ndense\n(\ns\n.\ntrim\n.\nsplit\n(\n' '\n).\nmap\n(\n_\n.\ntoDouble\n)))\n// Index documents with unique IDs\nval\ncorpus\n=\nparsedData\n.\nzipWithIndex\n().\nmap\n(\n_\n.\nswap\n).\ncache\n()\n// Cluster the documents into three topics using LDA\nval\nldaModel\n=\nnew\nLDA\n().\nsetK\n(\n3\n).\nrun\n(\ncorpus\n)\n// Output topics. Each is a distribution over words (matching word count vectors)\nprintln\n(\ns\n\"Learned topics (as distributions over vocab of ${ldaModel.vocabSize} words):\"\n)\nval\ntopics\n=\nldaModel\n.\ntopicsMatrix\nfor\n(\ntopic\n<-\nRange\n(\n0\n,\n3\n))\n{\nprint\n(\ns\n\"Topic $topic :\"\n)\nfor\n(\nword\n<-\nRange\n(\n0\n,\nldaModel\n.\nvocabSize\n))\n{\nprint\n(\ns\n\"${topics(word, topic)}\"\n)\n}\nprintln\n()\n}\n// Save and load model.\nldaModel\n.\nsave\n(\nsc\n,\n\"target/org/apache/spark/LatentDirichletAllocationExample/LDAModel\"\n)\nval\nsameModel\n=\nDistributedLDAModel\n.\nload\n(\nsc\n,\n\"target/org/apache/spark/LatentDirichletAllocationExample/LDAModel\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\" in the Spark repo.\nRefer to the\nLDA\nJava docs\nand\nDistributedLDAModel\nJava docs\nfor details on the API.\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.api.java.JavaPairRDD\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.mllib.clustering.DistributedLDAModel\n;\nimport\norg.apache.spark.mllib.clustering.LDA\n;\nimport\norg.apache.spark.mllib.clustering.LDAModel\n;\nimport\norg.apache.spark.mllib.linalg.Matrix\n;\nimport\norg.apache.spark.mllib.linalg.Vector\n;\nimport\norg.apache.spark.mllib.linalg.Vectors\n;\n// Load and parse the data\nString\npath\n=\n\"data/mllib/sample_lda_data.txt\"\n;\nJavaRDD\n<\nString\n>\ndata\n=\njsc\n.\ntextFile\n(\npath\n);\nJavaRDD\n<\nVector\n>\nparsedData\n=\ndata\n.\nmap\n(\ns\n->\n{\nString\n[]\nsarray\n=\ns\n.\ntrim\n().\nsplit\n(\n\" \"\n);\ndouble\n[]\nvalues\n=\nnew\ndouble\n[\nsarray\n.\nlength\n];\nfor\n(\nint\ni\n=\n0\n;\ni\n<\nsarray\n.\nlength\n;\ni\n++)\n{\nvalues\n[\ni\n]\n=\nDouble\n.\nparseDouble\n(\nsarray\n[\ni\n]);\n}\nreturn\nVectors\n.\ndense\n(\nvalues\n);\n});\n// Index documents with unique IDs\nJavaPairRDD\n<\nLong\n,\nVector\n>\ncorpus\n=\nJavaPairRDD\n.\nfromJavaRDD\n(\nparsedData\n.\nzipWithIndex\n().\nmap\n(\nTuple2:\n:\nswap\n));\ncorpus\n.\ncache\n();\n// Cluster the documents into three topics using LDA\nLDAModel\nldaModel\n=\nnew\nLDA\n().\nsetK\n(\n3\n).\nrun\n(\ncorpus\n);\n// Output topics. Each is a distribution over words (matching word count vectors)\nSystem\n.\nout\n.\nprintln\n(\n\"Learned topics (as distributions over vocab of \"\n+\nldaModel\n.\nvocabSize\n()\n+\n\" words):\"\n);\nMatrix\ntopics\n=\nldaModel\n.\ntopicsMatrix\n();\nfor\n(\nint\ntopic\n=\n0\n;\ntopic\n<\n3\n;\ntopic\n++)\n{\nSystem\n.\nout\n.\nprint\n(\n\"Topic \"\n+\ntopic\n+\n\":\"\n);\nfor\n(\nint\nword\n=\n0\n;\nword\n<\nldaModel\n.\nvocabSize\n();\nword\n++)\n{\nSystem\n.\nout\n.\nprint\n(\n\" \"\n+\ntopics\n.\napply\n(\nword\n,\ntopic\n));\n}\nSystem\n.\nout\n.\nprintln\n();\n}\nldaModel\n.\nsave\n(\njsc\n.\nsc\n(),\n\"target/org/apache/spark/JavaLatentDirichletAllocationExample/LDAModel\"\n);\nDistributedLDAModel\nsameModel\n=\nDistributedLDAModel\n.\nload\n(\njsc\n.\nsc\n(),\n\"target/org/apache/spark/JavaLatentDirichletAllocationExample/LDAModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\" in the Spark repo.\nBisecting k-means\nBisecting K-means can often be much faster than regular K-means, but it will generally produce a different clustering.\nBisecting k-means is a kind of\nhierarchical clustering\n.\nHierarchical clustering is one of the most commonly used  method of cluster analysis which seeks to build a hierarchy of clusters.\nStrategies for hierarchical clustering generally fall into two types:\nAgglomerative: This is a “bottom up” approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.\nDivisive: This is a “top down” approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.\nBisecting k-means algorithm is a kind of divisive algorithms.\nThe implementation in MLlib has the following parameters:\nk\n: the desired number of leaf clusters (default: 4). The actual number could be smaller if there are no divisible leaf clusters.\nmaxIterations\n: the max number of k-means iterations to split clusters (default: 20)\nminDivisibleClusterSize\n: the minimum number of points (if >= 1.0) or the minimum proportion of points (if < 1.0) of a divisible cluster (default: 1)\nseed\n: a random seed (default: hash value of the class name)\nExamples\nRefer to the\nBisectingKMeans\nPython docs\nand\nBisectingKMeansModel\nPython docs\nfor more details on the API.\nfrom\nnumpy\nimport\narray\nfrom\npyspark.mllib.clustering\nimport\nBisectingKMeans\n# Load and parse the data\ndata\n=\nsc\n.\ntextFile\n(\n\"\ndata/mllib/kmeans_data.txt\n\"\n)\nparsedData\n=\ndata\n.\nmap\n(\nlambda\nline\n:\narray\n([\nfloat\n(\nx\n)\nfor\nx\nin\nline\n.\nsplit\n(\n'\n'\n)]))\n# Build the model (cluster the data)\nmodel\n=\nBisectingKMeans\n.\ntrain\n(\nparsedData\n,\n2\n,\nmaxIterations\n=\n5\n)\n# Evaluate clustering\ncost\n=\nmodel\n.\ncomputeCost\n(\nparsedData\n)\nprint\n(\n\"\nBisecting K-means Cost =\n\"\n+\nstr\n(\ncost\n))\nFind full example code at \"examples/src/main/python/mllib/bisecting_k_means_example.py\" in the Spark repo.\nRefer to the\nBisectingKMeans\nScala docs\nand\nBisectingKMeansModel\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.clustering.BisectingKMeans\nimport\norg.apache.spark.mllib.linalg.\n{\nVector\n,\nVectors\n}\n// Loads and parses data\ndef\nparse\n(\nline\n:\nString\n)\n:\nVector\n=\nVectors\n.\ndense\n(\nline\n.\nsplit\n(\n\" \"\n).\nmap\n(\n_\n.\ntoDouble\n))\nval\ndata\n=\nsc\n.\ntextFile\n(\n\"data/mllib/kmeans_data.txt\"\n).\nmap\n(\nparse\n).\ncache\n()\n// Clustering the data into 6 clusters by BisectingKMeans.\nval\nbkm\n=\nnew\nBisectingKMeans\n().\nsetK\n(\n6\n)\nval\nmodel\n=\nbkm\n.\nrun\n(\ndata\n)\n// Show the compute cost and the cluster centers\nprintln\n(\ns\n\"Compute Cost: ${model.computeCost(data)}\"\n)\nmodel\n.\nclusterCenters\n.\nzipWithIndex\n.\nforeach\n{\ncase\n(\ncenter\n,\nidx\n)\n=>\nprintln\n(\ns\n\"Cluster Center ${idx}: ${center}\"\n)\n}\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\" in the Spark repo.\nRefer to the\nBisectingKMeans\nJava docs\nand\nBisectingKMeansModel\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.mllib.clustering.BisectingKMeans\n;\nimport\norg.apache.spark.mllib.clustering.BisectingKMeansModel\n;\nimport\norg.apache.spark.mllib.linalg.Vector\n;\nimport\norg.apache.spark.mllib.linalg.Vectors\n;\nList\n<\nVector\n>\nlocalData\n=\nArrays\n.\nasList\n(\nVectors\n.\ndense\n(\n0.1\n,\n0.1\n),\nVectors\n.\ndense\n(\n0.3\n,\n0.3\n),\nVectors\n.\ndense\n(\n10.1\n,\n10.1\n),\nVectors\n.\ndense\n(\n10.3\n,\n10.3\n),\nVectors\n.\ndense\n(\n20.1\n,\n20.1\n),\nVectors\n.\ndense\n(\n20.3\n,\n20.3\n),\nVectors\n.\ndense\n(\n30.1\n,\n30.1\n),\nVectors\n.\ndense\n(\n30.3\n,\n30.3\n)\n);\nJavaRDD\n<\nVector\n>\ndata\n=\nsc\n.\nparallelize\n(\nlocalData\n,\n2\n);\nBisectingKMeans\nbkm\n=\nnew\nBisectingKMeans\n()\n.\nsetK\n(\n4\n);\nBisectingKMeansModel\nmodel\n=\nbkm\n.\nrun\n(\ndata\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Compute Cost: \"\n+\nmodel\n.\ncomputeCost\n(\ndata\n));\nVector\n[]\nclusterCenters\n=\nmodel\n.\nclusterCenters\n();\nfor\n(\nint\ni\n=\n0\n;\ni\n<\nclusterCenters\n.\nlength\n;\ni\n++)\n{\nVector\nclusterCenter\n=\nclusterCenters\n[\ni\n];\nSystem\n.\nout\n.\nprintln\n(\n\"Cluster Center \"\n+\ni\n+\n\": \"\n+\nclusterCenter\n);\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\" in the Spark repo.\nStreaming k-means\nWhen data arrive in a stream, we may want to estimate clusters dynamically,\nupdating them as new data arrive.\nspark.mllib\nprovides support for streaming k-means clustering,\nwith parameters to control the decay (or “forgetfulness”) of the estimates. The algorithm\nuses a generalization of the mini-batch k-means update rule. For each batch of data, we assign\nall points to their nearest cluster, compute new cluster centers, then update each cluster using:\n\\begin{equation}\n    c_{t+1} = \\frac{c_tn_t\\alpha + x_tm_t}{n_t\\alpha+m_t}\n\\end{equation}\n\\begin{equation}\n    n_{t+1} = n_t + m_t\n\\end{equation}\nWhere\n$c_t$\nis the previous center for the cluster,\n$n_t$\nis the number of points assigned\nto the cluster thus far,\n$x_t$\nis the new cluster center from the current batch, and\n$m_t$\nis the number of points added to the cluster in the current batch. The decay factor\n$\\alpha$\ncan be used to ignore the past: with\n$\\alpha$=1\nall data will be used from the beginning;\nwith\n$\\alpha$=0\nonly the most recent data will be used. This is analogous to an\nexponentially-weighted moving average.\nThe decay can be specified using a\nhalfLife\nparameter, which determines the\ncorrect decay factor\na\nsuch that, for data acquired\nat time\nt\n, its contribution by time\nt + halfLife\nwill have dropped to 0.5.\nThe unit of time can be specified either as\nbatches\nor\npoints\nand the update rule\nwill be adjusted accordingly.\nExamples\nThis example shows how to estimate clusters on streaming data.\nRefer to the\nStreamingKMeans\nPython docs\nfor more details on the API.\nAnd Refer to\nSpark Streaming Programming Guide\nfor details on StreamingContext.\nfrom\npyspark.mllib.linalg\nimport\nVectors\nfrom\npyspark.mllib.regression\nimport\nLabeledPoint\nfrom\npyspark.mllib.clustering\nimport\nStreamingKMeans\n# we make an input stream of vectors for training,\n# as well as a stream of vectors for testing\ndef\nparse\n(\nlp\n):\nlabel\n=\nfloat\n(\nlp\n[\nlp\n.\nfind\n(\n'\n(\n'\n)\n+\n1\n:\nlp\n.\nfind\n(\n'\n)\n'\n)])\nvec\n=\nVectors\n.\ndense\n(\nlp\n[\nlp\n.\nfind\n(\n'\n[\n'\n)\n+\n1\n:\nlp\n.\nfind\n(\n'\n]\n'\n)].\nsplit\n(\n'\n,\n'\n))\nreturn\nLabeledPoint\n(\nlabel\n,\nvec\n)\ntrainingData\n=\nsc\n.\ntextFile\n(\n\"\ndata/mllib/kmeans_data.txt\n\"\n)\n\\\n.\nmap\n(\nlambda\nline\n:\nVectors\n.\ndense\n([\nfloat\n(\nx\n)\nfor\nx\nin\nline\n.\nstrip\n().\nsplit\n(\n'\n'\n)]))\ntestingData\n=\nsc\n.\ntextFile\n(\n\"\ndata/mllib/streaming_kmeans_data_test.txt\n\"\n).\nmap\n(\nparse\n)\ntrainingQueue\n=\n[\ntrainingData\n]\ntestingQueue\n=\n[\ntestingData\n]\ntrainingStream\n=\nssc\n.\nqueueStream\n(\ntrainingQueue\n)\ntestingStream\n=\nssc\n.\nqueueStream\n(\ntestingQueue\n)\n# We create a model with random clusters and specify the number of clusters to find\nmodel\n=\nStreamingKMeans\n(\nk\n=\n2\n,\ndecayFactor\n=\n1.0\n).\nsetRandomCenters\n(\n3\n,\n1.0\n,\n0\n)\n# Now register the streams for training and testing and start the job,\n# printing the predicted cluster assignments on new data points as they arrive.\nmodel\n.\ntrainOn\n(\ntrainingStream\n)\nresult\n=\nmodel\n.\npredictOnValues\n(\ntestingStream\n.\nmap\n(\nlambda\nlp\n:\n(\nlp\n.\nlabel\n,\nlp\n.\nfeatures\n)))\nresult\n.\npprint\n()\nssc\n.\nstart\n()\nssc\n.\nstop\n(\nstopSparkContext\n=\nTrue\n,\nstopGraceFully\n=\nTrue\n)\nFind full example code at \"examples/src/main/python/mllib/streaming_k_means_example.py\" in the Spark repo.\nRefer to the\nStreamingKMeans\nScala docs\nfor details on the API.\nAnd Refer to\nSpark Streaming Programming Guide\nfor details on StreamingContext.\nimport\norg.apache.spark.mllib.clustering.StreamingKMeans\nimport\norg.apache.spark.mllib.linalg.Vectors\nimport\norg.apache.spark.mllib.regression.LabeledPoint\nimport\norg.apache.spark.streaming.\n{\nSeconds\n,\nStreamingContext\n}\nval\nconf\n=\nnew\nSparkConf\n().\nsetAppName\n(\n\"StreamingKMeansExample\"\n)\nval\nssc\n=\nnew\nStreamingContext\n(\nconf\n,\nSeconds\n(\nargs\n(\n2\n).\ntoLong\n))\nval\ntrainingData\n=\nssc\n.\ntextFileStream\n(\nargs\n(\n0\n)).\nmap\n(\nVectors\n.\nparse\n)\nval\ntestData\n=\nssc\n.\ntextFileStream\n(\nargs\n(\n1\n)).\nmap\n(\nLabeledPoint\n.\nparse\n)\nval\nmodel\n=\nnew\nStreamingKMeans\n()\n.\nsetK\n(\nargs\n(\n3\n).\ntoInt\n)\n.\nsetDecayFactor\n(\n1.0\n)\n.\nsetRandomCenters\n(\nargs\n(\n4\n).\ntoInt\n,\n0.0\n)\nmodel\n.\ntrainOn\n(\ntrainingData\n)\nmodel\n.\npredictOnValues\n(\ntestData\n.\nmap\n(\nlp\n=>\n(\nlp\n.\nlabel\n,\nlp\n.\nfeatures\n))).\nprint\n()\nssc\n.\nstart\n()\nssc\n.\nawaitTermination\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\" in the Spark repo.\nAs you add new text files with data the cluster centers will update. Each training\npoint should be formatted as\n[x1, x2, x3]\n, and each test data point\nshould be formatted as\n(y, [x1, x2, x3])\n, where\ny\nis some useful label or identifier\n(e.g. a true category assignment). Anytime a text file is placed in\n/training/data/dir\nthe model will update. Anytime a text file is placed in\n/testing/data/dir\nyou will see predictions. With new data, the cluster centers will change!"}
{"url": "https://spark.apache.org/docs/latest/sql-getting-started.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nStarting Point: SparkSession\nCreating DataFrames\nUntyped Dataset Operations (DataFrame operations)\nRunning SQL Queries Programmatically\nGlobal Temporary View\nCreating Datasets\nInteroperating with RDDs\nScalar Functions\nAggregate Functions\nData Sources\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nGetting Started\nStarting Point: SparkSession\nCreating DataFrames\nUntyped Dataset Operations (aka DataFrame Operations)\nRunning SQL Queries Programmatically\nGlobal Temporary View\nCreating Datasets\nInteroperating with RDDs\nInferring the Schema Using Reflection\nProgrammatically Specifying the Schema\nScalar Functions\nAggregate Functions\nStarting Point: SparkSession\nThe entry point into all functionality in Spark is the\nSparkSession\nclass. To create a basic\nSparkSession\n, just use\nSparkSession.builder\n:\nfrom\npyspark.sql\nimport\nSparkSession\nspark\n=\nSparkSession\n\\\n.\nbuilder\n\\\n.\nappName\n(\n\"\nPython Spark SQL basic example\n\"\n)\n\\\n.\nconfig\n(\n\"\nspark.some.config.option\n\"\n,\n\"\nsome-value\n\"\n)\n\\\n.\ngetOrCreate\n()\nFind full example code at \"examples/src/main/python/sql/basic.py\" in the Spark repo.\nThe entry point into all functionality in Spark is the\nSparkSession\nclass. To create a basic\nSparkSession\n, just use\nSparkSession.builder()\n:\nimport\norg.apache.spark.sql.SparkSession\nval\nspark\n=\nSparkSession\n.\nbuilder\n()\n.\nappName\n(\n\"Spark SQL basic example\"\n)\n.\nconfig\n(\n\"spark.some.config.option\"\n,\n\"some-value\"\n)\n.\ngetOrCreate\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\" in the Spark repo.\nThe entry point into all functionality in Spark is the\nSparkSession\nclass. To create a basic\nSparkSession\n, just use\nSparkSession.builder()\n:\nimport\norg.apache.spark.sql.SparkSession\n;\nSparkSession\nspark\n=\nSparkSession\n.\nbuilder\n()\n.\nappName\n(\n\"Java Spark SQL basic example\"\n)\n.\nconfig\n(\n\"spark.some.config.option\"\n,\n\"some-value\"\n)\n.\ngetOrCreate\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\" in the Spark repo.\nThe entry point into all functionality in Spark is the\nSparkSession\nclass. To initialize a basic\nSparkSession\n, just call\nsparkR.session()\n:\nsparkR.session\n(\nappName\n=\n\"R Spark SQL basic example\"\n,\nsparkConfig\n=\nlist\n(\nspark.some.config.option\n=\n\"some-value\"\n))\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nNote that when invoked for the first time,\nsparkR.session()\ninitializes a global\nSparkSession\nsingleton instance, and always returns a reference to this instance for successive invocations. In this way, users only need to initialize the\nSparkSession\nonce, then SparkR functions like\nread.df\nwill be able to access this global instance implicitly, and users don’t need to pass the\nSparkSession\ninstance around.\nSparkSession\nin Spark 2.0 provides builtin support for Hive features including the ability to\nwrite queries using HiveQL, access to Hive UDFs, and the ability to read data from Hive tables.\nTo use these features, you do not need to have an existing Hive setup.\nCreating DataFrames\nWith a\nSparkSession\n, applications can create DataFrames from an\nexisting\nRDD\n,\nfrom a Hive table, or from\nSpark data sources\n.\nAs an example, the following creates a DataFrame based on the content of a JSON file:\n# spark is an existing SparkSession\ndf\n=\nspark\n.\nread\n.\njson\n(\n\"\nexamples/src/main/resources/people.json\n\"\n)\n# Displays the content of the DataFrame to stdout\ndf\n.\nshow\n()\n# +----+-------+\n# | age|   name|\n# +----+-------+\n# |null|Michael|\n# |  30|   Andy|\n# |  19| Justin|\n# +----+-------+\nFind full example code at \"examples/src/main/python/sql/basic.py\" in the Spark repo.\nWith a\nSparkSession\n, applications can create DataFrames from an\nexisting\nRDD\n,\nfrom a Hive table, or from\nSpark data sources\n.\nAs an example, the following creates a DataFrame based on the content of a JSON file:\nval\ndf\n=\nspark\n.\nread\n.\njson\n(\n\"examples/src/main/resources/people.json\"\n)\n// Displays the content of the DataFrame to stdout\ndf\n.\nshow\n()\n// +----+-------+\n// | age|   name|\n// +----+-------+\n// |null|Michael|\n// |  30|   Andy|\n// |  19| Justin|\n// +----+-------+\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\" in the Spark repo.\nWith a\nSparkSession\n, applications can create DataFrames from an\nexisting\nRDD\n,\nfrom a Hive table, or from\nSpark data sources\n.\nAs an example, the following creates a DataFrame based on the content of a JSON file:\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\nread\n().\njson\n(\n\"examples/src/main/resources/people.json\"\n);\n// Displays the content of the DataFrame to stdout\ndf\n.\nshow\n();\n// +----+-------+\n// | age|   name|\n// +----+-------+\n// |null|Michael|\n// |  30|   Andy|\n// |  19| Justin|\n// +----+-------+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\" in the Spark repo.\nWith a\nSparkSession\n, applications can create DataFrames from a local R data.frame,\nfrom a Hive table, or from\nSpark data sources\n.\nAs an example, the following creates a DataFrame based on the content of a JSON file:\ndf\n<-\nread.json\n(\n\"examples/src/main/resources/people.json\"\n)\n# Displays the content of the DataFrame\nhead\n(\ndf\n)\n##   age    name\n## 1  NA Michael\n## 2  30    Andy\n## 3  19  Justin\n# Another method to print the first few rows and optionally truncate the printing of long values\nshowDF\n(\ndf\n)\n## +----+-------+\n## | age|   name|\n## +----+-------+\n## |null|Michael|\n## |  30|   Andy|\n## |  19| Justin|\n## +----+-------+\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nUntyped Dataset Operations (aka DataFrame Operations)\nDataFrames provide a domain-specific language for structured data manipulation in\nPython\n,\nScala\n,\nJava\nand\nR\n.\nAs mentioned above, in Spark 2.0, DataFrames are just Dataset of\nRow\ns in Scala and Java API. These operations are also referred as “untyped transformations” in contrast to “typed transformations” come with strongly typed Scala/Java Datasets.\nHere we include some basic examples of structured data processing using Datasets:\nIn Python, it’s possible to access a DataFrame’s columns either by attribute\n(\ndf.age\n) or by indexing (\ndf['age']\n). While the former is convenient for\ninteractive data exploration, users are highly encouraged to use the\nlatter form, which is future proof and won’t break with column names that\nare also attributes on the DataFrame class.\n# spark, df are from the previous example\n# Print the schema in a tree format\ndf\n.\nprintSchema\n()\n# root\n# |-- age: long (nullable = true)\n# |-- name: string (nullable = true)\n# Select only the \"name\" column\ndf\n.\nselect\n(\n\"\nname\n\"\n).\nshow\n()\n# +-------+\n# |   name|\n# +-------+\n# |Michael|\n# |   Andy|\n# | Justin|\n# +-------+\n# Select everybody, but increment the age by 1\ndf\n.\nselect\n(\ndf\n[\n'\nname\n'\n],\ndf\n[\n'\nage\n'\n]\n+\n1\n).\nshow\n()\n# +-------+---------+\n# |   name|(age + 1)|\n# +-------+---------+\n# |Michael|     null|\n# |   Andy|       31|\n# | Justin|       20|\n# +-------+---------+\n# Select people older than 21\ndf\n.\nfilter\n(\ndf\n[\n'\nage\n'\n]\n>\n21\n).\nshow\n()\n# +---+----+\n# |age|name|\n# +---+----+\n# | 30|Andy|\n# +---+----+\n# Count people by age\ndf\n.\ngroupBy\n(\n\"\nage\n\"\n).\ncount\n().\nshow\n()\n# +----+-----+\n# | age|count|\n# +----+-----+\n# |  19|    1|\n# |null|    1|\n# |  30|    1|\n# +----+-----+\nFind full example code at \"examples/src/main/python/sql/basic.py\" in the Spark repo.\nFor a complete list of the types of operations that can be performed on a DataFrame refer to the\nAPI Documentation\n.\nIn addition to simple column references and expressions, DataFrames also have a rich library of functions including string manipulation, date arithmetic, common math operations and more. The complete list is available in the\nDataFrame Function Reference\n.\n// This import is needed to use the $-notation\nimport\nspark.implicits._\n// Print the schema in a tree format\ndf\n.\nprintSchema\n()\n// root\n// |-- age: long (nullable = true)\n// |-- name: string (nullable = true)\n// Select only the \"name\" column\ndf\n.\nselect\n(\n\"name\"\n).\nshow\n()\n// +-------+\n// |   name|\n// +-------+\n// |Michael|\n// |   Andy|\n// | Justin|\n// +-------+\n// Select everybody, but increment the age by 1\ndf\n.\nselect\n(\n$\n\"name\"\n,\n$\n\"age\"\n+\n1\n).\nshow\n()\n// +-------+---------+\n// |   name|(age + 1)|\n// +-------+---------+\n// |Michael|     null|\n// |   Andy|       31|\n// | Justin|       20|\n// +-------+---------+\n// Select people older than 21\ndf\n.\nfilter\n(\n$\n\"age\"\n>\n21\n).\nshow\n()\n// +---+----+\n// |age|name|\n// +---+----+\n// | 30|Andy|\n// +---+----+\n// Count people by age\ndf\n.\ngroupBy\n(\n\"age\"\n).\ncount\n().\nshow\n()\n// +----+-----+\n// | age|count|\n// +----+-----+\n// |  19|    1|\n// |null|    1|\n// |  30|    1|\n// +----+-----+\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\" in the Spark repo.\nFor a complete list of the types of operations that can be performed on a Dataset, refer to the\nAPI Documentation\n.\nIn addition to simple column references and expressions, Datasets also have a rich library of functions including string manipulation, date arithmetic, common math operations and more. The complete list is available in the\nDataFrame Function Reference\n.\n// col(\"...\") is preferable to df.col(\"...\")\nimport\nstatic\norg\n.\napache\n.\nspark\n.\nsql\n.\nfunctions\n.\ncol\n;\n// Print the schema in a tree format\ndf\n.\nprintSchema\n();\n// root\n// |-- age: long (nullable = true)\n// |-- name: string (nullable = true)\n// Select only the \"name\" column\ndf\n.\nselect\n(\n\"name\"\n).\nshow\n();\n// +-------+\n// |   name|\n// +-------+\n// |Michael|\n// |   Andy|\n// | Justin|\n// +-------+\n// Select everybody, but increment the age by 1\ndf\n.\nselect\n(\ncol\n(\n\"name\"\n),\ncol\n(\n\"age\"\n).\nplus\n(\n1\n)).\nshow\n();\n// +-------+---------+\n// |   name|(age + 1)|\n// +-------+---------+\n// |Michael|     null|\n// |   Andy|       31|\n// | Justin|       20|\n// +-------+---------+\n// Select people older than 21\ndf\n.\nfilter\n(\ncol\n(\n\"age\"\n).\ngt\n(\n21\n)).\nshow\n();\n// +---+----+\n// |age|name|\n// +---+----+\n// | 30|Andy|\n// +---+----+\n// Count people by age\ndf\n.\ngroupBy\n(\n\"age\"\n).\ncount\n().\nshow\n();\n// +----+-----+\n// | age|count|\n// +----+-----+\n// |  19|    1|\n// |null|    1|\n// |  30|    1|\n// +----+-----+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\" in the Spark repo.\nFor a complete list of the types of operations that can be performed on a Dataset refer to the\nAPI Documentation\n.\nIn addition to simple column references and expressions, Datasets also have a rich library of functions including string manipulation, date arithmetic, common math operations and more. The complete list is available in the\nDataFrame Function Reference\n.\n# Create the DataFrame\ndf\n<-\nread.json\n(\n\"examples/src/main/resources/people.json\"\n)\n# Show the content of the DataFrame\nhead\n(\ndf\n)\n##   age    name\n## 1  NA Michael\n## 2  30    Andy\n## 3  19  Justin\n# Print the schema in a tree format\nprintSchema\n(\ndf\n)\n## root\n## |-- age: long (nullable = true)\n## |-- name: string (nullable = true)\n# Select only the \"name\" column\nhead\n(\nselect\n(\ndf\n,\n\"name\"\n))\n##      name\n## 1 Michael\n## 2    Andy\n## 3  Justin\n# Select everybody, but increment the age by 1\nhead\n(\nselect\n(\ndf\n,\ndf\n$\nname\n,\ndf\n$\nage\n+\n1\n))\n##      name (age + 1.0)\n## 1 Michael          NA\n## 2    Andy          31\n## 3  Justin          20\n# Select people older than 21\nhead\n(\nwhere\n(\ndf\n,\ndf\n$\nage\n>\n21\n))\n##   age name\n## 1  30 Andy\n# Count people by age\nhead\n(\ncount\n(\ngroupBy\n(\ndf\n,\n\"age\"\n)))\n##   age count\n## 1  19     1\n## 2  NA     1\n## 3  30     1\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nFor a complete list of the types of operations that can be performed on a DataFrame refer to the\nAPI Documentation\n.\nIn addition to simple column references and expressions, DataFrames also have a rich library of functions including string manipulation, date arithmetic, common math operations and more. The complete list is available in the\nDataFrame Function Reference\n.\nRunning SQL Queries Programmatically\nThe\nsql\nfunction on a\nSparkSession\nenables applications to run SQL queries programmatically and returns the result as a\nDataFrame\n.\n# Register the DataFrame as a SQL temporary view\ndf\n.\ncreateOrReplaceTempView\n(\n\"\npeople\n\"\n)\nsqlDF\n=\nspark\n.\nsql\n(\n\"\nSELECT * FROM people\n\"\n)\nsqlDF\n.\nshow\n()\n# +----+-------+\n# | age|   name|\n# +----+-------+\n# |null|Michael|\n# |  30|   Andy|\n# |  19| Justin|\n# +----+-------+\nFind full example code at \"examples/src/main/python/sql/basic.py\" in the Spark repo.\nThe\nsql\nfunction on a\nSparkSession\nenables applications to run SQL queries programmatically and returns the result as a\nDataFrame\n.\n// Register the DataFrame as a SQL temporary view\ndf\n.\ncreateOrReplaceTempView\n(\n\"people\"\n)\nval\nsqlDF\n=\nspark\n.\nsql\n(\n\"SELECT * FROM people\"\n)\nsqlDF\n.\nshow\n()\n// +----+-------+\n// | age|   name|\n// +----+-------+\n// |null|Michael|\n// |  30|   Andy|\n// |  19| Justin|\n// +----+-------+\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\" in the Spark repo.\nThe\nsql\nfunction on a\nSparkSession\nenables applications to run SQL queries programmatically and returns the result as a\nDataset<Row>\n.\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\n// Register the DataFrame as a SQL temporary view\ndf\n.\ncreateOrReplaceTempView\n(\n\"people\"\n);\nDataset\n<\nRow\n>\nsqlDF\n=\nspark\n.\nsql\n(\n\"SELECT * FROM people\"\n);\nsqlDF\n.\nshow\n();\n// +----+-------+\n// | age|   name|\n// +----+-------+\n// |null|Michael|\n// |  30|   Andy|\n// |  19| Justin|\n// +----+-------+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\" in the Spark repo.\nThe\nsql\nfunction enables applications to run SQL queries programmatically and returns the result as a\nSparkDataFrame\n.\ndf\n<-\nsql\n(\n\"SELECT * FROM table\"\n)\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nGlobal Temporary View\nTemporary views in Spark SQL are session-scoped and will disappear if the session that creates it\nterminates. If you want to have a temporary view that is shared among all sessions and keep alive\nuntil the Spark application terminates, you can create a global temporary view. Global temporary\nview is tied to a system preserved database\nglobal_temp\n, and we must use the qualified name to\nrefer it, e.g.\nSELECT * FROM global_temp.view1\n.\n# Register the DataFrame as a global temporary view\ndf\n.\ncreateGlobalTempView\n(\n\"\npeople\n\"\n)\n# Global temporary view is tied to a system preserved database `global_temp`\nspark\n.\nsql\n(\n\"\nSELECT * FROM global_temp.people\n\"\n).\nshow\n()\n# +----+-------+\n# | age|   name|\n# +----+-------+\n# |null|Michael|\n# |  30|   Andy|\n# |  19| Justin|\n# +----+-------+\n# Global temporary view is cross-session\nspark\n.\nnewSession\n().\nsql\n(\n\"\nSELECT * FROM global_temp.people\n\"\n).\nshow\n()\n# +----+-------+\n# | age|   name|\n# +----+-------+\n# |null|Michael|\n# |  30|   Andy|\n# |  19| Justin|\n# +----+-------+\nFind full example code at \"examples/src/main/python/sql/basic.py\" in the Spark repo.\n// Register the DataFrame as a global temporary view\ndf\n.\ncreateGlobalTempView\n(\n\"people\"\n)\n// Global temporary view is tied to a system preserved database `global_temp`\nspark\n.\nsql\n(\n\"SELECT * FROM global_temp.people\"\n).\nshow\n()\n// +----+-------+\n// | age|   name|\n// +----+-------+\n// |null|Michael|\n// |  30|   Andy|\n// |  19| Justin|\n// +----+-------+\n// Global temporary view is cross-session\nspark\n.\nnewSession\n().\nsql\n(\n\"SELECT * FROM global_temp.people\"\n).\nshow\n()\n// +----+-------+\n// | age|   name|\n// +----+-------+\n// |null|Michael|\n// |  30|   Andy|\n// |  19| Justin|\n// +----+-------+\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\" in the Spark repo.\n// Register the DataFrame as a global temporary view\ndf\n.\ncreateGlobalTempView\n(\n\"people\"\n);\n// Global temporary view is tied to a system preserved database `global_temp`\nspark\n.\nsql\n(\n\"SELECT * FROM global_temp.people\"\n).\nshow\n();\n// +----+-------+\n// | age|   name|\n// +----+-------+\n// |null|Michael|\n// |  30|   Andy|\n// |  19| Justin|\n// +----+-------+\n// Global temporary view is cross-session\nspark\n.\nnewSession\n().\nsql\n(\n\"SELECT * FROM global_temp.people\"\n).\nshow\n();\n// +----+-------+\n// | age|   name|\n// +----+-------+\n// |null|Michael|\n// |  30|   Andy|\n// |  19| Justin|\n// +----+-------+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\" in the Spark repo.\nCREATE\nGLOBAL\nTEMPORARY\nVIEW\ntemp_view\nAS\nSELECT\na\n+\n1\n,\nb\n*\n2\nFROM\ntbl\nSELECT\n*\nFROM\nglobal_temp\n.\ntemp_view\nCreating Datasets\nDatasets are similar to RDDs, however, instead of using Java serialization or Kryo they use\na specialized\nEncoder\nto serialize the objects\nfor processing or transmitting over the network. While both encoders and standard serialization are\nresponsible for turning an object into bytes, encoders are code generated dynamically and use a format\nthat allows Spark to perform many operations like filtering, sorting and hashing without deserializing\nthe bytes back into an object.\ncase\nclass\nPerson\n(\nname\n:\nString\n,\nage\n:\nLong\n)\n// Encoders are created for case classes\nval\ncaseClassDS\n=\nSeq\n(\nPerson\n(\n\"Andy\"\n,\n32\n)).\ntoDS\n()\ncaseClassDS\n.\nshow\n()\n// +----+---+\n// |name|age|\n// +----+---+\n// |Andy| 32|\n// +----+---+\n// Encoders for most common types are automatically provided by importing spark.implicits._\nval\nprimitiveDS\n=\nSeq\n(\n1\n,\n2\n,\n3\n).\ntoDS\n()\nprimitiveDS\n.\nmap\n(\n_\n+\n1\n).\ncollect\n()\n// Returns: Array(2, 3, 4)\n// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name\nval\npath\n=\n\"examples/src/main/resources/people.json\"\nval\npeopleDS\n=\nspark\n.\nread\n.\njson\n(\npath\n).\nas\n[\nPerson\n]\npeopleDS\n.\nshow\n()\n// +----+-------+\n// | age|   name|\n// +----+-------+\n// |null|Michael|\n// |  30|   Andy|\n// |  19| Justin|\n// +----+-------+\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\" in the Spark repo.\nimport\njava.util.Arrays\n;\nimport\njava.util.Collections\n;\nimport\njava.io.Serializable\n;\nimport\norg.apache.spark.api.java.function.MapFunction\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.Encoder\n;\nimport\norg.apache.spark.sql.Encoders\n;\npublic\nstatic\nclass\nPerson\nimplements\nSerializable\n{\nprivate\nString\nname\n;\nprivate\nlong\nage\n;\npublic\nString\ngetName\n()\n{\nreturn\nname\n;\n}\npublic\nvoid\nsetName\n(\nString\nname\n)\n{\nthis\n.\nname\n=\nname\n;\n}\npublic\nlong\ngetAge\n()\n{\nreturn\nage\n;\n}\npublic\nvoid\nsetAge\n(\nlong\nage\n)\n{\nthis\n.\nage\n=\nage\n;\n}\n}\n// Create an instance of a Bean class\nPerson\nperson\n=\nnew\nPerson\n();\nperson\n.\nsetName\n(\n\"Andy\"\n);\nperson\n.\nsetAge\n(\n32\n);\n// Encoders are created for Java beans\nEncoder\n<\nPerson\n>\npersonEncoder\n=\nEncoders\n.\nbean\n(\nPerson\n.\nclass\n);\nDataset\n<\nPerson\n>\njavaBeanDS\n=\nspark\n.\ncreateDataset\n(\nCollections\n.\nsingletonList\n(\nperson\n),\npersonEncoder\n);\njavaBeanDS\n.\nshow\n();\n// +---+----+\n// |age|name|\n// +---+----+\n// | 32|Andy|\n// +---+----+\n// Encoders for most common types are provided in class Encoders\nEncoder\n<\nLong\n>\nlongEncoder\n=\nEncoders\n.\nLONG\n();\nDataset\n<\nLong\n>\nprimitiveDS\n=\nspark\n.\ncreateDataset\n(\nArrays\n.\nasList\n(\n1L\n,\n2L\n,\n3L\n),\nlongEncoder\n);\nDataset\n<\nLong\n>\ntransformedDS\n=\nprimitiveDS\n.\nmap\n(\n(\nMapFunction\n<\nLong\n,\nLong\n>)\nvalue\n->\nvalue\n+\n1L\n,\nlongEncoder\n);\ntransformedDS\n.\ncollect\n();\n// Returns [2, 3, 4]\n// DataFrames can be converted to a Dataset by providing a class. Mapping based on name\nString\npath\n=\n\"examples/src/main/resources/people.json\"\n;\nDataset\n<\nPerson\n>\npeopleDS\n=\nspark\n.\nread\n().\njson\n(\npath\n).\nas\n(\npersonEncoder\n);\npeopleDS\n.\nshow\n();\n// +----+-------+\n// | age|   name|\n// +----+-------+\n// |null|Michael|\n// |  30|   Andy|\n// |  19| Justin|\n// +----+-------+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\" in the Spark repo.\nInteroperating with RDDs\nSpark SQL supports two different methods for converting existing RDDs into Datasets. The first\nmethod uses reflection to infer the schema of an RDD that contains specific types of objects. This\nreflection-based approach leads to more concise code and works well when you already know the schema\nwhile writing your Spark application.\nThe second method for creating Datasets is through a programmatic interface that allows you to\nconstruct a schema and then apply it to an existing RDD. While this method is more verbose, it allows\nyou to construct Datasets when the columns and their types are not known until runtime.\nInferring the Schema Using Reflection\nSpark SQL can convert an RDD of Row objects to a DataFrame, inferring the datatypes. Rows are constructed by passing a list of\nkey/value pairs as kwargs to the Row class. The keys of this list define the column names of the table,\nand the types are inferred by sampling the whole dataset, similar to the inference that is performed on JSON files.\nfrom\npyspark.sql\nimport\nRow\nsc\n=\nspark\n.\nsparkContext\n# Load a text file and convert each line to a Row.\nlines\n=\nsc\n.\ntextFile\n(\n\"\nexamples/src/main/resources/people.txt\n\"\n)\nparts\n=\nlines\n.\nmap\n(\nlambda\nl\n:\nl\n.\nsplit\n(\n\"\n,\n\"\n))\npeople\n=\nparts\n.\nmap\n(\nlambda\np\n:\nRow\n(\nname\n=\np\n[\n0\n],\nage\n=\nint\n(\np\n[\n1\n])))\n# Infer the schema, and register the DataFrame as a table.\nschemaPeople\n=\nspark\n.\ncreateDataFrame\n(\npeople\n)\nschemaPeople\n.\ncreateOrReplaceTempView\n(\n\"\npeople\n\"\n)\n# SQL can be run over DataFrames that have been registered as a table.\nteenagers\n=\nspark\n.\nsql\n(\n\"\nSELECT name FROM people WHERE age >= 13 AND age <= 19\n\"\n)\n# The results of SQL queries are Dataframe objects.\n# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\nteenNames\n=\nteenagers\n.\nrdd\n.\nmap\n(\nlambda\np\n:\n\"\nName:\n\"\n+\np\n.\nname\n).\ncollect\n()\nfor\nname\nin\nteenNames\n:\nprint\n(\nname\n)\n# Name: Justin\nFind full example code at \"examples/src/main/python/sql/basic.py\" in the Spark repo.\nThe Scala interface for Spark SQL supports automatically converting an RDD containing case classes\nto a DataFrame. The case class\ndefines the schema of the table. The names of the arguments to the case class are read using\nreflection and become the names of the columns. Case classes can also be nested or contain complex\ntypes such as\nSeq\ns or\nArray\ns. This RDD can be implicitly converted to a DataFrame and then be\nregistered as a table. Tables can be used in subsequent SQL statements.\n// For implicit conversions from RDDs to DataFrames\nimport\nspark.implicits._\n// Create an RDD of Person objects from a text file, convert it to a Dataframe\nval\npeopleDF\n=\nspark\n.\nsparkContext\n.\ntextFile\n(\n\"examples/src/main/resources/people.txt\"\n)\n.\nmap\n(\n_\n.\nsplit\n(\n\",\"\n))\n.\nmap\n(\nattributes\n=>\nPerson\n(\nattributes\n(\n0\n),\nattributes\n(\n1\n).\ntrim\n.\ntoInt\n))\n.\ntoDF\n()\n// Register the DataFrame as a temporary view\npeopleDF\n.\ncreateOrReplaceTempView\n(\n\"people\"\n)\n// SQL statements can be run by using the sql methods provided by Spark\nval\nteenagersDF\n=\nspark\n.\nsql\n(\n\"SELECT name, age FROM people WHERE age BETWEEN 13 AND 19\"\n)\n// The columns of a row in the result can be accessed by field index\nteenagersDF\n.\nmap\n(\nteenager\n=>\n\"Name: \"\n+\nteenager\n(\n0\n)).\nshow\n()\n// +------------+\n// |       value|\n// +------------+\n// |Name: Justin|\n// +------------+\n// or by field name\nteenagersDF\n.\nmap\n(\nteenager\n=>\n\"Name: \"\n+\nteenager\n.\ngetAs\n[\nString\n](\n\"name\"\n)).\nshow\n()\n// +------------+\n// |       value|\n// +------------+\n// |Name: Justin|\n// +------------+\n// No pre-defined encoders for Dataset[Map[K,V]], define explicitly\nimplicit\nval\nmapEncoder\n:\nEncoder\n[\nMap\n[\nString\n,\nAny\n]]\n=\norg\n.\napache\n.\nspark\n.\nsql\n.\nEncoders\n.\nkryo\n[\nMap\n[\nString\n,\nAny\n]]\n// Primitive types and case classes can be also defined as\n// implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()\n// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]\nteenagersDF\n.\nmap\n(\nteenager\n=>\nteenager\n.\ngetValuesMap\n[\nAny\n](\nList\n(\n\"name\"\n,\n\"age\"\n))).\ncollect\n()\n// Array(Map(\"name\" -> \"Justin\", \"age\" -> 19))\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\" in the Spark repo.\nSpark SQL supports automatically converting an RDD of\nJavaBeans\ninto a DataFrame.\nThe\nBeanInfo\n, obtained using reflection, defines the schema of the table. Currently, Spark SQL\ndoes not support JavaBeans that contain\nMap\nfield(s). Nested JavaBeans and\nList\nor\nArray\nfields are supported though. You can create a JavaBean by creating a class that implements\nSerializable and has getters and setters for all of its fields.\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.api.java.function.Function\n;\nimport\norg.apache.spark.api.java.function.MapFunction\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.Encoder\n;\nimport\norg.apache.spark.sql.Encoders\n;\n// Create an RDD of Person objects from a text file\nJavaRDD\n<\nPerson\n>\npeopleRDD\n=\nspark\n.\nread\n()\n.\ntextFile\n(\n\"examples/src/main/resources/people.txt\"\n)\n.\njavaRDD\n()\n.\nmap\n(\nline\n->\n{\nString\n[]\nparts\n=\nline\n.\nsplit\n(\n\",\"\n);\nPerson\nperson\n=\nnew\nPerson\n();\nperson\n.\nsetName\n(\nparts\n[\n0\n]);\nperson\n.\nsetAge\n(\nInteger\n.\nparseInt\n(\nparts\n[\n1\n].\ntrim\n()));\nreturn\nperson\n;\n});\n// Apply a schema to an RDD of JavaBeans to get a DataFrame\nDataset\n<\nRow\n>\npeopleDF\n=\nspark\n.\ncreateDataFrame\n(\npeopleRDD\n,\nPerson\n.\nclass\n);\n// Register the DataFrame as a temporary view\npeopleDF\n.\ncreateOrReplaceTempView\n(\n\"people\"\n);\n// SQL statements can be run by using the sql methods provided by spark\nDataset\n<\nRow\n>\nteenagersDF\n=\nspark\n.\nsql\n(\n\"SELECT name FROM people WHERE age BETWEEN 13 AND 19\"\n);\n// The columns of a row in the result can be accessed by field index\nEncoder\n<\nString\n>\nstringEncoder\n=\nEncoders\n.\nSTRING\n();\nDataset\n<\nString\n>\nteenagerNamesByIndexDF\n=\nteenagersDF\n.\nmap\n(\n(\nMapFunction\n<\nRow\n,\nString\n>)\nrow\n->\n\"Name: \"\n+\nrow\n.\ngetString\n(\n0\n),\nstringEncoder\n);\nteenagerNamesByIndexDF\n.\nshow\n();\n// +------------+\n// |       value|\n// +------------+\n// |Name: Justin|\n// +------------+\n// or by field name\nDataset\n<\nString\n>\nteenagerNamesByFieldDF\n=\nteenagersDF\n.\nmap\n(\n(\nMapFunction\n<\nRow\n,\nString\n>)\nrow\n->\n\"Name: \"\n+\nrow\n.<\nString\n>\ngetAs\n(\n\"name\"\n),\nstringEncoder\n);\nteenagerNamesByFieldDF\n.\nshow\n();\n// +------------+\n// |       value|\n// +------------+\n// |Name: Justin|\n// +------------+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\" in the Spark repo.\nProgrammatically Specifying the Schema\nWhen a dictionary of kwargs cannot be defined ahead of time (for example,\nthe structure of records is encoded in a string, or a text dataset will be parsed and\nfields will be projected differently for different users),\na\nDataFrame\ncan be created programmatically with three steps.\nCreate an RDD of tuples or lists from the original RDD;\nCreate the schema represented by a\nStructType\nmatching the structure of\ntuples or lists in the RDD created in the step 1.\nApply the schema to the RDD via\ncreateDataFrame\nmethod provided by\nSparkSession\n.\nFor example:\n# Import data types\nfrom\npyspark.sql.types\nimport\nStringType\n,\nStructType\n,\nStructField\nsc\n=\nspark\n.\nsparkContext\n# Load a text file and convert each line to a Row.\nlines\n=\nsc\n.\ntextFile\n(\n\"\nexamples/src/main/resources/people.txt\n\"\n)\nparts\n=\nlines\n.\nmap\n(\nlambda\nl\n:\nl\n.\nsplit\n(\n\"\n,\n\"\n))\n# Each line is converted to a tuple.\npeople\n=\nparts\n.\nmap\n(\nlambda\np\n:\n(\np\n[\n0\n],\np\n[\n1\n].\nstrip\n()))\n# The schema is encoded in a string.\nschemaString\n=\n\"\nname age\n\"\nfields\n=\n[\nStructField\n(\nfield_name\n,\nStringType\n(),\nTrue\n)\nfor\nfield_name\nin\nschemaString\n.\nsplit\n()]\nschema\n=\nStructType\n(\nfields\n)\n# Apply the schema to the RDD.\nschemaPeople\n=\nspark\n.\ncreateDataFrame\n(\npeople\n,\nschema\n)\n# Creates a temporary view using the DataFrame\nschemaPeople\n.\ncreateOrReplaceTempView\n(\n\"\npeople\n\"\n)\n# SQL can be run over DataFrames that have been registered as a table.\nresults\n=\nspark\n.\nsql\n(\n\"\nSELECT name FROM people\n\"\n)\nresults\n.\nshow\n()\n# +-------+\n# |   name|\n# +-------+\n# |Michael|\n# |   Andy|\n# | Justin|\n# +-------+\nFind full example code at \"examples/src/main/python/sql/basic.py\" in the Spark repo.\nWhen case classes cannot be defined ahead of time (for example,\nthe structure of records is encoded in a string, or a text dataset will be parsed\nand fields will be projected differently for different users),\na\nDataFrame\ncan be created programmatically with three steps.\nCreate an RDD of\nRow\ns from the original RDD;\nCreate the schema represented by a\nStructType\nmatching the structure of\nRow\ns in the RDD created in Step 1.\nApply the schema to the RDD of\nRow\ns via\ncreateDataFrame\nmethod provided\nby\nSparkSession\n.\nFor example:\nimport\norg.apache.spark.sql.\n{\nEncoder\n,\nRow\n}\nimport\norg.apache.spark.sql.types._\n// Create an RDD\nval\npeopleRDD\n=\nspark\n.\nsparkContext\n.\ntextFile\n(\n\"examples/src/main/resources/people.txt\"\n)\n// The schema is encoded in a string\nval\nschemaString\n=\n\"name age\"\n// Generate the schema based on the string of schema\nval\nfields\n=\nschemaString\n.\nsplit\n(\n\" \"\n)\n.\nmap\n(\nfieldName\n=>\nStructField\n(\nfieldName\n,\nStringType\n,\nnullable\n=\ntrue\n))\nval\nschema\n=\nStructType\n(\nfields\n)\n// Convert records of the RDD (people) to Rows\nval\nrowRDD\n=\npeopleRDD\n.\nmap\n(\n_\n.\nsplit\n(\n\",\"\n))\n.\nmap\n(\nattributes\n=>\nRow\n(\nattributes\n(\n0\n),\nattributes\n(\n1\n).\ntrim\n))\n// Apply the schema to the RDD\nval\npeopleDF\n=\nspark\n.\ncreateDataFrame\n(\nrowRDD\n,\nschema\n)\n// Creates a temporary view using the DataFrame\npeopleDF\n.\ncreateOrReplaceTempView\n(\n\"people\"\n)\n// SQL can be run over a temporary view created using DataFrames\nval\nresults\n=\nspark\n.\nsql\n(\n\"SELECT name FROM people\"\n)\n// The results of SQL queries are DataFrames and support all the normal RDD operations\n// The columns of a row in the result can be accessed by field index or by field name\nresults\n.\nmap\n(\nattributes\n=>\n\"Name: \"\n+\nattributes\n(\n0\n)).\nshow\n()\n// +-------------+\n// |        value|\n// +-------------+\n// |Name: Michael|\n// |   Name: Andy|\n// | Name: Justin|\n// +-------------+\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\" in the Spark repo.\nWhen JavaBean classes cannot be defined ahead of time (for example,\nthe structure of records is encoded in a string, or a text dataset will be parsed and\nfields will be projected differently for different users),\na\nDataset<Row>\ncan be created programmatically with three steps.\nCreate an RDD of\nRow\ns from the original RDD;\nCreate the schema represented by a\nStructType\nmatching the structure of\nRow\ns in the RDD created in Step 1.\nApply the schema to the RDD of\nRow\ns via\ncreateDataFrame\nmethod provided\nby\nSparkSession\n.\nFor example:\nimport\njava.util.ArrayList\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.api.java.function.Function\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\n// Create an RDD\nJavaRDD\n<\nString\n>\npeopleRDD\n=\nspark\n.\nsparkContext\n()\n.\ntextFile\n(\n\"examples/src/main/resources/people.txt\"\n,\n1\n)\n.\ntoJavaRDD\n();\n// The schema is encoded in a string\nString\nschemaString\n=\n\"name age\"\n;\n// Generate the schema based on the string of schema\nList\n<\nStructField\n>\nfields\n=\nnew\nArrayList\n<>();\nfor\n(\nString\nfieldName\n:\nschemaString\n.\nsplit\n(\n\" \"\n))\n{\nStructField\nfield\n=\nDataTypes\n.\ncreateStructField\n(\nfieldName\n,\nDataTypes\n.\nStringType\n,\ntrue\n);\nfields\n.\nadd\n(\nfield\n);\n}\nStructType\nschema\n=\nDataTypes\n.\ncreateStructType\n(\nfields\n);\n// Convert records of the RDD (people) to Rows\nJavaRDD\n<\nRow\n>\nrowRDD\n=\npeopleRDD\n.\nmap\n((\nFunction\n<\nString\n,\nRow\n>)\nrecord\n->\n{\nString\n[]\nattributes\n=\nrecord\n.\nsplit\n(\n\",\"\n);\nreturn\nRowFactory\n.\ncreate\n(\nattributes\n[\n0\n],\nattributes\n[\n1\n].\ntrim\n());\n});\n// Apply the schema to the RDD\nDataset\n<\nRow\n>\npeopleDataFrame\n=\nspark\n.\ncreateDataFrame\n(\nrowRDD\n,\nschema\n);\n// Creates a temporary view using the DataFrame\npeopleDataFrame\n.\ncreateOrReplaceTempView\n(\n\"people\"\n);\n// SQL can be run over a temporary view created using DataFrames\nDataset\n<\nRow\n>\nresults\n=\nspark\n.\nsql\n(\n\"SELECT name FROM people\"\n);\n// The results of SQL queries are DataFrames and support all the normal RDD operations\n// The columns of a row in the result can be accessed by field index or by field name\nDataset\n<\nString\n>\nnamesDS\n=\nresults\n.\nmap\n(\n(\nMapFunction\n<\nRow\n,\nString\n>)\nrow\n->\n\"Name: \"\n+\nrow\n.\ngetString\n(\n0\n),\nEncoders\n.\nSTRING\n());\nnamesDS\n.\nshow\n();\n// +-------------+\n// |        value|\n// +-------------+\n// |Name: Michael|\n// |   Name: Andy|\n// | Name: Justin|\n// +-------------+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\" in the Spark repo.\nScalar Functions\nScalar functions are functions that return a single value per row, as opposed to aggregation functions, which return a value for a group of rows. Spark SQL supports a variety of\nBuilt-in Scalar Functions\n. It also supports\nUser Defined Scalar Functions\n.\nAggregate Functions\nAggregate functions are functions that return a single value on a group of rows. The\nBuilt-in Aggregate Functions\nprovide common aggregations such as\ncount()\n,\ncount_distinct()\n,\navg()\n,\nmax()\n,\nmin()\n, etc.\nUsers are not limited to the predefined aggregate functions and can create their own. For more details\nabout user defined aggregate functions, please refer to the documentation of\nUser Defined Aggregate Functions\n."}
{"url": "https://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nsingular value decomposition (SVD)\nprincipal component analysis (PCA)\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nDimensionality Reduction - RDD-based API\nSingular value decomposition (SVD)\nPerformance\nSVD Example\nPrincipal component analysis (PCA)\nDimensionality reduction\nis the process \nof reducing the number of variables under consideration.\nIt can be used to extract latent features from raw and noisy features\nor compress data while maintaining the structure.\nspark.mllib\nprovides support for dimensionality reduction on the\nRowMatrix\nclass.\nSingular value decomposition (SVD)\nSingular value decomposition (SVD)\nfactorizes a matrix into three matrices: $U$, $\\Sigma$, and $V$ such that\n\\[\nA = U \\Sigma V^T,\n\\]\nwhere\n$U$ is an orthonormal matrix, whose columns are called left singular vectors,\n$\\Sigma$ is a diagonal matrix with non-negative diagonals in descending order, \nwhose diagonals are called singular values,\n$V$ is an orthonormal matrix, whose columns are called right singular vectors.\nFor large matrices, usually we don’t need the complete factorization but only the top singular\nvalues and its associated singular vectors.  This can save storage, de-noise\nand recover the low-rank structure of the matrix.\nIf we keep the top $k$ singular values, then the dimensions of the resulting low-rank matrix will be:\n$U$\n:\n$m \\times k$\n,\n$\\Sigma$\n:\n$k \\times k$\n,\n$V$\n:\n$n \\times k$\n.\nPerformance\nWe assume $n$ is smaller than $m$. The singular values and the right singular vectors are derived\nfrom the eigenvalues and the eigenvectors of the Gramian matrix $A^T A$. The matrix\nstoring the left singular vectors $U$, is computed via matrix multiplication as\n$U = A (V S^{-1})$, if requested by the user via the computeU parameter. \nThe actual method to use is determined automatically based on the computational cost:\nIf $n$ is small ($n < 100$) or $k$ is large compared with $n$ ($k > n / 2$), we compute the Gramian matrix\nfirst and then compute its top eigenvalues and eigenvectors locally on the driver.\nThis requires a single pass with $O(n^2)$ storage on each executor and on the driver, and\n$O(n^2 k)$ time on the driver.\nOtherwise, we compute $(A^T A) v$ in a distributive way and send it to\nARPACK\nto\ncompute $(A^T A)$’s top eigenvalues and eigenvectors on the driver node. This requires $O(k)$\npasses, $O(n)$ storage on each executor, and $O(n k)$ storage on the driver.\nSVD Example\nspark.mllib\nprovides SVD functionality to row-oriented matrices, provided in the\nRowMatrix\nclass.\nRefer to the\nSingularValueDecomposition\nPython docs\nfor details on the API.\nfrom\npyspark.mllib.linalg\nimport\nVectors\nfrom\npyspark.mllib.linalg.distributed\nimport\nRowMatrix\nrows\n=\nsc\n.\nparallelize\n([\nVectors\n.\nsparse\n(\n5\n,\n{\n1\n:\n1.0\n,\n3\n:\n7.0\n}),\nVectors\n.\ndense\n(\n2.0\n,\n0.0\n,\n3.0\n,\n4.0\n,\n5.0\n),\nVectors\n.\ndense\n(\n4.0\n,\n0.0\n,\n0.0\n,\n6.0\n,\n7.0\n)\n])\nmat\n=\nRowMatrix\n(\nrows\n)\n# Compute the top 5 singular values and corresponding singular vectors.\nsvd\n=\nmat\n.\ncomputeSVD\n(\n5\n,\ncomputeU\n=\nTrue\n)\nU\n=\nsvd\n.\nU\n# The U factor is a RowMatrix.\ns\n=\nsvd\n.\ns\n# The singular values are stored in a local dense vector.\nV\n=\nsvd\n.\nV\n# The V factor is a local dense matrix.\nFind full example code at \"examples/src/main/python/mllib/svd_example.py\" in the Spark repo.\nThe same code applies to\nIndexedRowMatrix\nif\nU\nis defined as an\nIndexedRowMatrix\n.\nRefer to the\nSingularValueDecomposition\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.linalg.Matrix\nimport\norg.apache.spark.mllib.linalg.SingularValueDecomposition\nimport\norg.apache.spark.mllib.linalg.Vector\nimport\norg.apache.spark.mllib.linalg.Vectors\nimport\norg.apache.spark.mllib.linalg.distributed.RowMatrix\nval\ndata\n=\nArray\n(\nVectors\n.\nsparse\n(\n5\n,\nSeq\n((\n1\n,\n1.0\n),\n(\n3\n,\n7.0\n))),\nVectors\n.\ndense\n(\n2.0\n,\n0.0\n,\n3.0\n,\n4.0\n,\n5.0\n),\nVectors\n.\ndense\n(\n4.0\n,\n0.0\n,\n0.0\n,\n6.0\n,\n7.0\n))\nval\nrows\n=\nsc\n.\nparallelize\n(\nimmutable\n.\nArraySeq\n.\nunsafeWrapArray\n(\ndata\n))\nval\nmat\n:\nRowMatrix\n=\nnew\nRowMatrix\n(\nrows\n)\n// Compute the top 5 singular values and corresponding singular vectors.\nval\nsvd\n:\nSingularValueDecomposition\n[\nRowMatrix\n,\nMatrix\n]\n=\nmat\n.\ncomputeSVD\n(\n5\n,\ncomputeU\n=\ntrue\n)\nval\nU\n:\nRowMatrix\n=\nsvd\n.\nU\n// The U factor is a RowMatrix.\nval\ns\n:\nVector\n=\nsvd\n.\ns\n// The singular values are stored in a local dense vector.\nval\nV\n:\nMatrix\n=\nsvd\n.\nV\n// The V factor is a local dense matrix.\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\" in the Spark repo.\nThe same code applies to\nIndexedRowMatrix\nif\nU\nis defined as an\nIndexedRowMatrix\n.\nRefer to the\nSingularValueDecomposition\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.api.java.JavaSparkContext\n;\nimport\norg.apache.spark.mllib.linalg.Matrix\n;\nimport\norg.apache.spark.mllib.linalg.SingularValueDecomposition\n;\nimport\norg.apache.spark.mllib.linalg.Vector\n;\nimport\norg.apache.spark.mllib.linalg.Vectors\n;\nimport\norg.apache.spark.mllib.linalg.distributed.RowMatrix\n;\nList\n<\nVector\n>\ndata\n=\nArrays\n.\nasList\n(\nVectors\n.\nsparse\n(\n5\n,\nnew\nint\n[]\n{\n1\n,\n3\n},\nnew\ndouble\n[]\n{\n1.0\n,\n7.0\n}),\nVectors\n.\ndense\n(\n2.0\n,\n0.0\n,\n3.0\n,\n4.0\n,\n5.0\n),\nVectors\n.\ndense\n(\n4.0\n,\n0.0\n,\n0.0\n,\n6.0\n,\n7.0\n)\n);\nJavaRDD\n<\nVector\n>\nrows\n=\njsc\n.\nparallelize\n(\ndata\n);\n// Create a RowMatrix from JavaRDD<Vector>.\nRowMatrix\nmat\n=\nnew\nRowMatrix\n(\nrows\n.\nrdd\n());\n// Compute the top 5 singular values and corresponding singular vectors.\nSingularValueDecomposition\n<\nRowMatrix\n,\nMatrix\n>\nsvd\n=\nmat\n.\ncomputeSVD\n(\n5\n,\ntrue\n,\n1.0\nE\n-\n9\nd\n);\nRowMatrix\nU\n=\nsvd\n.\nU\n();\n// The U factor is a RowMatrix.\nVector\ns\n=\nsvd\n.\ns\n();\n// The singular values are stored in a local dense vector.\nMatrix\nV\n=\nsvd\n.\nV\n();\n// The V factor is a local dense matrix.\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\" in the Spark repo.\nThe same code applies to\nIndexedRowMatrix\nif\nU\nis defined as an\nIndexedRowMatrix\n.\nPrincipal component analysis (PCA)\nPrincipal component analysis (PCA)\nis a\nstatistical method to find a rotation such that the first coordinate has the largest variance\npossible, and each succeeding coordinate, in turn, has the largest variance possible. The columns of\nthe rotation matrix are called principal components. PCA is used widely in dimensionality reduction.\nspark.mllib\nsupports PCA for tall-and-skinny matrices stored in row-oriented format and any Vectors.\nThe following code demonstrates how to compute principal components on a\nRowMatrix\nand use them to project the vectors into a low-dimensional space.\nRefer to the\nRowMatrix\nPython docs\nfor details on the API.\nfrom\npyspark.mllib.linalg\nimport\nVectors\nfrom\npyspark.mllib.linalg.distributed\nimport\nRowMatrix\nrows\n=\nsc\n.\nparallelize\n([\nVectors\n.\nsparse\n(\n5\n,\n{\n1\n:\n1.0\n,\n3\n:\n7.0\n}),\nVectors\n.\ndense\n(\n2.0\n,\n0.0\n,\n3.0\n,\n4.0\n,\n5.0\n),\nVectors\n.\ndense\n(\n4.0\n,\n0.0\n,\n0.0\n,\n6.0\n,\n7.0\n)\n])\nmat\n=\nRowMatrix\n(\nrows\n)\n# Compute the top 4 principal components.\n# Principal components are stored in a local dense matrix.\npc\n=\nmat\n.\ncomputePrincipalComponents\n(\n4\n)\n# Project the rows to the linear space spanned by the top 4 principal components.\nprojected\n=\nmat\n.\nmultiply\n(\npc\n)\nFind full example code at \"examples/src/main/python/mllib/pca_rowmatrix_example.py\" in the Spark repo.\nThe following code demonstrates how to compute principal components on a\nRowMatrix\nand use them to project the vectors into a low-dimensional space.\nRefer to the\nRowMatrix\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.linalg.Matrix\nimport\norg.apache.spark.mllib.linalg.Vectors\nimport\norg.apache.spark.mllib.linalg.distributed.RowMatrix\nval\ndata\n=\nArray\n(\nVectors\n.\nsparse\n(\n5\n,\nSeq\n((\n1\n,\n1.0\n),\n(\n3\n,\n7.0\n))),\nVectors\n.\ndense\n(\n2.0\n,\n0.0\n,\n3.0\n,\n4.0\n,\n5.0\n),\nVectors\n.\ndense\n(\n4.0\n,\n0.0\n,\n0.0\n,\n6.0\n,\n7.0\n))\nval\nrows\n=\nsc\n.\nparallelize\n(\nimmutable\n.\nArraySeq\n.\nunsafeWrapArray\n(\ndata\n))\nval\nmat\n:\nRowMatrix\n=\nnew\nRowMatrix\n(\nrows\n)\n// Compute the top 4 principal components.\n// Principal components are stored in a local dense matrix.\nval\npc\n:\nMatrix\n=\nmat\n.\ncomputePrincipalComponents\n(\n4\n)\n// Project the rows to the linear space spanned by the top 4 principal components.\nval\nprojected\n:\nRowMatrix\n=\nmat\n.\nmultiply\n(\npc\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\" in the Spark repo.\nThe following code demonstrates how to compute principal components on source vectors\nand use them to project the vectors into a low-dimensional space while keeping associated labels:\nRefer to the\nPCA\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.feature.PCA\nimport\norg.apache.spark.mllib.linalg.Vectors\nimport\norg.apache.spark.mllib.regression.LabeledPoint\nimport\norg.apache.spark.rdd.RDD\nval\ndata\n:\nRDD\n[\nLabeledPoint\n]\n=\nsc\n.\nparallelize\n(\nSeq\n(\nnew\nLabeledPoint\n(\n0\n,\nVectors\n.\ndense\n(\n1\n,\n0\n,\n0\n,\n0\n,\n1\n)),\nnew\nLabeledPoint\n(\n1\n,\nVectors\n.\ndense\n(\n1\n,\n1\n,\n0\n,\n1\n,\n0\n)),\nnew\nLabeledPoint\n(\n1\n,\nVectors\n.\ndense\n(\n1\n,\n1\n,\n0\n,\n0\n,\n0\n)),\nnew\nLabeledPoint\n(\n0\n,\nVectors\n.\ndense\n(\n1\n,\n0\n,\n0\n,\n0\n,\n0\n)),\nnew\nLabeledPoint\n(\n1\n,\nVectors\n.\ndense\n(\n1\n,\n1\n,\n0\n,\n0\n,\n0\n))))\n// Compute the top 5 principal components.\nval\npca\n=\nnew\nPCA\n(\n5\n).\nfit\n(\ndata\n.\nmap\n(\n_\n.\nfeatures\n))\n// Project vectors to the linear space spanned by the top 5 principal\n// components, keeping the label\nval\nprojected\n=\ndata\n.\nmap\n(\np\n=>\np\n.\ncopy\n(\nfeatures\n=\npca\n.\ntransform\n(\np\n.\nfeatures\n)))\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\" in the Spark repo.\nThe following code demonstrates how to compute principal components on a\nRowMatrix\nand use them to project the vectors into a low-dimensional space.\nRefer to the\nRowMatrix\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.api.java.JavaSparkContext\n;\nimport\norg.apache.spark.mllib.linalg.Matrix\n;\nimport\norg.apache.spark.mllib.linalg.Vector\n;\nimport\norg.apache.spark.mllib.linalg.Vectors\n;\nimport\norg.apache.spark.mllib.linalg.distributed.RowMatrix\n;\nList\n<\nVector\n>\ndata\n=\nArrays\n.\nasList\n(\nVectors\n.\nsparse\n(\n5\n,\nnew\nint\n[]\n{\n1\n,\n3\n},\nnew\ndouble\n[]\n{\n1.0\n,\n7.0\n}),\nVectors\n.\ndense\n(\n2.0\n,\n0.0\n,\n3.0\n,\n4.0\n,\n5.0\n),\nVectors\n.\ndense\n(\n4.0\n,\n0.0\n,\n0.0\n,\n6.0\n,\n7.0\n)\n);\nJavaRDD\n<\nVector\n>\nrows\n=\njsc\n.\nparallelize\n(\ndata\n);\n// Create a RowMatrix from JavaRDD<Vector>.\nRowMatrix\nmat\n=\nnew\nRowMatrix\n(\nrows\n.\nrdd\n());\n// Compute the top 4 principal components.\n// Principal components are stored in a local dense matrix.\nMatrix\npc\n=\nmat\n.\ncomputePrincipalComponents\n(\n4\n);\n// Project the rows to the linear space spanned by the top 4 principal components.\nRowMatrix\nprojected\n=\nmat\n.\nmultiply\n(\npc\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\" in the Spark repo."}
{"url": "https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTroubleshooting\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nGeneric Load/Save Functions\nManually Specifying Options\nRun SQL on files directly\nSave Modes\nSaving to Persistent Tables\nBucketing, Sorting and Partitioning\nIn the simplest form, the default data source (\nparquet\nunless otherwise configured by\nspark.sql.sources.default\n) will be used for all operations.\nusers_df\n=\nspark\n.\nread\n.\nload\n(\n\"\nexamples/src/main/resources/users.parquet\n\"\n)\nusers_df\n.\nselect\n(\n\"\nname\n\"\n,\n\"\nfavorite_color\n\"\n).\nwrite\n.\nsave\n(\n\"\nnamesAndFavColors.parquet\n\"\n)\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\nval\nusersDF\n=\nspark\n.\nread\n.\nload\n(\n\"examples/src/main/resources/users.parquet\"\n)\nusersDF\n.\nselect\n(\n\"name\"\n,\n\"favorite_color\"\n).\nwrite\n.\nsave\n(\n\"namesAndFavColors.parquet\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nDataset\n<\nRow\n>\nusersDF\n=\nspark\n.\nread\n().\nload\n(\n\"examples/src/main/resources/users.parquet\"\n);\nusersDF\n.\nselect\n(\n\"name\"\n,\n\"favorite_color\"\n).\nwrite\n().\nsave\n(\n\"namesAndFavColors.parquet\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\ndf\n<-\nread.df\n(\n\"examples/src/main/resources/users.parquet\"\n)\nwrite.df\n(\nselect\n(\ndf\n,\n\"name\"\n,\n\"favorite_color\"\n),\n\"namesAndFavColors.parquet\"\n)\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nManually Specifying Options\nYou can also manually specify the data source that will be used along with any extra options\nthat you would like to pass to the data source. Data sources are specified by their fully qualified\nname (i.e.,\norg.apache.spark.sql.parquet\n), but for built-in sources you can also use their short\nnames (\njson\n,\nparquet\n,\njdbc\n,\norc\n,\nlibsvm\n,\ncsv\n,\ntext\n). DataFrames loaded from any data\nsource type can be converted into other types using this syntax.\nPlease refer the API documentation for available options of built-in sources, for example,\norg.apache.spark.sql.DataFrameReader\nand\norg.apache.spark.sql.DataFrameWriter\n. The\noptions documented there should be applicable through non-Scala Spark APIs (e.g. PySpark)\nas well. For other formats, refer to the API documentation of the particular format.\nTo load a JSON file you can use:\npeople_df\n=\nspark\n.\nread\n.\nload\n(\n\"\nexamples/src/main/resources/people.json\n\"\n,\nformat\n=\n\"\njson\n\"\n)\npeople_df\n.\nselect\n(\n\"\nname\n\"\n,\n\"\nage\n\"\n).\nwrite\n.\nsave\n(\n\"\nnamesAndAges.parquet\n\"\n,\nformat\n=\n\"\nparquet\n\"\n)\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\nval\npeopleDF\n=\nspark\n.\nread\n.\nformat\n(\n\"json\"\n).\nload\n(\n\"examples/src/main/resources/people.json\"\n)\npeopleDF\n.\nselect\n(\n\"name\"\n,\n\"age\"\n).\nwrite\n.\nformat\n(\n\"parquet\"\n).\nsave\n(\n\"namesAndAges.parquet\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nDataset\n<\nRow\n>\npeopleDF\n=\nspark\n.\nread\n().\nformat\n(\n\"json\"\n).\nload\n(\n\"examples/src/main/resources/people.json\"\n);\npeopleDF\n.\nselect\n(\n\"name\"\n,\n\"age\"\n).\nwrite\n().\nformat\n(\n\"parquet\"\n).\nsave\n(\n\"namesAndAges.parquet\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\ndf\n<-\nread.df\n(\n\"examples/src/main/resources/people.json\"\n,\n\"json\"\n)\nnamesAndAges\n<-\nselect\n(\ndf\n,\n\"name\"\n,\n\"age\"\n)\nwrite.df\n(\nnamesAndAges\n,\n\"namesAndAges.parquet\"\n,\n\"parquet\"\n)\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nTo load a CSV file you can use:\npeople_df\n=\nspark\n.\nread\n.\nload\n(\n\"\nexamples/src/main/resources/people.csv\n\"\n,\nformat\n=\n\"\ncsv\n\"\n,\nsep\n=\n\"\n;\n\"\n,\ninferSchema\n=\n\"\ntrue\n\"\n,\nheader\n=\n\"\ntrue\n\"\n)\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\nval\npeopleDFCsv\n=\nspark\n.\nread\n.\nformat\n(\n\"csv\"\n)\n.\noption\n(\n\"sep\"\n,\n\";\"\n)\n.\noption\n(\n\"inferSchema\"\n,\n\"true\"\n)\n.\noption\n(\n\"header\"\n,\n\"true\"\n)\n.\nload\n(\n\"examples/src/main/resources/people.csv\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nDataset\n<\nRow\n>\npeopleDFCsv\n=\nspark\n.\nread\n().\nformat\n(\n\"csv\"\n)\n.\noption\n(\n\"sep\"\n,\n\";\"\n)\n.\noption\n(\n\"inferSchema\"\n,\n\"true\"\n)\n.\noption\n(\n\"header\"\n,\n\"true\"\n)\n.\nload\n(\n\"examples/src/main/resources/people.csv\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\ndf\n<-\nread.df\n(\n\"examples/src/main/resources/people.csv\"\n,\n\"csv\"\n,\nsep\n=\n\";\"\n,\ninferSchema\n=\nTRUE\n,\nheader\n=\nTRUE\n)\nnamesAndAges\n<-\nselect\n(\ndf\n,\n\"name\"\n,\n\"age\"\n)\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nThe extra options are also used during write operation.\nFor example, you can control bloom filters and dictionary encodings for ORC data sources.\nThe following ORC example will create bloom filter and use dictionary encoding only for\nfavorite_color\n.\nFor Parquet, there exists\nparquet.bloom.filter.enabled\nand\nparquet.enable.dictionary\n, too.\nTo find more detailed information about the extra ORC/Parquet options,\nvisit the official Apache\nORC\n/\nParquet\nwebsites.\nORC data source:\nusers_df\n=\nspark\n.\nread\n.\norc\n(\n\"\nexamples/src/main/resources/users.orc\n\"\n)\n(\nusers_df\n.\nwrite\n.\nformat\n(\n\"\norc\n\"\n)\n.\noption\n(\n\"\norc.bloom.filter.columns\n\"\n,\n\"\nfavorite_color\n\"\n)\n.\noption\n(\n\"\norc.dictionary.key.threshold\n\"\n,\n\"\n1.0\n\"\n)\n.\noption\n(\n\"\norc.column.encoding.direct\n\"\n,\n\"\nname\n\"\n)\n.\nsave\n(\n\"\nusers_with_options.orc\n\"\n))\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\nusersDF\n.\nwrite\n.\nformat\n(\n\"orc\"\n)\n.\noption\n(\n\"orc.bloom.filter.columns\"\n,\n\"favorite_color\"\n)\n.\noption\n(\n\"orc.dictionary.key.threshold\"\n,\n\"1.0\"\n)\n.\noption\n(\n\"orc.column.encoding.direct\"\n,\n\"name\"\n)\n.\nsave\n(\n\"users_with_options.orc\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nusersDF\n.\nwrite\n().\nformat\n(\n\"orc\"\n)\n.\noption\n(\n\"orc.bloom.filter.columns\"\n,\n\"favorite_color\"\n)\n.\noption\n(\n\"orc.dictionary.key.threshold\"\n,\n\"1.0\"\n)\n.\noption\n(\n\"orc.column.encoding.direct\"\n,\n\"name\"\n)\n.\nsave\n(\n\"users_with_options.orc\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\ndf\n<-\nread.df\n(\n\"examples/src/main/resources/users.orc\"\n,\n\"orc\"\n)\nwrite.orc\n(\ndf\n,\n\"users_with_options.orc\"\n,\norc.bloom.filter.columns\n=\n\"favorite_color\"\n,\norc.dictionary.key.threshold\n=\n1.0\n,\norc.column.encoding.direct\n=\n\"name\"\n)\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nCREATE\nTABLE\nusers_with_options\n(\nname\nSTRING\n,\nfavorite_color\nSTRING\n,\nfavorite_numbers\narray\n<\ninteger\n>\n)\nUSING\nORC\nOPTIONS\n(\norc\n.\nbloom\n.\nfilter\n.\ncolumns\n'favorite_color'\n,\norc\n.\ndictionary\n.\nkey\n.\nthreshold\n'1.0'\n,\norc\n.\ncolumn\n.\nencoding\n.\ndirect\n'name'\n)\nParquet data source:\nusers_df\n=\nspark\n.\nread\n.\nparquet\n(\n\"\nexamples/src/main/resources/users.parquet\n\"\n)\n(\nusers_df\n.\nwrite\n.\nformat\n(\n\"\nparquet\n\"\n)\n.\noption\n(\n\"\nparquet.bloom.filter.enabled#favorite_color\n\"\n,\n\"\ntrue\n\"\n)\n.\noption\n(\n\"\nparquet.bloom.filter.expected.ndv#favorite_color\n\"\n,\n\"\n1000000\n\"\n)\n.\noption\n(\n\"\nparquet.enable.dictionary\n\"\n,\n\"\ntrue\n\"\n)\n.\noption\n(\n\"\nparquet.page.write-checksum.enabled\n\"\n,\n\"\nfalse\n\"\n)\n.\nsave\n(\n\"\nusers_with_options.parquet\n\"\n))\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\nusersDF\n.\nwrite\n.\nformat\n(\n\"parquet\"\n)\n.\noption\n(\n\"parquet.bloom.filter.enabled#favorite_color\"\n,\n\"true\"\n)\n.\noption\n(\n\"parquet.bloom.filter.expected.ndv#favorite_color\"\n,\n\"1000000\"\n)\n.\noption\n(\n\"parquet.enable.dictionary\"\n,\n\"true\"\n)\n.\noption\n(\n\"parquet.page.write-checksum.enabled\"\n,\n\"false\"\n)\n.\nsave\n(\n\"users_with_options.parquet\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nusersDF\n.\nwrite\n().\nformat\n(\n\"parquet\"\n)\n.\noption\n(\n\"parquet.bloom.filter.enabled#favorite_color\"\n,\n\"true\"\n)\n.\noption\n(\n\"parquet.bloom.filter.expected.ndv#favorite_color\"\n,\n\"1000000\"\n)\n.\noption\n(\n\"parquet.enable.dictionary\"\n,\n\"true\"\n)\n.\noption\n(\n\"parquet.page.write-checksum.enabled\"\n,\n\"false\"\n)\n.\nsave\n(\n\"users_with_options.parquet\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\ndf\n<-\nread.df\n(\n\"examples/src/main/resources/users.parquet\"\n,\n\"parquet\"\n)\nwrite.parquet\n(\ndf\n,\n\"users_with_options.parquet\"\n,\nparquet.bloom.filter.enabled\n#favorite_color = true, parquet.bloom.filter.expected.ndv#favorite_color = 1000000, parquet.enable.dictionary = true, parquet.page.write-checksum.enabled = false)\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nCREATE\nTABLE\nusers_with_options\n(\nname\nSTRING\n,\nfavorite_color\nSTRING\n,\nfavorite_numbers\narray\n<\ninteger\n>\n)\nUSING\nparquet\nOPTIONS\n(\n`parquet.bloom.filter.enabled#favorite_color`\ntrue\n,\n`parquet.bloom.filter.expected.ndv#favorite_color`\n1000000\n,\nparquet\n.\nenable\n.\ndictionary\ntrue\n,\nparquet\n.\npage\n.\nwrite\n-\nchecksum\n.\nenabled\ntrue\n)\nRun SQL on files directly\nInstead of using read API to load a file into DataFrame and query it, you can also query that\nfile directly with SQL.\ndf\n=\nspark\n.\nsql\n(\n\"\nSELECT * FROM parquet.`examples/src/main/resources/users.parquet`\n\"\n)\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\nval\nsqlDF\n=\nspark\n.\nsql\n(\n\"SELECT * FROM parquet.`examples/src/main/resources/users.parquet`\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nDataset\n<\nRow\n>\nsqlDF\n=\nspark\n.\nsql\n(\n\"SELECT * FROM parquet.`examples/src/main/resources/users.parquet`\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\ndf\n<-\nsql\n(\n\"SELECT * FROM parquet.`examples/src/main/resources/users.parquet`\"\n)\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nSELECT\n*\nFROM\nparquet\n.\n`examples/src/main/resources/users.parquet`\nSave Modes\nSave operations can optionally take a\nSaveMode\n, that specifies how to handle existing data if\npresent. It is important to realize that these save modes do not utilize any locking and are not\natomic. Additionally, when performing an\nOverwrite\n, the data will be deleted before writing out the\nnew data.\nScala/Java\nAny Language\nMeaning\nSaveMode.ErrorIfExists\n(default)\n\"error\" or \"errorifexists\"\n(default)\nWhen saving a DataFrame to a data source, if data already exists,\n    an exception is expected to be thrown.\nSaveMode.Append\n\"append\"\nWhen saving a DataFrame to a data source, if data/table already exists,\n    contents of the DataFrame are expected to be appended to existing data.\nSaveMode.Overwrite\n\"overwrite\"\nOverwrite mode means that when saving a DataFrame to a data source,\n    if data/table already exists, existing data is expected to be overwritten by the contents of\n    the DataFrame.\nSaveMode.Ignore\n\"ignore\"\nIgnore mode means that when saving a DataFrame to a data source, if data already exists,\n    the save operation is expected not to save the contents of the DataFrame and not to\n    change the existing data. This is similar to a\nCREATE TABLE IF NOT EXISTS\nin SQL.\nSaving to Persistent Tables\nDataFrames\ncan also be saved as persistent tables into Hive metastore using the\nsaveAsTable\ncommand. Notice that an existing Hive deployment is not necessary to use this feature. Spark will create a\ndefault local Hive metastore (using Derby) for you. Unlike the\ncreateOrReplaceTempView\ncommand,\nsaveAsTable\nwill materialize the contents of the DataFrame and create a pointer to the data in the\nHive metastore. Persistent tables will still exist even after your Spark program has restarted, as\nlong as you maintain your connection to the same metastore. A DataFrame for a persistent table can\nbe created by calling the\ntable\nmethod on a\nSparkSession\nwith the name of the table.\nFor file-based data source, e.g. text, parquet, json, etc. you can specify a custom table path via the\npath\noption, e.g.\ndf.write.option(\"path\", \"/some/path\").saveAsTable(\"t\")\n. When the table is dropped,\nthe custom table path will not be removed and the table data is still there. If no custom table path is\nspecified, Spark will write data to a default table path under the warehouse directory. When the table is\ndropped, the default table path will be removed too.\nStarting from Spark 2.1, persistent datasource tables have per-partition metadata stored in the Hive metastore. This brings several benefits:\nSince the metastore can return only necessary partitions for a query, discovering all the partitions on the first query to the table is no longer needed.\nHive DDLs such as\nALTER TABLE PARTITION ... SET LOCATION\nare now available for tables created with the Datasource API.\nNote that partition information is not gathered by default when creating external datasource tables (those with a\npath\noption). To sync the partition information in the metastore, you can invoke\nMSCK REPAIR TABLE\n.\nBucketing, Sorting and Partitioning\nFor file-based data source, it is also possible to bucket and sort or partition the output.\nBucketing and sorting are applicable only to persistent tables:\npeople_df\n=\nspark\n.\nread\n.\njson\n(\n\"\nexamples/src/main/resources/people.json\n\"\n)\npeople_df\n.\nwrite\n.\nbucketBy\n(\n42\n,\n\"\nname\n\"\n).\nsortBy\n(\n\"\nage\n\"\n).\nsaveAsTable\n(\n\"\npeople_bucketed\n\"\n)\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\npeopleDF\n.\nwrite\n.\nbucketBy\n(\n42\n,\n\"name\"\n).\nsortBy\n(\n\"age\"\n).\nsaveAsTable\n(\n\"people_bucketed\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\npeopleDF\n.\nwrite\n().\nbucketBy\n(\n42\n,\n\"name\"\n).\nsortBy\n(\n\"age\"\n).\nsaveAsTable\n(\n\"people_bucketed\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\nCREATE\nTABLE\npeople_bucketed\nUSING\njson\nCLUSTERED\nBY\n(\nname\n)\nINTO\n42\nBUCKETS\nAS\nSELECT\n*\nFROM\njson\n.\n`examples/src/main/resources/people.json`\n;\nwhile partitioning can be used with both\nsave\nand\nsaveAsTable\nwhen using the Dataset APIs.\nusers_df\n=\nspark\n.\nread\n.\nload\n(\n\"\nexamples/src/main/resources/users.parquet\n\"\n)\nusers_df\n.\nwrite\n.\npartitionBy\n(\n\"\nfavorite_color\n\"\n).\nformat\n(\n\"\nparquet\n\"\n).\nsave\n(\n\"\nnamesPartByColor.parquet\n\"\n)\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\nusersDF\n.\nwrite\n.\npartitionBy\n(\n\"favorite_color\"\n).\nformat\n(\n\"parquet\"\n).\nsave\n(\n\"namesPartByColor.parquet\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nusersDF\n.\nwrite\n()\n.\npartitionBy\n(\n\"favorite_color\"\n)\n.\nformat\n(\n\"parquet\"\n)\n.\nsave\n(\n\"namesPartByColor.parquet\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\nCREATE\nTABLE\nusers_by_favorite_color\nUSING\nparquet\nPARTITIONED\nBY\n(\nfavorite_color\n)\nAS\nSELECT\n*\nFROM\nparquet\n.\n`examples/src/main/resources/users.parquet`\n;\nIt is possible to use both partitioning and bucketing for a single table:\nusers_df\n=\nspark\n.\nread\n.\nparquet\n(\n\"\nexamples/src/main/resources/users.parquet\n\"\n)\n(\nusers_df\n.\nwrite\n.\npartitionBy\n(\n\"\nfavorite_color\n\"\n)\n.\nbucketBy\n(\n42\n,\n\"\nname\n\"\n)\n.\nsaveAsTable\n(\n\"\nusers_partitioned_bucketed\n\"\n))\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\nusersDF\n.\nwrite\n.\npartitionBy\n(\n\"favorite_color\"\n)\n.\nbucketBy\n(\n42\n,\n\"name\"\n)\n.\nsaveAsTable\n(\n\"users_partitioned_bucketed\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nusersDF\n.\nwrite\n()\n.\npartitionBy\n(\n\"favorite_color\"\n)\n.\nbucketBy\n(\n42\n,\n\"name\"\n)\n.\nsaveAsTable\n(\n\"users_partitioned_bucketed\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\nCREATE\nTABLE\nusers_partitioned_bucketed\nUSING\nparquet\nPARTITIONED\nBY\n(\nfavorite_color\n)\nCLUSTERED\nBY\n(\nname\n)\nSORTED\nBY\n(\nfavorite_numbers\n)\nINTO\n42\nBUCKETS\nAS\nSELECT\n*\nFROM\nparquet\n.\n`examples/src/main/resources/users.parquet`\n;\npartitionBy\ncreates a directory structure as described in the\nPartition Discovery\nsection.\nThus, it has limited applicability to columns with high cardinality. In contrast\nbucketBy\ndistributes\ndata across a fixed number of buckets and can be used when the number of unique values is unbounded."}
{"url": "https://spark.apache.org/docs/latest/streaming/getting-started.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nStructured Streaming Programming Guide\nOverview\nGetting Started\nQuick Example\nProgramming Model\nAPIs on DataFrames and Datasets\nPerformance Tips\nAdditional Information\nStructured Streaming Programming Guide\nQuick Example\nLet’s say you want to maintain a running word count of text data received from a data server listening on a TCP socket. Let’s see how you can express this using Structured Streaming. You can see the full code in\nPython\n/\nScala\n/\nJava\n/\nR\n.\nAnd if you\ndownload Spark\n, you can directly\nrun the example\n. In any case, let’s walk through the example step-by-step and understand how it works. First, we have to import the necessary classes and create a local SparkSession, the starting point of all functionalities related to Spark.\nfrom\npyspark.sql\nimport\nSparkSession\nfrom\npyspark.sql.functions\nimport\nexplode\nfrom\npyspark.sql.functions\nimport\nsplit\nspark\n=\nSparkSession\n\\\n.\nbuilder\n\\\n.\nappName\n(\n\"\nStructuredNetworkWordCount\n\"\n)\n\\\n.\ngetOrCreate\n()\nimport\norg.apache.spark.sql.functions._\nimport\norg.apache.spark.sql.SparkSession\nval\nspark\n=\nSparkSession\n.\nbuilder\n.\nappName\n(\n\"StructuredNetworkWordCount\"\n)\n.\ngetOrCreate\n()\nimport\nspark.implicits._\nimport\norg.apache.spark.api.java.function.FlatMapFunction\n;\nimport\norg.apache.spark.sql.*\n;\nimport\norg.apache.spark.sql.streaming.StreamingQuery\n;\nimport\njava.util.Arrays\n;\nimport\njava.util.Iterator\n;\nSparkSession\nspark\n=\nSparkSession\n.\nbuilder\n()\n.\nappName\n(\n\"JavaStructuredNetworkWordCount\"\n)\n.\ngetOrCreate\n();\nsparkR.session\n(\nappName\n=\n\"StructuredNetworkWordCount\"\n)\nNext, let’s create a streaming DataFrame that represents text data received from a server listening on localhost:9999, and transform the DataFrame to calculate word counts.\n# Create DataFrame representing the stream of input lines from connection to localhost:9999\nlines\n=\nspark\n\\\n.\nreadStream\n\\\n.\nformat\n(\n\"\nsocket\n\"\n)\n\\\n.\noption\n(\n\"\nhost\n\"\n,\n\"\nlocalhost\n\"\n)\n\\\n.\noption\n(\n\"\nport\n\"\n,\n9999\n)\n\\\n.\nload\n()\n# Split the lines into words\nwords\n=\nlines\n.\nselect\n(\nexplode\n(\nsplit\n(\nlines\n.\nvalue\n,\n\"\n\"\n)\n).\nalias\n(\n\"\nword\n\"\n)\n)\n# Generate running word count\nwordCounts\n=\nwords\n.\ngroupBy\n(\n\"\nword\n\"\n).\ncount\n()\nThis\nlines\nDataFrame represents an unbounded table containing the streaming text data. This table contains one column of strings named “value”, and each line in the streaming text data becomes a row in the table. Note, that this is not currently receiving any data as we are just setting up the transformation, and have not yet started it. Next, we have used two built-in SQL functions - split and explode, to split each line into multiple rows with a word each. In addition, we use the function\nalias\nto name the new column as “word”. Finally, we have defined the\nwordCounts\nDataFrame by grouping by the unique values in the Dataset and counting them. Note that this is a streaming DataFrame which represents the running word counts of the stream.\n// Create DataFrame representing the stream of input lines from connection to localhost:9999\nval\nlines\n=\nspark\n.\nreadStream\n.\nformat\n(\n\"socket\"\n)\n.\noption\n(\n\"host\"\n,\n\"localhost\"\n)\n.\noption\n(\n\"port\"\n,\n9999\n)\n.\nload\n()\n// Split the lines into words\nval\nwords\n=\nlines\n.\nas\n[\nString\n].\nflatMap\n(\n_\n.\nsplit\n(\n\" \"\n))\n// Generate running word count\nval\nwordCounts\n=\nwords\n.\ngroupBy\n(\n\"value\"\n).\ncount\n()\nThis\nlines\nDataFrame represents an unbounded table containing the streaming text data. This table contains one column of strings named “value”, and each line in the streaming text data becomes a row in the table. Note, that this is not currently receiving any data as we are just setting up the transformation, and have not yet started it. Next, we have converted the DataFrame to a  Dataset of String using\n.as[String]\n, so that we can apply the\nflatMap\noperation to split each line into multiple words. The resultant\nwords\nDataset contains all the words. Finally, we have defined the\nwordCounts\nDataFrame by grouping by the unique values in the Dataset and counting them. Note that this is a streaming DataFrame which represents the running word counts of the stream.\n// Create DataFrame representing the stream of input lines from connection to localhost:9999\nDataset\n<\nRow\n>\nlines\n=\nspark\n.\nreadStream\n()\n.\nformat\n(\n\"socket\"\n)\n.\noption\n(\n\"host\"\n,\n\"localhost\"\n)\n.\noption\n(\n\"port\"\n,\n9999\n)\n.\nload\n();\n// Split the lines into words\nDataset\n<\nString\n>\nwords\n=\nlines\n.\nas\n(\nEncoders\n.\nSTRING\n())\n.\nflatMap\n((\nFlatMapFunction\n<\nString\n,\nString\n>)\nx\n->\nArrays\n.\nasList\n(\nx\n.\nsplit\n(\n\" \"\n)).\niterator\n(),\nEncoders\n.\nSTRING\n());\n// Generate running word count\nDataset\n<\nRow\n>\nwordCounts\n=\nwords\n.\ngroupBy\n(\n\"value\"\n).\ncount\n();\nThis\nlines\nDataFrame represents an unbounded table containing the streaming text data. This table contains one column of strings named “value”, and each line in the streaming text data becomes a row in the table. Note, that this is not currently receiving any data as we are just setting up the transformation, and have not yet started it. Next, we have converted the DataFrame to a  Dataset of String using\n.as(Encoders.STRING())\n, so that we can apply the\nflatMap\noperation to split each line into multiple words. The resultant\nwords\nDataset contains all the words. Finally, we have defined the\nwordCounts\nDataFrame by grouping by the unique values in the Dataset and counting them. Note that this is a streaming DataFrame which represents the running word counts of the stream.\n# Create DataFrame representing the stream of input lines from connection to localhost:9999\nlines\n<-\nread.stream\n(\n\"socket\"\n,\nhost\n=\n\"localhost\"\n,\nport\n=\n9999\n)\n# Split the lines into words\nwords\n<-\nselectExpr\n(\nlines\n,\n\"explode(split(value, ' ')) as word\"\n)\n# Generate running word count\nwordCounts\n<-\ncount\n(\ngroup_by\n(\nwords\n,\n\"word\"\n))\nThis\nlines\nSparkDataFrame represents an unbounded table containing the streaming text data. This table contains one column of strings named “value”, and each line in the streaming text data becomes a row in the table. Note, that this is not currently receiving any data as we are just setting up the transformation, and have not yet started it. Next, we have a SQL expression with two SQL functions - split and explode, to split each line into multiple rows with a word each. In addition, we name the new column as “word”. Finally, we have defined the\nwordCounts\nSparkDataFrame by grouping by the unique values in the SparkDataFrame and counting them. Note that this is a streaming SparkDataFrame which represents the running word counts of the stream.\nWe have now set up the query on the streaming data. All that is left is to actually start receiving data and computing the counts. To do this, we set it up to print the complete set of counts (specified by\noutputMode(\"complete\")\n) to the console every time they are updated. And then start the streaming computation using\nstart()\n.\n# Start running the query that prints the running counts to the console\nquery\n=\nwordCounts\n\\\n.\nwriteStream\n\\\n.\noutputMode\n(\n\"\ncomplete\n\"\n)\n\\\n.\nformat\n(\n\"\nconsole\n\"\n)\n\\\n.\nstart\n()\nquery\n.\nawaitTermination\n()\n// Start running the query that prints the running counts to the console\nval\nquery\n=\nwordCounts\n.\nwriteStream\n.\noutputMode\n(\n\"complete\"\n)\n.\nformat\n(\n\"console\"\n)\n.\nstart\n()\nquery\n.\nawaitTermination\n()\n// Start running the query that prints the running counts to the console\nStreamingQuery\nquery\n=\nwordCounts\n.\nwriteStream\n()\n.\noutputMode\n(\n\"complete\"\n)\n.\nformat\n(\n\"console\"\n)\n.\nstart\n();\nquery\n.\nawaitTermination\n();\n# Start running the query that prints the running counts to the console\nquery\n<-\nwrite.stream\n(\nwordCounts\n,\n\"console\"\n,\noutputMode\n=\n\"complete\"\n)\nawaitTermination\n(\nquery\n)\nAfter this code is executed, the streaming computation will have started in the background. The\nquery\nobject is a handle to that active streaming query, and we have decided to wait for the termination of the query using\nawaitTermination()\nto prevent the process from exiting while the query is active.\nTo actually execute this example code, you can either compile the code in your own\nSpark application\n, or simply\nrun the example\nonce you have downloaded Spark. We are showing the latter. You will first need to run Netcat (a small utility found in most Unix-like systems) as a data server by using\n$ nc -lk 9999\nThen, in a different terminal, you can start the example by using\n$\n./bin/spark-submit examples/src/main/python/sql/streaming/structured_network_wordcount.py localhost 9999\n$\n./bin/run-example org.apache.spark.examples.sql.streaming.StructuredNetworkWordCount localhost 9999\n$\n./bin/run-example org.apache.spark.examples.sql.streaming.JavaStructuredNetworkWordCount localhost 9999\n$\n./bin/spark-submit examples/src/main/r/streaming/structured_network_wordcount.R localhost 9999\nThen, any lines typed in the terminal running the netcat server will be counted and printed on screen every second. It will look something like the following.\n# TERMINAL 1:\n# Running Netcat\n$\nnc\n-lk\n9999\napache spark\napache hadoop\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n...\n# TERMINAL 2: RUNNING structured_network_wordcount.py\n$\n./bin/spark-submit examples/src/main/python/sql/streaming/structured_network_wordcount.py localhost 9999\n-------------------------------------------\nBatch: 0\n-------------------------------------------\n+------+-----+\n| value|count|\n+------+-----+\n|apache|    1|\n| spark|    1|\n+------+-----+\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+------+-----+\n| value|count|\n+------+-----+\n|apache|    2|\n| spark|    1|\n|hadoop|    1|\n+------+-----+\n...\n# TERMINAL 2: RUNNING StructuredNetworkWordCount\n$\n./bin/run-example org.apache.spark.examples.sql.streaming.StructuredNetworkWordCount localhost 9999\n-------------------------------------------\nBatch: 0\n-------------------------------------------\n+------+-----+\n| value|count|\n+------+-----+\n|apache|    1|\n| spark|    1|\n+------+-----+\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+------+-----+\n| value|count|\n+------+-----+\n|apache|    2|\n| spark|    1|\n|hadoop|    1|\n+------+-----+\n...\n# TERMINAL 2: RUNNING JavaStructuredNetworkWordCount\n$\n./bin/run-example org.apache.spark.examples.sql.streaming.JavaStructuredNetworkWordCount localhost 9999\n-------------------------------------------\nBatch: 0\n-------------------------------------------\n+------+-----+\n| value|count|\n+------+-----+\n|apache|    1|\n| spark|    1|\n+------+-----+\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+------+-----+\n| value|count|\n+------+-----+\n|apache|    2|\n| spark|    1|\n|hadoop|    1|\n+------+-----+\n...\n# TERMINAL 2: RUNNING structured_network_wordcount.R\n$\n./bin/spark-submit examples/src/main/r/streaming/structured_network_wordcount.R localhost 9999\n-------------------------------------------\nBatch: 0\n-------------------------------------------\n+------+-----+\n| value|count|\n+------+-----+\n|apache|    1|\n| spark|    1|\n+------+-----+\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+------+-----+\n| value|count|\n+------+-----+\n|apache|    2|\n| spark|    1|\n|hadoop|    1|\n+------+-----+\n...\nProgramming Model\nThe key idea in Structured Streaming is to treat a live data stream as a\ntable that is being continuously appended. This leads to a new stream\nprocessing model that is very similar to a batch processing model. You will\nexpress your streaming computation as standard batch-like query as on a static\ntable, and Spark runs it as an\nincremental\nquery on the\nunbounded\ninput\ntable. Let’s understand this model in more detail.\nBasic Concepts\nConsider the input data stream as the “Input Table”. Every data item that is\narriving on the stream is like a new row being appended to the Input Table.\nA query on the input will generate the “Result Table”. Every trigger interval (say, every 1 second), new rows get appended to the Input Table, which eventually updates the Result Table. Whenever the result table gets updated, we would want to write the changed result rows to an external sink.\nThe “Output” is defined as what gets written out to the external storage. The output can be defined in a different mode:\nComplete Mode\n- The entire updated Result Table will be written to the external storage. It is up to the storage connector to decide how to handle writing of the entire table.\nAppend Mode\n- Only the new rows appended in the Result Table since the last trigger will be written to the external storage. This is applicable only on the queries where existing rows in the Result Table are not expected to change.\nUpdate Mode\n- Only the rows that were updated in the Result Table since the last trigger will be written to the external storage (available since Spark 2.1.1). Note that this is different from the Complete Mode in that this mode only outputs the rows that have changed since the last trigger. If the query doesn’t contain aggregations, it will be equivalent to Append mode.\nNote that each mode is applicable on certain types of queries. This is discussed in detail\nlater\n.\nTo illustrate the use of this model, let’s understand the model in context of\nthe\nQuick Example\nabove. The first\nlines\nDataFrame is the input table, and\nthe final\nwordCounts\nDataFrame is the result table. Note that the query on\nstreaming\nlines\nDataFrame to generate\nwordCounts\nis\nexactly the same\nas\nit would be a static DataFrame. However, when this query is started, Spark\nwill continuously check for new data from the socket connection. If there is\nnew data, Spark will run an “incremental” query that combines the previous\nrunning counts with the new data to compute updated counts, as shown below.\nNote that Structured Streaming does not materialize the entire table\n. It reads the latest\navailable data from the streaming data source, processes it incrementally to update the result,\nand then discards the source data. It only keeps around the minimal intermediate\nstate\ndata as\nrequired to update the result (e.g. intermediate counts in the earlier example).\nThis model is significantly different from many other stream processing\nengines. Many streaming systems require the user to maintain running\naggregations themselves, thus having to reason about fault-tolerance, and\ndata consistency (at-least-once, or at-most-once, or exactly-once). In this\nmodel, Spark is responsible for updating the Result Table when there is new\ndata, thus relieving the users from reasoning about it. As an example, let’s\nsee how this model handles event-time based processing and late arriving data.\nHandling Event-time and Late Data\nEvent-time is the time embedded in the data itself. For many applications, you may want to operate on this event-time. For example, if you want to get the number of events generated by IoT devices every minute, then you probably want to use the time when the data was generated (that is, event-time in the data), rather than the time Spark receives them. This event-time is very naturally expressed in this model – each event from the devices is a row in the table, and event-time is a column value in the row. This allows window-based aggregations (e.g. number of events every minute) to be just a special type of grouping and aggregation on the event-time column – each time window is a group and each row can belong to multiple windows/groups. Therefore, such event-time-window-based aggregation queries can be defined consistently on both a static dataset (e.g. from collected device events logs) as well as on a data stream, making the life of the user much easier.\nFurthermore, this model naturally handles data that has arrived later than\nexpected based on its event-time. Since Spark is updating the Result Table,\nit has full control over updating old aggregates when there is late data,\nas well as cleaning up old aggregates to limit the size of intermediate\nstate data. Since Spark 2.1, we have support for watermarking which\nallows the user to specify the threshold of late data, and allows the engine\nto accordingly clean up old state. These are explained later in more\ndetail in the\nWindow Operations\nsection.\nFault Tolerance Semantics\nDelivering end-to-end exactly-once semantics was one of key goals behind the design of Structured Streaming. To achieve that, we have designed the Structured Streaming sources, the sinks and the execution engine to reliably track the exact progress of the processing so that it can handle any kind of failure by restarting and/or reprocessing. Every streaming source is assumed to have offsets (similar to Kafka offsets, or Kinesis sequence numbers)\nto track the read position in the stream. The engine uses checkpointing and write-ahead logs to record the offset range of the data being processed in each trigger. The streaming sinks are designed to be idempotent for handling reprocessing. Together, using replayable sources and idempotent sinks, Structured Streaming can ensure\nend-to-end exactly-once semantics\nunder any failure."}
{"url": "https://spark.apache.org/docs/latest/streaming/performance-tips.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nStructured Streaming Programming Guide\nOverview\nGetting Started\nAPIs on DataFrames and Datasets\nPerformance Tips\nAsynchronous Progress Tracking\nContinuous Processing\nAdditional Information\nStructured Streaming Programming Guide\nTable of contents\nAsynchronous Progress Tracking\nWhat is it?\nAsynchronous progress tracking allows streaming queries to checkpoint progress asynchronously and in parallel to the actual data processing within a micro-batch, reducing latency associated with maintaining the offset log and commit log.\nHow does it work?\nStructured Streaming relies on persisting and managing offsets as progress indicators for query processing. Offset management operation directly impacts processing latency, because no data processing can occur until these operations are complete. Asynchronous progress tracking enables streaming queries to checkpoint progress without being impacted by these offset management operations.\nHow to use it?\nThe code snippet below provides an example of how to use this feature:\nval\nstream\n=\nspark\n.\nreadStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribe\"\n,\n\"in\"\n)\n.\nload\n()\nval\nquery\n=\nstream\n.\nwriteStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"topic\"\n,\n\"out\"\n)\n.\noption\n(\n\"checkpointLocation\"\n,\n\"/tmp/checkpoint\"\n)\n.\noption\n(\n\"asyncProgressTrackingEnabled\"\n,\n\"true\"\n)\n.\nstart\n()\nThe table below describes the configurations for this feature and default values associated with them.\nOption\nValue\nDefault\nDescription\nasyncProgressTrackingEnabled\ntrue/false\nfalse\nenable or disable asynchronous progress tracking\nasyncProgressTrackingCheckpointIntervalMs\nmillisecond\n1000\nthe interval in which we commit offsets and completion commits\nLimitations\nThe initial version of the feature has the following limitations:\nAsynchronous progress tracking is only supported in stateless queries using Kafka Sink\nExactly once end-to-end processing will not be supported with this asynchronous progress tracking because offset ranges for batch can be changed in case of failure. Though many sinks, such as Kafka sink, do not support writing exactly once anyways.\nSwitching the setting off\nTurning the async progress tracking off may cause the following exception to be thrown\njava\n.\nlang\n.\nIllegalStateException\n:\nbatch\nx\ndoesn\n'\nt\nexist\nAlso the following error message may be printed in the driver logs:\nThe offset log for batch x doesn't exist, which is required to restart the query from the latest batch x from the offset log. Please ensure there are two subsequent offset logs available for the latest batch via manually deleting the offset file(s). Please also ensure the latest batch for commit log is equal or one batch earlier than the latest batch for offset log.\nThis is caused by the fact that when async progress tracking is enabled, the framework will not checkpoint progress for every batch as would be done if async progress tracking is not used. To solve this problem simply re-enable “asyncProgressTrackingEnabled” and set “asyncProgressTrackingCheckpointIntervalMs” to 0 and run the streaming query until at least two micro-batches have been processed. Async progress tracking can be now safely disabled and restarting query should proceed normally.\nContinuous Processing\n[Experimental]\nContinuous processing\nis a new, experimental streaming execution mode introduced in Spark 2.3 that enables low (~1 ms) end-to-end latency with at-least-once fault-tolerance guarantees. Compare this with the default\nmicro-batch processing\nengine which can achieve exactly-once guarantees but achieve latencies of ~100ms at best. For some types of queries (discussed below), you can choose which mode to execute them in without modifying the application logic (i.e. without changing the DataFrame/Dataset operations).\nTo run a supported query in continuous processing mode, all you need to do is specify a\ncontinuous trigger\nwith the desired checkpoint interval as a parameter. For example,\nspark\n\\\n.\nreadStream\n\\\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\noption\n(\n\"\nsubscribe\n\"\n,\n\"\ntopic1\n\"\n)\n\\\n.\nload\n()\n\\\n.\nselectExpr\n(\n\"\nCAST(key AS STRING)\n\"\n,\n\"\nCAST(value AS STRING)\n\"\n)\n\\\n.\nwriteStream\n\\\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\noption\n(\n\"\ntopic\n\"\n,\n\"\ntopic1\n\"\n)\n\\\n.\ntrigger\n(\ncontinuous\n=\n\"\n1 second\n\"\n)\n\\\n# only change in query\n.\nstart\n()\nimport\norg.apache.spark.sql.streaming.Trigger\nspark\n.\nreadStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribe\"\n,\n\"topic1\"\n)\n.\nload\n()\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n)\n.\nwriteStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"topic\"\n,\n\"topic1\"\n)\n.\ntrigger\n(\nTrigger\n.\nContinuous\n(\n\"1 second\"\n))\n// only change in query\n.\nstart\n()\nimport\norg.apache.spark.sql.streaming.Trigger\n;\nspark\n.\nreadStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribe\"\n,\n\"topic1\"\n)\n.\nload\n()\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n)\n.\nwriteStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"topic\"\n,\n\"topic1\"\n)\n.\ntrigger\n(\nTrigger\n.\nContinuous\n(\n\"1 second\"\n))\n// only change in query\n.\nstart\n();\nA checkpoint interval of 1 second means that the continuous processing engine will record the progress of the query every second. The resulting checkpoints are in a format compatible with the micro-batch engine, hence any query can be restarted with any trigger. For example, a supported query started with the micro-batch mode can be restarted in continuous mode, and vice versa. Note that any time you switch to continuous mode, you will get at-least-once fault-tolerance guarantees.\nSupported Queries\nAs of Spark 2.4, only the following type of queries are supported in the continuous processing mode.\nOperations\n: Only map-like Dataset/DataFrame operations are supported in continuous mode, that is, only projections (\nselect\n,\nmap\n,\nflatMap\n,\nmapPartitions\n, etc.) and selections (\nwhere\n,\nfilter\n, etc.).\nAll SQL functions are supported except aggregation functions (since aggregations are not yet supported),\ncurrent_timestamp()\nand\ncurrent_date()\n(deterministic computations using time is challenging).\nSources\n:\nKafka source: All options are supported.\nRate source: Good for testing. Only options that are supported in the continuous mode are\nnumPartitions\nand\nrowsPerSecond\n.\nSinks\n:\nKafka sink: All options are supported.\nMemory sink: Good for debugging.\nConsole sink: Good for debugging. All options are supported. Note that the console will print every checkpoint interval that you have specified in the continuous trigger.\nSee\nInput Sources\nand\nOutput Sinks\nsections for more details on them. While the console sink is good for testing, the end-to-end low-latency processing can be best observed with Kafka as the source and sink, as this allows the engine to process the data and make the results available in the output topic within milliseconds of the input data being available in the input topic.\nCaveats\nContinuous processing engine launches multiple long-running tasks that continuously read data from sources, process it and continuously write to sinks. The number of tasks required by the query depends on how many partitions the query can read from the sources in parallel. Therefore, before starting a continuous processing query, you must ensure there are enough cores in the cluster to all the tasks in parallel. For example, if you are reading from a Kafka topic that has 10 partitions, then the cluster must have at least 10 cores for the query to make progress.\nStopping a continuous processing stream may produce spurious task termination warnings. These can be safely ignored.\nThere are currently no automatic retries of failed tasks. Any failure will lead to the query being stopped and it needs to be manually restarted from the checkpoint."}
{"url": "https://spark.apache.org/docs/latest/ml-migration-guide.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nMigration Guide: MLlib (Machine Learning)\nUpgrading from MLlib 3.5 to 4.0\nUpgrading from MLlib 2.4 to 3.0\nUpgrading from MLlib 2.2 to 2.3\nUpgrading from MLlib 2.1 to 2.2\nUpgrading from MLlib 2.0 to 2.1\nUpgrading from MLlib 1.6 to 2.0\nUpgrading from MLlib 1.5 to 1.6\nUpgrading from MLlib 1.4 to 1.5\nUpgrading from MLlib 1.3 to 1.4\nUpgrading from MLlib 1.2 to 1.3\nUpgrading from MLlib 1.1 to 1.2\nUpgrading from MLlib 1.0 to 1.1\nUpgrading from MLlib 0.9 to 1.0\nNote that this migration guide describes the items specific to MLlib.\nMany items of SQL migration can be applied when migrating MLlib to higher versions for DataFrame-based APIs.\nPlease refer\nMigration Guide: SQL, Datasets and DataFrame\n.\nUpgrading from MLlib 3.5 to 4.0\nBreaking changes\nThere are no breaking changes.\nDeprecations and changes of behavior\nDeprecations\nThere are no deprecations.\nChanges of behavior\nSPARK-51132\n:\nThe PMML XML schema version of exported PMML format models by\nPMML model export\nhas been upgraded from\nPMML-4_3\nto\nPMML-4_4\n.\nUpgrading from MLlib 2.4 to 3.0\nBreaking changes\nOneHotEncoder\nwhich is deprecated in 2.3, is removed in 3.0 and\nOneHotEncoderEstimator\nis now renamed to\nOneHotEncoder\n.\norg.apache.spark.ml.image.ImageSchema.readImages\nwhich is deprecated in 2.3, is removed in 3.0, use\nspark.read.format('image')\ninstead.\norg.apache.spark.mllib.clustering.KMeans.train\nwith param Int\nruns\nwhich is deprecated in 2.1, is removed in 3.0. Use\ntrain\nmethod without\nruns\ninstead.\norg.apache.spark.mllib.classification.LogisticRegressionWithSGD\nwhich is deprecated in 2.0, is removed in 3.0, use\norg.apache.spark.ml.classification.LogisticRegression\nor\nspark.mllib.classification.LogisticRegressionWithLBFGS\ninstead.\norg.apache.spark.mllib.feature.ChiSqSelectorModel.isSorted\nwhich is deprecated in 2.1, is removed in 3.0, is not intended for subclasses to use.\norg.apache.spark.mllib.regression.RidgeRegressionWithSGD\nwhich is deprecated in 2.0, is removed in 3.0, use\norg.apache.spark.ml.regression.LinearRegression\nwith\nelasticNetParam\n= 0.0. Note the default\nregParam\nis 0.01 for\nRidgeRegressionWithSGD\n, but is 0.0 for\nLinearRegression\n.\norg.apache.spark.mllib.regression.LassoWithSGD\nwhich is deprecated in 2.0, is removed in 3.0, use\norg.apache.spark.ml.regression.LinearRegression\nwith\nelasticNetParam\n= 1.0. Note the default\nregParam\nis 0.01 for\nLassoWithSGD\n, but is 0.0 for\nLinearRegression\n.\norg.apache.spark.mllib.regression.LinearRegressionWithSGD\nwhich is deprecated in 2.0, is removed in 3.0, use\norg.apache.spark.ml.regression.LinearRegression\nor\nLBFGS\ninstead.\norg.apache.spark.mllib.clustering.KMeans.getRuns\nand\nsetRuns\nwhich are deprecated in 2.1, are removed in 3.0, have no effect since Spark 2.0.0.\norg.apache.spark.ml.LinearSVCModel.setWeightCol\nwhich is deprecated in 2.4, is removed in 3.0, is not intended for users.\nFrom 3.0,\norg.apache.spark.ml.classification.MultilayerPerceptronClassificationModel\nextends\nMultilayerPerceptronParams\nto expose the training params. As a result,\nlayers\nin\nMultilayerPerceptronClassificationModel\nhas been changed from\nArray[Int]\nto\nIntArrayParam\n. Users should use\nMultilayerPerceptronClassificationModel.getLayers\ninstead of\nMultilayerPerceptronClassificationModel.layers\nto retrieve the size of layers.\norg.apache.spark.ml.classification.GBTClassifier.numTrees\nwhich is deprecated in 2.4.5, is removed in 3.0, use\ngetNumTrees\ninstead.\norg.apache.spark.ml.clustering.KMeansModel.computeCost\nwhich is deprecated in 2.4, is removed in 3.0, use\nClusteringEvaluator\ninstead.\nThe member variable\nprecision\nin\norg.apache.spark.mllib.evaluation.MulticlassMetrics\nwhich is deprecated in 2.0, is removed in 3.0. Use\naccuracy\ninstead.\nThe member variable\nrecall\nin\norg.apache.spark.mllib.evaluation.MulticlassMetrics\nwhich is deprecated in 2.0, is removed in 3.0. Use\naccuracy\ninstead.\nThe member variable\nfMeasure\nin\norg.apache.spark.mllib.evaluation.MulticlassMetrics\nwhich is deprecated in 2.0, is removed in 3.0. Use\naccuracy\ninstead.\norg.apache.spark.ml.util.GeneralMLWriter.context\nwhich is deprecated in 2.0, is removed in 3.0, use\nsession\ninstead.\norg.apache.spark.ml.util.MLWriter.context\nwhich is deprecated in 2.0, is removed in 3.0, use\nsession\ninstead.\norg.apache.spark.ml.util.MLReader.context\nwhich is deprecated in 2.0, is removed in 3.0, use\nsession\ninstead.\nabstract class UnaryTransformer[IN, OUT, T <: UnaryTransformer[IN, OUT, T]]\nis changed to\nabstract class UnaryTransformer[IN: TypeTag, OUT: TypeTag, T <: UnaryTransformer[IN, OUT, T]]\nin 3.0.\nDeprecations and changes of behavior\nDeprecations\nSPARK-11215\n:\nlabels\nin\nStringIndexerModel\nis deprecated and will be removed in 3.1.0. Use\nlabelsArray\ninstead.\nSPARK-25758\n:\ncomputeCost\nin\nBisectingKMeansModel\nis deprecated and will be removed in future versions. Use\nClusteringEvaluator\ninstead.\nChanges of behavior\nSPARK-11215\n:\n In Spark 2.4 and previous versions, when specifying\nfrequencyDesc\nor\nfrequencyAsc\nas\nstringOrderType\nparam in\nStringIndexer\n, in case of equal frequency, the order of\n strings is undefined. Since Spark 3.0, the strings with equal frequency are further\n sorted by alphabet. And since Spark 3.0,\nStringIndexer\nsupports encoding multiple\n columns.\nSPARK-20604\n:\n In prior to 3.0 releases,\nImputer\nrequires input column to be Double or Float. In 3.0, this\n restriction is lifted so\nImputer\ncan handle all numeric types.\nSPARK-23469\n:\nIn Spark 3.0, the\nHashingTF\nTransformer uses a corrected implementation of the murmur3 hash\nfunction to hash elements to vectors.\nHashingTF\nin Spark 3.0 will map elements to\ndifferent positions in vectors than in Spark 2. However,\nHashingTF\ncreated with Spark 2.x\nand loaded with Spark 3.0 will still use the previous hash function and will not change behavior.\nSPARK-28969\n:\nThe\nsetClassifier\nmethod in PySpark’s\nOneVsRestModel\nhas been removed in 3.0 for parity with\nthe Scala implementation. Callers should not need to set the classifier in the model after\ncreation.\nSPARK-25790\n:\n PCA adds the support for more than 65535 column matrix in Spark 3.0.\nSPARK-28927\n:\n When fitting ALS model on nondeterministic input data, previously if rerun happens, users\n would see ArrayIndexOutOfBoundsException caused by mismatch between In/Out user/item blocks.\n From 3.0, a SparkException with more clear message will be thrown, and original\n ArrayIndexOutOfBoundsException is wrapped.\nSPARK-29232\n:\n In prior to 3.0 releases,\nRandomForestRegressionModel\ndoesn’t update the parameter maps\n of the DecisionTreeRegressionModels underneath. This is fixed in 3.0.\nUpgrading from MLlib 2.2 to 2.3\nBreaking changes\nThe class and trait hierarchy for logistic regression model summaries was changed to be cleaner\nand better accommodate the addition of the multi-class summary. This is a breaking change for user\ncode that casts a\nLogisticRegressionTrainingSummary\nto a\nBinaryLogisticRegressionTrainingSummary\n. Users should instead use the\nmodel.binarySummary\nmethod. See\nSPARK-17139\nfor more detail\n(\nnote\nthis is an\nExperimental\nAPI). This\ndoes not\naffect the Python\nsummary\nmethod, which\nwill still work correctly for both multinomial and binary cases.\nDeprecations and changes of behavior\nDeprecations\nOneHotEncoder\nhas been deprecated and will be removed in\n3.0\n. It has been replaced by the\nnew\nOneHotEncoderEstimator\n(see\nSPARK-13030\n).\nNote\nthat\nOneHotEncoderEstimator\nwill be renamed to\nOneHotEncoder\nin\n3.0\n(but\nOneHotEncoderEstimator\nwill be kept as an alias).\nChanges of behavior\nSPARK-21027\n:\n The default parallelism used in\nOneVsRest\nis now set to 1 (i.e. serial). In\n2.2\nand\n earlier versions, the level of parallelism was set to the default threadpool size in Scala.\nSPARK-22156\n:\n The learning rate update for\nWord2Vec\nwas incorrect when\nnumIterations\nwas set greater than\n1\n. This will cause training results to be different between\n2.3\nand earlier versions.\nSPARK-21681\n:\n Fixed an edge case bug in multinomial logistic regression that resulted in incorrect coefficients\n when some features had zero variance.\nSPARK-16957\n:\n Tree algorithms now use mid-points for split values. This may change results from model training.\nSPARK-14657\n:\n Fixed an issue where the features generated by\nRFormula\nwithout an intercept were inconsistent\n with the output in R. This may change results from model training in this scenario.\nUpgrading from MLlib 2.1 to 2.2\nBreaking changes\nThere are no breaking changes.\nDeprecations and changes of behavior\nDeprecations\nThere are no deprecations.\nChanges of behavior\nSPARK-19787\n:\n Default value of\nregParam\nchanged from\n1.0\nto\n0.1\nfor\nALS.train\nmethod (marked\nDeveloperApi\n).\nNote\nthis does\nnot affect\nthe\nALS\nEstimator or Model, nor MLlib’s\nALS\nclass.\nSPARK-14772\n:\n Fixed inconsistency between Python and Scala APIs for\nParam.copy\nmethod.\nSPARK-11569\n:\nStringIndexer\nnow handles\nNULL\nvalues in the same way as unseen values. Previously an exception\n would always be thrown regardless of the setting of the\nhandleInvalid\nparameter.\nUpgrading from MLlib 2.0 to 2.1\nBreaking changes\nDeprecated methods removed\nsetLabelCol\nin\nfeature.ChiSqSelectorModel\nnumTrees\nin\nclassification.RandomForestClassificationModel\n(This now refers to the Param called\nnumTrees\n)\nnumTrees\nin\nregression.RandomForestRegressionModel\n(This now refers to the Param called\nnumTrees\n)\nmodel\nin\nregression.LinearRegressionSummary\nvalidateParams\nin\nPipelineStage\nvalidateParams\nin\nEvaluator\nDeprecations and changes of behavior\nDeprecations\nSPARK-18592\n:\nDeprecate all Param setter methods except for input/output column Params for\nDecisionTreeClassificationModel\n,\nGBTClassificationModel\n,\nRandomForestClassificationModel\n,\nDecisionTreeRegressionModel\n,\nGBTRegressionModel\nand\nRandomForestRegressionModel\nChanges of behavior\nSPARK-17870\n:\n Fix a bug of\nChiSqSelector\nwhich will likely change its result. Now\nChiSquareSelector\nuse pValue rather than raw statistic to select a fixed number of top features.\nSPARK-3261\n:\nKMeans\nreturns potentially fewer than k cluster centers in cases where k distinct centroids aren’t available or aren’t selected.\nSPARK-17389\n:\nKMeans\nreduces the default number of steps from 5 to 2 for the k-means|| initialization mode.\nUpgrading from MLlib 1.6 to 2.0\nBreaking changes\nThere were several breaking changes in Spark 2.0, which are outlined below.\nLinear algebra classes for DataFrame-based APIs\nSpark’s linear algebra dependencies were moved to a new project,\nmllib-local\n(see\nSPARK-13944\n). \nAs part of this change, the linear algebra classes were copied to a new package,\nspark.ml.linalg\n. \nThe DataFrame-based APIs in\nspark.ml\nnow depend on the\nspark.ml.linalg\nclasses, \nleading to a few breaking changes, predominantly in various model classes \n(see\nSPARK-14810\nfor a full list).\nNote:\nthe RDD-based APIs in\nspark.mllib\ncontinue to depend on the previous package\nspark.mllib.linalg\n.\nConverting vectors and matrices\nWhile most pipeline components support backward compatibility for loading, \nsome existing\nDataFrames\nand pipelines in Spark versions prior to 2.0, that contain vector or matrix \ncolumns, may need to be migrated to the new\nspark.ml\nvector and matrix types. \nUtilities for converting\nDataFrame\ncolumns from\nspark.mllib.linalg\nto\nspark.ml.linalg\ntypes\n(and vice versa) can be found in\nspark.mllib.util.MLUtils\n.\nThere are also utility methods available for converting single instances of \nvectors and matrices. Use the\nasML\nmethod on a\nmllib.linalg.Vector\n/\nmllib.linalg.Matrix\nfor converting to\nml.linalg\ntypes, and\nmllib.linalg.Vectors.fromML\n/\nmllib.linalg.Matrices.fromML\nfor converting to\nmllib.linalg\ntypes.\nfrom\npyspark.mllib.util\nimport\nMLUtils\n# convert DataFrame columns\nconvertedVecDF\n=\nMLUtils\n.\nconvertVectorColumnsToML\n(\nvecDF\n)\nconvertedMatrixDF\n=\nMLUtils\n.\nconvertMatrixColumnsToML\n(\nmatrixDF\n)\n# convert a single vector or matrix\nmlVec\n=\nmllibVec\n.\nasML\n()\nmlMat\n=\nmllibMat\n.\nasML\n()\nRefer to the\nMLUtils\nPython docs\nfor further detail.\nimport\norg.apache.spark.mllib.util.MLUtils\n// convert DataFrame columns\nval\nconvertedVecDF\n=\nMLUtils\n.\nconvertVectorColumnsToML\n(\nvecDF\n)\nval\nconvertedMatrixDF\n=\nMLUtils\n.\nconvertMatrixColumnsToML\n(\nmatrixDF\n)\n// convert a single vector or matrix\nval\nmlVec\n:\norg.apache.spark.ml.linalg.Vector\n=\nmllibVec\n.\nasML\nval\nmlMat\n:\norg.apache.spark.ml.linalg.Matrix\n=\nmllibMat\n.\nasML\nRefer to the\nMLUtils\nScala docs\nfor further detail.\nimport\norg.apache.spark.mllib.util.MLUtils\n;\nimport\norg.apache.spark.sql.Dataset\n;\n// convert DataFrame columns\nDataset\n<\nRow\n>\nconvertedVecDF\n=\nMLUtils\n.\nconvertVectorColumnsToML\n(\nvecDF\n);\nDataset\n<\nRow\n>\nconvertedMatrixDF\n=\nMLUtils\n.\nconvertMatrixColumnsToML\n(\nmatrixDF\n);\n// convert a single vector or matrix\norg\n.\napache\n.\nspark\n.\nml\n.\nlinalg\n.\nVector\nmlVec\n=\nmllibVec\n.\nasML\n();\norg\n.\napache\n.\nspark\n.\nml\n.\nlinalg\n.\nMatrix\nmlMat\n=\nmllibMat\n.\nasML\n();\nRefer to the\nMLUtils\nJava docs\nfor further detail.\nDeprecated methods removed\nSeveral deprecated methods were removed in the\nspark.mllib\nand\nspark.ml\npackages:\nsetScoreCol\nin\nml.evaluation.BinaryClassificationEvaluator\nweights\nin\nLinearRegression\nand\nLogisticRegression\nin\nspark.ml\nsetMaxNumIterations\nin\nmllib.optimization.LBFGS\n(marked as\nDeveloperApi\n)\ntreeReduce\nand\ntreeAggregate\nin\nmllib.rdd.RDDFunctions\n(these functions are available on\nRDD\ns directly, and were marked as\nDeveloperApi\n)\ndefaultStrategy\nin\nmllib.tree.configuration.Strategy\nbuild\nin\nmllib.tree.Node\nlibsvm loaders for multiclass and load/save labeledData methods in\nmllib.util.MLUtils\nA full list of breaking changes can be found at\nSPARK-14810\n.\nDeprecations and changes of behavior\nDeprecations\nDeprecations in the\nspark.mllib\nand\nspark.ml\npackages include:\nSPARK-14984\n:\n In\nspark.ml.regression.LinearRegressionSummary\n, the\nmodel\nfield has been deprecated.\nSPARK-13784\n:\n In\nspark.ml.regression.RandomForestRegressionModel\nand\nspark.ml.classification.RandomForestClassificationModel\n,\n the\nnumTrees\nparameter has been deprecated in favor of\ngetNumTrees\nmethod.\nSPARK-13761\n:\n In\nspark.ml.param.Params\n, the\nvalidateParams\nmethod has been deprecated.\n We move all functionality in overridden methods to the corresponding\ntransformSchema\n.\nSPARK-14829\n:\n In\nspark.mllib\npackage,\nLinearRegressionWithSGD\n,\nLassoWithSGD\n,\nRidgeRegressionWithSGD\nand\nLogisticRegressionWithSGD\nhave been deprecated.\n We encourage users to use\nspark.ml.regression.LinearRegression\nand\nspark.ml.classification.LogisticRegression\n.\nSPARK-14900\n:\n In\nspark.mllib.evaluation.MulticlassMetrics\n, the parameters\nprecision\n,\nrecall\nand\nfMeasure\nhave been deprecated in favor of\naccuracy\n.\nSPARK-15644\n:\n In\nspark.ml.util.MLReader\nand\nspark.ml.util.MLWriter\n, the\ncontext\nmethod has been deprecated in favor of\nsession\n.\nIn\nspark.ml.feature.ChiSqSelectorModel\n, the\nsetLabelCol\nmethod has been deprecated since it was not used by\nChiSqSelectorModel\n.\nChanges of behavior\nChanges of behavior in the\nspark.mllib\nand\nspark.ml\npackages include:\nSPARK-7780\n:\nspark.mllib.classification.LogisticRegressionWithLBFGS\ndirectly calls\nspark.ml.classification.LogisticRegression\nfor binary classification now.\n This will introduce the following behavior changes for\nspark.mllib.classification.LogisticRegressionWithLBFGS\n:\nThe intercept will not be regularized when training binary classification model with L1/L2 Updater.\nIf users set without regularization, training with or without feature scaling will return the same solution by the same convergence rate.\nSPARK-13429\n:\n In order to provide better and consistent result with\nspark.ml.classification.LogisticRegression\n,\n the default value of\nspark.mllib.classification.LogisticRegressionWithLBFGS\n:\nconvergenceTol\nhas been changed from 1E-4 to 1E-6.\nSPARK-12363\n:\n Fix a bug of\nPowerIterationClustering\nwhich will likely change its result.\nSPARK-13048\n:\nLDA\nusing the\nEM\noptimizer will keep the last checkpoint by default, if checkpointing is being used.\nSPARK-12153\n:\nWord2Vec\nnow respects sentence boundaries. Previously, it did not handle them correctly.\nSPARK-10574\n:\nHashingTF\nuses\nMurmurHash3\nas default hash algorithm in both\nspark.ml\nand\nspark.mllib\n.\nSPARK-14768\n:\n The\nexpectedType\nargument for PySpark\nParam\nwas removed.\nSPARK-14931\n:\n Some default\nParam\nvalues, which were mismatched between pipelines in Scala and Python, have been changed.\nSPARK-13600\n:\nQuantileDiscretizer\nnow uses\nspark.sql.DataFrameStatFunctions.approxQuantile\nto find splits (previously used custom sampling logic).\n The output buckets will differ for same input data and params.\nUpgrading from MLlib 1.5 to 1.6\nThere are no breaking API changes in the\nspark.mllib\nor\nspark.ml\npackages, but there are\ndeprecations and changes of behavior.\nDeprecations:\nSPARK-11358\n:\n In\nspark.mllib.clustering.KMeans\n, the\nruns\nparameter has been deprecated.\nSPARK-10592\n:\n In\nspark.ml.classification.LogisticRegressionModel\nand\nspark.ml.regression.LinearRegressionModel\n, the\nweights\nfield has been deprecated in favor of\n the new name\ncoefficients\n.  This helps disambiguate from instance (row) “weights” given to\n algorithms.\nChanges of behavior:\nSPARK-7770\n:\nspark.mllib.tree.GradientBoostedTrees\n:\nvalidationTol\nhas changed semantics in 1.6.\n Previously, it was a threshold for absolute change in error. Now, it resembles the behavior of\nGradientDescent\n’s\nconvergenceTol\n: For large errors, it uses relative error (relative to the\n previous error); for small errors (\n< 0.01\n), it uses absolute error.\nSPARK-11069\n:\nspark.ml.feature.RegexTokenizer\n: Previously, it did not convert strings to lowercase before\n tokenizing. Now, it converts to lowercase by default, with an option not to. This matches the\n behavior of the simpler\nTokenizer\ntransformer.\nUpgrading from MLlib 1.4 to 1.5\nIn the\nspark.mllib\npackage, there are no breaking API changes but several behavior changes:\nSPARK-9005\n:\nRegressionMetrics.explainedVariance\nreturns the average regression sum of squares.\nSPARK-8600\n:\nNaiveBayesModel.labels\nbecome\nsorted.\nSPARK-3382\n:\nGradientDescent\nhas a default\nconvergence tolerance\n1e-3\n, and hence iterations might end earlier than 1.4.\nIn the\nspark.ml\npackage, there exists one breaking API change and one behavior change:\nSPARK-9268\n: Java’s varargs support is removed\nfrom\nParams.setDefault\ndue to a\nScala compiler bug\n.\nSPARK-10097\n:\nEvaluator.isLargerBetter\nis\nadded to indicate metric ordering. Metrics like RMSE no longer flip signs as in 1.4.\nUpgrading from MLlib 1.3 to 1.4\nIn the\nspark.mllib\npackage, there were several breaking changes, but all in\nDeveloperApi\nor\nExperimental\nAPIs:\nGradient-Boosted Trees\n(Breaking change)\nThe signature of the\nLoss.gradient\nmethod was changed.  This is only an issues for users who wrote their own losses for GBTs.\n(Breaking change)\nThe\napply\nand\ncopy\nmethods for the case class\nBoostingStrategy\nhave been changed because of a modification to the case class fields.  This could be an issue for users who use\nBoostingStrategy\nto set GBT parameters.\n(Breaking change)\nThe return value of\nLDA.run\nhas changed.  It now returns an abstract class\nLDAModel\ninstead of the concrete class\nDistributedLDAModel\n.  The object of type\nLDAModel\ncan still be cast to the appropriate concrete type, which depends on the optimization algorithm.\nIn the\nspark.ml\npackage, several major API changes occurred, including:\nParam\nand other APIs for specifying parameters\nuid\nunique IDs for Pipeline components\nReorganization of certain classes\nSince the\nspark.ml\nAPI was an alpha component in Spark 1.3, we do not list all changes here.\nHowever, since 1.4\nspark.ml\nis no longer an alpha component, we will provide details on any API\nchanges for future releases.\nUpgrading from MLlib 1.2 to 1.3\nIn the\nspark.mllib\npackage, there were several breaking changes.  The first change (in\nALS\n) is the only one in a component not marked as Alpha or Experimental.\n(Breaking change)\nIn\nALS\n, the extraneous method\nsolveLeastSquares\nhas been removed.  The\nDeveloperApi\nmethod\nanalyzeBlocks\nwas also removed.\n(Breaking change)\nStandardScalerModel\nremains an Alpha component. In it, the\nvariance\nmethod has been replaced with the\nstd\nmethod.  To compute the column variance values returned by the original\nvariance\nmethod, simply square the standard deviation values returned by\nstd\n.\n(Breaking change)\nStreamingLinearRegressionWithSGD\nremains an Experimental component.  In it, there were two changes:\nThe constructor taking arguments was removed in favor of a builder pattern using the default constructor plus parameter setter methods.\nVariable\nmodel\nis no longer public.\n(Breaking change)\nDecisionTree\nremains an Experimental component.  In it and its associated classes, there were several changes:\nIn\nDecisionTree\n, the deprecated class method\ntrain\nhas been removed.  (The object/static\ntrain\nmethods remain.)\nIn\nStrategy\n, the\ncheckpointDir\nparameter has been removed.  Checkpointing is still supported, but the checkpoint directory must be set before calling tree and tree ensemble training.\nPythonMLlibAPI\n(the interface between Scala/Java and Python for MLlib) was a public API but is now private, declared\nprivate[python]\n.  This was never meant for external use.\nIn linear regression (including Lasso and ridge regression), the squared loss is now divided by 2.\nSo in order to produce the same result as in 1.2, the regularization parameter needs to be divided by 2 and the step size needs to be multiplied by 2.\nIn the\nspark.ml\npackage, the main API changes are from Spark SQL.  We list the most important changes here:\nThe old\nSchemaRDD\nhas been replaced with\nDataFrame\nwith a somewhat modified API.  All algorithms in\nspark.ml\nwhich used to use SchemaRDD now use DataFrame.\nIn Spark 1.2, we used implicit conversions from\nRDD\ns of\nLabeledPoint\ninto\nSchemaRDD\ns by calling\nimport sqlContext._\nwhere\nsqlContext\nwas an instance of\nSQLContext\n.  These implicits have been moved, so we now call\nimport sqlContext.implicits._\n.\nJava APIs for SQL have also changed accordingly.  Please see the examples above and the\nSpark SQL Programming Guide\nfor details.\nOther changes were in\nLogisticRegression\n:\nThe\nscoreCol\noutput column (with default value “score”) was renamed to be\nprobabilityCol\n(with default value “probability”).  The type was originally\nDouble\n(for the probability of class 1.0), but it is now\nVector\n(for the probability of each class, to support multiclass classification in the future).\nIn Spark 1.2,\nLogisticRegressionModel\ndid not include an intercept.  In Spark 1.3, it includes an intercept; however, it will always be 0.0 since it uses the default settings for\nspark.mllib.LogisticRegressionWithLBFGS\n.  The option to use an intercept will be added in the future.\nUpgrading from MLlib 1.1 to 1.2\nThe only API changes in MLlib v1.2 are in\nDecisionTree\n,\nwhich continues to be an experimental API in MLlib 1.2:\n(Breaking change)\nThe Scala API for classification takes a named argument specifying the number\nof classes.  In MLlib v1.1, this argument was called\nnumClasses\nin Python and\nnumClassesForClassification\nin Scala.  In MLlib v1.2, the names are both set to\nnumClasses\n.\nThis\nnumClasses\nparameter is specified either via\nStrategy\nor via\nDecisionTree\nstatic\ntrainClassifier\nand\ntrainRegressor\nmethods.\n(Breaking change)\nThe API for\nNode\nhas changed.\nThis should generally not affect user code, unless the user manually constructs decision trees\n(instead of using the\ntrainClassifier\nor\ntrainRegressor\nmethods).\nThe tree\nNode\nnow includes more information, including the probability of the predicted label\n(for classification).\nPrinting methods’ output has changed.  The\ntoString\n(Scala/Java) and\n__repr__\n(Python) methods used to print the full model; they now print a summary.  For the full model, use\ntoDebugString\n.\nExamples in the Spark distribution and examples in the\nDecision Trees Guide\nhave been updated accordingly.\nUpgrading from MLlib 1.0 to 1.1\nThe only API changes in MLlib v1.1 are in\nDecisionTree\n,\nwhich continues to be an experimental API in MLlib 1.1:\n(Breaking change)\nThe meaning of tree depth has been changed by 1 in order to match\nthe implementations of trees in\nscikit-learn\nand in\nrpart\n.\nIn MLlib v1.0, a depth-1 tree had 1 leaf node, and a depth-2 tree had 1 root node and 2 leaf nodes.\nIn MLlib v1.1, a depth-0 tree has 1 leaf node, and a depth-1 tree has 1 root node and 2 leaf nodes.\nThis depth is specified by the\nmaxDepth\nparameter in\nStrategy\nor via\nDecisionTree\nstatic\ntrainClassifier\nand\ntrainRegressor\nmethods.\n(Non-breaking change)\nWe recommend using the newly added\ntrainClassifier\nand\ntrainRegressor\nmethods to build a\nDecisionTree\n,\nrather than using the old parameter class\nStrategy\n.  These new training methods explicitly\nseparate classification and regression, and they replace specialized parameter types with\nsimple\nString\ntypes.\nExamples of the new recommended\ntrainClassifier\nand\ntrainRegressor\nare given in the\nDecision Trees Guide\n.\nUpgrading from MLlib 0.9 to 1.0\nIn MLlib v1.0, we support both dense and sparse input in a unified way, which introduces a few\nbreaking changes.  If your data is sparse, please store it in a sparse format instead of dense to\ntake advantage of sparsity in both storage and computation. Details are described below."}
{"url": "https://spark.apache.org/docs/latest/mllib-statistics.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nSummary statistics\nCorrelations\nStratified sampling\nHypothesis testing\nRandom data generation\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nBasic Statistics - RDD-based API\nSummary statistics\nCorrelations\nStratified sampling\nHypothesis testing\nStreaming Significance Testing\nRandom data generation\nKernel density estimation\n\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\wv}{\\mathbf{w}}\n\\newcommand{\\av}{\\mathbf{\\alpha}}\n\\newcommand{\\bv}{\\mathbf{b}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\id}{\\mathbf{I}}\n\\newcommand{\\ind}{\\mathbf{1}}\n\\newcommand{\\0}{\\mathbf{0}}\n\\newcommand{\\unit}{\\mathbf{e}}\n\\newcommand{\\one}{\\mathbf{1}}\n\\newcommand{\\zero}{\\mathbf{0}}\n\\]\nSummary statistics\nWe provide column summary statistics for\nRDD[Vector]\nthrough the function\ncolStats\navailable in\nStatistics\n.\ncolStats()\nreturns an instance of\nMultivariateStatisticalSummary\n,\nwhich contains the column-wise max, min, mean, variance, and number of nonzeros, as well as the\ntotal count.\nRefer to the\nMultivariateStatisticalSummary\nPython docs\nfor more details on the API.\nimport\nnumpy\nas\nnp\nfrom\npyspark.mllib.stat\nimport\nStatistics\nmat\n=\nsc\n.\nparallelize\n(\n[\nnp\n.\narray\n([\n1.0\n,\n10.0\n,\n100.0\n]),\nnp\n.\narray\n([\n2.0\n,\n20.0\n,\n200.0\n]),\nnp\n.\narray\n([\n3.0\n,\n30.0\n,\n300.0\n])]\n)\n# an RDD of Vectors\n# Compute column summary statistics.\nsummary\n=\nStatistics\n.\ncolStats\n(\nmat\n)\nprint\n(\nsummary\n.\nmean\n())\n# a dense vector containing the mean value for each column\nprint\n(\nsummary\n.\nvariance\n())\n# column-wise variance\nprint\n(\nsummary\n.\nnumNonzeros\n())\n# number of nonzeros in each column\nFind full example code at \"examples/src/main/python/mllib/summary_statistics_example.py\" in the Spark repo.\ncolStats()\nreturns an instance of\nMultivariateStatisticalSummary\n,\nwhich contains the column-wise max, min, mean, variance, and number of nonzeros, as well as the\ntotal count.\nRefer to the\nMultivariateStatisticalSummary\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.linalg.Vectors\nimport\norg.apache.spark.mllib.stat.\n{\nMultivariateStatisticalSummary\n,\nStatistics\n}\nval\nobservations\n=\nsc\n.\nparallelize\n(\nSeq\n(\nVectors\n.\ndense\n(\n1.0\n,\n10.0\n,\n100.0\n),\nVectors\n.\ndense\n(\n2.0\n,\n20.0\n,\n200.0\n),\nVectors\n.\ndense\n(\n3.0\n,\n30.0\n,\n300.0\n)\n)\n)\n// Compute column summary statistics.\nval\nsummary\n:\nMultivariateStatisticalSummary\n=\nStatistics\n.\ncolStats\n(\nobservations\n)\nprintln\n(\nsummary\n.\nmean\n)\n// a dense vector containing the mean value for each column\nprintln\n(\nsummary\n.\nvariance\n)\n// column-wise variance\nprintln\n(\nsummary\n.\nnumNonzeros\n)\n// number of nonzeros in each column\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\" in the Spark repo.\ncolStats()\nreturns an instance of\nMultivariateStatisticalSummary\n,\nwhich contains the column-wise max, min, mean, variance, and number of nonzeros, as well as the\ntotal count.\nRefer to the\nMultivariateStatisticalSummary\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.mllib.linalg.Vector\n;\nimport\norg.apache.spark.mllib.linalg.Vectors\n;\nimport\norg.apache.spark.mllib.stat.MultivariateStatisticalSummary\n;\nimport\norg.apache.spark.mllib.stat.Statistics\n;\nJavaRDD\n<\nVector\n>\nmat\n=\njsc\n.\nparallelize\n(\nArrays\n.\nasList\n(\nVectors\n.\ndense\n(\n1.0\n,\n10.0\n,\n100.0\n),\nVectors\n.\ndense\n(\n2.0\n,\n20.0\n,\n200.0\n),\nVectors\n.\ndense\n(\n3.0\n,\n30.0\n,\n300.0\n)\n)\n);\n// an RDD of Vectors\n// Compute column summary statistics.\nMultivariateStatisticalSummary\nsummary\n=\nStatistics\n.\ncolStats\n(\nmat\n.\nrdd\n());\nSystem\n.\nout\n.\nprintln\n(\nsummary\n.\nmean\n());\n// a dense vector containing the mean value for each column\nSystem\n.\nout\n.\nprintln\n(\nsummary\n.\nvariance\n());\n// column-wise variance\nSystem\n.\nout\n.\nprintln\n(\nsummary\n.\nnumNonzeros\n());\n// number of nonzeros in each column\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\" in the Spark repo.\nCorrelations\nCalculating the correlation between two series of data is a common operation in Statistics. In\nspark.mllib\nwe provide the flexibility to calculate pairwise correlations among many series. The supported\ncorrelation methods are currently Pearson’s and Spearman’s correlation.\nStatistics\nprovides methods to\ncalculate correlations between series. Depending on the type of input, two\nRDD[Double]\ns or\nan\nRDD[Vector]\n, the output will be a\nDouble\nor the correlation\nMatrix\nrespectively.\nRefer to the\nStatistics\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.stat\nimport\nStatistics\nseriesX\n=\nsc\n.\nparallelize\n([\n1.0\n,\n2.0\n,\n3.0\n,\n3.0\n,\n5.0\n])\n# a series\n# seriesY must have the same number of partitions and cardinality as seriesX\nseriesY\n=\nsc\n.\nparallelize\n([\n11.0\n,\n22.0\n,\n33.0\n,\n33.0\n,\n555.0\n])\n# Compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method.\n# If a method is not specified, Pearson's method will be used by default.\nprint\n(\n\"\nCorrelation is:\n\"\n+\nstr\n(\nStatistics\n.\ncorr\n(\nseriesX\n,\nseriesY\n,\nmethod\n=\n\"\npearson\n\"\n)))\ndata\n=\nsc\n.\nparallelize\n(\n[\nnp\n.\narray\n([\n1.0\n,\n10.0\n,\n100.0\n]),\nnp\n.\narray\n([\n2.0\n,\n20.0\n,\n200.0\n]),\nnp\n.\narray\n([\n5.0\n,\n33.0\n,\n366.0\n])]\n)\n# an RDD of Vectors\n# calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method.\n# If a method is not specified, Pearson's method will be used by default.\nprint\n(\nStatistics\n.\ncorr\n(\ndata\n,\nmethod\n=\n\"\npearson\n\"\n))\nFind full example code at \"examples/src/main/python/mllib/correlations_example.py\" in the Spark repo.\nStatistics\nprovides methods to\ncalculate correlations between series. Depending on the type of input, two\nRDD[Double]\ns or\nan\nRDD[Vector]\n, the output will be a\nDouble\nor the correlation\nMatrix\nrespectively.\nRefer to the\nStatistics\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.linalg._\nimport\norg.apache.spark.mllib.stat.Statistics\nimport\norg.apache.spark.rdd.RDD\nval\nseriesX\n:\nRDD\n[\nDouble\n]\n=\nsc\n.\nparallelize\n(\nimmutable\n.\nArraySeq\n.\nunsafeWrapArray\n(\nArray\n(\n1.0\n,\n2.0\n,\n3.0\n,\n3.0\n,\n5.0\n)))\n// a series\n// must have the same number of partitions and cardinality as seriesX\nval\nseriesY\n:\nRDD\n[\nDouble\n]\n=\nsc\n.\nparallelize\n(\nimmutable\n.\nArraySeq\n.\nunsafeWrapArray\n(\nArray\n(\n11.0\n,\n22.0\n,\n33.0\n,\n33.0\n,\n555.0\n)))\n// compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method. If a\n// method is not specified, Pearson's method will be used by default.\nval\ncorrelation\n:\nDouble\n=\nStatistics\n.\ncorr\n(\nseriesX\n,\nseriesY\n,\n\"pearson\"\n)\nprintln\n(\ns\n\"Correlation is: $correlation\"\n)\nval\ndata\n:\nRDD\n[\nVector\n]\n=\nsc\n.\nparallelize\n(\nSeq\n(\nVectors\n.\ndense\n(\n1.0\n,\n10.0\n,\n100.0\n),\nVectors\n.\ndense\n(\n2.0\n,\n20.0\n,\n200.0\n),\nVectors\n.\ndense\n(\n5.0\n,\n33.0\n,\n366.0\n))\n)\n// note that each Vector is a row and not a column\n// calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method\n// If a method is not specified, Pearson's method will be used by default.\nval\ncorrelMatrix\n:\nMatrix\n=\nStatistics\n.\ncorr\n(\ndata\n,\n\"pearson\"\n)\nprintln\n(\ncorrelMatrix\n.\ntoString\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\" in the Spark repo.\nStatistics\nprovides methods to\ncalculate correlations between series. Depending on the type of input, two\nJavaDoubleRDD\ns or\na\nJavaRDD<Vector>\n, the output will be a\nDouble\nor the correlation\nMatrix\nrespectively.\nRefer to the\nStatistics\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\norg.apache.spark.api.java.JavaDoubleRDD\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.mllib.linalg.Matrix\n;\nimport\norg.apache.spark.mllib.linalg.Vector\n;\nimport\norg.apache.spark.mllib.linalg.Vectors\n;\nimport\norg.apache.spark.mllib.stat.Statistics\n;\nJavaDoubleRDD\nseriesX\n=\njsc\n.\nparallelizeDoubles\n(\nArrays\n.\nasList\n(\n1.0\n,\n2.0\n,\n3.0\n,\n3.0\n,\n5.0\n));\n// a series\n// must have the same number of partitions and cardinality as seriesX\nJavaDoubleRDD\nseriesY\n=\njsc\n.\nparallelizeDoubles\n(\nArrays\n.\nasList\n(\n11.0\n,\n22.0\n,\n33.0\n,\n33.0\n,\n555.0\n));\n// compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method.\n// If a method is not specified, Pearson's method will be used by default.\ndouble\ncorrelation\n=\nStatistics\n.\ncorr\n(\nseriesX\n.\nsrdd\n(),\nseriesY\n.\nsrdd\n(),\n\"pearson\"\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Correlation is: \"\n+\ncorrelation\n);\n// note that each Vector is a row and not a column\nJavaRDD\n<\nVector\n>\ndata\n=\njsc\n.\nparallelize\n(\nArrays\n.\nasList\n(\nVectors\n.\ndense\n(\n1.0\n,\n10.0\n,\n100.0\n),\nVectors\n.\ndense\n(\n2.0\n,\n20.0\n,\n200.0\n),\nVectors\n.\ndense\n(\n5.0\n,\n33.0\n,\n366.0\n)\n)\n);\n// calculate the correlation matrix using Pearson's method.\n// Use \"spearman\" for Spearman's method.\n// If a method is not specified, Pearson's method will be used by default.\nMatrix\ncorrelMatrix\n=\nStatistics\n.\ncorr\n(\ndata\n.\nrdd\n(),\n\"pearson\"\n);\nSystem\n.\nout\n.\nprintln\n(\ncorrelMatrix\n.\ntoString\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\" in the Spark repo.\nStratified sampling\nUnlike the other statistics functions, which reside in\nspark.mllib\n, stratified sampling methods,\nsampleByKey\nand\nsampleByKeyExact\n, can be performed on RDD’s of key-value pairs. For stratified\nsampling, the keys can be thought of as a label and the value as a specific attribute. For example\nthe key can be man or woman, or document ids, and the respective values can be the list of ages\nof the people in the population or the list of words in the documents. The\nsampleByKey\nmethod\nwill flip a coin to decide whether an observation will be sampled or not, therefore requires one\npass over the data, and provides an\nexpected\nsample size.\nsampleByKeyExact\nrequires significant\nmore resources than the per-stratum simple random sampling used in\nsampleByKey\n, but will provide\nthe exact sampling size with 99.99% confidence.\nsampleByKeyExact\nis currently not supported in\npython.\nsampleByKey()\nallows users to\nsample approximately $\\lceil f_k \\cdot n_k \\rceil \\, \\forall k \\in K$ items, where $f_k$ is the\ndesired fraction for key $k$, $n_k$ is the number of key-value pairs for key $k$, and $K$ is the\nset of keys.\nNote:\nsampleByKeyExact()\nis currently not supported in Python.\n# an RDD of any key value pairs\ndata\n=\nsc\n.\nparallelize\n([(\n1\n,\n'\na\n'\n),\n(\n1\n,\n'\nb\n'\n),\n(\n2\n,\n'\nc\n'\n),\n(\n2\n,\n'\nd\n'\n),\n(\n2\n,\n'\ne\n'\n),\n(\n3\n,\n'\nf\n'\n)])\n# specify the exact fraction desired from each key as a dictionary\nfractions\n=\n{\n1\n:\n0.1\n,\n2\n:\n0.6\n,\n3\n:\n0.3\n}\napproxSample\n=\ndata\n.\nsampleByKey\n(\nFalse\n,\nfractions\n)\nFind full example code at \"examples/src/main/python/mllib/stratified_sampling_example.py\" in the Spark repo.\nsampleByKeyExact()\nallows users to\nsample exactly $\\lceil f_k \\cdot n_k \\rceil \\, \\forall k \\in K$ items, where $f_k$ is the desired\nfraction for key $k$, $n_k$ is the number of key-value pairs for key $k$, and $K$ is the set of\nkeys. Sampling without replacement requires one additional pass over the RDD to guarantee sample\nsize, whereas sampling with replacement requires two additional passes.\n// an RDD[(K, V)] of any key value pairs\nval\ndata\n=\nsc\n.\nparallelize\n(\nSeq\n((\n1\n,\n'a'\n),\n(\n1\n,\n'b'\n),\n(\n2\n,\n'c'\n),\n(\n2\n,\n'd'\n),\n(\n2\n,\n'e'\n),\n(\n3\n,\n'f'\n)))\n// specify the exact fraction desired from each key\nval\nfractions\n=\nMap\n(\n1\n->\n0.1\n,\n2\n->\n0.6\n,\n3\n->\n0.3\n)\n// Get an approximate sample from each stratum\nval\napproxSample\n=\ndata\n.\nsampleByKey\n(\nwithReplacement\n=\nfalse\n,\nfractions\n=\nfractions\n)\n// Get an exact sample from each stratum\nval\nexactSample\n=\ndata\n.\nsampleByKeyExact\n(\nwithReplacement\n=\nfalse\n,\nfractions\n=\nfractions\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\" in the Spark repo.\nsampleByKeyExact()\nallows users to\nsample exactly $\\lceil f_k \\cdot n_k \\rceil \\, \\forall k \\in K$ items, where $f_k$ is the desired\nfraction for key $k$, $n_k$ is the number of key-value pairs for key $k$, and $K$ is the set of\nkeys. Sampling without replacement requires one additional pass over the RDD to guarantee sample\nsize, whereas sampling with replacement requires two additional passes.\nimport\njava.util.*\n;\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.api.java.JavaPairRDD\n;\nList\n<\nTuple2\n<\nInteger\n,\nCharacter\n>>\nlist\n=\nArrays\n.\nasList\n(\nnew\nTuple2\n<>(\n1\n,\n'a'\n),\nnew\nTuple2\n<>(\n1\n,\n'b'\n),\nnew\nTuple2\n<>(\n2\n,\n'c'\n),\nnew\nTuple2\n<>(\n2\n,\n'd'\n),\nnew\nTuple2\n<>(\n2\n,\n'e'\n),\nnew\nTuple2\n<>(\n3\n,\n'f'\n)\n);\nJavaPairRDD\n<\nInteger\n,\nCharacter\n>\ndata\n=\njsc\n.\nparallelizePairs\n(\nlist\n);\n// specify the exact fraction desired from each key Map<K, Double>\nImmutableMap\n<\nInteger\n,\nDouble\n>\nfractions\n=\nImmutableMap\n.\nof\n(\n1\n,\n0.1\n,\n2\n,\n0.6\n,\n3\n,\n0.3\n);\n// Get an approximate sample from each stratum\nJavaPairRDD\n<\nInteger\n,\nCharacter\n>\napproxSample\n=\ndata\n.\nsampleByKey\n(\nfalse\n,\nfractions\n);\n// Get an exact sample from each stratum\nJavaPairRDD\n<\nInteger\n,\nCharacter\n>\nexactSample\n=\ndata\n.\nsampleByKeyExact\n(\nfalse\n,\nfractions\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\" in the Spark repo.\nHypothesis testing\nHypothesis testing is a powerful tool in statistics to determine whether a result is statistically\nsignificant, whether this result occurred by chance or not.\nspark.mllib\ncurrently supports Pearson’s\nchi-squared ( $\\chi^2$) tests for goodness of fit and independence. The input data types determine\nwhether the goodness of fit or the independence test is conducted. The goodness of fit test requires\nan input type of\nVector\n, whereas the independence test requires a\nMatrix\nas input.\nspark.mllib\nalso supports the input type\nRDD[LabeledPoint]\nto enable feature selection via chi-squared\nindependence tests.\nStatistics\nprovides methods to\nrun Pearson’s chi-squared tests. The following example demonstrates how to run and interpret\nhypothesis tests.\nRefer to the\nStatistics\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.linalg\nimport\nMatrices\n,\nVectors\nfrom\npyspark.mllib.regression\nimport\nLabeledPoint\nfrom\npyspark.mllib.stat\nimport\nStatistics\nvec\n=\nVectors\n.\ndense\n(\n0.1\n,\n0.15\n,\n0.2\n,\n0.3\n,\n0.25\n)\n# a vector composed of the frequencies of events\n# compute the goodness of fit. If a second vector to test against\n# is not supplied as a parameter, the test runs against a uniform distribution.\ngoodnessOfFitTestResult\n=\nStatistics\n.\nchiSqTest\n(\nvec\n)\n# summary of the test including the p-value, degrees of freedom,\n# test statistic, the method used, and the null hypothesis.\nprint\n(\n\"\n%s\n\\n\n\"\n%\ngoodnessOfFitTestResult\n)\nmat\n=\nMatrices\n.\ndense\n(\n3\n,\n2\n,\n[\n1.0\n,\n3.0\n,\n5.0\n,\n2.0\n,\n4.0\n,\n6.0\n])\n# a contingency matrix\n# conduct Pearson's independence test on the input contingency matrix\nindependenceTestResult\n=\nStatistics\n.\nchiSqTest\n(\nmat\n)\n# summary of the test including the p-value, degrees of freedom,\n# test statistic, the method used, and the null hypothesis.\nprint\n(\n\"\n%s\n\\n\n\"\n%\nindependenceTestResult\n)\nobs\n=\nsc\n.\nparallelize\n(\n[\nLabeledPoint\n(\n1.0\n,\n[\n1.0\n,\n0.0\n,\n3.0\n]),\nLabeledPoint\n(\n1.0\n,\n[\n1.0\n,\n2.0\n,\n0.0\n]),\nLabeledPoint\n(\n1.0\n,\n[\n-\n1.0\n,\n0.0\n,\n-\n0.5\n])]\n)\n# LabeledPoint(label, feature)\n# The contingency table is constructed from an RDD of LabeledPoint and used to conduct\n# the independence test. Returns an array containing the ChiSquaredTestResult for every feature\n# against the label.\nfeatureTestResults\n=\nStatistics\n.\nchiSqTest\n(\nobs\n)\nfor\ni\n,\nresult\nin\nenumerate\n(\nfeatureTestResults\n):\nprint\n(\n\"\nColumn %d:\n\\n\n%s\n\"\n%\n(\ni\n+\n1\n,\nresult\n))\nFind full example code at \"examples/src/main/python/mllib/hypothesis_testing_example.py\" in the Spark repo.\nStatistics\nprovides methods to\nrun Pearson’s chi-squared tests. The following example demonstrates how to run and interpret\nhypothesis tests.\nimport\norg.apache.spark.mllib.linalg._\nimport\norg.apache.spark.mllib.regression.LabeledPoint\nimport\norg.apache.spark.mllib.stat.Statistics\nimport\norg.apache.spark.mllib.stat.test.ChiSqTestResult\nimport\norg.apache.spark.rdd.RDD\n// a vector composed of the frequencies of events\nval\nvec\n:\nVector\n=\nVectors\n.\ndense\n(\n0.1\n,\n0.15\n,\n0.2\n,\n0.3\n,\n0.25\n)\n// compute the goodness of fit. If a second vector to test against is not supplied\n// as a parameter, the test runs against a uniform distribution.\nval\ngoodnessOfFitTestResult\n=\nStatistics\n.\nchiSqTest\n(\nvec\n)\n// summary of the test including the p-value, degrees of freedom, test statistic, the method\n// used, and the null hypothesis.\nprintln\n(\ns\n\"$goodnessOfFitTestResult\\n\"\n)\n// a contingency matrix. Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\nval\nmat\n:\nMatrix\n=\nMatrices\n.\ndense\n(\n3\n,\n2\n,\nArray\n(\n1.0\n,\n3.0\n,\n5.0\n,\n2.0\n,\n4.0\n,\n6.0\n))\n// conduct Pearson's independence test on the input contingency matrix\nval\nindependenceTestResult\n=\nStatistics\n.\nchiSqTest\n(\nmat\n)\n// summary of the test including the p-value, degrees of freedom\nprintln\n(\ns\n\"$independenceTestResult\\n\"\n)\nval\nobs\n:\nRDD\n[\nLabeledPoint\n]\n=\nsc\n.\nparallelize\n(\nSeq\n(\nLabeledPoint\n(\n1.0\n,\nVectors\n.\ndense\n(\n1.0\n,\n0.0\n,\n3.0\n)),\nLabeledPoint\n(\n1.0\n,\nVectors\n.\ndense\n(\n1.0\n,\n2.0\n,\n0.0\n)),\nLabeledPoint\n(-\n1.0\n,\nVectors\n.\ndense\n(-\n1.0\n,\n0.0\n,\n-\n0.5\n)\n)\n)\n)\n// (label, feature) pairs.\n// The contingency table is constructed from the raw (label, feature) pairs and used to conduct\n// the independence test. Returns an array containing the ChiSquaredTestResult for every feature\n// against the label.\nval\nfeatureTestResults\n:\nArray\n[\nChiSqTestResult\n]\n=\nStatistics\n.\nchiSqTest\n(\nobs\n)\nfeatureTestResults\n.\nzipWithIndex\n.\nforeach\n{\ncase\n(\nk\n,\nv\n)\n=>\nprintln\n(\ns\n\"Column ${(v + 1)} :\"\n)\nprintln\n(\nk\n)\n}\n// summary of the test\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\" in the Spark repo.\nStatistics\nprovides methods to\nrun Pearson’s chi-squared tests. The following example demonstrates how to run and interpret\nhypothesis tests.\nRefer to the\nChiSqTestResult\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.mllib.linalg.Matrices\n;\nimport\norg.apache.spark.mllib.linalg.Matrix\n;\nimport\norg.apache.spark.mllib.linalg.Vector\n;\nimport\norg.apache.spark.mllib.linalg.Vectors\n;\nimport\norg.apache.spark.mllib.regression.LabeledPoint\n;\nimport\norg.apache.spark.mllib.stat.Statistics\n;\nimport\norg.apache.spark.mllib.stat.test.ChiSqTestResult\n;\n// a vector composed of the frequencies of events\nVector\nvec\n=\nVectors\n.\ndense\n(\n0.1\n,\n0.15\n,\n0.2\n,\n0.3\n,\n0.25\n);\n// compute the goodness of fit. If a second vector to test against is not supplied\n// as a parameter, the test runs against a uniform distribution.\nChiSqTestResult\ngoodnessOfFitTestResult\n=\nStatistics\n.\nchiSqTest\n(\nvec\n);\n// summary of the test including the p-value, degrees of freedom, test statistic,\n// the method used, and the null hypothesis.\nSystem\n.\nout\n.\nprintln\n(\ngoodnessOfFitTestResult\n+\n\"\\n\"\n);\n// Create a contingency matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\nMatrix\nmat\n=\nMatrices\n.\ndense\n(\n3\n,\n2\n,\nnew\ndouble\n[]{\n1.0\n,\n3.0\n,\n5.0\n,\n2.0\n,\n4.0\n,\n6.0\n});\n// conduct Pearson's independence test on the input contingency matrix\nChiSqTestResult\nindependenceTestResult\n=\nStatistics\n.\nchiSqTest\n(\nmat\n);\n// summary of the test including the p-value, degrees of freedom...\nSystem\n.\nout\n.\nprintln\n(\nindependenceTestResult\n+\n\"\\n\"\n);\n// an RDD of labeled points\nJavaRDD\n<\nLabeledPoint\n>\nobs\n=\njsc\n.\nparallelize\n(\nArrays\n.\nasList\n(\nnew\nLabeledPoint\n(\n1.0\n,\nVectors\n.\ndense\n(\n1.0\n,\n0.0\n,\n3.0\n)),\nnew\nLabeledPoint\n(\n1.0\n,\nVectors\n.\ndense\n(\n1.0\n,\n2.0\n,\n0.0\n)),\nnew\nLabeledPoint\n(-\n1.0\n,\nVectors\n.\ndense\n(-\n1.0\n,\n0.0\n,\n-\n0.5\n))\n)\n);\n// The contingency table is constructed from the raw (label, feature) pairs and used to conduct\n// the independence test. Returns an array containing the ChiSquaredTestResult for every feature\n// against the label.\nChiSqTestResult\n[]\nfeatureTestResults\n=\nStatistics\n.\nchiSqTest\n(\nobs\n.\nrdd\n());\nint\ni\n=\n1\n;\nfor\n(\nChiSqTestResult\nresult\n:\nfeatureTestResults\n)\n{\nSystem\n.\nout\n.\nprintln\n(\n\"Column \"\n+\ni\n+\n\":\"\n);\nSystem\n.\nout\n.\nprintln\n(\nresult\n+\n\"\\n\"\n);\n// summary of the test\ni\n++;\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\" in the Spark repo.\nAdditionally,\nspark.mllib\nprovides a 1-sample, 2-sided implementation of the Kolmogorov-Smirnov (KS) test\nfor equality of probability distributions. By providing the name of a theoretical distribution\n(currently solely supported for the normal distribution) and its parameters, or a function to\ncalculate the cumulative distribution according to a given theoretical distribution, the user can\ntest the null hypothesis that their sample is drawn from that distribution. In the case that the\nuser tests against the normal distribution (\ndistName=\"norm\"\n), but does not provide distribution\nparameters, the test initializes to the standard normal distribution and logs an appropriate\nmessage.\nStatistics\nprovides methods to\nrun a 1-sample, 2-sided Kolmogorov-Smirnov test. The following example demonstrates how to run\nand interpret the hypothesis tests.\nRefer to the\nStatistics\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.stat\nimport\nStatistics\nparallelData\n=\nsc\n.\nparallelize\n([\n0.1\n,\n0.15\n,\n0.2\n,\n0.3\n,\n0.25\n])\n# run a KS test for the sample versus a standard normal distribution\ntestResult\n=\nStatistics\n.\nkolmogorovSmirnovTest\n(\nparallelData\n,\n\"\nnorm\n\"\n,\n0\n,\n1\n)\n# summary of the test including the p-value, test statistic, and null hypothesis\n# if our p-value indicates significance, we can reject the null hypothesis\n# Note that the Scala functionality of calling Statistics.kolmogorovSmirnovTest with\n# a lambda to calculate the CDF is not made available in the Python API\nprint\n(\ntestResult\n)\nFind full example code at \"examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\" in the Spark repo.\nStatistics\nprovides methods to\nrun a 1-sample, 2-sided Kolmogorov-Smirnov test. The following example demonstrates how to run\nand interpret the hypothesis tests.\nRefer to the\nStatistics\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.stat.Statistics\nimport\norg.apache.spark.rdd.RDD\nval\ndata\n:\nRDD\n[\nDouble\n]\n=\nsc\n.\nparallelize\n(\nSeq\n(\n0.1\n,\n0.15\n,\n0.2\n,\n0.3\n,\n0.25\n))\n// an RDD of sample data\n// run a KS test for the sample versus a standard normal distribution\nval\ntestResult\n=\nStatistics\n.\nkolmogorovSmirnovTest\n(\ndata\n,\n\"norm\"\n,\n0\n,\n1\n)\n// summary of the test including the p-value, test statistic, and null hypothesis if our p-value\n// indicates significance, we can reject the null hypothesis.\nprintln\n(\ntestResult\n)\nprintln\n()\n// perform a KS test using a cumulative distribution function of our making\nval\nmyCDF\n=\nMap\n(\n0.1\n->\n0.2\n,\n0.15\n->\n0.6\n,\n0.2\n->\n0.05\n,\n0.3\n->\n0.05\n,\n0.25\n->\n0.1\n)\nval\ntestResult2\n=\nStatistics\n.\nkolmogorovSmirnovTest\n(\ndata\n,\nmyCDF\n)\nprintln\n(\ntestResult2\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\" in the Spark repo.\nStatistics\nprovides methods to\nrun a 1-sample, 2-sided Kolmogorov-Smirnov test. The following example demonstrates how to run\nand interpret the hypothesis tests.\nRefer to the\nStatistics\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\norg.apache.spark.api.java.JavaDoubleRDD\n;\nimport\norg.apache.spark.mllib.stat.Statistics\n;\nimport\norg.apache.spark.mllib.stat.test.KolmogorovSmirnovTestResult\n;\nJavaDoubleRDD\ndata\n=\njsc\n.\nparallelizeDoubles\n(\nArrays\n.\nasList\n(\n0.1\n,\n0.15\n,\n0.2\n,\n0.3\n,\n0.25\n));\nKolmogorovSmirnovTestResult\ntestResult\n=\nStatistics\n.\nkolmogorovSmirnovTest\n(\ndata\n,\n\"norm\"\n,\n0.0\n,\n1.0\n);\n// summary of the test including the p-value, test statistic, and null hypothesis\n// if our p-value indicates significance, we can reject the null hypothesis\nSystem\n.\nout\n.\nprintln\n(\ntestResult\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\" in the Spark repo.\nStreaming Significance Testing\nspark.mllib\nprovides online implementations of some tests to support use cases\nlike A/B testing. These tests may be performed on a Spark Streaming\nDStream[(Boolean, Double)]\nwhere the first element of each tuple\nindicates control group (\nfalse\n) or treatment group (\ntrue\n) and the\nsecond element is the value of an observation.\nStreaming significance testing supports the following parameters:\npeacePeriod\n- The number of initial data points from the stream to\nignore, used to mitigate novelty effects.\nwindowSize\n- The number of past batches to perform hypothesis\ntesting over. Setting to\n0\nwill perform cumulative processing using\nall prior batches.\nStreamingTest\nprovides streaming hypothesis testing.\nval\ndata\n=\nssc\n.\ntextFileStream\n(\ndataDir\n).\nmap\n(\nline\n=>\nline\n.\nsplit\n(\n\",\"\n)\nmatch\n{\ncase\nArray\n(\nlabel\n,\nvalue\n)\n=>\nBinarySample\n(\nlabel\n.\ntoBoolean\n,\nvalue\n.\ntoDouble\n)\n})\nval\nstreamingTest\n=\nnew\nStreamingTest\n()\n.\nsetPeacePeriod\n(\n0\n)\n.\nsetWindowSize\n(\n0\n)\n.\nsetTestMethod\n(\n\"welch\"\n)\nval\nout\n=\nstreamingTest\n.\nregisterStream\n(\ndata\n)\nout\n.\nprint\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\" in the Spark repo.\nStreamingTest\nprovides streaming hypothesis testing.\nimport\norg.apache.spark.mllib.stat.test.BinarySample\n;\nimport\norg.apache.spark.mllib.stat.test.StreamingTest\n;\nimport\norg.apache.spark.mllib.stat.test.StreamingTestResult\n;\nJavaDStream\n<\nBinarySample\n>\ndata\n=\nssc\n.\ntextFileStream\n(\ndataDir\n).\nmap\n(\nline\n->\n{\nString\n[]\nts\n=\nline\n.\nsplit\n(\n\",\"\n);\nboolean\nlabel\n=\nBoolean\n.\nparseBoolean\n(\nts\n[\n0\n]);\ndouble\nvalue\n=\nDouble\n.\nparseDouble\n(\nts\n[\n1\n]);\nreturn\nnew\nBinarySample\n(\nlabel\n,\nvalue\n);\n});\nStreamingTest\nstreamingTest\n=\nnew\nStreamingTest\n()\n.\nsetPeacePeriod\n(\n0\n)\n.\nsetWindowSize\n(\n0\n)\n.\nsetTestMethod\n(\n\"welch\"\n);\nJavaDStream\n<\nStreamingTestResult\n>\nout\n=\nstreamingTest\n.\nregisterStream\n(\ndata\n);\nout\n.\nprint\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\" in the Spark repo.\nRandom data generation\nRandom data generation is useful for randomized algorithms, prototyping, and performance testing.\nspark.mllib\nsupports generating random RDDs with i.i.d. values drawn from a given distribution:\nuniform, standard normal, or Poisson.\nRandomRDDs\nprovides factory\nmethods to generate random double RDDs or vector RDDs.\nThe following example generates a random double RDD, whose values follows the standard normal\ndistribution\nN(0, 1)\n, and then map it to\nN(1, 4)\n.\nRefer to the\nRandomRDDs\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.random\nimport\nRandomRDDs\nsc\n=\n...\n# SparkContext\n# Generate a random double RDD that contains 1 million i.i.d. values drawn from the\n# standard normal distribution `N(0, 1)`, evenly distributed in 10 partitions.\nu\n=\nRandomRDDs\n.\nnormalRDD\n(\nsc\n,\n1000000L\n,\n10\n)\n# Apply a transform to get a random double RDD following `N(1, 4)`.\nv\n=\nu\n.\nmap\n(\nlambda\nx\n:\n1.0\n+\n2.0\n*\nx\n)\nRandomRDDs\nprovides factory\nmethods to generate random double RDDs or vector RDDs.\nThe following example generates a random double RDD, whose values follows the standard normal\ndistribution\nN(0, 1)\n, and then map it to\nN(1, 4)\n.\nRefer to the\nRandomRDDs\nScala docs\nfor details on the API.\nimport\norg.apache.spark.SparkContext\nimport\norg.apache.spark.mllib.random.RandomRDDs._\nval\nsc\n:\nSparkContext\n=\n...\n// Generate a random double RDD that contains 1 million i.i.d. values drawn from the\n// standard normal distribution `N(0, 1)`, evenly distributed in 10 partitions.\nval\nu\n=\nnormalRDD\n(\nsc\n,\n1000000L\n,\n10\n)\n// Apply a transform to get a random double RDD following `N(1, 4)`.\nval\nv\n=\nu\n.\nmap\n(\nx\n=>\n1.0\n+\n2.0\n*\nx\n)\nRandomRDDs\nprovides factory\nmethods to generate random double RDDs or vector RDDs.\nThe following example generates a random double RDD, whose values follows the standard normal\ndistribution\nN(0, 1)\n, and then map it to\nN(1, 4)\n.\nRefer to the\nRandomRDDs\nJava docs\nfor details on the API.\nimport\norg.apache.spark.SparkContext\n;\nimport\norg.apache.spark.api.JavaDoubleRDD\n;\nimport\nstatic\norg\n.\napache\n.\nspark\n.\nmllib\n.\nrandom\n.\nRandomRDDs\n.*;\nJavaSparkContext\njsc\n=\n...\n// Generate a random double RDD that contains 1 million i.i.d. values drawn from the\n// standard normal distribution `N(0, 1)`, evenly distributed in 10 partitions.\nJavaDoubleRDD\nu\n=\nnormalJavaRDD\n(\njsc\n,\n1000000L\n,\n10\n);\n// Apply a transform to get a random double RDD following `N(1, 4)`.\nJavaDoubleRDD\nv\n=\nu\n.\nmapToDouble\n(\nx\n->\n1.0\n+\n2.0\n*\nx\n);\nKernel density estimation\nKernel density estimation\nis a technique\nuseful for visualizing empirical probability distributions without requiring assumptions about the\nparticular distribution that the observed samples are drawn from. It computes an estimate of the\nprobability density function of a random variables, evaluated at a given set of points. It achieves\nthis estimate by expressing the PDF of the empirical distribution at a particular point as the\nmean of PDFs of normal distributions centered around each of the samples.\nKernelDensity\nprovides methods\nto compute kernel density estimates from an RDD of samples. The following example demonstrates how\nto do so.\nRefer to the\nKernelDensity\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.stat\nimport\nKernelDensity\n# an RDD of sample data\ndata\n=\nsc\n.\nparallelize\n([\n1.0\n,\n1.0\n,\n1.0\n,\n2.0\n,\n3.0\n,\n4.0\n,\n5.0\n,\n5.0\n,\n6.0\n,\n7.0\n,\n8.0\n,\n9.0\n,\n9.0\n])\n# Construct the density estimator with the sample data and a standard deviation for the Gaussian\n# kernels\nkd\n=\nKernelDensity\n()\nkd\n.\nsetSample\n(\ndata\n)\nkd\n.\nsetBandwidth\n(\n3.0\n)\n# Find density estimates for the given values\ndensities\n=\nkd\n.\nestimate\n([\n-\n1.0\n,\n2.0\n,\n5.0\n])\nFind full example code at \"examples/src/main/python/mllib/kernel_density_estimation_example.py\" in the Spark repo.\nKernelDensity\nprovides methods\nto compute kernel density estimates from an RDD of samples. The following example demonstrates how\nto do so.\nRefer to the\nKernelDensity\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.stat.KernelDensity\nimport\norg.apache.spark.rdd.RDD\n// an RDD of sample data\nval\ndata\n:\nRDD\n[\nDouble\n]\n=\nsc\n.\nparallelize\n(\nSeq\n(\n1\n,\n1\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n,\n5\n,\n6\n,\n7\n,\n8\n,\n9\n,\n9\n))\n// Construct the density estimator with the sample data and a standard deviation\n// for the Gaussian kernels\nval\nkd\n=\nnew\nKernelDensity\n()\n.\nsetSample\n(\ndata\n)\n.\nsetBandwidth\n(\n3.0\n)\n// Find density estimates for the given values\nval\ndensities\n=\nkd\n.\nestimate\n(\nArray\n(-\n1.0\n,\n2.0\n,\n5.0\n))\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\" in the Spark repo.\nKernelDensity\nprovides methods\nto compute kernel density estimates from an RDD of samples. The following example demonstrates how\nto do so.\nRefer to the\nKernelDensity\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.mllib.stat.KernelDensity\n;\n// an RDD of sample data\nJavaRDD\n<\nDouble\n>\ndata\n=\njsc\n.\nparallelize\n(\nArrays\n.\nasList\n(\n1.0\n,\n1.0\n,\n1.0\n,\n2.0\n,\n3.0\n,\n4.0\n,\n5.0\n,\n5.0\n,\n6.0\n,\n7.0\n,\n8.0\n,\n9.0\n,\n9.0\n));\n// Construct the density estimator with the sample data\n// and a standard deviation for the Gaussian kernels\nKernelDensity\nkd\n=\nnew\nKernelDensity\n().\nsetSample\n(\ndata\n).\nsetBandwidth\n(\n3.0\n);\n// Find density estimates for the given values\ndouble\n[]\ndensities\n=\nkd\n.\nestimate\n(\nnew\ndouble\n[]{-\n1.0\n,\n2.0\n,\n5.0\n});\nSystem\n.\nout\n.\nprintln\n(\nArrays\n.\ntoString\n(\ndensities\n));\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\" in the Spark repo."}
{"url": "https://spark.apache.org/docs/latest/sql-pipe-syntax.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nSQL Pipe Syntax\nSyntax\nOverview\nApache Spark supports SQL pipe syntax which allows composing queries from combinations of operators.\nAny query can have zero or more pipe operators as a suffix, delineated by the pipe character\n|>\n.\nEach pipe operator starts with one or more SQL keywords followed by its own grammar as described\nin the table below.\nMost of these operators reuse existing grammar for standard SQL clauses.\nOperators can apply in any order, any number of times.\nFROM <tableName>\nis now a supported standalone query which behaves the same as\nTABLE <tableName>\n. This provides a convenient starting place to begin a chained pipe SQL query,\nalthough it is possible to add one or more pipe operators to the end of any valid Spark SQL query\nwith the same consistent behavior as written here.\nPlease refer to the table at the end of this document for a full list of all supported operators\nand their semantics.\nExample\nFor example, this is query 13 from the TPC-H benchmark:\nSELECT\nc_count\n,\nCOUNT\n(\n*\n)\nAS\ncustdist\nFROM\n(\nSELECT\nc_custkey\n,\nCOUNT\n(\no_orderkey\n)\nc_count\nFROM\ncustomer\nLEFT\nOUTER\nJOIN\norders\nON\nc_custkey\n=\no_custkey\nAND\no_comment\nNOT\nLIKE\n'%unusual%packages%'\nGROUP\nBY\nc_custkey\n)\nAS\nc_orders\nGROUP\nBY\nc_count\nORDER\nBY\ncustdist\nDESC\n,\nc_count\nDESC\n;\nTo write the same logic using SQL pipe operators, we express it like this:\nFROM\ncustomer\n|>\nLEFT\nOUTER\nJOIN\norders\nON\nc_custkey\n=\no_custkey\nAND\no_comment\nNOT\nLIKE\n'%unusual%packages%'\n|>\nAGGREGATE\nCOUNT\n(\no_orderkey\n)\nc_count\nGROUP\nBY\nc_custkey\n|>\nAGGREGATE\nCOUNT\n(\n*\n)\nAS\ncustdist\nGROUP\nBY\nc_count\n|>\nORDER\nBY\ncustdist\nDESC\n,\nc_count\nDESC\n;\nSource Tables\nTo start a new query using SQL pipe syntax, use the\nFROM <tableName>\nor\nTABLE <tableName>\nclause, which creates a relation comprising all rows from the source table. Then append one or more\npipe operators to the end of this clause to perform further transformations.\nProjections\nSQL pipe syntax supports composable ways to evaluate expressions. A major advantage of these\nprojection features is that they support computing new expressions based on previous ones in an\nincremental way. No lateral column references are needed here since each operator applies\nindependently on its input table, regardless of the order in which the operators appear. Each of\nthese computed columns then becomes visible to use with the following operator.\nSELECT\nproduces a new table by evaluating the provided expressions.\nIt is possible to use\nDISTINCT\nand\n*\nas needed.\nThis works like the outermost\nSELECT\nin a table subquery in regular Spark SQL.\nEXTEND\nadds new columns to the input table by evaluating the provided expressions.\nThis also preserves table aliases.\nThis works like\nSELECT *, new_column\nin regular Spark SQL.\nDROP\nremoves columns from the input table.\nThis is similar to\nSELECT * EXCEPT (column)\nin regular Spark SQL.\nSET\nreplaces column values from the input table.\nThis is similar to\nSELECT * REPLACE (expression AS column)\nin regular Spark SQL.\nAS\nforwards the input table and introduces a new alias for each row.\nAggregations\nIn general, aggregation takes place differently using SQL pipe syntax as opposed to regular Spark\nSQL.\nTo perform full-table aggregation, use the\nAGGREGATE\noperator with a list of aggregate\nexpressions to evaluate. This returns one single row in the output table.\nTo perform aggregation with grouping, use the\nAGGREGATE\noperator with a\nGROUP BY\nclause.\nThis returns one row for each unique combination of values of the grouping expressions. The output\ntable contains the evaluated grouping expressions followed by the evaluated aggregate functions.\nGrouping expressions support assigning aliases for purposes of referring to them in future\noperators. In this way, it is not necessary to repeat entire expressions between\nGROUP BY\nand\nSELECT\n, since\nAGGREGATE\nis a single operator that performs both.\nOther Transformations\nThe remaining operators are used for other transformations, such as filtering, joining, sorting,\nsampling, and set operations. These operators generally work in the same way as in regular Spark\nSQL, as described in the table below.\nIndependence and Interoperability\nSQL pipe syntax works in Spark without any backwards-compatibility concerns with existing SQL\nqueries; it is possible to write any query using regular Spark SQL, pipe syntax, or a combination of\nthe two. As a consequence, the following invariants always hold:\nEach pipe operator receives an input table and operates the same way on its rows regardless of how\nit was computed.\nFor any valid chain of N SQL pipe operators, any subset of the first M <= N operators also\nrepresents a valid query.\nThis property can be useful for introspection and debugging, such as by selected a subset of\nlines and using the “run highlighted text” feature of SQL editors like Jupyter notebooks.\nIt is possible to append pipe operators to any valid query written in regular Spark SQL.\nThe canonical way of starting pipe syntax queries is with the\nFROM <tableName>\nclause.\nNote that this is a valid standalone query and may be replaced with any other Spark SQL query\nwithout loss of generality.\nTable subqueries can be written using either regular Spark SQL syntax or pipe syntax.\nThey may appear inside enclosing queries written in either syntax.\nOther Spark SQL statements such as views and DDL and DML commands may include queries written\nusing either syntax.\nSupported Operators\nOperator\nOutput rows\nFROM\nor\nTABLE\nReturns all the output rows from the source table unmodified.\nSELECT\nEvaluates the provided expressions over each of the rows of the input table.\nEXTEND\nAppends new columns to the input table by evaluating the specified expressions over each of the input rows.\nSET\nUpdates columns of the input table by replacing them with the result of evaluating the provided expressions.\nDROP\nDrops columns of the input table by name.\nAS\nRetains the same rows and column names of the input table but with a new table alias.\nWHERE\nReturns the subset of input rows passing the condition.\nLIMIT\nReturns the specified number of input rows, preserving ordering (if any).\nAGGREGATE\nPerforms aggregation with or without grouping.\nJOIN\nJoins rows from both inputs, returning a filtered cross-product of the input table and the table argument.\nORDER BY\nReturns the input rows after sorting as indicated.\nUNION ALL\nPerforms the union or other set operation over the combined rows from the input table plus other table argument(s).\nTABLESAMPLE\nReturns the subset of rows chosen by the provided sampling algorithm.\nPIVOT\nReturns a new table with the input rows pivoted to become columns.\nUNPIVOT\nReturns a new table with the input columns pivoted to become rows.\nThis table lists each of the supported pipe operators and describes the output rows they produce.\nNote that each operator accepts an input relation comprising the rows generated by the query\npreceding the\n|>\nsymbol.\nFROM or TABLE\nFROM\n<\ntableName\n>\nTABLE\n<\ntableName\n>\nReturns all the output rows from the source table unmodified.\nFor example:\nCREATE\nTABLE\nt\nAS\nVALUES\n(\n1\n,\n2\n),\n(\n3\n,\n4\n)\nAS\nt\n(\na\n,\nb\n);\nTABLE\nt\n;\n+\n---+---+\n|\na\n|\nb\n|\n+\n---+---+\n|\n1\n|\n2\n|\n|\n3\n|\n4\n|\n+\n---+---+\nSELECT\n|>\nSELECT\n<\nexpr\n>\n[[\nAS\n]\nalias\n],\n...\nEvaluates the provided expressions over each of the rows of the input table.\nIn general, this operator is not always required with SQL pipe syntax. It is possible to use it at\nor near the end of a query to evaluate expressions or specify a list of output columns.\nSince the final query result always comprises the columns returned from the last pipe operator,\nwhen this\nSELECT\noperator does not appear, the output includes all columns from the full row.\nThis behavior is similar to\nSELECT *\nin standard SQL syntax.\nIt is possible to use\nDISTINCT\nand\n*\nas needed.\nThis works like the outermost\nSELECT\nin a table subquery in regular Spark SQL.\nWindow functions are supported in the\nSELECT\nlist as well. To use them, the\nOVER\nclause must be\nprovided. You may provide the window specification in the\nWINDOW\nclause.\nAggregate functions are not supported in this operator. To perform aggregation, use the\nAGGREGATE\noperator instead.\nFor example:\nCREATE\nTABLE\nt\nAS\nVALUES\n(\n0\n),\n(\n1\n)\nAS\nt\n(\ncol\n);\nFROM\nt\n|>\nSELECT\ncol\n*\n2\nAS\nresult\n;\n+\n------+\n|\nresult\n|\n+\n------+\n|\n0\n|\n|\n2\n|\n+\n------+\nEXTEND\n|>\nEXTEND\n<\nexpr\n>\n[[\nAS\n]\nalias\n],\n...\nAppends new columns to the input table by evaluating the specified expressions over each of the\ninput rows.\nAfter an\nEXTEND\noperation, top-level column names are updated but table aliases still refer to the\noriginal row values (such as an inner join between two tables\nlhs\nand\nrhs\nwith a subsequent\nEXTEND\nand then\nSELECT lhs.col, rhs.col\n).\nFor example:\nVALUES\n(\n0\n),\n(\n1\n)\ntab\n(\ncol\n)\n|>\nEXTEND\ncol\n*\n2\nAS\nresult\n;\n+\n---+------+\n|\ncol\n|\nresult\n|\n+\n---+------+\n|\n0\n|\n0\n|\n|\n1\n|\n2\n|\n+\n---+------+\nSET\n|>\nSET\n<\ncolumn\n>\n=\n<\nexpression\n>\n,\n...\nUpdates columns of the input table by replacing them with the result of evaluating the provided\nexpressions. Each such column reference must appear in the input table exactly once.\nThis is similar to\nSELECT * EXCEPT (column), <expression> AS column\nin regular Spark SQL.\nIt is possible to perform multiple assignments in a single\nSET\nclause. Each assignment may refer\nto the result of previous assignments.\nAfter an assignment, top-level column names are updated but table aliases still refer to the\noriginal row values (such as an inner join between two tables\nlhs\nand\nrhs\nwith a subsequent\nSET\nand then\nSELECT lhs.col, rhs.col\n).\nFor example:\nVALUES\n(\n0\n),\n(\n1\n)\ntab\n(\ncol\n)\n|>\nSET\ncol\n=\ncol\n*\n2\n;\n+\n---+\n|\ncol\n|\n+\n---+\n|\n0\n|\n|\n2\n|\n+\n---+\nVALUES\n(\n0\n),\n(\n1\n)\ntab\n(\ncol\n)\n|>\nSET\ncol\n=\ncol\n*\n2\n;\n+\n---+\n|\ncol\n|\n+\n---+\n|\n0\n|\n|\n2\n|\n+\n---+\nDROP\n|>\nDROP\n<\ncolumn\n>\n,\n...\nDrops columns of the input table by name. Each such column reference must appear in the input table\nexactly once.\nThis is similar to\nSELECT * EXCEPT (column)\nin regular Spark SQL.\nAfter a\nDROP\noperation, top-level column names are updated but table aliases still refer to the\noriginal row values (such as an inner join between two tables\nlhs\nand\nrhs\nwith a subsequent\nDROP\nand then\nSELECT lhs.col, rhs.col\n).\nFor example:\nVALUES\n(\n0\n,\n1\n)\ntab\n(\ncol1\n,\ncol2\n)\n|>\nDROP\ncol1\n;\n+\n----+\n|\ncol2\n|\n+\n----+\n|\n1\n|\n+\n----+\nAS\n|>\nAS\n<\nalias\n>\nRetains the same rows and column names of the input table but with a new table alias.\nThis operator is useful for introducing a new alias for the input table, which can then be referred\nto in subsequent operators. Any existing alias for the table is replaced by the new alias.\nIt is useful to use this operator after adding new columns with\nSELECT\nor\nEXTEND\nor after\nperforming aggregation with\nAGGREGATE\n. This simplifies the process of referring to the columns\nfrom subsequent\nJOIN\noperators and allows for more readable queries.\nFor example:\nVALUES\n(\n0\n,\n1\n)\ntab\n(\ncol1\n,\ncol2\n)\n|>\nAS\nnew_tab\n|>\nSELECT\ncol1\n+\ncol2\nFROM\nnew_tab\n;\n+\n-----------+\n|\ncol1\n+\ncol2\n|\n+\n-----------+\n|\n1\n|\n+\n-----------+\nWHERE\n|>\nWHERE\n<\ncondition\n>\nReturns the subset of input rows passing the condition.\nSince this operator may appear anywhere, no separate\nHAVING\nor\nQUALIFY\nsyntax is needed.\nFor example:\nVALUES\n(\n0\n),\n(\n1\n)\ntab\n(\ncol\n)\n|>\nWHERE\ncol\n=\n1\n;\n+\n---+\n|\ncol\n|\n+\n---+\n|\n1\n|\n+\n---+\nLIMIT\n|>\n[\nLIMIT\n<\nn\n>\n]\n[\nOFFSET\n<\nm\n>\n]\nReturns the specified number of input rows, preserving ordering (if any).\nLIMIT\nand\nOFFSET\nare supported together. The\nLIMIT\nclause can also be used without the\nOFFSET\nclause, and the\nOFFSET\nclause can be used without the\nLIMIT\nclause.\nFor example:\nVALUES\n(\n0\n),\n(\n0\n)\ntab\n(\ncol\n)\n|>\nLIMIT\n1\n;\n+\n---+\n|\ncol\n|\n+\n---+\n|\n0\n|\n+\n---+\nAGGREGATE\n-- Full-table aggregation\n|>\nAGGREGATE\n<\nagg_expr\n>\n[[\nAS\n]\nalias\n],\n...\n-- Aggregation with grouping\n|>\nAGGREGATE\n[\n<\nagg_expr\n>\n[[\nAS\n]\nalias\n],\n...]\nGROUP\nBY\n<\ngrouping_expr\n>\n[\nAS\nalias\n],\n...\nPerforms aggregation across grouped rows or across the entire input table.\nIf no\nGROUP BY\nclause is present, this performs full-table aggregation, returning one result row\nwith a column for each aggregate expression. Othwrise, this performs aggregation with grouping,\nreturning one row per group. Aliases can be assigned directly on grouping expressions.\nThe output column list of this operator includes the grouping columns first (if any), and then the\naggregate columns afterward.\nEach\n<agg_expr>\nexpression can include standard aggregate function(s) like\nCOUNT\n,\nSUM\n,\nAVG\n,\nMIN\n, or any other aggregate function(s) that Spark SQL supports. Additional expressions may appear\nbelow or above the aggregate function(s), such as\nMIN(FLOOR(col)) + 1\n. Each\n<agg_expr>\nexpression must contain at least one aggregate function (or otherwise the query returns an error).\nEach\n<agg_expr>\nexpression may include a column alias with\nAS <alias>\n, and may also\ninclude a\nDISTINCT\nkeyword to remove duplicate values before applying the aggregate function (for\nexample,\nCOUNT(DISTINCT col)\n).\nIf present, the\nGROUP BY\nclause can include any number of grouping expressions, and each\n<agg_expr>\nexpression will evaluate over each unique combination of values of the grouping\nexpressions. The output table contains the evaluated grouping expressions followed by the evaluated\naggregate functions. The\nGROUP BY\nexpressions may include one-based ordinals. Unlike regular SQL\nin which such ordinals refer to the expressions in the accompanying\nSELECT\nclause, in SQL pipe\nsyntax, they refer to the columns of the relation produced by the preceding operator instead. For\nexample, in\nTABLE t |> AGGREGATE COUNT(*) GROUP BY 2\n, we refer to the second column of the input\ntable\nt\n.\nThere is no need to repeat entire expressions between\nGROUP BY\nand\nSELECT\n, since the\nAGGREGATE\noperator automatically includes the evaluated grouping expressions in its output. By the same token,\nafter an\nAGGREGATE\noperator, it is often unnecessary to issue a following\nSELECT\noperator, since\nAGGREGATE\nreturns both the grouping columns and the aggregate columns in a single step.\nFor example:\n-- Full-table aggregation\nVALUES\n(\n0\n),\n(\n1\n)\ntab\n(\ncol\n)\n|>\nAGGREGATE\nCOUNT\n(\ncol\n)\nAS\ncount\n;\n+\n-----+\n|\ncount\n|\n+\n-----+\n|\n2\n|\n+\n-----+\n-- Aggregation with grouping\nVALUES\n(\n0\n,\n1\n),\n(\n0\n,\n2\n)\ntab\n(\ncol1\n,\ncol2\n)\n|>\nAGGREGATE\nCOUNT\n(\ncol2\n)\nAS\ncount\nGROUP\nBY\ncol1\n;\n+\n----+-----+\n|\ncol1\n|\ncount\n|\n+\n----+-----+\n|\n0\n|\n2\n|\n+\n----+-----+\nJOIN\n|>\n[\nLEFT\n|\nRIGHT\n|\nFULL\n|\nCROSS\n|\nSEMI\n|\nANTI\n|\nNATURAL\n|\nLATERAL\n]\nJOIN\n<\ntable\n>\n[\nON\n<\ncondition\n>\n|\nUSING\n(\ncol\n,\n...)]\nJoins rows from both inputs, returning a filtered cross-product of the pipe input table and the\ntable expression following the JOIN keyword. This behaves a similar manner as the\nJOIN\nclause in\nregular SQL where the pipe operator input table becomes the left side of the join and the table\nargument becomes the right side of the join.\nStandard join modifiers like\nLEFT\n,\nRIGHT\n, and\nFULL\nare supported before the\nJOIN\nkeyword.\nThe join predicate may need to refer to columns from both inputs to the join. In this case, it may\nbe necessary to use table aliases to differentiate between columns in the event that both inputs\nhave columns with the same names. The\nAS\noperator can be useful here to introduce a new alias for\nthe pipe input table that becomes the left side of the join. Use standard syntax to assign an alias\nto the table argument that becomes the right side of the join, if needed.\nFor example:\nSELECT\n0\nAS\na\n,\n1\nAS\nb\n|>\nAS\nlhs\n|>\nJOIN\nVALUES\n(\n0\n,\n2\n)\nrhs\n(\na\n,\nb\n)\nON\n(\nlhs\n.\na\n=\nrhs\n.\na\n);\n+\n---+---+---+---+\n|\na\n|\nb\n|\nc\n|\nd\n|\n+\n---+---+---+---+\n|\n0\n|\n1\n|\n0\n|\n2\n|\n+\n---+---+---+---+\nVALUES\n(\n'apples'\n,\n3\n),\n(\n'bananas'\n,\n4\n)\nt\n(\nitem\n,\nsales\n)\n|>\nAS\nproduce_sales\n|>\nLEFT\nJOIN\n(\nSELECT\n\"apples\"\nAS\nitem\n,\n123\nAS\nid\n)\nAS\nproduce_data\nUSING\n(\nitem\n)\n|>\nSELECT\nproduce_sales\n.\nitem\n,\nsales\n,\nid\n;\n/*---------+-------+------+\n | item    | sales | id   |\n +---------+-------+------+\n | apples  | 3     | 123  |\n | bananas | 4     | NULL |\n +---------+-------+------*/\nORDER BY\n|>\nORDER\nBY\n<\nexpr\n>\n[\nASC\n|\nDESC\n],\n...\nReturns the input rows after sorting as indicated. Standard modifiers are supported including NULLS\nFIRST/LAST.\nFor example:\nVALUES\n(\n0\n),\n(\n1\n)\ntab\n(\ncol\n)\n|>\nORDER\nBY\ncol\nDESC\n;\n+\n---+\n|\ncol\n|\n+\n---+\n|\n1\n|\n|\n0\n|\n+\n---+\nUNION, INTERSECT, EXCEPT\n|>\n{\nUNION\n|\nINTERSECT\n|\nEXCEPT\n}\n{\nALL\n|\nDISTINCT\n}\n(\n<\nquery\n>\n)\nPerforms the union or other set operation over the combined rows from the input table or subquery.\nFor example:\nVALUES\n(\n0\n),\n(\n1\n)\ntab\n(\na\n,\nb\n)\n|>\nUNION\nALL\nVALUES\n(\n2\n),\n(\n3\n)\ntab\n(\nc\n,\nd\n);\n+\n---+----+\n|\na\n|\nb\n|\n+\n---+----+\n|\n0\n|\n1\n|\n|\n2\n|\n3\n|\n+\n---+----+\nTABLESAMPLE\n|>\nTABLESAMPLE\n<\nmethod\n>\n(\n<\nsize\n>\n{\nROWS\n|\nPERCENT\n})\nReturns the subset of rows chosen by the provided sampling algorithm.\nFor example:\nVALUES\n(\n0\n),\n(\n0\n),\n(\n0\n),\n(\n0\n)\ntab\n(\ncol\n)\n|>\nTABLESAMPLE\n(\n1\nROWS\n);\n+\n---+\n|\ncol\n|\n+\n---+\n|\n0\n|\n+\n---+\nVALUES\n(\n0\n),\n(\n0\n)\ntab\n(\ncol\n)\n|>\nTABLESAMPLE\n(\n100\nPERCENT\n);\n+\n---+\n|\ncol\n|\n+\n---+\n|\n0\n|\n|\n0\n|\n+\n---+\nPIVOT\n|>\nPIVOT\n(\nagg_expr\nFOR\ncol\nIN\n(\nval1\n,\n...))\nReturns a new table with the input rows pivoted to become columns.\nFor example:\nVALUES\n(\n\"dotNET\"\n,\n2012\n,\n10000\n),\n(\n\"Java\"\n,\n2012\n,\n20000\n),\n(\n\"dotNET\"\n,\n2012\n,\n5000\n),\n(\n\"dotNET\"\n,\n2013\n,\n48000\n),\n(\n\"Java\"\n,\n2013\n,\n30000\n)\ncourseSales\n(\ncourse\n,\nyear\n,\nearnings\n)\n|>\nPIVOT\n(\nSUM\n(\nearnings\n)\nFOR\nCOURSE\nIN\n(\n'dotNET'\n,\n'Java'\n)\n)\n+\n----+------+------+\n|\nyear\n|\ndotNET\n|\nJava\n|\n+\n----+------+------+\n|\n2012\n|\n15000\n|\n20000\n|\n|\n2013\n|\n48000\n|\n30000\n|\n+\n----+------+------+\nUNPIVOT\n|>\nUNPIVOT\n(\nvalue_col\nFOR\nkey_col\nIN\n(\ncol1\n,\n...))\nReturns a new table with the input columns pivoted to become rows.\nFor example:\nVALUES\n(\n\"dotNET\"\n,\n2012\n,\n10000\n),\n(\n\"Java\"\n,\n2012\n,\n20000\n),\n(\n\"dotNET\"\n,\n2012\n,\n5000\n),\n(\n\"dotNET\"\n,\n2013\n,\n48000\n),\n(\n\"Java\"\n,\n2013\n,\n30000\n)\ncourseSales\n(\ncourse\n,\nyear\n,\nearnings\n)\n|>\nUNPIVOT\n(\nearningsYear\nFOR\n`year`\nIN\n(\n`2012`\n,\n`2013`\n,\n`2014`\n)\n+\n--------+------+--------+\n|\ncourse\n|\nyear\n|\nearnings\n|\n+\n--------+------+--------+\n|\nJava\n|\n2012\n|\n20000\n|\n|\nJava\n|\n2013\n|\n30000\n|\n|\ndotNET\n|\n2012\n|\n15000\n|\n|\ndotNET\n|\n2013\n|\n48000\n|\n|\ndotNET\n|\n2014\n|\n22500\n|\n+\n--------+------+--------+"}
{"url": "https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTroubleshooting\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nHive Tables\nSpecifying storage format for Hive tables\nInteracting with Different Versions of Hive Metastore\nSpark SQL also supports reading and writing data stored in\nApache Hive\n.\nHowever, since Hive has a large number of dependencies, these dependencies are not included in the\ndefault Spark distribution. If Hive dependencies can be found on the classpath, Spark will load them\nautomatically. Note that these Hive dependencies must also be present on all of the worker nodes, as\nthey will need access to the Hive serialization and deserialization libraries (SerDes) in order to\naccess data stored in Hive.\nConfiguration of Hive is done by placing your\nhive-site.xml\n,\ncore-site.xml\n(for security configuration),\nand\nhdfs-site.xml\n(for HDFS configuration) file in\nconf/\n.\nWhen working with Hive, one must instantiate\nSparkSession\nwith Hive support, including\nconnectivity to a persistent Hive metastore, support for Hive serdes, and Hive user-defined functions.\nUsers who do not have an existing Hive deployment can still enable Hive support. When not configured\nby the\nhive-site.xml\n, the context automatically creates\nmetastore_db\nin the current directory and\ncreates a directory configured by\nspark.sql.warehouse.dir\n, which defaults to the directory\nspark-warehouse\nin the current directory that the Spark application is started. Note that\nthe\nhive.metastore.warehouse.dir\nproperty in\nhive-site.xml\nis deprecated since Spark 2.0.0.\nInstead, use\nspark.sql.warehouse.dir\nto specify the default location of database in warehouse.\nYou may need to grant write privilege to the user who starts the Spark application.\nfrom\nos.path\nimport\nabspath\nfrom\npyspark.sql\nimport\nSparkSession\nfrom\npyspark.sql\nimport\nRow\n# warehouse_location points to the default location for managed databases and tables\nwarehouse_location\n=\nabspath\n(\n'\nspark-warehouse\n'\n)\nspark\n=\nSparkSession\n\\\n.\nbuilder\n\\\n.\nappName\n(\n\"\nPython Spark SQL Hive integration example\n\"\n)\n\\\n.\nconfig\n(\n\"\nspark.sql.warehouse.dir\n\"\n,\nwarehouse_location\n)\n\\\n.\nenableHiveSupport\n()\n\\\n.\ngetOrCreate\n()\n# spark is an existing SparkSession\nspark\n.\nsql\n(\n\"\nCREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\n\"\n)\nspark\n.\nsql\n(\n\"\nLOAD DATA LOCAL INPATH\n'\nexamples/src/main/resources/kv1.txt\n'\nINTO TABLE src\n\"\n)\n# Queries are expressed in HiveQL\nspark\n.\nsql\n(\n\"\nSELECT * FROM src\n\"\n).\nshow\n()\n# +---+-------+\n# |key|  value|\n# +---+-------+\n# |238|val_238|\n# | 86| val_86|\n# |311|val_311|\n# ...\n# Aggregation queries are also supported.\nspark\n.\nsql\n(\n\"\nSELECT COUNT(*) FROM src\n\"\n).\nshow\n()\n# +--------+\n# |count(1)|\n# +--------+\n# |    500 |\n# +--------+\n# The results of SQL queries are themselves DataFrames and support all normal functions.\nsqlDF\n=\nspark\n.\nsql\n(\n\"\nSELECT key, value FROM src WHERE key < 10 ORDER BY key\n\"\n)\n# The items in DataFrames are of type Row, which allows you to access each column by ordinal.\nstringsDS\n=\nsqlDF\n.\nrdd\n.\nmap\n(\nlambda\nrow\n:\n\"\nKey: %d, Value: %s\n\"\n%\n(\nrow\n.\nkey\n,\nrow\n.\nvalue\n))\nfor\nrecord\nin\nstringsDS\n.\ncollect\n():\nprint\n(\nrecord\n)\n# Key: 0, Value: val_0\n# Key: 0, Value: val_0\n# Key: 0, Value: val_0\n# ...\n# You can also use DataFrames to create temporary views within a SparkSession.\nRecord\n=\nRow\n(\n\"\nkey\n\"\n,\n\"\nvalue\n\"\n)\nrecordsDF\n=\nspark\n.\ncreateDataFrame\n([\nRecord\n(\ni\n,\n\"\nval_\n\"\n+\nstr\n(\ni\n))\nfor\ni\nin\nrange\n(\n1\n,\n101\n)])\nrecordsDF\n.\ncreateOrReplaceTempView\n(\n\"\nrecords\n\"\n)\n# Queries can then join DataFrame data with data stored in Hive.\nspark\n.\nsql\n(\n\"\nSELECT * FROM records r JOIN src s ON r.key = s.key\n\"\n).\nshow\n()\n# +---+------+---+------+\n# |key| value|key| value|\n# +---+------+---+------+\n# |  2| val_2|  2| val_2|\n# |  4| val_4|  4| val_4|\n# |  5| val_5|  5| val_5|\n# ...\nFind full example code at \"examples/src/main/python/sql/hive.py\" in the Spark repo.\nimport\njava.io.File\nimport\norg.apache.spark.sql.\n{\nRow\n,\nSaveMode\n,\nSparkSession\n}\ncase\nclass\nRecord\n(\nkey\n:\nInt\n,\nvalue\n:\nString\n)\n// warehouseLocation points to the default location for managed databases and tables\nval\nwarehouseLocation\n=\nnew\nFile\n(\n\"spark-warehouse\"\n).\ngetAbsolutePath\nval\nspark\n=\nSparkSession\n.\nbuilder\n()\n.\nappName\n(\n\"Spark Hive Example\"\n)\n.\nconfig\n(\n\"spark.sql.warehouse.dir\"\n,\nwarehouseLocation\n)\n.\nenableHiveSupport\n()\n.\ngetOrCreate\n()\nimport\nspark.implicits._\nimport\nspark.sql\nsql\n(\n\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\"\n)\nsql\n(\n\"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src\"\n)\n// Queries are expressed in HiveQL\nsql\n(\n\"SELECT * FROM src\"\n).\nshow\n()\n// +---+-------+\n// |key|  value|\n// +---+-------+\n// |238|val_238|\n// | 86| val_86|\n// |311|val_311|\n// ...\n// Aggregation queries are also supported.\nsql\n(\n\"SELECT COUNT(*) FROM src\"\n).\nshow\n()\n// +--------+\n// |count(1)|\n// +--------+\n// |    500 |\n// +--------+\n// The results of SQL queries are themselves DataFrames and support all normal functions.\nval\nsqlDF\n=\nsql\n(\n\"SELECT key, value FROM src WHERE key < 10 ORDER BY key\"\n)\n// The items in DataFrames are of type Row, which allows you to access each column by ordinal.\nval\nstringsDS\n=\nsqlDF\n.\nmap\n{\ncase\nRow\n(\nkey\n:\nInt\n,\nvalue\n:\nString\n)\n=>\ns\n\"Key: $key, Value: $value\"\n}\nstringsDS\n.\nshow\n()\n// +--------------------+\n// |               value|\n// +--------------------+\n// |Key: 0, Value: val_0|\n// |Key: 0, Value: val_0|\n// |Key: 0, Value: val_0|\n// ...\n// You can also use DataFrames to create temporary views within a SparkSession.\nval\nrecordsDF\n=\nspark\n.\ncreateDataFrame\n((\n1\nto\n100\n).\nmap\n(\ni\n=>\nRecord\n(\ni\n,\ns\n\"val_$i\"\n)))\nrecordsDF\n.\ncreateOrReplaceTempView\n(\n\"records\"\n)\n// Queries can then join DataFrame data with data stored in Hive.\nsql\n(\n\"SELECT * FROM records r JOIN src s ON r.key = s.key\"\n).\nshow\n()\n// +---+------+---+------+\n// |key| value|key| value|\n// +---+------+---+------+\n// |  2| val_2|  2| val_2|\n// |  4| val_4|  4| val_4|\n// |  5| val_5|  5| val_5|\n// ...\n// Create a Hive managed Parquet table, with HQL syntax instead of the Spark SQL native syntax\n// `USING hive`\nsql\n(\n\"CREATE TABLE hive_records(key int, value string) STORED AS PARQUET\"\n)\n// Save DataFrame to the Hive managed table\nval\ndf\n=\nspark\n.\ntable\n(\n\"src\"\n)\ndf\n.\nwrite\n.\nmode\n(\nSaveMode\n.\nOverwrite\n).\nsaveAsTable\n(\n\"hive_records\"\n)\n// After insertion, the Hive managed table has data now\nsql\n(\n\"SELECT * FROM hive_records\"\n).\nshow\n()\n// +---+-------+\n// |key|  value|\n// +---+-------+\n// |238|val_238|\n// | 86| val_86|\n// |311|val_311|\n// ...\n// Prepare a Parquet data directory\nval\ndataDir\n=\n\"/tmp/parquet_data\"\nspark\n.\nrange\n(\n10\n).\nwrite\n.\nparquet\n(\ndataDir\n)\n// Create a Hive external Parquet table\nsql\n(\ns\n\"CREATE EXTERNAL TABLE hive_bigints(id bigint) STORED AS PARQUET LOCATION '$dataDir'\"\n)\n// The Hive external table should already have data\nsql\n(\n\"SELECT * FROM hive_bigints\"\n).\nshow\n()\n// +---+\n// | id|\n// +---+\n// |  0|\n// |  1|\n// |  2|\n// ... Order may vary, as spark processes the partitions in parallel.\n// Turn on flag for Hive Dynamic Partitioning\nspark\n.\nconf\n.\nset\n(\n\"hive.exec.dynamic.partition\"\n,\n\"true\"\n)\nspark\n.\nconf\n.\nset\n(\n\"hive.exec.dynamic.partition.mode\"\n,\n\"nonstrict\"\n)\n// Create a Hive partitioned table using DataFrame API\ndf\n.\nwrite\n.\npartitionBy\n(\n\"key\"\n).\nformat\n(\n\"hive\"\n).\nsaveAsTable\n(\n\"hive_part_tbl\"\n)\n// Partitioned column `key` will be moved to the end of the schema.\nsql\n(\n\"SELECT * FROM hive_part_tbl\"\n).\nshow\n()\n// +-------+---+\n// |  value|key|\n// +-------+---+\n// |val_238|238|\n// | val_86| 86|\n// |val_311|311|\n// ...\nspark\n.\nstop\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\" in the Spark repo.\nimport\njava.io.File\n;\nimport\njava.io.Serializable\n;\nimport\njava.util.ArrayList\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.api.java.function.MapFunction\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Encoders\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\npublic\nstatic\nclass\nRecord\nimplements\nSerializable\n{\nprivate\nint\nkey\n;\nprivate\nString\nvalue\n;\npublic\nint\ngetKey\n()\n{\nreturn\nkey\n;\n}\npublic\nvoid\nsetKey\n(\nint\nkey\n)\n{\nthis\n.\nkey\n=\nkey\n;\n}\npublic\nString\ngetValue\n()\n{\nreturn\nvalue\n;\n}\npublic\nvoid\nsetValue\n(\nString\nvalue\n)\n{\nthis\n.\nvalue\n=\nvalue\n;\n}\n}\n// warehouseLocation points to the default location for managed databases and tables\nString\nwarehouseLocation\n=\nnew\nFile\n(\n\"spark-warehouse\"\n).\ngetAbsolutePath\n();\nSparkSession\nspark\n=\nSparkSession\n.\nbuilder\n()\n.\nappName\n(\n\"Java Spark Hive Example\"\n)\n.\nconfig\n(\n\"spark.sql.warehouse.dir\"\n,\nwarehouseLocation\n)\n.\nenableHiveSupport\n()\n.\ngetOrCreate\n();\nspark\n.\nsql\n(\n\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\"\n);\nspark\n.\nsql\n(\n\"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src\"\n);\n// Queries are expressed in HiveQL\nspark\n.\nsql\n(\n\"SELECT * FROM src\"\n).\nshow\n();\n// +---+-------+\n// |key|  value|\n// +---+-------+\n// |238|val_238|\n// | 86| val_86|\n// |311|val_311|\n// ...\n// Aggregation queries are also supported.\nspark\n.\nsql\n(\n\"SELECT COUNT(*) FROM src\"\n).\nshow\n();\n// +--------+\n// |count(1)|\n// +--------+\n// |    500 |\n// +--------+\n// The results of SQL queries are themselves DataFrames and support all normal functions.\nDataset\n<\nRow\n>\nsqlDF\n=\nspark\n.\nsql\n(\n\"SELECT key, value FROM src WHERE key < 10 ORDER BY key\"\n);\n// The items in DataFrames are of type Row, which lets you to access each column by ordinal.\nDataset\n<\nString\n>\nstringsDS\n=\nsqlDF\n.\nmap\n(\n(\nMapFunction\n<\nRow\n,\nString\n>)\nrow\n->\n\"Key: \"\n+\nrow\n.\nget\n(\n0\n)\n+\n\", Value: \"\n+\nrow\n.\nget\n(\n1\n),\nEncoders\n.\nSTRING\n());\nstringsDS\n.\nshow\n();\n// +--------------------+\n// |               value|\n// +--------------------+\n// |Key: 0, Value: val_0|\n// |Key: 0, Value: val_0|\n// |Key: 0, Value: val_0|\n// ...\n// You can also use DataFrames to create temporary views within a SparkSession.\nList\n<\nRecord\n>\nrecords\n=\nnew\nArrayList\n<>();\nfor\n(\nint\nkey\n=\n1\n;\nkey\n<\n100\n;\nkey\n++)\n{\nRecord\nrecord\n=\nnew\nRecord\n();\nrecord\n.\nsetKey\n(\nkey\n);\nrecord\n.\nsetValue\n(\n\"val_\"\n+\nkey\n);\nrecords\n.\nadd\n(\nrecord\n);\n}\nDataset\n<\nRow\n>\nrecordsDF\n=\nspark\n.\ncreateDataFrame\n(\nrecords\n,\nRecord\n.\nclass\n);\nrecordsDF\n.\ncreateOrReplaceTempView\n(\n\"records\"\n);\n// Queries can then join DataFrames data with data stored in Hive.\nspark\n.\nsql\n(\n\"SELECT * FROM records r JOIN src s ON r.key = s.key\"\n).\nshow\n();\n// +---+------+---+------+\n// |key| value|key| value|\n// +---+------+---+------+\n// |  2| val_2|  2| val_2|\n// |  2| val_2|  2| val_2|\n// |  4| val_4|  4| val_4|\n// ...\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\" in the Spark repo.\nWhen working with Hive one must instantiate\nSparkSession\nwith Hive support. This\nadds support for finding tables in the MetaStore and writing queries using HiveQL.\n# enableHiveSupport defaults to TRUE\nsparkR.session\n(\nenableHiveSupport\n=\nTRUE\n)\nsql\n(\n\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\"\n)\nsql\n(\n\"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src\"\n)\n# Queries can be expressed in HiveQL.\nresults\n<-\ncollect\n(\nsql\n(\n\"FROM src SELECT key, value\"\n))\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nSpecifying storage format for Hive tables\nWhen you create a Hive table, you need to define how this table should read/write data from/to file system,\ni.e. the “input format” and “output format”. You also need to define how this table should deserialize the data\nto rows, or serialize rows to data, i.e. the “serde”. The following options can be used to specify the storage\nformat(“serde”, “input format”, “output format”), e.g.\nCREATE TABLE src(id int) USING hive OPTIONS(fileFormat 'parquet')\n.\nBy default, we will read the table files as plain text. Note that, Hive storage handler is not supported yet when\ncreating table, you can create a table using storage handler at Hive side, and use Spark SQL to read it.\nProperty Name\nMeaning\nfileFormat\nA fileFormat is kind of a package of storage format specifications, including \"serde\", \"input format\" and\n      \"output format\". Currently we support 6 fileFormats: 'sequencefile', 'rcfile', 'orc', 'parquet', 'textfile' and 'avro'.\ninputFormat, outputFormat\nThese 2 options specify the name of a corresponding\nInputFormat\nand\nOutputFormat\nclass as a string literal,\n      e.g.\norg.apache.hadoop.hive.ql.io.orc.OrcInputFormat\n. These 2 options must be appeared in a pair, and you can not\n      specify them if you already specified the\nfileFormat\noption.\nserde\nThis option specifies the name of a serde class. When the\nfileFormat\noption is specified, do not specify this option\n      if the given\nfileFormat\nalready include the information of serde. Currently \"sequencefile\", \"textfile\" and \"rcfile\"\n      don't include the serde information and you can use this option with these 3 fileFormats.\nfieldDelim, escapeDelim, collectionDelim, mapkeyDelim, lineDelim\nThese options can only be used with \"textfile\" fileFormat. They define how to read delimited files into rows.\nAll other properties defined with\nOPTIONS\nwill be regarded as Hive serde properties.\nInteracting with Different Versions of Hive Metastore\nOne of the most important pieces of Spark SQL’s Hive support is interaction with Hive metastore,\nwhich enables Spark SQL to access metadata of Hive tables. Starting from Spark 1.4.0, a single binary\nbuild of Spark SQL can be used to query different versions of Hive metastores, using the configuration described below.\nNote that independent of the version of Hive that is being used to talk to the metastore, internally Spark SQL\nwill compile against built-in Hive and use those classes for internal execution (serdes, UDFs, UDAFs, etc).\nThe following options can be used to configure the version of Hive that is used to retrieve metadata:\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.hive.metastore.version\n2.3.10\nVersion of the Hive metastore. Available\n      options are\n2.0.0\nthrough\n2.3.10\n,\n3.0.0\nthrough\n3.1.3\n, and\n4.0.0\nthrough\n4.0.1\n.\n1.4.0\nspark.sql.hive.metastore.jars\nbuiltin\nLocation of the jars that should be used to instantiate the HiveMetastoreClient. This\n      property can be one of four options:\nbuiltin\nUse Hive 2.3.10, which is bundled with the Spark assembly when\n-Phive\nis\n        enabled. When this option is chosen,\nspark.sql.hive.metastore.version\nmust be\n        either\n2.3.10\nor not defined.\nmaven\nUse Hive jars of specified version downloaded from Maven repositories. This configuration\n        is not generally recommended for production deployments.\npath\nUse Hive jars configured by\nspark.sql.hive.metastore.jars.path\nin comma separated format. Support both local or remote paths. The provided jars should be\n        the same version as\nspark.sql.hive.metastore.version\n.\nA classpath in the standard format for the JVM. This classpath must include all of Hive\n        and its dependencies, including the correct version of Hadoop. The provided jars should be\n        the same version as\nspark.sql.hive.metastore.version\n. These jars only need to be present on the\n        driver, but if you are running in yarn cluster mode then you must ensure they are packaged\n        with your application.\n1.4.0\nspark.sql.hive.metastore.jars.path\n(empty)\nComma-separated paths of the jars that used to instantiate the HiveMetastoreClient.\n      This configuration is useful only when\nspark.sql.hive.metastore.jars\nis set as\npath\n.\nThe paths can be any of the following format:\nfile://path/to/jar/foo.jar\nhdfs://nameservice/path/to/jar/foo.jar\n/path/to/jar/\n(path without URI scheme follow conf\nfs.defaultFS\n's URI schema)\n[http/https/ftp]://path/to/jar/foo.jar\nNote that 1, 2, and 3 support wildcard. For example:\nfile://path/to/jar/*,file://path2/to/jar/*/*.jar\nhdfs://nameservice/path/to/jar/*,hdfs://nameservice2/path/to/jar/*/*.jar\n3.1.0\nspark.sql.hive.metastore.sharedPrefixes\ncom.mysql.jdbc,\norg.postgresql,\ncom.microsoft.sqlserver,\noracle.jdbc\nA comma-separated list of class prefixes that should be loaded using the classloader that is\n        shared between Spark SQL and a specific version of Hive. An example of classes that should\n        be shared is JDBC drivers that are needed to talk to the metastore. Other classes that need\n        to be shared are those that interact with classes that are already shared. For example,\n        custom appenders that are used by log4j.\n1.4.0\nspark.sql.hive.metastore.barrierPrefixes\n(empty)\nA comma separated list of class prefixes that should explicitly be reloaded for each version\n        of Hive that Spark SQL is communicating with. For example, Hive UDFs that are declared in a\n        prefix that typically would be shared (i.e.\norg.apache.spark.*\n).\n1.4.0"}
{"url": "https://spark.apache.org/docs/latest/sql-data-sources-generic-options.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTroubleshooting\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nGeneric File Source Options\nIgnore Corrupt Files\nIgnore Missing Files\nPath Glob Filter\nRecursive File Lookup\nModification Time Path Filters\nThese generic options/configurations are effective only when using file-based sources: parquet, orc, avro, json, csv, text.\nPlease note that the hierarchy of directories used in examples below are:\ndir1/\n ├── dir2/\n │    └── file2.parquet (schema: <file: string>, content: \"file2.parquet\")\n └── file1.parquet (schema: <file, string>, content: \"file1.parquet\")\n └── file3.json (schema: <file, string>, content: \"{'file':'corrupt.json'}\")\nIgnore Corrupt Files\nSpark allows you to use the configuration\nspark.sql.files.ignoreCorruptFiles\nor the data source option\nignoreCorruptFiles\nto ignore corrupt files while reading data\nfrom files. When set to true, the Spark jobs will continue to run when encountering corrupted files and\nthe contents that have been read will still be returned.\nTo ignore corrupt files while reading data files, you can use:\n# enable ignore corrupt files via the data source option\n# dir1/file3.json is corrupt from parquet's view\ntest_corrupt_df0\n=\nspark\n.\nread\n.\noption\n(\n\"\nignoreCorruptFiles\n\"\n,\n\"\ntrue\n\"\n)\n\\\n.\nparquet\n(\n\"\nexamples/src/main/resources/dir1/\n\"\n,\n\"\nexamples/src/main/resources/dir1/dir2/\n\"\n)\ntest_corrupt_df0\n.\nshow\n()\n# +-------------+\n# |         file|\n# +-------------+\n# |file1.parquet|\n# |file2.parquet|\n# +-------------+\n# enable ignore corrupt files via the configuration\nspark\n.\nsql\n(\n\"\nset spark.sql.files.ignoreCorruptFiles=true\n\"\n)\n# dir1/file3.json is corrupt from parquet's view\ntest_corrupt_df1\n=\nspark\n.\nread\n.\nparquet\n(\n\"\nexamples/src/main/resources/dir1/\n\"\n,\n\"\nexamples/src/main/resources/dir1/dir2/\n\"\n)\ntest_corrupt_df1\n.\nshow\n()\n# +-------------+\n# |         file|\n# +-------------+\n# |file1.parquet|\n# |file2.parquet|\n# +-------------+\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\n// enable ignore corrupt files via the data source option\n// dir1/file3.json is corrupt from parquet's view\nval\ntestCorruptDF0\n=\nspark\n.\nread\n.\noption\n(\n\"ignoreCorruptFiles\"\n,\n\"true\"\n).\nparquet\n(\n\"examples/src/main/resources/dir1/\"\n,\n\"examples/src/main/resources/dir1/dir2/\"\n)\ntestCorruptDF0\n.\nshow\n()\n// +-------------+\n// |         file|\n// +-------------+\n// |file1.parquet|\n// |file2.parquet|\n// +-------------+\n// enable ignore corrupt files via the configuration\nspark\n.\nsql\n(\n\"set spark.sql.files.ignoreCorruptFiles=true\"\n)\n// dir1/file3.json is corrupt from parquet's view\nval\ntestCorruptDF1\n=\nspark\n.\nread\n.\nparquet\n(\n\"examples/src/main/resources/dir1/\"\n,\n\"examples/src/main/resources/dir1/dir2/\"\n)\ntestCorruptDF1\n.\nshow\n()\n// +-------------+\n// |         file|\n// +-------------+\n// |file1.parquet|\n// |file2.parquet|\n// +-------------+\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\n// enable ignore corrupt files via the data source option\n// dir1/file3.json is corrupt from parquet's view\nDataset\n<\nRow\n>\ntestCorruptDF0\n=\nspark\n.\nread\n().\noption\n(\n\"ignoreCorruptFiles\"\n,\n\"true\"\n).\nparquet\n(\n\"examples/src/main/resources/dir1/\"\n,\n\"examples/src/main/resources/dir1/dir2/\"\n);\ntestCorruptDF0\n.\nshow\n();\n// +-------------+\n// |         file|\n// +-------------+\n// |file1.parquet|\n// |file2.parquet|\n// +-------------+\n// enable ignore corrupt files via the configuration\nspark\n.\nsql\n(\n\"set spark.sql.files.ignoreCorruptFiles=true\"\n);\n// dir1/file3.json is corrupt from parquet's view\nDataset\n<\nRow\n>\ntestCorruptDF1\n=\nspark\n.\nread\n().\nparquet\n(\n\"examples/src/main/resources/dir1/\"\n,\n\"examples/src/main/resources/dir1/dir2/\"\n);\ntestCorruptDF1\n.\nshow\n();\n// +-------------+\n// |         file|\n// +-------------+\n// |file1.parquet|\n// |file2.parquet|\n// +-------------+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\n# enable ignore corrupt files via the data source option\n# dir1/file3.json is corrupt from parquet's view\ntestCorruptDF0\n<-\nread.parquet\n(\nc\n(\n\"examples/src/main/resources/dir1/\"\n,\n\"examples/src/main/resources/dir1/dir2/\"\n),\nignoreCorruptFiles\n=\n\"true\"\n)\nhead\n(\ntestCorruptDF0\n)\n#            file\n# 1 file1.parquet\n# 2 file2.parquet\n# enable ignore corrupt files via the configuration\nsql\n(\n\"set spark.sql.files.ignoreCorruptFiles=true\"\n)\n# dir1/file3.json is corrupt from parquet's view\ntestCorruptDF1\n<-\nread.parquet\n(\nc\n(\n\"examples/src/main/resources/dir1/\"\n,\n\"examples/src/main/resources/dir1/dir2/\"\n))\nhead\n(\ntestCorruptDF1\n)\n#            file\n# 1 file1.parquet\n# 2 file2.parquet\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nIgnore Missing Files\nSpark allows you to use the configuration\nspark.sql.files.ignoreMissingFiles\nor the data source option\nignoreMissingFiles\nto ignore missing files while reading data\nfrom files. Here, missing file really means the deleted file under directory after you construct the\nDataFrame\n. When set to true, the Spark jobs will continue to run when encountering missing files and\nthe contents that have been read will still be returned.\nPath Glob Filter\npathGlobFilter\nis used to only include files with file names matching the pattern. The syntax follows\norg.apache.hadoop.fs.GlobFilter\n. It does not change the behavior of partition discovery.\nTo load files with paths matching a given glob pattern while keeping the behavior of partition discovery,\nyou can use:\ndf\n=\nspark\n.\nread\n.\nload\n(\n\"\nexamples/src/main/resources/dir1\n\"\n,\nformat\n=\n\"\nparquet\n\"\n,\npathGlobFilter\n=\n\"\n*.parquet\n\"\n)\ndf\n.\nshow\n()\n# +-------------+\n# |         file|\n# +-------------+\n# |file1.parquet|\n# +-------------+\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\nval\ntestGlobFilterDF\n=\nspark\n.\nread\n.\nformat\n(\n\"parquet\"\n)\n.\noption\n(\n\"pathGlobFilter\"\n,\n\"*.parquet\"\n)\n// json file should be filtered out\n.\nload\n(\n\"examples/src/main/resources/dir1\"\n)\ntestGlobFilterDF\n.\nshow\n()\n// +-------------+\n// |         file|\n// +-------------+\n// |file1.parquet|\n// +-------------+\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nDataset\n<\nRow\n>\ntestGlobFilterDF\n=\nspark\n.\nread\n().\nformat\n(\n\"parquet\"\n)\n.\noption\n(\n\"pathGlobFilter\"\n,\n\"*.parquet\"\n)\n// json file should be filtered out\n.\nload\n(\n\"examples/src/main/resources/dir1\"\n);\ntestGlobFilterDF\n.\nshow\n();\n// +-------------+\n// |         file|\n// +-------------+\n// |file1.parquet|\n// +-------------+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\ndf\n<-\nread.df\n(\n\"examples/src/main/resources/dir1\"\n,\n\"parquet\"\n,\npathGlobFilter\n=\n\"*.parquet\"\n)\n#            file\n# 1 file1.parquet\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nRecursive File Lookup\nrecursiveFileLookup\nis used to recursively load files and it disables partition inferring. Its default value is\nfalse\n.\nIf data source explicitly specifies the\npartitionSpec\nwhen\nrecursiveFileLookup\nis true, exception will be thrown.\nTo load all files recursively, you can use:\nrecursive_loaded_df\n=\nspark\n.\nread\n.\nformat\n(\n\"\nparquet\n\"\n)\n\\\n.\noption\n(\n\"\nrecursiveFileLookup\n\"\n,\n\"\ntrue\n\"\n)\n\\\n.\nload\n(\n\"\nexamples/src/main/resources/dir1\n\"\n)\nrecursive_loaded_df\n.\nshow\n()\n# +-------------+\n# |         file|\n# +-------------+\n# |file1.parquet|\n# |file2.parquet|\n# +-------------+\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\nval\nrecursiveLoadedDF\n=\nspark\n.\nread\n.\nformat\n(\n\"parquet\"\n)\n.\noption\n(\n\"recursiveFileLookup\"\n,\n\"true\"\n)\n.\nload\n(\n\"examples/src/main/resources/dir1\"\n)\nrecursiveLoadedDF\n.\nshow\n()\n// +-------------+\n// |         file|\n// +-------------+\n// |file1.parquet|\n// |file2.parquet|\n// +-------------+\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nDataset\n<\nRow\n>\nrecursiveLoadedDF\n=\nspark\n.\nread\n().\nformat\n(\n\"parquet\"\n)\n.\noption\n(\n\"recursiveFileLookup\"\n,\n\"true\"\n)\n.\nload\n(\n\"examples/src/main/resources/dir1\"\n);\nrecursiveLoadedDF\n.\nshow\n();\n// +-------------+\n// |         file|\n// +-------------+\n// |file1.parquet|\n// |file2.parquet|\n// +-------------+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\nrecursiveLoadedDF\n<-\nread.df\n(\n\"examples/src/main/resources/dir1\"\n,\n\"parquet\"\n,\nrecursiveFileLookup\n=\n\"true\"\n)\nhead\n(\nrecursiveLoadedDF\n)\n#            file\n# 1 file1.parquet\n# 2 file2.parquet\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nModification Time Path Filters\nmodifiedBefore\nand\nmodifiedAfter\nare options that can be \napplied together or separately in order to achieve greater\ngranularity over which files may load during a Spark batch query.\n(Note that Structured Streaming file sources don’t support these options.)\nmodifiedBefore\n: an optional timestamp to only include files with\nmodification times occurring before the specified time. The provided timestamp\nmust be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\nmodifiedAfter\n: an optional timestamp to only include files with\nmodification times occurring after the specified time. The provided timestamp\nmust be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\nWhen a timezone option is not provided, the timestamps will be interpreted according\nto the Spark session timezone (\nspark.sql.session.timeZone\n).\nTo load files with paths matching a given modified time range, you can use:\n# Only load files modified before 07/1/2050 @ 08:30:00\ndf\n=\nspark\n.\nread\n.\nload\n(\n\"\nexamples/src/main/resources/dir1\n\"\n,\nformat\n=\n\"\nparquet\n\"\n,\nmodifiedBefore\n=\n\"\n2050-07-01T08:30:00\n\"\n)\ndf\n.\nshow\n()\n# +-------------+\n# |         file|\n# +-------------+\n# |file1.parquet|\n# +-------------+\n# Only load files modified after 06/01/2050 @ 08:30:00\ndf\n=\nspark\n.\nread\n.\nload\n(\n\"\nexamples/src/main/resources/dir1\n\"\n,\nformat\n=\n\"\nparquet\n\"\n,\nmodifiedAfter\n=\n\"\n2050-06-01T08:30:00\n\"\n)\ndf\n.\nshow\n()\n# +-------------+\n# |         file|\n# +-------------+\n# +-------------+\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\nval\nbeforeFilterDF\n=\nspark\n.\nread\n.\nformat\n(\n\"parquet\"\n)\n// Files modified before 07/01/2020 at 05:30 are allowed\n.\noption\n(\n\"modifiedBefore\"\n,\n\"2020-07-01T05:30:00\"\n)\n.\nload\n(\n\"examples/src/main/resources/dir1\"\n);\nbeforeFilterDF\n.\nshow\n();\n// +-------------+\n// |         file|\n// +-------------+\n// |file1.parquet|\n// +-------------+\nval\nafterFilterDF\n=\nspark\n.\nread\n.\nformat\n(\n\"parquet\"\n)\n// Files modified after 06/01/2020 at 05:30 are allowed\n.\noption\n(\n\"modifiedAfter\"\n,\n\"2020-06-01T05:30:00\"\n)\n.\nload\n(\n\"examples/src/main/resources/dir1\"\n);\nafterFilterDF\n.\nshow\n();\n// +-------------+\n// |         file|\n// +-------------+\n// +-------------+\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nDataset\n<\nRow\n>\nbeforeFilterDF\n=\nspark\n.\nread\n().\nformat\n(\n\"parquet\"\n)\n// Only load files modified before 7/1/2020 at 05:30\n.\noption\n(\n\"modifiedBefore\"\n,\n\"2020-07-01T05:30:00\"\n)\n// Only load files modified after 6/1/2020 at 05:30\n.\noption\n(\n\"modifiedAfter\"\n,\n\"2020-06-01T05:30:00\"\n)\n// Interpret both times above relative to CST timezone\n.\noption\n(\n\"timeZone\"\n,\n\"CST\"\n)\n.\nload\n(\n\"examples/src/main/resources/dir1\"\n);\nbeforeFilterDF\n.\nshow\n();\n// +-------------+\n// |         file|\n// +-------------+\n// |file1.parquet|\n// +-------------+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\nbeforeDF\n<-\nread.df\n(\n\"examples/src/main/resources/dir1\"\n,\n\"parquet\"\n,\nmodifiedBefore\n=\n\"2020-07-01T05:30:00\"\n)\n#            file\n# 1 file1.parquet\nafterDF\n<-\nread.df\n(\n\"examples/src/main/resources/dir1\"\n,\n\"parquet\"\n,\nmodifiedAfter\n=\n\"2020-06-01T05:30:00\"\n)\n#            file\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo."}
{"url": "https://spark.apache.org/docs/latest/sql-data-sources-parquet.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTroubleshooting\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nParquet Files\nLoading Data Programmatically\nPartition Discovery\nSchema Merging\nHive metastore Parquet table conversion\nHive/Parquet Schema Reconciliation\nMetadata Refreshing\nColumnar Encryption\nKMS Client\nData Source Option\nConfiguration\nParquet\nis a columnar format that is supported by many other data processing systems.\nSpark SQL provides support for both reading and writing Parquet files that automatically preserves the schema\nof the original data. When reading Parquet files, all columns are automatically converted to be nullable for\ncompatibility reasons.\nLoading Data Programmatically\nUsing the data from the above example:\npeopleDF\n=\nspark\n.\nread\n.\njson\n(\n\"\nexamples/src/main/resources/people.json\n\"\n)\n# DataFrames can be saved as Parquet files, maintaining the schema information.\npeopleDF\n.\nwrite\n.\nparquet\n(\n\"\npeople.parquet\n\"\n)\n# Read in the Parquet file created above.\n# Parquet files are self-describing so the schema is preserved.\n# The result of loading a parquet file is also a DataFrame.\nparquetFile\n=\nspark\n.\nread\n.\nparquet\n(\n\"\npeople.parquet\n\"\n)\n# Parquet files can also be used to create a temporary view and then used in SQL statements.\nparquetFile\n.\ncreateOrReplaceTempView\n(\n\"\nparquetFile\n\"\n)\nteenagers\n=\nspark\n.\nsql\n(\n\"\nSELECT name FROM parquetFile WHERE age >= 13 AND age <= 19\n\"\n)\nteenagers\n.\nshow\n()\n# +------+\n# |  name|\n# +------+\n# |Justin|\n# +------+\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\n// Encoders for most common types are automatically provided by importing spark.implicits._\nimport\nspark.implicits._\nval\npeopleDF\n=\nspark\n.\nread\n.\njson\n(\n\"examples/src/main/resources/people.json\"\n)\n// DataFrames can be saved as Parquet files, maintaining the schema information\npeopleDF\n.\nwrite\n.\nparquet\n(\n\"people.parquet\"\n)\n// Read in the parquet file created above\n// Parquet files are self-describing so the schema is preserved\n// The result of loading a Parquet file is also a DataFrame\nval\nparquetFileDF\n=\nspark\n.\nread\n.\nparquet\n(\n\"people.parquet\"\n)\n// Parquet files can also be used to create a temporary view and then used in SQL statements\nparquetFileDF\n.\ncreateOrReplaceTempView\n(\n\"parquetFile\"\n)\nval\nnamesDF\n=\nspark\n.\nsql\n(\n\"SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19\"\n)\nnamesDF\n.\nmap\n(\nattributes\n=>\n\"Name: \"\n+\nattributes\n(\n0\n)).\nshow\n()\n// +------------+\n// |       value|\n// +------------+\n// |Name: Justin|\n// +------------+\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nimport\norg.apache.spark.api.java.function.MapFunction\n;\nimport\norg.apache.spark.sql.Encoders\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nDataset\n<\nRow\n>\npeopleDF\n=\nspark\n.\nread\n().\njson\n(\n\"examples/src/main/resources/people.json\"\n);\n// DataFrames can be saved as Parquet files, maintaining the schema information\npeopleDF\n.\nwrite\n().\nparquet\n(\n\"people.parquet\"\n);\n// Read in the Parquet file created above.\n// Parquet files are self-describing so the schema is preserved\n// The result of loading a parquet file is also a DataFrame\nDataset\n<\nRow\n>\nparquetFileDF\n=\nspark\n.\nread\n().\nparquet\n(\n\"people.parquet\"\n);\n// Parquet files can also be used to create a temporary view and then used in SQL statements\nparquetFileDF\n.\ncreateOrReplaceTempView\n(\n\"parquetFile\"\n);\nDataset\n<\nRow\n>\nnamesDF\n=\nspark\n.\nsql\n(\n\"SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19\"\n);\nDataset\n<\nString\n>\nnamesDS\n=\nnamesDF\n.\nmap\n(\n(\nMapFunction\n<\nRow\n,\nString\n>)\nrow\n->\n\"Name: \"\n+\nrow\n.\ngetString\n(\n0\n),\nEncoders\n.\nSTRING\n());\nnamesDS\n.\nshow\n();\n// +------------+\n// |       value|\n// +------------+\n// |Name: Justin|\n// +------------+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\ndf\n<-\nread.df\n(\n\"examples/src/main/resources/people.json\"\n,\n\"json\"\n)\n# SparkDataFrame can be saved as Parquet files, maintaining the schema information.\nwrite.parquet\n(\ndf\n,\n\"people.parquet\"\n)\n# Read in the Parquet file created above. Parquet files are self-describing so the schema is preserved.\n# The result of loading a parquet file is also a DataFrame.\nparquetFile\n<-\nread.parquet\n(\n\"people.parquet\"\n)\n# Parquet files can also be used to create a temporary view and then used in SQL statements.\ncreateOrReplaceTempView\n(\nparquetFile\n,\n\"parquetFile\"\n)\nteenagers\n<-\nsql\n(\n\"SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19\"\n)\nhead\n(\nteenagers\n)\n##     name\n## 1 Justin\n# We can also run custom R-UDFs on Spark DataFrames. Here we prefix all the names with \"Name:\"\nschema\n<-\nstructType\n(\nstructField\n(\n\"name\"\n,\n\"string\"\n))\nteenNames\n<-\ndapply\n(\ndf\n,\nfunction\n(\np\n)\n{\ncbind\n(\npaste\n(\n\"Name:\"\n,\np\n$\nname\n))\n},\nschema\n)\nfor\n(\nteenName\nin\ncollect\n(\nteenNames\n)\n$\nname\n)\n{\ncat\n(\nteenName\n,\n\"\\n\"\n)\n}\n## Name: Michael\n## Name: Andy\n## Name: Justin\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nCREATE\nTEMPORARY\nVIEW\nparquetTable\nUSING\norg\n.\napache\n.\nspark\n.\nsql\n.\nparquet\nOPTIONS\n(\npath\n\"examples/src/main/resources/people.parquet\"\n)\nSELECT\n*\nFROM\nparquetTable\nPartition Discovery\nTable partitioning is a common optimization approach used in systems like Hive. In a partitioned\ntable, data are usually stored in different directories, with partitioning column values encoded in\nthe path of each partition directory. All built-in file sources (including Text/CSV/JSON/ORC/Parquet)\nare able to discover and infer partitioning information automatically.\nFor example, we can store all our previously used\npopulation data into a partitioned table using the following directory structure, with two extra\ncolumns,\ngender\nand\ncountry\nas partitioning columns:\npath\n└── to\n    └── table\n        ├── gender=male\n        │   ├── ...\n        │   │\n        │   ├── country=US\n        │   │   └── data.parquet\n        │   ├── country=CN\n        │   │   └── data.parquet\n        │   └── ...\n        └── gender=female\n            ├── ...\n            │\n            ├── country=US\n            │   └── data.parquet\n            ├── country=CN\n            │   └── data.parquet\n            └── ...\nBy passing\npath/to/table\nto either\nSparkSession.read.parquet\nor\nSparkSession.read.load\n, Spark SQL\nwill automatically extract the partitioning information from the paths.\nNow the schema of the returned DataFrame becomes:\nroot\n|-- name: string (nullable = true)\n|-- age: long (nullable = true)\n|-- gender: string (nullable = true)\n|-- country: string (nullable = true)\nNotice that the data types of the partitioning columns are automatically inferred. Currently,\nnumeric data types, date, timestamp and string type are supported. Sometimes users may not want\nto automatically infer the data types of the partitioning columns. For these use cases, the\nautomatic type inference can be configured by\nspark.sql.sources.partitionColumnTypeInference.enabled\n, which is default to\ntrue\n. When type\ninference is disabled, string type will be used for the partitioning columns.\nStarting from Spark 1.6.0, partition discovery only finds partitions under the given paths\nby default. For the above example, if users pass\npath/to/table/gender=male\nto either\nSparkSession.read.parquet\nor\nSparkSession.read.load\n,\ngender\nwill not be considered as a\npartitioning column. If users need to specify the base path that partition discovery\nshould start with, they can set\nbasePath\nin the data source options. For example,\nwhen\npath/to/table/gender=male\nis the path of the data and\nusers set\nbasePath\nto\npath/to/table/\n,\ngender\nwill be a partitioning column.\nSchema Merging\nLike Protocol Buffer, Avro, and Thrift, Parquet also supports schema evolution. Users can start with\na simple schema, and gradually add more columns to the schema as needed. In this way, users may end\nup with multiple Parquet files with different but mutually compatible schemas. The Parquet data\nsource is now able to automatically detect this case and merge schemas of all these files.\nSince schema merging is a relatively expensive operation, and is not a necessity in most cases, we\nturned it off by default starting from 1.5.0. You may enable it by\nsetting data source option\nmergeSchema\nto\ntrue\nwhen reading Parquet files (as shown in the\nexamples below), or\nsetting the global SQL option\nspark.sql.parquet.mergeSchema\nto\ntrue\n.\nfrom\npyspark.sql\nimport\nRow\n# spark is from the previous example.\n# Create a simple DataFrame, stored into a partition directory\nsc\n=\nspark\n.\nsparkContext\nsquaresDF\n=\nspark\n.\ncreateDataFrame\n(\nsc\n.\nparallelize\n(\nrange\n(\n1\n,\n6\n))\n.\nmap\n(\nlambda\ni\n:\nRow\n(\nsingle\n=\ni\n,\ndouble\n=\ni\n**\n2\n)))\nsquaresDF\n.\nwrite\n.\nparquet\n(\n\"\ndata/test_table/key=1\n\"\n)\n# Create another DataFrame in a new partition directory,\n# adding a new column and dropping an existing column\ncubesDF\n=\nspark\n.\ncreateDataFrame\n(\nsc\n.\nparallelize\n(\nrange\n(\n6\n,\n11\n))\n.\nmap\n(\nlambda\ni\n:\nRow\n(\nsingle\n=\ni\n,\ntriple\n=\ni\n**\n3\n)))\ncubesDF\n.\nwrite\n.\nparquet\n(\n\"\ndata/test_table/key=2\n\"\n)\n# Read the partitioned table\nmergedDF\n=\nspark\n.\nread\n.\noption\n(\n\"\nmergeSchema\n\"\n,\n\"\ntrue\n\"\n).\nparquet\n(\n\"\ndata/test_table\n\"\n)\nmergedDF\n.\nprintSchema\n()\n# The final schema consists of all 3 columns in the Parquet files together\n# with the partitioning column appeared in the partition directory paths.\n# root\n#  |-- double: long (nullable = true)\n#  |-- single: long (nullable = true)\n#  |-- triple: long (nullable = true)\n#  |-- key: integer (nullable = true)\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\n// This is used to implicitly convert an RDD to a DataFrame.\nimport\nspark.implicits._\n// Create a simple DataFrame, store into a partition directory\nval\nsquaresDF\n=\nspark\n.\nsparkContext\n.\nmakeRDD\n(\n1\nto\n5\n).\nmap\n(\ni\n=>\n(\ni\n,\ni\n*\ni\n)).\ntoDF\n(\n\"value\"\n,\n\"square\"\n)\nsquaresDF\n.\nwrite\n.\nparquet\n(\n\"data/test_table/key=1\"\n)\n// Create another DataFrame in a new partition directory,\n// adding a new column and dropping an existing column\nval\ncubesDF\n=\nspark\n.\nsparkContext\n.\nmakeRDD\n(\n6\nto\n10\n).\nmap\n(\ni\n=>\n(\ni\n,\ni\n*\ni\n*\ni\n)).\ntoDF\n(\n\"value\"\n,\n\"cube\"\n)\ncubesDF\n.\nwrite\n.\nparquet\n(\n\"data/test_table/key=2\"\n)\n// Read the partitioned table\nval\nmergedDF\n=\nspark\n.\nread\n.\noption\n(\n\"mergeSchema\"\n,\n\"true\"\n).\nparquet\n(\n\"data/test_table\"\n)\nmergedDF\n.\nprintSchema\n()\n// The final schema consists of all 3 columns in the Parquet files together\n// with the partitioning column appeared in the partition directory paths\n// root\n//  |-- value: int (nullable = true)\n//  |-- square: int (nullable = true)\n//  |-- cube: int (nullable = true)\n//  |-- key: int (nullable = true)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nimport\ncom.google.common.collect.Lists\n;\nimport\njava.io.Serializable\n;\nimport\njava.util.ArrayList\n;\nimport\njava.util.Arrays\n;\nimport\njava.util.Collections\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\npublic\nstatic\nclass\nSquare\nimplements\nSerializable\n{\nprivate\nint\nvalue\n;\nprivate\nint\nsquare\n;\n// Getters and setters...\n}\npublic\nstatic\nclass\nCube\nimplements\nSerializable\n{\nprivate\nint\nvalue\n;\nprivate\nint\ncube\n;\n// Getters and setters...\n}\nList\n<\nSquare\n>\nsquares\n=\nnew\nArrayList\n<>();\nfor\n(\nint\nvalue\n=\n1\n;\nvalue\n<=\n5\n;\nvalue\n++)\n{\nSquare\nsquare\n=\nnew\nSquare\n();\nsquare\n.\nsetValue\n(\nvalue\n);\nsquare\n.\nsetSquare\n(\nvalue\n*\nvalue\n);\nsquares\n.\nadd\n(\nsquare\n);\n}\n// Create a simple DataFrame, store into a partition directory\nDataset\n<\nRow\n>\nsquaresDF\n=\nspark\n.\ncreateDataFrame\n(\nsquares\n,\nSquare\n.\nclass\n);\nsquaresDF\n.\nwrite\n().\nparquet\n(\n\"data/test_table/key=1\"\n);\nList\n<\nCube\n>\ncubes\n=\nnew\nArrayList\n<>();\nfor\n(\nint\nvalue\n=\n6\n;\nvalue\n<=\n10\n;\nvalue\n++)\n{\nCube\ncube\n=\nnew\nCube\n();\ncube\n.\nsetValue\n(\nvalue\n);\ncube\n.\nsetCube\n(\nvalue\n*\nvalue\n*\nvalue\n);\ncubes\n.\nadd\n(\ncube\n);\n}\n// Create another DataFrame in a new partition directory,\n// adding a new column and dropping an existing column\nDataset\n<\nRow\n>\ncubesDF\n=\nspark\n.\ncreateDataFrame\n(\ncubes\n,\nCube\n.\nclass\n);\ncubesDF\n.\nwrite\n().\nparquet\n(\n\"data/test_table/key=2\"\n);\n// Read the partitioned table\nDataset\n<\nRow\n>\nmergedDF\n=\nspark\n.\nread\n().\noption\n(\n\"mergeSchema\"\n,\ntrue\n).\nparquet\n(\n\"data/test_table\"\n);\nmergedDF\n.\nprintSchema\n();\n// The final schema consists of all 3 columns in the Parquet files together\n// with the partitioning column appeared in the partition directory paths\n// root\n//  |-- value: int (nullable = true)\n//  |-- square: int (nullable = true)\n//  |-- cube: int (nullable = true)\n//  |-- key: int (nullable = true)\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\ndf1\n<-\ncreateDataFrame\n(\ndata.frame\n(\nsingle\n=\nc\n(\n12\n,\n29\n),\ndouble\n=\nc\n(\n19\n,\n23\n)))\ndf2\n<-\ncreateDataFrame\n(\ndata.frame\n(\ndouble\n=\nc\n(\n19\n,\n23\n),\ntriple\n=\nc\n(\n23\n,\n18\n)))\n# Create a simple DataFrame, stored into a partition directory\nwrite.df\n(\ndf1\n,\n\"data/test_table/key=1\"\n,\n\"parquet\"\n,\n\"overwrite\"\n)\n# Create another DataFrame in a new partition directory,\n# adding a new column and dropping an existing column\nwrite.df\n(\ndf2\n,\n\"data/test_table/key=2\"\n,\n\"parquet\"\n,\n\"overwrite\"\n)\n# Read the partitioned table\ndf3\n<-\nread.df\n(\n\"data/test_table\"\n,\n\"parquet\"\n,\nmergeSchema\n=\n\"true\"\n)\nprintSchema\n(\ndf3\n)\n# The final schema consists of all 3 columns in the Parquet files together\n# with the partitioning column appeared in the partition directory paths\n## root\n##  |-- single: double (nullable = true)\n##  |-- double: double (nullable = true)\n##  |-- triple: double (nullable = true)\n##  |-- key: integer (nullable = true)\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nHive metastore Parquet table conversion\nWhen reading from Hive metastore Parquet tables and writing to non-partitioned Hive metastore\nParquet tables, Spark SQL will try to use its own Parquet support instead of Hive SerDe for\nbetter performance. This behavior is controlled by the\nspark.sql.hive.convertMetastoreParquet\nconfiguration, and is turned on by default.\nHive/Parquet Schema Reconciliation\nThere are two key differences between Hive and Parquet from the perspective of table schema\nprocessing.\nHive is case insensitive, while Parquet is not\nHive considers all columns nullable, while nullability in Parquet is significant\nDue to this reason, we must reconcile Hive metastore schema with Parquet schema when converting a\nHive metastore Parquet table to a Spark SQL Parquet table. The reconciliation rules are:\nFields that have the same name in both schema must have the same data type regardless of\nnullability. The reconciled field should have the data type of the Parquet side, so that\nnullability is respected.\nThe reconciled schema contains exactly those fields defined in Hive metastore schema.\nAny fields that only appear in the Parquet schema are dropped in the reconciled schema.\nAny fields that only appear in the Hive metastore schema are added as nullable field in the\nreconciled schema.\nMetadata Refreshing\nSpark SQL caches Parquet metadata for better performance. When Hive metastore Parquet table\nconversion is enabled, metadata of those converted tables are also cached. If these tables are\nupdated by Hive or other external tools, you need to refresh them manually to ensure consistent\nmetadata.\n# spark is an existing SparkSession\nspark\n.\ncatalog\n.\nrefreshTable\n(\n\"\nmy_table\n\"\n)\n// spark is an existing SparkSession\nspark\n.\ncatalog\n.\nrefreshTable\n(\n\"my_table\"\n)\n// spark is an existing SparkSession\nspark\n.\ncatalog\n().\nrefreshTable\n(\n\"my_table\"\n);\nrefreshTable\n(\n\"my_table\"\n)\nREFRESH\nTABLE\nmy_table\n;\nColumnar Encryption\nSince Spark 3.2, columnar encryption is supported for Parquet tables with Apache Parquet 1.12+.\nParquet uses the envelope encryption practice, where file parts are encrypted with “data encryption keys” (DEKs), and the DEKs are encrypted with “master encryption keys” (MEKs). The DEKs are randomly generated by Parquet for each encrypted file/column. The MEKs are generated, stored and managed in a Key Management Service (KMS) of user’s choice. The Parquet Maven\nrepository\nhas a jar with a mock KMS implementation that allows to run column encryption and decryption using a spark-shell only, without deploying a KMS server (download the\nparquet-hadoop-tests.jar\nfile and place it in the Spark\njars\nfolder):\n# Set hadoop configuration properties, e.g. using configuration properties of\n# the Spark job:\n# --conf spark.hadoop.parquet.encryption.kms.client.class=\\\n#           \"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\"\\\n# --conf spark.hadoop.parquet.encryption.key.list=\\\n#           \"keyA:AAECAwQFBgcICQoLDA0ODw== ,  keyB:AAECAAECAAECAAECAAECAA==\"\\\n# --conf spark.hadoop.parquet.crypto.factory.class=\\\n#           \"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\"\n# Write encrypted dataframe files.\n# Column \"square\" will be protected with master key \"keyA\".\n# Parquet file footers will be protected with master key \"keyB\"\nsquaresDF\n.\nwrite\n\\\n.\noption\n(\n\"\nparquet.encryption.column.keys\n\"\n,\n\"\nkeyA:square\n\"\n)\n\\\n.\noption\n(\n\"\nparquet.encryption.footer.key\n\"\n,\n\"\nkeyB\n\"\n)\n\\\n.\nparquet\n(\n\"\n/path/to/table.parquet.encrypted\n\"\n)\n# Read encrypted dataframe files\ndf2\n=\nspark\n.\nread\n.\nparquet\n(\n\"\n/path/to/table.parquet.encrypted\n\"\n)\nsc\n.\nhadoopConfiguration\n.\nset\n(\n\"parquet.encryption.kms.client.class\"\n,\n\"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\"\n)\n// Explicit master keys (base64 encoded) - required only for mock InMemoryKMS\nsc\n.\nhadoopConfiguration\n.\nset\n(\n\"parquet.encryption.key.list\"\n,\n\"keyA:AAECAwQFBgcICQoLDA0ODw== ,  keyB:AAECAAECAAECAAECAAECAA==\"\n)\n// Activate Parquet encryption, driven by Hadoop properties\nsc\n.\nhadoopConfiguration\n.\nset\n(\n\"parquet.crypto.factory.class\"\n,\n\"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\"\n)\n// Write encrypted dataframe files.\n// Column \"square\" will be protected with master key \"keyA\".\n// Parquet file footers will be protected with master key \"keyB\"\nsquaresDF\n.\nwrite\n.\noption\n(\n\"parquet.encryption.column.keys\"\n,\n\"keyA:square\"\n).\noption\n(\n\"parquet.encryption.footer.key\"\n,\n\"keyB\"\n).\nparquet\n(\n\"/path/to/table.parquet.encrypted\"\n)\n// Read encrypted dataframe files\nval\ndf2\n=\nspark\n.\nread\n.\nparquet\n(\n\"/path/to/table.parquet.encrypted\"\n)\nsc\n.\nhadoopConfiguration\n().\nset\n(\n\"parquet.encryption.kms.client.class\"\n,\n\"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\"\n);\n// Explicit master keys (base64 encoded) - required only for mock InMemoryKMS\nsc\n.\nhadoopConfiguration\n().\nset\n(\n\"parquet.encryption.key.list\"\n,\n\"keyA:AAECAwQFBgcICQoLDA0ODw== ,  keyB:AAECAAECAAECAAECAAECAA==\"\n);\n// Activate Parquet encryption, driven by Hadoop properties\nsc\n.\nhadoopConfiguration\n().\nset\n(\n\"parquet.crypto.factory.class\"\n,\n\"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\"\n);\n// Write encrypted dataframe files.\n// Column \"square\" will be protected with master key \"keyA\".\n// Parquet file footers will be protected with master key \"keyB\"\nsquaresDF\n.\nwrite\n().\noption\n(\n\"parquet.encryption.column.keys\"\n,\n\"keyA:square\"\n).\noption\n(\n\"parquet.encryption.footer.key\"\n,\n\"keyB\"\n).\nparquet\n(\n\"/path/to/table.parquet.encrypted\"\n);\n// Read encrypted dataframe files\nDataset\n<\nRow\n>\ndf2\n=\nspark\n.\nread\n().\nparquet\n(\n\"/path/to/table.parquet.encrypted\"\n);\nKMS Client\nThe InMemoryKMS class is provided only for illustration and simple demonstration of Parquet encryption functionality.\nIt should not be used in a real deployment\n. The master encryption keys must be kept and managed in a production-grade KMS system, deployed in user’s organization. Rollout of Spark with Parquet encryption requires implementation of a client class for the KMS server. Parquet provides a plug-in\ninterface\nfor development of such classes,\npublic\ninterface\nKmsClient\n{\n// Wraps a key - encrypts it with the master key.\npublic\nString\nwrapKey\n(\nbyte\n[]\nkeyBytes\n,\nString\nmasterKeyIdentifier\n);\n// Decrypts (unwraps) a key with the master key.\npublic\nbyte\n[]\nunwrapKey\n(\nString\nwrappedKey\n,\nString\nmasterKeyIdentifier\n);\n// Use of initialization parameters is optional.\npublic\nvoid\ninitialize\n(\nConfiguration\nconfiguration\n,\nString\nkmsInstanceID\n,\nString\nkmsInstanceURL\n,\nString\naccessToken\n);\n}\nAn\nexample\nof such class for an open source\nKMS\ncan be found in the parquet-java repository. The production KMS client should be designed in cooperation with organization’s security administrators, and built by developers with an experience in access control management. Once such class is created, it can be passed to applications via the\nparquet.encryption.kms.client.class\nparameter and leveraged by general Spark users as shown in the encrypted dataframe write/read sample above.\nNote: By default, Parquet implements a “double envelope encryption” mode, that minimizes the interaction of Spark executors with a KMS server. In this mode, the DEKs are encrypted with “key encryption keys” (KEKs, randomly generated by Parquet). The KEKs are encrypted with MEKs in KMS; the result and the KEK itself are cached in Spark executor memory. Users interested in regular envelope encryption, can switch to it by setting the\nparquet.encryption.double.wrapping\nparameter to\nfalse\n. For more details on Parquet encryption parameters, visit the parquet-hadoop configuration\npage\n.\nData Source Option\nData source options of Parquet can be set via:\nthe\n.option\n/\n.options\nmethods of\nDataFrameReader\nDataFrameWriter\nDataStreamReader\nDataStreamWriter\nOPTIONS\nclause at\nCREATE TABLE USING DATA_SOURCE\nProperty Name\nDefault\nMeaning\nScope\ndatetimeRebaseMode\n(value of\nspark.sql.parquet.datetimeRebaseModeInRead\nconfiguration)\nThe\ndatetimeRebaseMode\noption allows to specify the rebasing mode for the values of the\nDATE\n,\nTIMESTAMP_MILLIS\n,\nTIMESTAMP_MICROS\nlogical types from the Julian to Proleptic Gregorian calendar.\nCurrently supported modes are:\nEXCEPTION\n: fails in reads of ancient dates/timestamps that are ambiguous between the two calendars.\nCORRECTED\n: loads dates/timestamps without rebasing.\nLEGACY\n: performs rebasing of ancient dates/timestamps from the Julian to Proleptic Gregorian calendar.\nread\nint96RebaseMode\n(value of\nspark.sql.parquet.int96RebaseModeInRead\nconfiguration)\nThe\nint96RebaseMode\noption allows to specify the rebasing mode for INT96 timestamps from the Julian to Proleptic Gregorian calendar.\nCurrently supported modes are:\nEXCEPTION\n: fails in reads of ancient INT96 timestamps that are ambiguous between the two calendars.\nCORRECTED\n: loads INT96 timestamps without rebasing.\nLEGACY\n: performs rebasing of ancient timestamps from the Julian to Proleptic Gregorian calendar.\nread\nmergeSchema\n(value of\nspark.sql.parquet.mergeSchema\nconfiguration)\nSets whether we should merge schemas collected from all Parquet part-files. This will override\nspark.sql.parquet.mergeSchema\n.\nread\ncompression\nsnappy\nCompression codec to use when saving to file. This can be one of the known case-insensitive shorten names (none, uncompressed, snappy, gzip, lzo, brotli, lz4, lz4_raw, and zstd). This will override\nspark.sql.parquet.compression.codec\n.\nwrite\nOther generic options can be found in\nGeneric Files Source Options\nConfiguration\nConfiguration of Parquet can be done via\nspark.conf.set\nor by running\nSET key=value\ncommands using SQL.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.parquet.binaryAsString\nfalse\nSome other Parquet-producing systems, in particular Impala, Hive, and older versions of Spark SQL, do\n    not differentiate between binary data and strings when writing out the Parquet schema. This\n    flag tells Spark SQL to interpret binary data as a string to provide compatibility with these systems.\n1.1.1\nspark.sql.parquet.int96AsTimestamp\ntrue\nSome Parquet-producing systems, in particular Impala and Hive, store Timestamp into INT96. This\n    flag tells Spark SQL to interpret INT96 data as a timestamp to provide compatibility with these systems.\n1.3.0\nspark.sql.parquet.int96TimestampConversion\nfalse\nThis controls whether timestamp adjustments should be applied to INT96 data when\n    converting to timestamps, for data written by Impala.  This is necessary because Impala\n    stores INT96 data with a different timezone offset than Hive & Spark.\n2.3.0\nspark.sql.parquet.outputTimestampType\nINT96\nSets which Parquet timestamp type to use when Spark writes data to Parquet files.\n    INT96 is a non-standard but commonly used timestamp type in Parquet. TIMESTAMP_MICROS\n    is a standard timestamp type in Parquet, which stores number of microseconds from the\n    Unix epoch. TIMESTAMP_MILLIS is also standard, but with millisecond precision, which\n    means Spark has to truncate the microsecond portion of its timestamp value.\n2.3.0\nspark.sql.parquet.compression.codec\nsnappy\nSets the compression codec used when writing Parquet files. If either\ncompression\nor\nparquet.compression\nis specified in the table-specific options/properties, the precedence would be\ncompression\n,\nparquet.compression\n,\nspark.sql.parquet.compression.codec\n. Acceptable values include:\n    none, uncompressed, snappy, gzip, lzo, brotli, lz4, lz4_raw, zstd.\n    Note that\nbrotli\nrequires\nBrotliCodec\nto be installed.\n1.1.1\nspark.sql.parquet.filterPushdown\ntrue\nEnables Parquet filter push-down optimization when set to true.\n1.2.0\nspark.sql.parquet.aggregatePushdown\nfalse\nIf true, aggregates will be pushed down to Parquet for optimization. Support MIN, MAX\n    and COUNT as aggregate expression. For MIN/MAX, support boolean, integer, float and date\n    type. For COUNT, support all data types. If statistics is missing from any Parquet file\n    footer, exception would be thrown.\n3.3.0\nspark.sql.hive.convertMetastoreParquet\ntrue\nWhen set to false, Spark SQL will use the Hive SerDe for parquet tables instead of the built in\n    support.\n1.1.1\nspark.sql.parquet.mergeSchema\nfalse\nWhen true, the Parquet data source merges schemas collected from all data files, otherwise the\n      schema is picked from the summary file or a random data file if no summary file is available.\n1.5.0\nspark.sql.parquet.respectSummaryFiles\nfalse\nWhen true, we make assumption that all part-files of Parquet are consistent with\n    summary files and we will ignore them when merging schema. Otherwise, if this is\n    false, which is the default, we will merge all part-files. This should be considered\n    as expert-only option, and shouldn't be enabled before knowing what it means exactly.\n1.5.0\nspark.sql.parquet.writeLegacyFormat\nfalse\nIf true, data will be written in a way of Spark 1.4 and earlier. For example, decimal values\n    will be written in Apache Parquet's fixed-length byte array format, which other systems such as\n    Apache Hive and Apache Impala use. If false, the newer format in Parquet will be used. For\n    example, decimals will be written in int-based format. If Parquet output is intended for use\n    with systems that do not support this newer format, set to true.\n1.6.0\nspark.sql.parquet.enableVectorizedReader\ntrue\nEnables vectorized parquet decoding.\n2.0.0\nspark.sql.parquet.enableNestedColumnVectorizedReader\ntrue\nEnables vectorized Parquet decoding for nested columns (e.g., struct, list, map).\n    Requires\nspark.sql.parquet.enableVectorizedReader\nto be enabled.\n3.3.0\nspark.sql.parquet.recordLevelFilter.enabled\nfalse\nIf true, enables Parquet's native record-level filtering using the pushed down filters.\n    This configuration only has an effect when\nspark.sql.parquet.filterPushdown\nis enabled and the vectorized reader is not used. You can ensure the vectorized reader\n    is not used by setting\nspark.sql.parquet.enableVectorizedReader\nto false.\n2.3.0\nspark.sql.parquet.columnarReaderBatchSize\n4096\nThe number of rows to include in a parquet vectorized reader batch. The number should\n    be carefully chosen to minimize overhead and avoid OOMs in reading data.\n2.4.0\nspark.sql.parquet.fieldId.write.enabled\ntrue\nField ID is a native field of the Parquet schema spec. When enabled,\n    Parquet writers will populate the field Id metadata (if present) in the Spark schema to the Parquet schema.\n3.3.0\nspark.sql.parquet.fieldId.read.enabled\nfalse\nField ID is a native field of the Parquet schema spec. When enabled, Parquet readers\n    will use field IDs (if present) in the requested Spark schema to look up Parquet\n    fields instead of using column names.\n3.3.0\nspark.sql.parquet.fieldId.read.ignoreMissing\nfalse\nWhen the Parquet file doesn't have any field IDs but the\n    Spark read schema is using field IDs to read, we will silently return nulls\n    when this flag is enabled, or error otherwise.\n3.3.0\nspark.sql.parquet.inferTimestampNTZ.enabled\ntrue\nWhen enabled, Parquet timestamp columns with annotation\nisAdjustedToUTC = false\nare inferred as TIMESTAMP_NTZ type during schema inference. Otherwise, all the Parquet\n    timestamp columns are inferred as TIMESTAMP_LTZ types. Note that Spark writes the\n    output schema into Parquet's footer metadata on file writing and leverages it on file\n    reading. Thus this configuration only affects the schema inference on Parquet files\n    which are not written by Spark.\n3.4.0\nspark.sql.parquet.datetimeRebaseModeInRead\nEXCEPTION\nThe rebasing mode for the values of the\nDATE\n,\nTIMESTAMP_MILLIS\n,\nTIMESTAMP_MICROS\nlogical types from the Julian to Proleptic Gregorian calendar:\nEXCEPTION\n: Spark will fail the reading if it sees ancient dates/timestamps that are ambiguous between the two calendars.\nCORRECTED\n: Spark will not do rebase and read the dates/timestamps as it is.\nLEGACY\n: Spark will rebase dates/timestamps from the legacy hybrid (Julian + Gregorian) calendar to Proleptic Gregorian calendar when reading Parquet files.\nThis config is only effective if the writer info (like Spark, Hive) of the Parquet files is unknown.\n3.0.0\nspark.sql.parquet.datetimeRebaseModeInWrite\nEXCEPTION\nThe rebasing mode for the values of the\nDATE\n,\nTIMESTAMP_MILLIS\n,\nTIMESTAMP_MICROS\nlogical types from the Proleptic Gregorian to Julian calendar:\nEXCEPTION\n: Spark will fail the writing if it sees ancient dates/timestamps that are ambiguous between the two calendars.\nCORRECTED\n: Spark will not do rebase and write the dates/timestamps as it is.\nLEGACY\n: Spark will rebase dates/timestamps from Proleptic Gregorian calendar to the legacy hybrid (Julian + Gregorian) calendar when writing Parquet files.\n3.0.0\nspark.sql.parquet.int96RebaseModeInRead\nEXCEPTION\nThe rebasing mode for the values of the\nINT96\ntimestamp type from the Julian to Proleptic Gregorian calendar:\nEXCEPTION\n: Spark will fail the reading if it sees ancient INT96 timestamps that are ambiguous between the two calendars.\nCORRECTED\n: Spark will not do rebase and read the dates/timestamps as it is.\nLEGACY\n: Spark will rebase INT96 timestamps from the legacy hybrid (Julian + Gregorian) calendar to Proleptic Gregorian calendar when reading Parquet files.\nThis config is only effective if the writer info (like Spark, Hive) of the Parquet files is unknown.\n3.1.0\nspark.sql.parquet.int96RebaseModeInWrite\nEXCEPTION\nThe rebasing mode for the values of the\nINT96\ntimestamp type from the Proleptic Gregorian to Julian calendar:\nEXCEPTION\n: Spark will fail the writing if it sees ancient timestamps that are ambiguous between the two calendars.\nCORRECTED\n: Spark will not do rebase and write the dates/timestamps as it is.\nLEGACY\n: Spark will rebase INT96 timestamps from Proleptic Gregorian calendar to the legacy hybrid (Julian + Gregorian) calendar when writing Parquet files.\n3.1.0"}
{"url": "https://spark.apache.org/docs/latest/sql-data-sources-orc.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTroubleshooting\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nORC Files\nORC Implementation\nVectorized Reader\nSchema Merging\nZstandard\nBloom Filters\nColumnar Encryption\nHive metastore ORC table conversion\nConfiguration\nData Source Option\nApache ORC\nis a columnar format which has more advanced features like native zstd compression, bloom filter and columnar encryption.\nORC Implementation\nSpark supports two ORC implementations (\nnative\nand\nhive\n) which is controlled by\nspark.sql.orc.impl\n.\nTwo implementations share most functionalities with different design goals.\nnative\nimplementation is designed to follow Spark’s data source behavior like\nParquet\n.\nhive\nimplementation is designed to follow Hive’s behavior and uses Hive SerDe.\nFor example, historically,\nnative\nimplementation handles\nCHAR/VARCHAR\nwith Spark’s native\nString\nwhile\nhive\nimplementation handles it via Hive\nCHAR/VARCHAR\n. The query results are different. Since Spark 3.1.0,\nSPARK-33480\nremoves this difference by supporting\nCHAR/VARCHAR\nfrom Spark-side.\nVectorized Reader\nnative\nimplementation supports a vectorized ORC reader and has been the default ORC implementation since Spark 2.3.\nThe vectorized reader is used for the native ORC tables (e.g., the ones created using the clause\nUSING ORC\n) when\nspark.sql.orc.impl\nis set to\nnative\nand\nspark.sql.orc.enableVectorizedReader\nis set to\ntrue\n.\nFor the Hive ORC serde tables (e.g., the ones created using the clause\nUSING HIVE OPTIONS (fileFormat 'ORC')\n),\nthe vectorized reader is used when\nspark.sql.hive.convertMetastoreOrc\nis also set to\ntrue\n, and is turned on by default.\nSchema Merging\nLike Protocol Buffer, Avro, and Thrift, ORC also supports schema evolution. Users can start with\na simple schema, and gradually add more columns to the schema as needed. In this way, users may end\nup with multiple ORC files with different but mutually compatible schemas. The ORC data\nsource is now able to automatically detect this case and merge schemas of all these files.\nSince schema merging is a relatively expensive operation, and is not a necessity in most cases, we\nturned it off by default . You may enable it by\nsetting data source option\nmergeSchema\nto\ntrue\nwhen reading ORC files, or\nsetting the global SQL option\nspark.sql.orc.mergeSchema\nto\ntrue\n.\nZstandard\nSince Spark 3.2, you can take advantage of Zstandard compression in ORC files.\nPlease see\nZstandard\nfor the benefits.\nCREATE\nTABLE\ncompressed\n(\nkey\nSTRING\n,\nvalue\nSTRING\n)\nUSING\nORC\nOPTIONS\n(\ncompression\n'zstd'\n)\nBloom Filters\nYou can control bloom filters and dictionary encodings for ORC data sources. The following ORC example will create bloom filter and use dictionary encoding only for\nfavorite_color\n. To find more detailed information about the extra ORC options, visit the official Apache ORC websites.\nCREATE\nTABLE\nusers_with_options\n(\nname\nSTRING\n,\nfavorite_color\nSTRING\n,\nfavorite_numbers\narray\n<\ninteger\n>\n)\nUSING\nORC\nOPTIONS\n(\norc\n.\nbloom\n.\nfilter\n.\ncolumns\n'favorite_color'\n,\norc\n.\ndictionary\n.\nkey\n.\nthreshold\n'1.0'\n,\norc\n.\ncolumn\n.\nencoding\n.\ndirect\n'name'\n)\nColumnar Encryption\nSince Spark 3.2, columnar encryption is supported for ORC tables with Apache ORC 1.6.\nThe following example is using Hadoop KMS as a key provider with the given location.\nPlease visit\nApache Hadoop KMS\nfor the detail.\nCREATE\nTABLE\nencrypted\n(\nssn\nSTRING\n,\nemail\nSTRING\n,\nname\nSTRING\n)\nUSING\nORC\nOPTIONS\n(\nhadoop\n.\nsecurity\n.\nkey\n.\nprovider\n.\npath\n\"kms://http@localhost:9600/kms\"\n,\norc\n.\nkey\n.\nprovider\n\"hadoop\"\n,\norc\n.\nencrypt\n\"pii:ssn,email\"\n,\norc\n.\nmask\n\"nullify:ssn;sha256:email\"\n)\nHive metastore ORC table conversion\nWhen reading from Hive metastore ORC tables and inserting to Hive metastore ORC tables, Spark SQL will try to use its own ORC support instead of Hive SerDe for better performance. For CTAS statement, only non-partitioned Hive metastore ORC tables are converted. This behavior is controlled by the\nspark.sql.hive.convertMetastoreOrc\nconfiguration, and is turned on by default.\nConfiguration\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.orc.impl\nnative\nThe name of ORC implementation. It can be one of\nnative\nand\nhive\n.\nnative\nmeans the native ORC support.\nhive\nmeans the ORC library\n      in Hive.\n2.3.0\nspark.sql.orc.enableVectorizedReader\ntrue\nEnables vectorized orc decoding in\nnative\nimplementation. If\nfalse\n,\n      a new non-vectorized ORC reader is used in\nnative\nimplementation.\n      For\nhive\nimplementation, this is ignored.\n2.3.0\nspark.sql.orc.columnarReaderBatchSize\n4096\nThe number of rows to include in an orc vectorized reader batch. The number should\n      be carefully chosen to minimize overhead and avoid OOMs in reading data.\n2.4.0\nspark.sql.orc.columnarWriterBatchSize\n1024\nThe number of rows to include in an orc vectorized writer batch. The number should\n      be carefully chosen to minimize overhead and avoid OOMs in writing data.\n3.4.0\nspark.sql.orc.enableNestedColumnVectorizedReader\ntrue\nEnables vectorized orc decoding in\nnative\nimplementation for nested data types\n      (array, map and struct). If\nspark.sql.orc.enableVectorizedReader\nis set to\nfalse\n, this is ignored.\n3.2.0\nspark.sql.orc.filterPushdown\ntrue\nWhen true, enable filter pushdown for ORC files.\n1.4.0\nspark.sql.orc.aggregatePushdown\nfalse\nIf true, aggregates will be pushed down to ORC for optimization. Support MIN, MAX and\n      COUNT as aggregate expression. For MIN/MAX, support boolean, integer, float and date\n      type. For COUNT, support all data types. If statistics is missing from any ORC file\n      footer, exception would be thrown.\n3.3.0\nspark.sql.orc.mergeSchema\nfalse\nWhen true, the ORC data source merges schemas collected from all data files,\n      otherwise the schema is picked from a random data file.\n3.0.0\nspark.sql.hive.convertMetastoreOrc\ntrue\nWhen set to false, Spark SQL will use the Hive SerDe for ORC tables instead of the built in\n    support.\n2.0.0\nData Source Option\nData source options of ORC can be set via:\nthe\n.option\n/\n.options\nmethods of\nDataFrameReader\nDataFrameWriter\nDataStreamReader\nDataStreamWriter\nOPTIONS\nclause at\nCREATE TABLE USING DATA_SOURCE\nProperty Name\nDefault\nMeaning\nScope\nmergeSchema\nfalse\nsets whether we should merge schemas collected from all ORC part-files. This will override\nspark.sql.orc.mergeSchema\n. The default value is specified in\nspark.sql.orc.mergeSchema\n.\nread\ncompression\nzstd\ncompression codec to use when saving to file. This can be one of the known case-insensitive shorten names (none, snappy, zlib, lzo, zstd, lz4 and brotli). This will override\norc.compress\nand\nspark.sql.orc.compression.codec\n. Note that\nbrotli\nrequires\nbrotli4j\nto be installed.\nwrite\nOther generic options can be found in\nGeneric File Source Options\n."}
{"url": "https://spark.apache.org/docs/latest/sql-data-sources-json.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTroubleshooting\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nJSON Files\nSpark SQL can automatically infer the schema of a JSON dataset and load it as a DataFrame.\nThis conversion can be done using\nSparkSession.read.json\non a JSON file.\nNote that the file that is offered as\na json file\nis not a typical JSON file. Each\nline must contain a separate, self-contained valid JSON object. For more information, please see\nJSON Lines text format, also called newline-delimited JSON\n.\nFor a regular multi-line JSON file, set the\nmultiLine\nparameter to\nTrue\n.\n# spark is from the previous example.\nsc\n=\nspark\n.\nsparkContext\n# A JSON dataset is pointed to by path.\n# The path can be either a single text file or a directory storing text files\npath\n=\n\"\nexamples/src/main/resources/people.json\n\"\npeopleDF\n=\nspark\n.\nread\n.\njson\n(\npath\n)\n# The inferred schema can be visualized using the printSchema() method\npeopleDF\n.\nprintSchema\n()\n# root\n#  |-- age: long (nullable = true)\n#  |-- name: string (nullable = true)\n# Creates a temporary view using the DataFrame\npeopleDF\n.\ncreateOrReplaceTempView\n(\n\"\npeople\n\"\n)\n# SQL statements can be run by using the sql methods provided by spark\nteenagerNamesDF\n=\nspark\n.\nsql\n(\n\"\nSELECT name FROM people WHERE age BETWEEN 13 AND 19\n\"\n)\nteenagerNamesDF\n.\nshow\n()\n# +------+\n# |  name|\n# +------+\n# |Justin|\n# +------+\n# Alternatively, a DataFrame can be created for a JSON dataset represented by\n# an RDD[String] storing one JSON object per string\njsonStrings\n=\n[\n'\n{\n\"\nname\n\"\n:\n\"\nYin\n\"\n,\n\"\naddress\n\"\n:{\n\"\ncity\n\"\n:\n\"\nColumbus\n\"\n,\n\"\nstate\n\"\n:\n\"\nOhio\n\"\n}}\n'\n]\notherPeopleRDD\n=\nsc\n.\nparallelize\n(\njsonStrings\n)\notherPeople\n=\nspark\n.\nread\n.\njson\n(\notherPeopleRDD\n)\notherPeople\n.\nshow\n()\n# +---------------+----+\n# |        address|name|\n# +---------------+----+\n# |[Columbus,Ohio]| Yin|\n# +---------------+----+\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\nSpark SQL can automatically infer the schema of a JSON dataset and load it as a\nDataset[Row]\n.\nThis conversion can be done using\nSparkSession.read.json()\non either a\nDataset[String]\n,\nor a JSON file.\nNote that the file that is offered as\na json file\nis not a typical JSON file. Each\nline must contain a separate, self-contained valid JSON object. For more information, please see\nJSON Lines text format, also called newline-delimited JSON\n.\nFor a regular multi-line JSON file, set the\nmultiLine\noption to\ntrue\n.\n// Primitive types (Int, String, etc) and Product types (case classes) encoders are\n// supported by importing this when creating a Dataset.\nimport\nspark.implicits._\n// A JSON dataset is pointed to by path.\n// The path can be either a single text file or a directory storing text files\nval\npath\n=\n\"examples/src/main/resources/people.json\"\nval\npeopleDF\n=\nspark\n.\nread\n.\njson\n(\npath\n)\n// The inferred schema can be visualized using the printSchema() method\npeopleDF\n.\nprintSchema\n()\n// root\n//  |-- age: long (nullable = true)\n//  |-- name: string (nullable = true)\n// Creates a temporary view using the DataFrame\npeopleDF\n.\ncreateOrReplaceTempView\n(\n\"people\"\n)\n// SQL statements can be run by using the sql methods provided by spark\nval\nteenagerNamesDF\n=\nspark\n.\nsql\n(\n\"SELECT name FROM people WHERE age BETWEEN 13 AND 19\"\n)\nteenagerNamesDF\n.\nshow\n()\n// +------+\n// |  name|\n// +------+\n// |Justin|\n// +------+\n// Alternatively, a DataFrame can be created for a JSON dataset represented by\n// a Dataset[String] storing one JSON object per string\nval\notherPeopleDataset\n=\nspark\n.\ncreateDataset\n(\n\"\"\"{\"name\":\"Yin\",\"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"}}\"\"\"\n::\nNil\n)\nval\notherPeople\n=\nspark\n.\nread\n.\njson\n(\notherPeopleDataset\n)\notherPeople\n.\nshow\n()\n// +---------------+----+\n// |        address|name|\n// +---------------+----+\n// |[Columbus,Ohio]| Yin|\n// +---------------+----+\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nSpark SQL can automatically infer the schema of a JSON dataset and load it as a\nDataset<Row>\n.\nThis conversion can be done using\nSparkSession.read().json()\non either a\nDataset<String>\n,\nor a JSON file.\nNote that the file that is offered as\na json file\nis not a typical JSON file. Each\nline must contain a separate, self-contained valid JSON object. For more information, please see\nJSON Lines text format, also called newline-delimited JSON\n.\nFor a regular multi-line JSON file, set the\nmultiLine\noption to\ntrue\n.\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\n// A JSON dataset is pointed to by path.\n// The path can be either a single text file or a directory storing text files\nDataset\n<\nRow\n>\npeople\n=\nspark\n.\nread\n().\njson\n(\n\"examples/src/main/resources/people.json\"\n);\n// The inferred schema can be visualized using the printSchema() method\npeople\n.\nprintSchema\n();\n// root\n//  |-- age: long (nullable = true)\n//  |-- name: string (nullable = true)\n// Creates a temporary view using the DataFrame\npeople\n.\ncreateOrReplaceTempView\n(\n\"people\"\n);\n// SQL statements can be run by using the sql methods provided by spark\nDataset\n<\nRow\n>\nnamesDF\n=\nspark\n.\nsql\n(\n\"SELECT name FROM people WHERE age BETWEEN 13 AND 19\"\n);\nnamesDF\n.\nshow\n();\n// +------+\n// |  name|\n// +------+\n// |Justin|\n// +------+\n// Alternatively, a DataFrame can be created for a JSON dataset represented by\n// a Dataset<String> storing one JSON object per string.\nList\n<\nString\n>\njsonData\n=\nArrays\n.\nasList\n(\n\"{\\\"name\\\":\\\"Yin\\\",\\\"address\\\":{\\\"city\\\":\\\"Columbus\\\",\\\"state\\\":\\\"Ohio\\\"}}\"\n);\nDataset\n<\nString\n>\nanotherPeopleDataset\n=\nspark\n.\ncreateDataset\n(\njsonData\n,\nEncoders\n.\nSTRING\n());\nDataset\n<\nRow\n>\nanotherPeople\n=\nspark\n.\nread\n().\njson\n(\nanotherPeopleDataset\n);\nanotherPeople\n.\nshow\n();\n// +---------------+----+\n// |        address|name|\n// +---------------+----+\n// |[Columbus,Ohio]| Yin|\n// +---------------+----+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\nSpark SQL can automatically infer the schema of a JSON dataset and load it as a DataFrame. using\nthe\nread.json()\nfunction, which loads data from a directory of JSON files where each line of the\nfiles is a JSON object.\nNote that the file that is offered as\na json file\nis not a typical JSON file. Each\nline must contain a separate, self-contained valid JSON object. For more information, please see\nJSON Lines text format, also called newline-delimited JSON\n.\nFor a regular multi-line JSON file, set a named parameter\nmultiLine\nto\nTRUE\n.\n# A JSON dataset is pointed to by path.\n# The path can be either a single text file or a directory storing text files.\npath\n<-\n\"examples/src/main/resources/people.json\"\n# Create a DataFrame from the file(s) pointed to by path\npeople\n<-\nread.json\n(\npath\n)\n# The inferred schema can be visualized using the printSchema() method.\nprintSchema\n(\npeople\n)\n## root\n##  |-- age: long (nullable = true)\n##  |-- name: string (nullable = true)\n# Register this DataFrame as a table.\ncreateOrReplaceTempView\n(\npeople\n,\n\"people\"\n)\n# SQL statements can be run by using the sql methods.\nteenagers\n<-\nsql\n(\n\"SELECT name FROM people WHERE age >= 13 AND age <= 19\"\n)\nhead\n(\nteenagers\n)\n##     name\n## 1 Justin\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nCREATE\nTEMPORARY\nVIEW\njsonTable\nUSING\norg\n.\napache\n.\nspark\n.\nsql\n.\njson\nOPTIONS\n(\npath\n\"examples/src/main/resources/people.json\"\n)\nSELECT\n*\nFROM\njsonTable\nData Source Option\nData source options of JSON can be set via:\nthe\n.option\n/\n.options\nmethods of\nDataFrameReader\nDataFrameWriter\nDataStreamReader\nDataStreamWriter\nthe built-in functions below\nfrom_json\nto_json\nschema_of_json\nOPTIONS\nclause at\nCREATE TABLE USING DATA_SOURCE\nProperty Name\nDefault\nMeaning\nScope\ntimeZone\n(value of\nspark.sql.session.timeZone\nconfiguration)\nSets the string that indicates a time zone ID to be used to format timestamps in the JSON datasources or partition values. The following formats of\ntimeZone\nare supported:\nRegion-based zone ID: It should have the form 'area/city', such as 'America/Los_Angeles'.\nZone offset: It should be in the format '(+|-)HH:mm', for example '-08:00' or '+01:00'. Also 'UTC' and 'Z' are supported as aliases of '+00:00'.\nOther short names like 'CST' are not recommended to use because they can be ambiguous.\nread/write\nprimitivesAsString\nfalse\nInfers all primitive values as a string type.\nread\nprefersDecimal\nfalse\nInfers all floating-point values as a decimal type. If the values do not fit in decimal, then it infers them as doubles.\nread\nallowComments\nfalse\nIgnores Java/C++ style comment in JSON records.\nread\nallowUnquotedFieldNames\nfalse\nAllows unquoted JSON field names.\nread\nallowSingleQuotes\ntrue\nAllows single quotes in addition to double quotes.\nread\nallowNumericLeadingZeros\nfalse\nAllows leading zeros in numbers (e.g. 00012).\nread\nallowBackslashEscapingAnyCharacter\nfalse\nAllows accepting quoting of all character using backslash quoting mechanism.\nread\nmode\nPERMISSIVE\nAllows a mode for dealing with corrupt records during parsing.\nPERMISSIVE\n: when it meets a corrupted record, puts the malformed string into a field configured by\ncolumnNameOfCorruptRecord\n, and sets malformed fields to\nnull\n. To keep corrupt records, an user can set a string type field named\ncolumnNameOfCorruptRecord\nin an user-defined schema. If a schema does not have the field, it drops corrupt records during parsing. When inferring a schema, it implicitly adds a\ncolumnNameOfCorruptRecord\nfield in an output schema.\nDROPMALFORMED\n: ignores the whole corrupted records. This mode is unsupported in the JSON built-in functions.\nFAILFAST\n: throws an exception when it meets corrupted records.\nread\ncolumnNameOfCorruptRecord\n(value of\nspark.sql.columnNameOfCorruptRecord\nconfiguration)\nAllows renaming the new field having malformed string created by\nPERMISSIVE\nmode. This overrides spark.sql.columnNameOfCorruptRecord.\nread\ndateFormat\nyyyy-MM-dd\nSets the string that indicates a date format. Custom date formats follow the formats at\ndatetime pattern\n. This applies to date type.\nread/write\ntimestampFormat\nyyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]\nSets the string that indicates a timestamp format. Custom date formats follow the formats at\ndatetime pattern\n. This applies to timestamp type.\nread/write\ntimestampNTZFormat\nyyyy-MM-dd'T'HH:mm:ss[.SSS]\nSets the string that indicates a timestamp without timezone format. Custom date formats follow the formats at\nDatetime Patterns\n. This applies to timestamp without timezone type, note that zone-offset and time-zone components are not supported when writing or reading this data type.\nread/write\nenableDateTimeParsingFallback\nEnabled if the time parser policy has legacy settings or if no custom date or timestamp pattern was provided.\nAllows falling back to the backward compatible (Spark 1.x and 2.0) behavior of parsing dates and timestamps if values do not match the set patterns.\nread\nmultiLine\nfalse\nParse one record, which may span multiple lines, per file. JSON built-in functions ignore this option.\nread\nallowUnquotedControlChars\nfalse\nAllows JSON Strings to contain unquoted control characters (ASCII characters with value less than 32, including tab and line feed characters) or not.\nread\nencoding\nDetected automatically when\nmultiLine\nis set to\ntrue\n(for reading),\nUTF-8\n(for writing)\nFor reading, allows to forcibly set one of standard basic or extended encoding for the JSON files. For example UTF-16BE, UTF-32LE. For writing, Specifies encoding (charset) of saved json files. JSON built-in functions ignore this option.\nread/write\nlineSep\n\\r\n,\n\\r\\n\n,\n\\n\n(for reading),\n\\n\n(for writing)\nDefines the line separator that should be used for parsing. JSON built-in functions ignore this option.\nread/write\nsamplingRatio\n1.0\nDefines fraction of input JSON objects used for schema inferring.\nread\ndropFieldIfAllNull\nfalse\nWhether to ignore column of all null values or empty array during schema inference.\nread\nlocale\nen-US\nSets a locale as language tag in IETF BCP 47 format. For instance,\nlocale\nis used while parsing dates and timestamps.\nread\nallowNonNumericNumbers\ntrue\nAllows JSON parser to recognize set of “Not-a-Number” (NaN) tokens as legal floating number values.\n+INF\n: for positive infinity, as well as alias of\n+Infinity\nand\nInfinity\n.\n-INF\n: for negative infinity, alias\n-Infinity\n.\nNaN\n: for other not-a-numbers, like result of division by zero.\nread\ncompression\n(none)\nCompression codec to use when saving to file. This can be one of the known case-insensitive shorten names (none, bzip2, gzip, lz4, snappy and deflate). JSON built-in functions ignore this option.\nwrite\nignoreNullFields\n(value of\nspark.sql.jsonGenerator.ignoreNullFields\nconfiguration)\nWhether to ignore null fields when generating JSON objects.\nwrite\nuseUnsafeRow\n(value of\nspark.sql.json.useUnsafeRow\nconfiguration)\nWhether to use UnsafeRow to represent struct result in the JSON parser.\nread\nOther generic options can be found in\nGeneric File Source Options\n."}
{"url": "https://spark.apache.org/docs/latest/sql-data-sources-csv.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTroubleshooting\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nCSV Files\nSpark SQL provides\nspark.read().csv(\"file_name\")\nto read a file or directory of files in CSV format into Spark DataFrame, and\ndataframe.write().csv(\"path\")\nto write to a CSV file. Function\noption()\ncan be used to customize the behavior of reading or writing, such as controlling behavior of the header, delimiter character, character set, and so on.\n# spark is from the previous example\nsc\n=\nspark\n.\nsparkContext\n# A CSV dataset is pointed to by path.\n# The path can be either a single CSV file or a directory of CSV files\npath\n=\n\"\nexamples/src/main/resources/people.csv\n\"\ndf\n=\nspark\n.\nread\n.\ncsv\n(\npath\n)\ndf\n.\nshow\n()\n# +------------------+\n# |               _c0|\n# +------------------+\n# |      name;age;job|\n# |Jorge;30;Developer|\n# |  Bob;32;Developer|\n# +------------------+\n# Read a csv with delimiter, the default delimiter is \",\"\ndf2\n=\nspark\n.\nread\n.\noption\n(\n\"\ndelimiter\n\"\n,\n\"\n;\n\"\n).\ncsv\n(\npath\n)\ndf2\n.\nshow\n()\n# +-----+---+---------+\n# |  _c0|_c1|      _c2|\n# +-----+---+---------+\n# | name|age|      job|\n# |Jorge| 30|Developer|\n# |  Bob| 32|Developer|\n# +-----+---+---------+\n# Read a csv with delimiter and a header\ndf3\n=\nspark\n.\nread\n.\noption\n(\n\"\ndelimiter\n\"\n,\n\"\n;\n\"\n).\noption\n(\n\"\nheader\n\"\n,\nTrue\n).\ncsv\n(\npath\n)\ndf3\n.\nshow\n()\n# +-----+---+---------+\n# | name|age|      job|\n# +-----+---+---------+\n# |Jorge| 30|Developer|\n# |  Bob| 32|Developer|\n# +-----+---+---------+\n# You can also use options() to use multiple options\ndf4\n=\nspark\n.\nread\n.\noptions\n(\ndelimiter\n=\n\"\n;\n\"\n,\nheader\n=\nTrue\n).\ncsv\n(\npath\n)\n# \"output\" is a folder which contains multiple csv files and a _SUCCESS file.\ndf3\n.\nwrite\n.\ncsv\n(\n\"\noutput\n\"\n)\n# Read all files in a folder, please make sure only CSV files should present in the folder.\nfolderPath\n=\n\"\nexamples/src/main/resources\n\"\ndf5\n=\nspark\n.\nread\n.\ncsv\n(\nfolderPath\n)\ndf5\n.\nshow\n()\n# Wrong schema because non-CSV files are read\n# +-----------+\n# |        _c0|\n# +-----------+\n# |238val_238|\n# |  86val_86|\n# |311val_311|\n# |  27val_27|\n# |165val_165|\n# +-----------+\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\n// A CSV dataset is pointed to by path.\n// The path can be either a single CSV file or a directory of CSV files\nval\npath\n=\n\"examples/src/main/resources/people.csv\"\nval\ndf\n=\nspark\n.\nread\n.\ncsv\n(\npath\n)\ndf\n.\nshow\n()\n// +------------------+\n// |               _c0|\n// +------------------+\n// |      name;age;job|\n// |Jorge;30;Developer|\n// |  Bob;32;Developer|\n// +------------------+\n// Read a csv with delimiter, the default delimiter is \",\"\nval\ndf2\n=\nspark\n.\nread\n.\noption\n(\n\"delimiter\"\n,\n\";\"\n).\ncsv\n(\npath\n)\ndf2\n.\nshow\n()\n// +-----+---+---------+\n// |  _c0|_c1|      _c2|\n// +-----+---+---------+\n// | name|age|      job|\n// |Jorge| 30|Developer|\n// |  Bob| 32|Developer|\n// +-----+---+---------+\n// Read a csv with delimiter and a header\nval\ndf3\n=\nspark\n.\nread\n.\noption\n(\n\"delimiter\"\n,\n\";\"\n).\noption\n(\n\"header\"\n,\n\"true\"\n).\ncsv\n(\npath\n)\ndf3\n.\nshow\n()\n// +-----+---+---------+\n// | name|age|      job|\n// +-----+---+---------+\n// |Jorge| 30|Developer|\n// |  Bob| 32|Developer|\n// +-----+---+---------+\n// You can also use options() to use multiple options\nval\ndf4\n=\nspark\n.\nread\n.\noptions\n(\nMap\n(\n\"delimiter\"\n->\n\";\"\n,\n\"header\"\n->\n\"true\"\n)).\ncsv\n(\npath\n)\n// \"output\" is a folder which contains multiple csv files and a _SUCCESS file.\ndf3\n.\nwrite\n.\ncsv\n(\n\"output\"\n)\n// Read all files in a folder, please make sure only CSV files should present in the folder.\nval\nfolderPath\n=\n\"examples/src/main/resources\"\n;\nval\ndf5\n=\nspark\n.\nread\n.\ncsv\n(\nfolderPath\n);\ndf5\n.\nshow\n();\n// Wrong schema because non-CSV files are read\n// +-----------+\n// |        _c0|\n// +-----------+\n// |238val_238|\n// |  86val_86|\n// |311val_311|\n// |  27val_27|\n// |165val_165|\n// +-----------+\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\n// A CSV dataset is pointed to by path.\n// The path can be either a single CSV file or a directory of CSV files\nString\npath\n=\n\"examples/src/main/resources/people.csv\"\n;\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\nread\n().\ncsv\n(\npath\n);\ndf\n.\nshow\n();\n// +------------------+\n// |               _c0|\n// +------------------+\n// |      name;age;job|\n// |Jorge;30;Developer|\n// |  Bob;32;Developer|\n// +------------------+\n// Read a csv with delimiter, the default delimiter is \",\"\nDataset\n<\nRow\n>\ndf2\n=\nspark\n.\nread\n().\noption\n(\n\"delimiter\"\n,\n\";\"\n).\ncsv\n(\npath\n);\ndf2\n.\nshow\n();\n// +-----+---+---------+\n// |  _c0|_c1|      _c2|\n// +-----+---+---------+\n// | name|age|      job|\n// |Jorge| 30|Developer|\n// |  Bob| 32|Developer|\n// +-----+---+---------+\n// Read a csv with delimiter and a header\nDataset\n<\nRow\n>\ndf3\n=\nspark\n.\nread\n().\noption\n(\n\"delimiter\"\n,\n\";\"\n).\noption\n(\n\"header\"\n,\n\"true\"\n).\ncsv\n(\npath\n);\ndf3\n.\nshow\n();\n// +-----+---+---------+\n// | name|age|      job|\n// +-----+---+---------+\n// |Jorge| 30|Developer|\n// |  Bob| 32|Developer|\n// +-----+---+---------+\n// You can also use options() to use multiple options\njava\n.\nutil\n.\nMap\n<\nString\n,\nString\n>\noptionsMap\n=\nnew\njava\n.\nutil\n.\nHashMap\n<\nString\n,\nString\n>();\noptionsMap\n.\nput\n(\n\"delimiter\"\n,\n\";\"\n);\noptionsMap\n.\nput\n(\n\"header\"\n,\n\"true\"\n);\nDataset\n<\nRow\n>\ndf4\n=\nspark\n.\nread\n().\noptions\n(\noptionsMap\n).\ncsv\n(\npath\n);\n// \"output\" is a folder which contains multiple csv files and a _SUCCESS file.\ndf3\n.\nwrite\n().\ncsv\n(\n\"output\"\n);\n// Read all files in a folder, please make sure only CSV files should present in the folder.\nString\nfolderPath\n=\n\"examples/src/main/resources\"\n;\nDataset\n<\nRow\n>\ndf5\n=\nspark\n.\nread\n().\ncsv\n(\nfolderPath\n);\ndf5\n.\nshow\n();\n// Wrong schema because non-CSV files are read\n// +-----------+\n// |        _c0|\n// +-----------+\n// |238val_238|\n// |  86val_86|\n// |311val_311|\n// |  27val_27|\n// |165val_165|\n// +-----------+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\nData Source Option\nData source options of CSV can be set via:\nthe\n.option\n/\n.options\nmethods of\nDataFrameReader\nDataFrameWriter\nDataStreamReader\nDataStreamWriter\nthe built-in functions below\nfrom_csv\nto_csv\nschema_of_csv\nOPTIONS\nclause at\nCREATE TABLE USING DATA_SOURCE\nProperty Name\nDefault\nMeaning\nScope\nsep\ndelimiter\n,\nSets a separator for each field and value. This separator can be one or more characters.\nread/write\nextension\ncsv\nSets the file extension for the output files. Limited to letters. Length must equal 3.\nwrite\nencoding\ncharset\nUTF-8\nFor reading, decodes the CSV files by the given encoding type. For writing, specifies encoding (charset) of saved CSV files. CSV built-in functions ignore this option.\nread/write\nquote\n\"\nSets a single character used for escaping quoted values where the separator can be part of the value. For reading, if you would like to turn off quotations, you need to set not\nnull\nbut an empty string. For writing, if an empty string is set, it uses\nu0000\n(null character).\nread/write\nquoteAll\nfalse\nA flag indicating whether all values should always be enclosed in quotes. Default is to only escape values containing a quote character.\nwrite\nescape\n\\\nSets a single character used for escaping quotes inside an already quoted value.\nread/write\nescapeQuotes\ntrue\nA flag indicating whether values containing quotes should always be enclosed in quotes. Default is to escape all values containing a quote character.\nwrite\ncomment\nSets a single character used for skipping lines beginning with this character. By default, it is disabled.\nread\nheader\nfalse\nFor reading, uses the first line as names of columns. For writing, writes the names of columns as the first line. Note that if the given path is a RDD of Strings, this header option will remove all lines same with the header if exists. CSV built-in functions ignore this option.\nread/write\ninferSchema\nfalse\nInfers the input schema automatically from data. It requires one extra pass over the data. CSV built-in functions ignore this option.\nread\npreferDate\ntrue\nDuring schema inference (\ninferSchema\n), attempts to infer string columns that contain dates as\nDate\nif the values satisfy the\ndateFormat\noption or default date format. For columns that contain a mixture of dates and timestamps, try inferring them as\nTimestampType\nif timestamp format not specified, otherwise infer them as\nStringType\n.\nread\nenforceSchema\ntrue\nIf it is set to\ntrue\n, the specified or inferred schema will be forcibly applied to datasource files, and headers in CSV files will be ignored. If the option is set to\nfalse\n, the schema will be validated against all headers in CSV files in the case when the\nheader\noption is set to\ntrue\n. Field names in the schema and column names in CSV headers are checked by their positions taking into account\nspark.sql.caseSensitive\n. Though the default value is true, it is recommended to disable the\nenforceSchema\noption to avoid incorrect results. CSV built-in functions ignore this option.\nread\nignoreLeadingWhiteSpace\nfalse\n(for reading),\ntrue\n(for writing)\nA flag indicating whether or not leading whitespaces from values being read/written should be skipped.\nread/write\nignoreTrailingWhiteSpace\nfalse\n(for reading),\ntrue\n(for writing)\nA flag indicating whether or not trailing whitespaces from values being read/written should be skipped.\nread/write\nnullValue\nSets the string representation of a null value. Since 2.0.1, this\nnullValue\nparam applies to all supported types including the string type.\nread/write\nnanValue\nNaN\nSets the string representation of a non-number value.\nread\npositiveInf\nInf\nSets the string representation of a positive infinity value.\nread\nnegativeInf\n-Inf\nSets the string representation of a negative infinity value.\nread\ndateFormat\nyyyy-MM-dd\nSets the string that indicates a date format. Custom date formats follow the formats at\nDatetime Patterns\n. This applies to date type.\nread/write\ntimestampFormat\nyyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]\nSets the string that indicates a timestamp format. Custom date formats follow the formats at\nDatetime Patterns\n. This applies to timestamp type.\nread/write\ntimestampNTZFormat\nyyyy-MM-dd'T'HH:mm:ss[.SSS]\nSets the string that indicates a timestamp without timezone format. Custom date formats follow the formats at\nDatetime Patterns\n. This applies to timestamp without timezone type, note that zone-offset and time-zone components are not supported when writing or reading this data type.\nread/write\nenableDateTimeParsingFallback\nEnabled if the time parser policy has legacy settings or if no custom date or timestamp pattern was provided.\nAllows falling back to the backward compatible (Spark 1.x and 2.0) behavior of parsing dates and timestamps if values do not match the set patterns.\nread\nmaxColumns\n20480\nDefines a hard limit of how many columns a record can have.\nread\nmaxCharsPerColumn\n-1\nDefines the maximum number of characters allowed for any given value being read. By default, it is -1 meaning unlimited length\nread\nmode\nPERMISSIVE\nAllows a mode for dealing with corrupt records during parsing. It supports the following case-insensitive modes. Note that Spark tries to parse only required columns in CSV under column pruning. Therefore, corrupt records can be different based on required set of fields. This behavior can be controlled by\nspark.sql.csv.parser.columnPruning.enabled\n(enabled by default).\nPERMISSIVE\n: when it meets a corrupted record, puts the malformed string into a field configured by\ncolumnNameOfCorruptRecord\n, and sets malformed fields to\nnull\n. To keep corrupt records, an user can set a string type field named\ncolumnNameOfCorruptRecord\nin an user-defined schema. If a schema does not have the field, it drops corrupt records during parsing. A record with less/more tokens than schema is not a corrupted record to CSV. When it meets a record having fewer tokens than the length of the schema, sets\nnull\nto extra fields. When the record has more tokens than the length of the schema, it drops extra tokens.\nDROPMALFORMED\n: ignores the whole corrupted records. This mode is unsupported in the CSV built-in functions.\nFAILFAST\n: throws an exception when it meets corrupted records.\nread\ncolumnNameOfCorruptRecord\n(value of\nspark.sql.columnNameOfCorruptRecord\nconfiguration)\nAllows renaming the new field having malformed string created by\nPERMISSIVE\nmode. This overrides\nspark.sql.columnNameOfCorruptRecord\n.\nread\nmultiLine\nfalse\nAllows a row to span multiple lines, by parsing line breaks within quoted values as part of the value itself. CSV built-in functions ignore this option.\nread\ncharToEscapeQuoteEscaping\nescape\nor\n\\0\nSets a single character used for escaping the escape for the quote character. The default value is escape character when escape and quote characters are different,\n\\0\notherwise.\nread/write\nsamplingRatio\n1.0\nDefines fraction of rows used for schema inferring. CSV built-in functions ignore this option.\nread\nemptyValue\n(for reading),\n\"\"\n(for writing)\nSets the string representation of an empty value.\nread/write\nlocale\nen-US\nSets a locale as language tag in IETF BCP 47 format. For instance, this is used while parsing dates and timestamps.\nread\nlineSep\n\\r\n,\n\\r\\n\nand\n\\n\n(for reading),\n\\n\n(for writing)\nDefines the line separator that should be used for parsing/writing. Maximum length is 1 character. CSV built-in functions ignore this option.\nread/write\nunescapedQuoteHandling\nSTOP_AT_DELIMITER\nDefines how the CsvParser will handle values with unescaped quotes.\nSTOP_AT_CLOSING_QUOTE\n: If unescaped quotes are found in the input, accumulate the quote character and proceed parsing the value as a quoted value, until a closing quote is found.\nBACK_TO_DELIMITER\n: If unescaped quotes are found in the input, consider the value as an unquoted value. This will make the parser accumulate all characters of the current parsed value until the delimiter is found. If no delimiter is found in the value, the parser will continue accumulating characters from the input until a delimiter or line ending is found.\nSTOP_AT_DELIMITER\n: If unescaped quotes are found in the input, consider the value as an unquoted value. This will make the parser accumulate all characters until the delimiter or a line ending is found in the input.\nSKIP_VALUE\n: If unescaped quotes are found in the input, the content parsed for the given value will be skipped and the value set in nullValue will be produced instead.\nRAISE_ERROR\n: If unescaped quotes are found in the input, a TextParsingException will be thrown.\nread\ncompression\ncodec\n(none)\nCompression codec to use when saving to file. This can be one of the known case-insensitive shorten names (\nnone\n,\nbzip2\n,\ngzip\n,\nlz4\n,\nsnappy\nand\ndeflate\n). CSV built-in functions ignore this option.\nwrite\ntimeZone\n(value of\nspark.sql.session.timeZone\nconfiguration)\nSets the string that indicates a time zone ID to be used to format timestamps in the JSON datasources or partition values. The following formats of\ntimeZone\nare supported:\nRegion-based zone ID: It should have the form 'area/city', such as 'America/Los_Angeles'.\nZone offset: It should be in the format '(+|-)HH:mm', for example '-08:00' or '+01:00'. Also 'UTC' and 'Z' are supported as aliases of '+00:00'.\nOther short names like 'CST' are not recommended to use because they can be ambiguous.\nread/write\nOther generic options can be found in\nGeneric File Source Options\n."}
{"url": "https://spark.apache.org/docs/latest/sql-data-sources-text.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTroubleshooting\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nText Files\nSpark SQL provides\nspark.read().text(\"file_name\")\nto read a file or directory of text files into a Spark DataFrame, and\ndataframe.write().text(\"path\")\nto write to a text file. When reading a text file, each line becomes each row that has string “value” column by default. The line separator can be changed as shown in the example below. The\noption()\nfunction can be used to customize the behavior of reading or writing, such as controlling behavior of the line separator, compression, and so on.\n# spark is from the previous example\nsc\n=\nspark\n.\nsparkContext\n# A text dataset is pointed to by path.\n# The path can be either a single text file or a directory of text files\npath\n=\n\"\nexamples/src/main/resources/people.txt\n\"\ndf1\n=\nspark\n.\nread\n.\ntext\n(\npath\n)\ndf1\n.\nshow\n()\n# +-----------+\n# |      value|\n# +-----------+\n# |Michael, 29|\n# |   Andy, 30|\n# | Justin, 19|\n# +-----------+\n# You can use 'lineSep' option to define the line separator.\n# The line separator handles all `\\r`, `\\r\\n` and `\\n` by default.\ndf2\n=\nspark\n.\nread\n.\ntext\n(\npath\n,\nlineSep\n=\n\"\n,\n\"\n)\ndf2\n.\nshow\n()\n# +-----------+\n# |      value|\n# +-----------+\n# |    Michael|\n# |   29\\nAndy|\n# | 30\\nJustin|\n# |       19\\n|\n# +-----------+\n# You can also use 'wholetext' option to read each input file as a single row.\ndf3\n=\nspark\n.\nread\n.\ntext\n(\npath\n,\nwholetext\n=\nTrue\n)\ndf3\n.\nshow\n()\n# +--------------------+\n# |               value|\n# +--------------------+\n# |Michael, 29\\nAndy...|\n# +--------------------+\n# \"output\" is a folder which contains multiple text files and a _SUCCESS file.\ndf1\n.\nwrite\n.\ncsv\n(\n\"\noutput\n\"\n)\n# You can specify the compression format using the 'compression' option.\ndf1\n.\nwrite\n.\ntext\n(\n\"\noutput_compressed\n\"\n,\ncompression\n=\n\"\ngzip\n\"\n)\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\n// A text dataset is pointed to by path.\n// The path can be either a single text file or a directory of text files\nval\npath\n=\n\"examples/src/main/resources/people.txt\"\nval\ndf1\n=\nspark\n.\nread\n.\ntext\n(\npath\n)\ndf1\n.\nshow\n()\n// +-----------+\n// |      value|\n// +-----------+\n// |Michael, 29|\n// |   Andy, 30|\n// | Justin, 19|\n// +-----------+\n// You can use 'lineSep' option to define the line separator.\n// The line separator handles all `\\r`, `\\r\\n` and `\\n` by default.\nval\ndf2\n=\nspark\n.\nread\n.\noption\n(\n\"lineSep\"\n,\n\",\"\n).\ntext\n(\npath\n)\ndf2\n.\nshow\n()\n// +-----------+\n// |      value|\n// +-----------+\n// |    Michael|\n// |   29\\nAndy|\n// | 30\\nJustin|\n// |       19\\n|\n// +-----------+\n// You can also use 'wholetext' option to read each input file as a single row.\nval\ndf3\n=\nspark\n.\nread\n.\noption\n(\n\"wholetext\"\n,\ntrue\n).\ntext\n(\npath\n)\ndf3\n.\nshow\n()\n//  +--------------------+\n//  |               value|\n//  +--------------------+\n//  |Michael, 29\\nAndy...|\n//  +--------------------+\n// \"output\" is a folder which contains multiple text files and a _SUCCESS file.\ndf1\n.\nwrite\n.\ntext\n(\n\"output\"\n)\n// You can specify the compression format using the 'compression' option.\ndf1\n.\nwrite\n.\noption\n(\n\"compression\"\n,\n\"gzip\"\n).\ntext\n(\n\"output_compressed\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\n// A text dataset is pointed to by path.\n// The path can be either a single text file or a directory of text files\nString\npath\n=\n\"examples/src/main/resources/people.txt\"\n;\nDataset\n<\nRow\n>\ndf1\n=\nspark\n.\nread\n().\ntext\n(\npath\n);\ndf1\n.\nshow\n();\n// +-----------+\n// |      value|\n// +-----------+\n// |Michael, 29|\n// |   Andy, 30|\n// | Justin, 19|\n// +-----------+\n// You can use 'lineSep' option to define the line separator.\n// The line separator handles all `\\r`, `\\r\\n` and `\\n` by default.\nDataset\n<\nRow\n>\ndf2\n=\nspark\n.\nread\n().\noption\n(\n\"lineSep\"\n,\n\",\"\n).\ntext\n(\npath\n);\ndf2\n.\nshow\n();\n// +-----------+\n// |      value|\n// +-----------+\n// |    Michael|\n// |   29\\nAndy|\n// | 30\\nJustin|\n// |       19\\n|\n// +-----------+\n// You can also use 'wholetext' option to read each input file as a single row.\nDataset\n<\nRow\n>\ndf3\n=\nspark\n.\nread\n().\noption\n(\n\"wholetext\"\n,\n\"true\"\n).\ntext\n(\npath\n);\ndf3\n.\nshow\n();\n//  +--------------------+\n//  |               value|\n//  +--------------------+\n//  |Michael, 29\\nAndy...|\n//  +--------------------+\n// \"output\" is a folder which contains multiple text files and a _SUCCESS file.\ndf1\n.\nwrite\n().\ntext\n(\n\"output\"\n);\n// You can specify the compression format using the 'compression' option.\ndf1\n.\nwrite\n().\noption\n(\n\"compression\"\n,\n\"gzip\"\n).\ntext\n(\n\"output_compressed\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\nData Source Option\nData source options of text can be set via:\nthe\n.option\n/\n.options\nmethods of\nDataFrameReader\nDataFrameWriter\nDataStreamReader\nDataStreamWriter\nOPTIONS\nclause at\nCREATE TABLE USING DATA_SOURCE\nProperty Name\nDefault\nMeaning\nScope\nwholetext\nfalse\nIf true, read each file from input path(s) as a single row.\nread\nlineSep\n\\r\n,\n\\r\\n\n,\n\\n\n(for reading),\n\\n\n(for writing)\nDefines the line separator that should be used for reading or writing.\nread/write\ncompression\n(none)\nCompression codec to use when saving to file. This can be one of the known case-insensitive shorten names (none, bzip2, gzip, lz4, snappy and deflate).\nwrite\nOther generic options can be found in\nGeneric File Source Options\n."}
{"url": "https://spark.apache.org/docs/latest/sql-data-sources-troubleshooting.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTroubleshooting\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nTroubleshooting\nThe JDBC driver class must be visible to the primordial class loader on the client session and on all executors. This is because Java’s DriverManager class does a security check that results in it ignoring all drivers not visible to the primordial class loader when one goes to open a connection. One convenient way to do this is to modify compute_classpath.sh on all worker nodes to include your driver JARs.\nSome databases, such as H2, convert all names to upper case. You’ll need to use upper case to refer to those names in Spark SQL.\nUsers can specify vendor-specific JDBC connection properties in the data source options to do special treatment. For example,\nspark.read.format(\"jdbc\").option(\"url\", oracleJdbcUrl).option(\"oracle.jdbc.mapDateToTimestamp\", \"false\")\n.\noracle.jdbc.mapDateToTimestamp\ndefaults to true, users often need to disable this flag to avoid Oracle date being resolved as timestamp."}
{"url": "https://spark.apache.org/docs/latest/sql-error-conditions.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nError Conditions\nThis is a list of error states and conditions that may be returned by Spark SQL.\nError State / SQLSTATE\nError Condition & Sub-Condition\nMessage\n07001\n#\nALL\n_PARAMETERS\n_MUST\n_BE\n_NAMED\nUsing name parameterized queries requires all parameters to be named. Parameters missing names:\n<exprs>\n.\n07501\n#\nINVALID\n_STATEMENT\n_FOR\n_EXECUTE\n_INTO\nThe INTO clause of EXECUTE IMMEDIATE is only valid for queries but the given statement is not a query:\n<sqlString>\n.\n07501\n#\nNESTED\n_EXECUTE\n_IMMEDIATE\nNested EXECUTE IMMEDIATE commands are not allowed. Please ensure that the SQL query provided (\n<sqlString>\n) does not contain another EXECUTE IMMEDIATE command.\n07501\n#\nSQL\n_SCRIPT\n_IN\n_EXECUTE\n_IMMEDIATE\nSQL Scripts in EXECUTE IMMEDIATE commands are not allowed. Please ensure that the SQL query provided (\n<sqlString>\n) is not SQL Script. Make sure the sql_string is a well-formed SQL statement and does not contain BEGIN and END.\n0A000\n#\nCANNOT\n_INVOKE\n_IN\n_TRANSFORMATIONS\nDataset transformations and actions can only be invoked by the driver, not inside of other Dataset transformations; for example, dataset1.map(x => dataset2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the dataset1.map transformation. For more information, see SPARK-28702.\n0A000\n#\nCANNOT\n_UPDATE\n_FIELD\nCannot update\n<table>\nfield\n<fieldName>\ntype:\n#\nARRAY\n_TYPE\nUpdate the element by updating\n<fieldName>\n.element.\n#\nINTERVAL\n_TYPE\nUpdate an interval by updating its fields.\n#\nMAP\n_TYPE\nUpdate a map by updating\n<fieldName>\n.key or\n<fieldName>\n.value.\n#\nSTRUCT\n_TYPE\nUpdate a struct by updating its fields.\n#\nUSER\n_DEFINED\n_TYPE\nUpdate a UserDefinedType[\n<udtSql>\n] by updating its fields.\n0A000\n#\nCLASS\n_UNSUPPORTED\n_BY\n_MAP\n_OBJECTS\nMapObjects\ndoes not support the class\n<cls>\nas resulting collection.\n0A000\n#\nCOLUMN\n_ARRAY\n_ELEMENT\n_TYPE\n_MISMATCH\nSome values in field\n<pos>\nare incompatible with the column array type. Expected type\n<type>\n.\n0A000\n#\nCONCURRENT\n_QUERY\nAnother instance of this query was just started by a concurrent session.\n0A000\n#\nCORRUPTED\n_CATALOG\n_FUNCTION\nCannot convert the catalog function '\n<identifier>\n' into a SQL function due to corrupted function information in catalog. If the function is not a SQL function, please make sure the class name '\n<className>\n' is loadable.\n0A000\n#\nCREATE\n_PERMANENT\n_VIEW\n_WITHOUT\n_ALIAS\nNot allowed to create the permanent view\n<name>\nwithout explicitly assigning an alias for the expression\n<attr>\n.\n0A000\n#\nDESCRIBE\n_JSON\n_NOT\n_EXTENDED\nDESCRIBE TABLE ... AS JSON only supported when [EXTENDED|FORMATTED] is specified. For example: DESCRIBE EXTENDED\n<tableName>\nAS JSON is supported but DESCRIBE\n<tableName>\nAS JSON is not.\n0A000\n#\nDISTINCT\n_WINDOW\n_FUNCTION\n_UNSUPPORTED\nDistinct window functions are not supported:\n<windowExpr>\n.\n0A000\n#\nEMPTY\n_SCHEMA\n_NOT\n_SUPPORTED\n_FOR\n_DATASOURCE\nThe\n<format>\ndatasource does not support writing empty or nested empty schemas. Please make sure the data schema has at least one or more column(s).\n0A000\n#\nINVALID\n_PANDAS\n_UDF\n_PLACEMENT\nThe group aggregate pandas UDF\n<functionList>\ncannot be invoked together with as other, non-pandas aggregate functions.\n0A000\n#\nINVALID\n_PARTITION\n_COLUMN\n_DATA\n_TYPE\nCannot use\n<type>\nfor partition column.\n0A000\n#\nMULTI\n_UDF\n_INTERFACE\n_ERROR\nNot allowed to implement multiple UDF interfaces, UDF class\n<className>\n.\n0A000\n#\nNAMED\n_PARAMETER\n_SUPPORT\n_DISABLED\nCannot call function\n<functionName>\nbecause named argument references are not enabled here. In this case, the named argument reference was\n<argument>\n. Set \"spark.sql.allowNamedFunctionArguments\" to \"true\" to turn on feature.\n0A000\n#\nNEGATIVE\n_SCALE\n_DISALLOWED\nNegative scale is not allowed: '\n<scale>\n'. Set the config\n<sqlConf>\nto \"true\" to allow it.\n0A000\n#\nNOT\n_SUPPORTED\n_CHANGE\n_COLUMN\nALTER TABLE ALTER/CHANGE COLUMN is not supported for changing\n<table>\n's column\n<originName>\nwith type\n<originType>\nto\n<newName>\nwith type\n<newType>\n.\n0A000\n#\nNOT\n_SUPPORTED\n_CHANGE\n_SAME\n_COLUMN\nALTER TABLE ALTER/CHANGE COLUMN is not supported for changing\n<table>\n's column\n<fieldName>\nincluding its nested fields multiple times in the same command.\n0A000\n#\nNOT\n_SUPPORTED\n_COMMAND\n_FOR\n_V2\n_TABLE\n<cmd>\nis not supported for v2 tables.\n0A000\n#\nNOT\n_SUPPORTED\n_COMMAND\n_WITHOUT\n_HIVE\n_SUPPORT\n<cmd>\nis not supported, if you want to enable it, please set \"spark.sql.catalogImplementation\" to \"hive\".\n0A000\n#\nNOT\n_SUPPORTED\n_IN\n_JDBC\n_CATALOG\nNot supported command in JDBC catalog:\n#\nCOMMAND\n<cmd>\n#\nCOMMAND\n_WITH\n_PROPERTY\n<cmd>\nwith property\n<property>\n.\n0A000\n#\nPIPE\n_OPERATOR\n_AGGREGATE\n_EXPRESSION\n_CONTAINS\n_NO\n_AGGREGATE\n_FUNCTION\nNon-grouping expression\n<expr>\nis provided as an argument to the |> AGGREGATE pipe operator but does not contain any aggregate function; please update it to include an aggregate function and then retry the query again.\n0A000\n#\nPIPE\n_OPERATOR\n_CONTAINS\n_AGGREGATE\n_FUNCTION\nAggregate function\n<expr>\nis not allowed when using the pipe operator |>\n<clause>\nclause; please use the pipe operator |> AGGREGATE clause instead.\n0A000\n#\nSCALAR\n_SUBQUERY\n_IS\n_IN\n_GROUP\n_BY\n_OR\n_AGGREGATE\n_FUNCTION\nThe correlated scalar subquery '\n<sqlExpr>\n' is neither present in GROUP BY, nor in an aggregate function. Add it to GROUP BY using ordinal position or wrap it in\nfirst()\n(or\nfirst_value\n) if you don't care which value you get.\n0A000\n#\nSTAR\n_GROUP\n_BY\n_POS\nStar (*) is not allowed in a select list when GROUP BY an ordinal position is used.\n0A000\n#\nTABLE\n_VALUED\n_ARGUMENTS\n_NOT\n_YET\n_IMPLEMENTED\n_FOR\n_SQL\n_FUNCTIONS\nCannot\n<action>\nSQL user-defined function\n<functionName>\nwith TABLE arguments because this functionality is not yet implemented.\n0A000\n#\nUNSUPPORTED\n_ADD\n_FILE\nDon't support add file.\n#\nDIRECTORY\nThe file\n<path>\nis a directory, consider to set \"spark.sql.legacy.addSingleFileInAddFile\" to \"false\".\n#\nLOCAL\n_DIRECTORY\nThe local directory\n<path>\nis not supported in a non-local master mode.\n0A000\n#\nUNSUPPORTED\n_ARROWTYPE\nUnsupported arrow type\n<typeName>\n.\n0A000\n#\nUNSUPPORTED\n_CALL\nCannot call the method \"\n<methodName>\n\" of the class \"\n<className>\n\".\n#\nFIELD\n_INDEX\nThe row shall have a schema to get an index of the field\n<fieldName>\n.\n#\nWITHOUT\n_SUGGESTION\n0A000\n#\nUNSUPPORTED\n_CHAR\n_OR\n_VARCHAR\n_AS\n_STRING\nThe char/varchar type can't be used in the table schema. If you want Spark treat them as string type as same as Spark 3.0 and earlier, please set \"spark.sql.legacy.charVarcharAsString\" to \"true\".\n0A000\n#\nUNSUPPORTED\n_COLLATION\nCollation\n<collationName>\nis not supported for:\n#\nFOR\n_FUNCTION\nfunction\n<functionName>\n. Please try to use a different collation.\n0A000\n#\nUNSUPPORTED\n_CONNECT\n_FEATURE\nFeature is not supported in Spark Connect:\n#\nDATASET\n_QUERY\n_EXECUTION\nAccess to the Dataset Query Execution. This is server side developer API.\n#\nRDD\nResilient Distributed Datasets (RDDs).\n#\nSESSION\n_BASE\n_RELATION\n_TO\n_DATAFRAME\nInvoking SparkSession 'baseRelationToDataFrame'. This is server side developer API\n#\nSESSION\n_EXPERIMENTAL\n_METHODS\nAccess to SparkSession Experimental (methods). This is server side developer API\n#\nSESSION\n_LISTENER\n_MANAGER\nAccess to the SparkSession Listener Manager. This is server side developer API\n#\nSESSION\n_SESSION\n_STATE\nAccess to the SparkSession Session State. This is server side developer API.\n#\nSESSION\n_SHARED\n_STATE\nAccess to the SparkSession Shared State. This is server side developer API.\n#\nSESSION\n_SPARK\n_CONTEXT\nAccess to the SparkContext.\n0A000\n#\nUNSUPPORTED\n_DATASOURCE\n_FOR\n_DIRECT\n_QUERY\nUnsupported data source type for direct query on files:\n<dataSourceType>\n0A000\n#\nUNSUPPORTED\n_DATATYPE\nUnsupported data type\n<typeName>\n.\n0A000\n#\nUNSUPPORTED\n_DATA\n_SOURCE\n_SAVE\n_MODE\nThe data source \"\n<source>\n\" cannot be written in the\n<createMode>\nmode. Please use either the \"Append\" or \"Overwrite\" mode instead.\n0A000\n#\nUNSUPPORTED\n_DATA\n_TYPE\n_FOR\n_DATASOURCE\nThe\n<format>\ndatasource doesn't support the column\n<columnName>\nof the type\n<columnType>\n.\n0A000\n#\nUNSUPPORTED\n_DATA\n_TYPE\n_FOR\n_ENCODER\nCannot create encoder for\n<dataType>\n. Please use a different output data type for your UDF or DataFrame.\n0A000\n#\nUNSUPPORTED\n_DEFAULT\n_VALUE\nDEFAULT column values is not supported.\n#\nWITHOUT\n_SUGGESTION\n#\nWITH\n_SUGGESTION\nEnable it by setting \"spark.sql.defaultColumn.enabled\" to \"true\".\n0A000\n#\nUNSUPPORTED\n_DESERIALIZER\nThe deserializer is not supported:\n#\nDATA\n_TYPE\n_MISMATCH\nneed a(n)\n<desiredType>\nfield but got\n<dataType>\n.\n#\nFIELD\n_NUMBER\n_MISMATCH\ntry to map\n<schema>\nto Tuple\n<ordinal>\n, but failed as the number of fields does not line up.\n0A000\n#\nUNSUPPORTED\n_FEATURE\nThe feature is not supported:\n#\nAES\n_MODE\nAES-\n<mode>\nwith the padding\n<padding>\nby the\n<functionName>\nfunction.\n#\nAES\n_MODE\n_AAD\n<functionName>\nwith AES-\n<mode>\ndoes not support additional authenticate data (AAD).\n#\nAES\n_MODE\n_IV\n<functionName>\nwith AES-\n<mode>\ndoes not support initialization vectors (IVs).\n#\nALTER\n_TABLE\n_SERDE\n_FOR\n_DATASOURCE\n_TABLE\nALTER TABLE SET SERDE is not supported for table\n<tableName>\ncreated with the datasource API. Consider using an external Hive table or updating the table properties with compatible options for your table format.\n#\nANALYZE\n_UNCACHED\n_TEMP\n_VIEW\nThe ANALYZE TABLE FOR COLUMNS command can operate on temporary views that have been cached already. Consider to cache the view\n<viewName>\n.\n#\nANALYZE\n_UNSUPPORTED\n_COLUMN\n_TYPE\nThe ANALYZE TABLE FOR COLUMNS command does not support the type\n<columnType>\nof the column\n<columnName>\nin the table\n<tableName>\n.\n#\nANALYZE\n_VIEW\nThe ANALYZE TABLE command does not support views.\n#\nCATALOG\n_OPERATION\nCatalog\n<catalogName>\ndoes not support\n<operation>\n.\n#\nCLAUSE\n_WITH\n_PIPE\n_OPERATORS\nThe SQL pipe operator syntax using |> does not support\n<clauses>\n.\n#\nCOLLATIONS\n_IN\n_MAP\n_KEYS\nCollated strings for keys of maps\n#\nCOMBINATION\n_QUERY\n_RESULT\n_CLAUSES\nCombination of ORDER BY/SORT BY/DISTRIBUTE BY/CLUSTER BY.\n#\nCOMMENT\n_NAMESPACE\nAttach a comment to the namespace\n<namespace>\n.\n#\nCONTINUE\n_EXCEPTION\n_HANDLER\nCONTINUE exception handler is not supported. Use EXIT handler.\n#\nDESC\n_TABLE\n_COLUMN\n_JSON\nDESC TABLE COLUMN AS JSON not supported for individual columns.\n#\nDESC\n_TABLE\n_COLUMN\n_PARTITION\nDESC TABLE COLUMN for a specific partition.\n#\nDROP\n_DATABASE\nDrop the default database\n<database>\n.\n#\nDROP\n_NAMESPACE\nDrop the namespace\n<namespace>\n.\n#\nHIVE\n_TABLE\n_TYPE\nThe\n<tableName>\nis hive\n<tableType>\n.\n#\nHIVE\n_WITH\n_ANSI\n_INTERVALS\nHive table\n<tableName>\nwith ANSI intervals.\n#\nINSERT\n_PARTITION\n_SPEC\n_IF\n_NOT\n_EXISTS\nINSERT INTO\n<tableName>\nwith IF NOT EXISTS in the PARTITION spec.\n#\nLAMBDA\n_FUNCTION\n_WITH\n_PYTHON\n_UDF\nLambda function with Python UDF\n<funcName>\nin a higher order function.\n#\nLATERAL\n_COLUMN\n_ALIAS\n_IN\n_AGGREGATE\n_FUNC\nReferencing a lateral column alias\n<lca>\nin the aggregate function\n<aggFunc>\n.\n#\nLATERAL\n_COLUMN\n_ALIAS\n_IN\n_AGGREGATE\n_WITH\n_WINDOW\n_AND\n_HAVING\nReferencing lateral column alias\n<lca>\nin the aggregate query both with window expressions and with having clause. Please rewrite the aggregate query by removing the having clause or removing lateral alias reference in the SELECT list.\n#\nLATERAL\n_COLUMN\n_ALIAS\n_IN\n_GENERATOR\nReferencing a lateral column alias\n<lca>\nin generator expression\n<generatorExpr>\n.\n#\nLATERAL\n_COLUMN\n_ALIAS\n_IN\n_GROUP\n_BY\nReferencing a lateral column alias via GROUP BY alias/ALL is not supported yet.\n#\nLATERAL\n_COLUMN\n_ALIAS\n_IN\n_WINDOW\nReferencing a lateral column alias\n<lca>\nin window expression\n<windowExpr>\n.\n#\nLATERAL\n_JOIN\n_USING\nJOIN USING with LATERAL correlation.\n#\nLITERAL\n_TYPE\nLiteral for '\n<value>\n' of\n<type>\n.\n#\nMULTIPLE\n_BUCKET\n_TRANSFORMS\nMultiple bucket TRANSFORMs.\n#\nMULTI\n_ACTION\n_ALTER\nThe target JDBC server hosting table\n<tableName>\ndoes not support ALTER TABLE with multiple actions. Split the ALTER TABLE up into individual actions to avoid this error.\n#\nOBJECT\n_LEVEL\n_COLLATIONS\nDefault collation for the specified object.\n#\nORC\n_TYPE\n_CAST\nUnable to convert\n<orcType>\nof Orc to data type\n<toType>\n.\n#\nOVERWRITE\n_BY\n_SUBQUERY\nINSERT OVERWRITE with a subquery condition.\n#\nPANDAS\n_UDAF\n_IN\n_PIVOT\nPandas user defined aggregate function in the PIVOT clause.\n#\nPARAMETER\n_MARKER\n_IN\n_UNEXPECTED\n_STATEMENT\nParameter markers are not allowed in\n<statement>\n.\n#\nPARTITION\n_BY\n_VARIANT\nCannot use VARIANT producing expressions to partition a DataFrame, but the type of expression\n<expr>\nis\n<dataType>\n.\n#\nPARTITION\n_WITH\n_NESTED\n_COLUMN\n_IS\n_UNSUPPORTED\nInvalid partitioning:\n<cols>\nis missing or is in a map or array.\n#\nPIPE\n_OPERATOR\n_AGGREGATE\n_UNSUPPORTED\n_CASE\nThe SQL pipe operator syntax with aggregation (using |> AGGREGATE) does not support\n<case>\n.\n#\nPIVOT\n_AFTER\n_GROUP\n_BY\nPIVOT clause following a GROUP BY clause. Consider pushing the GROUP BY into a subquery.\n#\nPIVOT\n_TYPE\nPivoting by the value '\n<value>\n' of the column data type\n<type>\n.\n#\nPURGE\n_PARTITION\nPartition purge.\n#\nPURGE\n_TABLE\nPurge table.\n#\nPYTHON\n_UDF\n_IN\n_ON\n_CLAUSE\nPython UDF in the ON clause of a\n<joinType>\nJOIN. In case of an INNER JOIN consider rewriting to a CROSS JOIN with a WHERE clause.\n#\nQUERY\n_ONLY\n_CORRUPT\n_RECORD\n_COLUMN\nQueries from raw JSON/CSV/XML files are disallowed when the referenced columns only include the internal corrupt record column (named\n_corrupt_record\nby default). For example:\nspark.read.schema(schema).json(file).filter($\"_corrupt_record\".isNotNull).count()\nand\nspark.read.schema(schema).json(file).select(\"_corrupt_record\").show()\n. Instead, you can cache or save the parsed results and then send the same query. For example,\nval df = spark.read.schema(schema).json(file).cache()\nand then\ndf.filter($\"_corrupt_record\".isNotNull).count()\n.\n#\nREMOVE\n_NAMESPACE\n_COMMENT\nRemove a comment from the namespace\n<namespace>\n.\n#\nREPLACE\n_NESTED\n_COLUMN\nThe replace function does not support nested column\n<colName>\n.\n#\nSET\n_NAMESPACE\n_PROPERTY\n<property>\nis a reserved namespace property,\n<msg>\n.\n#\nSET\n_OPERATION\n_ON\n_MAP\n_TYPE\nCannot have MAP type columns in DataFrame which calls set operations (INTERSECT, EXCEPT, etc.), but the type of column\n<colName>\nis\n<dataType>\n.\n#\nSET\n_OPERATION\n_ON\n_VARIANT\n_TYPE\nCannot have VARIANT type columns in DataFrame which calls set operations (INTERSECT, EXCEPT, etc.), but the type of column\n<colName>\nis\n<dataType>\n.\n#\nSET\n_PROPERTIES\n_AND\n_DBPROPERTIES\nset PROPERTIES and DBPROPERTIES at the same time.\n#\nSET\n_TABLE\n_PROPERTY\n<property>\nis a reserved table property,\n<msg>\n.\n#\nSET\n_VARIABLE\n_USING\n_SET\n<variableName>\nis a VARIABLE and cannot be updated using the SET statement. Use SET VARIABLE\n<variableName>\n= ... instead.\n#\nSQL\n_SCRIPTING\nSQL Scripting is under development and not all features are supported. SQL Scripting enables users to write procedural SQL including control flow and error handling. To enable existing features set\n<sqlScriptingEnabled>\nto\ntrue\n.\n#\nSQL\n_SCRIPTING\n_DROP\n_TEMPORARY\n_VARIABLE\nDROP TEMPORARY VARIABLE is not supported within SQL scripts. To bypass this, use\nEXECUTE IMMEDIATE 'DROP TEMPORARY VARIABLE ...'\n.\n#\nSQL\n_SCRIPTING\n_WITH\n_POSITIONAL\n_PARAMETERS\nPositional parameters are not supported with SQL Scripting.\n#\nSTATE\n_STORE\n_MULTIPLE\n_COLUMN\n_FAMILIES\nCreating multiple column families with\n<stateStoreProvider>\nis not supported.\n#\nSTATE\n_STORE\n_REMOVING\n_COLUMN\n_FAMILIES\nRemoving column families with\n<stateStoreProvider>\nis not supported.\n#\nSTATE\n_STORE\n_TTL\nState TTL with\n<stateStoreProvider>\nis not supported. Please use RocksDBStateStoreProvider.\n#\nTABLE\n_OPERATION\nTable\n<tableName>\ndoes not support\n<operation>\n. Please check the current catalog and namespace to make sure the qualified table name is expected, and also check the catalog implementation which is configured by \"spark.sql.catalog\".\n#\nTEMPORARY\n_VIEW\n_WITH\n_SCHEMA\n_BINDING\n_MODE\nTemporary views cannot be created with the WITH SCHEMA clause. Recreate the temporary view when the underlying schema changes, or use a persisted view.\n#\nTIME\n_TRAVEL\nTime travel on the relation:\n<relationId>\n.\n#\nTOO\n_MANY\n_TYPE\n_ARGUMENTS\n_FOR\n_UDF\n_CLASS\nUDF class with\n<num>\ntype arguments.\n#\nTRANSFORM\n_DISTINCT\n_ALL\nTRANSFORM with the DISTINCT/ALL clause.\n#\nTRANSFORM\n_NON\n_HIVE\nTRANSFORM with SERDE is only supported in hive mode.\n#\nTRIM\n_COLLATION\nTRIM specifier in the collation.\n#\nUPDATE\n_COLUMN\n_NULLABILITY\nUpdate column nullability for MySQL and MS SQL Server.\n#\nWRITE\n_FOR\n_BINARY\n_SOURCE\nWrite for the binary file data source.\n0A000\n#\nUNSUPPORTED\n_JOIN\n_TYPE\nUnsupported join type '\n<typ>\n'. Supported join types include:\n<supported>\n.\n0A000\n#\nUNSUPPORTED\n_PARTITION\n_TRANSFORM\nUnsupported partition transform:\n<transform>\n. The supported transforms are\nidentity\n,\nbucket\n, and\nclusterBy\n. Ensure your transform expression uses one of these.\n0A000\n#\nUNSUPPORTED\n_SAVE\n_MODE\nThe save mode\n<saveMode>\nis not supported for:\n#\nEXISTENT\n_PATH\nan existent path.\n#\nNON\n_EXISTENT\n_PATH\na non-existent path.\n0A000\n#\nUNSUPPORTED\n_SHOW\n_CREATE\n_TABLE\nUnsupported a SHOW CREATE TABLE command.\n#\nON\n_DATA\n_SOURCE\n_TABLE\n_WITH\n_AS\n_SERDE\nThe table\n<tableName>\nis a Spark data source table. Please use SHOW CREATE TABLE without AS SERDE instead.\n#\nON\n_TEMPORARY\n_VIEW\nThe command is not supported on a temporary view\n<tableName>\n.\n#\nON\n_TRANSACTIONAL\n_HIVE\n_TABLE\nFailed to execute the command against transactional Hive table\n<tableName>\n. Please use SHOW CREATE TABLE\n<tableName>\nAS SERDE to show Hive DDL instead.\n#\nWITH\n_UNSUPPORTED\n_FEATURE\nFailed to execute the command against table/view\n<tableName>\nwhich is created by Hive and uses the following unsupported features\n<unsupportedFeatures>\n#\nWITH\n_UNSUPPORTED\n_SERDE\n_CONFIGURATION\nFailed to execute the command against the table\n<tableName>\nwhich is created by Hive and uses the following unsupported serde configuration\n<configs>\nPlease use SHOW CREATE TABLE\n<tableName>\nAS SERDE to show Hive DDL instead.\n0A000\n#\nUNSUPPORTED\n_SINGLE\n_PASS\n_ANALYZER\n_FEATURE\nThe single-pass analyzer cannot process this query or command because it does not yet support\n<feature>\n.\n0A000\n#\nUNSUPPORTED\n_SQL\n_UDF\n_USAGE\nUsing SQL function\n<functionName>\nin\n<nodeName>\nis not supported.\n0A000\n#\nUNSUPPORTED\n_STREAMING\n_OPERATOR\n_WITHOUT\n_WATERMARK\n<outputMode>\noutput mode not supported for\n<statefulOperator>\non streaming DataFrames/DataSets without watermark.\n0A000\n#\nUNSUPPORTED\n_SUBQUERY\n_EXPRESSION\n_CATEGORY\nUnsupported subquery expression:\n#\nACCESSING\n_OUTER\n_QUERY\n_COLUMN\n_IS\n_NOT\n_ALLOWED\nAccessing outer query column is not allowed in this location:\n<treeNode>\n#\nAGGREGATE\n_FUNCTION\n_MIXED\n_OUTER\n_LOCAL\n_REFERENCES\nFound an aggregate function in a correlated predicate that has both outer and local references, which is not supported:\n<function>\n.\n#\nCORRELATED\n_COLUMN\n_IS\n_NOT\n_ALLOWED\n_IN\n_PREDICATE\nCorrelated column is not allowed in predicate:\n<treeNode>\n#\nCORRELATED\n_COLUMN\n_NOT\n_FOUND\nA correlated outer name reference within a subquery expression body was not found in the enclosing query:\n<value>\n.\n#\nCORRELATED\n_REFERENCE\nExpressions referencing the outer query are not supported outside of WHERE/HAVING clauses:\n<sqlExprs>\n.\n#\nHIGHER\n_ORDER\n_FUNCTION\nSubquery expressions are not supported within higher-order functions. Please remove all subquery expressions from higher-order functions and then try the query again.\n#\nLATERAL\n_JOIN\n_CONDITION\n_NON\n_DETERMINISTIC\nLateral join condition cannot be non-deterministic:\n<condition>\n.\n#\nMUST\n_AGGREGATE\n_CORRELATED\n_SCALAR\n_SUBQUERY\nCorrelated scalar subqueries must be aggregated to return at most one row.\n#\nNON\n_CORRELATED\n_COLUMNS\n_IN\n_GROUP\n_BY\nA GROUP BY clause in a scalar correlated subquery cannot contain non-correlated columns:\n<value>\n.\n#\nNON\n_DETERMINISTIC\n_LATERAL\n_SUBQUERIES\nNon-deterministic lateral subqueries are not supported when joining with outer relations that produce more than one row:\n<treeNode>\n#\nSCALAR\n_SUBQUERY\n_IN\n_VALUES\nScalar subqueries in the VALUES clause.\n#\nUNSUPPORTED\n_CORRELATED\n_EXPRESSION\n_IN\n_JOIN\n_CONDITION\nCorrelated subqueries in the join predicate cannot reference both join inputs:\n<subqueryExpression>\n#\nUNSUPPORTED\n_CORRELATED\n_REFERENCE\n_DATA\n_TYPE\nCorrelated column reference '\n<expr>\n' cannot be\n<dataType>\ntype.\n#\nUNSUPPORTED\n_CORRELATED\n_SCALAR\n_SUBQUERY\nCorrelated scalar subqueries can only be used in filters, aggregations, projections, and UPDATE/MERGE/DELETE commands:\n<treeNode>\n#\nUNSUPPORTED\n_IN\n_EXISTS\n_SUBQUERY\nIN/EXISTS predicate subqueries can only be used in filters, joins, aggregations, window functions, projections, and UPDATE/MERGE/DELETE commands:\n<treeNode>\n#\nUNSUPPORTED\n_TABLE\n_ARGUMENT\nTable arguments are used in a function where they are not supported:\n<treeNode>\n0A000\n#\nUNSUPPORTED\n_TYPED\n_LITERAL\nLiterals of the type\n<unsupportedType>\nare not supported. Supported types are\n<supportedTypes>\n.\n0AKD0\n#\nCANNOT\n_RENAME\n_ACROSS\n_SCHEMA\nRenaming a\n<type>\nacross schemas is not allowed.\n21000\n#\nBOOLEAN\n_STATEMENT\n_WITH\n_EMPTY\n_ROW\nBoolean statement\n<invalidStatement>\nis invalid. Expected single row with a value of the BOOLEAN type, but got an empty row.\n21000\n#\nROW\n_SUBQUERY\n_TOO\n_MANY\n_ROWS\nMore than one row returned by a subquery used as a row.\n21000\n#\nSCALAR\n_SUBQUERY\n_TOO\n_MANY\n_ROWS\nMore than one row returned by a subquery used as an expression.\n21S01\n#\nCREATE\n_VIEW\n_COLUMN\n_ARITY\n_MISMATCH\nCannot create view\n<viewName>\n, the reason is\n#\nNOT\n_ENOUGH\n_DATA\n_COLUMNS\nnot enough data columns: View columns:\n<viewColumns>\n. Data columns:\n<dataColumns>\n.\n#\nTOO\n_MANY\n_DATA\n_COLUMNS\ntoo many data columns: View columns:\n<viewColumns>\n. Data columns:\n<dataColumns>\n.\n21S01\n#\nINSERT\n_COLUMN\n_ARITY\n_MISMATCH\nCannot write to\n<tableName>\n, the reason is\n#\nNOT\n_ENOUGH\n_DATA\n_COLUMNS\nnot enough data columns: Table columns:\n<tableColumns>\n. Data columns:\n<dataColumns>\n.\n#\nTOO\n_MANY\n_DATA\n_COLUMNS\ntoo many data columns: Table columns:\n<tableColumns>\n. Data columns:\n<dataColumns>\n.\n21S01\n#\nINSERT\n_PARTITION\n_COLUMN\n_ARITY\n_MISMATCH\nCannot write to '\n<tableName>\n',\n<reason>\n: Table columns:\n<tableColumns>\n. Partition columns with static values:\n<staticPartCols>\n. Data columns:\n<dataColumns>\n.\n22000\n#\nHLL\n_UNION\n_DIFFERENT\n_LG\n_K\nSketches have different\nlgConfigK\nvalues:\n<left>\nand\n<right>\n. Set the\nallowDifferentLgConfigK\nparameter to true to call\n<function>\nwith different\nlgConfigK\nvalues.\n22000\n#\nMALFORMED\n_CHARACTER\n_CODING\nInvalid value found when performing\n<function>\nwith\n<charset>\n22003\n#\nARITHMETIC\n_OVERFLOW\n<message>\n.\n<alternative>\nIf necessary set\n<config>\nto \"false\" to bypass this error.\n22003\n#\nBINARY\n_ARITHMETIC\n_OVERFLOW\n<value1>\n<symbol>\n<value2>\ncaused overflow. Use\n<functionName>\nto ignore overflow problem and return NULL.\n22003\n#\nCAST\n_OVERFLOW\nThe value\n<value>\nof the type\n<sourceType>\ncannot be cast to\n<targetType>\ndue to an overflow. Use\ntry_cast\nto tolerate overflow and return NULL instead.\n22003\n#\nCAST\n_OVERFLOW\n_IN\n_TABLE\n_INSERT\nFail to assign a value of\n<sourceType>\ntype to the\n<targetType>\ntype column or variable\n<columnName>\ndue to an overflow. Use\ntry_cast\non the input value to tolerate overflow and return NULL instead.\n22003\n#\nCOLUMN\n_ORDINAL\n_OUT\n_OF\n_BOUNDS\nColumn ordinal out of bounds. The number of columns in the table is\n<attributesLength>\n, but the column ordinal is\n<ordinal>\n. Attributes are the following:\n<attributes>\n.\n22003\n#\nDECIMAL\n_PRECISION\n_EXCEEDS\n_MAX\n_PRECISION\nDecimal precision\n<precision>\nexceeds max precision\n<maxPrecision>\n.\n22003\n#\nINCORRECT\n_RAMP\n_UP\n_RATE\nMax offset with\n<rowsPerSecond>\nrowsPerSecond is\n<maxSeconds>\n, but 'rampUpTimeSeconds' is\n<rampUpTimeSeconds>\n.\n22003\n#\nINVALID\n_ARRAY\n_INDEX\nThe index\n<indexValue>\nis out of bounds. The array has\n<arraySize>\nelements. Use the SQL function\nget()\nto tolerate accessing element at invalid index and return NULL instead.\n22003\n#\nINVALID\n_ARRAY\n_INDEX\n_IN\n_ELEMENT\n_AT\nThe index\n<indexValue>\nis out of bounds. The array has\n<arraySize>\nelements. Use\ntry_element_at\nto tolerate accessing element at invalid index and return NULL instead.\n22003\n#\nINVALID\n_BITMAP\n_POSITION\nThe 0-indexed bitmap position\n<bitPosition>\nis out of bounds. The bitmap has\n<bitmapNumBits>\nbits (\n<bitmapNumBytes>\nbytes).\n22003\n#\nINVALID\n_BOUNDARY\nThe boundary\n<boundary>\nis invalid:\n<invalidValue>\n.\n#\nEND\nExpected the value is '0', '\n<longMaxValue>\n', '[\n<intMinValue>\n,\n<intMaxValue>\n]'.\n#\nSTART\nExpected the value is '0', '\n<longMinValue>\n', '[\n<intMinValue>\n,\n<intMaxValue>\n]'.\n22003\n#\nINVALID\n_INDEX\n_OF\n_ZERO\nThe index 0 is invalid. An index shall be either\n< 0 or >\n0 (the first element has index 1).\n22003\n#\nINVALID\n_NUMERIC\n_LITERAL\n_RANGE\nNumeric literal\n<rawStrippedQualifier>\nis outside the valid range for\n<typeName>\nwith minimum value of\n<minValue>\nand maximum value of\n<maxValue>\n. Please adjust the value accordingly.\n22003\n#\nNEGATIVE\n_VALUES\n_IN\n_FREQUENCY\n_EXPRESSION\nFound the negative value in\n<frequencyExpression>\n:\n<negativeValue>\n, but expected a positive integral value.\n22003\n#\nNUMERIC\n_OUT\n_OF\n_SUPPORTED\n_RANGE\nThe value\n<value>\ncannot be interpreted as a numeric since it has more than 38 digits.\n22003\n#\nNUMERIC\n_VALUE\n_OUT\n_OF\n_RANGE\n#\nWITHOUT\n_SUGGESTION\nThe\n<roundedValue>\nrounded half up from\n<originalValue>\ncannot be represented as Decimal(\n<precision>\n,\n<scale>\n).\n#\nWITH\n_SUGGESTION\n<value>\ncannot be represented as Decimal(\n<precision>\n,\n<scale>\n). If necessary set\n<config>\nto \"false\" to bypass this error, and return NULL instead.\n22003\n#\nSUM\n_OF\n_LIMIT\n_AND\n_OFFSET\n_EXCEEDS\n_MAX\n_INT\nThe sum of the LIMIT clause and the OFFSET clause must not be greater than the maximum 32-bit integer value (2,147,483,647) but found limit =\n<limit>\n, offset =\n<offset>\n.\n22004\n#\nCOMPARATOR\n_RETURNS\n_NULL\nThe comparator has returned a NULL for a comparison between\n<firstValue>\nand\n<secondValue>\n. It should return a positive integer for \"greater than\", 0 for \"equal\" and a negative integer for \"less than\". To revert to deprecated behavior where NULL is treated as 0 (equal), you must set \"spark.sql.legacy.allowNullComparisonResultInArraySort\" to \"true\".\n22004\n#\nNULL\n_QUERY\n_STRING\n_EXECUTE\n_IMMEDIATE\nExecute immediate requires a non-null variable as the query string, but the provided variable\n<varName>\nis null.\n22004\n#\nTUPLE\n_IS\n_EMPTY\nDue to Scala's limited support of tuple, empty tuple is not supported.\n22006\n#\nCANNOT\n_PARSE\n_INTERVAL\nUnable to parse\n<intervalString>\n. Please ensure that the value provided is in a valid format for defining an interval. You can reference the documentation for the correct format. If the issue persists, please double check that the input value is not null or empty and try again.\n22006\n#\nINVALID\n_INTERVAL\n_FORMAT\nError parsing '\n<input>\n' to interval. Please ensure that the value provided is in a valid format for defining an interval. You can reference the documentation for the correct format.\n#\nARITHMETIC\n_EXCEPTION\nUncaught arithmetic exception while parsing '\n<input>\n'.\n#\nDAY\n_TIME\n_PARSING\nError parsing interval day-time string:\n<msg>\n.\n#\nINPUT\n_IS\n_EMPTY\nInterval string cannot be empty.\n#\nINPUT\n_IS\n_NULL\nInterval string cannot be null.\n#\nINTERVAL\n_PARSING\nError parsing interval\n<interval>\nstring.\n#\nINVALID\n_FRACTION\n<unit>\ncannot have fractional part.\n#\nINVALID\n_PRECISION\nInterval can only support nanosecond precision,\n<value>\nis out of range.\n#\nINVALID\n_PREFIX\nInvalid interval prefix\n<prefix>\n.\n#\nINVALID\n_UNIT\nInvalid unit\n<unit>\n.\n#\nINVALID\n_VALUE\nInvalid value\n<value>\n.\n#\nMISSING\n_NUMBER\nExpect a number after\n<word>\nbut hit EOL.\n#\nMISSING\n_UNIT\nExpect a unit name after\n<word>\nbut hit EOL.\n#\nSECOND\n_NANO\n_FORMAT\nInterval string does not match second-nano format of ss.nnnnnnnnn.\n#\nTIMEZONE\n_INTERVAL\n_OUT\n_OF\n_RANGE\nThe interval value must be in the range of [-18, +18] hours with second precision.\n#\nUNKNOWN\n_PARSING\n_ERROR\nUnknown error when parsing\n<word>\n.\n#\nUNMATCHED\n_FORMAT\n_STRING\nInterval string does not match\n<intervalStr>\nformat of\n<supportedFormat>\nwhen cast to\n<typeName>\n:\n<input>\n.\n#\nUNMATCHED\n_FORMAT\n_STRING\n_WITH\n_NOTICE\nInterval string does not match\n<intervalStr>\nformat of\n<supportedFormat>\nwhen cast to\n<typeName>\n:\n<input>\n. Set \"spark.sql.legacy.fromDayTimeString.enabled\" to \"true\" to restore the behavior before Spark 3.0.\n#\nUNRECOGNIZED\n_NUMBER\nUnrecognized number\n<number>\n.\n#\nUNSUPPORTED\n_FROM\n_TO\n_EXPRESSION\nCannot support (interval '\n<input>\n'\n<from>\nto\n<to>\n) expression.\n22006\n#\nINVALID\n_INTERVAL\n_WITH\n_MICROSECONDS\n_ADDITION\nCannot add an interval to a date because its microseconds part is not 0. To resolve this, cast the input date to a timestamp, which supports the addition of intervals with non-zero microseconds.\n22007\n#\nCANNOT\n_PARSE\n_TIMESTAMP\n<message>\n. Use\n<func>\nto tolerate invalid input string and return NULL instead.\n22007\n#\nINVALID\n_DATETIME\n_PATTERN\nUnrecognized datetime pattern:\n<pattern>\n.\n#\nILLEGAL\n_CHARACTER\nIllegal pattern character found in datetime pattern:\n<c>\n. Please provide legal character.\n#\nLENGTH\nToo many letters in datetime pattern:\n<pattern>\n. Please reduce pattern length.\n#\nSECONDS\n_FRACTION\nCannot detect a seconds fraction pattern of variable length. Please make sure the pattern contains 'S', and does not contain illegal characters.\n22008\n#\nDATETIME\n_OVERFLOW\nDatetime operation overflow:\n<operation>\n.\n22009\n#\nILLEGAL\n_DAY\n_OF\n_WEEK\nIllegal input for day of week:\n<string>\n.\n22009\n#\nINVALID\n_TIMEZONE\nThe timezone:\n<timeZone>\nis invalid. The timezone must be either a region-based zone ID or a zone offset. Region IDs must have the form 'area/city', such as 'America/Los_Angeles'. Zone offsets must be in the format '(+|-)HH', '(+|-)HH:mm’ or '(+|-)HH:mm:ss', e.g '-08' , '+01:00' or '-13:33:33', and must be in the range from -18:00 to +18:00. 'Z' and 'UTC' are accepted as synonyms for '+00:00'.\n2200E\n#\nNULL\n_MAP\n_KEY\nCannot use null as map key.\n22012\n#\nDIVIDE\n_BY\n_ZERO\nDivision by zero. Use\ntry_divide\nto tolerate divisor being 0 and return NULL instead. If necessary set\n<config>\nto \"false\" to bypass this error.\n22012\n#\nINTERVAL\n_DIVIDED\n_BY\n_ZERO\nDivision by zero. Use\ntry_divide\nto tolerate divisor being 0 and return NULL instead.\n22015\n#\nINTERVAL\n_ARITHMETIC\n_OVERFLOW\nInteger overflow while operating with intervals.\n#\nWITHOUT\n_SUGGESTION\nTry devising appropriate values for the interval parameters.\n#\nWITH\n_SUGGESTION\nUse\n<functionName>\nto tolerate overflow and return NULL instead.\n22018\n#\nCANNOT\n_PARSE\n_DECIMAL\nCannot parse decimal. Please ensure that the input is a valid number with optional decimal point or comma separators.\n22018\n#\nCANNOT\n_PARSE\n_PROTOBUF\n_DESCRIPTOR\nError parsing descriptor bytes into Protobuf FileDescriptorSet.\n22018\n#\nCAST\n_INVALID\n_INPUT\nThe value\n<expression>\nof the type\n<sourceType>\ncannot be cast to\n<targetType>\nbecause it is malformed. Correct the value as per the syntax, or change its target type. Use\ntry_cast\nto tolerate malformed input and return NULL instead.\n22018\n#\nCONVERSION\n_INVALID\n_INPUT\nThe value\n<str>\n(\n<fmt>\n) cannot be converted to\n<targetType>\nbecause it is malformed. Correct the value as per the syntax, or change its format. Use\n<suggestion>\nto tolerate malformed input and return NULL instead.\n22018\n#\nFAILED\n_PARSE\n_STRUCT\n_TYPE\nFailed parsing struct:\n<raw>\n.\n2201E\n#\nSTRUCT\n_ARRAY\n_LENGTH\n_MISMATCH\nInput row doesn't have expected number of values required by the schema.\n<expected>\nfields are required while\n<actual>\nvalues are provided.\n22022\n#\nINVALID\n_CONF\n_VALUE\nThe value '\n<confValue>\n' in the config \"\n<confName>\n\" is invalid.\n#\nTIME\n_ZONE\nCannot resolve the given timezone.\n22023\n#\nDATETIME\n_FIELD\n_OUT\n_OF\n_BOUNDS\n<rangeMessage>\n. If necessary set\n<ansiConfig>\nto \"false\" to bypass this error.\n22023\n#\nINVALID\n_FRACTION\n_OF\n_SECOND\nValid range for seconds is [0, 60] (inclusive), but the provided value is\n<secAndMicros>\n. To avoid this error, use\ntry_make_timestamp\n, which returns NULL on error. If you do not want to use the session default timestamp version of this function, use\ntry_make_timestamp_ntz\nor\ntry_make_timestamp_ltz\n.\n22023\n#\nINVALID\n_JSON\n_RECORD\n_TYPE\nDetected an invalid type of a JSON record while inferring a common schema in the mode\n<failFastMode>\n. Expected a STRUCT type, but found\n<invalidType>\n.\n22023\n#\nINVALID\n_PARAMETER\n_VALUE\nThe value of parameter(s)\n<parameter>\nin\n<functionName>\nis invalid:\n#\nAES\n_CRYPTO\n_ERROR\ndetail message:\n<detailMessage>\n#\nAES\n_IV\n_LENGTH\nsupports 16-byte CBC IVs and 12-byte GCM IVs, but got\n<actualLength>\nbytes for\n<mode>\n.\n#\nAES\n_KEY\n_LENGTH\nexpects a binary value with 16, 24 or 32 bytes, but got\n<actualLength>\nbytes.\n#\nBINARY\n_FORMAT\nexpects one of binary formats 'base64', 'hex', 'utf-8', but got\n<invalidFormat>\n.\n#\nBIT\n_POSITION\n_RANGE\nexpects an integer value in [0,\n<upper>\n), but got\n<invalidValue>\n.\n#\nBOOLEAN\nexpects a boolean literal, but got\n<invalidValue>\n.\n#\nCHARSET\nexpects one of the\n<charsets>\n, but got\n<charset>\n.\n#\nDATETIME\n_UNIT\nexpects one of the units without quotes YEAR, QUARTER, MONTH, WEEK, DAY, DAYOFYEAR, HOUR, MINUTE, SECOND, MILLISECOND, MICROSECOND, but got the string literal\n<invalidValue>\n.\n#\nDOUBLE\nexpects an double literal, but got\n<invalidValue>\n.\n#\nDTYPE\nUnsupported dtype:\n<invalidValue>\n. Valid values: float64, float32.\n#\nEXTENSION\nInvalid extension:\n<invalidValue>\n. Extension is limited to exactly 3 letters (e.g. csv, tsv, etc...)\n#\nINTEGER\nexpects an integer literal, but got\n<invalidValue>\n.\n#\nLENGTH\nExpects\nlength\ngreater than or equal to 0, but got\n<length>\n.\n#\nLONG\nexpects a long literal, but got\n<invalidValue>\n.\n#\nNULL\nexpects a non-NULL value.\n#\nPATTERN\n<value>\n.\n#\nREGEX\n_GROUP\n_INDEX\nExpects group index between 0 and\n<groupCount>\n, but got\n<groupIndex>\n.\n#\nSTART\nExpects a positive or a negative value for\nstart\n, but got 0.\n#\nSTRING\nexpects a string literal, but got\n<invalidValue>\n.\n#\nZERO\n_INDEX\nexpects %1$, %2$ and so on, but got %0$.\n22023\n#\nINVALID\n_REGEXP\n_REPLACE\nCould not perform regexp_replace for source = \"\n<source>\n\", pattern = \"\n<pattern>\n\", replacement = \"\n<replacement>\n\" and position =\n<position>\n.\n22023\n#\nINVALID\n_VARIANT\n_CAST\nThe variant value\n<value>\ncannot be cast into\n<dataType>\n. Please use\ntry_variant_get\ninstead.\n22023\n#\nINVALID\n_VARIANT\n_FROM\n_PARQUET\nInvalid variant.\n#\nMISSING\n_FIELD\nMissing\n<field>\nfield.\n#\nNULLABLE\n_OR\n_NOT\n_BINARY\n_FIELD\nThe\n<field>\nmust be a non-nullable binary.\n#\nWRONG\n_NUM\n_FIELDS\nVariant column must contain exactly two fields.\n22023\n#\nINVALID\n_VARIANT\n_GET\n_PATH\nThe path\n<path>\nis not a valid variant extraction path in\n<functionName>\n. A valid path should start with\n$\nand is followed by zero or more segments like\n[123]\n,\n.name\n,\n['name']\n, or\n[\"name\"]\n.\n22023\n#\nINVALID\n_VARIANT\n_SHREDDING\n_SCHEMA\nThe schema\n<schema>\nis not a valid variant shredding schema.\n22023\n#\nMALFORMED\n_RECORD\n_IN\n_PARSING\nMalformed records are detected in record parsing:\n<badRecord>\n. Parse Mode:\n<failFastMode>\n. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n#\nCANNOT\n_PARSE\n_JSON\n_ARRAYS\n_AS\n_STRUCTS\nParsing JSON arrays as structs is forbidden.\n#\nCANNOT\n_PARSE\n_STRING\n_AS\n_DATATYPE\nCannot parse the value\n<fieldValue>\nof the field\n<fieldName>\nas target spark data type\n<targetType>\nfrom the input type\n<inputType>\n.\n#\nWITHOUT\n_SUGGESTION\n22023\n#\nMALFORMED\n_VARIANT\nVariant binary is malformed. Please check the data source is valid.\n22023\n#\nROW\n_VALUE\n_IS\n_NULL\nFound NULL in a row at the index\n<index>\n, expected a non-NULL value.\n22023\n#\nRULE\n_ID\n_NOT\n_FOUND\nNot found an id for the rule name \"\n<ruleName>\n\". Please modify RuleIdCollection.scala if you are adding a new rule.\n22023\n#\nSECOND\n_FUNCTION\n_ARGUMENT\n_NOT\n_INTEGER\nThe second argument of\n<functionName>\nfunction needs to be an integer.\n22023\n#\nTABLE\n_VALUED\n_FUNCTION\n_REQUIRED\n_METADATA\n_INCOMPATIBLE\n_WITH\n_CALL\nFailed to evaluate the table function\n<functionName>\nbecause its table metadata\n<requestedMetadata>\n, but the function call\n<invalidFunctionCallProperty>\n.\n22023\n#\nTABLE\n_VALUED\n_FUNCTION\n_REQUIRED\n_METADATA\n_INVALID\nFailed to evaluate the table function\n<functionName>\nbecause its table metadata was invalid;\n<reason>\n.\n22023\n#\nUNKNOWN\n_PRIMITIVE\n_TYPE\n_IN\n_VARIANT\nUnknown primitive type with id\n<id>\nwas found in a variant value.\n22023\n#\nVARIANT\n_CONSTRUCTOR\n_SIZE\n_LIMIT\nCannot construct a Variant larger than 16 MiB. The maximum allowed size of a Variant value is 16 MiB.\n22023\n#\nVARIANT\n_DUPLICATE\n_KEY\nFailed to build variant because of a duplicate object key\n<key>\n.\n22023\n#\nVARIANT\n_SIZE\n_LIMIT\nCannot build variant bigger than\n<sizeLimit>\nin\n<functionName>\n. Please avoid large input strings to this expression (for example, add function calls(s) to check the expression size and convert it to NULL first if it is too big).\n22024\n#\nNULL\n_DATA\n_SOURCE\n_OPTION\nData source read/write option\n<option>\ncannot have null value.\n22029\n#\nINVALID\n_UTF8\n_STRING\nInvalid UTF8 byte sequence found in string:\n<str>\n.\n22032\n#\nINVALID\n_JSON\n_ROOT\n_FIELD\nCannot convert JSON root field to target Spark type.\n22032\n#\nINVALID\n_JSON\n_SCHEMA\n_MAP\n_TYPE\nInput schema\n<jsonSchema>\ncan only contain STRING as a key type for a MAP.\n2203G\n#\nCANNOT\n_PARSE\n_JSON\n_FIELD\nCannot parse the field name\n<fieldName>\nand the value\n<fieldValue>\nof the JSON token type\n<jsonType>\nto target Spark data type\n<dataType>\n.\n2203G\n#\nFAILED\n_ROW\n_TO\n_JSON\nFailed to convert the row value\n<value>\nof the class\n<class>\nto the target SQL type\n<sqlType>\nin the JSON format.\n2203G\n#\nINVALID\n_JSON\n_DATA\n_TYPE\nFailed to convert the JSON string '\n<invalidType>\n' to a data type. Please enter a valid data type.\n2203G\n#\nINVALID\n_JSON\n_DATA\n_TYPE\n_FOR\n_COLLATIONS\nCollations can only be applied to string types, but the JSON data type is\n<jsonType>\n.\n22546\n#\nCANNOT\n_DECODE\n_URL\nThe provided URL cannot be decoded:\n<url>\n. Please ensure that the URL is properly formatted and try again.\n22546\n#\nHLL\n_INVALID\n_INPUT\n_SKETCH\n_BUFFER\nInvalid call to\n<function>\n; only valid HLL sketch buffers are supported as inputs (such as those produced by the\nhll_sketch_agg\nfunction).\n22546\n#\nHLL\n_INVALID\n_LG\n_K\nInvalid call to\n<function>\n; the\nlgConfigK\nvalue must be between\n<min>\nand\n<max>\n, inclusive:\n<value>\n.\n22546\n#\nINVALID\n_BOOLEAN\n_STATEMENT\nBoolean statement is expected in the condition, but\n<invalidStatement>\nwas found.\n22KD3\n#\nAVRO\n_INCOMPATIBLE\n_READ\n_TYPE\nCannot convert Avro\n<avroPath>\nto SQL\n<sqlPath>\nbecause the original encoded data type is\n<avroType>\n, however you're trying to read the field as\n<sqlType>\n, which would lead to an incorrect answer. To allow reading this field, enable the SQL configuration: \"spark.sql.legacy.avro.allowIncompatibleSchema\".\n22KD3\n#\nAVRO\n_NOT\n_LOADED\n_SQL\n_FUNCTIONS\n_UNUSABLE\nCannot call the\n<functionName>\nSQL function because the Avro data source is not loaded. Please restart your job or session with the 'spark-avro' package loaded, such as by using the --packages argument on the command line, and then retry your query or command again.\n22KD3\n#\nCANNOT\n_USE\n_KRYO\nCannot load Kryo serialization codec. Kryo serialization cannot be used in the Spark Connect client. Use Java serialization, provide a custom Codec, or use Spark Classic instead.\n22KD3\n#\nPROTOBUF\n_NOT\n_LOADED\n_SQL\n_FUNCTIONS\n_UNUSABLE\nCannot call the\n<functionName>\nSQL function because the Protobuf data source is not loaded. Please restart your job or session with the 'spark-protobuf' package loaded, such as by using the --packages argument on the command line, and then retry your query or command again.\n22P02\n#\nINVALID\n_URL\nThe url is invalid:\n<url>\n. Use\ntry_parse_url\nto tolerate invalid URL and return NULL instead.\n22P03\n#\nINVALID\n_BYTE\n_STRING\nThe expected format is ByteString, but was\n<unsupported>\n(\n<class>\n).\n23505\n#\nDUPLICATED\n_MAP\n_KEY\nDuplicate map key\n<key>\nwas found, please check the input data. If you want to remove the duplicated keys, you can set\n<mapKeyDedupPolicy>\nto \"LAST_WIN\" so that the key inserted at last takes precedence.\n23505\n#\nDUPLICATE\n_KEY\nFound duplicate keys\n<keyColumn>\n.\n23K01\n#\nMERGE\n_CARDINALITY\n_VIOLATION\nThe ON search condition of the MERGE statement matched a single row from the target table with multiple rows of the source table. This could result in the target row being operated on more than once with an update or delete operation and is not allowed.\n2BP01\n#\nSCHEMA\n_NOT\n_EMPTY\nCannot drop a schema\n<schemaName>\nbecause it contains objects. Use DROP SCHEMA ... CASCADE to drop the schema and all its objects.\n38000\n#\nCLASS\n_NOT\n_OVERRIDE\n_EXPECTED\n_METHOD\n<className>\nmust override either\n<method1>\nor\n<method2>\n.\n38000\n#\nFAILED\n_FUNCTION\n_CALL\nFailed preparing of the function\n<funcName>\nfor call. Please, double check function's arguments.\n38000\n#\nFAILED\n_TO\n_LOAD\n_ROUTINE\nFailed to load routine\n<routineName>\n.\n38000\n#\nINVALID\n_UDF\n_IMPLEMENTATION\nFunction\n<funcName>\ndoes not implement a ScalarFunction or AggregateFunction.\n38000\n#\nNO\n_UDF\n_INTERFACE\nUDF class\n<className>\ndoesn't implement any UDF interface.\n38000\n#\nPYTHON\n_DATA\n_SOURCE\n_ERROR\nFailed to\n<action>\nPython data source\n<type>\n:\n<msg>\n38000\n#\nPYTHON\n_STREAMING\n_DATA\n_SOURCE\n_RUNTIME\n_ERROR\nFailed when Python streaming data source perform\n<action>\n:\n<msg>\n38000\n#\nTABLE\n_VALUED\n_FUNCTION\n_FAILED\n_TO\n_ANALYZE\n_IN\n_PYTHON\nFailed to analyze the Python user defined table function:\n<msg>\n39000\n#\nFAILED\n_EXECUTE\n_UDF\nUser defined function (\n<functionName>\n: (\n<signature>\n) =>\n<result>\n) failed due to:\n<reason>\n.\n39000\n#\nFLATMAPGROUPSWITHSTATE\n_USER\n_FUNCTION\n_ERROR\nAn error occurred in the user provided function in flatMapGroupsWithState. Reason:\n<reason>\n39000\n#\nFOREACH\n_BATCH\n_USER\n_FUNCTION\n_ERROR\nAn error occurred in the user provided function in foreach batch sink. Reason:\n<reason>\n39000\n#\nFOREACH\n_USER\n_FUNCTION\n_ERROR\nAn error occurred in the user provided function in foreach sink. Reason:\n<reason>\n3F000\n#\nMISSING\n_DATABASE\n_FOR\n_V1\n_SESSION\n_CATALOG\nDatabase name is not specified in the v1 session catalog. Please ensure to provide a valid database name when interacting with the v1 catalog.\n40000\n#\nCONCURRENT\n_STREAM\n_LOG\n_UPDATE\nConcurrent update to the log. Multiple streaming jobs detected for\n<batchId>\n. Please make sure only one streaming job runs on a specific checkpoint location at a time.\n42000\n#\nAMBIGUOUS\n_REFERENCE\n_TO\n_FIELDS\nAmbiguous reference to the field\n<field>\n. It appears\n<count>\ntimes in the schema.\n42000\n#\nCANNOT\n_REMOVE\n_RESERVED\n_PROPERTY\nCannot remove reserved property:\n<property>\n.\n42000\n#\nCLUSTERING\n_NOT\n_SUPPORTED\n'\n<operation>\n' does not support clustering.\n42000\n#\nINVALID\n_COLUMN\n_OR\n_FIELD\n_DATA\n_TYPE\nColumn or field\n<name>\nis of type\n<type>\nwhile it's required to be\n<expectedType>\n.\n42000\n#\nINVALID\n_EXTRACT\n_BASE\n_FIELD\n_TYPE\nCan't extract a value from\n<base>\n. Need a complex type [STRUCT, ARRAY, MAP] but got\n<other>\n.\n42000\n#\nINVALID\n_EXTRACT\n_FIELD\n_TYPE\nField name should be a non-null string literal, but it's\n<extraction>\n.\n42000\n#\nINVALID\n_FIELD\n_NAME\nField name\n<fieldName>\nis invalid:\n<path>\nis not a struct.\n42000\n#\nINVALID\n_INLINE\n_TABLE\nInvalid inline table.\n#\nCANNOT\n_EVALUATE\n_EXPRESSION\n_IN\n_INLINE\n_TABLE\nCannot evaluate the expression\n<expr>\nin inline table definition.\n#\nFAILED\n_SQL\n_EXPRESSION\n_EVALUATION\nFailed to evaluate the SQL expression\n<sqlExpr>\n. Please check your syntax and ensure all required tables and columns are available.\n#\nINCOMPATIBLE\n_TYPES\n_IN\n_INLINE\n_TABLE\nFound incompatible types in the column\n<colName>\nfor inline table.\n#\nNUM\n_COLUMNS\n_MISMATCH\nInline table expected\n<expectedNumCols>\ncolumns but found\n<actualNumCols>\ncolumns in row\n<rowIndex>\n.\n42000\n#\nINVALID\n_RESET\n_COMMAND\n_FORMAT\nExpected format is 'RESET' or 'RESET key'. If you want to include special characters in key, please use quotes, e.g., RESET\nkey\n.\n42000\n#\nINVALID\n_SAVE\n_MODE\nThe specified save mode\n<mode>\nis invalid. Valid save modes include \"append\", \"overwrite\", \"ignore\", \"error\", \"errorifexists\", and \"default\".\n42000\n#\nINVALID\n_SET\n_SYNTAX\nExpected format is 'SET', 'SET key', or 'SET key=value'. If you want to include special characters in key, or include semicolon in value, please use backquotes, e.g., SET\nkey\n=\nvalue\n.\n42000\n#\nINVALID\n_SQL\n_SYNTAX\nInvalid SQL syntax:\n#\nANALYZE\n_TABLE\n_UNEXPECTED\n_NOSCAN\nANALYZE TABLE(S) ... COMPUTE STATISTICS ...\n<ctx>\nmust be either NOSCAN or empty.\n#\nCREATE\n_FUNC\n_WITH\n_COLUMN\n_CONSTRAINTS\nCREATE FUNCTION with constraints on parameters is not allowed.\n#\nCREATE\n_FUNC\n_WITH\n_GENERATED\n_COLUMNS\n_AS\n_PARAMETERS\nCREATE FUNCTION with generated columns as parameters is not allowed.\n#\nCREATE\n_ROUTINE\n_WITH\n_IF\n_NOT\n_EXISTS\n_AND\n_REPLACE\nCannot create a routine with both IF NOT EXISTS and REPLACE specified.\n#\nCREATE\n_TEMP\n_FUNC\n_WITH\n_DATABASE\nCREATE TEMPORARY FUNCTION with specifying a database(\n<database>\n) is not allowed.\n#\nCREATE\n_TEMP\n_FUNC\n_WITH\n_IF\n_NOT\n_EXISTS\nCREATE TEMPORARY FUNCTION with IF NOT EXISTS is not allowed.\n#\nEMPTY\n_PARTITION\n_VALUE\nPartition key\n<partKey>\nmust set value.\n#\nFUNCTION\n_WITH\n_UNSUPPORTED\n_SYNTAX\nThe function\n<prettyName>\ndoes not support\n<syntax>\n.\n#\nINVALID\n_COLUMN\n_REFERENCE\nExpected a column reference for transform\n<transform>\n:\n<expr>\n.\n#\nINVALID\n_TABLE\n_FUNCTION\n_IDENTIFIER\n_ARGUMENT\n_MISSING\n_PARENTHESES\nSyntax error: call to table-valued function is invalid because parentheses are missing around the provided TABLE argument\n<argumentName>\n; please surround this with parentheses and try again.\n#\nINVALID\n_TABLE\n_VALUED\n_FUNC\n_NAME\nTable valued function cannot specify database name:\n<funcName>\n.\n#\nINVALID\n_WINDOW\n_REFERENCE\nWindow reference\n<windowName>\nis not a window specification.\n#\nLATERAL\n_WITHOUT\n_SUBQUERY\n_OR\n_TABLE\n_VALUED\n_FUNC\nLATERAL can only be used with subquery and table-valued functions.\n#\nMULTI\n_PART\n_NAME\n<statement>\nwith multiple part name(\n<name>\n) is not allowed.\n#\nOPTION\n_IS\n_INVALID\noption or property key\n<key>\nis invalid; only\n<supported>\nare supported\n#\nREPETITIVE\n_WINDOW\n_DEFINITION\nThe definition of window\n<windowName>\nis repetitive.\n#\nSHOW\n_FUNCTIONS\n_INVALID\n_PATTERN\nInvalid pattern in SHOW FUNCTIONS:\n<pattern>\n. It must be a \"STRING\" literal.\n#\nSHOW\n_FUNCTIONS\n_INVALID\n_SCOPE\nSHOW\n<scope>\nFUNCTIONS not supported.\n#\nTRANSFORM\n_WRONG\n_NUM\n_ARGS\nThe transform\n<transform>\nrequires\n<expectedNum>\nparameters but the actual number is\n<actualNum>\n.\n#\nUNRESOLVED\n_WINDOW\n_REFERENCE\nCannot resolve window reference\n<windowName>\n.\n#\nUNSUPPORTED\n_FUNC\n_NAME\nUnsupported function name\n<funcName>\n.\n#\nUNSUPPORTED\n_SQL\n_STATEMENT\nUnsupported SQL statement:\n<sqlText>\n.\n#\nVARIABLE\n_TYPE\n_OR\n_DEFAULT\n_REQUIRED\nThe definition of a SQL variable requires either a datatype or a DEFAULT clause. For example, use\nDECLARE name STRING\nor\nDECLARE name = 'SQL'\ninstead of\nDECLARE name\n.\n42000\n#\nINVALID\n_USAGE\n_OF\n_STAR\n_OR\n_REGEX\nInvalid usage of\n<elem>\nin\n<prettyName>\n.\n42000\n#\nINVALID\n_WRITE\n_DISTRIBUTION\nThe requested write distribution is invalid.\n#\nPARTITION\n_NUM\n_AND\n_SIZE\nThe partition number and advisory partition size can't be specified at the same time.\n#\nPARTITION\n_NUM\n_WITH\n_UNSPECIFIED\n_DISTRIBUTION\nThe number of partitions can't be specified with unspecified distribution.\n#\nPARTITION\n_SIZE\n_WITH\n_UNSPECIFIED\n_DISTRIBUTION\nThe advisory partition size can't be specified with unspecified distribution.\n42000\n#\nMULTIPLE\n_QUERY\n_RESULT\n_CLAUSES\n_WITH\n_PIPE\n_OPERATORS\n<clause1>\nand\n<clause2>\ncannot coexist in the same SQL pipe operator using '|>'. Please separate the multiple result clauses into separate pipe operators and then retry the query again.\n42000\n#\nNON\n_PARTITION\n_COLUMN\nPARTITION clause cannot contain the non-partition column:\n<columnName>\n.\n42000\n#\nNOT\n_NULL\n_ASSERT\n_VIOLATION\nNULL value appeared in non-nullable field:\n<walkedTypePath>\nIf the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (such as java.lang.Integer instead of int/scala.Int).\n42000\n#\nNOT\n_NULL\n_CONSTRAINT\n_VIOLATION\nAssigning a NULL is not allowed here.\n#\nARRAY\n_ELEMENT\nThe array\n<columnPath>\nis defined to contain only elements that are NOT NULL.\n#\nMAP\n_VALUE\nThe map\n<columnPath>\nis defined to contain only values that are NOT NULL.\n42000\n#\nNO\n_HANDLER\n_FOR\n_UDAF\nNo handler for UDAF '\n<functionName>\n'. Use sparkSession.udf.register(...) instead.\n42000\n#\nNULLABLE\n_COLUMN\n_OR\n_FIELD\nColumn or field\n<name>\nis nullable while it's required to be non-nullable.\n42000\n#\nNULLABLE\n_ROW\n_ID\n_ATTRIBUTES\nRow ID attributes cannot be nullable:\n<nullableRowIdAttrs>\n.\n42000\n#\nPARTITION\n_COLUMN\n_NOT\n_FOUND\n_IN\n_SCHEMA\nPartition column\n<column>\nnot found in schema\n<schema>\n. Please provide the existing column for partitioning.\n42000\n#\nUNSUPPORTED\n_TABLE\n_CHANGE\n_IN\n_JDBC\n_CATALOG\nThe table change\n<change>\nis not supported for the JDBC catalog on table\n<tableName>\n. Supported changes include: AddColumn, RenameColumn, DeleteColumn, UpdateColumnType, UpdateColumnNullability.\n42001\n#\nINVALID\n_AGNOSTIC\n_ENCODER\nFound an invalid agnostic encoder. Expects an instance of AgnosticEncoder but got\n<encoderType>\n. For more information consult '\n<docroot>\n/api/java/index.html?org/apache/spark/sql/Encoder.html'.\n42001\n#\nINVALID\n_EXPRESSION\n_ENCODER\nFound an invalid expression encoder. Expects an instance of ExpressionEncoder but got\n<encoderType>\n. For more information consult '\n<docroot>\n/api/java/index.html?org/apache/spark/sql/Encoder.html'.\n42601\n#\nCOLUMN\n_ALIASES\n_NOT\n_ALLOWED\nColumn aliases are not allowed in\n<op>\n.\n42601\n#\nIDENTIFIER\n_TOO\n_MANY\n_NAME\n_PARTS\n<identifier>\nis not a valid identifier as it has more than 2 name parts.\n42601\n#\nIDENTITY\n_COLUMNS\n_DUPLICATED\n_SEQUENCE\n_GENERATOR\n_OPTION\nDuplicated IDENTITY column sequence generator option:\n<sequenceGeneratorOption>\n.\n42601\n#\nILLEGAL\n_STATE\n_STORE\n_VALUE\nIllegal value provided to the State Store\n#\nEMPTY\n_LIST\n_VALUE\nCannot write empty list values to State Store for StateName\n<stateName>\n.\n#\nNULL\n_VALUE\nCannot write null values to State Store for StateName\n<stateName>\n.\n42601\n#\nINVALID\n_ATTRIBUTE\n_NAME\n_SYNTAX\nSyntax error in the attribute name:\n<name>\n. Check that backticks appear in pairs, a quoted string is a complete name part and use a backtick only inside quoted name parts.\n42601\n#\nINVALID\n_BUCKET\n_COLUMN\n_DATA\n_TYPE\nCannot use\n<type>\nfor bucket column. Collated data types are not supported for bucketing.\n42601\n#\nINVALID\n_EXTRACT\n_FIELD\nCannot extract\n<field>\nfrom\n<expr>\n.\n42601\n#\nINVALID\n_FORMAT\nThe format is invalid:\n<format>\n.\n#\nCONT\n_THOUSANDS\n_SEPS\nThousands separators (, or G) must have digits in between them in the number format.\n#\nCUR\n_MUST\n_BEFORE\n_DEC\nCurrency characters must appear before any decimal point in the number format.\n#\nCUR\n_MUST\n_BEFORE\n_DIGIT\nCurrency characters must appear before digits in the number format.\n#\nEMPTY\nThe number format string cannot be empty.\n#\nESC\n_AT\n_THE\n_END\nThe escape character is not allowed to end with.\n#\nESC\n_IN\n_THE\n_MIDDLE\nThe escape character is not allowed to precede\n<char>\n.\n#\nMISMATCH\n_INPUT\nThe input\n<inputType>\n<input>\ndoes not match the format.\n#\nTHOUSANDS\n_SEPS\n_MUST\n_BEFORE\n_DEC\nThousands separators (, or G) may not appear after the decimal point in the number format.\n#\nUNEXPECTED\n_TOKEN\nFound the unexpected\n<token>\nin the format string; the structure of the format string must match:\n[MI|S]\n[$]\n[0|9|G|,]*\n[.|D]\n[0|9]*\n[$]\n[PR|MI|S]\n.\n#\nWRONG\n_NUM\n_DIGIT\nThe format string requires at least one number digit.\n#\nWRONG\n_NUM\n_TOKEN\nAt most one\n<token>\nis allowed in the number format.\n42601\n#\nINVALID\n_PARTITION\n_OPERATION\nThe partition command is invalid.\n#\nPARTITION\n_MANAGEMENT\n_IS\n_UNSUPPORTED\nTable\n<name>\ndoes not support partition management.\n#\nPARTITION\n_SCHEMA\n_IS\n_EMPTY\nTable\n<name>\nis not partitioned.\n42601\n#\nINVALID\n_STATEMENT\n_OR\n_CLAUSE\nThe statement or clause:\n<operation>\nis not valid.\n42601\n#\nINVALID\n_WINDOW\n_SPEC\n_FOR\n_AGGREGATION\n_FUNC\nCannot specify ORDER BY or a window frame for\n<aggFunc>\n.\n42601\n#\nLOCAL\n_MUST\n_WITH\n_SCHEMA\n_FILE\nLOCAL must be used together with the schema of\nfile\n, but got:\n<actualSchema>\n.\n42601\n#\nMERGE\n_WITHOUT\n_WHEN\nThere must be at least one WHEN clause in a MERGE statement.\n42601\n#\nNOT\n_ALLOWED\n_IN\n_FROM\nNot allowed in the FROM clause:\n#\nLATERAL\n_WITH\n_PIVOT\nLATERAL together with PIVOT.\n#\nLATERAL\n_WITH\n_UNPIVOT\nLATERAL together with UNPIVOT.\n#\nUNPIVOT\n_WITH\n_PIVOT\nUNPIVOT together with PIVOT.\n42601\n#\nNOT\n_ALLOWED\n_IN\n_PIPE\n_OPERATOR\n_WHERE\nNot allowed in the pipe WHERE clause:\n#\nWINDOW\n_CLAUSE\nWINDOW clause.\n42601\n#\nNOT\n_A\n_CONSTANT\n_STRING\nThe expression\n<expr>\nused for the routine or clause\n<name>\nmust be a constant STRING which is NOT NULL.\n#\nNOT\n_CONSTANT\nTo be considered constant the expression must not depend on any columns, contain a subquery, or invoke a non deterministic function such as rand().\n#\nNULL\nThe expression evaluates to NULL.\n#\nWRONG\n_TYPE\nThe data type of the expression is\n<dataType>\n.\n42601\n#\nNOT\n_UNRESOLVED\n_ENCODER\nUnresolved encoder expected, but\n<attr>\nwas found.\n42601\n#\nPARSE\n_MODE\n_UNSUPPORTED\nThe function\n<funcName>\ndoesn't support the\n<mode>\nmode. Acceptable modes are PERMISSIVE and FAILFAST.\n42601\n#\nPARSE\n_SYNTAX\n_ERROR\nSyntax error at or near\n<error>``<hint>\n.\n42601\n#\nREF\n_DEFAULT\n_VALUE\n_IS\n_NOT\n_ALLOWED\n_IN\n_PARTITION\nReferences to DEFAULT column values are not allowed within the PARTITION clause.\n42601\n#\nSORT\n_BY\n_WITHOUT\n_BUCKETING\nsortBy must be used together with bucketBy.\n42601\n#\nSPECIFY\n_BUCKETING\n_IS\n_NOT\n_ALLOWED\nA CREATE TABLE without explicit column list cannot specify bucketing information. Please use the form with explicit column list and specify bucketing information. Alternatively, allow bucketing information to be inferred by omitting the clause.\n42601\n#\nSPECIFY\n_PARTITION\n_IS\n_NOT\n_ALLOWED\nA CREATE TABLE without explicit column list cannot specify PARTITIONED BY. Please use the form with explicit column list and specify PARTITIONED BY. Alternatively, allow partitioning to be inferred by omitting the PARTITION BY clause.\n42601\n#\nSTDS\n_REQUIRED\n_OPTION\n_UNSPECIFIED\n'\n<optionName>\n' must be specified.\n42601\n#\nSYNTAX\n_DISCONTINUED\nSupport of the clause or keyword:\n<clause>\nhas been discontinued in this context.\n#\nBANG\n_EQUALS\n_NOT\nThe '!' keyword is only supported as an alias for the prefix operator 'NOT'. Use the 'NOT' keyword instead for infix clauses such as\nNOT LIKE\n,\nNOT IN\n,\nNOT BETWEEN\n, etc. To re-enable the '!' keyword, set \"spark.sql.legacy.bangEqualsNot\" to \"true\".\n42601\n#\nTRAILING\n_COMMA\n_IN\n_SELECT\nTrailing comma detected in SELECT clause. Remove the trailing comma before the FROM clause.\n42601\n#\nUNCLOSED\n_BRACKETED\n_COMMENT\nFound an unclosed bracketed comment. Please, append */ at the end of the comment.\n42601\n#\nUSER\n_DEFINED\n_FUNCTIONS\nUser defined function is invalid:\n#\nCANNOT\n_CONTAIN\n_COMPLEX\n_FUNCTIONS\nSQL scalar function cannot contain aggregate/window/generate functions:\n<queryText>\n#\nCANNOT\n_REPLACE\n_NON\n_SQL\n_UDF\n_WITH\n_SQL\n_UDF\nCannot replace the non-SQL function\n<name>\nwith a SQL function.\n#\nNOT\n_A\n_VALID\n_DEFAULT\n_EXPRESSION\nThe DEFAULT expression of\n<functionName>\n.\n<parameterName>\nis not supported because it contains a subquery.\n#\nNOT\n_A\n_VALID\n_DEFAULT\n_PARAMETER\n_POSITION\nIn routine\n<functionName>\nparameter\n<parameterName>\nwith DEFAULT must not be followed by parameter\n<nextParameterName>\nwithout DEFAULT.\n#\nNOT\n_NULL\n_ON\n_FUNCTION\n_PARAMETERS\nCannot specify NOT NULL on function parameters:\n<input>\n#\nRETURN\n_COLUMN\n_COUNT\n_MISMATCH\nThe number of columns produced by the RETURN clause (num:\n<outputSize>\n) does not match the number of column names specified by the RETURNS clause (num:\n<returnParamSize>\n) of\n<name>\n.\n#\nROUTINE\n_PROPERTY\n_TOO\n_LARGE\nCannot convert user defined routine\n<name>\nto catalog function: routine properties are too large.\n#\nSQL\n_TABLE\n_UDF\n_BODY\n_MUST\n_BE\n_A\n_QUERY\nSQL table function\n<name>\nbody must be a query.\n#\nSQL\n_TABLE\n_UDF\n_MISSING\n_COLUMN\n_NAMES\nThe relation returned by the query in the CREATE FUNCTION statement for\n<functionName>\nwith RETURNS TABLE clause lacks explicit names for one or more output columns; please rewrite the function body to provide explicit column names or add column names to the RETURNS TABLE clause, and re-run the command.\n42601\n#\nWINDOW\n_FUNCTION\n_WITHOUT\n_OVER\n_CLAUSE\nWindow function\n<funcName>\nrequires an OVER clause.\n42601\n#\nWRITE\n_STREAM\n_NOT\n_ALLOWED\nwriteStream\ncan be called only on streaming Dataset/DataFrame.\n42602\n#\nCIRCULAR\n_CLASS\n_REFERENCE\nCannot have circular references in class, but got the circular reference of class\n<t>\n.\n42602\n#\nDUPLICATED\n_CTE\n_NAMES\nCTE definition can't have duplicate names:\n<duplicateNames>\n.\n42602\n#\nINVALID\n_DELIMITER\n_VALUE\nInvalid value for delimiter.\n#\nDELIMITER\n_LONGER\n_THAN\n_EXPECTED\nDelimiter cannot be more than one character:\n<str>\n.\n#\nEMPTY\n_STRING\nDelimiter cannot be empty string.\n#\nNULL\n_VALUE\nDelimiter cannot be null.\n#\nSINGLE\n_BACKSLASH\nSingle backslash is prohibited. It has special meaning as beginning of an escape sequence. To get the backslash character, pass a string with two backslashes as the delimiter.\n#\nUNSUPPORTED\n_SPECIAL\n_CHARACTER\nUnsupported special character for delimiter:\n<str>\n.\n42602\n#\nINVALID\n_IDENTIFIER\nThe unquoted identifier\n<ident>\nis invalid and must be back quoted as:\n<ident>\n. Unquoted identifiers can only contain ASCII letters ('a' - 'z', 'A' - 'Z'), digits ('0' - '9'), and underbar ('_'). Unquoted identifiers must also not start with a digit. Different data sources and meta stores may impose additional restrictions on valid identifiers.\n42602\n#\nINVALID\n_PROPERTY\n_KEY\n<key>\nis an invalid property key, please use quotes, e.g. SET\n<key>\n=\n<value>\n.\n42602\n#\nINVALID\n_PROPERTY\n_VALUE\n<value>\nis an invalid property value, please use quotes, e.g. SET\n<key>\n=\n<value>\n42602\n#\nINVALID\n_SCHEMA\n_OR\n_RELATION\n_NAME\n<name>\nis not a valid name for tables/schemas. Valid names only contain alphabet characters, numbers and _.\n42604\n#\nAS\n_OF\n_JOIN\nInvalid as-of join.\n#\nTOLERANCE\n_IS\n_NON\n_NEGATIVE\nThe input argument\ntolerance\nmust be non-negative.\n#\nTOLERANCE\n_IS\n_UNFOLDABLE\nThe input argument\ntolerance\nmust be a constant.\n#\nUNSUPPORTED\n_DIRECTION\nUnsupported as-of join direction '\n<direction>\n'. Supported as-of join direction include:\n<supported>\n.\n42604\n#\nEMPTY\n_JSON\n_FIELD\n_VALUE\nFailed to parse an empty string for data type\n<dataType>\n.\n42604\n#\nINVALID\n_ESC\nFound an invalid escape string:\n<invalidEscape>\n. The escape string must contain only one character.\n42604\n#\nINVALID\n_ESCAPE\n_CHAR\nEscapeChar\nshould be a string literal of length one, but got\n<sqlExpr>\n.\n42604\n#\nINVALID\n_TYPED\n_LITERAL\nThe value of the typed literal\n<valueType>\nis invalid:\n<value>\n.\n42605\n#\nWRONG\n_NUM\n_ARGS\nThe\n<functionName>\nrequires\n<expectedNum>\nparameters but the actual number is\n<actualNum>\n.\n#\nWITHOUT\n_SUGGESTION\nPlease, refer to '\n<docroot>\n/sql-ref-functions.html' for a fix.\n#\nWITH\n_SUGGESTION\nIf you have to call this function with\n<legacyNum>\nparameters, set the legacy configuration\n<legacyConfKey>\nto\n<legacyConfValue>\n.\n42607\n#\nNESTED\n_AGGREGATE\n_FUNCTION\nIt is not allowed to use an aggregate function in the argument of another aggregate function. Please use the inner aggregate function in a sub-query.\n42608\n#\nDEFAULT\n_PLACEMENT\n_INVALID\nA DEFAULT keyword in a MERGE, INSERT, UPDATE, or SET VARIABLE command could not be directly assigned to a target column because it was part of an expression. For example:\nUPDATE SET c1 = DEFAULT\nis allowed, but\nUPDATE T SET c1 = DEFAULT + 1\nis not allowed.\n42608\n#\nNO\n_DEFAULT\n_COLUMN\n_VALUE\n_AVAILABLE\nCan't determine the default value for\n<colName>\nsince it is not nullable and it has no default value.\n42611\n#\nCANNOT\n_ASSIGN\n_EVENT\n_TIME\n_COLUMN\n_WITHOUT\n_WATERMARK\nWatermark needs to be defined to reassign event time column. Failed to find watermark definition in the streaming query.\n42611\n#\nIDENTITY\n_COLUMNS\n_ILLEGAL\n_STEP\nIDENTITY column step cannot be 0.\n42613\n#\nINCOMPATIBLE\n_JOIN\n_TYPES\nThe join types\n<joinType1>\nand\n<joinType2>\nare incompatible.\n42613\n#\nINVALID\n_JOIN\n_TYPE\n_FOR\n_JOINWITH\nInvalid join type in joinWith:\n<joinType>\n.\n42613\n#\nINVALID\n_LATERAL\n_JOIN\n_TYPE\nThe\n<joinType>\nJOIN with LATERAL correlation is not allowed because an OUTER subquery cannot correlate to its join partner. Remove the LATERAL correlation or use an INNER JOIN, or LEFT OUTER JOIN instead.\n42613\n#\nINVALID\n_QUERY\n_MIXED\n_QUERY\n_PARAMETERS\nParameterized query must either use positional, or named parameters, but not both.\n42613\n#\nINVALID\n_SINGLE\n_VARIANT\n_COLUMN\nThe\nsingleVariantColumn\noption cannot be used if there is also a user specified schema.\n42613\n#\nNON\n_LAST\n_MATCHED\n_CLAUSE\n_OMIT\n_CONDITION\nWhen there are more than one MATCHED clauses in a MERGE statement, only the last MATCHED clause can omit the condition.\n42613\n#\nNON\n_LAST\n_NOT\n_MATCHED\n_BY\n_SOURCE\n_CLAUSE\n_OMIT\n_CONDITION\nWhen there are more than one NOT MATCHED BY SOURCE clauses in a MERGE statement, only the last NOT MATCHED BY SOURCE clause can omit the condition.\n42613\n#\nNON\n_LAST\n_NOT\n_MATCHED\n_BY\n_TARGET\n_CLAUSE\n_OMIT\n_CONDITION\nWhen there are more than one NOT MATCHED [BY TARGET] clauses in a MERGE statement, only the last NOT MATCHED [BY TARGET] clause can omit the condition.\n42613\n#\nSTDS\n_CONFLICT\n_OPTIONS\nThe options\n<options>\ncannot be specified together. Please specify the one.\n42614\n#\nDUPLICATE\n_CLAUSES\nFound duplicate clauses:\n<clauseName>\n. Please, remove one of them.\n42614\n#\nREPEATED\n_CLAUSE\nThe\n<clause>\nclause may be used at most once per\n<operation>\noperation.\n42616\n#\nINVALID\n_SPARK\n_CONFIG\nInvalid Spark config:\n#\nINVALID\n_EXECUTOR\n_HEARTBEAT\n_INTERVAL\nThe value of\n<networkTimeoutKey>\n=\n<networkTimeoutValue>\nms must be greater than the value of\n<executorHeartbeatIntervalKey>\n=\n<executorHeartbeatIntervalValue>\nms.\n#\nINVALID\n_EXECUTOR\n_MEMORY\n_OPTIONS\n<executorOptsKey>\nis not allowed to specify max heap memory settings (was '\n<javaOpts>\n'). Use spark.executor.memory instead.\n#\nINVALID\n_EXECUTOR\n_SPARK\n_OPTIONS\n<executorOptsKey>\nis not allowed to set Spark options (was '\n<javaOpts>\n'). Set them directly on a SparkConf or in a properties file when using ./bin/spark-submit.\n#\nINVALID\n_MEMORY\n_FRACTION\n<memoryFractionKey>\nshould be between 0 and 1 (was '\n<memoryFractionValue>\n').\n#\nINVALID\n_SPARK\n_SUBMIT\n_DEPLOY\n_MODE\n_KEY\n<sparkSubmitDeployModeKey>\ncan only be \"cluster\" or \"client\".\n#\nNETWORK\n_AUTH\n_MUST\n_BE\n_ENABLED\n<networkAuthEnabledConf>\nmust be enabled when enabling encryption.\n42616\n#\nSTDS\n_INVALID\n_OPTION\n_VALUE\nInvalid value for source option '\n<optionName>\n':\n#\nIS\n_EMPTY\ncannot be empty.\n#\nIS\n_NEGATIVE\ncannot be negative.\n#\nWITH\n_MESSAGE\n<message>\n42617\n#\nPARSE\n_EMPTY\n_STATEMENT\nSyntax error, unexpected empty statement.\n42621\n#\nUNSUPPORTED\n_EXPRESSION\n_GENERATED\n_COLUMN\nCannot create generated column\n<fieldName>\nwith generation expression\n<expressionStr>\nbecause\n<reason>\n.\n42623\n#\nADD\n_DEFAULT\n_UNSUPPORTED\nFailed to execute\n<statementType>\ncommand because DEFAULT values are not supported when adding new columns to previously existing target data source with table provider: \"\n<dataSource>\n\".\n42623\n#\nDEFAULT\n_UNSUPPORTED\nFailed to execute\n<statementType>\ncommand because DEFAULT values are not supported for target data source with table provider: \"\n<dataSource>\n\".\n42623\n#\nGENERATED\n_COLUMN\n_WITH\n_DEFAULT\n_VALUE\nA column cannot have both a default value and a generation expression but column\n<colName>\nhas default value: (\n<defaultValue>\n) and generation expression: (\n<genExpr>\n).\n42623\n#\nIDENTITY\n_COLUMN\n_WITH\n_DEFAULT\n_VALUE\nA column cannot have both a default value and an identity column specification but column\n<colName>\nhas default value: (\n<defaultValue>\n) and identity column specification: (\n<identityColumnSpec>\n).\n42623\n#\nINVALID\n_DEFAULT\n_VALUE\nFailed to execute\n<statement>\ncommand because the destination column or variable\n<colName>\nhas a DEFAULT value\n<defaultValue>\n,\n#\nDATA\n_TYPE\nwhich requires\n<expectedType>\ntype, but the statement provided a value of incompatible\n<actualType>\ntype.\n#\nNOT\n_CONSTANT\nwhich is not a constant expression whose equivalent value is known at query planning time.\n#\nSUBQUERY\n_EXPRESSION\nwhich contains subquery expressions.\n#\nUNRESOLVED\n_EXPRESSION\nwhich fails to resolve as a valid expression.\n42701\n#\nDUPLICATE\n_ASSIGNMENTS\nThe columns or variables\n<nameList>\nappear more than once as assignment targets.\n42701\n#\nEXEC\n_IMMEDIATE\n_DUPLICATE\n_ARGUMENT\n_ALIASES\nThe USING clause of this EXECUTE IMMEDIATE command contained multiple arguments with same alias (\n<aliases>\n), which is invalid; please update the command to specify unique aliases and then try it again.\n42702\n#\nAMBIGUOUS\n_COLUMN\n_OR\n_FIELD\nColumn or field\n<name>\nis ambiguous and has\n<n>\nmatches.\n42702\n#\nAMBIGUOUS\n_COLUMN\n_REFERENCE\nColumn\n<name>\nis ambiguous. It's because you joined several DataFrame together, and some of these DataFrames are the same. This column points to one of the DataFrames but Spark is unable to figure out which one. Please alias the DataFrames with different names via\nDataFrame.alias\nbefore joining them, and specify the column using qualified name, e.g.\ndf.alias(\"a\").join(df.alias(\"b\"), col(\"a.id\") > col(\"b.id\"))\n.\n42702\n#\nAMBIGUOUS\n_LATERAL\n_COLUMN\n_ALIAS\nLateral column alias\n<name>\nis ambiguous and has\n<n>\nmatches.\n42702\n#\nEXCEPT\n_OVERLAPPING\n_COLUMNS\nColumns in an EXCEPT list must be distinct and non-overlapping, but got (\n<columns>\n).\n42703\n#\nCOLUMN\n_NOT\n_DEFINED\n_IN\n_TABLE\n<colType>\ncolumn\n<colName>\nis not defined in table\n<tableName>\n, defined table columns are:\n<tableCols>\n.\n42703\n#\nCOLUMN\n_NOT\n_FOUND\nThe column\n<colName>\ncannot be found. Verify the spelling and correctness of the column name according to the SQL config\n<caseSensitiveConfig>\n.\n42703\n#\nUNRESOLVED\n_COLUMN\nA column, variable, or function parameter with name\n<objectName>\ncannot be resolved.\n#\nWITHOUT\n_SUGGESTION\n#\nWITH\n_SUGGESTION\nDid you mean one of the following? [\n<proposal>\n].\n42703\n#\nUNRESOLVED\n_FIELD\nA field with name\n<fieldName>\ncannot be resolved with the struct-type column\n<columnPath>\n.\n#\nWITHOUT\n_SUGGESTION\n#\nWITH\n_SUGGESTION\nDid you mean one of the following? [\n<proposal>\n].\n42703\n#\nUNRESOLVED\n_MAP\n_KEY\nCannot resolve column\n<objectName>\nas a map key. If the key is a string literal, add the single quotes '' around it.\n#\nWITHOUT\n_SUGGESTION\n#\nWITH\n_SUGGESTION\nOtherwise did you mean one of the following column(s)? [\n<proposal>\n].\n42703\n#\nUNRESOLVED\n_USING\n_COLUMN\n_FOR\n_JOIN\nUSING column\n<colName>\ncannot be resolved on the\n<side>\nside of the join. The\n<side>\n-side columns: [\n<suggestion>\n].\n42704\n#\nAMBIGUOUS\n_REFERENCE\nReference\n<name>\nis ambiguous, could be:\n<referenceNames>\n.\n42704\n#\nCANNOT\n_RESOLVE\n_DATAFRAME\n_COLUMN\nCannot resolve dataframe column\n<name>\n. It's probably because of illegal references like\ndf1.select(df2.col(\"a\"))\n.\n42704\n#\nCANNOT\n_RESOLVE\n_STAR\n_EXPAND\nCannot resolve\n<targetString>\n.* given input columns\n<columns>\n. Please check that the specified table or struct exists and is accessible in the input columns.\n42704\n#\nCODEC\n_SHORT\n_NAME\n_NOT\n_FOUND\nCannot find a short name for the codec\n<codecName>\n.\n42704\n#\nCOLLATION\n_INVALID\n_NAME\nThe value\n<collationName>\ndoes not represent a correct collation name. Suggested valid collation names: [\n<proposals>\n].\n42704\n#\nCOLLATION\n_INVALID\n_PROVIDER\nThe value\n<provider>\ndoes not represent a correct collation provider. Supported providers are: [\n<supportedProviders>\n].\n42704\n#\nDATA\n_SOURCE\n_NOT\n_EXIST\nData source '\n<provider>\n' not found. Please make sure the data source is registered.\n42704\n#\nDEFAULT\n_DATABASE\n_NOT\n_EXISTS\nDefault database\n<defaultDatabase>\ndoes not exist, please create it first or change default database to\n<defaultDatabase>\n.\n42704\n#\nENCODER\n_NOT\n_FOUND\nNot found an encoder of the type\n<typeName>\nto Spark SQL internal representation. Consider to change the input type to one of supported at '\n<docroot>\n/sql-ref-datatypes.html'.\n42704\n#\nFIELD\n_NOT\n_FOUND\nNo such struct field\n<fieldName>\nin\n<fields>\n.\n42704\n#\nINDEX\n_NOT\n_FOUND\nCannot find the index\n<indexName>\non table\n<tableName>\n.\n42704\n#\nSCHEMA\n_NOT\n_FOUND\nThe schema\n<schemaName>\ncannot be found. Verify the spelling and correctness of the schema and catalog. If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog. To tolerate the error on drop use DROP SCHEMA IF EXISTS.\n42704\n#\nUNRECOGNIZED\n_SQL\n_TYPE\nUnrecognized SQL type - name:\n<typeName>\n, id:\n<jdbcType>\n.\n42704\n#\nUNRECOGNIZED\n_STATISTIC\nThe statistic\n<stats>\nis not recognized. Valid statistics include\ncount\n,\ncount_distinct\n,\napprox_count_distinct\n,\nmean\n,\nstddev\n,\nmin\n,\nmax\n, and percentile values. Percentile must be a numeric value followed by '%', within the range 0% to 100%.\n42710\n#\nALTER\n_TABLE\n_COLUMN\n_DESCRIPTOR\n_DUPLICATE\nALTER TABLE\n<type>\ncolumn\n<columnName>\nspecifies descriptor \"\n<optionName>\n\" more than once, which is invalid.\n42710\n#\nCREATE\n_TABLE\n_COLUMN\n_DESCRIPTOR\n_DUPLICATE\nCREATE TABLE column\n<columnName>\nspecifies descriptor \"\n<optionName>\n\" more than once, which is invalid.\n42710\n#\nDATA\n_SOURCE\n_ALREADY\n_EXISTS\nData source '\n<provider>\n' already exists. Please choose a different name for the new data source.\n42710\n#\nDUPLICATED\n_METRICS\n_NAME\nThe metric name is not unique:\n<metricName>\n. The same name cannot be used for metrics with different results. However multiple instances of metrics with with same result and name are allowed (e.g. self-joins).\n42710\n#\nFIELD\n_ALREADY\n_EXISTS\nCannot\n<op>\ncolumn, because\n<fieldNames>\nalready exists in\n<struct>\n.\n42710\n#\nFOUND\n_MULTIPLE\n_DATA\n_SOURCES\nDetected multiple data sources with the name '\n<provider>\n'. Please check the data source isn't simultaneously registered and located in the classpath.\n42710\n#\nINDEX\n_ALREADY\n_EXISTS\nCannot create the index\n<indexName>\non table\n<tableName>\nbecause it already exists.\n42710\n#\nLOCATION\n_ALREADY\n_EXISTS\nCannot name the managed table as\n<identifier>\n, as its associated location\n<location>\nalready exists. Please pick a different table name, or remove the existing location first.\n42710\n#\nMULTIPLE\n_XML\n_DATA\n_SOURCE\nDetected multiple data sources with the name\n<provider>\n(\n<sourceNames>\n). Please specify the fully qualified class name or remove\n<externalSource>\nfrom the classpath.\n42711\n#\nCOLUMN\n_ALREADY\n_EXISTS\nThe column\n<columnName>\nalready exists. Choose another name or rename the existing column.\n42711\n#\nDUPLICATE\n_ROUTINE\n_RETURNS\n_COLUMNS\nFound duplicate column(s) in the RETURNS clause column list of the user-defined routine\n<routineName>\n:\n<columns>\n.\n42713\n#\nARTIFACT\n_ALREADY\n_EXISTS\nThe artifact\n<normalizedRemoteRelativePath>\nalready exists. Please choose a different name for the new artifact because it cannot be overwritten.\n42713\n#\nDUPLICATED\n_FIELD\n_NAME\n_IN\n_ARROW\n_STRUCT\nDuplicated field names in Arrow Struct are not allowed, got\n<fieldNames>\n.\n42713\n#\nSTATIC\n_PARTITION\n_COLUMN\n_IN\n_INSERT\n_COLUMN\n_LIST\nStatic partition column\n<staticName>\nis also specified in the column list.\n42723\n#\nROUTINE\n_ALREADY\n_EXISTS\nCannot create the\n<newRoutineType>\n<routineName>\nbecause a\n<existingRoutineType>\nof that name already exists. Choose a different name, drop or replace the existing\n<existingRoutineType>\n, or add the IF NOT EXISTS clause to tolerate a pre-existing\n<newRoutineType>\n.\n42723\n#\nVARIABLE\n_ALREADY\n_EXISTS\nCannot create the variable\n<variableName>\nbecause it already exists. Choose a different name, or drop or replace the existing variable.\n42734\n#\nDUPLICATE\n_CONDITION\n_IN\n_SCOPE\nFound duplicate condition\n<condition>\nin the scope. Please, remove one of them.\n42734\n#\nDUPLICATE\n_EXCEPTION\n_HANDLER\nFound duplicate handlers. Please, remove one of them.\n#\nCONDITION\nFound duplicate handlers for the same condition\n<condition>\n.\n#\nSQLSTATE\nFound duplicate handlers for the same SQLSTATE\n<sqlState>\n.\n42734\n#\nDUPLICATE\n_ROUTINE\n_PARAMETER\n_NAMES\nFound duplicate name(s) in the parameter list of the user-defined routine\n<routineName>\n:\n<names>\n.\n4274K\n#\nDUPLICATE\n_ROUTINE\n_PARAMETER\n_ASSIGNMENT\nCall to routine\n<routineName>\nis invalid because it includes multiple argument assignments to the same parameter name\n<parameterName>\n.\n#\nBOTH\n_POSITIONAL\n_AND\n_NAMED\nA positional argument and named argument both referred to the same parameter. Please remove the named argument referring to this parameter.\n#\nDOUBLE\n_NAMED\n_ARGUMENT\n_REFERENCE\nMore than one named argument referred to the same parameter. Please assign a value only once.\n4274K\n#\nNAMED\n_PARAMETERS\n_NOT\n_SUPPORTED\nNamed parameters are not supported for function\n<functionName>\n; please retry the query with positional arguments to the function call instead.\n4274K\n#\nREQUIRED\n_PARAMETER\n_NOT\n_FOUND\nCannot invoke routine\n<routineName>\nbecause the parameter named\n<parameterName>\nis required, but the routine call did not supply a value. Please update the routine call to supply an argument value (either positionally at index\n<index>\nor by name) and retry the query again.\n4274K\n#\nUNEXPECTED\n_POSITIONAL\n_ARGUMENT\nCannot invoke routine\n<routineName>\nbecause it contains positional argument(s) following the named argument assigned to\n<parameterName>\n; please rearrange them so the positional arguments come first and then retry the query again.\n4274K\n#\nUNRECOGNIZED\n_PARAMETER\n_NAME\nCannot invoke routine\n<routineName>\nbecause the routine call included a named argument reference for the argument named\n<argumentName>\n, but this routine does not include any signature containing an argument with this name. Did you mean one of the following? [\n<proposal>\n].\n42802\n#\nASSIGNMENT\n_ARITY\n_MISMATCH\nThe number of columns or variables assigned or aliased:\n<numTarget>\ndoes not match the number of source expressions:\n<numExpr>\n.\n42802\n#\nSTATEFUL\n_PROCESSOR\n_CANNOT\n_PERFORM\n_OPERATION\n_WITH\n_INVALID\n_HANDLE\n_STATE\nFailed to perform stateful processor operation=\n<operationType>\nwith invalid handle state=\n<handleState>\n.\n42802\n#\nSTATEFUL\n_PROCESSOR\n_CANNOT\n_PERFORM\n_OPERATION\n_WITH\n_INVALID\n_TIME\n_MODE\nFailed to perform stateful processor operation=\n<operationType>\nwith invalid timeMode=\n<timeMode>\n42802\n#\nSTATEFUL\n_PROCESSOR\n_DUPLICATE\n_STATE\n_VARIABLE\n_DEFINED\nState variable with name\n<stateVarName>\nhas already been defined in the StatefulProcessor.\n42802\n#\nSTATEFUL\n_PROCESSOR\n_INCORRECT\n_TIME\n_MODE\n_TO\n_ASSIGN\n_TTL\nCannot use TTL for state=\n<stateName>\nin timeMode=\n<timeMode>\n, use TimeMode.ProcessingTime() instead.\n42802\n#\nSTATEFUL\n_PROCESSOR\n_TTL\n_DURATION\n_MUST\n_BE\n_POSITIVE\nTTL duration must be greater than zero for State store operation=\n<operationType>\non state=\n<stateName>\n.\n42802\n#\nSTATEFUL\n_PROCESSOR\n_UNKNOWN\n_TIME\n_MODE\nUnknown time mode\n<timeMode>\n. Accepted timeMode modes are 'none', 'processingTime', 'eventTime'\n42802\n#\nSTATE\n_STORE\n_CANNOT\n_CREATE\n_COLUMN\n_FAMILY\n_WITH\n_RESERVED\n_CHARS\nFailed to create column family with unsupported starting character and name=\n<colFamilyName>\n.\n42802\n#\nSTATE\n_STORE\n_CANNOT\n_USE\n_COLUMN\n_FAMILY\n_WITH\n_INVALID\n_NAME\nFailed to perform column family operation=\n<operationName>\nwith invalid name=\n<colFamilyName>\n. Column family name cannot be empty or include leading/trailing spaces or use the reserved keyword=default\n42802\n#\nSTATE\n_STORE\n_COLUMN\n_FAMILY\n_SCHEMA\n_INCOMPATIBLE\nIncompatible schema transformation with column family=\n<colFamilyName>\n, oldSchema=\n<oldSchema>\n, newSchema=\n<newSchema>\n.\n42802\n#\nSTATE\n_STORE\n_HANDLE\n_NOT\n_INITIALIZED\nThe handle has not been initialized for this StatefulProcessor. Please only use the StatefulProcessor within the transformWithState operator.\n42802\n#\nSTATE\n_STORE\n_INCORRECT\n_NUM\n_ORDERING\n_COLS\n_FOR\n_RANGE\n_SCAN\nIncorrect number of ordering ordinals=\n<numOrderingCols>\nfor range scan encoder. The number of ordering ordinals cannot be zero or greater than number of schema columns.\n42802\n#\nSTATE\n_STORE\n_INCORRECT\n_NUM\n_PREFIX\n_COLS\n_FOR\n_PREFIX\n_SCAN\nIncorrect number of prefix columns=\n<numPrefixCols>\nfor prefix scan encoder. Prefix columns cannot be zero or greater than or equal to num of schema columns.\n42802\n#\nSTATE\n_STORE\n_NULL\n_TYPE\n_ORDERING\n_COLS\n_NOT\n_SUPPORTED\nNull type ordering column with name=\n<fieldName>\nat index=\n<index>\nis not supported for range scan encoder.\n42802\n#\nSTATE\n_STORE\n_UNSUPPORTED\n_OPERATION\n_ON\n_MISSING\n_COLUMN\n_FAMILY\nState store operation=\n<operationType>\nnot supported on missing column family=\n<colFamilyName>\n.\n42802\n#\nSTATE\n_STORE\n_VARIABLE\n_SIZE\n_ORDERING\n_COLS\n_NOT\n_SUPPORTED\nVariable size ordering column with name=\n<fieldName>\nat index=\n<index>\nis not supported for range scan encoder.\n42802\n#\nUDTF\n_ALIAS\n_NUMBER\n_MISMATCH\nThe number of aliases supplied in the AS clause does not match the number of columns output by the UDTF. Expected\n<aliasesSize>\naliases, but got\n<aliasesNames>\n. Please ensure that the number of aliases provided matches the number of columns output by the UDTF.\n42802\n#\nUDTF\n_INVALID\n_ALIAS\n_IN\n_REQUESTED\n_ORDERING\n_STRING\n_FROM\n_ANALYZE\n_METHOD\nFailed to evaluate the user-defined table function because its 'analyze' method returned a requested OrderingColumn whose column name expression included an unnecessary alias\n<aliasName>\n; please remove this alias and then try the query again.\n42802\n#\nUDTF\n_INVALID\n_REQUESTED\n_SELECTED\n_EXPRESSION\n_FROM\n_ANALYZE\n_METHOD\n_REQUIRES\n_ALIAS\nFailed to evaluate the user-defined table function because its 'analyze' method returned a requested 'select' expression (\n<expression>\n) that does not include a corresponding alias; please update the UDTF to specify an alias there and then try the query again.\n42803\n#\nGROUPING\n_COLUMN\n_MISMATCH\nColumn of grouping (\n<grouping>\n) can't be found in grouping columns\n<groupingColumns>\n.\n42803\n#\nGROUPING\n_ID\n_COLUMN\n_MISMATCH\nColumns of grouping_id (\n<groupingIdColumn>\n) does not match grouping columns (\n<groupByColumns>\n).\n42803\n#\nMISSING\n_AGGREGATION\nThe non-aggregating expression\n<expression>\nis based on columns which are not participating in the GROUP BY clause. Add the columns or the expression to the GROUP BY, aggregate the expression, or use\n<expressionAnyValue>\nif you do not care which of the values within a group is returned.\n42803\n#\nMISSING\n_GROUP\n_BY\nThe query does not include a GROUP BY clause. Add GROUP BY or turn it into the window functions using OVER clauses.\n42803\n#\nUNRESOLVED\n_ALL\n_IN\n_GROUP\n_BY\nCannot infer grouping columns for GROUP BY ALL based on the select clause. Please explicitly specify the grouping columns.\n42804\n#\nINVALID\n_CORRUPT\n_RECORD\n_TYPE\nThe column\n<columnName>\nfor corrupt records must have the nullable STRING type, but got\n<actualType>\n.\n42804\n#\nTRANSPOSE\n_INVALID\n_INDEX\n_COLUMN\nInvalid index column for TRANSPOSE because:\n<reason>\n42805\n#\nGROUP\n_BY\n_POS\n_OUT\n_OF\n_RANGE\nGROUP BY position\n<index>\nis not in select list (valid range is [1,\n<size>\n]).\n42805\n#\nORDER\n_BY\n_POS\n_OUT\n_OF\n_RANGE\nORDER BY position\n<index>\nis not in select list (valid range is [1,\n<size>\n]).\n42809\n#\nEXPECT\n_PERMANENT\n_VIEW\n_NOT\n_TEMP\n'\n<operation>\n' expects a permanent view but\n<viewName>\nis a temp view.\n42809\n#\nEXPECT\n_TABLE\n_NOT\n_VIEW\n'\n<operation>\n' expects a table but\n<viewName>\nis a view.\n#\nNO\n_ALTERNATIVE\n#\nUSE\n_ALTER\n_VIEW\nPlease use ALTER VIEW instead.\n42809\n#\nEXPECT\n_VIEW\n_NOT\n_TABLE\nThe table\n<tableName>\ndoes not support\n<operation>\n.\n#\nNO\n_ALTERNATIVE\n#\nUSE\n_ALTER\n_TABLE\nPlease use ALTER TABLE instead.\n42809\n#\nFORBIDDEN\n_OPERATION\nThe operation\n<statement>\nis not allowed on the\n<objectType>\n:\n<objectName>\n.\n42809\n#\nNOT\n_A\n_PARTITIONED\n_TABLE\nOperation\n<operation>\nis not allowed for\n<tableIdentWithDB>\nbecause it is not a partitioned table.\n42809\n#\nUNSUPPORTED\n_INSERT\nCan't insert into the target.\n#\nMULTI\n_PATH\nCan only write data to relations with a single path but given paths are\n<paths>\n.\n#\nNOT\n_ALLOWED\nThe target relation\n<relationId>\ndoes not allow insertion.\n#\nNOT\n_PARTITIONED\nThe target relation\n<relationId>\nis not partitioned.\n#\nRDD\n_BASED\nAn RDD-based table is not allowed.\n#\nREAD\n_FROM\nThe target relation\n<relationId>\nis also being read from.\n42809\n#\nWRONG\n_COMMAND\n_FOR\n_OBJECT\n_TYPE\nThe operation\n<operation>\nrequires a\n<requiredType>\n. But\n<objectName>\nis a\n<foundType>\n. Use\n<alternative>\ninstead.\n42815\n#\nEMITTING\n_ROWS\n_OLDER\n_THAN\n_WATERMARK\n_NOT\n_ALLOWED\nPrevious node emitted a row with eventTime=\n<emittedRowEventTime>\nwhich is older than current_watermark_value=\n<currentWatermark>\nThis can lead to correctness issues in the stateful operators downstream in the execution pipeline. Please correct the operator logic to emit rows after current global watermark value.\n42818\n#\nINCOMPARABLE\n_PIVOT\n_COLUMN\nInvalid pivot column\n<columnName>\n. Pivot columns must be comparable.\n42822\n#\nEXPRESSION\n_TYPE\n_IS\n_NOT\n_ORDERABLE\nColumn expression\n<expr>\ncannot be sorted because its type\n<exprType>\nis not orderable.\n42822\n#\nGROUP\n_EXPRESSION\n_TYPE\n_IS\n_NOT\n_ORDERABLE\nThe expression\n<sqlExpr>\ncannot be used as a grouping expression because its data type\n<dataType>\nis not an orderable data type.\n42822\n#\nHINT\n_UNSUPPORTED\n_FOR\n_JDBC\n_DIALECT\nThe option\nhint\nis not supported for\n<jdbcDialect>\nin JDBC data source. Supported dialects are\nMySQLDialect\n,\nOracleDialect\nand\nDatabricksDialect\n.\n42823\n#\nINVALID\n_SUBQUERY\n_EXPRESSION\nInvalid subquery:\n#\nSCALAR\n_SUBQUERY\n_RETURN\n_MORE\n_THAN\n_ONE\n_OUTPUT\n_COLUMN\nScalar subquery must return only one column, but got\n<number>\n.\n#\nSTREAMING\n_QUERY\nStreaming query is not allowed in subquery expressions.\n42825\n#\nCANNOT\n_MERGE\n_INCOMPATIBLE\n_DATA\n_TYPE\nFailed to merge incompatible data types\n<left>\nand\n<right>\n. Please check the data types of the columns being merged and ensure that they are compatible. If necessary, consider casting the columns to compatible data types before attempting the merge.\n42825\n#\nINCOMPATIBLE\n_COLUMN\n_TYPE\n<operator>\ncan only be performed on tables with compatible column types. The\n<columnOrdinalNumber>\ncolumn of the\n<tableOrdinalNumber>\ntable is\n<dataType1>\ntype which is not compatible with\n<dataType2>\nat the same column of the first table.\n<hint>\n.\n42826\n#\nNUM\n_COLUMNS\n_MISMATCH\n<operator>\ncan only be performed on inputs with the same number of columns, but the first input has\n<firstNumColumns>\ncolumns and the\n<invalidOrdinalNum>\ninput has\n<invalidNumColumns>\ncolumns.\n42826\n#\nNUM\n_TABLE\n_VALUE\n_ALIASES\n_MISMATCH\nNumber of given aliases does not match number of output columns. Function name:\n<funcName>\n; number of aliases:\n<aliasesNum>\n; number of output columns:\n<outColsNum>\n.\n42836\n#\nINVALID\n_RECURSIVE\n_CTE\nInvalid recursive definition found. Recursive queries must contain an UNION or an UNION ALL statement with 2 children. The first child needs to be the anchor term without any recursive references.\n42836\n#\nINVALID\n_RECURSIVE\n_REFERENCE\nInvalid recursive reference found inside WITH RECURSIVE clause.\n#\nNUMBER\nMultiple self-references to one recursive CTE are not allowed.\n#\nPLACE\nRecursive references cannot be used on the right side of left outer/semi/anti joins, on the left side of right outer joins, in full outer joins, in aggregates, and in subquery expressions.\n42836\n#\nRECURSIVE\n_CTE\n_IN\n_LEGACY\n_MODE\nRecursive definitions cannot be used in legacy CTE precedence mode (spark.sql.legacy.ctePrecedencePolicy=LEGACY).\n42836\n#\nRECURSIVE\n_CTE\n_WHEN\n_INLINING\n_IS\n_FORCED\nRecursive definitions cannot be used when CTE inlining is forced.\n42845\n#\nAGGREGATE\n_FUNCTION\n_WITH\n_NONDETERMINISTIC\n_EXPRESSION\nNon-deterministic expression\n<sqlExpr>\nshould not appear in the arguments of an aggregate function.\n42846\n#\nCANNOT\n_CAST\n_DATATYPE\nCannot cast\n<sourceType>\nto\n<targetType>\n.\n42846\n#\nCANNOT\n_CONVERT\n_PROTOBUF\n_FIELD\n_TYPE\n_TO\n_SQL\n_TYPE\nCannot convert Protobuf\n<protobufColumn>\nto SQL\n<sqlColumn>\nbecause schema is incompatible (protobufType =\n<protobufType>\n, sqlType =\n<sqlType>\n).\n42846\n#\nCANNOT\n_CONVERT\n_PROTOBUF\n_MESSAGE\n_TYPE\n_TO\n_SQL\n_TYPE\nUnable to convert\n<protobufType>\nof Protobuf to SQL type\n<toType>\n.\n42846\n#\nCANNOT\n_CONVERT\n_SQL\n_TYPE\n_TO\n_PROTOBUF\n_FIELD\n_TYPE\nCannot convert SQL\n<sqlColumn>\nto Protobuf\n<protobufColumn>\nbecause schema is incompatible (protobufType =\n<protobufType>\n, sqlType =\n<sqlType>\n).\n42846\n#\nCANNOT\n_CONVERT\n_SQL\n_VALUE\n_TO\n_PROTOBUF\n_ENUM\n_TYPE\nCannot convert SQL\n<sqlColumn>\nto Protobuf\n<protobufColumn>\nbecause\n<data>\nis not in defined values for enum:\n<enumString>\n.\n42846\n#\nCANNOT\n_UP\n_CAST\n_DATATYPE\nCannot up cast\n<expression>\nfrom\n<sourceType>\nto\n<targetType>\n.\n<details>\n42846\n#\nEXPRESSION\n_DECODING\n_FAILED\nFailed to decode a row to a value of the expressions:\n<expressions>\n.\n42846\n#\nEXPRESSION\n_ENCODING\n_FAILED\nFailed to encode a value of the expressions:\n<expressions>\nto a row.\n42846\n#\nINVALID\n_PARTITION\n_VALUE\nFailed to cast value\n<value>\nto data type\n<dataType>\nfor partition column\n<columnName>\n. Ensure the value matches the expected data type for this partition column.\n42846\n#\nPARQUET\n_CONVERSION\n_FAILURE\nUnable to create a Parquet converter for the data type\n<dataType>\nwhose Parquet type is\n<parquetType>\n.\n#\nDECIMAL\nParquet DECIMAL type can only be backed by INT32, INT64, FIXED_LEN_BYTE_ARRAY, or BINARY.\n#\nUNSUPPORTED\nPlease modify the conversion making sure it is supported.\n#\nWITHOUT\n_DECIMAL\n_METADATA\nPlease read this column/field as Spark BINARY type.\n42846\n#\nPARQUET\n_TYPE\n_ILLEGAL\nIllegal Parquet type:\n<parquetType>\n.\n42846\n#\nPARQUET\n_TYPE\n_NOT\n_RECOGNIZED\nUnrecognized Parquet type:\n<field>\n.\n42846\n#\nPARQUET\n_TYPE\n_NOT\n_SUPPORTED\nParquet type not yet supported:\n<parquetType>\n.\n42846\n#\nUNEXPECTED\n_SERIALIZER\n_FOR\n_CLASS\nThe class\n<className>\nhas an unexpected expression serializer. Expects \"STRUCT\" or \"IF\" which returns \"STRUCT\" but found\n<expr>\n.\n42883\n#\nROUTINE\n_NOT\n_FOUND\nThe routine\n<routineName>\ncannot be found. Verify the spelling and correctness of the schema and catalog. If you did not qualify the name with a schema and catalog, verify the current_schema() output, or qualify the name with the correct schema and catalog. To tolerate the error on drop use DROP ... IF EXISTS.\n42883\n#\nUNRESOLVABLE\n_TABLE\n_VALUED\n_FUNCTION\nCould not resolve\n<name>\nto a table-valued function. Please make sure that\n<name>\nis defined as a table-valued function and that all required parameters are provided correctly. If\n<name>\nis not defined, please create the table-valued function before using it. For more information about defining table-valued functions, please refer to the Apache Spark documentation.\n42883\n#\nUNRESOLVED\n_ROUTINE\nCannot resolve routine\n<routineName>\non search path\n<searchPath>\n.\n42883\n#\nUNRESOLVED\n_VARIABLE\nCannot resolve variable\n<variableName>\non search path\n<searchPath>\n.\n42883\n#\nVARIABLE\n_NOT\n_FOUND\nThe variable\n<variableName>\ncannot be found. Verify the spelling and correctness of the schema and catalog. If you did not qualify the name with a schema and catalog, verify the current_schema() output, or qualify the name with the correct schema and catalog. To tolerate the error on drop use DROP VARIABLE IF EXISTS.\n428B3\n#\nINVALID\n_SQLSTATE\nInvalid SQLSTATE value: '\n<sqlState>\n'. SQLSTATE must be exactly 5 characters long and contain only A-Z and 0-9. SQLSTATE must not start with '00', '01', or 'XX'.\n428C4\n#\nUNPIVOT\n_VALUE\n_SIZE\n_MISMATCH\nAll unpivot value columns must have the same size as there are value column names (\n<names>\n).\n428EK\n#\nTEMP\n_VIEW\n_NAME\n_TOO\n_MANY\n_NAME\n_PARTS\nCREATE TEMPORARY VIEW or the corresponding Dataset APIs only accept single-part view names, but got:\n<actualName>\n.\n428FR\n#\nCANNOT\n_ALTER\n_COLLATION\n_BUCKET\n_COLUMN\nALTER TABLE (ALTER|CHANGE) COLUMN cannot change collation of type/subtypes of bucket columns, but found the bucket column\n<columnName>\nin the table\n<tableName>\n.\n428FR\n#\nCANNOT\n_ALTER\n_PARTITION\n_COLUMN\nALTER TABLE (ALTER|CHANGE) COLUMN is not supported for partition columns, but found the partition column\n<columnName>\nin the table\n<tableName>\n.\n428FT\n#\nPARTITIONS\n_ALREADY\n_EXIST\nCannot ADD or RENAME TO partition(s)\n<partitionList>\nin table\n<tableName>\nbecause they already exist. Choose a different name, drop the existing partition, or add the IF NOT EXISTS clause to tolerate a pre-existing partition.\n428FT\n#\nPARTITIONS\n_NOT\n_FOUND\nThe partition(s)\n<partitionList>\ncannot be found in table\n<tableName>\n. Verify the partition specification and table name. To tolerate the error on drop use ALTER TABLE … DROP IF EXISTS PARTITION.\n428H2\n#\nEXCEPT\n_NESTED\n_COLUMN\n_INVALID\n_TYPE\nEXCEPT column\n<columnName>\nwas resolved and expected to be StructType, but found type\n<dataType>\n.\n428H2\n#\nIDENTITY\n_COLUMNS\n_UNSUPPORTED\n_DATA\n_TYPE\nDataType\n<dataType>\nis not supported for IDENTITY columns.\n42902\n#\nUNSUPPORTED\n_OVERWRITE\nCan't overwrite the target that is also being read from.\n#\nPATH\nThe target path is\n<path>\n.\n#\nTABLE\nThe target table is\n<table>\n.\n42903\n#\nGROUP\n_BY\n_AGGREGATE\nAggregate functions are not allowed in GROUP BY, but found\n<sqlExpr>\n.\n42903\n#\nGROUP\n_BY\n_POS\n_AGGREGATE\nGROUP BY\n<index>\nrefers to an expression\n<aggExpr>\nthat contains an aggregate function. Aggregate functions are not allowed in GROUP BY.\n42903\n#\nINVALID\n_AGGREGATE\n_FILTER\nThe FILTER expression\n<filterExpr>\nin an aggregate function is invalid.\n#\nCONTAINS\n_AGGREGATE\nExpected a FILTER expression without an aggregation, but found\n<aggExpr>\n.\n#\nCONTAINS\n_WINDOW\n_FUNCTION\nExpected a FILTER expression without a window function, but found\n<windowExpr>\n.\n#\nNON\n_DETERMINISTIC\nExpected a deterministic FILTER expression.\n#\nNOT\n_BOOLEAN\nExpected a FILTER expression of the BOOLEAN type.\n42903\n#\nINVALID\n_WHERE\n_CONDITION\nThe WHERE condition\n<condition>\ncontains invalid expressions:\n<expressionList>\n. Rewrite the query to avoid window functions, aggregate functions, and generator functions in the WHERE clause.\n42908\n#\nSPECIFY\n_CLUSTER\n_BY\n_WITH\n_BUCKETING\n_IS\n_NOT\n_ALLOWED\nCannot specify both CLUSTER BY and CLUSTERED BY INTO BUCKETS.\n42908\n#\nSPECIFY\n_CLUSTER\n_BY\n_WITH\n_PARTITIONED\n_BY\n_IS\n_NOT\n_ALLOWED\nCannot specify both CLUSTER BY and PARTITIONED BY.\n429BB\n#\nCANNOT\n_RECOGNIZE\n_HIVE\n_TYPE\nCannot recognize hive type string:\n<fieldType>\n, column:\n<fieldName>\n. The specified data type for the field cannot be recognized by Spark SQL. Please check the data type of the specified field and ensure that it is a valid Spark SQL data type. Refer to the Spark SQL documentation for a list of valid data types and their format. If the data type is correct, please ensure that you are using a supported version of Spark SQL.\n42K01\n#\nDATATYPE\n_MISSING\n_SIZE\nDataType\n<type>\nrequires a length parameter, for example\n<type>\n(10). Please specify the length.\n42K01\n#\nINCOMPLETE\n_TYPE\n_DEFINITION\nIncomplete complex type:\n#\nARRAY\nThe definition of \"ARRAY\" type is incomplete. You must provide an element type. For example: \"ARRAY\n<elementType>\n\".\n#\nMAP\nThe definition of \"MAP\" type is incomplete. You must provide a key type and a value type. For example: \"MAP\n<TIMESTAMP, INT>\n\".\n#\nSTRUCT\nThe definition of \"STRUCT\" type is incomplete. You must provide at least one field type. For example: \"STRUCT\n<name STRING, phone DECIMAL(10, 0)>\n\".\n42K02\n#\nDATA\n_SOURCE\n_NOT\n_FOUND\nFailed to find the data source:\n<provider>\n. Make sure the provider name is correct and the package is properly registered and compatible with your Spark version.\n42K03\n#\nBATCH\n_METADATA\n_NOT\n_FOUND\nUnable to find batch\n<batchMetadataFile>\n.\n42K03\n#\nCANNOT\n_LOAD\n_PROTOBUF\n_CLASS\nCould not load Protobuf class with name\n<protobufClassName>\n.\n<explanation>\n.\n42K03\n#\nDATA\n_SOURCE\n_TABLE\n_SCHEMA\n_MISMATCH\nThe schema of the data source table does not match the expected schema. If you are using the DataFrameReader.schema API or creating a table, avoid specifying the schema. Data Source schema:\n<dsSchema>\nExpected schema:\n<expectedSchema>\n42K03\n#\nLOAD\n_DATA\n_PATH\n_NOT\n_EXISTS\nLOAD DATA input path does not exist:\n<path>\n.\n42K03\n#\nPATH\n_NOT\n_FOUND\nPath does not exist:\n<path>\n.\n42K03\n#\nRENAME\n_SRC\n_PATH\n_NOT\n_FOUND\nFailed to rename as\n<sourcePath>\nwas not found.\n42K03\n#\nSTDS\n_FAILED\n_TO\n_READ\n_OPERATOR\n_METADATA\nFailed to read the operator metadata for checkpointLocation=\n<checkpointLocation>\nand batchId=\n<batchId>\n. Either the file does not exist, or the file is corrupted. Rerun the streaming query to construct the operator metadata, and report to the corresponding communities or vendors if the error persists.\n42K03\n#\nSTDS\n_FAILED\n_TO\n_READ\n_STATE\n_SCHEMA\nFailed to read the state schema. Either the file does not exist, or the file is corrupted. options:\n<sourceOptions>\n. Rerun the streaming query to construct the state schema, and report to the corresponding communities or vendors if the error persists.\n42K03\n#\nSTREAMING\n_STATEFUL\n_OPERATOR\n_NOT\n_MATCH\n_IN\n_STATE\n_METADATA\nStreaming stateful operator name does not match with the operator in state metadata. This likely to happen when user adds/removes/changes stateful operator of existing streaming query. Stateful operators in the metadata: [\n<OpsInMetadataSeq>\n]; Stateful operators in current batch: [\n<OpsInCurBatchSeq>\n].\n42K04\n#\nFAILED\n_RENAME\n_PATH\nFailed to rename\n<sourcePath>\nto\n<targetPath>\nas destination already exists.\n42K04\n#\nPATH\n_ALREADY\n_EXISTS\nPath\n<outputPath>\nalready exists. Set mode as \"overwrite\" to overwrite the existing path.\n42K05\n#\nINVALID\n_EMPTY\n_LOCATION\nThe location name cannot be empty string, but\n<location>\nwas given.\n42K05\n#\nREQUIRES\n_SINGLE\n_PART\n_NAMESPACE\n<sessionCatalog>\nrequires a single-part namespace, but got\n<namespace>\n.\n42K05\n#\nSHOW\n_COLUMNS\n_WITH\n_CONFLICT\n_NAMESPACE\nSHOW COLUMNS with conflicting namespaces:\n<namespaceA>\n!=\n<namespaceB>\n.\n42K06\n#\nINVALID\n_OPTIONS\nInvalid options:\n#\nNON\n_MAP\n_FUNCTION\nMust use the\nmap()\nfunction for options.\n#\nNON\n_STRING\n_TYPE\nA type of keys and values in\nmap()\nmust be string, but got\n<mapType>\n.\n42K06\n#\nSTATE\n_STORE\n_INVALID\n_CONFIG\n_AFTER\n_RESTART\nCannot change\n<configName>\nfrom\n<oldConfig>\nto\n<newConfig>\nbetween restarts. Please set\n<configName>\nto\n<oldConfig>\n, or restart with a new checkpoint directory.\n42K06\n#\nSTATE\n_STORE\n_INVALID\n_PROVIDER\nThe given State Store Provider\n<inputClass>\ndoes not extend org.apache.spark.sql.execution.streaming.state.StateStoreProvider.\n42K06\n#\nSTATE\n_STORE\n_INVALID\n_VARIABLE\n_TYPE\n_CHANGE\nCannot change\n<stateVarName>\nto\n<newType>\nbetween query restarts. Please set\n<stateVarName>\nto\n<oldType>\n, or restart with a new checkpoint directory.\n42K06\n#\nSTATE\n_STORE\n_PROVIDER\n_DOES\n_NOT\n_SUPPORT\n_FINE\n_GRAINED\n_STATE\n_REPLAY\nThe given State Store Provider\n<inputClass>\ndoes not extend org.apache.spark.sql.execution.streaming.state.SupportsFineGrainedReplay. Therefore, it does not support option snapshotStartBatchId or readChangeFeed in state data source.\n42K06\n#\nSTATE\n_STORE\n_STATE\n_SCHEMA\n_FILES\n_THRESHOLD\n_EXCEEDED\nThe number of state schema files\n<numStateSchemaFiles>\nexceeds the maximum number of state schema files for this query:\n<maxStateSchemaFiles>\n. Added:\n<addedColumnFamilies>\n, Removed:\n<removedColumnFamilies>\nPlease set 'spark.sql.streaming.stateStore.stateSchemaFilesThreshold' to a higher number, or revert state schema modifications\n42K06\n#\nSTATE\n_STORE\n_VALUE\n_SCHEMA\n_EVOLUTION\n_THRESHOLD\n_EXCEEDED\nThe number of state schema evolutions\n<numSchemaEvolutions>\nexceeds the maximum number of state schema evolutions,\n<maxSchemaEvolutions>\n, allowed for this column family. Offending column family:\n<colFamilyName>\nPlease set 'spark.sql.streaming.stateStore.valueStateSchemaEvolutionThreshold' to a higher number, or revert state schema modifications\n42K07\n#\nINVALID\n_SCHEMA\nThe input schema\n<inputSchema>\nis not a valid schema string.\n#\nNON\n_STRING\n_LITERAL\nThe input expression must be string literal and not null.\n#\nNON\n_STRUCT\n_TYPE\nThe input expression should be evaluated to struct type, but got\n<dataType>\n.\n#\nPARSE\n_ERROR\nCannot parse the schema:\n<reason>\n42K08\n#\nINVALID\n_SQL\n_ARG\nThe argument\n<name>\nof\nsql()\nis invalid. Consider to replace it either by a SQL literal or by collection constructor functions such as\nmap()\n,\narray()\n,\nstruct()\n.\n42K08\n#\nNON\n_FOLDABLE\n_ARGUMENT\nThe function\n<funcName>\nrequires the parameter\n<paramName>\nto be a foldable expression of the type\n<paramType>\n, but the actual argument is a non-foldable.\n42K08\n#\nNON\n_LITERAL\n_PIVOT\n_VALUES\nLiteral expressions required for pivot values, found\n<expression>\n.\n42K08\n#\nSEED\n_EXPRESSION\n_IS\n_UNFOLDABLE\nThe seed expression\n<seedExpr>\nof the expression\n<exprWithSeed>\nmust be foldable.\n42K09\n#\nCOMPLEX\n_EXPRESSION\n_UNSUPPORTED\n_INPUT\nCannot process input data types for the expression:\n<expression>\n.\n#\nBAD\n_INPUTS\nThe input data types to\n<functionName>\nmust be valid, but found the input types\n<dataType>\n.\n#\nMISMATCHED\n_TYPES\nAll input types must be the same except nullable, containsNull, valueContainsNull flags, but found the input types\n<inputTypes>\n.\n#\nNO\n_INPUTS\nThe collection of input data types must not be empty.\n42K09\n#\nDATATYPE\n_MISMATCH\nCannot resolve\n<sqlExpr>\ndue to data type mismatch:\n#\nARRAY\n_FUNCTION\n_DIFF\n_TYPES\nInput to\n<functionName>\nshould have been\n<dataType>\nfollowed by a value with same element type, but it's [\n<leftType>\n,\n<rightType>\n].\n#\nBINARY\n_ARRAY\n_DIFF\n_TYPES\nInput to function\n<functionName>\nshould have been two\n<arrayType>\nwith same element type, but it's [\n<leftType>\n,\n<rightType>\n].\n#\nBINARY\n_OP\n_DIFF\n_TYPES\nthe left and right operands of the binary operator have incompatible types (\n<left>\nand\n<right>\n).\n#\nBINARY\n_OP\n_WRONG\n_TYPE\nthe binary operator requires the input type\n<inputType>\n, not\n<actualDataType>\n.\n#\nBLOOM\n_FILTER\n_BINARY\n_OP\n_WRONG\n_TYPE\nThe Bloom filter binary input to\n<functionName>\nshould be either a constant value or a scalar subquery expression, but it's\n<actual>\n.\n#\nBLOOM\n_FILTER\n_WRONG\n_TYPE\nInput to function\n<functionName>\nshould have been\n<expectedLeft>\nfollowed by value with\n<expectedRight>\n, but it's [\n<actual>\n].\n#\nCANNOT\n_CONVERT\n_TO\n_JSON\nUnable to convert column\n<name>\nof type\n<type>\nto JSON.\n#\nCANNOT\n_DROP\n_ALL\n_FIELDS\nCannot drop all fields in struct.\n#\nCAST\n_WITHOUT\n_SUGGESTION\ncannot cast\n<srcType>\nto\n<targetType>\n.\n#\nCAST\n_WITH\n_CONF\n_SUGGESTION\ncannot cast\n<srcType>\nto\n<targetType>\nwith ANSI mode on. If you have to cast\n<srcType>\nto\n<targetType>\n, you can set\n<config>\nas\n<configVal>\n.\n#\nCAST\n_WITH\n_FUNC\n_SUGGESTION\ncannot cast\n<srcType>\nto\n<targetType>\n. To convert values from\n<srcType>\nto\n<targetType>\n, you can use the functions\n<functionNames>\ninstead.\n#\nCREATE\n_MAP\n_KEY\n_DIFF\n_TYPES\nThe given keys of function\n<functionName>\nshould all be the same type, but they are\n<dataType>\n.\n#\nCREATE\n_MAP\n_VALUE\n_DIFF\n_TYPES\nThe given values of function\n<functionName>\nshould all be the same type, but they are\n<dataType>\n.\n#\nCREATE\n_NAMED\n_STRUCT\n_WITHOUT\n_FOLDABLE\n_STRING\nOnly foldable\nSTRING\nexpressions are allowed to appear at odd position, but they are\n<inputExprs>\n.\n#\nDATA\n_DIFF\n_TYPES\nInput to\n<functionName>\nshould all be the same type, but it's\n<dataType>\n.\n#\nFILTER\n_NOT\n_BOOLEAN\nFilter expression\n<filter>\nof type\n<type>\nis not a boolean.\n#\nHASH\n_MAP\n_TYPE\nInput to the function\n<functionName>\ncannot contain elements of the \"MAP\" type. In Spark, same maps may have different hashcode, thus hash expressions are prohibited on \"MAP\" elements. To restore previous behavior set \"spark.sql.legacy.allowHashOnMapType\" to \"true\".\n#\nHASH\n_VARIANT\n_TYPE\nInput to the function\n<functionName>\ncannot contain elements of the \"VARIANT\" type yet.\n#\nINPUT\n_SIZE\n_NOT\n_ONE\nLength of\n<exprName>\nshould be 1.\n#\nINVALID\n_ARG\n_VALUE\nThe\n<inputName>\nvalue must to be a\n<requireType>\nliteral of\n<validValues>\n, but got\n<inputValue>\n.\n#\nINVALID\n_JSON\n_MAP\n_KEY\n_TYPE\nInput schema\n<schema>\ncan only contain STRING as a key type for a MAP.\n#\nINVALID\n_JSON\n_SCHEMA\nInput schema\n<schema>\nmust be a struct, an array, a map or a variant.\n#\nINVALID\n_MAP\n_KEY\n_TYPE\nThe key of map cannot be/contain\n<keyType>\n.\n#\nINVALID\n_ORDERING\n_TYPE\nThe\n<functionName>\ndoes not support ordering on type\n<dataType>\n.\n#\nINVALID\n_ROW\n_LEVEL\n_OPERATION\n_ASSIGNMENTS\n<errors>\n#\nINVALID\n_XML\n_MAP\n_KEY\n_TYPE\nInput schema\n<schema>\ncan only contain STRING as a key type for a MAP.\n#\nIN\n_SUBQUERY\n_DATA\n_TYPE\n_MISMATCH\nThe data type of one or more elements in the left hand side of an IN subquery is not compatible with the data type of the output of the subquery. Mismatched columns: [\n<mismatchedColumns>\n], left side: [\n<leftType>\n], right side: [\n<rightType>\n].\n#\nIN\n_SUBQUERY\n_LENGTH\n_MISMATCH\nThe number of columns in the left hand side of an IN subquery does not match the number of columns in the output of subquery. Left hand side columns(length:\n<leftLength>\n): [\n<leftColumns>\n], right hand side columns(length:\n<rightLength>\n): [\n<rightColumns>\n].\n#\nMAP\n_CONCAT\n_DIFF\n_TYPES\nThe\n<functionName>\nshould all be of type map, but it's\n<dataType>\n.\n#\nMAP\n_FUNCTION\n_DIFF\n_TYPES\nInput to\n<functionName>\nshould have been\n<dataType>\nfollowed by a value with same key type, but it's [\n<leftType>\n,\n<rightType>\n].\n#\nMAP\n_ZIP\n_WITH\n_DIFF\n_TYPES\nInput to the\n<functionName>\nshould have been two maps with compatible key types, but it's [\n<leftType>\n,\n<rightType>\n].\n#\nNON\n_FOLDABLE\n_INPUT\nthe input\n<inputName>\nshould be a foldable\n<inputType>\nexpression; however, got\n<inputExpr>\n.\n#\nNON\n_STRING\n_TYPE\nall arguments of the function\n<funcName>\nmust be strings.\n#\nNON\n_STRUCT\n_TYPE\nthe input\n<inputName>\nshould be a struct expression; however, got\n<inputType>\n.\n#\nNULL\n_TYPE\nNull typed values cannot be used as arguments of\n<functionName>\n.\n#\nPARAMETER\n_CONSTRAINT\n_VIOLATION\nThe\n<leftExprName>\n(\n<leftExprValue>\n) must be\n<constraint>\nthe\n<rightExprName>\n(\n<rightExprValue>\n).\n#\nRANGE\n_FRAME\n_INVALID\n_TYPE\nThe data type\n<orderSpecType>\nused in the order specification does not support the data type\n<valueBoundaryType>\nwhich is used in the range frame.\n#\nRANGE\n_FRAME\n_MULTI\n_ORDER\nA range window frame with value boundaries cannot be used in a window specification with multiple order by expressions:\n<orderSpec>\n.\n#\nRANGE\n_FRAME\n_WITHOUT\n_ORDER\nA range window frame cannot be used in an unordered window specification.\n#\nSEQUENCE\n_WRONG\n_INPUT\n_TYPES\n<functionName>\nuses the wrong parameter type. The parameter type must conform to: 1. The start and stop expressions must resolve to the same type. 2. If start and stop expressions resolve to the\n<startType>\ntype, then the step expression must resolve to the\n<stepType>\ntype. 3. Otherwise, if start and stop expressions resolve to the\n<otherStartType>\ntype, then the step expression must resolve to the same type.\n#\nSPECIFIED\n_WINDOW\n_FRAME\n_DIFF\n_TYPES\nWindow frame bounds\n<lower>\nand\n<upper>\ndo not have the same type:\n<lowerType>\n<>\n<upperType>\n.\n#\nSPECIFIED\n_WINDOW\n_FRAME\n_INVALID\n_BOUND\nWindow frame upper bound\n<upper>\ndoes not follow the lower bound\n<lower>\n.\n#\nSPECIFIED\n_WINDOW\n_FRAME\n_UNACCEPTED\n_TYPE\nThe data type of the\n<location>\nbound\n<exprType>\ndoes not match the expected data type\n<expectedType>\n.\n#\nSPECIFIED\n_WINDOW\n_FRAME\n_WITHOUT\n_FOLDABLE\nWindow frame\n<location>\nbound\n<expression>\nis not a literal.\n#\nSPECIFIED\n_WINDOW\n_FRAME\n_WRONG\n_COMPARISON\nThe lower bound of a window frame must be\n<comparison>\nto the upper bound.\n#\nSTACK\n_COLUMN\n_DIFF\n_TYPES\nThe data type of the column (\n<columnIndex>\n) do not have the same type:\n<leftType>\n(\n<leftParamIndex>\n)\n<>\n<rightType>\n(\n<rightParamIndex>\n).\n#\nTYPE\n_CHECK\n_FAILURE\n_WITH\n_HINT\n<msg>``<hint>\n.\n#\nUNEXPECTED\n_CLASS\n_TYPE\nclass\n<className>\nnot found.\n#\nUNEXPECTED\n_INPUT\n_TYPE\nThe\n<paramIndex>\nparameter requires the\n<requiredType>\ntype, however\n<inputSql>\nhas the type\n<inputType>\n.\n#\nUNEXPECTED\n_NULL\nThe\n<exprName>\nmust not be null.\n#\nUNEXPECTED\n_RETURN\n_TYPE\nThe\n<functionName>\nrequires return\n<expectedType>\ntype, but the actual is\n<actualType>\ntype.\n#\nUNEXPECTED\n_STATIC\n_METHOD\ncannot find a static method\n<methodName>\nthat matches the argument types in\n<className>\n.\n#\nUNSUPPORTED\n_INPUT\n_TYPE\nThe input of\n<functionName>\ncan't be\n<dataType>\ntype data.\n#\nVALUE\n_OUT\n_OF\n_RANGE\nThe\n<exprName>\nmust be between\n<valueRange>\n(current value =\n<currentValue>\n).\n#\nWRONG\n_NUM\n_ARG\n_TYPES\nThe expression requires\n<expectedNum>\nargument types but the actual number is\n<actualNum>\n.\n#\nWRONG\n_NUM\n_ENDPOINTS\nThe number of endpoints must be >= 2 to construct intervals but the actual number is\n<actualNumber>\n.\n42K09\n#\nEVENT\n_TIME\n_IS\n_NOT\n_ON\n_TIMESTAMP\n_TYPE\nThe event time\n<eventName>\nhas the invalid type\n<eventType>\n, but expected \"TIMESTAMP\".\n42K09\n#\nINVALID\n_VARIABLE\n_TYPE\n_FOR\n_QUERY\n_EXECUTE\n_IMMEDIATE\nVariable type must be string type but got\n<varType>\n.\n42K09\n#\nPIVOT\n_VALUE\n_DATA\n_TYPE\n_MISMATCH\nInvalid pivot value '\n<value>\n': value data type\n<valueType>\ndoes not match pivot column data type\n<pivotType>\n.\n42K09\n#\nTRANSPOSE\n_NO\n_LEAST\n_COMMON\n_TYPE\nTranspose requires non-index columns to share a least common type, but\n<dt1>\nand\n<dt2>\ndo not.\n42K09\n#\nUNEXPECTED\n_INPUT\n_TYPE\nParameter\n<paramIndex>\nof function\n<functionName>\nrequires the\n<requiredType>\ntype, however\n<inputSql>\nhas the type\n<inputType>\n.\n42K09\n#\nUNPIVOT\n_VALUE\n_DATA\n_TYPE\n_MISMATCH\nUnpivot value columns must share a least common type, some types do not: [\n<types>\n].\n42K0A\n#\nUNPIVOT\n_REQUIRES\n_ATTRIBUTES\nUNPIVOT requires all given\n<given>\nexpressions to be columns when no\n<empty>\nexpressions are given. These are not columns: [\n<expressions>\n].\n42K0A\n#\nUNPIVOT\n_REQUIRES\n_VALUE\n_COLUMNS\nAt least one value column needs to be specified for UNPIVOT, all columns specified as ids.\n42K0B\n#\nINCONSISTENT\n_BEHAVIOR\n_CROSS\n_VERSION\nYou may get a different result due to the upgrading to\n#\nDATETIME\n_PATTERN\n_RECOGNITION\nSpark >= 3.0: Fail to recognize\n<pattern>\npattern in the DateTimeFormatter. 1) You can set\n<config>\nto \"LEGACY\" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from '\n<docroot>\n/sql-ref-datetime-pattern.html'.\n#\nDATETIME\n_WEEK\n_BASED\n_PATTERN\nSpark >= 3.0: All week-based patterns are unsupported since Spark 3.0, detected week-based character:\n<c>\n. Please use the SQL function EXTRACT instead.\n#\nPARSE\n_DATETIME\n_BY\n_NEW\n_PARSER\nSpark >= 3.0: Fail to parse\n<datetime>\nin the new parser. You can set\n<config>\nto \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n#\nREAD\n_ANCIENT\n_DATETIME\nSpark >= 3.0: reading dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z from\n<format>\nfiles can be ambiguous, as the files may be written by Spark 2.x or legacy versions of Hive, which uses a legacy hybrid calendar that is different from Spark 3.0+'s Proleptic Gregorian calendar. See more details in SPARK-31404. You can set the SQL config\n<config>\nor the datasource option\n<option>\nto \"LEGACY\" to rebase the datetime values w.r.t. the calendar difference during reading. To read the datetime values as it is, set the SQL config or the datasource option to \"CORRECTED\".\n#\nWRITE\n_ANCIENT\n_DATETIME\nSpark >= 3.0: writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z into\n<format>\nfiles can be dangerous, as the files may be read by Spark 2.x or legacy versions of Hive later, which uses a legacy hybrid calendar that is different from Spark 3.0+'s Proleptic Gregorian calendar. See more details in SPARK-31404. You can set\n<config>\nto \"LEGACY\" to rebase the datetime values w.r.t. the calendar difference during writing, to get maximum interoperability. Or set the config to \"CORRECTED\" to write the datetime values as it is, if you are sure that the written files will only be read by Spark 3.0+ or other systems that use Proleptic Gregorian calendar.\n42K0D\n#\nINVALID\n_LAMBDA\n_FUNCTION\n_CALL\nInvalid lambda function call.\n#\nDUPLICATE\n_ARG\n_NAMES\nThe lambda function has duplicate arguments\n<args>\n. Please, consider to rename the argument names or set\n<caseSensitiveConfig>\nto \"true\".\n#\nNON\n_HIGHER\n_ORDER\n_FUNCTION\nA lambda function should only be used in a higher order function. However, its class is\n<class>\n, which is not a higher order function.\n#\nNUM\n_ARGS\n_MISMATCH\nA higher order function expects\n<expectedNumArgs>\narguments, but got\n<actualNumArgs>\n.\n#\nPARAMETER\n_DOES\n_NOT\n_ACCEPT\n_LAMBDA\n_FUNCTION\nYou passed a lambda function to a parameter that does not accept it. Please check if lambda function argument is in the correct position.\n42K0E\n#\nINVALID\n_LIMIT\n_LIKE\n_EXPRESSION\nThe limit like expression\n<expr>\nis invalid.\n#\nDATA\n_TYPE\nThe\n<name>\nexpression must be integer type, but got\n<dataType>\n.\n#\nIS\n_NEGATIVE\nThe\n<name>\nexpression must be equal to or greater than 0, but got\n<v>\n.\n#\nIS\n_NULL\nThe evaluated\n<name>\nexpression must not be null.\n#\nIS\n_UNFOLDABLE\nThe\n<name>\nexpression must evaluate to a constant value.\n42K0E\n#\nINVALID\n_NON\n_DETERMINISTIC\n_EXPRESSIONS\nThe operator expects a deterministic expression, but the actual expression is\n<sqlExprs>\n.\n42K0E\n#\nINVALID\n_OBSERVED\n_METRICS\nInvalid observed metrics.\n#\nAGGREGATE\n_EXPRESSION\n_WITH\n_DISTINCT\n_UNSUPPORTED\nAggregate expressions with DISTINCT are not allowed in observed metrics, but found:\n<expr>\n.\n#\nAGGREGATE\n_EXPRESSION\n_WITH\n_FILTER\n_UNSUPPORTED\nAggregate expression with FILTER predicate are not allowed in observed metrics, but found:\n<expr>\n.\n#\nMISSING\n_NAME\nThe observed metrics should be named:\n<operator>\n.\n#\nNESTED\n_AGGREGATES\n_UNSUPPORTED\nNested aggregates are not allowed in observed metrics, but found:\n<expr>\n.\n#\nNON\n_AGGREGATE\n_FUNC\n_ARG\n_IS\n_ATTRIBUTE\nAttribute\n<expr>\ncan only be used as an argument to an aggregate function.\n#\nNON\n_AGGREGATE\n_FUNC\n_ARG\n_IS\n_NON\n_DETERMINISTIC\nNon-deterministic expression\n<expr>\ncan only be used as an argument to an aggregate function.\n#\nWINDOW\n_EXPRESSIONS\n_UNSUPPORTED\nWindow expressions are not allowed in observed metrics, but found:\n<expr>\n.\n42K0E\n#\nINVALID\n_TIME\n_TRAVEL\n_SPEC\nCannot specify both version and timestamp when time travelling the table.\n42K0E\n#\nINVALID\n_TIME\n_TRAVEL\n_TIMESTAMP\n_EXPR\nThe time travel timestamp expression\n<expr>\nis invalid.\n#\nINPUT\nCannot be casted to the \"TIMESTAMP\" type.\n#\nNON\n_DETERMINISTIC\nMust be deterministic.\n#\nOPTION\nTimestamp string in the options must be able to cast to TIMESTAMP type.\n#\nUNEVALUABLE\nMust be evaluable.\n42K0E\n#\nJOIN\n_CONDITION\n_IS\n_NOT\n_BOOLEAN\n_TYPE\nThe join condition\n<joinCondition>\nhas the invalid type\n<conditionType>\n, expected \"BOOLEAN\".\n42K0E\n#\nMULTIPLE\n_TIME\n_TRAVEL\n_SPEC\nCannot specify time travel in both the time travel clause and options.\n42K0E\n#\nMULTI\n_ALIAS\n_WITHOUT\n_GENERATOR\nMulti part aliasing (\n<names>\n) is not supported with\n<expr>\nas it is not a generator function.\n42K0E\n#\nMULTI\n_SOURCES\n_UNSUPPORTED\n_FOR\n_EXPRESSION\nThe expression\n<expr>\ndoes not support more than one source.\n42K0E\n#\nNO\n_MERGE\n_ACTION\n_SPECIFIED\ndf.mergeInto needs to be followed by at least one of whenMatched/whenNotMatched/whenNotMatchedBySource.\n42K0E\n#\nUNSUPPORTED\n_EXPR\n_FOR\n_OPERATOR\nA query operator contains one or more unsupported expressions. Consider to rewrite it to avoid window functions, aggregate functions, and generator functions in the WHERE clause. Invalid expressions: [\n<invalidExprSqls>\n]\n42K0E\n#\nUNSUPPORTED\n_EXPR\n_FOR\n_PARAMETER\nA query parameter contains unsupported expression. Parameters can either be variables or literals. Invalid expression: [\n<invalidExprSql>\n]\n42K0E\n#\nUNSUPPORTED\n_GENERATOR\nThe generator is not supported:\n#\nMULTI\n_GENERATOR\nonly one generator allowed per SELECT clause but found\n<num>\n:\n<generators>\n.\n#\nNESTED\n_IN\n_EXPRESSIONS\nnested in expressions\n<expression>\n.\n#\nNOT\n_GENERATOR\n<functionName>\nis expected to be a generator. However, its class is\n<classCanonicalName>\n, which is not a generator.\n#\nOUTSIDE\n_SELECT\noutside the SELECT clause, found:\n<plan>\n.\n42K0E\n#\nUNSUPPORTED\n_GROUPING\n_EXPRESSION\ngrouping()/grouping_id() can only be used with GroupingSets/Cube/Rollup.\n42K0E\n#\nUNSUPPORTED\n_MERGE\n_CONDITION\nMERGE operation contains unsupported\n<condName>\ncondition.\n#\nAGGREGATE\nAggregates are not allowed:\n<cond>\n.\n#\nNON\n_DETERMINISTIC\nNon-deterministic expressions are not allowed:\n<cond>\n.\n#\nSUBQUERY\nSubqueries are not allowed:\n<cond>\n.\n42K0E\n#\nUNTYPED\n_SCALA\n_UDF\nYou're using untyped Scala UDF, which does not have the input type information. Spark may blindly pass null to the Scala closure with primitive-type argument, and the closure will see the default value of the Java type for the null argument, e.g.\nudf((x: Int) => x, IntegerType)\n, the result is 0 for null input. To get rid of this error, you could: 1. use typed Scala UDF APIs(without return type parameter), e.g.\nudf((x: Int) => x)\n. 2. use Java UDF APIs, e.g.\nudf(new UDF1[String, Integer] { override def call(s: String): Integer = s.length() }, IntegerType)\n, if input types are all non primitive. 3. set \"spark.sql.legacy.allowUntypedScalaUDF\" to \"true\" and use this API with caution.\n42K0E\n#\nWINDOW\n_FUNCTION\n_AND\n_FRAME\n_MISMATCH\n<funcName>\nfunction can only be evaluated in an ordered row-based window frame with a single offset:\n<windowExpr>\n.\n42K0F\n#\nINVALID\n_TEMP\n_OBJ\n_REFERENCE\nCannot create the persistent object\n<objName>\nof the type\n<obj>\nbecause it references to the temporary object\n<tempObjName>\nof the type\n<tempObj>\n. Please make the temporary object\n<tempObjName>\npersistent, or make the persistent object\n<objName>\ntemporary.\n42K0G\n#\nPROTOBUF\n_DEPENDENCY\n_NOT\n_FOUND\nCould not find dependency:\n<dependencyName>\n.\n42K0G\n#\nPROTOBUF\n_DESCRIPTOR\n_FILE\n_NOT\n_FOUND\nError reading Protobuf descriptor file at path:\n<filePath>\n.\n42K0G\n#\nPROTOBUF\n_FIELD\n_MISSING\nSearching for\n<field>\nin Protobuf schema at\n<protobufSchema>\ngave\n<matchSize>\nmatches. Candidates:\n<matches>\n.\n42K0G\n#\nPROTOBUF\n_FIELD\n_MISSING\n_IN\n_SQL\n_SCHEMA\nFound\n<field>\nin Protobuf schema but there is no match in the SQL schema.\n42K0G\n#\nPROTOBUF\n_FIELD\n_TYPE\n_MISMATCH\nType mismatch encountered for field:\n<field>\n.\n42K0G\n#\nPROTOBUF\n_MESSAGE\n_NOT\n_FOUND\nUnable to locate Message\n<messageName>\nin Descriptor.\n42K0G\n#\nPROTOBUF\n_TYPE\n_NOT\n_SUPPORT\nProtobuf type not yet supported:\n<protobufType>\n.\n42K0G\n#\nRECURSIVE\n_PROTOBUF\n_SCHEMA\nFound recursive reference in Protobuf schema, which can not be processed by Spark by default:\n<fieldDescriptor>\n. try setting the option\nrecursive.fields.max.depth\n1 to 10. Going beyond 10 levels of recursion is not allowed.\n42K0G\n#\nUNABLE\n_TO\n_CONVERT\n_TO\n_PROTOBUF\n_MESSAGE\n_TYPE\nUnable to convert SQL type\n<toType>\nto Protobuf type\n<protobufType>\n.\n42K0G\n#\nUNKNOWN\n_PROTOBUF\n_MESSAGE\n_TYPE\nAttempting to treat\n<descriptorName>\nas a Message, but it was\n<containingType>\n.\n42K0H\n#\nRECURSIVE\n_VIEW\nRecursive view\n<viewIdent>\ndetected (cycle:\n<newPath>\n).\n42K0I\n#\nSQL\n_CONF\n_NOT\n_FOUND\nThe SQL config\n<sqlConf>\ncannot be found. Please verify that the config exists.\n42K0K\n#\nINVALID\n_WITHIN\n_GROUP\n_EXPRESSION\nInvalid function\n<funcName>\nwith WITHIN GROUP.\n#\nDISTINCT\n_UNSUPPORTED\nThe function does not support DISTINCT with WITHIN GROUP.\n#\nMISMATCH\n_WITH\n_DISTINCT\n_INPUT\nThe function is invoked with DISTINCT and WITHIN GROUP but expressions\n<funcArg>\nand\n<orderingExpr>\ndo not match. The WITHIN GROUP ordering expression must be picked from the function inputs.\n#\nWITHIN\n_GROUP\n_MISSING\nWITHIN GROUP is required for the function.\n#\nWRONG\n_NUM\n_ORDERINGS\nThe function requires\n<expectedNum>\norderings in WITHIN GROUP but got\n<actualNum>\n.\n42K0L\n#\nEND\n_LABEL\n_WITHOUT\n_BEGIN\n_LABEL\nEnd label\n<endLabel>\ncan not exist without begin label.\n42K0L\n#\nINVALID\n_LABEL\n_USAGE\nThe usage of the label\n<labelName>\nis invalid.\n#\nDOES\n_NOT\n_EXIST\nLabel was used in the\n<statementType>\nstatement, but the label does not belong to any surrounding block.\n#\nITERATE\n_IN\n_COMPOUND\nITERATE statement cannot be used with a label that belongs to a compound (BEGIN...END) body.\n#\nQUALIFIED\n_LABEL\n_NAME\nLabel cannot be qualified.\n42K0L\n#\nLABELS\n_MISMATCH\nBegin label\n<beginLabel>\ndoes not match the end label\n<endLabel>\n.\n42K0L\n#\nLABEL\n_ALREADY\n_EXISTS\nThe label\n<label>\nalready exists. Choose another name or rename the existing label.\n42K0L\n#\nLABEL\n_NAME\n_FORBIDDEN\nThe label name\n<label>\nis forbidden.\n42K0M\n#\nINVALID\n_VARIABLE\n_DECLARATION\nInvalid variable declaration.\n#\nNOT\n_ALLOWED\n_IN\n_SCOPE\nDeclaration of the variable\n<varName>\nis not allowed in this scope.\n#\nONLY\n_AT\n_BEGINNING\nVariable\n<varName>\ncan only be declared at the beginning of the compound.\n#\nQUALIFIED\n_LOCAL\n_VARIABLE\nThe variable\n<varName>\nmust be declared without a qualifier, as qualifiers are not allowed for local variable declarations.\n#\nREPLACE\n_LOCAL\n_VARIABLE\nThe variable\n<varName>\ndoes not support DECLARE OR REPLACE, as local variables cannot be replaced.\n42K0N\n#\nINVALID\n_EXTERNAL\n_TYPE\nThe external type\n<externalType>\nis not valid for the type\n<type>\nat the expression\n<expr>\n.\n42K0O\n#\nSCALAR\n_FUNCTION\n_NOT\n_COMPATIBLE\nScalarFunction\n<scalarFunc>\nnot overrides method 'produceResult(InternalRow)' with custom implementation.\n42K0P\n#\nSCALAR\n_FUNCTION\n_NOT\n_FULLY\n_IMPLEMENTED\nScalarFunction\n<scalarFunc>\nnot implements or overrides method 'produceResult(InternalRow)'.\n42K0Q\n#\nINVALID\n_HANDLER\n_DECLARATION\nInvalid handler declaration.\n#\nCONDITION\n_NOT\n_FOUND\nCondition\n<condition>\nnot found.\n#\nDUPLICATE\n_CONDITION\n_IN\n_HANDLER\n_DECLARATION\nFound duplicate condition\n<condition>\nin the handler declaration. Please, remove one of them.\n#\nDUPLICATE\n_SQLSTATE\n_IN\n_HANDLER\n_DECLARATION\nFound duplicate sqlState\n<sqlState>\nin the handler declaration. Please, remove one of them.\n#\nINVALID\n_CONDITION\n_COMBINATION\nInvalid combination of conditions in the handler declaration. SQLEXCEPTION and NOT FOUND cannot be used together with other condition/sqlstate values.\n#\nWRONG\n_PLACE\n_OF\n_DECLARATION\nHandlers must be declared after variable/condition declaration, and before other statements.\n42K0R\n#\nINVALID\n_ERROR\n_CONDITION\n_DECLARATION\nInvalid condition declaration.\n#\nNOT\n_AT\n_START\n_OF\n_COMPOUND\n_STATEMENT\nCondition\n<conditionName>\ncan only be declared at the start of a BEGIN END compound statement.\n#\nQUALIFIED\n_CONDITION\n_NAME\nCondition\n<conditionName>\ncannot be qualified.\n#\nSPECIAL\n_CHARACTER\n_FOUND\nSpecial character found in condition name\n<conditionName>\n. Only alphanumeric characters and underscores are allowed.\n42KD0\n#\nAMBIGUOUS\n_ALIAS\n_IN\n_NESTED\n_CTE\nName\n<name>\nis ambiguous in nested CTE. Please set\n<config>\nto \"CORRECTED\" so that name defined in inner CTE takes precedence. If set it to \"LEGACY\", outer CTE definitions will take precedence. See '\n<docroot>\n/sql-migration-guide.html#query-engine'.\n42KD9\n#\nCANNOT\n_MERGE\n_SCHEMAS\nFailed merging schemas: Initial schema:\n<left>\nSchema that cannot be merged with the initial schema:\n<right>\n.\n42KD9\n#\nUNABLE\n_TO\n_INFER\n_SCHEMA\nUnable to infer schema for\n<format>\n. It must be specified manually.\n42KDE\n#\nCALL\n_ON\n_STREAMING\n_DATASET\n_UNSUPPORTED\nThe method\n<methodName>\ncan not be called on streaming Dataset/DataFrame.\n42KDE\n#\nCANNOT\n_CREATE\n_DATA\n_SOURCE\n_TABLE\nFailed to create data source table\n<tableName>\n:\n#\nEXTERNAL\n_METADATA\n_UNSUPPORTED\nprovider '\n<provider>\n' does not support external metadata but a schema is provided. Please remove the schema when creating the table.\n42KDE\n#\nINVALID\n_WRITER\n_COMMIT\n_MESSAGE\nThe data source writer has generated an invalid number of commit messages. Expected exactly one writer commit message from each task, but received\n<detail>\n.\n42KDE\n#\nNON\n_TIME\n_WINDOW\n_NOT\n_SUPPORTED\n_IN\n_STREAMING\nWindow function is not supported in\n<windowFunc>\n(as column\n<columnName>\n) on streaming DataFrames/Datasets. Structured Streaming only supports time-window aggregation using the WINDOW function. (window specification:\n<windowSpec>\n)\n42KDE\n#\nSTREAMING\n_OUTPUT\n_MODE\nInvalid streaming output mode:\n<outputMode>\n.\n#\nINVALID\nAccepted output modes are 'Append', 'Complete', 'Update'.\n#\nUNSUPPORTED\n_DATASOURCE\nThis output mode is not supported in Data Source\n<className>\n.\n#\nUNSUPPORTED\n_OPERATION\nThis output mode is not supported for\n<operation>\non streaming DataFrames/DataSets.\n42KDF\n#\nXML\n_ROW\n_TAG\n_MISSING\n<rowTag>\noption is required for reading/writing files in XML format.\n42P01\n#\nTABLE\n_OR\n_VIEW\n_NOT\n_FOUND\nThe table or view\n<relationName>\ncannot be found. Verify the spelling and correctness of the schema and catalog. If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog. To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.\n42P01\n#\nVIEW\n_NOT\n_FOUND\nThe view\n<relationName>\ncannot be found. Verify the spelling and correctness of the schema and catalog. If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog. To tolerate the error on drop use DROP VIEW IF EXISTS.\n42P02\n#\nUNBOUND\n_SQL\n_PARAMETER\nFound the unbound parameter:\n<name>\n. Please, fix\nargs\nand provide a mapping of the parameter to either a SQL literal or collection constructor functions such as\nmap()\n,\narray()\n,\nstruct()\n.\n42P06\n#\nSCHEMA\n_ALREADY\n_EXISTS\nCannot create schema\n<schemaName>\nbecause it already exists. Choose a different name, drop the existing schema, or add the IF NOT EXISTS clause to tolerate pre-existing schema.\n42P07\n#\nTABLE\n_OR\n_VIEW\n_ALREADY\n_EXISTS\nCannot create table or view\n<relationName>\nbecause it already exists. Choose a different name, drop or replace the existing object, or add the IF NOT EXISTS clause to tolerate pre-existing objects.\n42P07\n#\nTEMP\n_TABLE\n_OR\n_VIEW\n_ALREADY\n_EXISTS\nCannot create the temporary view\n<relationName>\nbecause it already exists. Choose a different name, drop or replace the existing view,  or add the IF NOT EXISTS clause to tolerate pre-existing views.\n42P07\n#\nVIEW\n_ALREADY\n_EXISTS\nCannot create view\n<relationName>\nbecause it already exists. Choose a different name, drop or replace the existing object, or add the IF NOT EXISTS clause to tolerate pre-existing objects.\n42P08\n#\nCATALOG\n_NOT\n_FOUND\nThe catalog\n<catalogName>\nnot found. Consider to set the SQL config\n<config>\nto a catalog plugin.\n42P10\n#\nCLUSTERING\n_COLUMNS\n_MISMATCH\nSpecified clustering does not match that of the existing table\n<tableName>\n. Specified clustering columns: [\n<specifiedClusteringString>\n]. Existing clustering columns: [\n<existingClusteringString>\n].\n42P20\n#\nMISSING\n_WINDOW\n_SPECIFICATION\nWindow specification is not defined in the WINDOW clause for\n<windowName>\n. For more information about WINDOW clauses, please refer to '\n<docroot>\n/sql-ref-syntax-qry-select-window.html'.\n42P20\n#\nUNSUPPORTED\n_EXPR\n_FOR\n_WINDOW\nExpression\n<sqlExpr>\nnot supported within a window function.\n42P21\n#\nCOLLATION\n_MISMATCH\nCould not determine which collation to use for string functions and operators.\n#\nEXPLICIT\nError occurred due to the mismatch between explicit collations: [\n<explicitTypes>\n]. Decide on a single explicit collation and remove others.\n#\nIMPLICIT\nError occurred due to the mismatch between implicit collations: [\n<implicitTypes>\n]. Use COLLATE function to set the collation explicitly.\n42P22\n#\nINDETERMINATE\n_COLLATION\nCould not determine which collation to use for string operation. Use COLLATE clause to set the collation explicitly.\n42P22\n#\nINDETERMINATE\n_COLLATION\n_IN\n_EXPRESSION\nData type of\n<expr>\nhas indeterminate collation. Use COLLATE clause to set the collation explicitly.\n42P22\n#\nINDETERMINATE\n_COLLATION\n_IN\n_SCHEMA\nSchema contains indeterminate collation at: [\n<columnPaths>\n]. Use COLLATE clause to set the collation explicitly.\n42S22\n#\nNO\n_SQL\n_TYPE\n_IN\n_PROTOBUF\n_SCHEMA\nCannot find\n<catalystFieldPath>\nin Protobuf schema.\n42S23\n#\nPARTITION\n_TRANSFORM\n_EXPRESSION\n_NOT\n_IN\n_PARTITIONED\n_BY\nThe expression\n<expression>\nmust be inside 'partitionedBy'.\n46103\n#\nCANNOT\n_LOAD\n_FUNCTION\n_CLASS\nCannot load class\n<className>\nwhen registering the function\n<functionName>\n, please make sure it is on the classpath.\n46110\n#\nCANNOT\n_MODIFY\n_CONFIG\nCannot modify the value of the Spark config:\n<key>\n. See also '\n<docroot>\n/sql-migration-guide.html#ddl-statements'.\n46121\n#\nINVALID\n_COLUMN\n_NAME\n_AS\n_PATH\nThe datasource\n<datasource>\ncannot save the column\n<columnName>\nbecause its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.\n46121\n#\nINVALID\n_JAVA\n_IDENTIFIER\n_AS\n_FIELD\n_NAME\n<fieldName>\nis not a valid identifier of Java and cannot be used as field name\n<walkedTypePath>\n.\n51024\n#\nINCOMPATIBLE\n_VIEW\n_SCHEMA\n_CHANGE\nThe SQL query of view\n<viewName>\nhas an incompatible schema change and column\n<colName>\ncannot be resolved. Expected\n<expectedNum>\ncolumns named\n<colName>\nbut got\n<actualCols>\n. Please try to re-create the view by running:\n<suggestion>\n.\n53200\n#\nUNABLE\n_TO\n_ACQUIRE\n_MEMORY\nUnable to acquire\n<requestedBytes>\nbytes of memory, got\n<receivedBytes>\n.\n54000\n#\nCOLLECTION\n_SIZE\n_LIMIT\n_EXCEEDED\nCan't create array with\n<numberOfElements>\nelements which exceeding the array size limit\n<maxRoundedArrayLength>\n,\n#\nFUNCTION\nunsuccessful try to create arrays in the function\n<functionName>\n.\n#\nINITIALIZE\ncannot initialize an array with specified parameters.\n#\nPARAMETER\nthe value of parameter(s)\n<parameter>\nin the function\n<functionName>\nis invalid.\n54000\n#\nGROUPING\n_SIZE\n_LIMIT\n_EXCEEDED\nGrouping sets size cannot be greater than\n<maxSize>\n.\n54001\n#\nFAILED\n_TO\n_PARSE\n_TOO\n_COMPLEX\nThe statement, including potential SQL functions and referenced views, was too complex to parse. To mitigate this error divide the statement into multiple, less complex chunks.\n54006\n#\nEXCEED\n_LIMIT\n_LENGTH\nExceeds char/varchar type length limitation:\n<limit>\n.\n54006\n#\nKRYO\n_BUFFER\n_OVERFLOW\nKryo serialization failed:\n<exceptionMsg>\n. To avoid this, increase \"\n<bufferSizeConfKey>\n\" value.\n54006\n#\nTRANSPOSE\n_EXCEED\n_ROW\n_LIMIT\nNumber of rows exceeds the allowed limit of\n<maxValues>\nfor TRANSPOSE. If this was intended, set\n<config>\nto at least the current row count.\n54011\n#\nTUPLE\n_SIZE\n_EXCEEDS\n_LIMIT\nDue to Scala's limited support of tuple, tuples with more than 22 elements are not supported.\n54023\n#\nTABLE\n_VALUED\n_FUNCTION\n_TOO\n_MANY\n_TABLE\n_ARGUMENTS\nThere are too many table arguments for table-valued function. It allows one table argument, but got:\n<num>\n. If you want to allow it, please set \"spark.sql.allowMultipleTableArguments.enabled\" to \"true\"\n54K00\n#\nVIEW\n_EXCEED\n_MAX\n_NESTED\n_DEPTH\nThe depth of view\n<viewName>\nexceeds the maximum view resolution depth (\n<maxNestedDepth>\n). Analysis is aborted to avoid errors. If you want to work around this, please try to increase the value of \"spark.sql.view.maxNestedViewDepth\".\n56000\n#\nCHECKPOINT\n_RDD\n_BLOCK\n_ID\n_NOT\n_FOUND\nCheckpoint block\n<rddBlockId>\nnot found! Either the executor that originally checkpointed this partition is no longer alive, or the original RDD is unpersisted. If this problem persists, you may consider using\nrdd.checkpoint()\ninstead, which is slower than local checkpointing but more fault-tolerant.\n56038\n#\nCODEC\n_NOT\n_AVAILABLE\nThe codec\n<codecName>\nis not available.\n#\nWITH\n_AVAILABLE\n_CODECS\n_SUGGESTION\nAvailable codecs are\n<availableCodecs>\n.\n#\nWITH\n_CONF\n_SUGGESTION\nConsider to set the config\n<configKey>\nto\n<configVal>\n.\n56038\n#\nFEATURE\n_NOT\n_ENABLED\nThe feature\n<featureName>\nis not enabled. Consider setting the config\n<configKey>\nto\n<configValue>\nto enable this capability.\n56038\n#\nGET\n_TABLES\n_BY\n_TYPE\n_UNSUPPORTED\n_BY\n_HIVE\n_VERSION\nHive 2.2 and lower versions don't support getTablesByType. Please use Hive 2.3 or higher version.\n56038\n#\nINCOMPATIBLE\n_DATASOURCE\n_REGISTER\nDetected an incompatible DataSourceRegister. Please remove the incompatible library from classpath or upgrade it. Error:\n<message>\n56K00\n#\nCONNECT\nGeneric Spark Connect error.\n#\nINTERCEPTOR\n_CTOR\n_MISSING\nCannot instantiate GRPC interceptor because\n<cls>\nis missing a default constructor without arguments.\n#\nINTERCEPTOR\n_RUNTIME\n_ERROR\nError instantiating GRPC interceptor:\n<msg>\n#\nPLUGIN\n_CTOR\n_MISSING\nCannot instantiate Spark Connect plugin because\n<cls>\nis missing a default constructor without arguments.\n#\nPLUGIN\n_RUNTIME\n_ERROR\nError instantiating Spark Connect plugin:\n<msg>\n#\nSESSION\n_NOT\n_SAME\nBoth Datasets must belong to the same SparkSession.\n58030\n#\nCANNOT\n_LOAD\n_STATE\n_STORE\nAn error occurred during loading state.\n#\nCANNOT\n_FIND\n_BASE\n_SNAPSHOT\n_CHECKPOINT\nCannot find a base snapshot checkpoint with lineage:\n<lineage>\n.\n#\nCANNOT\n_READ\n_CHECKPOINT\nCannot read RocksDB checkpoint metadata. Expected\n<expectedVersion>\n, but found\n<actualVersion>\n.\n#\nCANNOT\n_READ\n_DELTA\n_FILE\n_KEY\n_SIZE\nError reading delta file\n<fileToRead>\nof\n<clazz>\n: key size cannot be\n<keySize>\n.\n#\nCANNOT\n_READ\n_DELTA\n_FILE\n_NOT\n_EXISTS\nError reading delta file\n<fileToRead>\nof\n<clazz>\n:\n<fileToRead>\ndoes not exist.\n#\nCANNOT\n_READ\n_MISSING\n_SNAPSHOT\n_FILE\nError reading snapshot file\n<fileToRead>\nof\n<clazz>\n:\n<fileToRead>\ndoes not exist.\n#\nCANNOT\n_READ\n_SNAPSHOT\n_FILE\n_KEY\n_SIZE\nError reading snapshot file\n<fileToRead>\nof\n<clazz>\n: key size cannot be\n<keySize>\n.\n#\nCANNOT\n_READ\n_SNAPSHOT\n_FILE\n_VALUE\n_SIZE\nError reading snapshot file\n<fileToRead>\nof\n<clazz>\n: value size cannot be\n<valueSize>\n.\n#\nCANNOT\n_READ\n_STREAMING\n_STATE\n_FILE\nError reading streaming state file of\n<fileToRead>\ndoes not exist. If the stream job is restarted with a new or updated state operation, please create a new checkpoint location or clear the existing checkpoint location.\n#\nHDFS\n_STORE\n_PROVIDER\n_OUT\n_OF\n_MEMORY\nCould not load HDFS state store with id\n<stateStoreId>\nbecause of an out of memory exception.\n#\nINVALID\n_CHANGE\n_LOG\n_READER\n_VERSION\nThe change log reader version cannot be\n<version>\n. The checkpoint probably is from a future Spark version, please upgrade your Spark.\n#\nINVALID\n_CHANGE\n_LOG\n_WRITER\n_VERSION\nThe change log writer version cannot be\n<version>\n.\n#\nROCKSDB\n_STORE\n_PROVIDER\n_OUT\n_OF\n_MEMORY\nCould not load RocksDB state store with id\n<stateStoreId>\nbecause of an out of memory exception.\n#\nSNAPSHOT\n_PARTITION\n_ID\n_NOT\n_FOUND\nPartition id\n<snapshotPartitionId>\nnot found for state of operator\n<operatorId>\nat\n<checkpointLocation>\n.\n#\nUNCATEGORIZED\n#\nUNEXPECTED\n_FILE\n_SIZE\nCopied\n<dfsFile>\nto\n<localFile>\n, expected\n<expectedSize>\nbytes, found\n<localFileSize>\nbytes.\n#\nUNEXPECTED\n_VERSION\nVersion cannot be\n<version>\nbecause it is less than 0.\n#\nUNRELEASED\n_THREAD\n_ERROR\n<loggingId>\n: RocksDB instance could not be acquired by\n<newAcquiredThreadInfo>\nfor operationType=\n<operationType>\nas it was not released by\n<acquiredThreadInfo>\nafter\n<timeWaitedMs>\nms. Thread holding the lock has trace:\n<stackTraceOutput>\n58030\n#\nCANNOT\n_RESTORE\n_PERMISSIONS\n_FOR\n_PATH\nFailed to set permissions on created path\n<path>\nback to\n<permission>\n.\n58030\n#\nCANNOT\n_WRITE\n_STATE\n_STORE\nError writing state store files for provider\n<providerClass>\n.\n#\nCANNOT\n_COMMIT\nCannot perform commit during state checkpoint.\n58030\n#\nFAILED\n_RENAME\n_TEMP\n_FILE\nFailed to rename temp file\n<srcPath>\nto\n<dstPath>\nas FileSystem.rename returned false.\n58030\n#\nINVALID\n_BUCKET\n_FILE\nInvalid bucket file:\n<path>\n.\n58030\n#\nTASK\n_WRITE\n_FAILED\nTask failed while writing rows to\n<path>\n.\n58030\n#\nUNABLE\n_TO\n_FETCH\n_HIVE\n_TABLES\nUnable to fetch tables of Hive database:\n<dbName>\n.\nF0000\n#\nINVALID\n_DRIVER\n_MEMORY\nSystem memory\n<systemMemory>\nmust be at least\n<minSystemMemory>\n. Please increase heap size using the --driver-memory option or \"\n<config>\n\" in Spark configuration.\nF0000\n#\nINVALID\n_EXECUTOR\n_MEMORY\nExecutor memory\n<executorMemory>\nmust be at least\n<minSystemMemory>\n. Please increase executor memory using the --executor-memory option or \"\n<config>\n\" in Spark configuration.\nF0000\n#\nINVALID\n_KRYO\n_SERIALIZER\n_BUFFER\n_SIZE\nThe value of the config \"\n<bufferSizeConfKey>\n\" must be less than 2048 MiB, but got\n<bufferSizeConfValue>\nMiB.\nHV000\n#\nFAILED\n_JDBC\nFailed JDBC\n<url>\non the operation:\n#\nALTER\n_TABLE\nAlter the table\n<tableName>\n.\n#\nCREATE\n_INDEX\nCreate the index\n<indexName>\nin the\n<tableName>\ntable.\n#\nCREATE\n_NAMESPACE\nCreate the namespace\n<namespace>\n.\n#\nCREATE\n_NAMESPACE\n_COMMENT\nCreate a comment on the namespace:\n<namespace>\n.\n#\nCREATE\n_TABLE\nCreate the table\n<tableName>\n.\n#\nDROP\n_INDEX\nDrop the index\n<indexName>\nin the\n<tableName>\ntable.\n#\nDROP\n_NAMESPACE\nDrop the namespace\n<namespace>\n.\n#\nGET\n_TABLES\nGet tables from the namespace:\n<namespace>\n.\n#\nLIST\n_NAMESPACES\nList namespaces.\n#\nLOAD\n_TABLE\nLoad the table\n<tableName>\n.\n#\nNAMESPACE\n_EXISTS\nCheck that the namespace\n<namespace>\nexists.\n#\nREMOVE\n_NAMESPACE\n_COMMENT\nRemove a comment on the namespace:\n<namespace>\n.\n#\nRENAME\n_TABLE\nRename the table\n<oldName>\nto\n<newName>\n.\n#\nTABLE\n_EXISTS\nCheck that the table\n<tableName>\nexists.\n#\nUNCLASSIFIED\n<message>\nHV091\n#\nNONEXISTENT\n_FIELD\n_NAME\n_IN\n_LIST\nField(s)\n<nonExistFields>\ndo(es) not exist. Available fields:\n<fieldNames>\nHY000\n#\nINVALID\n_HANDLE\nThe handle\n<handle>\nis invalid.\n#\nFORMAT\nHandle must be an UUID string of the format '00112233-4455-6677-8899-aabbccddeeff'\n#\nOPERATION\n_ABANDONED\nOperation was considered abandoned because of inactivity and removed.\n#\nOPERATION\n_ALREADY\n_EXISTS\nOperation already exists.\n#\nOPERATION\n_NOT\n_FOUND\nOperation not found.\n#\nSESSION\n_CHANGED\nThe existing Spark server driver instance has restarted. Please reconnect.\n#\nSESSION\n_CLOSED\nSession was closed.\n#\nSESSION\n_NOT\n_FOUND\nSession not found.\nHY000\n#\nMISSING\n_TIMEOUT\n_CONFIGURATION\nThe operation has timed out, but no timeout duration is configured. To set a processing time-based timeout, use 'GroupState.setTimeoutDuration()' in your 'mapGroupsWithState' or 'flatMapGroupsWithState' operation. For event-time-based timeout, use 'GroupState.setTimeoutTimestamp()' and define a watermark using 'Dataset.withWatermark()'.\nHY008\n#\nOPERATION\n_CANCELED\nOperation has been canceled.\nHY109\n#\nINVALID\n_CURSOR\nThe cursor is invalid.\n#\nDISCONNECTED\nThe cursor has been disconnected by the server.\n#\nNOT\n_REATTACHABLE\nThe cursor is not reattachable.\n#\nPOSITION\n_NOT\n_AVAILABLE\nThe cursor position id\n<responseId>\nis no longer available at index\n<index>\n.\n#\nPOSITION\n_NOT\n_FOUND\nThe cursor position id\n<responseId>\nis not found.\nKD000\n#\nFAILED\n_REGISTER\n_CLASS\n_WITH\n_KRYO\nFailed to register classes with Kryo.\nKD000\n#\nGRAPHITE\n_SINK\n_INVALID\n_PROTOCOL\nInvalid Graphite protocol:\n<protocol>\n.\nKD000\n#\nGRAPHITE\n_SINK\n_PROPERTY\n_MISSING\nGraphite sink requires '\n<property>\n' property.\nKD000\n#\nINCOMPATIBLE\n_DATA\n_FOR\n_TABLE\nCannot write incompatible data for the table\n<tableName>\n:\n#\nAMBIGUOUS\n_COLUMN\n_NAME\nAmbiguous column name in the input data\n<colName>\n.\n#\nCANNOT\n_FIND\n_DATA\nCannot find data for the output column\n<colName>\n.\n#\nCANNOT\n_SAFELY\n_CAST\nCannot safely cast\n<colName>\n<srcType>\nto\n<targetType>\n.\n#\nEXTRA\n_COLUMNS\nCannot write extra columns\n<extraColumns>\n.\n#\nEXTRA\n_STRUCT\n_FIELDS\nCannot write extra fields\n<extraFields>\nto the struct\n<colName>\n.\n#\nNULLABLE\n_ARRAY\n_ELEMENTS\nCannot write nullable elements to array of non-nulls:\n<colName>\n.\n#\nNULLABLE\n_COLUMN\nCannot write nullable values to non-null column\n<colName>\n.\n#\nNULLABLE\n_MAP\n_VALUES\nCannot write nullable values to map of non-nulls:\n<colName>\n.\n#\nSTRUCT\n_MISSING\n_FIELDS\nStruct\n<colName>\nmissing fields:\n<missingFields>\n.\n#\nUNEXPECTED\n_COLUMN\n_NAME\nStruct\n<colName>\n<order>\n-th field name does not match (may be out of order): expected\n<expected>\n, found\n<found>\n.\nKD000\n#\nMALFORMED\n_CSV\n_RECORD\nMalformed CSV record:\n<badRecord>\nKD001\n#\nFAILED\n_READ\n_FILE\nEncountered error while reading file\n<path>\n.\n#\nCANNOT\n_READ\n_FILE\n_FOOTER\nCould not read footer. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.\n#\nFILE\n_NOT\n_EXIST\nFile does not exist. It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n#\nNO\n_HINT\n#\nPARQUET\n_COLUMN\n_DATA\n_TYPE\n_MISMATCH\nData type mismatches when reading Parquet column\n<column>\n. Expected Spark type\n<expectedType>\n, actual Parquet type\n<actualType>\n.\n#\nUNSUPPORTED\n_FILE\n_SYSTEM\nThe file system\n<fileSystemClass>\nhasn't implemented\n<method>\n.\nKD002\n#\nINVALID\n_LOG\n_VERSION\nUnsupportedLogVersion.\n#\nEXACT\n_MATCH\n_VERSION\nThe only supported log version is v\n<matchVersion>\n, but encountered v\n<version>\n.\n#\nMAX\n_SUPPORTED\n_VERSION\nThe maximum supported log version is v\n<maxSupportedVersion>\n, but encountered v\n<version>\n. The log file was produced by a newer version of Spark and cannot be read by this version. You need to upgrade.\nKD002\n#\nMALFORMED\n_LOG\n_FILE\nLog file was malformed: failed to read correct log version from\n<text>\n.\nKD005\n#\nALL\n_PARTITION\n_COLUMNS\n_NOT\n_ALLOWED\nCannot use all columns for partition columns.\nKD006\n#\nSTDS\n_COMMITTED\n_BATCH\n_UNAVAILABLE\nNo committed batch found, checkpoint location:\n<checkpointLocation>\n. Ensure that the query has run and committed any microbatch before stopping.\nKD006\n#\nSTDS\n_NO\n_PARTITION\n_DISCOVERED\n_IN\n_STATE\n_STORE\nThe state does not have any partition. Please double check that the query points to the valid state. options:\n<sourceOptions>\nKD006\n#\nSTDS\n_OFFSET\n_LOG\n_UNAVAILABLE\nThe offset log for\n<batchId>\ndoes not exist, checkpoint location:\n<checkpointLocation>\n. Please specify the batch ID which is available for querying - you can query the available batch IDs via using state metadata data source.\nKD006\n#\nSTDS\n_OFFSET\n_METADATA\n_LOG\n_UNAVAILABLE\nMetadata is not available for offset log for\n<batchId>\n, checkpoint location:\n<checkpointLocation>\n. The checkpoint seems to be only run with older Spark version(s). Run the streaming query with the recent Spark version, so that Spark constructs the state metadata.\nKD009\n#\nCONFLICTING\n_DIRECTORY\n_STRUCTURES\nConflicting directory structures detected. Suspicious paths:\n<discoveredBasePaths>\nIf provided paths are partition directories, please set \"basePath\" in the options of the data source to specify the root directory of the table. If there are multiple root directories, please load them separately and then union them.\nKD009\n#\nCONFLICTING\n_PARTITION\n_COLUMN\n_NAMES\nConflicting partition column names detected:\n<distinctPartColLists>\nFor partitioned table directories, data files should only live in leaf directories. And directories at the same level should have the same partition column name. Please check the following directories for unexpected files or inconsistent partition column names:\n<suspiciousPaths>\nKD00B\n#\nERROR\n_READING\n_AVRO\n_UNKNOWN\n_FINGERPRINT\nError reading avro data -- encountered an unknown fingerprint:\n<fingerprint>\n, not sure what schema to use. This could happen if you registered additional schemas after starting your spark context.\nKD010\n#\nDATA\n_SOURCE\n_EXTERNAL\n_ERROR\nEncountered error when saving to external data source.\nP0001\n#\nUSER\n_RAISED\n_EXCEPTION\n<errorMessage>\nP0001\n#\nUSER\n_RAISED\n_EXCEPTION\n_PARAMETER\n_MISMATCH\nThe\nraise_error()\nfunction was used to raise error class:\n<errorClass>\nwhich expects parameters:\n<expectedParms>\n. The provided parameters\n<providedParms>\ndo not match the expected parameters. Please make sure to provide all expected parameters.\nP0001\n#\nUSER\n_RAISED\n_EXCEPTION\n_UNKNOWN\n_ERROR\n_CLASS\nThe\nraise_error()\nfunction was used to raise an unknown error class:\n<errorClass>\nXX000\n#\nAMBIGUOUS\n_RESOLVER\n_EXTENSION\nThe single-pass analyzer cannot process this query or command because the extension choice for\n<operator>\nis ambiguous:\n<extensions>\n.\nXX000\n#\nCONNECT\n_ML\nGeneric Spark Connect ML error.\n#\nATTRIBUTE\n_NOT\n_ALLOWED\n<attribute>\nin\n<className>\nis not allowed to be accessed.\n#\nCACHE\n_INVALID\nCannot retrieve\n<objectName>\nfrom the ML cache. It is probably because the entry has been evicted.\n#\nUNSUPPORTED\n_EXCEPTION\n<message>\nXX000\n#\nHYBRID\n_ANALYZER\n_EXCEPTION\nAn failure occurred when attempting to resolve a query or command with both the legacy fixed-point analyzer as well as the single-pass resolver.\n#\nFIXED\n_POINT\n_FAILED\n_SINGLE\n_PASS\n_SUCCEEDED\nFixed-point resolution failed, but single-pass resolution succeeded. Single-pass analyzer output:\n<singlePassOutput>\n#\nLOGICAL\n_PLAN\n_COMPARISON\n_MISMATCH\nOutputs of fixed-point and single-pass analyzers do not match. Fixed-point analyzer output:\n<fixedPointOutput>\nSingle-pass analyzer output:\n<singlePassOutput>\n#\nOUTPUT\n_SCHEMA\n_COMPARISON\n_MISMATCH\nOutput schemas of fixed-point and single-pass analyzers do not match. Fixed-point analyzer output schema:\n<fixedPointOutputSchema>\nSingle-pass analyzer output schema:\n<singlePassOutputSchema>\nXX000\n#\nMALFORMED\n_PROTOBUF\n_MESSAGE\nMalformed Protobuf messages are detected in message deserialization. Parse Mode:\n<failFastMode>\n. To process malformed protobuf message as null result, try setting the option 'mode' as 'PERMISSIVE'.\nXX000\n#\nMISSING\n_ATTRIBUTES\nResolved attribute(s)\n<missingAttributes>\nmissing from\n<input>\nin operator\n<operator>\n.\n#\nRESOLVED\n_ATTRIBUTE\n_APPEAR\n_IN\n_OPERATION\nAttribute(s) with the same name appear in the operation:\n<operation>\n. Please check if the right attribute(s) are used.\n#\nRESOLVED\n_ATTRIBUTE\n_MISSING\n_FROM\n_INPUT\nXX000\n#\nSTATE\n_STORE\n_KEY\n_ROW\n_FORMAT\n_VALIDATION\n_FAILURE\nThe streaming query failed to validate written state for key row. The following reasons may cause this: 1. An old Spark version wrote the checkpoint that is incompatible with the current one 2. Corrupt checkpoint files 3. The query changed in an incompatible way between restarts For the first case, use a new checkpoint directory or use the original Spark version to process the streaming state. Retrieved error_message=\n<errorMsg>\nXX000\n#\nSTATE\n_STORE\n_VALUE\n_ROW\n_FORMAT\n_VALIDATION\n_FAILURE\nThe streaming query failed to validate written state for value row. The following reasons may cause this: 1. An old Spark version wrote the checkpoint that is incompatible with the current one 2. Corrupt checkpoint files 3. The query changed in an incompatible way between restarts For the first case, use a new checkpoint directory or use the original Spark version to process the streaming state. Retrieved error_message=\n<errorMsg>\nXXKD0\n#\nINVALID\n_SQL\n_FUNCTION\n_PLAN\n_STRUCTURE\nInvalid SQL function plan structure\n<plan>\nXXKD0\n#\nPLAN\n_VALIDATION\n_FAILED\n_RULE\n_EXECUTOR\nThe input plan of\n<ruleExecutor>\nis invalid:\n<reason>\nXXKD0\n#\nPLAN\n_VALIDATION\n_FAILED\n_RULE\n_IN\n_BATCH\nRule\n<rule>\nin batch\n<batch>\ngenerated an invalid plan:\n<reason>\nXXKDA\n#\nSPARK\n_JOB\n_CANCELLED\nJob\n<jobId>\ncancelled\n<reason>\nXXKST\n#\nSTATE\n_STORE\n_INVALID\n_VALUE\n_SCHEMA\n_EVOLUTION\nSchema evolution is not possible new value_schema=\n<newValueSchema>\nand old value_schema=\n<oldValueSchema>\nPlease check https://avro.apache.org/docs/1.11.1/specification/_print/#schema-resolution for valid schema evolution.\nXXKST\n#\nSTATE\n_STORE\n_KEY\n_SCHEMA\n_NOT\n_COMPATIBLE\nProvided key schema does not match existing state key schema. Please check number and type of fields. Existing key_schema=\n<storedKeySchema>\nand new key_schema=\n<newKeySchema>\n. If you want to force running the query without schema validation, please set spark.sql.streaming.stateStore.stateSchemaCheck to false. However, please note that running the query with incompatible schema could cause non-deterministic behavior.\nXXKST\n#\nSTATE\n_STORE\n_OPERATION\n_OUT\n_OF\n_ORDER\nStreaming stateful operator attempted to access state store out of order. This is a bug, please retry. error_msg=\n<errorMsg>\nXXKST\n#\nSTATE\n_STORE\n_UNSUPPORTED\n_OPERATION\n<operationType>\noperation not supported with\n<entity>\nXXKST\n#\nSTATE\n_STORE\n_UNSUPPORTED\n_OPERATION\n_BINARY\n_INEQUALITY\nBinary inequality column is not supported with state store. Provided schema:\n<schema>\n.\nXXKST\n#\nSTATE\n_STORE\n_VALUE\n_SCHEMA\n_NOT\n_COMPATIBLE\nProvided value schema does not match existing state value schema. Please check number and type of fields. Existing value_schema=\n<storedValueSchema>\nand new value_schema=\n<newValueSchema>\n. If you want to force running the query without schema validation, please set spark.sql.streaming.stateStore.stateSchemaCheck to false. However, please note that running the query with incompatible schema could cause non-deterministic behavior.\nXXKST\n#\nSTDS\n_INTERNAL\n_ERROR\nInternal error:\n<message>\nPlease, report this bug to the corresponding communities or vendors, and provide the full stack trace.\nXXKST\n#\nSTREAMING\n_PYTHON\n_RUNNER\n_INITIALIZATION\n_FAILURE\nStreaming Runner initialization failed, returned\n<resFromPython>\n. Cause:\n<msg>\nXXKST\n#\nSTREAM\n_FAILED\nQuery [id =\n<id>\n, runId =\n<runId>\n] terminated with exception:\n<message>\nXXKST\n#\nTRANSFORM\n_WITH\n_STATE\n_SCHEMA\n_MUST\n_BE\n_NULLABLE\nIf Avro encoding is enabled, all the fields in the schema for column family\n<columnFamilyName>\nmust be nullable when using the TransformWithState operator. Please make the schema nullable. Current schema:\n<schema>\nXXKUC\n#\nINSUFFICIENT\n_TABLE\n_PROPERTY\nCan't find table property:\n#\nMISSING\n_KEY\n<key>\n.\n#\nMISSING\n_KEY\n_PART\n<key>\n,\n<totalAmountOfParts>\nparts are expected."}
{"url": "https://spark.apache.org/docs/latest/streaming/apis-on-dataframes-and-datasets.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nStructured Streaming Programming Guide\nOverview\nGetting Started\nAPIs on DataFrames and Datasets\nCreating Streaming DataFrames and Streaming Datasets\nOperations on Streaming DataFrames/Datasets\nStarting Streaming Queries\nManaging Streaming Queries\nMonitoring Streaming Queries\nRecovering from Failures with Checkpointing\nRecovery Semantics after Changes in a Streaming Query\nPerformance Tips\nAdditional Information\nStructured Streaming Programming Guide\nAPI using Datasets and DataFrames\nSince Spark 2.0, DataFrames and Datasets can represent static, bounded data, as well as streaming, unbounded data. Similar to static Datasets/DataFrames, you can use the common entry point\nSparkSession\n(\nPython\n/\nScala\n/\nJava\n/\nR\ndocs)\nto create streaming DataFrames/Datasets from streaming sources, and apply the same operations on them as static DataFrames/Datasets. If you are not familiar with Datasets/DataFrames, you are strongly advised to familiarize yourself with them using the\nDataFrame/Dataset Programming Guide\n.\nCreating streaming DataFrames and streaming Datasets\nStreaming DataFrames can be created through the\nDataStreamReader\ninterface\n(\nPython\n/\nScala\n/\nJava\ndocs)\nreturned by\nSparkSession.readStream()\n. In\nR\n, with the\nread.stream()\nmethod. Similar to the read interface for creating static DataFrame, you can specify the details of the source – data format, schema, options, etc.\nInput Sources\nThere are a few built-in sources.\nFile source\n- Reads files written in a directory as a stream of data. Files will be processed in the order of file modification time. If\nlatestFirst\nis set, order will be reversed. Supported file formats are text, CSV, JSON, ORC, Parquet. See the docs of the DataStreamReader interface for a more up-to-date list, and supported options for each file format. Note that the files must be atomically placed in the given directory, which in most file systems, can be achieved by file move operations.\nKafka source\n- Reads data from Kafka. It’s compatible with Kafka broker versions 0.10.0 or higher. See the\nKafka Integration Guide\nfor more details.\nSocket source (for testing)\n- Reads UTF8 text data from a socket connection. The listening server socket is at the driver. Note that this should be used only for testing as this does not provide end-to-end fault-tolerance guarantees.\nRate source (for testing)\n- Generates data at the specified number of rows per second, each output row contains a\ntimestamp\nand\nvalue\n. Where\ntimestamp\nis a\nTimestamp\ntype containing the time of message dispatch, and\nvalue\nis of\nLong\ntype containing the message count, starting from 0 as the first row. This source is intended for testing and benchmarking.\nRate Per Micro-Batch source (for testing)\n- Generates data at the specified number of rows per micro-batch, each output row contains a\ntimestamp\nand\nvalue\n. Where\ntimestamp\nis a\nTimestamp\ntype containing the time of message dispatch, and\nvalue\nis of\nLong\ntype containing the message count, starting from 0 as the first row. Unlike\nrate\ndata source, this data source provides a consistent set of input rows per micro-batch regardless of query execution (configuration of trigger, query being lagging, etc.), say, batch 0 will produce 0~999 and batch 1 will produce 1000~1999, and so on. Same applies to the generated time. This source is intended for testing and benchmarking.\nSome sources are not fault-tolerant because they do not guarantee that data can be replayed using\ncheckpointed offsets after a failure. See the earlier section on\nfault-tolerance semantics\nHere are the details of all the sources in Spark.\nSource\nOptions\nFault-tolerant\nNotes\nFile source\npath\n: path to the input directory, and common to all file formats.\nmaxFilesPerTrigger\n: maximum number of new files to be considered in every trigger (default: no max)\nmaxBytesPerTrigger\n: maximum total size of new files to be considered in every trigger (default: no max).\nmaxBytesPerTrigger\nand\nmaxFilesPerTrigger\ncan't both be set at the same time, only one of two must be chosen. Note that a stream always reads at least one file so it can make progress and not get stuck on a file larger than a given maximum.\nlatestFirst\n: whether to process the latest new files first, useful when there is a large backlog of files (default: false)\nfileNameOnly\n: whether to check new files based on only the filename instead of on the full path (default: false). With this set to `true`, the following files would be considered as the same file, because their filenames, \"dataset.txt\", are the same:\n\"file:///dataset.txt\"\n\"s3://a/dataset.txt\"\n\"s3n://a/b/dataset.txt\"\n\"s3a://a/b/c/dataset.txt\"\nmaxFileAge\n: Maximum age of a file that can be found in this directory, before it is ignored. For the first batch all files will be considered valid. If\nlatestFirst\nis set to `true` and\nmaxFilesPerTrigger\nor\nmaxBytesPerTrigger\nis set, then this parameter will be ignored, because old files that are valid, and should be processed, may be ignored. The max age is specified with respect to the timestamp of the latest file, and not the timestamp of the current system.(default: 1 week)\nmaxCachedFiles\n: maximum number of files to cache to be processed in subsequent batches (default: 10000).  If files are available in the cache, they will be read from first before listing from the input source.\ndiscardCachedInputRatio\n: ratio of cached files/bytes to max files/bytes to allow for listing from input source when there is less cached input than could be available to be read (default: 0.2).  For example, if there are only 10 cached files remaining for a batch but the\nmaxFilesPerTrigger\nis set to 100, the 10 cached files would be discarded and a new listing would be performed instead. Similarly, if there are cached files that are 10 MB remaining for a batch, but the\nmaxBytesPerTrigger\nis set to 100MB, the cached files would be discarded.\ncleanSource\n: option to clean up completed files after processing.\nAvailable options are \"archive\", \"delete\", \"off\". If the option is not provided, the default value is \"off\".\nWhen \"archive\" is provided, additional option\nsourceArchiveDir\nmust be provided as well. The value of \"sourceArchiveDir\" must not match with source pattern in depth (the number of directories from the root directory), where the depth is minimum of depth on both paths. This will ensure archived files are never included as new source files.\nFor example, suppose you provide '/hello?/spark/*' as source pattern, '/hello1/spark/archive/dir' cannot be used as the value of \"sourceArchiveDir\", as '/hello?/spark/*' and '/hello1/spark/archive' will be matched. '/hello1/spark' cannot be also used as the value of \"sourceArchiveDir\", as '/hello?/spark' and '/hello1/spark' will be matched. '/archived/here' would be OK as it doesn't match.\nSpark will move source files respecting their own path. For example, if the path of source file is\n/a/b/dataset.txt\nand the path of archive directory is\n/archived/here\n, file will be moved to\n/archived/here/a/b/dataset.txt\n.\nNOTE: Both archiving (via moving) or deleting completed files will introduce overhead (slow down, even if it's happening in separate thread) in each micro-batch, so you need to understand the cost for each operation in your file system before enabling this option. On the other hand, enabling this option will reduce the cost to list source files which can be an expensive operation.\nNumber of threads used in completed file cleaner can be configured with\nspark.sql.streaming.fileSource.cleaner.numThreads\n(default: 1).\nNOTE 2: The source path should not be used from multiple sources or queries when enabling this option. Similarly, you must ensure the source path doesn't match to any files in output directory of file stream sink.\nNOTE 3: Both delete and move actions are best effort. Failing to delete or move files will not fail the streaming query. Spark may not clean up some source files in some circumstances - e.g. the application doesn't shut down gracefully, too many files are queued to clean up.\nFor file-format-specific options, see the related methods in\nDataStreamReader\n(\nPython\n/\nScala\n/\nJava\n/\nR\n).\n        E.g. for \"parquet\" format options see\nDataStreamReader.parquet()\n.\nIn addition, there are session configurations that affect certain file-formats. See the\nSQL Programming Guide\nfor more details. E.g., for \"parquet\", see\nParquet configuration\nsection.\nYes\nSupports glob paths, but does not support multiple comma-separated paths/globs.\nSocket Source\nhost\n: host to connect to, must be specified\nport\n: port to connect to, must be specified\nNo\nRate Source\nrowsPerSecond\n(e.g. 100, default: 1): How many rows should be generated per second.\nrampUpTime\n(e.g. 5s, default: 0s): How long to ramp up before the generating speed becomes\nrowsPerSecond\n. Using finer granularities than seconds will be truncated to integer seconds.\nnumPartitions\n(e.g. 10, default: Spark's default parallelism): The partition number for the generated rows.\nThe source will try its best to reach\nrowsPerSecond\n, but the query may be resource constrained, and\nnumPartitions\ncan be tweaked to help reach the desired speed.\nYes\nRate Per Micro-Batch Source\n(format:\nrate-micro-batch\n)\nrowsPerBatch\n(e.g. 100): How many rows should be generated per micro-batch.\nnumPartitions\n(e.g. 10, default: Spark's default parallelism): The partition number for the generated rows.\nstartTimestamp\n(e.g. 1000, default: 0): starting value of generated time.\nadvanceMillisPerBatch\n(e.g. 1000, default: 1000): the amount of time being advanced in generated time on each micro-batch.\nYes\nKafka Source\nSee the\nKafka Integration Guide\n.\nYes\nHere are some examples.\nspark\n=\nSparkSession\n.\n...\n# Read text from socket\nsocketDF\n=\nspark\n\\\n.\nreadStream\n\\\n.\nformat\n(\n\"\nsocket\n\"\n)\n\\\n.\noption\n(\n\"\nhost\n\"\n,\n\"\nlocalhost\n\"\n)\n\\\n.\noption\n(\n\"\nport\n\"\n,\n9999\n)\n\\\n.\nload\n()\nsocketDF\n.\nisStreaming\n()\n# Returns True for DataFrames that have streaming sources\nsocketDF\n.\nprintSchema\n()\n# Read all the csv files written atomically in a directory\nuserSchema\n=\nStructType\n().\nadd\n(\n\"\nname\n\"\n,\n\"\nstring\n\"\n).\nadd\n(\n\"\nage\n\"\n,\n\"\ninteger\n\"\n)\ncsvDF\n=\nspark\n\\\n.\nreadStream\n\\\n.\noption\n(\n\"\nsep\n\"\n,\n\"\n;\n\"\n)\n\\\n.\nschema\n(\nuserSchema\n)\n\\\n.\ncsv\n(\n\"\n/path/to/directory\n\"\n)\n# Equivalent to format(\"csv\").load(\"/path/to/directory\")\nval\nspark\n:\nSparkSession\n=\n...\n// Read text from socket\nval\nsocketDF\n=\nspark\n.\nreadStream\n.\nformat\n(\n\"socket\"\n)\n.\noption\n(\n\"host\"\n,\n\"localhost\"\n)\n.\noption\n(\n\"port\"\n,\n9999\n)\n.\nload\n()\nsocketDF\n.\nisStreaming\n// Returns True for DataFrames that have streaming sources\nsocketDF\n.\nprintSchema\n// Read all the csv files written atomically in a directory\nval\nuserSchema\n=\nnew\nStructType\n().\nadd\n(\n\"name\"\n,\n\"string\"\n).\nadd\n(\n\"age\"\n,\n\"integer\"\n)\nval\ncsvDF\n=\nspark\n.\nreadStream\n.\noption\n(\n\"sep\"\n,\n\";\"\n)\n.\nschema\n(\nuserSchema\n)\n// Specify schema of the csv files\n.\ncsv\n(\n\"/path/to/directory\"\n)\n// Equivalent to format(\"csv\").load(\"/path/to/directory\")\nSparkSession\nspark\n=\n...\n// Read text from socket\nDataset\n<\nRow\n>\nsocketDF\n=\nspark\n.\nreadStream\n()\n.\nformat\n(\n\"socket\"\n)\n.\noption\n(\n\"host\"\n,\n\"localhost\"\n)\n.\noption\n(\n\"port\"\n,\n9999\n)\n.\nload\n();\nsocketDF\n.\nisStreaming\n();\n// Returns True for DataFrames that have streaming sources\nsocketDF\n.\nprintSchema\n();\n// Read all the csv files written atomically in a directory\nStructType\nuserSchema\n=\nnew\nStructType\n().\nadd\n(\n\"name\"\n,\n\"string\"\n).\nadd\n(\n\"age\"\n,\n\"integer\"\n);\nDataset\n<\nRow\n>\ncsvDF\n=\nspark\n.\nreadStream\n()\n.\noption\n(\n\"sep\"\n,\n\";\"\n)\n.\nschema\n(\nuserSchema\n)\n// Specify schema of the csv files\n.\ncsv\n(\n\"/path/to/directory\"\n);\n// Equivalent to format(\"csv\").load(\"/path/to/directory\")\nsparkR.session\n(\n...\n)\n# Read text from socket\nsocketDF\n<-\nread.stream\n(\n\"socket\"\n,\nhost\n=\nhostname\n,\nport\n=\nport\n)\nisStreaming\n(\nsocketDF\n)\n# Returns TRUE for SparkDataFrames that have streaming sources\nprintSchema\n(\nsocketDF\n)\n# Read all the csv files written atomically in a directory\nschema\n<-\nstructType\n(\nstructField\n(\n\"name\"\n,\n\"string\"\n),\nstructField\n(\n\"age\"\n,\n\"integer\"\n))\ncsvDF\n<-\nread.stream\n(\n\"csv\"\n,\npath\n=\n\"/path/to/directory\"\n,\nschema\n=\nschema\n,\nsep\n=\n\";\"\n)\nThese examples generate streaming DataFrames that are untyped, meaning that the schema of the DataFrame is not checked at compile time, only checked at runtime when the query is submitted. Some operations like\nmap\n,\nflatMap\n, etc. need the type to be known at compile time. To do those, you can convert these untyped streaming DataFrames to typed streaming Datasets using the same methods as static DataFrame. See the\nSQL Programming Guide\nfor more details. Additionally, more details on the supported streaming sources are discussed later in the document.\nSince Spark 3.1, you can also create streaming DataFrames from tables with\nDataStreamReader.table()\n. See\nStreaming Table APIs\nfor more details.\nSchema inference and partition of streaming DataFrames/Datasets\nBy default, Structured Streaming from file based sources requires you to specify the schema, rather than rely on Spark to infer it automatically. This restriction ensures a consistent schema will be used for the streaming query, even in the case of failures. For ad-hoc use cases, you can reenable schema inference by setting\nspark.sql.streaming.schemaInference\nto\ntrue\n.\nPartition discovery does occur when subdirectories that are named\n/key=value/\nare present and listing will automatically recurse into these directories. If these columns appear in the user-provided schema, they will be filled in by Spark based on the path of the file being read. The directories that make up the partitioning scheme must be present when the query starts and must remain static. For example, it is okay to add\n/data/year=2016/\nwhen\n/data/year=2015/\nwas present, but it is invalid to change the partitioning column (i.e. by creating the directory\n/data/date=2016-04-17/\n).\nOperations on streaming DataFrames/Datasets\nYou can apply all kinds of operations on streaming DataFrames/Datasets – ranging from untyped, SQL-like operations (e.g.\nselect\n,\nwhere\n,\ngroupBy\n), to typed RDD-like operations (e.g.\nmap\n,\nfilter\n,\nflatMap\n). See the\nSQL programming guide\nfor more details. Let’s take a look at a few example operations that you can use.\nBasic Operations - Selection, Projection, Aggregation\nMost of the common operations on DataFrame/Dataset are supported for streaming. The few operations that are not supported are\ndiscussed later\nin this section.\ndf\n=\n...\n# streaming DataFrame with IOT device data with schema { device: string, deviceType: string, signal: double, time: DateType }\n# Select the devices which have signal more than 10\ndf\n.\nselect\n(\n\"\ndevice\n\"\n).\nwhere\n(\n\"\nsignal > 10\n\"\n)\n# Running count of the number of updates for each device type\ndf\n.\ngroupBy\n(\n\"\ndeviceType\n\"\n).\ncount\n()\ncase\nclass\nDeviceData\n(\ndevice\n:\nString\n,\ndeviceType\n:\nString\n,\nsignal\n:\nDouble\n,\ntime\n:\nDateTime\n)\nval\ndf\n:\nDataFrame\n=\n...\n// streaming DataFrame with IOT device data with schema { device: string, deviceType: string, signal: double, time: string }\nval\nds\n:\nDataset\n[\nDeviceData\n]\n=\ndf\n.\nas\n[\nDeviceData\n]\n// streaming Dataset with IOT device data\n// Select the devices which have signal more than 10\ndf\n.\nselect\n(\n\"device\"\n).\nwhere\n(\n\"signal > 10\"\n)\n// using untyped APIs\nds\n.\nfilter\n(\n_\n.\nsignal\n>\n10\n).\nmap\n(\n_\n.\ndevice\n)\n// using typed APIs\n// Running count of the number of updates for each device type\ndf\n.\ngroupBy\n(\n\"deviceType\"\n).\ncount\n()\n// using untyped API\n// Running average signal for each device type\nimport\norg.apache.spark.sql.expressions.scalalang.typed\nds\n.\ngroupByKey\n(\n_\n.\ndeviceType\n).\nagg\n(\ntyped\n.\navg\n(\n_\n.\nsignal\n))\n// using typed API\nimport\norg.apache.spark.api.java.function.*\n;\nimport\norg.apache.spark.sql.*\n;\nimport\norg.apache.spark.sql.expressions.javalang.typed\n;\nimport\norg.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n;\npublic\nclass\nDeviceData\n{\nprivate\nString\ndevice\n;\nprivate\nString\ndeviceType\n;\nprivate\nDouble\nsignal\n;\nprivate\njava\n.\nsql\n.\nDate\ntime\n;\n...\n// Getter and setter methods for each field\n}\nDataset\n<\nRow\n>\ndf\n=\n...;\n// streaming DataFrame with IOT device data with schema { device: string, type: string, signal: double, time: DateType }\nDataset\n<\nDeviceData\n>\nds\n=\ndf\n.\nas\n(\nExpressionEncoder\n.\njavaBean\n(\nDeviceData\n.\nclass\n));\n// streaming Dataset with IOT device data\n// Select the devices which have signal more than 10\ndf\n.\nselect\n(\n\"device\"\n).\nwhere\n(\n\"signal > 10\"\n);\n// using untyped APIs\nds\n.\nfilter\n((\nFilterFunction\n<\nDeviceData\n>)\nvalue\n->\nvalue\n.\ngetSignal\n()\n>\n10\n)\n.\nmap\n((\nMapFunction\n<\nDeviceData\n,\nString\n>)\nvalue\n->\nvalue\n.\ngetDevice\n(),\nEncoders\n.\nSTRING\n());\n// Running count of the number of updates for each device type\ndf\n.\ngroupBy\n(\n\"deviceType\"\n).\ncount\n();\n// using untyped API\n// Running average signal for each device type\nds\n.\ngroupByKey\n((\nMapFunction\n<\nDeviceData\n,\nString\n>)\nvalue\n->\nvalue\n.\ngetDeviceType\n(),\nEncoders\n.\nSTRING\n())\n.\nagg\n(\ntyped\n.\navg\n((\nMapFunction\n<\nDeviceData\n,\nDouble\n>)\nvalue\n->\nvalue\n.\ngetSignal\n()));\ndf\n<-\n...\n# streaming DataFrame with IOT device data with schema { device: string, deviceType: string, signal: double, time: DateType }\n# Select the devices which have signal more than 10\nselect\n(\nwhere\n(\ndf\n,\n\"signal > 10\"\n),\n\"device\"\n)\n# Running count of the number of updates for each device type\ncount\n(\ngroupBy\n(\ndf\n,\n\"deviceType\"\n))\nYou can also register a streaming DataFrame/Dataset as a temporary view and then apply SQL commands on it.\ndf\n.\ncreateOrReplaceTempView\n(\n\"\nupdates\n\"\n)\nspark\n.\nsql\n(\n\"\nselect count(*) from updates\n\"\n)\n# returns another streaming DF\ndf\n.\ncreateOrReplaceTempView\n(\n\"updates\"\n)\nspark\n.\nsql\n(\n\"select count(*) from updates\"\n)\n// returns another streaming DF\ndf\n.\ncreateOrReplaceTempView\n(\n\"updates\"\n);\nspark\n.\nsql\n(\n\"select count(*) from updates\"\n);\n// returns another streaming DF\ncreateOrReplaceTempView\n(\ndf\n,\n\"updates\"\n)\nsql\n(\n\"select count(*) from updates\"\n)\nNote, you can identify whether a DataFrame/Dataset has streaming data or not by using\ndf.isStreaming\n.\ndf\n.\nisStreaming\n()\ndf\n.\nisStreaming\ndf\n.\nisStreaming\n()\nisStreaming\n(\ndf\n)\nYou may want to check the query plan of the query, as Spark could inject stateful operations during interpret of SQL statement against streaming dataset. Once stateful operations are injected in the query plan, you may need to check your query with considerations in stateful operations. (e.g. output mode, watermark, state store size maintenance, etc.)\nWindow Operations on Event Time\nAggregations over a sliding event-time window are straightforward with Structured Streaming and are very similar to grouped aggregations. In a grouped aggregation, aggregate values (e.g. counts) are maintained for each unique value in the user-specified grouping column. In case of window-based aggregations, aggregate values are maintained for each window the event-time of a row falls into. Let’s understand this with an illustration.\nImagine our\nquick example\nis modified and the stream now contains lines along with the time when the line was generated. Instead of running word counts, we want to count words within 10 minute windows, updating every 5 minutes. That is, word counts in words received between 10 minute windows 12:00 - 12:10, 12:05 - 12:15, 12:10 - 12:20, etc. Note that 12:00 - 12:10 means data that arrived after 12:00 but before 12:10. Now, consider a word that was received at 12:07. This word should increment the counts corresponding to two windows 12:00 - 12:10 and 12:05 - 12:15. So the counts will be indexed by both, the grouping key (i.e. the word) and the window (can be calculated from the event-time).\nThe result tables would look something like the following.\nSince this windowing is similar to grouping, in code, you can use\ngroupBy()\nand\nwindow()\noperations to express windowed aggregations. You can see the full code for the below examples in\nPython\n/\nScala\n/\nJava\n.\nwords\n=\n...\n# streaming DataFrame of schema { timestamp: Timestamp, word: String }\n# Group the data by window and word and compute the count of each group\nwindowedCounts\n=\nwords\n.\ngroupBy\n(\nwindow\n(\nwords\n.\ntimestamp\n,\n\"\n10 minutes\n\"\n,\n\"\n5 minutes\n\"\n),\nwords\n.\nword\n).\ncount\n()\nimport\nspark.implicits._\nval\nwords\n=\n...\n// streaming DataFrame of schema { timestamp: Timestamp, word: String }\n// Group the data by window and word and compute the count of each group\nval\nwindowedCounts\n=\nwords\n.\ngroupBy\n(\nwindow\n(\n$\n\"timestamp\"\n,\n\"10 minutes\"\n,\n\"5 minutes\"\n),\n$\n\"word\"\n).\ncount\n()\nDataset\n<\nRow\n>\nwords\n=\n...\n// streaming DataFrame of schema { timestamp: Timestamp, word: String }\n// Group the data by window and word and compute the count of each group\nDataset\n<\nRow\n>\nwindowedCounts\n=\nwords\n.\ngroupBy\n(\nfunctions\n.\nwindow\n(\nwords\n.\ncol\n(\n\"timestamp\"\n),\n\"10 minutes\"\n,\n\"5 minutes\"\n),\nwords\n.\ncol\n(\n\"word\"\n)\n).\ncount\n();\nwords\n<-\n...\n# streaming DataFrame of schema { timestamp: Timestamp, word: String }\n# Group the data by window and word and compute the count of each group\nwindowedCounts\n<-\ncount\n(\ngroupBy\n(\nwords\n,\nwindow\n(\nwords\n$\ntimestamp\n,\n\"10 minutes\"\n,\n\"5 minutes\"\n),\nwords\n$\nword\n))\nHandling Late Data and Watermarking\nNow consider what happens if one of the events arrives late to the application.\nFor example, say, a word generated at 12:04 (i.e. event time) could be received by\nthe application at 12:11. The application should use the time 12:04 instead of 12:11\nto update the older counts for the window\n12:00 - 12:10\n. This occurs\nnaturally in our window-based grouping – Structured Streaming can maintain the intermediate state\nfor partial aggregates for a long period of time such that late data can update aggregates of\nold windows correctly, as illustrated below.\nHowever, to run this query for days, it’s necessary for the system to bound the amount of\nintermediate in-memory state it accumulates. This means the system needs to know when an old\naggregate can be dropped from the in-memory state because the application is not going to receive\nlate data for that aggregate any more. To enable this, in Spark 2.1, we have introduced\nwatermarking\n, which lets the engine automatically track the current event time in the data\nand attempt to clean up old state accordingly. You can define the watermark of a query by\nspecifying the event time column and the threshold on how late the data is expected to be in terms of\nevent time. For a specific window ending at time\nT\n, the engine will maintain state and allow late\ndata to update the state until\n(max event time seen by the engine - late threshold > T)\n.\nIn other words, late data within the threshold will be aggregated,\nbut data later than the threshold will start getting dropped\n(see\nlater\nin the section for the exact guarantees). Let’s understand this with an example. We can\neasily define watermarking on the previous example using\nwithWatermark()\nas shown below.\nwords\n=\n...\n# streaming DataFrame of schema { timestamp: Timestamp, word: String }\n# Group the data by window and word and compute the count of each group\nwindowedCounts\n=\nwords\n\\\n.\nwithWatermark\n(\n\"\ntimestamp\n\"\n,\n\"\n10 minutes\n\"\n)\n\\\n.\ngroupBy\n(\nwindow\n(\nwords\n.\ntimestamp\n,\n\"\n10 minutes\n\"\n,\n\"\n5 minutes\n\"\n),\nwords\n.\nword\n)\n\\\n.\ncount\n()\nimport\nspark.implicits._\nval\nwords\n=\n...\n// streaming DataFrame of schema { timestamp: Timestamp, word: String }\n// Group the data by window and word and compute the count of each group\nval\nwindowedCounts\n=\nwords\n.\nwithWatermark\n(\n\"timestamp\"\n,\n\"10 minutes\"\n)\n.\ngroupBy\n(\nwindow\n(\n$\n\"timestamp\"\n,\n\"10 minutes\"\n,\n\"5 minutes\"\n),\n$\n\"word\"\n)\n.\ncount\n()\nDataset\n<\nRow\n>\nwords\n=\n...\n// streaming DataFrame of schema { timestamp: Timestamp, word: String }\n// Group the data by window and word and compute the count of each group\nDataset\n<\nRow\n>\nwindowedCounts\n=\nwords\n.\nwithWatermark\n(\n\"timestamp\"\n,\n\"10 minutes\"\n)\n.\ngroupBy\n(\nwindow\n(\ncol\n(\n\"timestamp\"\n),\n\"10 minutes\"\n,\n\"5 minutes\"\n),\ncol\n(\n\"word\"\n))\n.\ncount\n();\nwords\n<-\n...\n# streaming DataFrame of schema { timestamp: Timestamp, word: String }\n# Group the data by window and word and compute the count of each group\nwords\n<-\nwithWatermark\n(\nwords\n,\n\"timestamp\"\n,\n\"10 minutes\"\n)\nwindowedCounts\n<-\ncount\n(\ngroupBy\n(\nwords\n,\nwindow\n(\nwords\n$\ntimestamp\n,\n\"10 minutes\"\n,\n\"5 minutes\"\n),\nwords\n$\nword\n))\nIn this example, we are defining the watermark of the query on the value of the column “timestamp”,\nand also defining “10 minutes” as the threshold of how late is the data allowed to be. If this query\nis run in Update output mode (discussed later in\nOutput Modes\nsection),\nthe engine will keep updating counts of a window in the Result Table until the window is older\nthan the watermark, which lags behind the current event time in column “timestamp” by 10 minutes.\nHere is an illustration.\nAs shown in the illustration, the maximum event time tracked by the engine is the\nblue dashed line\n, and the watermark set as\n(max event time - '10 mins')\nat the beginning of every trigger is the red line. For example, when the engine observes the data\n(12:14, dog)\n, it sets the watermark for the next trigger as\n12:04\n.\nThis watermark lets the engine maintain intermediate state for additional 10 minutes to allow late\ndata to be counted. For example, the data\n(12:09, cat)\nis out of order and late, and it falls in\nwindows\n12:00 - 12:10\nand\n12:05 - 12:15\n. Since, it is still ahead of the watermark\n12:04\nin\nthe trigger, the engine still maintains the intermediate counts as state and correctly updates the\ncounts of the related windows. However, when the watermark is updated to\n12:11\n, the intermediate\nstate for window\n(12:00 - 12:10)\nis cleared, and all subsequent data (e.g.\n(12:04, donkey)\n)\nis considered “too late” and therefore ignored. Note that after every trigger,\nthe updated counts (i.e. purple rows) are written to sink as the trigger output, as dictated by\nthe Update mode.\nSome sinks (e.g. files) may not supported fine-grained updates that Update Mode requires. To work\nwith them, we have also support Append Mode, where only the\nfinal counts\nare written to sink.\nThis is illustrated below.\nNote that using\nwithWatermark\non a non-streaming Dataset is no-op. As the watermark should not affect\nany batch query in any way, we will ignore it directly.\nSimilar to the Update Mode earlier, the engine maintains intermediate counts for each window.\nHowever, the partial counts are not updated to the Result Table and not written to sink. The engine\nwaits for “10 mins” for late date to be counted,\nthen drops intermediate state of a window < watermark, and appends the final\ncounts to the Result Table/sink. For example, the final counts of window\n12:00 - 12:10\nis\nappended to the Result Table only after the watermark is updated to\n12:11\n.\nTypes of time windows\nSpark supports three types of time windows: tumbling (fixed), sliding and session.\nTumbling windows are a series of fixed-sized, non-overlapping and contiguous time intervals. An input\ncan only be bound to a single window.\nSliding windows are similar to the tumbling windows from the point of being “fixed-sized”, but windows\ncan overlap if the duration of slide is smaller than the duration of window, and in this case an input\ncan be bound to the multiple windows.\nTumbling and sliding window use\nwindow\nfunction, which has been described on above examples.\nSession windows have different characteristic compared to the previous two types. Session window has a dynamic size\nof the window length, depending on the inputs. A session window starts with an input, and expands itself\nif following input has been received within gap duration. For static gap duration, a session window closes when\nthere’s no input received within gap duration after receiving the latest input.\nSession window uses\nsession_window\nfunction. The usage of the function is similar to the\nwindow\nfunction.\nevents\n=\n...\n# streaming DataFrame of schema { timestamp: Timestamp, userId: String }\n# Group the data by session window and userId, and compute the count of each group\nsessionizedCounts\n=\nevents\n\\\n.\nwithWatermark\n(\n\"\ntimestamp\n\"\n,\n\"\n10 minutes\n\"\n)\n\\\n.\ngroupBy\n(\nsession_window\n(\nevents\n.\ntimestamp\n,\n\"\n5 minutes\n\"\n),\nevents\n.\nuserId\n)\n\\\n.\ncount\n()\nimport\nspark.implicits._\nval\nevents\n=\n...\n// streaming DataFrame of schema { timestamp: Timestamp, userId: String }\n// Group the data by session window and userId, and compute the count of each group\nval\nsessionizedCounts\n=\nevents\n.\nwithWatermark\n(\n\"timestamp\"\n,\n\"10 minutes\"\n)\n.\ngroupBy\n(\nsession_window\n(\n$\n\"timestamp\"\n,\n\"5 minutes\"\n),\n$\n\"userId\"\n)\n.\ncount\n()\nDataset\n<\nRow\n>\nevents\n=\n...\n// streaming DataFrame of schema { timestamp: Timestamp, userId: String }\n// Group the data by session window and userId, and compute the count of each group\nDataset\n<\nRow\n>\nsessionizedCounts\n=\nevents\n.\nwithWatermark\n(\n\"timestamp\"\n,\n\"10 minutes\"\n)\n.\ngroupBy\n(\nsession_window\n(\ncol\n(\n\"timestamp\"\n),\n\"5 minutes\"\n),\ncol\n(\n\"userId\"\n))\n.\ncount\n();\nInstead of static value, we can also provide an expression to specify gap duration dynamically\nbased on the input row. Note that the rows with negative or zero gap duration will be filtered\nout from the aggregation.\nWith dynamic gap duration, the closing of a session window does not depend on the latest input\nanymore. A session window’s range is the union of all events’ ranges which are determined by\nevent start time and evaluated gap duration during the query execution.\nfrom\npyspark.sql\nimport\nfunctions\nas\nsf\nevents\n=\n...\n# streaming DataFrame of schema { timestamp: Timestamp, userId: String }\nsession_window\n=\nsession_window\n(\nevents\n.\ntimestamp\n,\n\\\nsf\n.\nwhen\n(\nevents\n.\nuserId\n==\n\"\nuser1\n\"\n,\n\"\n5 seconds\n\"\n)\n\\\n.\nwhen\n(\nevents\n.\nuserId\n==\n\"\nuser2\n\"\n,\n\"\n20 seconds\n\"\n).\notherwise\n(\n\"\n5 minutes\n\"\n))\n# Group the data by session window and userId, and compute the count of each group\nsessionizedCounts\n=\nevents\n\\\n.\nwithWatermark\n(\n\"\ntimestamp\n\"\n,\n\"\n10 minutes\n\"\n)\n\\\n.\ngroupBy\n(\nsession_window\n,\nevents\n.\nuserId\n)\n\\\n.\ncount\n()\nimport\nspark.implicits._\nval\nevents\n=\n...\n// streaming DataFrame of schema { timestamp: Timestamp, userId: String }\nval\nsessionWindow\n=\nsession_window\n(\n$\n\"timestamp\"\n,\nwhen\n(\n$\n\"userId\"\n===\n\"user1\"\n,\n\"5 seconds\"\n)\n.\nwhen\n(\n$\n\"userId\"\n===\n\"user2\"\n,\n\"20 seconds\"\n)\n.\notherwise\n(\n\"5 minutes\"\n))\n// Group the data by session window and userId, and compute the count of each group\nval\nsessionizedCounts\n=\nevents\n.\nwithWatermark\n(\n\"timestamp\"\n,\n\"10 minutes\"\n)\n.\ngroupBy\n(\nColumn\n(\nsessionWindow\n),\n$\n\"userId\"\n)\n.\ncount\n()\nDataset\n<\nRow\n>\nevents\n=\n...\n// streaming DataFrame of schema { timestamp: Timestamp, userId: String }\nSessionWindow\nsessionWindow\n=\nsession_window\n(\ncol\n(\n\"timestamp\"\n),\nwhen\n(\ncol\n(\n\"userId\"\n).\nequalTo\n(\n\"user1\"\n),\n\"5 seconds\"\n)\n.\nwhen\n(\ncol\n(\n\"userId\"\n).\nequalTo\n(\n\"user2\"\n),\n\"20 seconds\"\n)\n.\notherwise\n(\n\"5 minutes\"\n))\n// Group the data by session window and userId, and compute the count of each group\nDataset\n<\nRow\n>\nsessionizedCounts\n=\nevents\n.\nwithWatermark\n(\n\"timestamp\"\n,\n\"10 minutes\"\n)\n.\ngroupBy\n(\nnew\nColumn\n(\nsessionWindow\n),\ncol\n(\n\"userId\"\n))\n.\ncount\n();\nNote that there are some restrictions when you use session window in streaming query, like below:\n“Update mode” as output mode is not supported.\nThere should be at least one column in addition to\nsession_window\nin grouping key.\nFor batch query, global window (only having\nsession_window\nin grouping key) is supported.\nBy default, Spark does not perform partial aggregation for session window aggregation, since it requires additional\nsort in local partitions before grouping. It works better for the case there are only few number of input rows in\nsame group key for each local partition, but for the case there are numerous input rows having same group key in\nlocal partition, doing partial aggregation can still increase the performance significantly despite additional sort.\nYou can enable\nspark.sql.streaming.sessionWindow.merge.sessions.in.local.partition\nto indicate Spark to perform partial aggregation.\nRepresentation of the time for time window\nIn some use cases, it is necessary to extract the representation of the time for time window, to apply operations requiring timestamp to the time windowed data.\nOne example is chained time window aggregations, where users want to define another time window against the time window. Say, someone wants to aggregate 5 minutes time windows as 1 hour tumble time window.\nThere are two ways to achieve this, like below:\nUse\nwindow_time\nSQL function with time window column as parameter\nUse\nwindow\nSQL function with time window column as parameter\nwindow_time\nfunction will produce a timestamp which represents the time for time window.\nUser can pass the result to the parameter of\nwindow\nfunction (or anywhere requiring timestamp) to perform operation(s) with time window which requires timestamp.\nwords\n=\n...\n# streaming DataFrame of schema { timestamp: Timestamp, word: String }\n# Group the data by window and word and compute the count of each group\nwindowedCounts\n=\nwords\n.\ngroupBy\n(\nwindow\n(\nwords\n.\ntimestamp\n,\n\"\n10 minutes\n\"\n,\n\"\n5 minutes\n\"\n),\nwords\n.\nword\n).\ncount\n()\n# Group the windowed data by another window and word and compute the count of each group\nanotherWindowedCounts\n=\nwindowedCounts\n.\ngroupBy\n(\nwindow\n(\nwindow_time\n(\nwindowedCounts\n.\nwindow\n),\n\"\n1 hour\n\"\n),\nwindowedCounts\n.\nword\n).\ncount\n()\nimport\nspark.implicits._\nval\nwords\n=\n...\n// streaming DataFrame of schema { timestamp: Timestamp, word: String }\n// Group the data by window and word and compute the count of each group\nval\nwindowedCounts\n=\nwords\n.\ngroupBy\n(\nwindow\n(\n$\n\"timestamp\"\n,\n\"10 minutes\"\n,\n\"5 minutes\"\n),\n$\n\"word\"\n).\ncount\n()\n// Group the windowed data by another window and word and compute the count of each group\nval\nanotherWindowedCounts\n=\nwindowedCounts\n.\ngroupBy\n(\nwindow\n(\nwindow_time\n(\n$\n\"window\"\n),\n\"1 hour\"\n),\n$\n\"word\"\n).\ncount\n()\nDataset\n<\nRow\n>\nwords\n=\n...\n// streaming DataFrame of schema { timestamp: Timestamp, word: String }\n// Group the data by window and word and compute the count of each group\nDataset\n<\nRow\n>\nwindowedCounts\n=\nwords\n.\ngroupBy\n(\nfunctions\n.\nwindow\n(\nwords\n.\ncol\n(\n\"timestamp\"\n),\n\"10 minutes\"\n,\n\"5 minutes\"\n),\nwords\n.\ncol\n(\n\"word\"\n)\n).\ncount\n();\n// Group the windowed data by another window and word and compute the count of each group\nDataset\n<\nRow\n>\nanotherWindowedCounts\n=\nwindowedCounts\n.\ngroupBy\n(\nfunctions\n.\nwindow\n(\nfunctions\n.\nwindow_time\n(\n\"window\"\n),\n\"1 hour\"\n),\nwindowedCounts\n.\ncol\n(\n\"word\"\n)\n).\ncount\n();\nwindow\nfunction does not only take timestamp column, but also take the time window column. This is specifically useful for cases where users want to apply chained time window aggregations.\nwords\n=\n...\n# streaming DataFrame of schema { timestamp: Timestamp, word: String }\n# Group the data by window and word and compute the count of each group\nwindowedCounts\n=\nwords\n.\ngroupBy\n(\nwindow\n(\nwords\n.\ntimestamp\n,\n\"\n10 minutes\n\"\n,\n\"\n5 minutes\n\"\n),\nwords\n.\nword\n).\ncount\n()\n# Group the windowed data by another window and word and compute the count of each group\nanotherWindowedCounts\n=\nwindowedCounts\n.\ngroupBy\n(\nwindow\n(\nwindowedCounts\n.\nwindow\n,\n\"\n1 hour\n\"\n),\nwindowedCounts\n.\nword\n).\ncount\n()\nimport\nspark.implicits._\nval\nwords\n=\n...\n// streaming DataFrame of schema { timestamp: Timestamp, word: String }\n// Group the data by window and word and compute the count of each group\nval\nwindowedCounts\n=\nwords\n.\ngroupBy\n(\nwindow\n(\n$\n\"timestamp\"\n,\n\"10 minutes\"\n,\n\"5 minutes\"\n),\n$\n\"word\"\n).\ncount\n()\n// Group the windowed data by another window and word and compute the count of each group\nval\nanotherWindowedCounts\n=\nwindowedCounts\n.\ngroupBy\n(\nwindow\n(\n$\n\"window\"\n,\n\"1 hour\"\n),\n$\n\"word\"\n).\ncount\n()\nDataset\n<\nRow\n>\nwords\n=\n...\n// streaming DataFrame of schema { timestamp: Timestamp, word: String }\n// Group the data by window and word and compute the count of each group\nDataset\n<\nRow\n>\nwindowedCounts\n=\nwords\n.\ngroupBy\n(\nfunctions\n.\nwindow\n(\nwords\n.\ncol\n(\n\"timestamp\"\n),\n\"10 minutes\"\n,\n\"5 minutes\"\n),\nwords\n.\ncol\n(\n\"word\"\n)\n).\ncount\n();\n// Group the windowed data by another window and word and compute the count of each group\nDataset\n<\nRow\n>\nanotherWindowedCounts\n=\nwindowedCounts\n.\ngroupBy\n(\nfunctions\n.\nwindow\n(\n\"window\"\n,\n\"1 hour\"\n),\nwindowedCounts\n.\ncol\n(\n\"word\"\n)\n).\ncount\n();\nConditions for watermarking to clean aggregation state\nIt is important to note that the following conditions must be satisfied for the watermarking to\nclean the state in aggregation queries\n(as of Spark 2.1.1, subject to change in the future)\n.\nOutput mode must be Append or Update.\nComplete mode requires all aggregate data to be preserved,\nand hence cannot use watermarking to drop intermediate state. See the\nOutput Modes\nsection for detailed explanation of the semantics of each output mode.\nThe aggregation must have either the event-time column, or a\nwindow\non the event-time column.\nwithWatermark\nmust be called on the\nsame column as the timestamp column used in the aggregate. For example,\ndf.withWatermark(\"time\", \"1 min\").groupBy(\"time2\").count()\nis invalid\nin Append output mode, as watermark is defined on a different column\nfrom the aggregation column.\nwithWatermark\nmust be called before the aggregation for the watermark details to be used.\nFor example,\ndf.groupBy(\"time\").count().withWatermark(\"time\", \"1 min\")\nis invalid in Append\noutput mode.\nSemantic Guarantees of Aggregation with Watermarking\nA watermark delay (set with\nwithWatermark\n) of “2 hours” guarantees that the engine will never\ndrop any data that is less than 2 hours delayed. In other words, any data less than 2 hours behind\n(in terms of event-time) the latest data processed till then is guaranteed to be aggregated.\nHowever, the guarantee is strict only in one direction. Data delayed by more than 2 hours is\nnot guaranteed to be dropped; it may or may not get aggregated. More delayed is the data, less\nlikely is the engine going to process it.\nJoin Operations\nStructured Streaming supports joining a streaming Dataset/DataFrame with a static Dataset/DataFrame\nas well as another streaming Dataset/DataFrame. The result of the streaming join is generated\nincrementally, similar to the results of streaming aggregations in the previous section. In this\nsection we will explore what type of joins (i.e. inner, outer, semi, etc.) are supported in the above\ncases. Note that in all the supported join types, the result of the join with a streaming\nDataset/DataFrame will be the exactly the same as if it was with a static Dataset/DataFrame\ncontaining the same data in the stream.\nStream-static Joins\nSince the introduction in Spark 2.0, Structured Streaming has supported joins (inner join and some\ntype of outer joins) between a streaming and a static DataFrame/Dataset. Here is a simple example.\nstaticDf\n=\nspark\n.\nread\n.\n...\nstreamingDf\n=\nspark\n.\nreadStream\n.\n...\nstreamingDf\n.\njoin\n(\nstaticDf\n,\n\"\ntype\n\"\n)\n# inner equi-join with a static DF\nstreamingDf\n.\njoin\n(\nstaticDf\n,\n\"\ntype\n\"\n,\n\"\nleft_outer\n\"\n)\n# left outer join with a static DF\nval\nstaticDf\n=\nspark\n.\nread\n.\n...\nval\nstreamingDf\n=\nspark\n.\nreadStream\n.\n...\nstreamingDf\n.\njoin\n(\nstaticDf\n,\n\"type\"\n)\n// inner equi-join with a static DF\nstreamingDf\n.\njoin\n(\nstaticDf\n,\n\"type\"\n,\n\"left_outer\"\n)\n// left outer join with a static DF\nDataset\n<\nRow\n>\nstaticDf\n=\nspark\n.\nread\n().\n...;\nDataset\n<\nRow\n>\nstreamingDf\n=\nspark\n.\nreadStream\n().\n...;\nstreamingDf\n.\njoin\n(\nstaticDf\n,\n\"type\"\n);\n// inner equi-join with a static DF\nstreamingDf\n.\njoin\n(\nstaticDf\n,\n\"type\"\n,\n\"left_outer\"\n);\n// left outer join with a static DF\nstaticDf\n<-\nread.df\n(\n...\n)\nstreamingDf\n<-\nread.stream\n(\n...\n)\njoined\n<-\nmerge\n(\nstreamingDf\n,\nstaticDf\n,\nsort\n=\nFALSE\n)\n# inner equi-join with a static DF\njoined\n<-\njoin\n(\nstreamingDf\n,\nstaticDf\n,\nstreamingDf\n$\nvalue\n==\nstaticDf\n$\nvalue\n,\n\"left_outer\"\n)\n# left outer join with a static DF\nNote that stream-static joins are not stateful, so no state management is necessary.\nHowever, a few types of stream-static outer joins are not yet supported.\nThese are listed at the\nend of this Join section\n.\nStream-stream Joins\nIn Spark 2.3, we have added support for stream-stream joins, that is, you can join two streaming\nDatasets/DataFrames. The challenge of generating join results between two data streams is that,\nat any point of time, the view of the dataset is incomplete for both sides of the join making\nit much harder to find matches between inputs. Any row received from one input stream can match\nwith any future, yet-to-be-received row from the other input stream. Hence, for both the input\nstreams, we buffer past input as streaming state, so that we can match every future input with\npast input and accordingly generate joined results. Furthermore, similar to streaming aggregations,\nwe automatically handle late, out-of-order data and can limit the state using watermarks.\nLet’s discuss the different types of supported stream-stream joins and how to use them.\nInner Joins with optional Watermarking\nInner joins on any kind of columns along with any kind of join conditions are supported.\nHowever, as the stream runs, the size of streaming state will keep growing indefinitely as\nall\npast input must be saved as any new input can match with any input from the past.\nTo avoid unbounded state, you have to define additional join conditions such that indefinitely\nold inputs cannot match with future inputs and therefore can be cleared from the state.\nIn other words, you will have to do the following additional steps in the join.\nDefine watermark delays on both inputs such that the engine knows how delayed the input can be\n(similar to streaming aggregations)\nDefine a constraint on event-time across the two inputs such that the engine can figure out when\nold rows of one input is not going to be required (i.e. will not satisfy the time constraint) for\nmatches with the other input. This constraint can be defined in one of the two ways.\nTime range join conditions (e.g.\n...JOIN ON leftTime BETWEEN rightTime AND rightTime + INTERVAL 1 HOUR\n),\nJoin on event-time windows (e.g.\n...JOIN ON leftTimeWindow = rightTimeWindow\n).\nLet’s understand this with an example.\nLet’s say we want to join a stream of advertisement impressions (when an ad was shown) with\nanother stream of user clicks on advertisements to correlate when impressions led to\nmonetizable clicks. To allow the state cleanup in this stream-stream join, you will have to\nspecify the watermarking delays and the time constraints as follows.\nWatermark delays: Say, the impressions and the corresponding clicks can be late/out-of-order\nin event-time by at most 2 and 3 hours, respectively.\nEvent-time range condition: Say, a click can occur within a time range of 0 seconds to 1 hour\nafter the corresponding impression.\nThe code would look like this.\nfrom\npyspark.sql.functions\nimport\nexpr\nimpressions\n=\nspark\n.\nreadStream\n.\n...\nclicks\n=\nspark\n.\nreadStream\n.\n...\n# Apply watermarks on event-time columns\nimpressionsWithWatermark\n=\nimpressions\n.\nwithWatermark\n(\n\"\nimpressionTime\n\"\n,\n\"\n2 hours\n\"\n)\nclicksWithWatermark\n=\nclicks\n.\nwithWatermark\n(\n\"\nclickTime\n\"\n,\n\"\n3 hours\n\"\n)\n# Join with event-time constraints\nimpressionsWithWatermark\n.\njoin\n(\nclicksWithWatermark\n,\nexpr\n(\n\"\"\"\nclickAdId = impressionAdId AND\n    clickTime >= impressionTime AND\n    clickTime <= impressionTime + interval 1 hour\n\"\"\"\n)\n)\nimport\norg.apache.spark.sql.functions.expr\nval\nimpressions\n=\nspark\n.\nreadStream\n.\n...\nval\nclicks\n=\nspark\n.\nreadStream\n.\n...\n// Apply watermarks on event-time columns\nval\nimpressionsWithWatermark\n=\nimpressions\n.\nwithWatermark\n(\n\"impressionTime\"\n,\n\"2 hours\"\n)\nval\nclicksWithWatermark\n=\nclicks\n.\nwithWatermark\n(\n\"clickTime\"\n,\n\"3 hours\"\n)\n// Join with event-time constraints\nimpressionsWithWatermark\n.\njoin\n(\nclicksWithWatermark\n,\nexpr\n(\n\"\"\"\n    clickAdId = impressionAdId AND\n    clickTime >= impressionTime AND\n    clickTime <= impressionTime + interval 1 hour\n    \"\"\"\n)\n)\nimport\nstatic\norg\n.\napache\n.\nspark\n.\nsql\n.\nfunctions\n.\nexpr\nDataset\n<\nRow\n>\nimpressions\n=\nspark\n.\nreadStream\n().\n...\nDataset\n<\nRow\n>\nclicks\n=\nspark\n.\nreadStream\n().\n...\n// Apply watermarks on event-time columns\nDataset\n<\nRow\n>\nimpressionsWithWatermark\n=\nimpressions\n.\nwithWatermark\n(\n\"impressionTime\"\n,\n\"2 hours\"\n);\nDataset\n<\nRow\n>\nclicksWithWatermark\n=\nclicks\n.\nwithWatermark\n(\n\"clickTime\"\n,\n\"3 hours\"\n);\n// Join with event-time constraints\nimpressionsWithWatermark\n.\njoin\n(\nclicksWithWatermark\n,\nexpr\n(\n\"clickAdId = impressionAdId AND \"\n+\n\"clickTime >= impressionTime AND \"\n+\n\"clickTime <= impressionTime + interval 1 hour \"\n)\n);\nimpressions\n<-\nread.stream\n(\n...\n)\nclicks\n<-\nread.stream\n(\n...\n)\n# Apply watermarks on event-time columns\nimpressionsWithWatermark\n<-\nwithWatermark\n(\nimpressions\n,\n\"impressionTime\"\n,\n\"2 hours\"\n)\nclicksWithWatermark\n<-\nwithWatermark\n(\nclicks\n,\n\"clickTime\"\n,\n\"3 hours\"\n)\n# Join with event-time constraints\njoined\n<-\njoin\n(\nimpressionsWithWatermark\n,\nclicksWithWatermark\n,\nexpr\n(\npaste\n(\n\"clickAdId = impressionAdId AND\"\n,\n\"clickTime >= impressionTime AND\"\n,\n\"clickTime <= impressionTime + interval 1 hour\"\n)))\nSemantic Guarantees of Stream-stream Inner Joins with Watermarking\nThis is similar to the\nguarantees provided by watermarking on aggregations\n.\nA watermark delay of “2 hours” guarantees that the engine will never drop any data that is less than\n 2 hours delayed. But data delayed by more than 2 hours may or may not get processed.\nOuter Joins with Watermarking\nWhile the watermark + event-time constraints is optional for inner joins, for outer joins\nthey must be specified. This is because for generating the NULL results in outer join, the\nengine must know when an input row is not going to match with anything in future. Hence, the\nwatermark + event-time constraints must be specified for generating correct results. Therefore,\na query with outer-join will look quite like the ad-monetization example earlier, except that\nthere will be an additional parameter specifying it to be an outer-join.\nimpressionsWithWatermark\n.\njoin\n(\nclicksWithWatermark\n,\nexpr\n(\n\"\"\"\nclickAdId = impressionAdId AND\n    clickTime >= impressionTime AND\n    clickTime <= impressionTime + interval 1 hour\n\"\"\"\n),\n\"\nleftOuter\n\"\n# can be \"inner\", \"leftOuter\", \"rightOuter\", \"fullOuter\", \"leftSemi\"\n)\nimpressionsWithWatermark\n.\njoin\n(\nclicksWithWatermark\n,\nexpr\n(\n\"\"\"\n    clickAdId = impressionAdId AND\n    clickTime >= impressionTime AND\n    clickTime <= impressionTime + interval 1 hour\n    \"\"\"\n),\njoinType\n=\n\"leftOuter\"\n// can be \"inner\", \"leftOuter\", \"rightOuter\", \"fullOuter\", \"leftSemi\"\n)\nimpressionsWithWatermark\n.\njoin\n(\nclicksWithWatermark\n,\nexpr\n(\n\"clickAdId = impressionAdId AND \"\n+\n\"clickTime >= impressionTime AND \"\n+\n\"clickTime <= impressionTime + interval 1 hour \"\n),\n\"leftOuter\"\n// can be \"inner\", \"leftOuter\", \"rightOuter\", \"fullOuter\", \"leftSemi\"\n);\njoined\n<-\njoin\n(\nimpressionsWithWatermark\n,\nclicksWithWatermark\n,\nexpr\n(\npaste\n(\n\"clickAdId = impressionAdId AND\"\n,\n\"clickTime >= impressionTime AND\"\n,\n\"clickTime <= impressionTime + interval 1 hour\"\n),\n\"left_outer\"\n# can be \"inner\", \"left_outer\", \"right_outer\", \"full_outer\", \"left_semi\"\n))\nSemantic Guarantees of Stream-stream Outer Joins with Watermarking\nOuter joins have the same guarantees as\ninner joins\nregarding watermark delays and whether data will be dropped or not.\nCaveats\nThere are a few important characteristics to note regarding how the outer results are generated.\nThe outer NULL results will be generated with a delay that depends on the specified watermark\ndelay and the time range condition.\nThis is because the engine has to wait for that long to ensure\nthere were no matches and there will be no more matches in future.\nIn the current implementation in the micro-batch engine, watermarks are advanced at the end of a\nmicro-batch, and the next micro-batch uses the updated watermark to clean up state and output\nouter results. Since we trigger a micro-batch only when there is new data to be processed, the\ngeneration of the outer result may get delayed if there no new data being received in the stream.\nIn short, if any of the two input streams being joined does not receive data for a while, the\nouter (both cases, left or right) output may get delayed.\nSemi Joins with Watermarking\nA semi join returns values from the left side of the relation that has a match with the right.\nIt is also referred to as a left semi join. Similar to outer joins, watermark + event-time\nconstraints must be specified for semi join. This is to evict unmatched input rows on left side,\nthe engine must know when an input row on left side is not going to match with anything on right\nside in future.\nSemantic Guarantees of Stream-stream Semi Joins with Watermarking\nSemi joins have the same guarantees as\ninner joins\nregarding watermark delays and whether data will be dropped or not.\nSupport matrix for joins in streaming queries\nLeft Input\nRight Input\nJoin Type\nStatic\nStatic\nAll types\nSupported, since its not on streaming data even though it\n        can be present in a streaming query\nStream\nStatic\nInner\nSupported, not stateful\nLeft Outer\nSupported, not stateful\nRight Outer\nNot supported\nFull Outer\nNot supported\nLeft Semi\nSupported, not stateful\nStatic\nStream\nInner\nSupported, not stateful\nLeft Outer\nNot supported\nRight Outer\nSupported, not stateful\nFull Outer\nNot supported\nLeft Semi\nNot supported\nStream\nStream\nInner\nSupported, optionally specify watermark on both sides +\n      time constraints for state cleanup\nLeft Outer\nConditionally supported, must specify watermark on right + time constraints for correct\n      results, optionally specify watermark on left for all state cleanup\nRight Outer\nConditionally supported, must specify watermark on left + time constraints for correct\n      results, optionally specify watermark on right for all state cleanup\nFull Outer\nConditionally supported, must specify watermark on one side + time constraints for correct\n      results, optionally specify watermark on the other side for all state cleanup\nLeft Semi\nConditionally supported, must specify watermark on right + time constraints for correct\n      results, optionally specify watermark on left for all state cleanup\nAdditional details on supported joins:\nJoins can be cascaded, that is, you can do\ndf1.join(df2, ...).join(df3, ...).join(df4, ....)\n.\nAs of Spark 2.4, you can use joins only when the query is in Append output mode. Other output modes are not yet supported.\nYou cannot use mapGroupsWithState and flatMapGroupsWithState before and after joins.\nIn append output mode, you can construct a query having non-map-like operations e.g. aggregation, deduplication, stream-stream join before/after join.\nFor example, here’s an example of time window aggregation in both streams followed by stream-stream join with event time window:\nclicksWindow\n=\nclicksWithWatermark\n.\ngroupBy\n(\nclicksWithWatermark\n.\nclickAdId\n,\nwindow\n(\nclicksWithWatermark\n.\nclickTime\n,\n\"\n1 hour\n\"\n)\n).\ncount\n()\nimpressionsWindow\n=\nimpressionsWithWatermark\n.\ngroupBy\n(\nimpressionsWithWatermark\n.\nimpressionAdId\n,\nwindow\n(\nimpressionsWithWatermark\n.\nimpressionTime\n,\n\"\n1 hour\n\"\n)\n).\ncount\n()\nclicksWindow\n.\njoin\n(\nimpressionsWindow\n,\n\"\nwindow\n\"\n,\n\"\ninner\n\"\n)\nval\nclicksWindow\n=\nclicksWithWatermark\n.\ngroupBy\n(\nwindow\n(\n\"clickTime\"\n,\n\"1 hour\"\n))\n.\ncount\n()\nval\nimpressionsWindow\n=\nimpressionsWithWatermark\n.\ngroupBy\n(\nwindow\n(\n\"impressionTime\"\n,\n\"1 hour\"\n))\n.\ncount\n()\nclicksWindow\n.\njoin\n(\nimpressionsWindow\n,\n\"window\"\n,\n\"inner\"\n)\nDataset\n<\nRow\n>\nclicksWindow\n=\nclicksWithWatermark\n.\ngroupBy\n(\nfunctions\n.\nwindow\n(\nclicksWithWatermark\n.\ncol\n(\n\"clickTime\"\n),\n\"1 hour\"\n))\n.\ncount\n();\nDataset\n<\nRow\n>\nimpressionsWindow\n=\nimpressionsWithWatermark\n.\ngroupBy\n(\nfunctions\n.\nwindow\n(\nimpressionsWithWatermark\n.\ncol\n(\n\"impressionTime\"\n),\n\"1 hour\"\n))\n.\ncount\n();\nclicksWindow\n.\njoin\n(\nimpressionsWindow\n,\n\"window\"\n,\n\"inner\"\n);\nHere’s another example of stream-stream join with time range join condition followed by time window aggregation:\njoined\n=\nimpressionsWithWatermark\n.\njoin\n(\nclicksWithWatermark\n,\nexpr\n(\n\"\"\"\nclickAdId = impressionAdId AND\n    clickTime >= impressionTime AND\n    clickTime <= impressionTime + interval 1 hour\n\"\"\"\n),\n\"\nleftOuter\n\"\n# can be \"inner\", \"leftOuter\", \"rightOuter\", \"fullOuter\", \"leftSemi\"\n)\njoined\n.\ngroupBy\n(\njoined\n.\nclickAdId\n,\nwindow\n(\njoined\n.\nclickTime\n,\n\"\n1 hour\n\"\n)\n).\ncount\n()\nval\njoined\n=\nimpressionsWithWatermark\n.\njoin\n(\nclicksWithWatermark\n,\nexpr\n(\n\"\"\"\n    clickAdId = impressionAdId AND\n    clickTime >= impressionTime AND\n    clickTime <= impressionTime + interval 1 hour\n  \"\"\"\n),\njoinType\n=\n\"leftOuter\"\n// can be \"inner\", \"leftOuter\", \"rightOuter\", \"fullOuter\", \"leftSemi\"\n)\njoined\n.\ngroupBy\n(\n$\n\"clickAdId\"\n,\nwindow\n(\n$\n\"clickTime\"\n,\n\"1 hour\"\n))\n.\ncount\n()\nDataset\n<\nRow\n>\njoined\n=\nimpressionsWithWatermark\n.\njoin\n(\nclicksWithWatermark\n,\nexpr\n(\n\"clickAdId = impressionAdId AND \"\n+\n\"clickTime >= impressionTime AND \"\n+\n\"clickTime <= impressionTime + interval 1 hour \"\n),\n\"leftOuter\"\n// can be \"inner\", \"leftOuter\", \"rightOuter\", \"fullOuter\", \"leftSemi\"\n);\njoined\n.\ngroupBy\n(\njoined\n.\ncol\n(\n\"clickAdId\"\n),\nfunctions\n.\nwindow\n(\njoined\n.\ncol\n(\n\"clickTime\"\n),\n\"1 hour\"\n))\n.\ncount\n();\nStreaming Deduplication\nYou can deduplicate records in data streams using a unique identifier in the events. This is exactly same as deduplication on static using a unique identifier column. The query will store the necessary amount of data from previous records such that it can filter duplicate records. Similar to aggregations, you can use deduplication with or without watermarking.\nWith watermark\n- If there is an upper bound on how late a duplicate record may arrive, then you can define a watermark on an event time column and deduplicate using both the guid and the event time columns. The query will use the watermark to remove old state data from past records that are not expected to get any duplicates any more. This bounds the amount of the state the query has to maintain.\nWithout watermark\n- Since there are no bounds on when a duplicate record may arrive, the query stores the data from all the past records as state.\nstreamingDf\n=\nspark\n.\nreadStream\n.\n...\n# Without watermark using guid column\nstreamingDf\n.\ndropDuplicates\n([\n\"\nguid\n\"\n])\n# With watermark using guid and eventTime columns\nstreamingDf\n\\\n.\nwithWatermark\n(\n\"\neventTime\n\"\n,\n\"\n10 seconds\n\"\n)\n\\\n.\ndropDuplicates\n([\n\"\nguid\n\"\n,\n\"\neventTime\n\"\n])\nval\nstreamingDf\n=\nspark\n.\nreadStream\n.\n...\n// columns: guid, eventTime, ...\n// Without watermark using guid column\nstreamingDf\n.\ndropDuplicates\n(\n\"guid\"\n)\n// With watermark using guid and eventTime columns\nstreamingDf\n.\nwithWatermark\n(\n\"eventTime\"\n,\n\"10 seconds\"\n)\n.\ndropDuplicates\n(\n\"guid\"\n,\n\"eventTime\"\n)\nDataset\n<\nRow\n>\nstreamingDf\n=\nspark\n.\nreadStream\n().\n...;\n// columns: guid, eventTime, ...\n// Without watermark using guid column\nstreamingDf\n.\ndropDuplicates\n(\n\"guid\"\n);\n// With watermark using guid and eventTime columns\nstreamingDf\n.\nwithWatermark\n(\n\"eventTime\"\n,\n\"10 seconds\"\n)\n.\ndropDuplicates\n(\n\"guid\"\n,\n\"eventTime\"\n);\nstreamingDf\n<-\nread.stream\n(\n...\n)\n# Without watermark using guid column\nstreamingDf\n<-\ndropDuplicates\n(\nstreamingDf\n,\n\"guid\"\n)\n# With watermark using guid and eventTime columns\nstreamingDf\n<-\nwithWatermark\n(\nstreamingDf\n,\n\"eventTime\"\n,\n\"10 seconds\"\n)\nstreamingDf\n<-\ndropDuplicates\n(\nstreamingDf\n,\n\"guid\"\n,\n\"eventTime\"\n)\nSpecifically for streaming, you can deduplicate records in data streams using a unique identifier in the events, within the time range of watermark.\nFor example, if you set the delay threshold of watermark as “1 hour”, duplicated events which occurred within 1 hour can be correctly deduplicated.\n(For more details, please refer to the API doc of\ndropDuplicatesWithinWatermark\n.)\nThis can be used to deal with use case where event time column cannot be a part of unique identifier, mostly due to the case\nwhere event times are somehow different for the same records. (E.g. non-idempotent writer where issuing event time happens at write)\nUsers are encouraged to set the delay threshold of watermark longer than max timestamp differences among duplicated events.\nThis feature requires watermark with delay threshold to be set in streaming DataFrame/Dataset.\nstreamingDf\n=\nspark\n.\nreadStream\n.\n...\n# deduplicate using guid column with watermark based on eventTime column\nstreamingDf\n\\\n.\nwithWatermark\n(\n\"\neventTime\n\"\n,\n\"\n10 hours\n\"\n)\n\\\n.\ndropDuplicatesWithinWatermark\n([\n\"\nguid\n\"\n])\nval\nstreamingDf\n=\nspark\n.\nreadStream\n.\n...\n// columns: guid, eventTime, ...\n// deduplicate using guid column with watermark based on eventTime column\nstreamingDf\n.\nwithWatermark\n(\n\"eventTime\"\n,\n\"10 hours\"\n)\n.\ndropDuplicatesWithinWatermark\n(\n\"guid\"\n)\nDataset\n<\nRow\n>\nstreamingDf\n=\nspark\n.\nreadStream\n().\n...;\n// columns: guid, eventTime, ...\n// deduplicate using guid column with watermark based on eventTime column\nstreamingDf\n.\nwithWatermark\n(\n\"eventTime\"\n,\n\"10 hours\"\n)\n.\ndropDuplicatesWithinWatermark\n(\n\"guid\"\n);\nPolicy for handling multiple watermarks\nA streaming query can have multiple input streams that are unioned or joined together.\nEach of the input streams can have a different threshold of late data that needs to\nbe tolerated for stateful operations. You specify these thresholds using\nwithWatermarks(\"eventTime\", delay)\non each of the input streams. For example, consider\na query with stream-stream joins between\ninputStream1\nand\ninputStream2\n.\ninputStream1\n.\nwithWatermark\n(\n\"eventTime1\"\n,\n\"1 hour\"\n)\n.\njoin\n(\ninputStream2\n.\nwithWatermark\n(\n\"eventTime2\"\n,\n\"2 hours\"\n),\njoinCondition\n)\nWhile executing the query, Structured Streaming individually tracks the maximum\nevent time seen in each input stream, calculates watermarks based on the corresponding delay,\nand chooses a single global watermark with them to be used for stateful operations. By default,\nthe minimum is chosen as the global watermark because it ensures that no data is\naccidentally dropped as too late if one of the streams falls behind the others\n(for example, one of the streams stops receiving data due to upstream failures). In other words,\nthe global watermark will safely move at the pace of the slowest stream and the query output will\nbe delayed accordingly.\nHowever, in some cases, you may want to get faster results even if it means dropping data from the\nslowest stream. Since Spark 2.4, you can set the multiple watermark policy to choose\nthe maximum value as the global watermark by setting the SQL configuration\nspark.sql.streaming.multipleWatermarkPolicy\nto\nmax\n(default is\nmin\n).\nThis lets the global watermark move at the pace of the fastest stream.\nHowever, as a side effect, data from the slower streams will be aggressively dropped. Hence, use\nthis configuration judiciously.\nArbitrary Stateful Operations\nMany usecases require more advanced stateful operations than aggregations. For example, in many usecases, you have to track sessions from data streams of events. For doing such sessionization, you will have to save arbitrary types of data as state, and perform arbitrary operations on the state using the data stream events in every trigger.\nSince Spark 2.2, this can be done using the legacy\nmapGroupsWithState\nand\nflatMapGroupsWithState\noperators. Both operators allow you to apply user-defined code on grouped Datasets to update user-defined state. For more concrete details, take a look at the API documentation (\nScala\n/\nJava\n) and the examples (\nScala\n/\nJava\n).\nSince the Spark 4.0 release, users are encouraged to use the new\ntransformWithState\noperator to build their complex stateful applications. For more details, please refer to the in-depth documentation\nhere\n.\nThough Spark cannot check and force it, the state function should be implemented with respect to the semantics of the output mode. For example, in Update mode Spark doesn’t expect that the state function will emit rows which are older than current watermark plus allowed late record delay, whereas in Append mode the state function can emit these rows.\nUnsupported Operations\nThere are a few DataFrame/Dataset operations that are not supported with streaming DataFrames/Datasets.\nSome of them are as follows.\nLimit and take the first N rows are not supported on streaming Datasets.\nDistinct operations on streaming Datasets are not supported.\nSorting operations are supported on streaming Datasets only after an aggregation and in Complete Output Mode.\nFew types of outer joins on streaming Datasets are not supported. See the\nsupport matrix in the Join Operations section\nfor more details.\nChaining multiple stateful operations on streaming Datasets is not supported with Update and Complete mode.\nIn addition, mapGroupsWithState/flatMapGroupsWithState operation followed by other stateful operation is not supported in Append mode.\nA known workaround is to split your streaming query into multiple queries having a single stateful operation per each query,\nand ensure end-to-end exactly once per query. Ensuring end-to-end exactly once for the last query is optional.\nIn addition, there are some Dataset methods that will not work on streaming Datasets. They are actions that will immediately run queries and return results, which does not make sense on a streaming Dataset. Rather, those functionalities can be done by explicitly starting a streaming query (see the next section regarding that).\ncount()\n- Cannot return a single count from a streaming Dataset. Instead, use\nds.groupBy().count()\nwhich returns a streaming Dataset containing a running count.\nforeach()\n- Instead use\nds.writeStream.foreach(...)\n(see next section).\nshow()\n- Instead use the console sink (see next section).\nIf you try any of these operations, you will see an\nAnalysisException\nlike “operation XYZ is not supported with streaming DataFrames/Datasets”.\nWhile some of them may be supported in future releases of Spark,\nthere are others which are fundamentally hard to implement on streaming data efficiently.\nFor example, sorting on the input stream is not supported, as it requires keeping\ntrack of all the data received in the stream. This is therefore fundamentally hard to execute\nefficiently.\nState Store\nState store is a versioned key-value store which provides both read and write operations. In\nStructured Streaming, we use the state store provider to handle the stateful operations across\nbatches. There are two built-in state store provider implementations. End users can also implement\ntheir own state store provider by extending StateStoreProvider interface.\nHDFS state store provider\nThe HDFS backend state store provider is the default implementation of [[StateStoreProvider]] and\n[[StateStore]] in which all the data is stored in memory map in the first stage, and then backed\nby files in an HDFS-compatible file system. All updates to the store have to be done in sets\ntransactionally, and each set of updates increments the store’s version. These versions can be\nused to re-execute the updates (by retries in RDD operations) on the correct version of the store,\nand regenerate the store version.\nRocksDB state store implementation\nAs of Spark 3.2, we add a new built-in state store implementation, RocksDB state store provider.\nIf you have stateful operations in your streaming query (for example, streaming aggregation,\nstreaming dropDuplicates, stream-stream joins, mapGroupsWithState, or flatMapGroupsWithState)\nand you want to maintain millions of keys in the state, then you may face issues related to large\nJVM garbage collection (GC) pauses causing high variations in the micro-batch processing times.\nThis occurs because, by the implementation of HDFSBackedStateStore, the state data is maintained\nin the JVM memory of the executors and large number of state objects puts memory pressure on the\nJVM causing high GC pauses.\nIn such cases, you can choose to use a more optimized state management solution based on\nRocksDB\n. Rather than keeping the state in the JVM memory, this solution\nuses RocksDB to efficiently manage the state in the native memory and the local disk. Furthermore,\nany changes to this state are automatically saved by Structured Streaming to the checkpoint\nlocation you have provided, thus providing full fault-tolerance guarantees (the same as default\nstate management).\nTo enable the new build-in state store implementation, set\nspark.sql.streaming.stateStore.providerClass\nto\norg.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider\n.\nHere are the configs regarding to RocksDB instance of the state store provider:\nConfig Name\nDescription\nDefault Value\nspark.sql.streaming.stateStore.rocksdb.compactOnCommit\nWhether we perform a range compaction of RocksDB instance for commit operation\nFalse\nspark.sql.streaming.stateStore.rocksdb.changelogCheckpointing.enabled\nWhether to upload changelog instead of snapshot during RocksDB StateStore commit\nFalse\nspark.sql.streaming.stateStore.rocksdb.blockSizeKB\nApproximate size in KB of user data packed per block for a RocksDB BlockBasedTable, which is a RocksDB's default SST file format.\n4\nspark.sql.streaming.stateStore.rocksdb.blockCacheSizeMB\nThe size capacity in MB for a cache of blocks.\n8\nspark.sql.streaming.stateStore.rocksdb.lockAcquireTimeoutMs\nThe waiting time in millisecond for acquiring lock in the load operation for RocksDB instance.\n60000\nspark.sql.streaming.stateStore.rocksdb.maxOpenFiles\nThe number of open files that can be used by the RocksDB instance. Value of -1 means that files opened are always kept open. If the open file limit is reached, RocksDB will evict entries from the open file cache and close those file descriptors and remove the entries from the cache.\n-1\nspark.sql.streaming.stateStore.rocksdb.resetStatsOnLoad\nWhether we resets all ticker and histogram stats for RocksDB on load.\nTrue\nspark.sql.streaming.stateStore.rocksdb.trackTotalNumberOfRows\nWhether we track the total number of rows in state store. Please refer the details in\nPerformance-aspect considerations\n.\nTrue\nspark.sql.streaming.stateStore.rocksdb.writeBufferSizeMB\nThe maximum size of MemTable in RocksDB. Value of -1 means that RocksDB internal default values will be used\n-1\nspark.sql.streaming.stateStore.rocksdb.maxWriteBufferNumber\nThe maximum number of MemTables in RocksDB, both active and immutable. Value of -1 means that RocksDB internal default values will be used\n-1\nspark.sql.streaming.stateStore.rocksdb.boundedMemoryUsage\nWhether total memory usage for RocksDB state store instances on a single node is bounded.\nfalse\nspark.sql.streaming.stateStore.rocksdb.maxMemoryUsageMB\nTotal memory limit in MB for RocksDB state store instances on a single node.\n500\nspark.sql.streaming.stateStore.rocksdb.writeBufferCacheRatio\nTotal memory to be occupied by write buffers as a fraction of memory allocated across all RocksDB instances on a single node using maxMemoryUsageMB.\n0.5\nspark.sql.streaming.stateStore.rocksdb.highPriorityPoolRatio\nTotal memory to be occupied by blocks in high priority pool as a fraction of memory allocated across all RocksDB instances on a single node using maxMemoryUsageMB.\n0.1\nspark.sql.streaming.stateStore.rocksdb.allowFAllocate\nAllow the rocksdb runtime to use fallocate to pre-allocate disk space for logs, etc...  Disable for apps that have many smaller state stores to trade off disk space for write performance.\ntrue\nspark.sql.streaming.stateStore.rocksdb.compression\nCompression type used in RocksDB. The string is converted RocksDB compression type through RocksDB Java API getCompressionType().\nlz4\nRocksDB State Store Memory Management\nRocksDB allocates memory for different objects such as memtables, block cache and filter/index blocks. If left unbounded, RocksDB memory usage across multiple instances could grow indefinitely and potentially cause OOM (out-of-memory) issues.\nRocksDB provides a way to limit the memory usage for all DB instances running on a single node by using the write buffer manager functionality.\nIf you want to cap RocksDB memory usage in your Spark Structured Streaming deployment, this feature can be enabled by setting the\nspark.sql.streaming.stateStore.rocksdb.boundedMemoryUsage\nconfig to\ntrue\n.\nYou can also determine the max allowed memory for RocksDB instances by setting the\nspark.sql.streaming.stateStore.rocksdb.maxMemoryUsageMB\nvalue to a static number or as a fraction of the physical memory available on the node.\nLimits for individual RocksDB instances can also be configured by setting\nspark.sql.streaming.stateStore.rocksdb.writeBufferSizeMB\nand\nspark.sql.streaming.stateStore.rocksdb.maxWriteBufferNumber\nto the required values. By default, RocksDB internal defaults are used for these settings.\nNote that the\nboundedMemoryUsage\nconfig will enable a soft limit on the total memory usage for RocksDB.\nSo the total memory used by RocksDB can temporarily exceed this value if all blocks allocated to higher level readers are in use.\nEnabling a strict limit is not possible at this time since it will cause query failures and we do not support re-balancing of the state across additional nodes.\nRocksDB State Store Changelog Checkpointing\nIn newer version of Spark, changelog checkpointing is introduced for RocksDB state store. The traditional checkpointing mechanism for RocksDB State Store is incremental snapshot checkpointing, where the manifest files and newly generated RocksDB SST files of RocksDB instances are uploaded to a durable storage.\nInstead of uploading data files of RocksDB instances, changelog checkpointing uploads changes made to the state since the last checkpoint for durability.\nSnapshots are persisted periodically in the background for predictable failure recovery and changelog trimming.\nChangelog checkpointing avoids cost of capturing and uploading snapshots of RocksDB instances and significantly reduce streaming query latency.\nChangelog checkpointing is disabled by default. You can enable RocksDB State Store changelog checkpointing by setting\nspark.sql.streaming.stateStore.rocksdb.changelogCheckpointing.enabled\nconfig to\ntrue\n.\nChangelog checkpointing is designed to be backward compatible with traditional checkpointing mechanism.\nRocksDB state store provider offers seamless support for transitioning between two checkpointing mechanisms in both directions. This allows you to leverage the performance benefits of changelog checkpointing without discarding the old state checkpoint.\nIn a version of spark that supports changelog checkpointing, you can migrate streaming queries from older versions of Spark to changelog checkpointing by enabling changelog checkpointing in the spark session.\nVice versa, you can disable changelog checkpointing safely in newer version of Spark, then any query that already run with changelog checkpointing will switch back to traditional checkpointing.\nYou would need to restart you streaming queries for change in checkpointing mechanism to be applied, but you won’t observe any performance degrade in the process.\nPerformance-aspect considerations\nYou may want to disable the track of total number of rows to aim the better performance on RocksDB state store.\nTracking the number of rows brings additional lookup on write operations - you’re encouraged to try turning off the config on tuning RocksDB state store, especially the values of metrics for state operator are big -\nnumRowsUpdated\n,\nnumRowsRemoved\n.\nYou can change the config during restarting the query, which enables you to change the trade-off decision on “observability vs performance”.\nIf the config is disabled, the number of rows in state (\nnumTotalStateRows\n) will be reported as 0.\nState Store and task locality\nThe stateful operations store states for events in state stores of executors. State stores occupy resources such as memory and disk space to store the states.\nSo it is more efficient to keep a state store provider running in the same executor across different streaming batches.\nChanging the location of a state store provider requires the extra overhead of loading checkpointed states. The overhead of loading state from checkpoint depends\non the external storage and the size of the state, which tends to hurt the latency of micro-batch run. For some use cases such as processing very large state data,\nloading new state store providers from checkpointed states can be very time-consuming and inefficient.\nThe stateful operations in Structured Streaming queries rely on the preferred location feature of Spark’s RDD to run the state store provider on the same executor.\nIf in the next batch the corresponding state store provider is scheduled on this executor again, it could reuse the previous states and save the time of loading checkpointed states.\nHowever, generally the preferred location is not a hard requirement and it is still possible that Spark schedules tasks to the executors other than the preferred ones.\nIn this case, Spark will load state store providers from checkpointed states on new executors. The state store providers run in the previous batch will not be unloaded immediately.\nSpark runs a maintenance task which checks and unloads the state store providers that are inactive on the executors.\nBy changing the Spark configurations related to task scheduling, for example\nspark.locality.wait\n, users can configure Spark how long to wait to launch a data-local task.\nFor stateful operations in Structured Streaming, it can be used to let state store providers running on the same executors across batches.\nSpecifically for built-in HDFS state store provider, users can check the state store metrics such as\nloadedMapCacheHitCount\nand\nloadedMapCacheMissCount\n. Ideally,\nit is best if cache missing count is minimized that means Spark won’t waste too much time on loading checkpointed state.\nUser can increase Spark locality waiting configurations to avoid loading state store providers in different executors across batches.\nState Data Source (Experimental)\nApache Spark provides a streaming state related data source that provides the ability to manipulate state stores in the checkpoint. Users can run the batch query with State Data Source to get the visibility of the states for existing streaming query.\nAs of Spark 4.0, the data source only supports read feature. See\nState Data Source Integration Guide\nfor more details.\nNOTE: this data source is currently marked as experimental - source options and the behavior (output) might be subject to change.\nStarting Streaming Queries\nOnce you have defined the final result DataFrame/Dataset, all that is left is for you to start the streaming computation. To do that, you have to use the\nDataStreamWriter\n(\nPython\n/\nScala\n/\nJava\ndocs)\nreturned through\nDataset.writeStream()\n. You will have to specify one or more of the following in this interface.\nDetails of the output sink:\nData format, location, etc.\nOutput mode:\nSpecify what gets written to the output sink.\nQuery name:\nOptionally, specify a unique name of the query for identification.\nTrigger interval:\nOptionally, specify the trigger interval. If it is not specified, the system will check for availability of new data as soon as the previous processing has been completed. If a trigger time is missed because the previous processing has not been completed, then the system will trigger processing immediately.\nCheckpoint location:\nFor some output sinks where the end-to-end fault-tolerance can be guaranteed, specify the location where the system will write all the checkpoint information. This should be a directory in an HDFS-compatible fault-tolerant file system. The semantics of checkpointing is discussed in more detail in the next section.\nOutput Modes\nThere are a few types of output modes.\nAppend mode (default)\n- This is the default mode, where only the\nnew rows added to the Result Table since the last trigger will be\noutputted to the sink. This is supported for only those queries where\nrows added to the Result Table is never going to change. Hence, this mode\nguarantees that each row will be output only once (assuming\nfault-tolerant sink). For example, queries with only\nselect\n,\nwhere\n,\nmap\n,\nflatMap\n,\nfilter\n,\njoin\n, etc. will support Append mode.\nComplete mode\n- The whole Result Table will be outputted to the sink after every trigger.\n This is supported for aggregation queries.\nUpdate mode\n- (\nAvailable since Spark 2.1.1\n) Only the rows in the Result Table that were\nupdated since the last trigger will be outputted to the sink.\nMore information to be added in future releases.\nDifferent types of streaming queries support different output modes.\nHere is the compatibility matrix.\nQuery Type\nSupported Output Modes\nNotes\nQueries with aggregation\nAggregation on event-time with watermark\nAppend, Update, Complete\nAppend mode uses watermark to drop old aggregation state. But the output of a\n        windowed aggregation is delayed the late threshold specified in\nwithWatermark()\nas by\n        the modes semantics, rows can be added to the Result Table only once after they are\n        finalized (i.e. after watermark is crossed). See the\nLate Data\nsection for more details.\nUpdate mode uses watermark to drop old aggregation state.\nComplete mode does not drop old aggregation state since by definition this mode\n        preserves all data in the Result Table.\nOther aggregations\nComplete, Update\nSince no watermark is defined (only defined in other category),\n        old aggregation state is not dropped.\nAppend mode is not supported as aggregates can update thus violating the semantics of\n        this mode.\nQueries with\nmapGroupsWithState\nUpdate\nAggregations not allowed in a query with\nmapGroupsWithState\n.\nQueries with\nflatMapGroupsWithState\nAppend operation mode\nAppend\nAggregations are allowed after\nflatMapGroupsWithState\n.\nUpdate operation mode\nUpdate\nAggregations not allowed in a query with\nflatMapGroupsWithState\n.\nQueries with\njoins\nAppend\nUpdate and Complete mode not supported yet. See the\nsupport matrix in the Join Operations section\nfor more details on what types of joins are supported.\nOther queries\nAppend, Update\nComplete mode not supported as it is infeasible to keep all unaggregated data in the Result Table.\nOutput Sinks\nThere are a few types of built-in output sinks.\nFile sink\n- Stores the output to a directory.\nwriteStream\n.\nformat\n(\n\"parquet\"\n)\n// can be \"orc\", \"json\", \"csv\", etc.\n.\noption\n(\n\"path\"\n,\n\"path/to/destination/dir\"\n)\n.\nstart\n()\nKafka sink\n- Stores the output to one or more topics in Kafka.\nwriteStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"topic\"\n,\n\"updates\"\n)\n.\nstart\n()\nForeach sink\n- Runs arbitrary computation on the records in the output. See later in the section for more details.\nwriteStream\n.\nforeach\n(...)\n.\nstart\n()\nConsole sink (for debugging)\n- Prints the output to the console/stdout every time there is a trigger. Both, Append and Complete output modes, are supported. This should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver’s memory after every trigger.\nwriteStream\n.\nformat\n(\n\"console\"\n)\n.\nstart\n()\nMemory sink (for debugging)\n- The output is stored in memory as an in-memory table.\nBoth, Append and Complete output modes, are supported. This should be used for debugging purposes\non low data volumes as the entire output is collected and stored in the driver’s memory.\nHence, use it with caution.\nwriteStream\n.\nformat\n(\n\"memory\"\n)\n.\nqueryName\n(\n\"tableName\"\n)\n.\nstart\n()\nSome sinks are not fault-tolerant because they do not guarantee persistence of the output and are\nmeant for debugging purposes only. See the earlier section on\nfault-tolerance semantics\n.\nHere are the details of all the sinks in Spark.\nSink\nSupported Output Modes\nOptions\nFault-tolerant\nNotes\nFile Sink\nAppend\npath\n: path to the output directory, must be specified.\nretention\n: time to live (TTL) for output files. Output files which batches were\n        committed older than TTL will be eventually excluded in metadata log. This means reader queries which read\n        the sink's output directory may not process them. You can provide the value as string format of the time. (like \"12h\", \"7d\", etc.)\n        By default it's disabled.\nFor file-format-specific options, see the related methods in DataFrameWriter\n        (\nPython\n/\nScala\n/\nJava\n/\nR\n).\n        E.g. for \"parquet\" format options see\nDataFrameWriter.parquet()\nYes (exactly-once)\nSupports writes to partitioned tables. Partitioning by time may be useful.\nKafka Sink\nAppend, Update, Complete\nSee the\nKafka Integration Guide\nYes (at-least-once)\nMore details in the\nKafka Integration Guide\nForeach Sink\nAppend, Update, Complete\nNone\nYes (at-least-once)\nMore details in the\nnext section\nForeachBatch Sink\nAppend, Update, Complete\nNone\nDepends on the implementation\nMore details in the\nnext section\nConsole Sink\nAppend, Update, Complete\nnumRows\n: Number of rows to print every trigger (default: 20)\ntruncate\n: Whether to truncate the output if too long (default: true)\nNo\nMemory Sink\nAppend, Complete\nNone\nNo. But in Complete Mode, restarted query will recreate the full table.\nTable name is the query name.\nNote that you have to call\nstart()\nto actually start the execution of the query. This returns a StreamingQuery object which is a handle to the continuously running execution. You can use this object to manage the query, which we will discuss in the next subsection. For now, let’s understand all this with a few examples.\n# ========== DF with no aggregations ==========\nnoAggDF\n=\ndeviceDataDf\n.\nselect\n(\n\"\ndevice\n\"\n).\nwhere\n(\n\"\nsignal > 10\n\"\n)\n# Print new data to console\nnoAggDF\n\\\n.\nwriteStream\n\\\n.\nformat\n(\n\"\nconsole\n\"\n)\n\\\n.\nstart\n()\n# Write new data to Parquet files\nnoAggDF\n\\\n.\nwriteStream\n\\\n.\nformat\n(\n\"\nparquet\n\"\n)\n\\\n.\noption\n(\n\"\ncheckpointLocation\n\"\n,\n\"\npath/to/checkpoint/dir\n\"\n)\n\\\n.\noption\n(\n\"\npath\n\"\n,\n\"\npath/to/destination/dir\n\"\n)\n\\\n.\nstart\n()\n# ========== DF with aggregation ==========\naggDF\n=\ndf\n.\ngroupBy\n(\n\"\ndevice\n\"\n).\ncount\n()\n# Print updated aggregations to console\naggDF\n\\\n.\nwriteStream\n\\\n.\noutputMode\n(\n\"\ncomplete\n\"\n)\n\\\n.\nformat\n(\n\"\nconsole\n\"\n)\n\\\n.\nstart\n()\n# Have all the aggregates in an in-memory table. The query name will be the table name\naggDF\n\\\n.\nwriteStream\n\\\n.\nqueryName\n(\n\"\naggregates\n\"\n)\n\\\n.\noutputMode\n(\n\"\ncomplete\n\"\n)\n\\\n.\nformat\n(\n\"\nmemory\n\"\n)\n\\\n.\nstart\n()\nspark\n.\nsql\n(\n\"\nselect * from aggregates\n\"\n).\nshow\n()\n# interactively query in-memory table\n// ========== DF with no aggregations ==========\nval\nnoAggDF\n=\ndeviceDataDf\n.\nselect\n(\n\"device\"\n).\nwhere\n(\n\"signal > 10\"\n)\n// Print new data to console\nnoAggDF\n.\nwriteStream\n.\nformat\n(\n\"console\"\n)\n.\nstart\n()\n// Write new data to Parquet files\nnoAggDF\n.\nwriteStream\n.\nformat\n(\n\"parquet\"\n)\n.\noption\n(\n\"checkpointLocation\"\n,\n\"path/to/checkpoint/dir\"\n)\n.\noption\n(\n\"path\"\n,\n\"path/to/destination/dir\"\n)\n.\nstart\n()\n// ========== DF with aggregation ==========\nval\naggDF\n=\ndf\n.\ngroupBy\n(\n\"device\"\n).\ncount\n()\n// Print updated aggregations to console\naggDF\n.\nwriteStream\n.\noutputMode\n(\n\"complete\"\n)\n.\nformat\n(\n\"console\"\n)\n.\nstart\n()\n// Have all the aggregates in an in-memory table\naggDF\n.\nwriteStream\n.\nqueryName\n(\n\"aggregates\"\n)\n// this query name will be the table name\n.\noutputMode\n(\n\"complete\"\n)\n.\nformat\n(\n\"memory\"\n)\n.\nstart\n()\nspark\n.\nsql\n(\n\"select * from aggregates\"\n).\nshow\n()\n// interactively query in-memory table\n// ========== DF with no aggregations ==========\nDataset\n<\nRow\n>\nnoAggDF\n=\ndeviceDataDf\n.\nselect\n(\n\"device\"\n).\nwhere\n(\n\"signal > 10\"\n);\n// Print new data to console\nnoAggDF\n.\nwriteStream\n()\n.\nformat\n(\n\"console\"\n)\n.\nstart\n();\n// Write new data to Parquet files\nnoAggDF\n.\nwriteStream\n()\n.\nformat\n(\n\"parquet\"\n)\n.\noption\n(\n\"checkpointLocation\"\n,\n\"path/to/checkpoint/dir\"\n)\n.\noption\n(\n\"path\"\n,\n\"path/to/destination/dir\"\n)\n.\nstart\n();\n// ========== DF with aggregation ==========\nDataset\n<\nRow\n>\naggDF\n=\ndf\n.\ngroupBy\n(\n\"device\"\n).\ncount\n();\n// Print updated aggregations to console\naggDF\n.\nwriteStream\n()\n.\noutputMode\n(\n\"complete\"\n)\n.\nformat\n(\n\"console\"\n)\n.\nstart\n();\n// Have all the aggregates in an in-memory table\naggDF\n.\nwriteStream\n()\n.\nqueryName\n(\n\"aggregates\"\n)\n// this query name will be the table name\n.\noutputMode\n(\n\"complete\"\n)\n.\nformat\n(\n\"memory\"\n)\n.\nstart\n();\nspark\n.\nsql\n(\n\"select * from aggregates\"\n).\nshow\n();\n// interactively query in-memory table\n# ========== DF with no aggregations ==========\nnoAggDF\n<-\nselect\n(\nwhere\n(\ndeviceDataDf\n,\n\"signal > 10\"\n),\n\"device\"\n)\n# Print new data to console\nwrite.stream\n(\nnoAggDF\n,\n\"console\"\n)\n# Write new data to Parquet files\nwrite.stream\n(\nnoAggDF\n,\n\"parquet\"\n,\npath\n=\n\"path/to/destination/dir\"\n,\ncheckpointLocation\n=\n\"path/to/checkpoint/dir\"\n)\n# ========== DF with aggregation ==========\naggDF\n<-\ncount\n(\ngroupBy\n(\ndf\n,\n\"device\"\n))\n# Print updated aggregations to console\nwrite.stream\n(\naggDF\n,\n\"console\"\n,\noutputMode\n=\n\"complete\"\n)\n# Have all the aggregates in an in memory table. The query name will be the table name\nwrite.stream\n(\naggDF\n,\n\"memory\"\n,\nqueryName\n=\n\"aggregates\"\n,\noutputMode\n=\n\"complete\"\n)\n# Interactively query in-memory table\nhead\n(\nsql\n(\n\"select * from aggregates\"\n))\nUsing Foreach and ForeachBatch\nThe\nforeach\nand\nforeachBatch\noperations allow you to apply arbitrary operations and writing\nlogic on the output of a streaming query. They have slightly different use cases - while\nforeach\nallows custom write logic on every row,\nforeachBatch\nallows arbitrary operations\nand custom logic on the output of each micro-batch. Let’s understand their usages in more detail.\nForeachBatch\nforeachBatch(...)\nallows you to specify a function that is executed on\nthe output data of every micro-batch of a streaming query. Since Spark 2.4, this is supported in Scala, Java and Python.\nIt takes two parameters: a DataFrame or Dataset that has the output data of a micro-batch and the unique ID of the micro-batch.\ndef\nforeach_batch_function\n(\ndf\n,\nepoch_id\n):\n# Transform and write batchDF\npass\nstreamingDF\n.\nwriteStream\n.\nforeachBatch\n(\nforeach_batch_function\n).\nstart\n()\nstreamingDF\n.\nwriteStream\n.\nforeachBatch\n{\n(\nbatchDF\n:\nDataFrame\n,\nbatchId\n:\nLong\n)\n=>\n// Transform and write batchDF\n}.\nstart\n()\nstreamingDatasetOfString\n.\nwriteStream\n().\nforeachBatch\n(\nnew\nVoidFunction2\n<\nDataset\n<\nString\n>,\nLong\n>()\n{\npublic\nvoid\ncall\n(\nDataset\n<\nString\n>\ndataset\n,\nLong\nbatchId\n)\n{\n// Transform and write batchDF\n}\n}\n).\nstart\n();\nR is not yet supported.\nWith\nforeachBatch\n, you can do the following.\nReuse existing batch data sources\n- For many storage systems, there may not be a streaming sink available yet,\nbut there may already exist a data writer for batch queries. Using\nforeachBatch\n, you can use the batch\ndata writers on the output of each micro-batch.\nWrite to multiple locations\n- If you want to write the output of a streaming query to multiple locations,\nthen you can simply write the output DataFrame/Dataset multiple times. However, each attempt to write can\ncause the output data to be recomputed (including possible re-reading of the input data). To avoid recomputations,\nyou should cache the output DataFrame/Dataset, write it to multiple locations, and then uncache it. Here is an outline.\nstreamingDF\n.\nwriteStream\n.\nforeachBatch\n{\n(\nbatchDF\n:\nDataFrame\n,\nbatchId\n:\nLong\n)\n=>\nbatchDF\n.\npersist\n()\nbatchDF\n.\nwrite\n.\nformat\n(...).\nsave\n(...)\n// location 1\nbatchDF\n.\nwrite\n.\nformat\n(...).\nsave\n(...)\n// location 2\nbatchDF\n.\nunpersist\n()\n}\nApply additional DataFrame operations\n- Many DataFrame and Dataset operations are not supported\nin streaming DataFrames because Spark does not support generating incremental plans in those cases.\nUsing\nforeachBatch\n, you can apply some of these operations on each micro-batch output. However, you will have to reason about the end-to-end semantics of doing that operation yourself.\nNote:\nBy default,\nforeachBatch\nprovides only at-least-once write guarantees. However, you can use the\nbatchId provided to the function as way to deduplicate the output and get an exactly-once guarantee.\nforeachBatch\ndoes not work with the continuous processing mode as it fundamentally relies on the\nmicro-batch execution of a streaming query. If you write data in the continuous mode, use\nforeach\ninstead.\nIf\nforeachBatch\nis used with stateful streaming queries and multiple DataFrame actions are performed\non the same DataFrame (such as\ndf.count()\nfollowed by\ndf.collect()\n), the query will be evaluated multiple times leading to\nthe state being reloaded multiple times within the same batch resulting in degraded performance. In this case,\nit’s highly recommended for users to call\npersist\nand\nunpersist\non the DataFrame,\nwithin the\nforeachBatch\nUDF (user-defined function) to avoid recomputation.\nForeach\nIf\nforeachBatch\nis not an option (for example, corresponding batch data writer does not exist, or\ncontinuous processing mode), then you can express your custom writer logic using\nforeach\n.\nSpecifically, you can express the data writing logic by dividing it into three methods:\nopen\n,\nprocess\n, and\nclose\n.\nSince Spark 2.4,\nforeach\nis available in Scala, Java and Python.\nIn Python, you can invoke foreach in two ways: in a function or in an object.\nThe function offers a simple way to express your processing logic but does not allow you to\ndeduplicate generated data when failures cause reprocessing of some input data.\nFor that situation you must specify the processing logic in an object.\nFirst, the function takes a row as input.\ndef\nprocess_row\n(\nrow\n):\n# Write row to storage\npass\nquery\n=\nstreamingDF\n.\nwriteStream\n.\nforeach\n(\nprocess_row\n).\nstart\n()\nSecond, the object has a process method and optional open and close methods:\nclass\nForeachWriter\n:\ndef\nopen\n(\nself\n,\npartition_id\n,\nepoch_id\n):\n# Open connection. This method is optional in Python.\npass\ndef\nprocess\n(\nself\n,\nrow\n):\n# Write row to connection. This method is NOT optional in Python.\npass\ndef\nclose\n(\nself\n,\nerror\n):\n# Close the connection. This method in optional in Python.\npass\nquery\n=\nstreamingDF\n.\nwriteStream\n.\nforeach\n(\nForeachWriter\n()).\nstart\n()\nIn Scala, you have to extend the class\nForeachWriter\n(\ndocs\n).\nstreamingDatasetOfString\n.\nwriteStream\n.\nforeach\n(\nnew\nForeachWriter\n[\nString\n]\n{\ndef\nopen\n(\npartitionId\n:\nLong\n,\nversion\n:\nLong\n)\n:\nBoolean\n=\n{\n// Open connection\n}\ndef\nprocess\n(\nrecord\n:\nString\n)\n:\nUnit\n=\n{\n// Write string to connection\n}\ndef\nclose\n(\nerrorOrNull\n:\nThrowable\n)\n:\nUnit\n=\n{\n// Close the connection\n}\n}\n).\nstart\n()\nIn Java, you have to extend the class\nForeachWriter\n(\ndocs\n).\nstreamingDatasetOfString\n.\nwriteStream\n().\nforeach\n(\nnew\nForeachWriter\n<\nString\n>()\n{\n@Override\npublic\nboolean\nopen\n(\nlong\npartitionId\n,\nlong\nversion\n)\n{\n// Open connection\n}\n@Override\npublic\nvoid\nprocess\n(\nString\nrecord\n)\n{\n// Write string to connection\n}\n@Override\npublic\nvoid\nclose\n(\nThrowable\nerrorOrNull\n)\n{\n// Close the connection\n}\n}\n).\nstart\n();\nR is not yet supported.\nExecution semantics\nWhen the streaming query is started, Spark calls the function or the object’s methods in the following way:\nA single copy of this object is responsible for all the data generated by a single task in a query.\nIn other words, one instance is responsible for processing one partition of the data generated in a distributed manner.\nThis object must be serializable, because each task will get a fresh serialized-deserialized copy\nof the provided object. Hence, it is strongly recommended that any initialization for writing data\n(for example. opening a connection or starting a transaction) is done after the open() method has\nbeen called, which signifies that the task is ready to generate data.\nThe lifecycle of the methods are as follows:\nFor each partition with partition_id:\nFor each batch/epoch of streaming data with epoch_id:\nMethod open(partitionId, epochId) is called.\nIf open(…) returns true, for each row in the partition and batch/epoch, method process(row) is called.\nMethod close(error) is called with error (if any) seen while processing rows.\nThe close() method (if it exists) is called if an open() method exists and returns successfully (irrespective of the return value), except if the JVM or Python process crashes in the middle.\nNote:\nSpark does not guarantee same output for (partitionId, epochId), so deduplication\ncannot be achieved with (partitionId, epochId). e.g. source provides different number of\npartitions for some reasons, Spark optimization changes number of partitions, etc.\nSee\nSPARK-28650\nfor more details.\nIf you need deduplication on output, try out\nforeachBatch\ninstead.\nStreaming Table APIs\nSince Spark 3.1, you can also use\nDataStreamReader.table()\nto read tables as streaming DataFrames and use\nDataStreamWriter.toTable()\nto write streaming DataFrames as tables:\nspark\n=\n...\n# spark session\n# Create a streaming DataFrame\ndf\n=\nspark\n.\nreadStream\n\\\n.\nformat\n(\n\"\nrate\n\"\n)\n\\\n.\noption\n(\n\"\nrowsPerSecond\n\"\n,\n10\n)\n\\\n.\nload\n()\n# Write the streaming DataFrame to a table\ndf\n.\nwriteStream\n\\\n.\noption\n(\n\"\ncheckpointLocation\n\"\n,\n\"\npath/to/checkpoint/dir\n\"\n)\n\\\n.\ntoTable\n(\n\"\nmyTable\n\"\n)\n# Check the table result\nspark\n.\nread\n.\ntable\n(\n\"\nmyTable\n\"\n).\nshow\n()\n# Transform the source dataset and write to a new table\nspark\n.\nreadStream\n\\\n.\ntable\n(\n\"\nmyTable\n\"\n)\n\\\n.\nselect\n(\n\"\nvalue\n\"\n)\n\\\n.\nwriteStream\n\\\n.\noption\n(\n\"\ncheckpointLocation\n\"\n,\n\"\npath/to/checkpoint/dir\n\"\n)\n\\\n.\nformat\n(\n\"\nparquet\n\"\n)\n\\\n.\ntoTable\n(\n\"\nnewTable\n\"\n)\n# Check the new table result\nspark\n.\nread\n.\ntable\n(\n\"\nnewTable\n\"\n).\nshow\n()\nval\nspark\n:\nSparkSession\n=\n...\n// Create a streaming DataFrame\nval\ndf\n=\nspark\n.\nreadStream\n.\nformat\n(\n\"rate\"\n)\n.\noption\n(\n\"rowsPerSecond\"\n,\n10\n)\n.\nload\n()\n// Write the streaming DataFrame to a table\ndf\n.\nwriteStream\n.\noption\n(\n\"checkpointLocation\"\n,\n\"path/to/checkpoint/dir\"\n)\n.\ntoTable\n(\n\"myTable\"\n)\n// Check the table result\nspark\n.\nread\n.\ntable\n(\n\"myTable\"\n).\nshow\n()\n// Transform the source dataset and write to a new table\nspark\n.\nreadStream\n.\ntable\n(\n\"myTable\"\n)\n.\nselect\n(\n\"value\"\n)\n.\nwriteStream\n.\noption\n(\n\"checkpointLocation\"\n,\n\"path/to/checkpoint/dir\"\n)\n.\nformat\n(\n\"parquet\"\n)\n.\ntoTable\n(\n\"newTable\"\n)\n// Check the new table result\nspark\n.\nread\n.\ntable\n(\n\"newTable\"\n).\nshow\n()\nSparkSession\nspark\n=\n...\n// Create a streaming DataFrame\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\nreadStream\n()\n.\nformat\n(\n\"rate\"\n)\n.\noption\n(\n\"rowsPerSecond\"\n,\n10\n)\n.\nload\n();\n// Write the streaming DataFrame to a table\ndf\n.\nwriteStream\n()\n.\noption\n(\n\"checkpointLocation\"\n,\n\"path/to/checkpoint/dir\"\n)\n.\ntoTable\n(\n\"myTable\"\n);\n// Check the table result\nspark\n.\nread\n().\ntable\n(\n\"myTable\"\n).\nshow\n();\n// Transform the source dataset and write to a new table\nspark\n.\nreadStream\n()\n.\ntable\n(\n\"myTable\"\n)\n.\nselect\n(\n\"value\"\n)\n.\nwriteStream\n()\n.\noption\n(\n\"checkpointLocation\"\n,\n\"path/to/checkpoint/dir\"\n)\n.\nformat\n(\n\"parquet\"\n)\n.\ntoTable\n(\n\"newTable\"\n);\n// Check the new table result\nspark\n.\nread\n().\ntable\n(\n\"newTable\"\n).\nshow\n();\nNot available in R.\nFor more details, please check the docs for DataStreamReader (\nPython\n/\nScala\n/\nJava\ndocs) and DataStreamWriter (\nPython\n/\nScala\n/\nJava\ndocs).\nTriggers\nThe trigger settings of a streaming query define the timing of streaming data processing, whether\nthe query is going to be executed as micro-batch query with a fixed batch interval or as a continuous processing query.\nHere are the different kinds of triggers that are supported.\nTrigger Type\nDescription\nunspecified (default)\nIf no trigger setting is explicitly specified, then by default, the query will be\n        executed in micro-batch mode, where micro-batches will be generated as soon as\n        the previous micro-batch has completed processing.\nFixed interval micro-batches\nThe query will be executed with micro-batches mode, where micro-batches will be kicked off\n        at the user-specified intervals.\nIf the previous micro-batch completes within the interval, then the engine will wait until\n          the interval is over before kicking off the next micro-batch.\nIf the previous micro-batch takes longer than the interval to complete (i.e. if an\n          interval boundary is missed), then the next micro-batch will start as soon as the\n          previous one completes (i.e., it will not wait for the next interval boundary).\nIf no new data is available, then no micro-batch will be kicked off.\nOne-time micro-batch\n(deprecated)\nThe query will execute\nonly one\nmicro-batch to process all the available data and then\n        stop on its own. This is useful in scenarios you want to periodically spin up a cluster,\n        process everything that is available since the last period, and then shutdown the\n        cluster. In some case, this may lead to significant cost savings.\n        Note that this trigger is deprecated and users are encouraged to migrate to\nAvailable-now micro-batch\n,\n        as it provides the better guarantee of processing, fine-grained scale of batches, and better gradual processing\n        of watermark advancement including no-data batch.\nAvailable-now micro-batch\nSimilar to queries one-time micro-batch trigger, the query will process all the available data and then\n        stop on its own. The difference is that, it will process the data in (possibly) multiple micro-batches\n        based on the source options (e.g.\nmaxFilesPerTrigger\nor\nmaxBytesPerTrigger\nfor file \n        source), which will result in better query scalability.\nThis trigger provides a strong guarantee of processing: regardless of how many batches were\n                left over in previous run, it ensures all available data at the time of execution gets\n                processed before termination. All uncommitted batches will be processed first.\nWatermark gets advanced per each batch, and no-data batch gets executed before termination\n                if the last batch advances the watermark. This helps to maintain smaller and predictable\n                state size and smaller latency on the output of stateful operators.\nNOTE: this trigger will be deactivated when there is any source which does not support Trigger.AvailableNow.\n        Spark will perform one-time micro-batch as a fall-back. Check the above differences for a risk of fallback.\nContinuous with fixed checkpoint interval\n(experimental)\nThe query will be executed in the new low-latency, continuous processing mode. Read more\n        about this in the\nContinuous Processing section\nbelow.\nHere are a few code examples.\n# Default trigger (runs micro-batch as soon as it can)\ndf\n.\nwriteStream\n\\\n.\nformat\n(\n\"\nconsole\n\"\n)\n\\\n.\nstart\n()\n# ProcessingTime trigger with two-seconds micro-batch interval\ndf\n.\nwriteStream\n\\\n.\nformat\n(\n\"\nconsole\n\"\n)\n\\\n.\ntrigger\n(\nprocessingTime\n=\n'\n2 seconds\n'\n)\n\\\n.\nstart\n()\n# One-time trigger (Deprecated, encouraged to use Available-now trigger)\ndf\n.\nwriteStream\n\\\n.\nformat\n(\n\"\nconsole\n\"\n)\n\\\n.\ntrigger\n(\nonce\n=\nTrue\n)\n\\\n.\nstart\n()\n# Available-now trigger\ndf\n.\nwriteStream\n\\\n.\nformat\n(\n\"\nconsole\n\"\n)\n\\\n.\ntrigger\n(\navailableNow\n=\nTrue\n)\n\\\n.\nstart\n()\n# Continuous trigger with one-second checkpointing interval\ndf\n.\nwriteStream\n.\nformat\n(\n\"\nconsole\n\"\n)\n.\ntrigger\n(\ncontinuous\n=\n'\n1 second\n'\n)\n.\nstart\n()\nimport\norg.apache.spark.sql.streaming.Trigger\n// Default trigger (runs micro-batch as soon as it can)\ndf\n.\nwriteStream\n.\nformat\n(\n\"console\"\n)\n.\nstart\n()\n// ProcessingTime trigger with two-seconds micro-batch interval\ndf\n.\nwriteStream\n.\nformat\n(\n\"console\"\n)\n.\ntrigger\n(\nTrigger\n.\nProcessingTime\n(\n\"2 seconds\"\n))\n.\nstart\n()\n// One-time trigger (Deprecated, encouraged to use Available-now trigger)\ndf\n.\nwriteStream\n.\nformat\n(\n\"console\"\n)\n.\ntrigger\n(\nTrigger\n.\nOnce\n())\n.\nstart\n()\n// Available-now trigger\ndf\n.\nwriteStream\n.\nformat\n(\n\"console\"\n)\n.\ntrigger\n(\nTrigger\n.\nAvailableNow\n())\n.\nstart\n()\n// Continuous trigger with one-second checkpointing interval\ndf\n.\nwriteStream\n.\nformat\n(\n\"console\"\n)\n.\ntrigger\n(\nTrigger\n.\nContinuous\n(\n\"1 second\"\n))\n.\nstart\n()\nimport\norg.apache.spark.sql.streaming.Trigger\n// Default trigger (runs micro-batch as soon as it can)\ndf\n.\nwriteStream\n.\nformat\n(\n\"console\"\n)\n.\nstart\n();\n// ProcessingTime trigger with two-seconds micro-batch interval\ndf\n.\nwriteStream\n.\nformat\n(\n\"console\"\n)\n.\ntrigger\n(\nTrigger\n.\nProcessingTime\n(\n\"2 seconds\"\n))\n.\nstart\n();\n// One-time trigger (Deprecated, encouraged to use Available-now trigger)\ndf\n.\nwriteStream\n.\nformat\n(\n\"console\"\n)\n.\ntrigger\n(\nTrigger\n.\nOnce\n())\n.\nstart\n();\n// Available-now trigger\ndf\n.\nwriteStream\n.\nformat\n(\n\"console\"\n)\n.\ntrigger\n(\nTrigger\n.\nAvailableNow\n())\n.\nstart\n();\n// Continuous trigger with one-second checkpointing interval\ndf\n.\nwriteStream\n.\nformat\n(\n\"console\"\n)\n.\ntrigger\n(\nTrigger\n.\nContinuous\n(\n\"1 second\"\n))\n.\nstart\n();\n# Default trigger (runs micro-batch as soon as it can)\nwrite.stream\n(\ndf\n,\n\"console\"\n)\n# ProcessingTime trigger with two-seconds micro-batch interval\nwrite.stream\n(\ndf\n,\n\"console\"\n,\ntrigger.processingTime\n=\n\"2 seconds\"\n)\n# One-time trigger\nwrite.stream\n(\ndf\n,\n\"console\"\n,\ntrigger.once\n=\nTRUE\n)\n# Continuous trigger is not yet supported\nManaging Streaming Queries\nThe\nStreamingQuery\nobject created when a query is started can be used to monitor and manage the query.\nquery\n=\ndf\n.\nwriteStream\n.\nformat\n(\n\"\nconsole\n\"\n).\nstart\n()\n# get the query object\nquery\n.\nid\n()\n# get the unique identifier of the running query that persists across restarts from checkpoint data\nquery\n.\nrunId\n()\n# get the unique id of this run of the query, which will be generated at every start/restart\nquery\n.\nname\n()\n# get the name of the auto-generated or user-specified name\nquery\n.\nexplain\n()\n# print detailed explanations of the query\nquery\n.\nstop\n()\n# stop the query\nquery\n.\nawaitTermination\n()\n# block until query is terminated, with stop() or with error\nquery\n.\nexception\n()\n# the exception if the query has been terminated with error\nquery\n.\nrecentProgress\n# a list of the most recent progress updates for this query\nquery\n.\nlastProgress\n# the most recent progress update of this streaming query\nval\nquery\n=\ndf\n.\nwriteStream\n.\nformat\n(\n\"console\"\n).\nstart\n()\n// get the query object\nquery\n.\nid\n// get the unique identifier of the running query that persists across restarts from checkpoint data\nquery\n.\nrunId\n// get the unique id of this run of the query, which will be generated at every start/restart\nquery\n.\nname\n// get the name of the auto-generated or user-specified name\nquery\n.\nexplain\n()\n// print detailed explanations of the query\nquery\n.\nstop\n()\n// stop the query\nquery\n.\nawaitTermination\n()\n// block until query is terminated, with stop() or with error\nquery\n.\nexception\n// the exception if the query has been terminated with error\nquery\n.\nrecentProgress\n// an array of the most recent progress updates for this query\nquery\n.\nlastProgress\n// the most recent progress update of this streaming query\nStreamingQuery\nquery\n=\ndf\n.\nwriteStream\n().\nformat\n(\n\"console\"\n).\nstart\n();\n// get the query object\nquery\n.\nid\n();\n// get the unique identifier of the running query that persists across restarts from checkpoint data\nquery\n.\nrunId\n();\n// get the unique id of this run of the query, which will be generated at every start/restart\nquery\n.\nname\n();\n// get the name of the auto-generated or user-specified name\nquery\n.\nexplain\n();\n// print detailed explanations of the query\nquery\n.\nstop\n();\n// stop the query\nquery\n.\nawaitTermination\n();\n// block until query is terminated, with stop() or with error\nquery\n.\nexception\n();\n// the exception if the query has been terminated with error\nquery\n.\nrecentProgress\n();\n// an array of the most recent progress updates for this query\nquery\n.\nlastProgress\n();\n// the most recent progress update of this streaming query\nquery\n<-\nwrite.stream\n(\ndf\n,\n\"console\"\n)\n# get the query object\nqueryName\n(\nquery\n)\n# get the name of the auto-generated or user-specified name\nexplain\n(\nquery\n)\n# print detailed explanations of the query\nstopQuery\n(\nquery\n)\n# stop the query\nawaitTermination\n(\nquery\n)\n# block until query is terminated, with stop() or with error\nlastProgress\n(\nquery\n)\n# the most recent progress update of this streaming query\nYou can start any number of queries in a single SparkSession. They will all be running concurrently sharing the cluster resources. You can use\nsparkSession.streams()\nto get the\nStreamingQueryManager\n(\nPython\n/\nScala\n/\nJava\ndocs)\nthat can be used to manage the currently active queries.\nspark\n=\n...\n# spark session\nspark\n.\nstreams\n.\nactive\n# get the list of currently active streaming queries\nspark\n.\nstreams\n.\nget\n(\nid\n)\n# get a query object by its unique id\nspark\n.\nstreams\n.\nawaitAnyTermination\n()\n# block until any one of them terminates\nval\nspark\n:\nSparkSession\n=\n...\nspark\n.\nstreams\n.\nactive\n// get the list of currently active streaming queries\nspark\n.\nstreams\n.\nget\n(\nid\n)\n// get a query object by its unique id\nspark\n.\nstreams\n.\nawaitAnyTermination\n()\n// block until any one of them terminates\nSparkSession\nspark\n=\n...\nspark\n.\nstreams\n().\nactive\n();\n// get the list of currently active streaming queries\nspark\n.\nstreams\n().\nget\n(\nid\n);\n// get a query object by its unique id\nspark\n.\nstreams\n().\nawaitAnyTermination\n();\n// block until any one of them terminates\nNot available\nin\nR.\nMonitoring Streaming Queries\nThere are multiple ways to monitor active streaming queries. You can either push metrics to external systems using Spark’s Dropwizard Metrics support, or access them programmatically.\nReading Metrics Interactively\nYou can directly get the current status and metrics of an active query using\nstreamingQuery.lastProgress()\nand\nstreamingQuery.status()\n.\nlastProgress()\nreturns a\nStreamingQueryProgress\nobject\nin\nScala\nand\nJava\nand a dictionary with the same fields in Python. It has all the information about\nthe progress made in the last trigger of the stream - what data was processed,\nwhat were the processing rates, latencies, etc. There is also\nstreamingQuery.recentProgress\nwhich returns an array of last few progresses.\nIn addition,\nstreamingQuery.status()\nreturns a\nStreamingQueryStatus\nobject\nin\nScala\nand\nJava\nand a dictionary with the same fields in Python. It gives information about\nwhat the query is immediately doing - is a trigger active, is data being processed, etc.\nHere are a few examples.\nquery\n=\n...\n# a StreamingQuery\nprint\n(\nquery\n.\nlastProgress\n)\n'''\nWill print something like the following.\n\n{u\n'\nstateOperators\n'\n: [], u\n'\neventTime\n'\n: {u\n'\nwatermark\n'\n: u\n'\n2016-12-14T18:45:24.873Z\n'\n}, u\n'\nname\n'\n: u\n'\nMyQuery\n'\n, u\n'\ntimestamp\n'\n: u\n'\n2016-12-14T18:45:24.873Z\n'\n, u\n'\nprocessedRowsPerSecond\n'\n: 200.0, u\n'\ninputRowsPerSecond\n'\n: 120.0, u\n'\nnumInputRows\n'\n: 10, u\n'\nsources\n'\n: [{u\n'\ndescription\n'\n: u\n'\nKafkaSource[Subscribe[topic-0]]\n'\n, u\n'\nendOffset\n'\n: {u\n'\ntopic-0\n'\n: {u\n'\n1\n'\n: 134, u\n'\n0\n'\n: 534, u\n'\n3\n'\n: 21, u\n'\n2\n'\n: 0, u\n'\n4\n'\n: 115}}, u\n'\nprocessedRowsPerSecond\n'\n: 200.0, u\n'\ninputRowsPerSecond\n'\n: 120.0, u\n'\nnumInputRows\n'\n: 10, u\n'\nstartOffset\n'\n: {u\n'\ntopic-0\n'\n: {u\n'\n1\n'\n: 1, u\n'\n0\n'\n: 1, u\n'\n3\n'\n: 1, u\n'\n2\n'\n: 0, u\n'\n4\n'\n: 1}}}], u\n'\ndurationMs\n'\n: {u\n'\ngetOffset\n'\n: 2, u\n'\ntriggerExecution\n'\n: 3}, u\n'\nrunId\n'\n: u\n'\n88e2ff94-ede0-45a8-b687-6316fbef529a\n'\n, u\n'\nid\n'\n: u\n'\nce011fdc-8762-4dcb-84eb-a77333e28109\n'\n, u\n'\nsink\n'\n: {u\n'\ndescription\n'\n: u\n'\nMemorySink\n'\n}}\n'''\nprint\n(\nquery\n.\nstatus\n)\n'''\nWill print something like the following.\n\n{u\n'\nmessage\n'\n: u\n'\nWaiting for data to arrive\n'\n, u\n'\nisTriggerActive\n'\n: False, u\n'\nisDataAvailable\n'\n: False}\n'''\nval\nquery\n:\nStreamingQuery\n=\n...\nprintln\n(\nquery\n.\nlastProgress\n)\n/* Will print something like the following.\n\n{\n  \"id\" : \"ce011fdc-8762-4dcb-84eb-a77333e28109\",\n  \"runId\" : \"88e2ff94-ede0-45a8-b687-6316fbef529a\",\n  \"name\" : \"MyQuery\",\n  \"timestamp\" : \"2016-12-14T18:45:24.873Z\",\n  \"numInputRows\" : 10,\n  \"inputRowsPerSecond\" : 120.0,\n  \"processedRowsPerSecond\" : 200.0,\n  \"durationMs\" : {\n    \"triggerExecution\" : 3,\n    \"getOffset\" : 2\n  },\n  \"eventTime\" : {\n    \"watermark\" : \"2016-12-14T18:45:24.873Z\"\n  },\n  \"stateOperators\" : [ ],\n  \"sources\" : [ {\n    \"description\" : \"KafkaSource[Subscribe[topic-0]]\",\n    \"startOffset\" : {\n      \"topic-0\" : {\n        \"2\" : 0,\n        \"4\" : 1,\n        \"1\" : 1,\n        \"3\" : 1,\n        \"0\" : 1\n      }\n    },\n    \"endOffset\" : {\n      \"topic-0\" : {\n        \"2\" : 0,\n        \"4\" : 115,\n        \"1\" : 134,\n        \"3\" : 21,\n        \"0\" : 534\n      }\n    },\n    \"numInputRows\" : 10,\n    \"inputRowsPerSecond\" : 120.0,\n    \"processedRowsPerSecond\" : 200.0\n  } ],\n  \"sink\" : {\n    \"description\" : \"MemorySink\"\n  }\n}\n*/\nprintln\n(\nquery\n.\nstatus\n)\n/*  Will print something like the following.\n{\n  \"message\" : \"Waiting for data to arrive\",\n  \"isDataAvailable\" : false,\n  \"isTriggerActive\" : false\n}\n*/\nStreamingQuery\nquery\n=\n...\nSystem\n.\nout\n.\nprintln\n(\nquery\n.\nlastProgress\n());\n/* Will print something like the following.\n\n{\n  \"id\" : \"ce011fdc-8762-4dcb-84eb-a77333e28109\",\n  \"runId\" : \"88e2ff94-ede0-45a8-b687-6316fbef529a\",\n  \"name\" : \"MyQuery\",\n  \"timestamp\" : \"2016-12-14T18:45:24.873Z\",\n  \"numInputRows\" : 10,\n  \"inputRowsPerSecond\" : 120.0,\n  \"processedRowsPerSecond\" : 200.0,\n  \"durationMs\" : {\n    \"triggerExecution\" : 3,\n    \"getOffset\" : 2\n  },\n  \"eventTime\" : {\n    \"watermark\" : \"2016-12-14T18:45:24.873Z\"\n  },\n  \"stateOperators\" : [ ],\n  \"sources\" : [ {\n    \"description\" : \"KafkaSource[Subscribe[topic-0]]\",\n    \"startOffset\" : {\n      \"topic-0\" : {\n        \"2\" : 0,\n        \"4\" : 1,\n        \"1\" : 1,\n        \"3\" : 1,\n        \"0\" : 1\n      }\n    },\n    \"endOffset\" : {\n      \"topic-0\" : {\n        \"2\" : 0,\n        \"4\" : 115,\n        \"1\" : 134,\n        \"3\" : 21,\n        \"0\" : 534\n      }\n    },\n    \"numInputRows\" : 10,\n    \"inputRowsPerSecond\" : 120.0,\n    \"processedRowsPerSecond\" : 200.0\n  } ],\n  \"sink\" : {\n    \"description\" : \"MemorySink\"\n  }\n}\n*/\nSystem\n.\nout\n.\nprintln\n(\nquery\n.\nstatus\n());\n/*  Will print something like the following.\n{\n  \"message\" : \"Waiting for data to arrive\",\n  \"isDataAvailable\" : false,\n  \"isTriggerActive\" : false\n}\n*/\nquery\n<-\n...\n# a StreamingQuery\nlastProgress\n(\nquery\n)\n'''\nWill print something like the following.\n\n{\n  \"id\" : \"8c57e1ec-94b5-4c99-b100-f694162df0b9\",\n  \"runId\" : \"ae505c5a-a64e-4896-8c28-c7cbaf926f16\",\n  \"name\" : null,\n  \"timestamp\" : \"2017-04-26T08:27:28.835Z\",\n  \"numInputRows\" : 0,\n  \"inputRowsPerSecond\" : 0.0,\n  \"processedRowsPerSecond\" : 0.0,\n  \"durationMs\" : {\n    \"getOffset\" : 0,\n    \"triggerExecution\" : 1\n  },\n  \"stateOperators\" : [ {\n    \"numRowsTotal\" : 4,\n    \"numRowsUpdated\" : 0\n  } ],\n  \"sources\" : [ {\n    \"description\" : \"TextSocketSource[host: localhost, port: 9999]\",\n    \"startOffset\" : 1,\n    \"endOffset\" : 1,\n    \"numInputRows\" : 0,\n    \"inputRowsPerSecond\" : 0.0,\n    \"processedRowsPerSecond\" : 0.0\n  } ],\n  \"sink\" : {\n    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSink@76b37531\"\n  }\n}\n'''\nstatus\n(\nquery\n)\n'''\nWill print something like the following.\n\n{\n  \"message\" : \"Waiting for data to arrive\",\n  \"isDataAvailable\" : false,\n  \"isTriggerActive\" : false\n}\n'''\nReporting Metrics programmatically using Asynchronous APIs\nYou can also asynchronously monitor all queries associated with a\nSparkSession\nby attaching a\nStreamingQueryListener\n(\nPython\n/\nScala\n/\nJava\ndocs).\nOnce you attach your custom\nStreamingQueryListener\nobject with\nsparkSession.streams.addListener()\n, you will get callbacks when a query is started and\nstopped and when there is progress made in an active query. Here is an example,\nspark\n=\n...\nclass\nListener\n(\nStreamingQueryListener\n):\ndef\nonQueryStarted\n(\nself\n,\nevent\n):\nprint\n(\n\"\nQuery started:\n\"\n+\nqueryStarted\n.\nid\n)\ndef\nonQueryProgress\n(\nself\n,\nevent\n):\nprint\n(\n\"\nQuery made progress:\n\"\n+\nqueryProgress\n.\nprogress\n)\ndef\nonQueryTerminated\n(\nself\n,\nevent\n):\nprint\n(\n\"\nQuery terminated:\n\"\n+\nqueryTerminated\n.\nid\n)\nspark\n.\nstreams\n.\naddListener\n(\nListener\n())\nval\nspark\n:\nSparkSession\n=\n...\nspark\n.\nstreams\n.\naddListener\n(\nnew\nStreamingQueryListener\n()\n{\noverride\ndef\nonQueryStarted\n(\nqueryStarted\n:\nQueryStartedEvent\n)\n:\nUnit\n=\n{\nprintln\n(\n\"Query started: \"\n+\nqueryStarted\n.\nid\n)\n}\noverride\ndef\nonQueryTerminated\n(\nqueryTerminated\n:\nQueryTerminatedEvent\n)\n:\nUnit\n=\n{\nprintln\n(\n\"Query terminated: \"\n+\nqueryTerminated\n.\nid\n)\n}\noverride\ndef\nonQueryProgress\n(\nqueryProgress\n:\nQueryProgressEvent\n)\n:\nUnit\n=\n{\nprintln\n(\n\"Query made progress: \"\n+\nqueryProgress\n.\nprogress\n)\n}\n})\nSparkSession\nspark\n=\n...\nspark\n.\nstreams\n().\naddListener\n(\nnew\nStreamingQueryListener\n()\n{\n@Override\npublic\nvoid\nonQueryStarted\n(\nQueryStartedEvent\nqueryStarted\n)\n{\nSystem\n.\nout\n.\nprintln\n(\n\"Query started: \"\n+\nqueryStarted\n.\nid\n());\n}\n@Override\npublic\nvoid\nonQueryTerminated\n(\nQueryTerminatedEvent\nqueryTerminated\n)\n{\nSystem\n.\nout\n.\nprintln\n(\n\"Query terminated: \"\n+\nqueryTerminated\n.\nid\n());\n}\n@Override\npublic\nvoid\nonQueryProgress\n(\nQueryProgressEvent\nqueryProgress\n)\n{\nSystem\n.\nout\n.\nprintln\n(\n\"Query made progress: \"\n+\nqueryProgress\n.\nprogress\n());\n}\n});\nNot available\nin\nR.\nReporting Metrics using Dropwizard\nSpark supports reporting metrics using the\nDropwizard Library\n. To enable metrics of Structured Streaming queries to be reported as well, you have to explicitly enable the configuration\nspark.sql.streaming.metricsEnabled\nin the SparkSession.\nspark\n.\nconf\n.\nset\n(\n\"\nspark.sql.streaming.metricsEnabled\n\"\n,\n\"\ntrue\n\"\n)\n# or\nspark\n.\nsql\n(\n\"\nSET spark.sql.streaming.metricsEnabled=true\n\"\n)\nspark\n.\nconf\n.\nset\n(\n\"spark.sql.streaming.metricsEnabled\"\n,\n\"true\"\n)\n// or\nspark\n.\nsql\n(\n\"SET spark.sql.streaming.metricsEnabled=true\"\n)\nspark\n.\nconf\n().\nset\n(\n\"spark.sql.streaming.metricsEnabled\"\n,\n\"true\"\n);\n// or\nspark\n.\nsql\n(\n\"SET spark.sql.streaming.metricsEnabled=true\"\n);\nsql\n(\n\"SET spark.sql.streaming.metricsEnabled=true\"\n)\nAll queries started in the SparkSession after this configuration has been enabled will report metrics through Dropwizard to whatever\nsinks\nhave been configured (e.g. Ganglia, Graphite, JMX, etc.).\nRecovering from Failures with Checkpointing\nIn case of a failure or intentional shutdown, you can recover the previous progress and state of a previous query, and continue where it left off. This is done using checkpointing and write-ahead logs. You can configure a query with a checkpoint location, and the query will save all the progress information (i.e. range of offsets processed in each trigger) and the running aggregates (e.g. word counts in the\nquick example\n) to the checkpoint location. This checkpoint location has to be a path in an HDFS compatible file system, and can be set as an option in the DataStreamWriter when\nstarting a query\n.\naggDF\n\\\n.\nwriteStream\n\\\n.\noutputMode\n(\n\"\ncomplete\n\"\n)\n\\\n.\noption\n(\n\"\ncheckpointLocation\n\"\n,\n\"\npath/to/HDFS/dir\n\"\n)\n\\\n.\nformat\n(\n\"\nmemory\n\"\n)\n\\\n.\nstart\n()\naggDF\n.\nwriteStream\n.\noutputMode\n(\n\"complete\"\n)\n.\noption\n(\n\"checkpointLocation\"\n,\n\"path/to/HDFS/dir\"\n)\n.\nformat\n(\n\"memory\"\n)\n.\nstart\n()\naggDF\n.\nwriteStream\n()\n.\noutputMode\n(\n\"complete\"\n)\n.\noption\n(\n\"checkpointLocation\"\n,\n\"path/to/HDFS/dir\"\n)\n.\nformat\n(\n\"memory\"\n)\n.\nstart\n();\nwrite.stream\n(\naggDF\n,\n\"memory\"\n,\noutputMode\n=\n\"complete\"\n,\ncheckpointLocation\n=\n\"path/to/HDFS/dir\"\n)\nRecovery Semantics after Changes in a Streaming Query\nThere are limitations on what changes in a streaming query are allowed between restarts from the\nsame checkpoint location. Here are a few kinds of changes that are either not allowed, or\nthe effect of the change is not well-defined. For all of them:\nThe term\nallowed\nmeans you can do the specified change but whether the semantics of its effect\nis well-defined depends on the query and the change.\nThe term\nnot allowed\nmeans you should not do the specified change as the restarted query is likely\nto fail with unpredictable errors.\nsdf\nrepresents a streaming DataFrame/Dataset\ngenerated with sparkSession.readStream.\nTypes of changes\nChanges in the number or type (i.e. different source) of input sources\n: This is not allowed.\nChanges in the parameters of input sources\n: Whether this is allowed and whether the semantics\nof the change are well-defined depends on the source and the query. Here are a few examples.\nAddition/deletion/modification of rate limits is allowed:\nspark.readStream.format(\"kafka\").option(\"subscribe\", \"topic\")\nto\nspark.readStream.format(\"kafka\").option(\"subscribe\", \"topic\").option(\"maxOffsetsPerTrigger\", ...)\nChanges to subscribed topics/files are generally not allowed as the results are unpredictable:\nspark.readStream.format(\"kafka\").option(\"subscribe\", \"topic\")\nto\nspark.readStream.format(\"kafka\").option(\"subscribe\", \"newTopic\")\nChanges in the type of output sink\n: Changes between a few specific combinations of sinks\nare allowed. This needs to be verified on a case-by-case basis. Here are a few examples.\nFile sink to Kafka sink is allowed. Kafka will see only the new data.\nKafka sink to file sink is not allowed.\nKafka sink changed to foreach, or vice versa is allowed.\nChanges in the parameters of output sink\n: Whether this is allowed and whether the semantics of\nthe change are well-defined depends on the sink and the query. Here are a few examples.\nChanges to output directory of a file sink are not allowed:\nsdf.writeStream.format(\"parquet\").option(\"path\", \"/somePath\")\nto\nsdf.writeStream.format(\"parquet\").option(\"path\", \"/anotherPath\")\nChanges to output topic are allowed:\nsdf.writeStream.format(\"kafka\").option(\"topic\", \"someTopic\")\nto\nsdf.writeStream.format(\"kafka\").option(\"topic\", \"anotherTopic\")\nChanges to the user-defined foreach sink (that is, the\nForeachWriter\ncode) are allowed, but the semantics of the change depends on the code.\nChanges in projection / filter / map-like operations\n: Some cases are allowed. For example:\nAddition / deletion of filters is allowed:\nsdf.selectExpr(\"a\")\nto\nsdf.where(...).selectExpr(\"a\").filter(...)\n.\nChanges in projections with same output schema are allowed:\nsdf.selectExpr(\"stringColumn AS json\").writeStream\nto\nsdf.selectExpr(\"anotherStringColumn AS json\").writeStream\nChanges in projections with different output schema are conditionally allowed:\nsdf.selectExpr(\"a\").writeStream\nto\nsdf.selectExpr(\"b\").writeStream\nis allowed only if the output sink allows the schema change from\n\"a\"\nto\n\"b\"\n.\nChanges in stateful operations\n: Some operations in streaming queries need to maintain\nstate data in order to continuously update the result. Structured Streaming automatically checkpoints\nthe state data to fault-tolerant storage (for example, HDFS, AWS S3, Azure Blob storage) and restores it after restart.\nHowever, this assumes that the schema of the state data remains same across restarts. This means that\nany changes (that is, additions, deletions, or schema modifications) to the stateful operations of a streaming query are not allowed between restarts\n.\nHere is the list of stateful operations whose schema should not be changed between restarts in order to ensure state recovery:\nStreaming aggregation\n: For example,\nsdf.groupBy(\"a\").agg(...)\n. Any change in number or type of grouping keys or aggregates is not allowed.\nStreaming deduplication\n: For example,\nsdf.dropDuplicates(\"a\")\n. Any change in number or type of deduplicating columns is not allowed.\nStream-stream join\n: For example,\nsdf1.join(sdf2, ...)\n(i.e. both inputs are generated with\nsparkSession.readStream\n). Changes\nin the schema or equi-joining columns are not allowed. Changes in join type (outer or inner) are not allowed. Other changes in the join condition are ill-defined.\nArbitrary stateful operation\n: For example,\nsdf.groupByKey(...).mapGroupsWithState(...)\nor\nsdf.groupByKey(...).flatMapGroupsWithState(...)\n.\nAny change to the schema of the user-defined state and the type of timeout is not allowed.\nAny change within the user-defined state-mapping function are allowed, but the semantic effect of the change depends on the user-defined logic.\nIf you really want to support state schema changes, then you can explicitly encode/decode your complex state data\nstructures into bytes using an encoding/decoding scheme that supports schema migration. For example,\nif you save your state as Avro-encoded bytes, then you are free to change the Avro-state-schema between query\nrestarts as the binary state will always be restored successfully."}
{"url": "https://spark.apache.org/docs/latest/sql-data-sources-xml.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTroubleshooting\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nXML Files\nSpark SQL provides\nspark.read().xml(\"file_1_path\",\"file_2_path\")\nto read a file or directory of files in XML format into a Spark DataFrame, and\ndataframe.write().xml(\"path\")\nto write to a xml file. The\nrowTag\noption must be specified to indicate the XML element that maps to a\nDataFrame row\n. The option() function can be used to customize the behavior of reading or writing, such as controlling behavior of the XML attributes, XSD validation, compression, and so on.\n# Primitive types (Int, String, etc) and Product types (case classes) encoders are\n# supported by importing this when creating a Dataset.\n# An XML dataset is pointed to by path.\n# The path can be either a single xml file or more xml files\npath\n=\n\"\nexamples/src/main/resources/people.xml\n\"\npeopleDF\n=\nspark\n.\nread\n.\noption\n(\n\"\nrowTag\n\"\n,\n\"\nperson\n\"\n).\nformat\n(\n\"\nxml\n\"\n).\nload\n(\npath\n)\n# The inferred schema can be visualized using the printSchema() method\npeopleDF\n.\nprintSchema\n()\n# root\n#  |-- age: long (nullable = true)\n#  |-- name: string (nullable = true)\n# Creates a temporary view using the DataFrame\npeopleDF\n.\ncreateOrReplaceTempView\n(\n\"\npeople\n\"\n)\n# SQL statements can be run by using the sql methods provided by spark\nteenagerNamesDF\n=\nspark\n.\nsql\n(\n\"\nSELECT name FROM people WHERE age BETWEEN 13 AND 19\n\"\n)\nteenagerNamesDF\n.\nshow\n()\n# +------+\n# |  name|\n# +------+\n# |Justin|\n# +------+\n# Alternatively, a DataFrame can be created for an XML dataset represented by a Dataset[String]\nxmlStrings\n=\n[\n\"\"\"\n<person>\n          <name>laglangyue</name>\n          <job>Developer</job>\n          <age>28</age>\n      </person>\n\"\"\"\n]\nxmlRDD\n=\nspark\n.\nsparkContext\n.\nparallelize\n(\nxmlStrings\n)\notherPeople\n=\nspark\n.\nread\n\\\n.\noption\n(\n\"\nrowTag\n\"\n,\n\"\nperson\n\"\n)\n\\\n.\nxml\n(\nxmlRDD\n)\notherPeople\n.\nshow\n()\n# +---+---------+----------+\n# |age|      job|      name|\n# +---+---------+----------+\n# | 28|Developer|laglangyue|\n# +---+---------+----------+\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\n// Primitive types (Int, String, etc) and Product types (case classes) encoders are\n// supported by importing this when creating a Dataset.\nimport\nspark.implicits._\n// An XML dataset is pointed to by path.\n// The path can be either a single xml file or more xml files\nval\npath\n=\n\"examples/src/main/resources/people.xml\"\nval\npeopleDF\n=\nspark\n.\nread\n.\noption\n(\n\"rowTag\"\n,\n\"person\"\n).\nxml\n(\npath\n)\n// The inferred schema can be visualized using the printSchema() method\npeopleDF\n.\nprintSchema\n()\n// root\n//  |-- age: long (nullable = true)\n//  |-- name: string (nullable = true)\n// Creates a temporary view using the DataFrame\npeopleDF\n.\ncreateOrReplaceTempView\n(\n\"people\"\n)\n// SQL statements can be run by using the sql methods provided by spark\nval\nteenagerNamesDF\n=\nspark\n.\nsql\n(\n\"SELECT name FROM people WHERE age BETWEEN 13 AND 19\"\n)\nteenagerNamesDF\n.\nshow\n()\n// +------+\n// |  name|\n// +------+\n// |Justin|\n// +------+\n// Alternatively, a DataFrame can be created for a XML dataset represented by a Dataset[String]\nval\notherPeopleDataset\n=\nspark\n.\ncreateDataset\n(\n\"\"\"\n    |<person>\n    |    <name>laglangyue</name>\n    |    <job>Developer</job>\n    |    <age>28</age>\n    |</person>\n    |\"\"\"\n.\nstripMargin\n::\nNil\n)\nval\notherPeople\n=\nspark\n.\nread\n.\noption\n(\n\"rowTag\"\n,\n\"person\"\n)\n.\nxml\n(\notherPeopleDataset\n)\notherPeople\n.\nshow\n()\n// +---+---------+----------+\n// |age|      job|      name|\n// +---+---------+----------+\n// | 28|Developer|laglangyue|\n// +---+---------+----------+\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\n// Primitive types (Int, String, etc) and Product types (case classes) encoders are\n// supported by importing this when creating a Dataset.\n// An XML dataset is pointed to by path.\n// The path can be either a single xml file or more xml files\nString\npath\n=\n\"examples/src/main/resources/people.xml\"\n;\nDataset\n<\nRow\n>\npeopleDF\n=\nspark\n.\nread\n().\noption\n(\n\"rowTag\"\n,\n\"person\"\n).\nxml\n(\npath\n);\n// The inferred schema can be visualized using the printSchema() method\npeopleDF\n.\nprintSchema\n();\n// root\n//  |-- age: long (nullable = true)\n//  |-- name: string (nullable = true)\n// Creates a temporary view using the DataFrame\npeopleDF\n.\ncreateOrReplaceTempView\n(\n\"people\"\n);\n// SQL statements can be run by using the sql methods provided by spark\nDataset\n<\nRow\n>\nteenagerNamesDF\n=\nspark\n.\nsql\n(\n\"SELECT name FROM people WHERE age BETWEEN 13 AND 19\"\n);\nteenagerNamesDF\n.\nshow\n();\n// +------+\n// |  name|\n// +------+\n// |Justin|\n// +------+\n// Alternatively, a DataFrame can be created for an XML dataset represented by a Dataset[String]\nList\n<\nString\n>\nxmlData\n=\nCollections\n.\nsingletonList\n(\n\"<person>\"\n+\n\"<name>laglangyue</name><job>Developer</job><age>28</age>\"\n+\n\"</person>\"\n);\nDataset\n<\nString\n>\notherPeopleDataset\n=\nspark\n.\ncreateDataset\n(\nLists\n.\nnewArrayList\n(\nxmlData\n),\nEncoders\n.\nSTRING\n());\nDataset\n<\nRow\n>\notherPeople\n=\nspark\n.\nread\n()\n.\noption\n(\n\"rowTag\"\n,\n\"person\"\n)\n.\nxml\n(\notherPeopleDataset\n);\notherPeople\n.\nshow\n();\n// +---+---------+----------+\n// |age|      job|      name|\n// +---+---------+----------+\n// | 28|Developer|laglangyue|\n// +---+---------+----------+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\nData Source Option\nData source options of XML can be set via:\nthe\n.option\n/\n.options\nmethods of\nDataFrameReader\nDataFrameWriter\nDataStreamReader\nDataStreamWriter\nthe built-in functions below\nfrom_xml\nto_xml\nschema_of_xml\nOPTIONS\nclause at\nCREATE TABLE USING DATA_SOURCE\nProperty Name\nDefault\nMeaning\nScope\nrowTag\nThe row tag of your xml files to treat as a row. For example, in this xml:\n...\nthe appropriate value would be book. This is a required option for both read and write.\nread/write\nsamplingRatio\n1.0\nDefines fraction of rows used for schema inferring. XML built-in functions ignore this option.\nread\nexcludeAttribute\nfalse\nWhether to exclude attributes in elements.\nread\nmode\nPERMISSIVE\nAllows a mode for dealing with corrupt records during parsing.\nPERMISSIVE\n: when it meets a corrupted record, puts the malformed string into a field configured by columnNameOfCorruptRecord, and sets malformed fields to null. To keep corrupt records, an user can set a string type field named columnNameOfCorruptRecord in an user-defined schema. If a schema does not have the field, it drops corrupt records during parsing. When inferring a schema, it implicitly adds a columnNameOfCorruptRecord field in an output schema.\nDROPMALFORMED\n: ignores the whole corrupted records. This mode is unsupported in the XML built-in functions.\nFAILFAST\n: throws an exception when it meets corrupted records.\nread\ninferSchema\ntrue\nIf true, attempts to infer an appropriate type for each resulting DataFrame column. If false, all resulting columns are of string type.\nread\ncolumnNameOfCorruptRecord\nspark.sql.columnNameOfCorruptRecord\nAllows renaming the new field having a malformed string created by PERMISSIVE mode.\nread\nattributePrefix\n_\nThe prefix for attributes to differentiate attributes from elements. This will be the prefix for field names. Can be empty for reading XML, but not for writing.\nread/write\nvalueTag\n_VALUE\nThe tag used for the value when there are attributes in the element having no child.\nread/write\nencoding\nUTF-8\nFor reading, decodes the XML files by the given encoding type. For writing, specifies encoding (charset) of saved XML files. XML built-in functions ignore this option.\nread/write\nignoreSurroundingSpaces\ntrue\nDefines whether surrounding whitespaces from values being read should be skipped.\nread\nrowValidationXSDPath\nnull\nPath to an optional XSD file that is used to validate the XML for each row individually. Rows that fail to validate are treated like parse errors as above. The XSD does not otherwise affect the schema provided, or inferred.\nread\nignoreNamespace\nfalse\nIf true, namespaces prefixes on XML elements and attributes are ignored. Tags <abc:author> and <def:author> would, for example, be treated as if both are just <author>. Note that, at the moment, namespaces cannot be ignored on the rowTag element, only its children. Note that XML parsing is in general not namespace-aware even if false.\nread\ntimeZone\n(value of\nspark.sql.session.timeZone\nconfiguration)\nSets the string that indicates a time zone ID to be used to format timestamps in the XML datasources or partition values. The following formats of timeZone are supported:\nRegion-based zone ID: It should have the form 'area/city', such as 'America/Los_Angeles'.\nZone offset: It should be in the format '(+|-)HH:mm', for example '-08:00' or '+01:00', also 'UTC' and 'Z' are supported as aliases of '+00:00'.\nOther short names like 'CST' are not recommended to use because they can be ambiguous.\nread/write\ntimestampFormat\nyyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]\nSets the string that indicates a timestamp format. Custom date formats follow the formats at\ndatetime pattern\n. This applies to timestamp type.\nread/write\ntimestampNTZFormat\nyyyy-MM-dd'T'HH:mm:ss[.SSS]\nSets the string that indicates a timestamp without timezone format. Custom date formats follow the formats at\nDatetime Patterns\n. This applies to timestamp without timezone type, note that zone-offset and time-zone components are not supported when writing or reading this data type.\nread/write\ndateFormat\nyyyy-MM-dd\nSets the string that indicates a date format. Custom date formats follow the formats at\ndatetime pattern\n. This applies to date type.\nread/write\nlocale\nen-US\nSets a locale as a language tag in IETF BCP 47 format. For instance, locale is used while parsing dates and timestamps.\nread/write\nrootTag\nROWS\nRoot tag of the xml files. For example, in this xml:\n...\nthe appropriate value would be books. It can include basic attributes by specifying a value like 'books'\nwrite\ndeclaration\nversion=\"1.0\"\nencoding=\"UTF-8\"\nstandalone=\"yes\"\nContent of XML declaration to write at the start of every output XML file, before the rootTag. For example, a value of foo causes\nto be written. Set to empty string to suppress\nwrite\narrayElementName\nitem\nName of XML element that encloses each element of an array-valued column when writing.\nwrite\nnullValue\nnull\nSets the string representation of a null value. Default is string null. When this is null, it does not write attributes and elements for fields.\nread/write\nwildcardColName\nxs_any\nName of a column existing in the provided schema which is interpreted as a 'wildcard'. It must have type string or array of strings. It will match any XML child element that is not otherwise matched by the schema. The XML of the child becomes the string value of the column. If an array, then all unmatched elements will be returned as an array of strings. As its name implies, it is meant to emulate XSD's xs:any type.\nread\ncompression\nnone\nCompression codec to use when saving to file. This can be one of the known case-insensitive shortened names (none, bzip2, gzip, lz4, snappy and deflate). XML built-in functions ignore this option.\nwrite\nvalidateName\ntrue\nIf true, throws error on XML element name validation failure. For example, SQL field names can have spaces, but XML element names cannot.\nwrite\nOther generic options can be found in\nGeneric File Source Options\n."}
{"url": "https://spark.apache.org/docs/latest/sql-data-sources-binaryFile.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTroubleshooting\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nBinary File Data Source\nSince Spark 3.0, Spark supports binary file data source,\nwhich reads binary files and converts each file into a single record that contains the raw content\nand metadata of the file.\nIt produces a DataFrame with the following columns and possibly partition columns:\npath\n: StringType\nmodificationTime\n: TimestampType\nlength\n: LongType\ncontent\n: BinaryType\nTo read whole binary files, you need to specify the data source\nformat\nas\nbinaryFile\n.\nTo load files with paths matching a given glob pattern while keeping the behavior of partition discovery,\nyou can use the general data source option\npathGlobFilter\n.\nFor example, the following code reads all PNG files from the input directory:\nspark\n.\nread\n.\nformat\n(\n\"\nbinaryFile\n\"\n).\noption\n(\n\"\npathGlobFilter\n\"\n,\n\"\n*.png\n\"\n).\nload\n(\n\"\n/path/to/data\n\"\n)\nspark\n.\nread\n.\nformat\n(\n\"binaryFile\"\n).\noption\n(\n\"pathGlobFilter\"\n,\n\"*.png\"\n).\nload\n(\n\"/path/to/data\"\n)\nspark\n.\nread\n().\nformat\n(\n\"binaryFile\"\n).\noption\n(\n\"pathGlobFilter\"\n,\n\"*.png\"\n).\nload\n(\n\"/path/to/data\"\n);\nread.df\n(\n\"/path/to/data\"\n,\nsource\n=\n\"binaryFile\"\n,\npathGlobFilter\n=\n\"*.png\"\n)\nBinary file data source does not support writing a DataFrame back to the original files."}
{"url": "https://spark.apache.org/docs/latest/sql-data-sources-avro.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTroubleshooting\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nApache Avro Data Source Guide\nDeploying\nLoad and Save Functions\nto_avro() and from_avro()\nData Source Option\nConfiguration\nCompatibility with Databricks spark-avro\nSupported types for Avro -> Spark SQL conversion\nSupported types for Spark SQL -> Avro conversion\nHandling circular references of Avro fields\nSince Spark 2.4 release,\nSpark SQL\nprovides built-in support for reading and writing Apache Avro data.\nDeploying\nThe\nspark-avro\nmodule is external and not included in\nspark-submit\nor\nspark-shell\nby default.\nAs with any Spark applications,\nspark-submit\nis used to launch your application.\nspark-avro_2.13\nand its dependencies can be directly added to\nspark-submit\nusing\n--packages\n, such as,\n./bin/spark-submit --packages org.apache.spark:spark-avro_2.13:4.0.0 ...\nFor experimenting on\nspark-shell\n, you can also use\n--packages\nto add\norg.apache.spark:spark-avro_2.13\nand its dependencies directly,\n./bin/spark-shell --packages org.apache.spark:spark-avro_2.13:4.0.0 ...\nSee\nApplication Submission Guide\nfor more details about submitting applications with external dependencies.\nLoad and Save Functions\nSince\nspark-avro\nmodule is external, there is no\n.avro\nAPI in\nDataFrameReader\nor\nDataFrameWriter\n.\nTo load/save data in Avro format, you need to specify the data source option\nformat\nas\navro\n(or\norg.apache.spark.sql.avro\n).\ndf\n=\nspark\n.\nread\n.\nformat\n(\n\"\navro\n\"\n).\nload\n(\n\"\nexamples/src/main/resources/users.avro\n\"\n)\ndf\n.\nselect\n(\n\"\nname\n\"\n,\n\"\nfavorite_color\n\"\n).\nwrite\n.\nformat\n(\n\"\navro\n\"\n).\nsave\n(\n\"\nnamesAndFavColors.avro\n\"\n)\nval\nusersDF\n=\nspark\n.\nread\n.\nformat\n(\n\"avro\"\n).\nload\n(\n\"examples/src/main/resources/users.avro\"\n)\nusersDF\n.\nselect\n(\n\"name\"\n,\n\"favorite_color\"\n).\nwrite\n.\nformat\n(\n\"avro\"\n).\nsave\n(\n\"namesAndFavColors.avro\"\n)\nDataset\n<\nRow\n>\nusersDF\n=\nspark\n.\nread\n().\nformat\n(\n\"avro\"\n).\nload\n(\n\"examples/src/main/resources/users.avro\"\n);\nusersDF\n.\nselect\n(\n\"name\"\n,\n\"favorite_color\"\n).\nwrite\n().\nformat\n(\n\"avro\"\n).\nsave\n(\n\"namesAndFavColors.avro\"\n);\ndf\n<-\nread.df\n(\n\"examples/src/main/resources/users.avro\"\n,\n\"avro\"\n)\nwrite.df\n(\nselect\n(\ndf\n,\n\"name\"\n,\n\"favorite_color\"\n),\n\"namesAndFavColors.avro\"\n,\n\"avro\"\n)\nto_avro() and from_avro()\nThe Avro package provides function\nto_avro\nto encode a column as binary in Avro\nformat, and\nfrom_avro()\nto decode Avro binary data into a column. Both functions transform one column to\nanother column, and the input/output SQL data type can be a complex type or a primitive type.\nUsing Avro record as columns is useful when reading from or writing to a streaming source like Kafka. Each\nKafka key-value record will be augmented with some metadata, such as the ingestion timestamp into Kafka, the offset in Kafka, etc.\nIf the “value” field that contains your data is in Avro, you could use\nfrom_avro()\nto extract your data, enrich it, clean it, and then push it downstream to Kafka again or write it out to a file.\nto_avro()\ncan be used to turn structs into Avro records. This method is particularly useful when you would like to re-encode multiple columns into a single one when writing data out to Kafka.\nfrom\npyspark.sql.avro.functions\nimport\nfrom_avro\n,\nto_avro\n# `from_avro` requires Avro schema in JSON string format.\njsonFormatSchema\n=\nopen\n(\n\"\nexamples/src/main/resources/user.avsc\n\"\n,\n\"\nr\n\"\n).\nread\n()\ndf\n=\nspark\n\\\n.\nreadStream\n\\\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\noption\n(\n\"\nsubscribe\n\"\n,\n\"\ntopic1\n\"\n)\n\\\n.\nload\n()\n# 1. Decode the Avro data into a struct;\n# 2. Filter by column `favorite_color`;\n# 3. Encode the column `name` in Avro format.\noutput\n=\ndf\n\\\n.\nselect\n(\nfrom_avro\n(\n\"\nvalue\n\"\n,\njsonFormatSchema\n).\nalias\n(\n\"\nuser\n\"\n))\n\\\n.\nwhere\n(\n'\nuser.favorite_color ==\n\"\nred\n\"'\n)\n\\\n.\nselect\n(\nto_avro\n(\n\"\nuser.name\n\"\n).\nalias\n(\n\"\nvalue\n\"\n))\nquery\n=\noutput\n\\\n.\nwriteStream\n\\\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\noption\n(\n\"\ntopic\n\"\n,\n\"\ntopic2\n\"\n)\n\\\n.\nstart\n()\nimport\norg.apache.spark.sql.avro.functions._\n// `from_avro` requires Avro schema in JSON string format.\nval\njsonFormatSchema\n=\nnew\nString\n(\nFiles\n.\nreadAllBytes\n(\nPaths\n.\nget\n(\n\"./examples/src/main/resources/user.avsc\"\n)))\nval\ndf\n=\nspark\n.\nreadStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribe\"\n,\n\"topic1\"\n)\n.\nload\n()\n// 1. Decode the Avro data into a struct;\n// 2. Filter by column `favorite_color`;\n// 3. Encode the column `name` in Avro format.\nval\noutput\n=\ndf\n.\nselect\n(\nfrom_avro\n(\n$\n\"value\"\n,\njsonFormatSchema\n)\nas\n$\n\"user\"\n)\n.\nwhere\n(\n\"user.favorite_color == \\\"red\\\"\"\n)\n.\nselect\n(\nto_avro\n(\n$\n\"user.name\"\n)\nas\n$\n\"value\"\n)\nval\nquery\n=\noutput\n.\nwriteStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"topic\"\n,\n\"topic2\"\n)\n.\nstart\n()\nimport\nstatic\norg\n.\napache\n.\nspark\n.\nsql\n.\nfunctions\n.\ncol\n;\nimport\nstatic\norg\n.\napache\n.\nspark\n.\nsql\n.\navro\n.\nfunctions\n.*;\n// `from_avro` requires Avro schema in JSON string format.\nString\njsonFormatSchema\n=\nnew\nString\n(\nFiles\n.\nreadAllBytes\n(\nPaths\n.\nget\n(\n\"./examples/src/main/resources/user.avsc\"\n)));\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\nreadStream\n()\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribe\"\n,\n\"topic1\"\n)\n.\nload\n();\n// 1. Decode the Avro data into a struct;\n// 2. Filter by column `favorite_color`;\n// 3. Encode the column `name` in Avro format.\nDataset\n<\nRow\n>\noutput\n=\ndf\n.\nselect\n(\nfrom_avro\n(\ncol\n(\n\"value\"\n),\njsonFormatSchema\n).\nas\n(\n\"user\"\n))\n.\nwhere\n(\n\"user.favorite_color == \\\"red\\\"\"\n)\n.\nselect\n(\nto_avro\n(\ncol\n(\n\"user.name\"\n)).\nas\n(\n\"value\"\n));\nStreamingQuery\nquery\n=\noutput\n.\nwriteStream\n()\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"topic\"\n,\n\"topic2\"\n)\n.\nstart\n();\n# `from_avro` requires Avro schema in JSON string format.\njsonFormatSchema\n<-\npaste0\n(\nreadLines\n(\n\"examples/src/main/resources/user.avsc\"\n),\ncollapse\n=\n\" \"\n)\ndf\n<-\nread.stream\n(\n\"kafka\"\n,\nkafka.bootstrap.servers\n=\n\"host1:port1,host2:port2\"\n,\nsubscribe\n=\n\"topic1\"\n)\n# 1. Decode the Avro data into a struct;\n# 2. Filter by column `favorite_color`;\n# 3. Encode the column `name` in Avro format.\noutput\n<-\nselect\n(\nfilter\n(\nselect\n(\ndf\n,\nalias\n(\nfrom_avro\n(\n\"value\"\n,\njsonFormatSchema\n),\n\"user\"\n)),\ncolumn\n(\n\"user.favorite_color\"\n)\n==\n\"red\"\n),\nalias\n(\nto_avro\n(\n\"user.name\"\n),\n\"value\"\n)\n)\nwrite.stream\n(\noutput\n,\n\"kafka\"\n,\nkafka.bootstrap.servers\n=\n\"host1:port1,host2:port2\"\n,\ntopic\n=\n\"topic2\"\n)\nCREATE\nTABLE\nt\nAS\nSELECT\nNAMED_STRUCT\n(\n'u'\n,\nNAMED_STRUCT\n(\n'member0'\n,\nmember0\n,\n'member1'\n,\nmember1\n))\nAS\ns\nFROM\nVALUES\n(\n1\n,\nNULL\n),\n(\nNULL\n,\n'a'\n)\ntab\n(\nmember0\n,\nmember1\n);\nDECLARE\navro_schema\nSTRING\n;\nSET\nVARIABLE\navro_schema\n=\n'{ \"type\": \"record\", \"name\": \"struct\", \"fields\": [{ \"name\": \"u\", \"type\": [\"int\",\"string\"] }] }'\n;\nSELECT\nTO_AVRO\n(\ns\n,\navro_schema\n)\nAS\nRESULT\nFROM\nt\n;\nSELECT\nFROM_AVRO\n(\nresult\n,\navro_schema\n,\nMAP\n()).\nu\nFROM\n(\nSELECT\nTO_AVRO\n(\ns\n,\navro_schema\n)\nAS\nRESULT\nFROM\nt\n);\nDROP\nTEMPORARY\nVARIABLE\navro_schema\n;\nDROP\nTABLE\nt\n;\nData Source Option\nData source options of Avro can be set via:\nthe\n.option\nmethod on\nDataFrameReader\nor\nDataFrameWriter\n.\nthe\noptions\nparameter in function\nfrom_avro\n.\nProperty Name\nDefault\nMeaning\nScope\nSince Version\navroSchema\nNone\nOptional schema provided by a user in JSON format.\nWhen reading Avro files or calling function\nfrom_avro\n, this option can be set to an evolved schema, which is compatible but different with\n          the actual Avro schema. The deserialization schema will be consistent with the evolved schema.\n          For example, if we set an evolved schema containing one additional column with a default value,\n          the reading result in Spark will contain the new column too. Note that when using this option with\nfrom_avro\n, you still need to pass the actual Avro schema as a parameter to the function.\nWhen writing Avro, this option can be set if the expected output Avro schema doesn't match the\n          schema converted by Spark. For example, the expected schema of one column is of \"enum\" type,\n          instead of \"string\" type in the default converted schema.\nread, write and function\nfrom_avro\n2.4.0\nrecordName\ntopLevelRecord\nTop level record name in write result, which is required in Avro spec.\nwrite\n2.4.0\nrecordNamespace\n\"\"\nRecord namespace in write result.\nwrite\n2.4.0\nignoreExtension\ntrue\nThe option controls ignoring of files without\n.avro\nextensions in read.\nIf the option is enabled, all files (with and without\n.avro\nextension) are loaded.\nThe option has been deprecated, and it will be removed in the future releases. Please use the general data source option\npathGlobFilter\nfor filtering file names.\nread\n2.4.0\ncompression\nsnappy\nThe\ncompression\noption allows to specify a compression codec used in write.\nCurrently supported codecs are\nuncompressed\n,\nsnappy\n,\ndeflate\n,\nbzip2\n,\nxz\nand\nzstandard\n.\nIf the option is not set, the configuration\nspark.sql.avro.compression.codec\nconfig is taken into account.\nwrite\n2.4.0\nmode\nFAILFAST\nThe\nmode\noption allows to specify parse mode for function\nfrom_avro\n.\nCurrently supported modes are:\nFAILFAST\n: Throws an exception on processing corrupted record.\nPERMISSIVE\n: Corrupt records are processed as null result. Therefore, the\n        data schema is forced to be fully nullable, which might be different from the one user provided.\nfunction\nfrom_avro\n2.4.0\ndatetimeRebaseMode\n(value of\nspark.sql.avro.datetimeRebaseModeInRead\nconfiguration)\nThe\ndatetimeRebaseMode\noption allows to specify the rebasing mode for the values of the\ndate\n,\ntimestamp-micros\n,\ntimestamp-millis\nlogical types from the Julian to Proleptic Gregorian calendar.\nCurrently supported modes are:\nEXCEPTION\n: fails in reads of ancient dates/timestamps that are ambiguous between the two calendars.\nCORRECTED\n: loads dates/timestamps without rebasing.\nLEGACY\n: performs rebasing of ancient dates/timestamps from the Julian to Proleptic Gregorian calendar.\nread and function\nfrom_avro\n3.2.0\npositionalFieldMatching\nfalse\nThis can be used in tandem with the `avroSchema` option to adjust the behavior for matching the fields in the provided Avro schema with those in the SQL schema. By default, the matching will be performed using field names, ignoring their positions. If this option is set to \"true\", the matching will be based on the position of the fields.\nread and write\n3.2.0\nenableStableIdentifiersForUnionType\nfalse\nIf it is set to true, Avro schema is deserialized into Spark SQL schema, and the Avro Union type is transformed into a structure where the field names remain consistent with their respective types. The resulting field names are converted to lowercase, e.g. member_int or member_string. If two user-defined type names or a user-defined type name and a built-in type name are identical regardless of case, an exception will be raised. However, in other cases, the field names can be uniquely identified.\nread\n3.5.0\nstableIdentifierPrefixForUnionType\nmember_\nWhen `enableStableIdentifiersForUnionType` is enabled, the option allows to configure the prefix for fields of Avro Union type.\nread\n4.0.0\nrecursiveFieldMaxDepth\n-1\nIf this option is specified to negative or is set to 0, recursive fields are not permitted. Setting it to 1 drops all recursive fields, 2 allows recursive fields to be recursed once, and 3 allows it to be recursed twice and so on, up to 15. Values larger than 15 are not allowed in order to avoid inadvertently creating very large schemas. If an avro message has depth beyond this limit, the Spark struct returned is truncated after the recursion limit. An example of usage can be found in section\nHandling circular references of Avro fields\nread\n4.0.0\nConfiguration\nConfiguration of Avro can be done via\nspark.conf.set\nor by running\nSET key=value\ncommands using SQL.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.legacy.replaceDatabricksSparkAvro.enabled\ntrue\nIf it is set to true, the data source provider\ncom.databricks.spark.avro\nis mapped\n      to the built-in but external Avro data source module for backward compatibility.\nNote:\nthe SQL config has been deprecated in Spark 3.2 and might be removed in the future.\n2.4.0\nspark.sql.avro.compression.codec\nsnappy\nCompression codec used in writing of AVRO files. Supported codecs: uncompressed, deflate,\n      snappy, bzip2, xz and zstandard. Default codec is snappy.\n2.4.0\nspark.sql.avro.deflate.level\n-1\nCompression level for the deflate codec used in writing of AVRO files. Valid value must be in\n      the range of from 1 to 9 inclusive or -1. The default value is -1 which corresponds to 6 level\n      in the current implementation.\n2.4.0\nspark.sql.avro.xz.level\n6\nCompression level for the xz codec used in writing of AVRO files. Valid value must be in\n      the range of from 1 to 9 inclusive. The default value is 6 in the current implementation.\n4.0.0\nspark.sql.avro.zstandard.level\n3\nCompression level for the zstandard codec used in writing of AVRO files.\n      The default value is 3 in the current implementation.\n4.0.0\nspark.sql.avro.zstandard.bufferPool.enabled\nfalse\nIf true, enable buffer pool of ZSTD JNI library when writing of AVRO files.\n4.0.0\nspark.sql.avro.datetimeRebaseModeInRead\nEXCEPTION\nThe rebasing mode for the values of the\ndate\n,\ntimestamp-micros\n,\ntimestamp-millis\nlogical types from the Julian to Proleptic Gregorian calendar:\nEXCEPTION\n: Spark will fail the reading if it sees ancient dates/timestamps that are ambiguous between the two calendars.\nCORRECTED\n: Spark will not do rebase and read the dates/timestamps as it is.\nLEGACY\n: Spark will rebase dates/timestamps from the legacy hybrid (Julian + Gregorian) calendar to Proleptic Gregorian calendar when reading Avro files.\nThis config is only effective if the writer info (like Spark, Hive) of the Avro files is unknown.\n3.0.0\nspark.sql.avro.datetimeRebaseModeInWrite\nEXCEPTION\nThe rebasing mode for the values of the\ndate\n,\ntimestamp-micros\n,\ntimestamp-millis\nlogical types from the Proleptic Gregorian to Julian calendar:\nEXCEPTION\n: Spark will fail the writing if it sees ancient dates/timestamps that are ambiguous between the two calendars.\nCORRECTED\n: Spark will not do rebase and write the dates/timestamps as it is.\nLEGACY\n: Spark will rebase dates/timestamps from Proleptic Gregorian calendar to the legacy hybrid (Julian + Gregorian) calendar when writing Avro files.\n3.0.0\nspark.sql.avro.filterPushdown.enabled\ntrue\nWhen true, enable filter pushdown to Avro datasource.\n3.1.0\nCompatibility with Databricks spark-avro\nThis Avro data source module is originally from and compatible with Databricks’s open source repository\nspark-avro\n.\nBy default with the SQL configuration\nspark.sql.legacy.replaceDatabricksSparkAvro.enabled\nenabled, the data source provider\ncom.databricks.spark.avro\nis\nmapped to this built-in Avro module. For the Spark tables created with\nProvider\nproperty as\ncom.databricks.spark.avro\nin\ncatalog meta store, the mapping is essential to load these tables if you are using this built-in Avro module.\nNote in Databricks’s\nspark-avro\n, implicit classes\nAvroDataFrameWriter\nand\nAvroDataFrameReader\nwere created for shortcut function\n.avro()\n. In this\nbuilt-in but external module, both implicit classes are removed. Please use\n.format(\"avro\")\nin\nDataFrameWriter\nor\nDataFrameReader\ninstead, which should be clean and good enough.\nIf you prefer using your own build of\nspark-avro\njar file, you can simply disable the configuration\nspark.sql.legacy.replaceDatabricksSparkAvro.enabled\n, and use the option\n--jars\non deploying your\napplications. Read the\nAdvanced Dependency Management\nsection in the Application\nSubmission Guide for more details.\nSupported types for Avro -> Spark SQL conversion\nCurrently Spark supports reading all\nprimitive types\nand\ncomplex types\nunder records of Avro.\nAvro type\nSpark SQL type\nboolean\nBooleanType\nint\nIntegerType\nlong\nLongType\nfloat\nFloatType\ndouble\nDoubleType\nstring\nStringType\nenum\nStringType\nfixed\nBinaryType\nbytes\nBinaryType\nrecord\nStructType\narray\nArrayType\nmap\nMapType\nunion\nSee below\nIn addition to the types listed above, it supports reading\nunion\ntypes. The following three types are considered basic\nunion\ntypes:\nunion(int, long)\nwill be mapped to LongType.\nunion(float, double)\nwill be mapped to DoubleType.\nunion(something, null)\n, where something is any supported Avro type. This will be mapped to the same Spark SQL type as that of something, with nullable set to true.\nAll other union types are considered complex. They will be mapped to StructType where field names are member0, member1, etc., in accordance with members of the union. This is consistent with the behavior when converting between Avro and Parquet.\nIt also supports reading the following Avro\nlogical types\n:\nAvro logical type\nAvro type\nSpark SQL type\ndate\nint\nDateType\ntimestamp-millis\nlong\nTimestampType\ntimestamp-micros\nlong\nTimestampType\ndecimal\nfixed\nDecimalType\ndecimal\nbytes\nDecimalType\nAt the moment, it ignores docs, aliases and other properties present in the Avro file.\nSupported types for Spark SQL -> Avro conversion\nSpark supports writing of all Spark SQL types into Avro. For most types, the mapping from Spark types to Avro types is straightforward (e.g. IntegerType gets converted to int); however, there are a few special cases which are listed below:\nSpark SQL type\nAvro type\nAvro logical type\nByteType\nint\nShortType\nint\nBinaryType\nbytes\nDateType\nint\ndate\nTimestampType\nlong\ntimestamp-micros\nDecimalType\nfixed\ndecimal\nYou can also specify the whole output Avro schema with the option\navroSchema\n, so that Spark SQL types can be converted into other Avro types. The following conversions are not applied by default and require user specified Avro schema:\nSpark SQL type\nAvro type\nAvro logical type\nBinaryType\nfixed\nStringType\nenum\nTimestampType\nlong\ntimestamp-millis\nDecimalType\nbytes\ndecimal\nHandling circular references of Avro fields\nIn Avro, a circular reference occurs when the type of a field is defined in one of the parent records. This can cause issues when parsing the data, as it can result in infinite loops or other unexpected behavior.\nTo read Avro data with schema that has circular reference, users can use the\nrecursiveFieldMaxDepth\noption to specify the maximum number of levels of recursion to allow when parsing the schema. By default, Spark Avro data source will not permit recursive fields by setting\nrecursiveFieldMaxDepth\nto -1. However, you can set this option to 1 to 15 if needed.\nSetting\nrecursiveFieldMaxDepth\nto 1 drops all recursive fields, setting it to 2 allows it to be recursed once, and setting it to 3 allows it to be recursed twice. A\nrecursiveFieldMaxDepth\nvalue greater than 15 is not allowed, as it can lead to performance issues and even stack overflows.\nSQL Schema for the below Avro message will vary based on the value of\nrecursiveFieldMaxDepth\n.\nThis div is only used to make markdown editor/viewer happy and does not display on web\n\n```avro\n{\n  \"type\": \"record\",\n  \"name\": \"Node\",\n  \"fields\": [\n    {\"name\": \"Id\", \"type\": \"int\"},\n    {\"name\": \"Next\", \"type\": [\"null\", \"Node\"]}\n  ]\n}\n\n// The Avro schema defined above, would be converted into a Spark SQL columns with the following\n// structure based on `recursiveFieldMaxDepth` value.\n\n1: struct<Id: int>\n2: struct<Id: int, Next: struct<Id: int>>\n3: struct<Id: int, Next: struct<Id: int, Next: struct<Id: int>>>\n```"}
{"url": "https://spark.apache.org/docs/latest/mllib-isotonic-regression.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nRegression - RDD-based API\nIsotonic regression\nIsotonic regression\nbelongs to the family of regression algorithms. Formally isotonic regression is a problem where\ngiven a finite set of real numbers\n$Y = {y_1, y_2, ..., y_n}$\nrepresenting observed responses\nand\n$X = {x_1, x_2, ..., x_n}$\nthe unknown response values to be fitted\nfinding a function that minimizes\n\\begin{equation}\n  f(x) = \\sum_{i=1}^n w_i (y_i - x_i)^2\n\\end{equation}\nwith respect to complete order subject to\n$x_1\\le x_2\\le ...\\le x_n$\nwhere\n$w_i$\nare positive weights.\nThe resulting function is called isotonic regression and it is unique.\nIt can be viewed as least squares problem under order restriction.\nEssentially isotonic regression is a\nmonotonic function\nbest fitting the original data points.\nspark.mllib\nsupports a\npool adjacent violators algorithm\nwhich uses an approach to\nparallelizing isotonic regression\n.\nThe training input is an RDD of tuples of three double values that represent\nlabel, feature and weight in this order. In case there are multiple tuples with\nthe same feature then these tuples are aggregated into a single tuple as follows:\nAggregated label is the weighted average of all labels.\nAggregated feature is the unique feature value.\nAggregated weight is the sum of all weights.\nAdditionally, IsotonicRegression algorithm has one\noptional parameter called $isotonic$ defaulting to true.\nThis argument specifies if the isotonic regression is\nisotonic (monotonically increasing) or antitonic (monotonically decreasing).\nTraining returns an IsotonicRegressionModel that can be used to predict\nlabels for both known and unknown features. The result of isotonic regression\nis treated as piecewise linear function. The rules for prediction therefore are:\nIf the prediction input exactly matches a training feature\nthen associated prediction is returned.\nIf the prediction input is lower or higher than all training features\nthen prediction with lowest or highest feature is returned respectively.\nIf the prediction input falls between two training features then prediction is treated\nas piecewise linear function and interpolated value is calculated from the\npredictions of the two closest features.\nExamples\nData are read from a file where each line has a format label,feature\ni.e. 4710.28,500.00. The data are split to training and testing set.\nModel is created using the training set and a mean squared error is calculated from the predicted\nlabels and real labels in the test set.\nRefer to the\nIsotonicRegression\nPython docs\nand\nIsotonicRegressionModel\nPython docs\nfor more details on the API.\nimport\nmath\nfrom\npyspark.mllib.regression\nimport\nIsotonicRegression\n,\nIsotonicRegressionModel\nfrom\npyspark.mllib.util\nimport\nMLUtils\n# Load and parse the data\ndef\nparsePoint\n(\nlabeledData\n):\nreturn\n(\nlabeledData\n.\nlabel\n,\nlabeledData\n.\nfeatures\n[\n0\n],\n1.0\n)\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"\ndata/mllib/sample_isotonic_regression_libsvm_data.txt\n\"\n)\n# Create label, feature, weight tuples from input data with weight set to default value 1.0.\nparsedData\n=\ndata\n.\nmap\n(\nparsePoint\n)\n# Split data into training (60%) and test (40%) sets.\ntraining\n,\ntest\n=\nparsedData\n.\nrandomSplit\n([\n0.6\n,\n0.4\n],\n11\n)\n# Create isotonic regression model from training data.\n# Isotonic parameter defaults to true so it is only shown for demonstration\nmodel\n=\nIsotonicRegression\n.\ntrain\n(\ntraining\n)\n# Create tuples of predicted and real labels.\npredictionAndLabel\n=\ntest\n.\nmap\n(\nlambda\np\n:\n(\nmodel\n.\npredict\n(\np\n[\n1\n]),\np\n[\n0\n]))\n# Calculate mean squared error between predicted and real labels.\nmeanSquaredError\n=\npredictionAndLabel\n.\nmap\n(\nlambda\npl\n:\nmath\n.\npow\n((\npl\n[\n0\n]\n-\npl\n[\n1\n]),\n2\n)).\nmean\n()\nprint\n(\n\"\nMean Squared Error =\n\"\n+\nstr\n(\nmeanSquaredError\n))\n# Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"\ntarget/tmp/myIsotonicRegressionModel\n\"\n)\nsameModel\n=\nIsotonicRegressionModel\n.\nload\n(\nsc\n,\n\"\ntarget/tmp/myIsotonicRegressionModel\n\"\n)\nFind full example code at \"examples/src/main/python/mllib/isotonic_regression_example.py\" in the Spark repo.\nData are read from a file where each line has a format label,feature\ni.e. 4710.28,500.00. The data are split to training and testing set.\nModel is created using the training set and a mean squared error is calculated from the predicted\nlabels and real labels in the test set.\nRefer to the\nIsotonicRegression\nScala docs\nand\nIsotonicRegressionModel\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.regression.\n{\nIsotonicRegression\n,\nIsotonicRegressionModel\n}\nimport\norg.apache.spark.mllib.util.MLUtils\nval\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_isotonic_regression_libsvm_data.txt\"\n).\ncache\n()\n// Create label, feature, weight tuples from input data with weight set to default value 1.0.\nval\nparsedData\n=\ndata\n.\nmap\n{\nlabeledPoint\n=>\n(\nlabeledPoint\n.\nlabel\n,\nlabeledPoint\n.\nfeatures\n(\n0\n),\n1.0\n)\n}\n// Split data into training (60%) and test (40%) sets.\nval\nsplits\n=\nparsedData\n.\nrandomSplit\n(\nArray\n(\n0.6\n,\n0.4\n),\nseed\n=\n11L\n)\nval\ntraining\n=\nsplits\n(\n0\n)\nval\ntest\n=\nsplits\n(\n1\n)\n// Create isotonic regression model from training data.\n// Isotonic parameter defaults to true so it is only shown for demonstration\nval\nmodel\n=\nnew\nIsotonicRegression\n().\nsetIsotonic\n(\ntrue\n).\nrun\n(\ntraining\n)\n// Create tuples of predicted and real labels.\nval\npredictionAndLabel\n=\ntest\n.\nmap\n{\npoint\n=>\nval\npredictedLabel\n=\nmodel\n.\npredict\n(\npoint\n.\n_2\n)\n(\npredictedLabel\n,\npoint\n.\n_1\n)\n}\n// Calculate mean squared error between predicted and real labels.\nval\nmeanSquaredError\n=\npredictionAndLabel\n.\nmap\n{\ncase\n(\np\n,\nl\n)\n=>\nmath\n.\npow\n((\np\n-\nl\n),\n2\n)\n}.\nmean\n()\nprintln\n(\ns\n\"Mean Squared Error = $meanSquaredError\"\n)\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/myIsotonicRegressionModel\"\n)\nval\nsameModel\n=\nIsotonicRegressionModel\n.\nload\n(\nsc\n,\n\"target/tmp/myIsotonicRegressionModel\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\" in the Spark repo.\nData are read from a file where each line has a format label,feature\ni.e. 4710.28,500.00. The data are split to training and testing set.\nModel is created using the training set and a mean squared error is calculated from the predicted\nlabels and real labels in the test set.\nRefer to the\nIsotonicRegression\nJava docs\nand\nIsotonicRegressionModel\nJava docs\nfor details on the API.\nimport\nscala.Tuple2\n;\nimport\nscala.Tuple3\n;\nimport\norg.apache.spark.api.java.JavaPairRDD\n;\nimport\norg.apache.spark.api.java.JavaSparkContext\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.mllib.regression.IsotonicRegression\n;\nimport\norg.apache.spark.mllib.regression.IsotonicRegressionModel\n;\nimport\norg.apache.spark.mllib.regression.LabeledPoint\n;\nimport\norg.apache.spark.mllib.util.MLUtils\n;\nJavaRDD\n<\nLabeledPoint\n>\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\njsc\n.\nsc\n(),\n\"data/mllib/sample_isotonic_regression_libsvm_data.txt\"\n).\ntoJavaRDD\n();\n// Create label, feature, weight tuples from input data with weight set to default value 1.0.\nJavaRDD\n<\nTuple3\n<\nDouble\n,\nDouble\n,\nDouble\n>>\nparsedData\n=\ndata\n.\nmap\n(\npoint\n->\nnew\nTuple3\n<>(\npoint\n.\nlabel\n(),\npoint\n.\nfeatures\n().\napply\n(\n0\n),\n1.0\n));\n// Split data into training (60%) and test (40%) sets.\nJavaRDD\n<\nTuple3\n<\nDouble\n,\nDouble\n,\nDouble\n>>[]\nsplits\n=\nparsedData\n.\nrandomSplit\n(\nnew\ndouble\n[]{\n0.6\n,\n0.4\n},\n11L\n);\nJavaRDD\n<\nTuple3\n<\nDouble\n,\nDouble\n,\nDouble\n>>\ntraining\n=\nsplits\n[\n0\n];\nJavaRDD\n<\nTuple3\n<\nDouble\n,\nDouble\n,\nDouble\n>>\ntest\n=\nsplits\n[\n1\n];\n// Create isotonic regression model from training data.\n// Isotonic parameter defaults to true so it is only shown for demonstration\nIsotonicRegressionModel\nmodel\n=\nnew\nIsotonicRegression\n().\nsetIsotonic\n(\ntrue\n).\nrun\n(\ntraining\n);\n// Create tuples of predicted and real labels.\nJavaPairRDD\n<\nDouble\n,\nDouble\n>\npredictionAndLabel\n=\ntest\n.\nmapToPair\n(\npoint\n->\nnew\nTuple2\n<>(\nmodel\n.\npredict\n(\npoint\n.\n_2\n()),\npoint\n.\n_1\n()));\n// Calculate mean squared error between predicted and real labels.\ndouble\nmeanSquaredError\n=\npredictionAndLabel\n.\nmapToDouble\n(\npl\n->\n{\ndouble\ndiff\n=\npl\n.\n_1\n()\n-\npl\n.\n_2\n();\nreturn\ndiff\n*\ndiff\n;\n}).\nmean\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Mean Squared Error = \"\n+\nmeanSquaredError\n);\n// Save and load model\nmodel\n.\nsave\n(\njsc\n.\nsc\n(),\n\"target/tmp/myIsotonicRegressionModel\"\n);\nIsotonicRegressionModel\nsameModel\n=\nIsotonicRegressionModel\n.\nload\n(\njsc\n.\nsc\n(),\n\"target/tmp/myIsotonicRegressionModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\" in the Spark repo."}
{"url": "https://spark.apache.org/docs/latest/sql-data-sources-protobuf.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTroubleshooting\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nProtobuf Data Source Guide\nDeploying\nto_protobuf() and from_protobuf()\nSupported types for Protobuf -> Spark SQL conversion\nSupported types for Spark SQL -> Protobuf conversion\nHandling circular references protobuf fields\nData Source Option\nSince Spark 3.4.0 release,\nSpark SQL\nprovides built-in support for reading and writing protobuf data.\nDeploying\nThe\nspark-protobuf\nmodule is external and not included in\nspark-submit\nor\nspark-shell\nby default.\nAs with any Spark applications,\nspark-submit\nis used to launch your application.\nspark-protobuf_2.13\nand its dependencies can be directly added to\nspark-submit\nusing\n--packages\n, such as,\n./bin/spark-submit --packages org.apache.spark:spark-protobuf_2.13:4.0.0 ...\nFor experimenting on\nspark-shell\n, you can also use\n--packages\nto add\norg.apache.spark:spark-protobuf_2.13\nand its dependencies directly,\n./bin/spark-shell --packages org.apache.spark:spark-protobuf_2.13:4.0.0 ...\nSee\nApplication Submission Guide\nfor more details about submitting applications with external dependencies.\nto_protobuf() and from_protobuf()\nThe spark-protobuf package provides function\nto_protobuf\nto encode a column as binary in protobuf\nformat, and\nfrom_protobuf()\nto decode protobuf binary data into a column. Both functions transform one column to\nanother column, and the input/output SQL data type can be a complex type or a primitive type.\nUsing protobuf message as columns is useful when reading from or writing to a streaming source like Kafka. Each\nKafka key-value record will be augmented with some metadata, such as the ingestion timestamp into Kafka, the offset in Kafka, etc.\nIf the “value” field that contains your data is in protobuf, you could use\nfrom_protobuf()\nto extract your data, enrich it, clean it, and then push it downstream to Kafka again or write it out to a different sink.\nto_protobuf()\ncan be used to turn structs into protobuf message. This method is particularly useful when you would like to re-encode multiple columns into a single one when writing data out to Kafka.\nSpark SQL schema is generated based on the protobuf descriptor file or protobuf class passed to\nfrom_protobuf\nand\nto_protobuf\n. The specified protobuf class or protobuf descriptor file must match the data, otherwise, the behavior is undefined: it may fail or return arbitrary results.\nThis div is only used to make markdown editor/viewer happy and does not display on web\n\n```python\nfrom\npyspark.sql.protobuf.functions\nimport\nfrom_protobuf\n,\nto_protobuf\n# from_protobuf and to_protobuf provide two schema choices. Via Protobuf descriptor file,\n# or via shaded Java class.\n# give input .proto protobuf schema\n# syntax = \"proto3\"\n# message AppEvent {\n#   string name = 1;\n#   int64 id = 2;\n#   string context = 3;\n# }\ndf\n=\nspark\n.\nreadStream\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n.\noption\n(\n\"\nsubscribe\n\"\n,\n\"\ntopic1\n\"\n)\n.\nload\n()\n# 1. Decode the Protobuf data of schema `AppEvent` into a struct;\n# 2. Filter by column `name`;\n# 3. Encode the column `event` in Protobuf format.\n# The Protobuf protoc command can be used to generate a protobuf descriptor file for give .proto file.\noutput\n=\ndf\n.\nselect\n(\nfrom_protobuf\n(\n\"\nvalue\n\"\n,\n\"\nAppEvent\n\"\n,\ndescriptorFilePath\n).\nalias\n(\n\"\nevent\n\"\n))\n.\nwhere\n(\n'\nevent.name ==\n\"\nalice\n\"'\n)\n.\nselect\n(\nto_protobuf\n(\n\"\nevent\n\"\n,\n\"\nAppEvent\n\"\n,\ndescriptorFilePath\n).\nalias\n(\n\"\nevent\n\"\n))\n# Alternatively, you can decode and encode the SQL columns into protobuf format using protobuf\n# class name. The specified Protobuf class must match the data, otherwise the behavior is undefined:\n# it may fail or return arbitrary result. To avoid conflicts, the jar file containing the\n# 'com.google.protobuf.*' classes should be shaded. An example of shading can be found at\n# https://github.com/rangadi/shaded-protobuf-classes.\noutput\n=\ndf\n.\nselect\n(\nfrom_protobuf\n(\n\"\nvalue\n\"\n,\n\"\norg.sparkproject.spark_protobuf.protobuf.AppEvent\n\"\n).\nalias\n(\n\"\nevent\n\"\n))\n.\nwhere\n(\n'\nevent.name ==\n\"\nalice\n\"'\n)\noutput\n.\nprintSchema\n()\n# root\n#  |--event: struct (nullable = true)\n#  |   |-- name : string (nullable = true)\n#  |   |-- id: long (nullable = true)\n#  |   |-- context: string (nullable = true)\noutput\n=\noutput\n.\nselect\n(\nto_protobuf\n(\n\"\nevent\n\"\n,\n\"\norg.sparkproject.spark_protobuf.protobuf.AppEvent\n\"\n).\nalias\n(\n\"\nevent\n\"\n))\nquery\n=\noutput\n.\nwriteStream\n.\nformat\n(\n\"\nkafka\n\"\n)\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\noption\n(\n\"\ntopic\n\"\n,\n\"\ntopic2\n\"\n)\n.\nstart\n()\n```\nThis div is only used to make markdown editor/viewer happy and does not display on web\n\n```scala\nimport\norg.apache.spark.sql.protobuf.functions._\n// `from_protobuf` and `to_protobuf` provides two schema choices. Via the protobuf descriptor file,\n// or via shaded Java class.\n// give input .proto protobuf schema\n// syntax = \"proto3\"\n// message AppEvent {\n//   string name = 1;\n//   int64 id = 2;\n//   string context = 3;\n// }\nval\ndf\n=\nspark\n.\nreadStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribe\"\n,\n\"topic1\"\n)\n.\nload\n()\n// 1. Decode the Protobuf data of schema `AppEvent` into a struct;\n// 2. Filter by column `name`;\n// 3. Encode the column `event` in Protobuf format.\n// The Protobuf protoc command can be used to generate a protobuf descriptor file for give .proto file.\nval\noutput\n=\ndf\n.\nselect\n(\nfrom_protobuf\n(\n$\n\"value\"\n,\n\"AppEvent\"\n,\ndescriptorFilePath\n)\nas\n$\n\"event\"\n)\n.\nwhere\n(\n\"event.name == \\\"alice\\\"\"\n)\n.\nselect\n(\nto_protobuf\n(\n$\n\"user\"\n,\n\"AppEvent\"\n,\ndescriptorFilePath\n)\nas\n$\n\"event\"\n)\nval\nquery\n=\noutput\n.\nwriteStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"topic\"\n,\n\"topic2\"\n)\n.\nstart\n()\n// Alternatively, you can decode and encode the SQL columns into protobuf format using protobuf\n// class name. The specified Protobuf class must match the data, otherwise the behavior is undefined:\n// it may fail or return arbitrary result. To avoid conflicts, the jar file containing the\n// 'com.google.protobuf.*' classes should be shaded. An example of shading can be found at\n// https://github.com/rangadi/shaded-protobuf-classes.\nvar\noutput\n=\ndf\n.\nselect\n(\nfrom_protobuf\n(\n$\n\"value\"\n,\n\"org.example.protos..AppEvent\"\n)\nas\n$\n\"event\"\n)\n.\nwhere\n(\n\"event.name == \\\"alice\\\"\"\n)\noutput\n.\nprintSchema\n()\n// root\n//  |--event: struct (nullable = true)\n//  |    |-- name : string (nullable = true)\n//  |    |-- id: long (nullable = true)\n//  |    |-- context: string (nullable = true)\noutput\n=\noutput\n.\nselect\n(\nto_protobuf\n(\n$\n\"event\"\n,\n\"org.sparkproject.spark_protobuf.protobuf.AppEvent\"\n)\nas\n$\n\"event\"\n)\nval\nquery\n=\noutput\n.\nwriteStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"topic\"\n,\n\"topic2\"\n)\n.\nstart\n()\n```\nThis div is only used to make markdown editor/viewer happy and does not display on web\n\n```java\nimport\nstatic\norg\n.\napache\n.\nspark\n.\nsql\n.\nfunctions\n.\ncol\n;\nimport\nstatic\norg\n.\napache\n.\nspark\n.\nsql\n.\nprotobuf\n.\nfunctions\n.*;\n// `from_protobuf` and `to_protobuf` provides two schema choices. Via the protobuf descriptor file,\n// or via shaded Java class.\n// give input .proto protobuf schema\n// syntax = \"proto3\"\n// message AppEvent {\n//   string name = 1;\n//   int64 id = 2;\n//   string context = 3;\n// }\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\nreadStream\n()\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribe\"\n,\n\"topic1\"\n)\n.\nload\n();\n// 1. Decode the Protobuf data of schema `AppEvent` into a struct;\n// 2. Filter by column `name`;\n// 3. Encode the column `event` in Protobuf format.\n// The Protobuf protoc command can be used to generate a protobuf descriptor file for give .proto file.\nDataset\n<\nRow\n>\noutput\n=\ndf\n.\nselect\n(\nfrom_protobuf\n(\ncol\n(\n\"value\"\n),\n\"AppEvent\"\n,\ndescriptorFilePath\n).\nas\n(\n\"event\"\n))\n.\nwhere\n(\n\"event.name == \\\"alice\\\"\"\n)\n.\nselect\n(\nto_protobuf\n(\ncol\n(\n\"event\"\n),\n\"AppEvent\"\n,\ndescriptorFilePath\n).\nas\n(\n\"event\"\n));\n// Alternatively, you can decode and encode the SQL columns into protobuf format using protobuf\n// class name. The specified Protobuf class must match the data, otherwise the behavior is undefined:\n// it may fail or return arbitrary result. To avoid conflicts, the jar file containing the\n// 'com.google.protobuf.*' classes should be shaded. An example of shading can be found at\n// https://github.com/rangadi/shaded-protobuf-classes.\nDataset\n<\nRow\n>\noutput\n=\ndf\n.\nselect\n(\nfrom_protobuf\n(\ncol\n(\n\"value\"\n),\n\"org.sparkproject.spark_protobuf.protobuf.AppEvent\"\n).\nas\n(\n\"event\"\n))\n.\nwhere\n(\n\"event.name == \\\"alice\\\"\"\n)\noutput\n.\nprintSchema\n()\n// root\n//  |--event: struct (nullable = true)\n//  |    |-- name : string (nullable = true)\n//  |    |-- id: long (nullable = true)\n//  |    |-- context: string (nullable = true)\noutput\n=\noutput\n.\nselect\n(\nto_protobuf\n(\ncol\n(\n\"event\"\n),\n\"org.sparkproject.spark_protobuf.protobuf.AppEvent\"\n).\nas\n(\n\"event\"\n));\nStreamingQuery\nquery\n=\noutput\n.\nwriteStream\n()\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"topic\"\n,\n\"topic2\"\n)\n.\nstart\n();\n```\nSupported types for Protobuf -> Spark SQL conversion\nCurrently Spark supports reading\nprotobuf scalar types\n,\nenum types\n,\nnested type\n, and\nmaps type\nunder messages of Protobuf.\nIn addition to the these types,\nspark-protobuf\nalso introduces support for Protobuf\nOneOf\nfields. which allows you to handle messages that can have multiple possible sets of fields, but only one set can be present at a time. This is useful for situations where the data you are working with is not always in the same format, and you need to be able to handle messages with different sets of fields without encountering errors.\nProtobuf type\nSpark SQL type\nboolean\nBooleanType\nint\nIntegerType\nlong\nLongType\nfloat\nFloatType\ndouble\nDoubleType\nstring\nStringType\nenum\nStringType\nbytes\nBinaryType\nMessage\nStructType\nrepeated\nArrayType\nmap\nMapType\nOneOf\nStruct\nIt also supports reading the following Protobuf types\nTimestamp\nand\nDuration\nProtobuf logical type\nProtobuf schema\nSpark SQL type\nduration\nMessageType{seconds: Long, nanos: Int}\nDayTimeIntervalType\ntimestamp\nMessageType{seconds: Long, nanos: Int}\nTimestampType\nSupported types for Spark SQL -> Protobuf conversion\nSpark supports the writing of all Spark SQL types into Protobuf. For most types, the mapping from Spark types to Protobuf types is straightforward (e.g. IntegerType gets converted to int);\nSpark SQL type\nProtobuf type\nBooleanType\nboolean\nIntegerType\nint\nLongType\nlong\nFloatType\nfloat\nDoubleType\ndouble\nStringType\nstring\nStringType\nenum\nBinaryType\nbytes\nStructType\nmessage\nArrayType\nrepeated\nMapType\nmap\nHandling circular references protobuf fields\nOne common issue that can arise when working with Protobuf data is the presence of circular references. In Protobuf, a circular reference occurs when a field refers back to itself or to another field that refers back to the original field. This can cause issues when parsing the data, as it can result in infinite loops or other unexpected behavior.\nTo address this issue, the latest version of spark-protobuf introduces a new feature: the ability to check for circular references through field types. This allows users use the\nrecursive.fields.max.depth\noption to specify the maximum number of levels of recursion to allow when parsing the schema. By default,\nspark-protobuf\nwill not permit recursive fields by setting\nrecursive.fields.max.depth\nto -1. However, you can set this option to 1 to 10 if needed.\nSetting\nrecursive.fields.max.depth\nto 1 drops all recursive fields, setting it to 2 allows it to be recursed once, and setting it to 3 allows it to be recursed twice. A\nrecursive.fields.max.depth\nvalue greater than 10 is not allowed, as it can lead to performance issues and even stack overflows.\nSQL Schema for the below protobuf message will vary based on the value of\nrecursive.fields.max.depth\n.\nThis div is only used to make markdown editor/viewer happy and does not display on web\n\n```protobuf\nsyntax\n=\n\"proto3\"\nmessage\nPerson\n{\nstring\nname\n=\n1\n;\nPerson\nbff\n=\n2\n}\n// The protobuf schema defined above, would be converted into a Spark SQL columns with the following\n// structure based on `recursive.fields.max.depth` value.\n1\n:\nstruct\n<\nname\n:\nstring\n>\n2\n:\nstruct\n<\nname\n:\nstring\n,\nbff\n:\nstruct\n<\nname\n:\nstring\n>>\n3\n:\nstruct\n<\nname\n:\nstring\n,\nbff\n:\nstruct\n<\nname\n:\nstring\n,\nbff\n:\nstruct\n<\nname\n:\nstring\n>>>\n...\n```\nData Source Option\nData source options of Protobuf can be set via:\nthe built-in functions below\nfrom_protobuf\nto_protobuf\nProperty Name\nDefault\nMeaning\nScope\nmode\nFAILFAST\nAllows a mode for dealing with corrupt records during parsing.\nPERMISSIVE\n: when it meets a corrupted record, sets all fields to\nnull\n.\nDROPMALFORMED\n: ignores the whole corrupted records. This mode is unsupported in the Protobuf built-in functions.\nFAILFAST\n: throws an exception when it meets corrupted records.\nread\nrecursive.fields.max.depth\n-1\nSpecifies the maximum number of recursion levels to allow when parsing the schema. For more details refers to the section\nHandling circular references protobuf fields\n.\nread\nconvert.any.fields.to.json\nfalse\nEnables converting Protobuf\nAny\nfields to JSON. This option should be enabled carefully. JSON conversion and processing are inefficient. In addition, schema safety is also reduced making downstream processing error-prone.\nread\nemit.default.values\nfalse\nWhether to render fields with zero values when deserializing Protobuf to a Spark struct. When a field is empty in the serialized Protobuf, this library will deserialize them as\nnull\nby default, this option can control whether to render the type-specific zero values.\nread\nenums.as.ints\nfalse\nWhether to render enum fields as their integer values. When this option set to\nfalse\n, an enum field will be mapped to\nStringType\n, and the value is the name of enum; when set to\ntrue\n, an enum field will be mapped to\nIntegerType\n, the value is its integer value.\nread\nupcast.unsigned.ints\nfalse\nWhether to upcast unsigned integers into a larger type. Setting this option to\ntrue\n,\nLongType\nis used for\nuint32\nand\nDecimal(20, 0)\nis used for\nuint64\n, so their representation can contain large unsigned values without overflow.\nread\nunwrap.primitive.wrapper.types\nfalse\nWhether to unwrap the struct representation for well-known primitive wrapper types when deserializing. By default, the wrapper types for primitives (i.e. google.protobuf.Int32Value, google.protobuf.Int64Value, etc.) will get deserialized as structs.\nread\nretain.empty.message.types\nfalse\nWhether to retain fields of the empty proto message type in Schema. Since Spark doesn't allow writing empty\nStructType\n, the empty proto message type will be dropped by default. Setting this option to\ntrue\nwill insert a dummy column(\n__dummy_field_in_empty_struct\n) to the empty proto message so that the empty message fields will be retained.\nread"}
{"url": "https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTroubleshooting\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nJDBC To Other Databases\nData Source Option\nData Type Mapping\nMapping Spark SQL Data Types from MySQL\nMapping Spark SQL Data Types to MySQL\nMapping Spark SQL Data Types from PostgreSQL\nMapping Spark SQL Data Types to PostgreSQL\nMapping Spark SQL Data Types from Oracle\nMapping Spark SQL Data Types to Oracle\nMapping Spark SQL Data Types from Microsoft SQL Server\nMapping Spark SQL Data Types to Microsoft SQL Server\nMapping Spark SQL Data Types from DB2\nMapping Spark SQL Data Types to DB2\nMapping Spark SQL Data Types from Teradata\nMapping Spark SQL Data Types to Teradata\nSpark SQL also includes a data source that can read data from other databases using JDBC. This\nfunctionality should be preferred over using\nJdbcRDD\n.\nThis is because the results are returned\nas a DataFrame and they can easily be processed in Spark SQL or joined with other data sources.\nThe JDBC data source is also easier to use from Java or Python as it does not require the user to\nprovide a ClassTag.\n(Note that this is different than the Spark SQL JDBC server, which allows other applications to\nrun queries using Spark SQL).\nTo get started you will need to include the JDBC driver for your particular database on the\nspark classpath. For example, to connect to postgres from the Spark Shell you would run the\nfollowing command:\n./bin/spark-shell\n--driver-class-path\npostgresql-9.4.1207.jar\n--jars\npostgresql-9.4.1207.jar\nData Source Option\nSpark supports the following case-insensitive options for JDBC. The Data source options of JDBC can be set via:\nthe\n.option\n/\n.options\nmethods of\nDataFrameReader\nDataFrameWriter\nOPTIONS\nclause at\nCREATE TABLE USING DATA_SOURCE\nFor connection properties, users can specify the JDBC connection properties in the data source options.\nuser\nand\npassword\nare normally provided as connection properties for\nlogging into the data sources.\nProperty Name\nDefault\nMeaning\nScope\nurl\n(none)\nThe JDBC URL of the form\njdbc:subprotocol:subname\nto connect to. The source-specific connection properties may be specified in the URL. e.g.,\njdbc:postgresql://localhost/test?user=fred&password=secret\nread/write\ndbtable\n(none)\nThe JDBC table that should be read from or written into. Note that when using it in the read\n      path anything that is valid in a\nFROM\nclause of a SQL query can be used.\n      For example, instead of a full table you could also use a subquery in parentheses. It is not\n      allowed to specify\ndbtable\nand\nquery\noptions at the same time.\nread/write\nquery\n(none)\nA query that will be used to read data into Spark. The specified query will be parenthesized and used\n      as a subquery in the\nFROM\nclause. Spark will also assign an alias to the subquery clause.\n      As an example, spark will issue a query of the following form to the JDBC Source.\nSELECT <columns> FROM (<user_specified_query>) spark_gen_alias\nBelow are a couple of restrictions while using this option.\nIt is not allowed to specify\ndbtable\nand\nquery\noptions at the same time.\nIt is not allowed to specify\nquery\nand\npartitionColumn\noptions at the same time. When specifying\npartitionColumn\noption is required, the subquery can be specified using\ndbtable\noption instead and\n            partition columns can be qualified using the subquery alias provided as part of\ndbtable\n.\nExample:\nspark.read.format(\"jdbc\")\n.option(\"url\", jdbcUrl)\n.option(\"query\", \"select c1, c2 from t1\")\n.load()\nread\nprepareQuery\n(none)\nA prefix that will form the final query together with\nquery\n.\n      As the specified\nquery\nwill be parenthesized as a subquery in the\nFROM\nclause and some databases do not\n      support all clauses in subqueries, the\nprepareQuery\nproperty offers a way to run such complex queries.\n      As an example, spark will issue a query of the following form to the JDBC Source.\n<prepareQuery> SELECT <columns> FROM (<user_specified_query>) spark_gen_alias\nBelow are a couple of examples.\nMSSQL Server does not accept\nWITH\nclauses in subqueries but it is possible to split such a query to\nprepareQuery\nand\nquery\n:\nspark.read.format(\"jdbc\")\n.option(\"url\", jdbcUrl)\n.option(\"prepareQuery\", \"WITH t AS (SELECT x, y FROM tbl)\")\n.option(\"query\", \"SELECT * FROM t WHERE x > 10\")\n.load()\nMSSQL Server does not accept temp table clauses in subqueries but it is possible to split such a query to\nprepareQuery\nand\nquery\n:\nspark.read.format(\"jdbc\")\n.option(\"url\", jdbcUrl)\n.option(\"prepareQuery\", \"(SELECT * INTO #TempTable FROM (SELECT * FROM tbl) t)\")\n.option(\"query\", \"SELECT * FROM #TempTable\")\n.load()\nread/write\ndriver\n(none)\nThe class name of the JDBC driver to use to connect to this URL.\nread/write\npartitionColumn, lowerBound, upperBound\n(none)\nThese options must all be specified if any of them is specified. In addition,\nnumPartitions\nmust be specified. They describe how to partition the table when\n      reading in parallel from multiple workers.\npartitionColumn\nmust be a numeric, date, or timestamp column from the table in question.\n      Notice that\nlowerBound\nand\nupperBound\nare just used to decide the\n      partition stride, not for filtering the rows in table. So all rows in the table will be\n      partitioned and returned. This option applies only to reading.\nExample:\nspark.read.format(\"jdbc\")\n.option(\"url\", jdbcUrl)\n.option(\"dbtable\", \"(select c1, c2 from t1) as subq\")\n.option(\"partitionColumn\", \"c1\")\n.option(\"lowerBound\", \"1\")\n.option(\"upperBound\", \"100\")\n.option(\"numPartitions\", \"3\")\n.load()\nread\nnumPartitions\n(none)\nThe maximum number of partitions that can be used for parallelism in table reading and\n      writing. This also determines the maximum number of concurrent JDBC connections.\n      If the number of partitions to write exceeds this limit, we decrease it to this limit by\n      calling\ncoalesce(numPartitions)\nbefore writing.\nread/write\nqueryTimeout\n0\nThe number of seconds the driver will wait for a Statement object to execute to the given\n      number of seconds. Zero means there is no limit. In the write path, this option depends on\n      how JDBC drivers implement the API\nsetQueryTimeout\n, e.g., the h2 JDBC driver\n      checks the timeout of each query instead of an entire JDBC batch.\nread/write\nfetchsize\n0\nThe JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (e.g. Oracle with 10 rows).\nread\nbatchsize\n1000\nThe JDBC batch size, which determines how many rows to insert per round trip. This can help performance on JDBC drivers. This option applies only to writing.\nwrite\nisolationLevel\nREAD_UNCOMMITTED\nThe transaction isolation level, which applies to current connection. It can be one of\nNONE\n,\nREAD_COMMITTED\n,\nREAD_UNCOMMITTED\n,\nREPEATABLE_READ\n, or\nSERIALIZABLE\n, corresponding to standard transaction isolation levels defined by JDBC's Connection object, with default of\nREAD_UNCOMMITTED\n. Please refer the documentation in\njava.sql.Connection\n.\nwrite\nsessionInitStatement\n(none)\nAfter each database session is opened to the remote DB and before starting to read data, this option executes a custom SQL statement (or a PL/SQL block). Use this to implement session initialization code. Example:\noption(\"sessionInitStatement\", \"\"\"BEGIN execute immediate 'alter session set \"_serial_direct_read\"=true'; END;\"\"\")\nread\ntruncate\nfalse\nThis is a JDBC writer related option. When\nSaveMode.Overwrite\nis enabled, this option causes Spark to truncate an existing table instead of dropping and recreating it. This can be more efficient, and prevents the table metadata (e.g., indices) from being removed. However, it will not work in some cases, such as when the new data has a different schema. In case of failures, users should turn off\ntruncate\noption to use\nDROP TABLE\nagain. Also, due to the different behavior of\nTRUNCATE TABLE\namong DBMSes, it's not always safe to use this. MySQLDialect, DB2Dialect, MsSqlServerDialect, DerbyDialect, and OracleDialect supports this while PostgresDialect and default JDBCDialect doesn't. For unknown and unsupported JDBCDialect, the user option\ntruncate\nis ignored.\nwrite\ncascadeTruncate\nthe default cascading truncate behaviour of the JDBC database in question, specified in the\nisCascadeTruncate\nin each JDBCDialect\nThis is a JDBC writer related option. If enabled and supported by the JDBC database (PostgreSQL and Oracle at the moment), this options allows execution of a\nTRUNCATE TABLE t CASCADE\n(in the case of PostgreSQL a\nTRUNCATE TABLE ONLY t CASCADE\nis executed to prevent inadvertently truncating descendant tables). This will affect other tables, and thus should be used with care.\nwrite\ncreateTableOptions\nThis is a JDBC writer related option. If specified, this option allows setting of database-specific table and partition options when creating a table (e.g.,\nCREATE TABLE t (name string) ENGINE=InnoDB.\n).\nwrite\ncreateTableColumnTypes\n(none)\nThe database column data types to use instead of the defaults, when creating the table. Data type information should be specified in the same format as CREATE TABLE columns syntax (e.g:\n\"name CHAR(64), comments VARCHAR(1024)\")\n. The specified types should be valid spark sql data types.\nwrite\ncustomSchema\n(none)\nThe custom schema to use for reading data from JDBC connectors. For example,\n\"id DECIMAL(38, 0), name STRING\"\n. You can also specify partial fields, and the others use the default type mapping. For example,\n\"id DECIMAL(38, 0)\"\n. The column names should be identical to the corresponding column names of JDBC table. Users can specify the corresponding data types of Spark SQL instead of using the defaults.\nread\npushDownPredicate\ntrue\nThe option to enable or disable predicate push-down into the JDBC data source. The default value is true, in which case Spark will push down filters to the JDBC data source as much as possible. Otherwise, if set to false, no filter will be pushed down to the JDBC data source and thus all filters will be handled by Spark. Predicate push-down is usually turned off when the predicate filtering is performed faster by Spark than by the JDBC data source.\nread\npushDownAggregate\ntrue\nThe option to enable or disable aggregate push-down in V2 JDBC data source. The default value is true, in which case Spark will push down aggregates to the JDBC data source. Otherwise, if sets to false, aggregates will not be pushed down to the JDBC data source. Aggregate push-down is usually turned off when the aggregate is performed faster by Spark than by the JDBC data source. Please note that aggregates can be pushed down if and only if all the aggregate functions and the related filters can be pushed down. If\nnumPartitions\nequals to 1 or the group by key is the same as\npartitionColumn\n, Spark will push down aggregate to data source completely and not apply a final aggregate over the data source output. Otherwise, Spark will apply a final aggregate over the data source output.\nread\npushDownLimit\ntrue\nThe option to enable or disable LIMIT push-down into V2 JDBC data source. The LIMIT push-down also includes LIMIT + SORT , a.k.a. the Top N operator. The default value is true, in which case Spark push down LIMIT or LIMIT with SORT to the JDBC data source. Otherwise, if sets to false, LIMIT or LIMIT with SORT is not pushed down to the JDBC data source. If\nnumPartitions\nis greater than 1, Spark still applies LIMIT or LIMIT with SORT on the result from data source even if LIMIT or LIMIT with SORT is pushed down. Otherwise, if LIMIT or LIMIT with SORT is pushed down and\nnumPartitions\nequals to 1, Spark will not apply LIMIT or LIMIT with SORT on the result from data source.\nread\npushDownOffset\ntrue\nThe option to enable or disable OFFSET push-down into V2 JDBC data source. The default value is true, in which case Spark will push down OFFSET to the JDBC data source. Otherwise, if sets to false, Spark will not try to push down OFFSET to the JDBC data source. If\npushDownOffset\nis true and\nnumPartitions\nis equal to 1, OFFSET will be pushed down to the JDBC data source. Otherwise, OFFSET will not be pushed down and Spark still applies OFFSET on the result from data source.\nread\npushDownTableSample\ntrue\nThe option to enable or disable TABLESAMPLE push-down into V2 JDBC data source. The default value is true, in which case Spark push down TABLESAMPLE to the JDBC data source. Otherwise, if value sets to false, TABLESAMPLE is not pushed down to the JDBC data source.\nread\nkeytab\n(none)\nLocation of the kerberos keytab file (which must be pre-uploaded to all nodes either by\n--files\noption of spark-submit or manually) for the JDBC client. When path information found then Spark considers the keytab distributed manually, otherwise\n--files\nassumed. If both\nkeytab\nand\nprincipal\nare defined then Spark tries to do kerberos authentication.\nread/write\nprincipal\n(none)\nSpecifies kerberos principal name for the JDBC client. If both\nkeytab\nand\nprincipal\nare defined then Spark tries to do kerberos authentication.\nread/write\nrefreshKrb5Config\nfalse\nThis option controls whether the kerberos configuration is to be refreshed or not for the JDBC client before\n      establishing a new connection. Set to true if you want to refresh the configuration, otherwise set to false.\n      The default value is false. Note that if you set this option to true and try to establish multiple connections,\n      a race condition can occur. One possible situation would be like as follows.\nrefreshKrb5Config flag is set with security context 1\nA JDBC connection provider is used for the corresponding DBMS\nThe krb5.conf is modified but the JVM not yet realized that it must be reloaded\nSpark authenticates successfully for security context 1\nThe JVM loads security context 2 from the modified krb5.conf\nSpark restores the previously saved security context 1\nThe modified krb5.conf content just gone\nread/write\nconnectionProvider\n(none)\nThe name of the JDBC connection provider to use to connect to this URL, e.g.\ndb2\n,\nmssql\n.\n      Must be one of the providers loaded with the JDBC data source. Used to disambiguate when more than one provider can handle\n      the specified driver and options. The selected provider must not be disabled by\nspark.sql.sources.disabledJdbcConnProviderList\n.\nread/write\npreferTimestampNTZ\nfalse\nWhen the option is set to\ntrue\n, TIMESTAMP WITHOUT TIME ZONE type is inferred as Spark's TimestampNTZ type.\n      Otherwise, it is interpreted as Spark's Timestamp type(equivalent to TIMESTAMP WITH LOCAL TIME ZONE).\n      This setting specifically affects only the inference of TIMESTAMP WITHOUT TIME ZONE data type. Both TIMESTAMP WITH LOCAL TIME ZONE and TIMESTAMP WITH TIME ZONE data types are consistently interpreted as Spark's Timestamp type regardless of this setting.\nread\nhint\n(none)\nThis option is used to specify the hint for reading. The supported hint format is a variant of C-style comments: it needs to start with `/*+ ` and end with ` */`. Currently, this option is only supported in MySQLDialect, OracleDialect and DatabricksDialect.\nread\nNote that kerberos authentication with keytab is not always supported by the JDBC driver.\nBefore using\nkeytab\nand\nprincipal\nconfiguration options, please make sure the following requirements are met:\nThe included JDBC driver version supports kerberos authentication with keytab.\nThere is a built-in connection provider which supports the used database.\nThere is a built-in connection providers for the following databases:\nDB2\nMariaDB\nMS Sql\nOracle\nPostgreSQL\nIf the requirements are not met, please consider using the\nJdbcConnectionProvider\ndeveloper API to handle custom authentication.\n# Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods\n# Loading data from a JDBC source\njdbcDF\n=\nspark\n.\nread\n\\\n.\nformat\n(\n\"\njdbc\n\"\n)\n\\\n.\noption\n(\n\"\nurl\n\"\n,\n\"\njdbc:postgresql:dbserver\n\"\n)\n\\\n.\noption\n(\n\"\ndbtable\n\"\n,\n\"\nschema.tablename\n\"\n)\n\\\n.\noption\n(\n\"\nuser\n\"\n,\n\"\nusername\n\"\n)\n\\\n.\noption\n(\n\"\npassword\n\"\n,\n\"\npassword\n\"\n)\n\\\n.\nload\n()\njdbcDF2\n=\nspark\n.\nread\n\\\n.\njdbc\n(\n\"\njdbc:postgresql:dbserver\n\"\n,\n\"\nschema.tablename\n\"\n,\nproperties\n=\n{\n\"\nuser\n\"\n:\n\"\nusername\n\"\n,\n\"\npassword\n\"\n:\n\"\npassword\n\"\n})\n# Specifying dataframe column data types on read\njdbcDF3\n=\nspark\n.\nread\n\\\n.\nformat\n(\n\"\njdbc\n\"\n)\n\\\n.\noption\n(\n\"\nurl\n\"\n,\n\"\njdbc:postgresql:dbserver\n\"\n)\n\\\n.\noption\n(\n\"\ndbtable\n\"\n,\n\"\nschema.tablename\n\"\n)\n\\\n.\noption\n(\n\"\nuser\n\"\n,\n\"\nusername\n\"\n)\n\\\n.\noption\n(\n\"\npassword\n\"\n,\n\"\npassword\n\"\n)\n\\\n.\noption\n(\n\"\ncustomSchema\n\"\n,\n\"\nid DECIMAL(38, 0), name STRING\n\"\n)\n\\\n.\nload\n()\n# Saving data to a JDBC source\njdbcDF\n.\nwrite\n\\\n.\nformat\n(\n\"\njdbc\n\"\n)\n\\\n.\noption\n(\n\"\nurl\n\"\n,\n\"\njdbc:postgresql:dbserver\n\"\n)\n\\\n.\noption\n(\n\"\ndbtable\n\"\n,\n\"\nschema.tablename\n\"\n)\n\\\n.\noption\n(\n\"\nuser\n\"\n,\n\"\nusername\n\"\n)\n\\\n.\noption\n(\n\"\npassword\n\"\n,\n\"\npassword\n\"\n)\n\\\n.\nsave\n()\njdbcDF2\n.\nwrite\n\\\n.\njdbc\n(\n\"\njdbc:postgresql:dbserver\n\"\n,\n\"\nschema.tablename\n\"\n,\nproperties\n=\n{\n\"\nuser\n\"\n:\n\"\nusername\n\"\n,\n\"\npassword\n\"\n:\n\"\npassword\n\"\n})\n# Specifying create table column data types on write\njdbcDF\n.\nwrite\n\\\n.\noption\n(\n\"\ncreateTableColumnTypes\n\"\n,\n\"\nname CHAR(64), comments VARCHAR(1024)\n\"\n)\n\\\n.\njdbc\n(\n\"\njdbc:postgresql:dbserver\n\"\n,\n\"\nschema.tablename\n\"\n,\nproperties\n=\n{\n\"\nuser\n\"\n:\n\"\nusername\n\"\n,\n\"\npassword\n\"\n:\n\"\npassword\n\"\n})\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\n// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods\n// Loading data from a JDBC source\nval\njdbcDF\n=\nspark\n.\nread\n.\nformat\n(\n\"jdbc\"\n)\n.\noption\n(\n\"url\"\n,\n\"jdbc:postgresql:dbserver\"\n)\n.\noption\n(\n\"dbtable\"\n,\n\"schema.tablename\"\n)\n.\noption\n(\n\"user\"\n,\n\"username\"\n)\n.\noption\n(\n\"password\"\n,\n\"password\"\n)\n.\nload\n()\nval\nconnectionProperties\n=\nnew\nProperties\n()\nconnectionProperties\n.\nput\n(\n\"user\"\n,\n\"username\"\n)\nconnectionProperties\n.\nput\n(\n\"password\"\n,\n\"password\"\n)\nval\njdbcDF2\n=\nspark\n.\nread\n.\njdbc\n(\n\"jdbc:postgresql:dbserver\"\n,\n\"schema.tablename\"\n,\nconnectionProperties\n)\n// Specifying the custom data types of the read schema\nconnectionProperties\n.\nput\n(\n\"customSchema\"\n,\n\"id DECIMAL(38, 0), name STRING\"\n)\nval\njdbcDF3\n=\nspark\n.\nread\n.\njdbc\n(\n\"jdbc:postgresql:dbserver\"\n,\n\"schema.tablename\"\n,\nconnectionProperties\n)\n// Saving data to a JDBC source\njdbcDF\n.\nwrite\n.\nformat\n(\n\"jdbc\"\n)\n.\noption\n(\n\"url\"\n,\n\"jdbc:postgresql:dbserver\"\n)\n.\noption\n(\n\"dbtable\"\n,\n\"schema.tablename\"\n)\n.\noption\n(\n\"user\"\n,\n\"username\"\n)\n.\noption\n(\n\"password\"\n,\n\"password\"\n)\n.\nsave\n()\njdbcDF2\n.\nwrite\n.\njdbc\n(\n\"jdbc:postgresql:dbserver\"\n,\n\"schema.tablename\"\n,\nconnectionProperties\n)\n// Specifying create table column data types on write\njdbcDF\n.\nwrite\n.\noption\n(\n\"createTableColumnTypes\"\n,\n\"name CHAR(64), comments VARCHAR(1024)\"\n)\n.\njdbc\n(\n\"jdbc:postgresql:dbserver\"\n,\n\"schema.tablename\"\n,\nconnectionProperties\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\n// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods\n// Loading data from a JDBC source\nDataset\n<\nRow\n>\njdbcDF\n=\nspark\n.\nread\n()\n.\nformat\n(\n\"jdbc\"\n)\n.\noption\n(\n\"url\"\n,\n\"jdbc:postgresql:dbserver\"\n)\n.\noption\n(\n\"dbtable\"\n,\n\"schema.tablename\"\n)\n.\noption\n(\n\"user\"\n,\n\"username\"\n)\n.\noption\n(\n\"password\"\n,\n\"password\"\n)\n.\nload\n();\nProperties\nconnectionProperties\n=\nnew\nProperties\n();\nconnectionProperties\n.\nput\n(\n\"user\"\n,\n\"username\"\n);\nconnectionProperties\n.\nput\n(\n\"password\"\n,\n\"password\"\n);\nDataset\n<\nRow\n>\njdbcDF2\n=\nspark\n.\nread\n()\n.\njdbc\n(\n\"jdbc:postgresql:dbserver\"\n,\n\"schema.tablename\"\n,\nconnectionProperties\n);\n// Saving data to a JDBC source\njdbcDF\n.\nwrite\n()\n.\nformat\n(\n\"jdbc\"\n)\n.\noption\n(\n\"url\"\n,\n\"jdbc:postgresql:dbserver\"\n)\n.\noption\n(\n\"dbtable\"\n,\n\"schema.tablename\"\n)\n.\noption\n(\n\"user\"\n,\n\"username\"\n)\n.\noption\n(\n\"password\"\n,\n\"password\"\n)\n.\nsave\n();\njdbcDF2\n.\nwrite\n()\n.\njdbc\n(\n\"jdbc:postgresql:dbserver\"\n,\n\"schema.tablename\"\n,\nconnectionProperties\n);\n// Specifying create table column data types on write\njdbcDF\n.\nwrite\n()\n.\noption\n(\n\"createTableColumnTypes\"\n,\n\"name CHAR(64), comments VARCHAR(1024)\"\n)\n.\njdbc\n(\n\"jdbc:postgresql:dbserver\"\n,\n\"schema.tablename\"\n,\nconnectionProperties\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\n# Loading data from a JDBC source\ndf\n<-\nread.jdbc\n(\n\"jdbc:postgresql:dbserver\"\n,\n\"schema.tablename\"\n,\nuser\n=\n\"username\"\n,\npassword\n=\n\"password\"\n)\n# Saving data to a JDBC source\nwrite.jdbc\n(\ndf\n,\n\"jdbc:postgresql:dbserver\"\n,\n\"schema.tablename\"\n,\nuser\n=\n\"username\"\n,\npassword\n=\n\"password\"\n)\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nCREATE\nTEMPORARY\nVIEW\njdbcTable\nUSING\norg\n.\napache\n.\nspark\n.\nsql\n.\njdbc\nOPTIONS\n(\nurl\n\"jdbc:postgresql:dbserver\"\n,\ndbtable\n\"schema.tablename\"\n,\nuser\n'username'\n,\npassword\n'password'\n)\nINSERT\nINTO\nTABLE\njdbcTable\nSELECT\n*\nFROM\nresultTable\nData Type Mapping\nMapping Spark SQL Data Types from MySQL\nThe below table describes the data type conversions from MySQL data types to Spark SQL Data Types,\nwhen reading data from a MySQL table using the built-in jdbc data source with the MySQL Connector/J\nas the activated JDBC Driver. Note that, different JDBC drivers, such as Maria Connector/J, which\nare also available to connect MySQL, may have different mapping rules.\nMySQL Data Type\nSpark SQL Data Type\nRemarks\nBIT(1)\nBooleanType\nBIT( >1 )\nBinaryType\n(Default)\nBIT( >1 )\nLongType\nspark.sql.legacy.mysql.bitArrayMapping.enabled=true\nTINYINT(1)\nBooleanType\nTINYINT(1)\nByteType\ntinyInt1isBit=false\nBOOLEAN\nBooleanType\nBOOLEAN\nByteType\ntinyInt1isBit=false\nTINYINT( >1 )\nByteType\nTINYINT( any ) UNSIGNED\nShortType\nSMALLINT\nShortType\nSMALLINT UNSIGNED\nIntegerType\nMEDIUMINT [UNSIGNED]\nIntegerType\nINT\nIntegerType\nINT UNSIGNED\nLongType\nBIGINT\nLongType\nBIGINT UNSIGNED\nDecimalType(20,0)\nFLOAT\nFloatType\nFLOAT UNSIGNED\nDoubleType\nDOUBLE [UNSIGNED]\nDoubleType\nDECIMAL(p,s) [UNSIGNED]\nDecimalType(min(38, p),(min(18,s)))\nThe column type is bounded to DecimalType(38, 18), if 'p>38', the fraction part will be truncated if exceeded. And if any value of this column have an actual precision greater 38 will fail with NUMERIC_VALUE_OUT_OF_RANGE.WITHOUT_SUGGESTION error\nDATE\nDateType\nDATETIME\nTimestampType\n(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\nDATETIME\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTAMP_NTZ\nTIMESTAMP\nTimestampType\nTIME\nTimestampType\n(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\nTIME\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTAMP_NTZ\nYEAR\nDateType\nyearIsDateType=true\nYEAR\nIntegerType\nyearIsDateType=false\nCHAR(n)\nCharType(n)\nVARCHAR(n)\nVarcharType(n)\nBINARY(n)\nBinaryType\nVARBINARY(n)\nBinaryType\nCHAR(n) BINARY\nBinaryType\nVARCHAR(n) BINARY\nBinaryType\nBLOB\nBinaryType\nTINYBLOB\nBinaryType\nMEDIUMBLOB\nBinaryType\nLONGBLOB\nBinaryType\nTEXT\nStringType\nTINYTEXT\nStringType\nMEDIUMTEXT\nStringType\nLONGTEXT\nStringType\nJSON\nStringType\nGEOMETRY\nBinaryType\nENUM\nCharType(n)\nSET\nCharType(n)\nMapping Spark SQL Data Types to MySQL\nThe below table describes the data type conversions from Spark SQL Data Types to MySQL data types,\nwhen creating, altering, or writing data to a MySQL table using the built-in jdbc data source with\nthe MySQL Connector/J as the activated JDBC Driver.\nNote that, different JDBC drivers, such as Maria Connector/J, which are also available to connect MySQL,\nmay have different mapping rules.\nSpark SQL Data Type\nMySQL Data Type\nRemarks\nBooleanType\nBIT(1)\nByteType\nTINYINT\nShortType\nSMALLINT\nFor Spark 3.5 and previous, it maps to INTEGER\nIntegerType\nINTEGER\nLongType\nBIGINT\nFloatType\nFLOAT\nDoubleType\nDOUBLE PRECISION\nDecimalType(p, s)\nDECIMAL(p,s)\nDateType\nDATE\nTimestampType\nTIMESTAMP\nTimestampNTZType\nDATETIME\nStringType\nLONGTEXT\nBinaryType\nBLOB\nCharType(n)\nCHAR(n)\nVarcharType(n)\nVARCHAR(n)\nThe Spark Catalyst data types below are not supported with suitable MYSQL types.\nDayTimeIntervalType\nYearMonthIntervalType\nCalendarIntervalType\nArrayType\nMapType\nStructType\nUserDefinedType\nNullType\nObjectType\nVariantType\nMapping Spark SQL Data Types from PostgreSQL\nThe below table describes the data type conversions from PostgreSQL data types to Spark SQL Data Types,\nwhen reading data from a Postgres table using the built-in jdbc data source with the\nPostgreSQL JDBC Driver\nas the activated JDBC Driver. Note that, different JDBC drivers, or different versions might result slightly different.\nPostgreSQL Data Type\nSpark SQL Data Type\nRemarks\nboolean\nBooleanType\nsmallint, smallserial\nShortType\ninteger, serial\nIntegerType\nbigint, bigserial\nLongType\nfloat, float(p),  real\nFloatType\n1 ≤ p ≤ 24\nfloat(p)\nDoubleType\n25 ≤ p ≤ 53\ndouble precision\nDoubleType\nnumeric, decimal\nDecimalType\nSince PostgreSQL 15, 's' can be negative. If 's<0' it'll be adjusted to DecimalType(min(p-s, 38), 0); Otherwise, DecimalType(p, s)\nIf 'p>38', the fraction part will be truncated if exceeded. And if any value of this column have an actual precision greater 38 will fail with NUMERIC_VALUE_OUT_OF_RANGE.WITHOUT_SUGGESTION error.\nSpecial numeric values, 'NaN', 'infinity' and '-infinity' is not supported\ncharacter varying(n), varchar(n)\nVarcharType(n)\ncharacter(n), char(n), bpchar(n)\nCharType(n)\nbpchar\nStringType\ntext\nStringType\nbytea\nBinaryType\ndate\nDateType\ntimestamp [ (p) ] [ without time zone ]\nTimestampType\n(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\ntimestamp [ (p) ] [ without time zone ]\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTAMP_NTZ\ntimestamp [ (p) ] with time zone\nTimestampType\ntime [ (p) ] [ without time zone ]\nTimestampType\n(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\ntime [ (p) ] [ without time zone ]\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTAMP_NTZ\ntime [ (p) ] with time zone\nTimestampType\ninterval [ fields ] [ (p) ]\nStringType\nENUM\nStringType\nmoney\nStringType\nMonetary Types\ninet, cidr, macaddr, macaddr8\nStringType\nNetwork Address Types\npoint, line, lseg, box, path, polygon, circle\nStringType\nGeometric Types\npg_lsn\nStringType\nLog Sequence Number\nbit, bit(1)\nBooleanType\nbit( >1 )\nBinaryType\nbit varying( any )\nBinaryType\ntsvector, tsquery\nStringType\nText Search Types\nuuid\nStringType\nUniversally Unique Identifier Type\nxml\nStringType\nXML Type\njson, jsonb\nStringType\nJSON Types\narray\nArrayType\nComposite Types\nStringType\nTypes created by CREATE TYPE syntax.\nint4range, int8range, numrange, tsrange, tstzrange, daterange, etc\nStringType\nRange Types\nDomain Types\n(Decided by the underlying type)\noid\nDecimalType(20, 0)\nObject Identifier Types\nregxxx\nStringType\nObject Identifier Types\nvoid\nNullType\nvoid is a Postgres pseudo type, other pseudo types have not yet been verified\nMapping Spark SQL Data Types to PostgreSQL\nThe below table describes the data type conversions from Spark SQL Data Types to PostgreSQL data types,\nwhen creating, altering, or writing data to a PostgreSQL table using the built-in jdbc data source with\nthe\nPostgreSQL JDBC Driver\nas the activated JDBC Driver.\nSpark SQL Data Type\nPostgreSQL Data Type\nRemarks\nBooleanType\nboolean\nByteType\nsmallint\nShortType\nsmallint\nIntegerType\ninteger\nLongType\nbigint\nFloatType\nfloat4\nDoubleType\nfloat8\nDecimalType(p, s)\nnumeric(p,s)\nDateType\ndate\nTimestampType\ntimestamp with time zone\nBefore Spark 4.0, it was mapped as timestamp. Please refer to the migration guide for more information\nTimestampNTZType\ntimestamp\nStringType\ntext\nBinaryType\nbytea\nCharType(n)\nCHAR(n)\nVarcharType(n)\nVARCHAR(n)\nArrayType\nElement type\nPG Array\nBooleanType\nboolean[]\nByteType\nsmallint[]\nShortType\nsmallint[]\nIntegerType\ninteger[]\nLongType\nbigint[]\nFloatType\nfloat4[]\nDoubleType\nfloat8[]\nDecimalType(p, s)\nnumeric(p,s)[]\nDateType\ndate[]\nTimestampType\ntimestamp[]\nTimestampNTZType\ntimestamp[]\nStringType\ntext[]\nBinaryType\nbytea[]\nCharType(n)\nchar(n)[]\nVarcharType(n)\nvarchar(n)[]\nIf the element type is an ArrayType, it converts to Postgres multidimensional array.\nFor instance,\nArrayType(ArrayType(StringType))\nconverts to\ntext[][]\n,\nArrayType(ArrayType(ArrayType(LongType)))\nconverts to\nbigint[][][]\nThe Spark Catalyst data types below are not supported with suitable PostgreSQL types.\nDayTimeIntervalType\nYearMonthIntervalType\nCalendarIntervalType\nArrayType - if the element type is not listed above\nMapType\nStructType\nUserDefinedType\nNullType\nObjectType\nVariantType\nMapping Spark SQL Data Types from Oracle\nThe below table describes the data type conversions from Oracle data types to Spark SQL Data Types,\nwhen reading data from an Oracle table using the built-in jdbc data source with the Oracle JDBC\nas the activated JDBC Driver.\nOracle Data Type\nSpark SQL Data Type\nRemarks\nBOOLEAN\nBooleanType\nIntroduced since Oracle Release 23c\nNUMBER[(p[,s])]\nDecimalType(p,s)\n's' can be negative in Oracle. If 's<0' it'll be adjusted to DecimalType(min(p-s, 38), 0); Otherwise, DecimalType(p, s), and if 'p>38', the fraction part will be truncated if exceeded. And if any value of this column have an actual precision greater 38 will fail with NUMERIC_VALUE_OUT_OF_RANGE.WITHOUT_SUGGESTION error\nFLOAT[(p)]\nDecimalType(38, 10)\nBINARY_FLOAT\nFloatType\nBINARY_DOUBLE\nDoubleType\nLONG\nBinaryType\nRAW(size)\nBinaryType\nLONG RAW\nBinaryType\nDATE\nTimestampType\nWhen oracle.jdbc.mapDateToTimestamp=true, it follows TIMESTAMP's behavior below\nDATE\nDateType\nWhen oracle.jdbc.mapDateToTimestamp=false, it maps to DateType\nTIMESTAMP\nTimestampType\n(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\nTIMESTAMP\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTAMP_NTZ\nTIMESTAMP WITH TIME ZONE\nTimestampType\nTIMESTAMP WITH LOCAL TIME ZONE\nTimestampType\nINTERVAL YEAR TO MONTH\nYearMonthIntervalType\nINTERVAL DAY TO SECOND\nDayTimeIntervalType\nCHAR[(size [BYTE | CHAR])]\nCharType(size)\nNCHAR[(size)]\nStringType\nVARCHAR2(size [BYTE | CHAR])\nVarcharType(size)\nNVARCHAR2\nStringType\nROWID/UROWID\nStringType\nCLOB\nStringType\nNCLOB\nStringType\nBLOB\nBinaryType\nBFILE\nUNRECOGNIZED_SQL_TYPE error raised\nMapping Spark SQL Data Types to Oracle\nThe below table describes the data type conversions from Spark SQL Data Types to Oracle data types,\nwhen creating, altering, or writing data to an Oracle table using the built-in jdbc data source with\nthe Oracle JDBC as the activated JDBC Driver.\nSpark SQL Data Type\nOracle Data Type\nRemarks\nBooleanType\nNUMBER(1, 0)\nBooleanType maps to NUMBER(1, 0) as BOOLEAN is introduced since Oracle Release 23c\nByteType\nNUMBER(3)\nShortType\nNUMBER(5)\nIntegerType\nNUMBER(10)\nLongType\nNUMBER(19)\nFloatType\nNUMBER(19, 4)\nDoubleType\nNUMBER(19, 4)\nDecimalType(p, s)\nNUMBER(p,s)\nDateType\nDATE\nTimestampType\nTIMESTAMP WITH LOCAL TIME ZONE\nTimestampNTZType\nTIMESTAMP\nStringType\nVARCHAR2(255)\nFor historical reason, a string value has maximum 255 characters\nBinaryType\nBLOB\nCharType(n)\nCHAR(n)\nVarcharType(n)\nVARCHAR2(n)\nThe Spark Catalyst data types below are not supported with suitable Oracle types.\nDayTimeIntervalType\nYearMonthIntervalType\nCalendarIntervalType\nArrayType\nMapType\nStructType\nUserDefinedType\nNullType\nObjectType\nVariantType\nMapping Spark SQL Data Types from Microsoft SQL Server\nThe below table describes the data type conversions from Microsoft SQL Server data types to Spark SQL Data Types,\nwhen reading data from a Microsoft SQL Server table using the built-in jdbc data source with the mssql-jdbc\nas the activated JDBC Driver.\nSQL Server  Data Type\nSpark SQL Data Type\nRemarks\nbit\nBooleanType\ntinyint\nShortType\nsmallint\nShortType\nint\nIntegerType\nbigint\nLongType\nfloat(p), real\nFloatType\n1 ≤ p ≤ 24\nfloat[(p)]\nDoubleType\n25 ≤ p ≤ 53\ndouble precision\nDoubleType\nsmallmoney\nDecimalType(10, 4)\nmoney\nDecimalType(19, 4)\ndecimal[(p[, s])], numeric[(p[, s])]\nDecimalType(p, s)\ndate\nDateType\ndatetime\nTimestampType\n(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\ndatetime\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTAMP_NTZ\ndatetime2 [ (fractional seconds precision) ]\nTimestampType\n(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\ndatetime2 [ (fractional seconds precision) ]\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTAMP_NTZ\ndatetimeoffset [ (fractional seconds precision) ]\nTimestampType\nsmalldatetime\nTimestampType\n(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\nsmalldatetime\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTAMP_NTZ\ntime [ (fractional second scale) ]\nTimestampType\n(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\ntime [ (fractional second scale) ]\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTAMP_NTZ\nbinary [ ( n ) ]\nBinaryType\nvarbinary [ ( n | max ) ]\nBinaryType\nchar [ ( n ) ]\nCharType(n)\nvarchar [ ( n | max ) ]\nVarcharType(n)\nnchar [ ( n ) ]\nStringType\nnvarchar [ ( n | max ) ]\nStringType\ntext\nStringType\nntext\nStringType\nimage\nStringType\ngeography\nBinaryType\ngeometry\nBinaryType\nrowversion\nBinaryType\nsql_variant\nUNRECOGNIZED_SQL_TYPE error raised\nMapping Spark SQL Data Types to Microsoft SQL Server\nThe below table describes the data type conversions from Spark SQL Data Types to Microsoft SQL Server data types,\nwhen creating, altering, or writing data to a Microsoft SQL Server table using the built-in jdbc data source with\nthe mssql-jdbc as the activated JDBC Driver.\nSpark SQL Data Type\nSQL Server Data Type\nRemarks\nBooleanType\nbit\nByteType\nsmallint\nSupported since Spark 4.0.0, previous versions throw errors\nShortType\nsmallint\nIntegerType\nint\nLongType\nbigint\nFloatType\nreal\nDoubleType\ndouble precision\nDecimalType(p, s)\nnumber(p,s)\nDateType\ndate\nTimestampType\ndatetime\nTimestampNTZType\ndatetime\nStringType\nnvarchar(max)\nBinaryType\nvarbinary(max)\nCharType(n)\nchar(n)\nVarcharType(n)\nvarchar(n)\nThe Spark Catalyst data types below are not supported with suitable SQL Server types.\nDayTimeIntervalType\nYearMonthIntervalType\nCalendarIntervalType\nArrayType\nMapType\nStructType\nUserDefinedType\nNullType\nObjectType\nVariantType\nMapping Spark SQL Data Types from DB2\nThe below table describes the data type conversions from DB2 data types to Spark SQL Data Types,\nwhen reading data from a DB2 table using the built-in jdbc data source with the\nIBM Data Server Driver For JDBC and SQLJ\nas the activated JDBC Driver.\nDB2 Data Type\nSpark SQL Data Type\nRemarks\nBOOLEAN\nBinaryType\nSMALLINT\nShortType\nINTEGER\nIntegerType\nBIGINT\nLongType\nREAL\nFloatType\nDOUBLE, FLOAT\nDoubleType\nFLOAT is double precision floating-point in db2\nDECIMAL, NUMERIC, DECFLOAT\nDecimalType\nDATE\nDateType\nTIMESTAMP, TIMESTAMP WITHOUT TIME ZONE\nTimestampType\n(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\nTIMESTAMP, TIMESTAMP WITHOUT TIME ZONE\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTAMP_NTZ\nTIMESTAMP WITH TIME ZONE\nTimestampType\nTIME\nTimestampType\n(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\nTIME\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTAMP_NTZ\nCHAR(n)\nCharType(n)\nVARCHAR(n)\nVarcharType(n)\nCHAR(n) FOR BIT DATA\nBinaryType\nVARCHAR(n) FOR BIT DATA\nBinaryType\nBINARY(n)\nBinaryType\nVARBINARY(n)\nBinaryType\nCLOB(n)\nStringType\nDBCLOB(n)\nStringType\nBLOB(n)\nBinaryType\nGRAPHIC(n)\nStringType\nVARGRAPHIC(n)\nStringType\nXML\nStringType\nROWID\nStringType\nMapping Spark SQL Data Types to DB2\nThe below table describes the data type conversions from Spark SQL Data Types to DB2 data types,\nwhen creating, altering, or writing data to a DB2 table using the built-in jdbc data source with\nthe\nIBM Data Server Driver For JDBC and SQLJ\nas the activated JDBC Driver.\nSpark SQL Data Type\nDB2 Data Type\nRemarks\nBooleanType\nBOOLEAN\nByteType\nSMALLINT\nShortType\nSMALLINT\nIntegerType\nINTEGER\nLongType\nBIGINT\nFloatType\nREAL\nDoubleType\nDOUBLE PRECISION\nDecimalType(p, s)\nDECIMAL(p,s)\nThe maximum value for 'p' is 31 in DB2, while it is 38 in Spark. It might fail when storing DecimalType(p>=32, s) to DB2\nDateType\nDATE\nTimestampType\nTIMESTAMP\nTimestampNTZType\nTIMESTAMP\nStringType\nCLOB\nBinaryType\nBLOB\nCharType(n)\nCHAR(n)\nThe maximum value for 'n' is 255 in DB2, while it is unlimited in Spark.\nVarcharType(n)\nVARCHAR(n)\nThe maximum value for 'n' is 255 in DB2, while it is unlimited in Spark.\nThe Spark Catalyst data types below are not supported with suitable DB2 types.\nDayTimeIntervalType\nYearMonthIntervalType\nCalendarIntervalType\nArrayType\nMapType\nStructType\nUserDefinedType\nNullType\nObjectType\nVariantType\nMapping Spark SQL Data Types from Teradata\nThe below table describes the data type conversions from Teradata data types to Spark SQL Data Types,\nwhen reading data from a Teradata table using the built-in jdbc data source with the\nTeradata JDBC Driver\nas the activated JDBC Driver.\nTeradata Data Type\nSpark SQL Data Type\nRemarks\nBYTEINT\nByteType\nSMALLINT\nShortType\nINTEGER, INT\nIntegerType\nBIGINT\nLongType\nREAL, DOUBLE PRECISION, FLOAT\nDoubleType\nDECIMAL, NUMERIC, NUMBER\nDecimalType\nDATE\nDateType\nTIMESTAMP, TIMESTAMP WITH TIME ZONE\nTimestampType\n(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\nTIMESTAMP, TIMESTAMP WITH TIME ZONE\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTAMP_NTZ\nTIME, TIME WITH TIME ZONE\nTimestampType\n(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\nTIME, TIME WITH TIME ZONE\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTAMP_NTZ\nCHARACTER(n), CHAR(n), GRAPHIC(n)\nCharType(n)\nVARCHAR(n), VARGRAPHIC(n)\nVarcharType(n)\nBYTE(n), VARBYTE(n)\nBinaryType\nCLOB\nStringType\nBLOB\nBinaryType\nINTERVAL Data Types\n-\nThe INTERVAL data types are unknown yet\nPeriod Data Types, ARRAY, UDT\n-\nNot Supported\nMapping Spark SQL Data Types to Teradata\nThe below table describes the data type conversions from Spark SQL Data Types to Teradata data types,\nwhen creating, altering, or writing data to a Teradata table using the built-in jdbc data source with\nthe\nTeradata JDBC Driver\nas the activated JDBC Driver.\nSpark SQL Data Type\nTeradata Data Type\nRemarks\nBooleanType\nCHAR(1)\nByteType\nBYTEINT\nShortType\nSMALLINT\nIntegerType\nINTEGER\nLongType\nBIGINT\nFloatType\nREAL\nDoubleType\nDOUBLE PRECISION\nDecimalType(p, s)\nDECIMAL(p,s)\nDateType\nDATE\nTimestampType\nTIMESTAMP\nTimestampNTZType\nTIMESTAMP\nStringType\nVARCHAR(255)\nBinaryType\nBLOB\nCharType(n)\nCHAR(n)\nVarcharType(n)\nVARCHAR(n)\nThe Spark Catalyst data types below are not supported with suitable Teradata types.\nDayTimeIntervalType\nYearMonthIntervalType\nCalendarIntervalType\nArrayType\nMapType\nStructType\nUserDefinedType\nNullType\nObjectType\nVariantType"}
{"url": "https://spark.apache.org/docs/latest/mllib-naive-bayes.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nNaive Bayes - RDD-based API\nNaive Bayes\nis a simple\nmulticlass classification algorithm with the assumption of independence between\nevery pair of features. Naive Bayes can be trained very efficiently. Within a\nsingle pass to the training data, it computes the conditional probability\ndistribution of each feature given label, and then it applies Bayes’ theorem to\ncompute the conditional probability distribution of label given an observation\nand use it for prediction.\nspark.mllib\nsupports\nmultinomial naive\nBayes\nand\nBernoulli naive Bayes\n.\nThese models are typically used for\ndocument classification\n.\nWithin that context, each observation is a document and each\nfeature represents a term whose value is the frequency of the term (in multinomial naive Bayes) or\na zero or one indicating whether the term was found in the document (in Bernoulli naive Bayes).\nFeature values must be nonnegative. The model type is selected with an optional parameter\n“multinomial” or “bernoulli” with “multinomial” as the default.\nAdditive smoothing\ncan be used by\nsetting the parameter $\\lambda$ (default to $1.0$). For document classification, the input feature\nvectors are usually sparse, and sparse vectors should be supplied as input to take advantage of\nsparsity. Since the training data is only used once, it is not necessary to cache it.\nExamples\nNaiveBayes\nimplements multinomial\nnaive Bayes. It takes an RDD of\nLabeledPoint\nand an optionally\nsmoothing parameter\nlambda\nas input, and output a\nNaiveBayesModel\n, which can be\nused for evaluation and prediction.\nNote that the Python API does not yet support model save/load but will in the future.\nRefer to the\nNaiveBayes\nPython docs\nand\nNaiveBayesModel\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.classification\nimport\nNaiveBayes\n,\nNaiveBayesModel\nfrom\npyspark.mllib.util\nimport\nMLUtils\n# Load and parse the data file.\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\n# Split data approximately into training (60%) and test (40%)\ntraining\n,\ntest\n=\ndata\n.\nrandomSplit\n([\n0.6\n,\n0.4\n])\n# Train a naive Bayes model.\nmodel\n=\nNaiveBayes\n.\ntrain\n(\ntraining\n,\n1.0\n)\n# Make prediction and test accuracy.\npredictionAndLabel\n=\ntest\n.\nmap\n(\nlambda\np\n:\n(\nmodel\n.\npredict\n(\np\n.\nfeatures\n),\np\n.\nlabel\n))\naccuracy\n=\n1.0\n*\npredictionAndLabel\n.\nfilter\n(\nlambda\npl\n:\npl\n[\n0\n]\n==\npl\n[\n1\n]).\ncount\n()\n/\ntest\n.\ncount\n()\nprint\n(\n'\nmodel accuracy {}\n'\n.\nformat\n(\naccuracy\n))\n# Save and load model\noutput_dir\n=\n'\ntarget/tmp/myNaiveBayesModel\n'\nshutil\n.\nrmtree\n(\noutput_dir\n,\nignore_errors\n=\nTrue\n)\nmodel\n.\nsave\n(\nsc\n,\noutput_dir\n)\nsameModel\n=\nNaiveBayesModel\n.\nload\n(\nsc\n,\noutput_dir\n)\npredictionAndLabel\n=\ntest\n.\nmap\n(\nlambda\np\n:\n(\nsameModel\n.\npredict\n(\np\n.\nfeatures\n),\np\n.\nlabel\n))\naccuracy\n=\n1.0\n*\npredictionAndLabel\n.\nfilter\n(\nlambda\npl\n:\npl\n[\n0\n]\n==\npl\n[\n1\n]).\ncount\n()\n/\ntest\n.\ncount\n()\nprint\n(\n'\nsameModel accuracy {}\n'\n.\nformat\n(\naccuracy\n))\nFind full example code at \"examples/src/main/python/mllib/naive_bayes_example.py\" in the Spark repo.\nNaiveBayes\nimplements\nmultinomial naive Bayes. It takes an RDD of\nLabeledPoint\nand an optional\nsmoothing parameter\nlambda\nas input, an optional model type parameter (default is “multinomial”), and outputs a\nNaiveBayesModel\n, which\ncan be used for evaluation and prediction.\nRefer to the\nNaiveBayes\nScala docs\nand\nNaiveBayesModel\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.classification.\n{\nNaiveBayes\n,\nNaiveBayesModel\n}\nimport\norg.apache.spark.mllib.util.MLUtils\n// Load and parse the data file.\nval\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Split data into training (60%) and test (40%).\nval\nArray\n(\ntraining\n,\ntest\n)\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.6\n,\n0.4\n))\nval\nmodel\n=\nNaiveBayes\n.\ntrain\n(\ntraining\n,\nlambda\n=\n1.0\n,\nmodelType\n=\n\"multinomial\"\n)\nval\npredictionAndLabel\n=\ntest\n.\nmap\n(\np\n=>\n(\nmodel\n.\npredict\n(\np\n.\nfeatures\n),\np\n.\nlabel\n))\nval\naccuracy\n=\n1.0\n*\npredictionAndLabel\n.\nfilter\n(\nx\n=>\nx\n.\n_1\n==\nx\n.\n_2\n).\ncount\n()\n/\ntest\n.\ncount\n()\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/myNaiveBayesModel\"\n)\nval\nsameModel\n=\nNaiveBayesModel\n.\nload\n(\nsc\n,\n\"target/tmp/myNaiveBayesModel\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\" in the Spark repo.\nNaiveBayes\nimplements\nmultinomial naive Bayes. It takes a Scala RDD of\nLabeledPoint\nand an\noptionally smoothing parameter\nlambda\nas input, and output a\nNaiveBayesModel\n, which\ncan be used for evaluation and prediction.\nRefer to the\nNaiveBayes\nJava docs\nand\nNaiveBayesModel\nJava docs\nfor details on the API.\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.api.java.JavaPairRDD\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.api.java.JavaSparkContext\n;\nimport\norg.apache.spark.mllib.classification.NaiveBayes\n;\nimport\norg.apache.spark.mllib.classification.NaiveBayesModel\n;\nimport\norg.apache.spark.mllib.regression.LabeledPoint\n;\nimport\norg.apache.spark.mllib.util.MLUtils\n;\nString\npath\n=\n\"data/mllib/sample_libsvm_data.txt\"\n;\nJavaRDD\n<\nLabeledPoint\n>\ninputData\n=\nMLUtils\n.\nloadLibSVMFile\n(\njsc\n.\nsc\n(),\npath\n).\ntoJavaRDD\n();\nJavaRDD\n<\nLabeledPoint\n>[]\ntmp\n=\ninputData\n.\nrandomSplit\n(\nnew\ndouble\n[]{\n0.6\n,\n0.4\n});\nJavaRDD\n<\nLabeledPoint\n>\ntraining\n=\ntmp\n[\n0\n];\n// training set\nJavaRDD\n<\nLabeledPoint\n>\ntest\n=\ntmp\n[\n1\n];\n// test set\nNaiveBayesModel\nmodel\n=\nNaiveBayes\n.\ntrain\n(\ntraining\n.\nrdd\n(),\n1.0\n);\nJavaPairRDD\n<\nDouble\n,\nDouble\n>\npredictionAndLabel\n=\ntest\n.\nmapToPair\n(\np\n->\nnew\nTuple2\n<>(\nmodel\n.\npredict\n(\np\n.\nfeatures\n()),\np\n.\nlabel\n()));\ndouble\naccuracy\n=\npredictionAndLabel\n.\nfilter\n(\npl\n->\npl\n.\n_1\n().\nequals\n(\npl\n.\n_2\n())).\ncount\n()\n/\n(\ndouble\n)\ntest\n.\ncount\n();\n// Save and load model\nmodel\n.\nsave\n(\njsc\n.\nsc\n(),\n\"target/tmp/myNaiveBayesModel\"\n);\nNaiveBayesModel\nsameModel\n=\nNaiveBayesModel\n.\nload\n(\njsc\n.\nsc\n(),\n\"target/tmp/myNaiveBayesModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\" in the Spark repo."}
{"url": "https://spark.apache.org/docs/latest/sql-migration-guide.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nMigration Guide: SQL, Datasets and DataFrame\nUpgrading from Spark SQL 3.5 to 4.0\nUpgrading from Spark SQL 3.5.3 to 3.5.4\nUpgrading from Spark SQL 3.5.1 to 3.5.2\nUpgrading from Spark SQL 3.5.0 to 3.5.1\nUpgrading from Spark SQL 3.4 to 3.5\nUpgrading from Spark SQL 3.3 to 3.4\nUpgrading from Spark SQL 3.2 to 3.3\nUpgrading from Spark SQL 3.1 to 3.2\nUpgrading from Spark SQL 3.0 to 3.1\nUpgrading from Spark SQL 3.0.1 to 3.0.2\nUpgrading from Spark SQL 3.0 to 3.0.1\nUpgrading from Spark SQL 2.4 to 3.0\nDataset/DataFrame APIs\nDDL Statements\nUDFs and Built-in Functions\nQuery Engine\nData Sources\nOthers\nUpgrading from Spark SQL 2.4.7 to 2.4.8\nUpgrading from Spark SQL 2.4.5 to 2.4.6\nUpgrading from Spark SQL 2.4.4 to 2.4.5\nUpgrading from Spark SQL 2.4.3 to 2.4.4\nUpgrading from Spark SQL 2.4 to 2.4.1\nUpgrading from Spark SQL 2.3 to 2.4\nUpgrading from Spark SQL 2.2 to 2.3\nUpgrading from Spark SQL 2.1 to 2.2\nUpgrading from Spark SQL 2.0 to 2.1\nUpgrading from Spark SQL 1.6 to 2.0\nUpgrading from Spark SQL 1.5 to 1.6\nUpgrading from Spark SQL 1.4 to 1.5\nUpgrading from Spark SQL 1.3 to 1.4\nUpgrading from Spark SQL 1.0-1.2 to 1.3\nCompatibility with Apache Hive\nUpgrading from Spark SQL 3.5 to 4.0\nSince Spark 4.0,\nspark.sql.ansi.enabled\nis on by default. To restore the previous behavior, set\nspark.sql.ansi.enabled\nto\nfalse\nor\nSPARK_ANSI_SQL_MODE\nto\nfalse\n.\nSince Spark 4.0,\nCREATE TABLE\nsyntax without\nUSING\nand\nSTORED AS\nwill use the value of\nspark.sql.sources.default\nas the table provider instead of\nHive\n. To restore the previous behavior, set\nspark.sql.legacy.createHiveTableByDefault\nto\ntrue\nor\nSPARK_SQL_LEGACY_CREATE_HIVE_TABLE\nto\ntrue\n.\nSince Spark 4.0, the default behaviour when inserting elements in a map is changed to first normalize keys -0.0 to 0.0. The affected SQL functions are\ncreate_map\n,\nmap_from_arrays\n,\nmap_from_entries\n, and\nmap_concat\n. To restore the previous behaviour, set\nspark.sql.legacy.disableMapKeyNormalization\nto\ntrue\n.\nSince Spark 4.0, the default value of\nspark.sql.maxSinglePartitionBytes\nis changed from\nLong.MaxValue\nto\n128m\n. To restore the previous behavior, set\nspark.sql.maxSinglePartitionBytes\nto\n9223372036854775807\n(\nLong.MaxValue\n).\nSince Spark 4.0, any read of SQL tables takes into consideration the SQL configs\nspark.sql.files.ignoreCorruptFiles\n/\nspark.sql.files.ignoreMissingFiles\ninstead of the core config\nspark.files.ignoreCorruptFiles\n/\nspark.files.ignoreMissingFiles\n.\nSince Spark 4.0, when reading SQL tables hits\norg.apache.hadoop.security.AccessControlException\nand\norg.apache.hadoop.hdfs.BlockMissingException\n, the exception will be thrown and fail the task, even if\nspark.sql.files.ignoreCorruptFiles\nis set to\ntrue\n.\nSince Spark 4.0,\nspark.sql.hive.metastore\ndrops the support of Hive prior to 2.0.0 as they require JDK 8 that Spark does not support anymore. Users should migrate to higher versions.\nSince Spark 4.0, Spark removes\nhive-llap-common\ndependency. To restore the previous behavior, add\nhive-llap-common\njar to the class path.\nSince Spark 4.0,\nspark.sql.parquet.compression.codec\ndrops the support of codec name\nlz4raw\n, please use\nlz4_raw\ninstead.\nSince Spark 4.0, when overflowing during casting timestamp to byte/short/int under non-ansi mode, Spark will return null instead a wrapping value.\nSince Spark 4.0, the\nencode()\nand\ndecode()\nfunctions support only the following charsets ‘US-ASCII’, ‘ISO-8859-1’, ‘UTF-8’, ‘UTF-16BE’, ‘UTF-16LE’, ‘UTF-16’, ‘UTF-32’. To restore the previous behavior when the function accepts charsets of the current JDK used by Spark, set\nspark.sql.legacy.javaCharsets\nto\ntrue\n.\nSince Spark 4.0, the\nencode()\nand\ndecode()\nfunctions raise\nMALFORMED_CHARACTER_CODING\nerror when handling unmappable characters, while in Spark 3.5 and earlier versions, these characters will be replaced with mojibakes. To restore the previous behavior, set\nspark.sql.legacy.codingErrorAction\nto\ntrue\n. For example, if you try to\ndecode\na string value\ntést\n/ [116, -23, 115, 116] (encoded in latin1) with ‘UTF-8’, you get\nt�st\n.\nSince Spark 4.0, the legacy datetime rebasing SQL configs with the prefix\nspark.sql.legacy\nare removed. To restore the previous behavior, use the following configs:\nspark.sql.parquet.int96RebaseModeInWrite\ninstead of\nspark.sql.legacy.parquet.int96RebaseModeInWrite\nspark.sql.parquet.datetimeRebaseModeInWrite\ninstead of\nspark.sql.legacy.parquet.datetimeRebaseModeInWrite\nspark.sql.parquet.int96RebaseModeInRead\ninstead of\nspark.sql.legacy.parquet.int96RebaseModeInRead\nspark.sql.avro.datetimeRebaseModeInWrite\ninstead of\nspark.sql.legacy.avro.datetimeRebaseModeInWrite\nspark.sql.avro.datetimeRebaseModeInRead\ninstead of\nspark.sql.legacy.avro.datetimeRebaseModeInRead\nSince Spark 4.0, the default value of\nspark.sql.orc.compression.codec\nis changed from\nsnappy\nto\nzstd\n. To restore the previous behavior, set\nspark.sql.orc.compression.codec\nto\nsnappy\n.\nSince Spark 4.0, the SQL config\nspark.sql.legacy.allowZeroIndexInFormatString\nis deprecated. Consider to change\nstrfmt\nof the\nformat_string\nfunction to use 1-based indexes. The first argument must be referenced by\n1$\n, the second by\n2$\n, etc.\nSince Spark 4.0, Postgres JDBC datasource will read JDBC read TIMESTAMP WITH TIME ZONE as TimestampType regardless of the JDBC read option\npreferTimestampNTZ\n, while in 3.5 and previous, TimestampNTZType when\npreferTimestampNTZ=true\n. To restore the previous behavior, set\nspark.sql.legacy.postgres.datetimeMapping.enabled\nto\ntrue\n.\nSince Spark 4.0, Postgres JDBC datasource will write TimestampType as TIMESTAMP WITH TIME ZONE, while in 3.5 and previous, it wrote as TIMESTAMP a.k.a. TIMESTAMP WITHOUT TIME ZONE. To restore the previous behavior, set\nspark.sql.legacy.postgres.datetimeMapping.enabled\nto\ntrue\n.\nSince Spark 4.0, MySQL JDBC datasource will read TIMESTAMP as TimestampType regardless of the JDBC read option\npreferTimestampNTZ\n, while in 3.5 and previous, TimestampNTZType when\npreferTimestampNTZ=true\n. To restore the previous behavior, set\nspark.sql.legacy.mysql.timestampNTZMapping.enabled\nto\ntrue\n, MySQL DATETIME is not affected.\nSince Spark 4.0, MySQL JDBC datasource will read SMALLINT as ShortType, while in Spark 3.5 and previous, it was read as IntegerType. MEDIUMINT UNSIGNED is read as IntegerType, while in Spark 3.5 and previous, it was read as LongType. To restore the previous behavior, you can cast the column to the old type.\nSince Spark 4.0, MySQL JDBC datasource will read FLOAT as FloatType, while in Spark 3.5 and previous, it was read as DoubleType. To restore the previous behavior, you can cast the column to the old type.\nSince Spark 4.0, MySQL JDBC datasource will read BIT(n > 1) as BinaryType, while in Spark 3.5 and previous, read as LongType. To restore the previous behavior, set\nspark.sql.legacy.mysql.bitArrayMapping.enabled\nto\ntrue\n.\nSince Spark 4.0, MySQL JDBC datasource will write ShortType as SMALLINT, while in Spark 3.5 and previous, write as INTEGER. To restore the previous behavior, you can replace the column with IntegerType whenever before writing.\nSince Spark 4.0, MySQL JDBC datasource will write TimestampNTZType as MySQL DATETIME because they both represent TIMESTAMP WITHOUT TIME ZONE, while in 3.5 and previous, it wrote as MySQL TIMESTAMP. To restore the previous behavior, set\nspark.sql.legacy.mysql.timestampNTZMapping.enabled\nto\ntrue\n.\nSince Spark 4.0, Oracle JDBC datasource will write TimestampType as TIMESTAMP WITH LOCAL TIME ZONE, while in Spark 3.5 and previous, write as TIMESTAMP. To restore the previous behavior, set\nspark.sql.legacy.oracle.timestampMapping.enabled\nto\ntrue\n.\nSince Spark 4.0, MsSQL Server JDBC datasource will read TINYINT as ShortType, while in Spark 3.5 and previous, read as IntegerType. To restore the previous behavior, set\nspark.sql.legacy.mssqlserver.numericMapping.enabled\nto\ntrue\n.\nSince Spark 4.0, MsSQL Server JDBC datasource will read DATETIMEOFFSET as TimestampType, while in Spark 3.5 and previous, read as StringType. To restore the previous behavior, set\nspark.sql.legacy.mssqlserver.datetimeoffsetMapping.enabled\nto\ntrue\n.\nSince Spark 4.0, DB2 JDBC datasource will read SMALLINT as ShortType, while in Spark 3.5 and previous, it was read as IntegerType. To restore the previous behavior, set\nspark.sql.legacy.db2.numericMapping.enabled\nto\ntrue\n.\nSince Spark 4.0, DB2 JDBC datasource will write BooleanType as BOOLEAN, while in Spark 3.5 and previous, write as CHAR(1). To restore the previous behavior, set\nspark.sql.legacy.db2.booleanMapping.enabled\nto\ntrue\n.\nSince Spark 4.0, The default value for\nspark.sql.legacy.ctePrecedencePolicy\nhas been changed from\nEXCEPTION\nto\nCORRECTED\n. Instead of raising an error, inner CTE definitions take precedence over outer definitions.\nSince Spark 4.0, The default value for\nspark.sql.legacy.timeParserPolicy\nhas been changed from\nEXCEPTION\nto\nCORRECTED\n. Instead of raising an\nINCONSISTENT_BEHAVIOR_CROSS_VERSION\nerror,\nCANNOT_PARSE_TIMESTAMP\nwill be raised if ANSI mode is enable.\nNULL\nwill be returned if ANSI mode is disabled. See\nDatetime Patterns for Formatting and Parsing\n.\nSince Spark 4.0, A bug falsely allowing\n!\ninstead of\nNOT\nwhen\n!\nis not a prefix operator has been fixed. Clauses such as\nexpr ! IN (...)\n,\nexpr ! BETWEEN ...\n, or\ncol ! NULL\nnow raise syntax errors. To restore the previous behavior, set\nspark.sql.legacy.bangEqualsNot\nto\ntrue\n.\nSince Spark 4.0, By default views tolerate column type changes in the query and compensate with casts. To restore the previous behavior, allowing up-casts only, set\nspark.sql.legacy.viewSchemaCompensation\nto\nfalse\n.\nSince Spark 4.0, Views allow control over how they react to underlying query changes. By default views tolerate column type changes in the query and compensate with casts. To disable this feature set\nspark.sql.legacy.viewSchemaBindingMode\nto\nfalse\n. This also removes the clause from\nDESCRIBE EXTENDED\nand\nSHOW CREATE TABLE\n.\nSince Spark 4.0, The Storage-Partitioned Join feature flag\nspark.sql.sources.v2.bucketing.pushPartValues.enabled\nis set to\ntrue\n. To restore the previous behavior, set\nspark.sql.sources.v2.bucketing.pushPartValues.enabled\nto\nfalse\n.\nSince Spark 4.0, the\nsentences\nfunction uses\nLocale(language)\ninstead of\nLocale.US\nwhen\nlanguage\nparameter is not\nNULL\nand\ncountry\nparameter is\nNULL\n.\nSince Spark 4.0, reading from a file source table will correctly respect query options, e.g. delimiters. Previously, the first query plan was cached and subsequent option changes ignored. To restore the previous behavior, set\nspark.sql.legacy.readFileSourceTableCacheIgnoreOptions\nto\ntrue\n.\nUpgrading from Spark SQL 3.5.3 to 3.5.4\nSince Spark 3.5.4, when reading SQL tables hits\norg.apache.hadoop.security.AccessControlException\nand\norg.apache.hadoop.hdfs.BlockMissingException\n, the exception will be thrown and fail the task, even if\nspark.sql.files.ignoreCorruptFiles\nis set to\ntrue\n.\nUpgrading from Spark SQL 3.5.1 to 3.5.2\nSince 3.5.2, MySQL JDBC datasource will read TINYINT UNSIGNED as ShortType, while in 3.5.1, it was wrongly read as ByteType.\nUpgrading from Spark SQL 3.5.0 to 3.5.1\nSince Spark 3.5.1, MySQL JDBC datasource will read TINYINT(n > 1) and TINYINT UNSIGNED as ByteType, while in Spark 3.5.0 and below, they were read as IntegerType. To restore the previous behavior, you can cast the column to the old type.\nUpgrading from Spark SQL 3.4 to 3.5\nSince Spark 3.5, the JDBC options related to DS V2 pushdown are\ntrue\nby default. These options include:\npushDownAggregate\n,\npushDownLimit\n,\npushDownOffset\nand\npushDownTableSample\n. To restore the legacy behavior, please set them to\nfalse\n. e.g. set\nspark.sql.catalog.your_catalog_name.pushDownAggregate\nto\nfalse\n.\nSince Spark 3.5, Spark thrift server will interrupt task when canceling a running statement. To restore the previous behavior, set\nspark.sql.thriftServer.interruptOnCancel\nto\nfalse\n.\nSince Spark 3.5, Row’s json and prettyJson methods are moved to\nToJsonUtil\n.\nSince Spark 3.5, the\nplan\nfield is moved from\nAnalysisException\nto\nEnhancedAnalysisException\n.\nSince Spark 3.5,\nspark.sql.optimizer.canChangeCachedPlanOutputPartitioning\nis enabled by default. To restore the previous behavior, set\nspark.sql.optimizer.canChangeCachedPlanOutputPartitioning\nto\nfalse\n.\nSince Spark 3.5, the\narray_insert\nfunction is 1-based for negative indexes. It inserts new element at the end of input arrays for the index -1. To restore the previous behavior, set\nspark.sql.legacy.negativeIndexInArrayInsert\nto\ntrue\n.\nSince Spark 3.5, the Avro will throw\nAnalysisException\nwhen reading Interval types as Date or Timestamp types, or reading Decimal types with lower precision. To restore the legacy behavior, set\nspark.sql.legacy.avro.allowIncompatibleSchema\nto\ntrue\nUpgrading from Spark SQL 3.3 to 3.4\nSince Spark 3.4, INSERT INTO commands with explicit column lists comprising fewer columns than the target table will automatically add the corresponding default values for the remaining columns (or NULL for any column lacking an explicitly-assigned default value). In Spark 3.3 or earlier, these commands would have failed returning errors reporting that the number of provided columns does not match the number of columns in the target table. Note that disabling\nspark.sql.defaultColumn.useNullsForMissingDefaultValues\nwill restore the previous behavior.\nSince Spark 3.4, Number or Number(*) from Teradata will be treated as Decimal(38,18). In Spark 3.3 or earlier, Number or Number(*) from Teradata will be treated as Decimal(38, 0), in which case the fractional part will be removed.\nSince Spark 3.4, v1 database, table, permanent view and function identifier will include ‘spark_catalog’ as the catalog name if database is defined, e.g. a table identifier will be:\nspark_catalog.default.t\n. To restore the legacy behavior, set\nspark.sql.legacy.v1IdentifierNoCatalog\nto\ntrue\n.\nSince Spark 3.4, when ANSI SQL mode(configuration\nspark.sql.ansi.enabled\n) is on, Spark SQL always returns NULL result on getting a map value with a non-existing key. In Spark 3.3 or earlier, there will be an error.\nSince Spark 3.4, the SQL CLI\nspark-sql\ndoes not print the prefix\nError in query:\nbefore the error message of\nAnalysisException\n.\nSince Spark 3.4,\nsplit\nfunction ignores trailing empty strings when\nregex\nparameter is empty.\nSince Spark 3.4, the\nto_binary\nfunction throws error for a malformed\nstr\ninput. Use\ntry_to_binary\nto tolerate malformed input and return NULL instead.\nValid Base64 string should include symbols from in base64 alphabet (A-Za-z0-9+/), optional padding (\n=\n), and optional whitespaces. Whitespaces are skipped in conversion except when they are preceded by padding symbol(s). If padding is present it should conclude the string and follow rules described in RFC 4648 § 4.\nValid hexadecimal strings should include only allowed symbols (0-9A-Fa-f).\nValid values for\nfmt\nare case-insensitive\nhex\n,\nbase64\n,\nutf-8\n,\nutf8\n.\nSince Spark 3.4, Spark throws only\nPartitionsAlreadyExistException\nwhen it creates partitions but some of them exist already. In Spark 3.3 or earlier, Spark can throw either\nPartitionsAlreadyExistException\nor\nPartitionAlreadyExistsException\n.\nSince Spark 3.4, Spark will do validation for partition spec in ALTER PARTITION to follow the behavior of\nspark.sql.storeAssignmentPolicy\nwhich may cause an exception if type conversion fails, e.g.\nALTER TABLE .. ADD PARTITION(p='a')\nif column\np\nis int type. To restore the legacy behavior, set\nspark.sql.legacy.skipTypeValidationOnAlterPartition\nto\ntrue\n.\nSince Spark 3.4, vectorized readers are enabled by default for the nested data types (array, map and struct). To restore the legacy behavior, set\nspark.sql.orc.enableNestedColumnVectorizedReader\nand\nspark.sql.parquet.enableNestedColumnVectorizedReader\nto\nfalse\n.\nSince Spark 3.4,\nBinaryType\nis not supported in CSV datasource. In Spark 3.3 or earlier, users can write binary columns in CSV datasource, but the output content in CSV files is\nObject.toString()\nwhich is meaningless; meanwhile, if users read CSV tables with binary columns, Spark will throw an\nUnsupported type: binary\nexception.\nSince Spark 3.4, bloom filter joins are enabled by default. To restore the legacy behavior, set\nspark.sql.optimizer.runtime.bloomFilter.enabled\nto\nfalse\n.\nSince Spark 3.4, when schema inference on external Parquet files, INT64 timestamps with annotation\nisAdjustedToUTC=false\nwill be inferred as TimestampNTZ type instead of Timestamp type. To restore the legacy behavior, set\nspark.sql.parquet.inferTimestampNTZ.enabled\nto\nfalse\n.\nSince Spark 3.4, the behavior for\nCREATE TABLE AS SELECT ...\nis changed from OVERWRITE to APPEND when\nspark.sql.legacy.allowNonEmptyLocationInCTAS\nis set to\ntrue\n. Users are recommended to avoid CTAS with a non-empty table location.\nUpgrading from Spark SQL 3.2 to 3.3\nSince Spark 3.3, the\nhistogram_numeric\nfunction in Spark SQL returns an output type of an array of structs (x, y), where the type of the ‘x’ field in the return value is propagated from the input values consumed in the aggregate function. In Spark 3.2 or earlier, ‘x’ always had double type. Optionally, use the configuration\nspark.sql.legacy.histogramNumericPropagateInputType\nsince Spark 3.3 to revert back to the previous behavior.\nSince Spark 3.3,\nDayTimeIntervalType\nin Spark SQL is mapped to Arrow’s\nDuration\ntype in\nArrowWriter\nand\nArrowColumnVector\ndeveloper APIs. Previously,\nDayTimeIntervalType\nwas mapped to Arrow’s\nInterval\ntype which does not match with the types of other languages Spark SQL maps. For example,\nDayTimeIntervalType\nis mapped to\njava.time.Duration\nin Java.\nSince Spark 3.3, the functions\nlpad\nand\nrpad\nhave been overloaded to support byte sequences. When the first argument is a byte sequence, the optional padding pattern must also be a byte sequence and the result is a BINARY value. The default padding pattern in this case is the zero byte. To restore the legacy behavior of always returning string types, set\nspark.sql.legacy.lpadRpadAlwaysReturnString\nto\ntrue\n.\nSince Spark 3.3, Spark turns a non-nullable schema into nullable for API\nDataFrameReader.schema(schema: StructType).json(jsonDataset: Dataset[String])\nand\nDataFrameReader.schema(schema: StructType).csv(csvDataset: Dataset[String])\nwhen the schema is specified by the user and contains non-nullable fields. To restore the legacy behavior of respecting the nullability, set\nspark.sql.legacy.respectNullabilityInTextDatasetConversion\nto\ntrue\n.\nSince Spark 3.3, when the date or timestamp pattern is not specified, Spark converts an input string to a date/timestamp using the\nCAST\nexpression approach. The changes affect CSV/JSON datasources and parsing of partition values. In Spark 3.2 or earlier, when the date or timestamp pattern is not set, Spark uses the default patterns:\nyyyy-MM-dd\nfor dates and\nyyyy-MM-dd HH:mm:ss\nfor timestamps. After the changes, Spark still recognizes the pattern together with\nDate patterns:\n[+-]yyyy*\n[+-]yyyy*-[m]m\n[+-]yyyy*-[m]m-[d]d\n[+-]yyyy*-[m]m-[d]d\n[+-]yyyy*-[m]m-[d]d *\n[+-]yyyy*-[m]m-[d]dT*\nTimestamp patterns:\n[+-]yyyy*\n[+-]yyyy*-[m]m\n[+-]yyyy*-[m]m-[d]d\n[+-]yyyy*-[m]m-[d]d\n[+-]yyyy*-[m]m-[d]d [h]h:[m]m:[s]s.[ms][ms][ms][us][us][us][zone_id]\n[+-]yyyy*-[m]m-[d]dT[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us][zone_id]\n[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us][zone_id]\nT[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us][zone_id]\nSince Spark 3.3, the\nstrfmt\nin\nformat_string(strfmt, obj, ...)\nand\nprintf(strfmt, obj, ...)\nwill no longer support to use\n0$\nto specify the first argument, the first argument should always reference by\n1$\nwhen use argument index to indicating the position of the argument in the argument list.\nSince Spark 3.3, nulls are written as empty strings in CSV data source by default. In Spark 3.2 or earlier, nulls were written as empty strings as quoted empty strings,\n\"\"\n. To restore the previous behavior, set\nnullValue\nto\n\"\"\n, or set the configuration\nspark.sql.legacy.nullValueWrittenAsQuotedEmptyStringCsv\nto\ntrue\n.\nSince Spark 3.3, DESCRIBE FUNCTION fails if the function does not exist. In Spark 3.2 or earlier, DESCRIBE FUNCTION can still run and print “Function: func_name not found”.\nSince Spark 3.3, the table property\nexternal\nbecomes reserved. Certain commands will fail if you specify the\nexternal\nproperty, such as\nCREATE TABLE ... TBLPROPERTIES\nand\nALTER TABLE ... SET TBLPROPERTIES\n. In Spark 3.2 and earlier, the table property\nexternal\nis silently ignored. You can set\nspark.sql.legacy.notReserveProperties\nto\ntrue\nto restore the old behavior.\nSince Spark 3.3, DROP FUNCTION fails if the function name matches one of the built-in functions’ name and is not qualified. In Spark 3.2 or earlier, DROP FUNCTION can still drop a persistent function even if the name is not qualified and is the same as a built-in function’s name.\nSince Spark 3.3, when reading values from a JSON attribute defined as\nFloatType\nor\nDoubleType\n, the strings\n\"+Infinity\"\n,\n\"+INF\"\n, and\n\"-INF\"\nare now parsed to the appropriate values, in addition to the already supported\n\"Infinity\"\nand\n\"-Infinity\"\nvariations. This change was made to improve consistency with Jackson’s parsing of the unquoted versions of these values. Also, the\nallowNonNumericNumbers\noption is now respected so these strings will now be considered invalid if this option is disabled.\nSince Spark 3.3, Spark will try to use built-in data source writer instead of Hive serde in\nINSERT OVERWRITE DIRECTORY\n. This behavior is effective only if\nspark.sql.hive.convertMetastoreParquet\nor\nspark.sql.hive.convertMetastoreOrc\nis enabled respectively for Parquet and ORC formats. To restore the behavior before Spark 3.3, you can set\nspark.sql.hive.convertMetastoreInsertDir\nto\nfalse\n.\nSince Spark 3.3, the precision of the return type of round-like functions has been fixed. This may cause Spark throw\nAnalysisException\nof the\nCANNOT_UP_CAST_DATATYPE\nerror class when using views created by prior versions. In such cases, you need to recreate the views using ALTER VIEW AS or CREATE OR REPLACE VIEW AS with newer Spark versions.\nSince Spark 3.3, the\nunbase64\nfunction throws error for a malformed\nstr\ninput. Use\ntry_to_binary(<str>, 'base64')\nto tolerate malformed input and return NULL instead. In Spark 3.2 and earlier, the\nunbase64\nfunction returns a best-efforts result for a malformed\nstr\ninput.\nSince Spark 3.3, when reading Parquet files that were not produced by Spark, Parquet timestamp columns with annotation\nisAdjustedToUTC = false\nare inferred as TIMESTAMP_NTZ type during schema inference. In Spark 3.2 and earlier, these columns are inferred as TIMESTAMP type. To restore the behavior before Spark 3.3, you can set\nspark.sql.parquet.inferTimestampNTZ.enabled\nto\nfalse\n.\nSince Spark 3.3.1 and 3.2.3, for\nSELECT ... GROUP BY a GROUPING SETS (b)\n-style SQL statements,\ngrouping__id\nreturns different values from Apache Spark 3.2.0, 3.2.1, 3.2.2, and 3.3.0. It computes based on user-given group-by expressions plus grouping set columns. To restore the behavior before 3.3.1 and 3.2.3, you can set\nspark.sql.legacy.groupingIdWithAppendedUserGroupBy\n. For details, see\nSPARK-40218\nand\nSPARK-40562\n.\nUpgrading from Spark SQL 3.1 to 3.2\nSince Spark 3.2, ADD FILE/JAR/ARCHIVE commands require each path to be enclosed by\n\"\nor\n'\nif the path contains whitespaces.\nSince Spark 3.2, all the supported JDBC dialects use StringType for ROWID. In Spark 3.1 or earlier, Oracle dialect uses StringType and the other dialects use LongType.\nSince Spark 3.2, Parquet files with nanosecond precision for timestamp type (\nINT64 (TIMESTAMP(NANOS, true))\n) are not readable. To restore the behavior before Spark 3.2, you can set\nspark.sql.legacy.parquet.nanosAsLong\nto\ntrue\n.\nIn Spark 3.2, PostgreSQL JDBC dialect uses StringType for MONEY and MONEY[] is not supported due to the JDBC driver for PostgreSQL can’t handle those types properly. In Spark 3.1 or earlier, DoubleType and ArrayType of DoubleType are used respectively.\nIn Spark 3.2,\nspark.sql.adaptive.enabled\nis enabled by default. To restore the behavior before Spark 3.2, you can set\nspark.sql.adaptive.enabled\nto\nfalse\n.\nIn Spark 3.2, the following meta-characters are escaped in the\nshow()\naction. In Spark 3.1 or earlier, the following metacharacters are output as it is.\n\\n\n(new line)\n\\r\n(carriage ret)\n\\t\n(horizontal tab)\n\\f\n(form feed)\n\\b\n(backspace)\n\\u000B\n(vertical tab)\n\\u0007\n(bell)\nIn Spark 3.2,\nALTER TABLE .. RENAME TO PARTITION\nthrows\nPartitionAlreadyExistsException\ninstead of\nAnalysisException\nfor tables from Hive external when the target partition already exists.\nIn Spark 3.2, script transform default FIELD DELIMIT is\n\\u0001\nfor no serde mode, serde property\nfield.delim\nis\n\\t\nfor Hive serde mode when user specifies serde. In Spark 3.1 or earlier, the default FIELD DELIMIT is\n\\t\n, serde property\nfield.delim\nis\n\\u0001\nfor Hive serde mode when user specifies serde.\nIn Spark 3.2, the auto-generated\nCast\n(such as those added by type coercion rules) will be stripped when generating column alias names. E.g.,\nsql(\"SELECT floor(1)\").columns\nwill be\nFLOOR(1)\ninstead of\nFLOOR(CAST(1 AS DOUBLE))\n.\nIn Spark 3.2, the output schema of\nSHOW TABLES\nbecomes\nnamespace: string, tableName: string, isTemporary: boolean\n. In Spark 3.1 or earlier, the\nnamespace\nfield was named\ndatabase\nfor the builtin catalog, and there is no\nisTemporary\nfield for v2 catalogs. To restore the old schema with the builtin catalog, you can set\nspark.sql.legacy.keepCommandOutputSchema\nto\ntrue\n.\nIn Spark 3.2, the output schema of\nSHOW TABLE EXTENDED\nbecomes\nnamespace: string, tableName: string, isTemporary: boolean, information: string\n. In Spark 3.1 or earlier, the\nnamespace\nfield was named\ndatabase\nfor the builtin catalog, and no change for the v2 catalogs. To restore the old schema with the builtin catalog, you can set\nspark.sql.legacy.keepCommandOutputSchema\nto\ntrue\n.\nIn Spark 3.2, the output schema of\nSHOW TBLPROPERTIES\nbecomes\nkey: string, value: string\nwhether you specify the table property key or not. In Spark 3.1 and earlier, the output schema of\nSHOW TBLPROPERTIES\nis\nvalue: string\nwhen you specify the table property key. To restore the old schema with the builtin catalog, you can set\nspark.sql.legacy.keepCommandOutputSchema\nto\ntrue\n.\nIn Spark 3.2, the output schema of\nDESCRIBE NAMESPACE\nbecomes\ninfo_name: string, info_value: string\n. In Spark 3.1 or earlier, the\ninfo_name\nfield was named\ndatabase_description_item\nand the\ninfo_value\nfield was named\ndatabase_description_value\nfor the builtin catalog. To restore the old schema with the builtin catalog, you can set\nspark.sql.legacy.keepCommandOutputSchema\nto\ntrue\n.\nIn Spark 3.2, table refreshing clears cached data of the table as well as of all its dependents such as views while keeping the dependents cached. The following commands perform table refreshing:\nALTER TABLE .. ADD PARTITION\nALTER TABLE .. RENAME PARTITION\nALTER TABLE .. DROP PARTITION\nALTER TABLE .. RECOVER PARTITIONS\nMSCK REPAIR TABLE\nLOAD DATA\nREFRESH TABLE\nTRUNCATE TABLE\nand the method\nspark.catalog.refreshTable\nIn Spark 3.1 and earlier, table refreshing leaves dependents uncached.\nIn Spark 3.2, the usage of\ncount(tblName.*)\nis blocked to avoid producing ambiguous results. Because\ncount(*)\nand\ncount(tblName.*)\nwill output differently if there is any null values. To restore the behavior before Spark 3.2, you can set\nspark.sql.legacy.allowStarWithSingleTableIdentifierInCount\nto\ntrue\n.\nIn Spark 3.2, we support typed literals in the partition spec of INSERT and ADD/DROP/RENAME PARTITION. For example,\nADD PARTITION(dt = date'2020-01-01')\nadds a partition with date value\n2020-01-01\n. In Spark 3.1 and earlier, the partition value will be parsed as string value\ndate '2020-01-01'\n, which is an illegal date value, and we add a partition with null value at the end.\nIn Spark 3.2,\nDataFrameNaFunctions.replace()\nno longer uses exact string match for the input column names, to match the SQL syntax and support qualified column names. Input column name having a dot in the name (not nested) needs to be escaped with backtick `. Now, it throws\nAnalysisException\nif the column is not found in the data frame schema. It also throws\nIllegalArgumentException\nif the input column name is a nested column. In Spark 3.1 and earlier, it used to ignore invalid input column name and nested column name.\nIn Spark 3.2, the dates subtraction expression such as\ndate1 - date2\nreturns values of\nDayTimeIntervalType\n. In Spark 3.1 and earlier, the returned type is\nCalendarIntervalType\n. To restore the behavior before Spark 3.2, you can set\nspark.sql.legacy.interval.enabled\nto\ntrue\n.\nIn Spark 3.2, the timestamps subtraction expression such as\ntimestamp '2021-03-31 23:48:00' - timestamp '2021-01-01 00:00:00'\nreturns values of\nDayTimeIntervalType\n. In Spark 3.1 and earlier, the type of the same expression is\nCalendarIntervalType\n. To restore the behavior before Spark 3.2, you can set\nspark.sql.legacy.interval.enabled\nto\ntrue\n.\nIn Spark 3.2,\nCREATE TABLE .. LIKE ..\ncommand can not use reserved properties. You need their specific clauses to specify them, for example,\nCREATE TABLE test1 LIKE test LOCATION 'some path'\n. You can set\nspark.sql.legacy.notReserveProperties\nto\ntrue\nto ignore the\nParseException\n, in this case, these properties will be silently removed, for example:\nTBLPROPERTIES('owner'='yao')\nwill have no effect. In Spark version 3.1 and below, the reserved properties can be used in\nCREATE TABLE .. LIKE ..\ncommand but have no side effects, for example,\nTBLPROPERTIES('location'='/tmp')\ndoes not change the location of the table but only create a headless property just like\n'a'='b'\n.\nIn Spark 3.2,\nTRANSFORM\noperator can’t support alias in inputs. In Spark 3.1 and earlier, we can write script transform like\nSELECT TRANSFORM(a AS c1, b AS c2) USING 'cat' FROM TBL\n.\nIn Spark 3.2,\nTRANSFORM\noperator can support\nArrayType/MapType/StructType\nwithout Hive SerDe, in this mode, we use\nStructsToJson\nto convert\nArrayType/MapType/StructType\ncolumn to\nSTRING\nand use\nJsonToStructs\nto parse\nSTRING\nto\nArrayType/MapType/StructType\n. In Spark 3.1, Spark just support case\nArrayType/MapType/StructType\ncolumn as\nSTRING\nbut can’t support parse\nSTRING\nto\nArrayType/MapType/StructType\noutput columns.\nIn Spark 3.2, the unit-to-unit interval literals like\nINTERVAL '1-1' YEAR TO MONTH\nand the unit list interval literals like\nINTERVAL '3' DAYS '1' HOUR\nare converted to ANSI interval types:\nYearMonthIntervalType\nor\nDayTimeIntervalType\n. In Spark 3.1 and earlier, such interval literals are converted to\nCalendarIntervalType\n. To restore the behavior before Spark 3.2, you can set\nspark.sql.legacy.interval.enabled\nto\ntrue\n.\nIn Spark 3.2, the unit list interval literals can not mix year-month fields (YEAR and MONTH) and day-time fields (WEEK, DAY, …, MICROSECOND). For example,\nINTERVAL 1 month 1 hour\nis invalid in Spark 3.2. In Spark 3.1 and earlier, there is no such limitation and the literal returns value of\nCalendarIntervalType\n. To restore the behavior before Spark 3.2, you can set\nspark.sql.legacy.interval.enabled\nto\ntrue\n.\nIn Spark 3.2, Spark supports\nDayTimeIntervalType\nand\nYearMonthIntervalType\nas inputs and outputs of\nTRANSFORM\nclause in Hive\nSERDE\nmode, the behavior is different between Hive\nSERDE\nmode and\nROW FORMAT DELIMITED\nmode when these two types are used as inputs. In Hive\nSERDE\nmode,\nDayTimeIntervalType\ncolumn is converted to\nHiveIntervalDayTime\n, its string format is\n[-]?d h:m:s.n\n, but in\nROW FORMAT DELIMITED\nmode the format is\nINTERVAL '[-]?d h:m:s.n' DAY TO TIME\n. In Hive\nSERDE\nmode,\nYearMonthIntervalType\ncolumn is converted to\nHiveIntervalYearMonth\n, its string format is\n[-]?y-m\n, but in\nROW FORMAT DELIMITED\nmode the format is\nINTERVAL '[-]?y-m' YEAR TO MONTH\n.\nIn Spark 3.2,\nhash(0) == hash(-0)\nfor floating point types. Previously, different values were generated.\nIn Spark 3.2,\nCREATE TABLE AS SELECT\nwith non-empty\nLOCATION\nwill throw\nAnalysisException\n. To restore the behavior before Spark 3.2, you can set\nspark.sql.legacy.allowNonEmptyLocationInCTAS\nto\ntrue\n.\nIn Spark 3.2, special datetime values such as\nepoch\n,\ntoday\n,\nyesterday\n,\ntomorrow\n, and\nnow\nare supported in typed literals or in cast of foldable strings only, for instance,\nselect timestamp'now'\nor\nselect cast('today' as date)\n. In Spark 3.1 and 3.0, such special values are supported in any casts of strings to dates/timestamps. To keep these special values as dates/timestamps in Spark 3.1 and 3.0, you should replace them manually, e.g.\nif (c in ('now', 'today'), current_date(), cast(c as date))\n.\nIn Spark 3.2,\nFloatType\nis mapped to\nFLOAT\nin MySQL. Prior to this, it used to be mapped to\nREAL\n, which is by default a synonym to\nDOUBLE PRECISION\nin MySQL.\nIn Spark 3.2, the query executions triggered by\nDataFrameWriter\nare always named\ncommand\nwhen being sent to\nQueryExecutionListener\n. In Spark 3.1 and earlier, the name is one of\nsave\n,\ninsertInto\n,\nsaveAsTable\n.\nIn Spark 3.2,\nDataset.unionByName\nwith\nallowMissingColumns\nset to true will add missing nested fields to the end of structs. In Spark 3.1, nested struct fields are sorted alphabetically.\nIn Spark 3.2, create/alter view will fail if the input query output columns contain auto-generated alias. This is necessary to make sure the query output column names are stable across different spark versions. To restore the behavior before Spark 3.2, set\nspark.sql.legacy.allowAutoGeneratedAliasForView\nto\ntrue\n.\nIn Spark 3.2, date +/- interval with only day-time fields such as\ndate '2011-11-11' + interval 12 hours\nreturns timestamp. In Spark 3.1 and earlier, the same expression returns date. To restore the behavior before Spark 3.2, you can use\ncast\nto convert timestamp as date.\nUpgrading from Spark SQL 3.0 to 3.1\nIn Spark 3.1, statistical aggregation function includes\nstd\n,\nstddev\n,\nstddev_samp\n,\nvariance\n,\nvar_samp\n,\nskewness\n,\nkurtosis\n,\ncovar_samp\n,\ncorr\nwill return\nNULL\ninstead of\nDouble.NaN\nwhen\nDivideByZero\noccurs during expression evaluation, for example, when\nstddev_samp\napplied on a single element set. In Spark version 3.0 and earlier, it will return\nDouble.NaN\nin such case. To restore the behavior before Spark 3.1, you can set\nspark.sql.legacy.statisticalAggregate\nto\ntrue\n.\nIn Spark 3.1, grouping_id() returns long values. In Spark version 3.0 and earlier, this function returns int values. To restore the behavior before Spark 3.1, you can set\nspark.sql.legacy.integerGroupingId\nto\ntrue\n.\nIn Spark 3.1, SQL UI data adopts the\nformatted\nmode for the query plan explain results. To restore the behavior before Spark 3.1, you can set\nspark.sql.ui.explainMode\nto\nextended\n.\nIn Spark 3.1,\nfrom_unixtime\n,\nunix_timestamp\n,\nto_unix_timestamp\n,\nto_timestamp\nand\nto_date\nwill fail if the specified datetime pattern is invalid. In Spark 3.0 or earlier, they result\nNULL\n.\nIn Spark 3.1, the Parquet, ORC, Avro and JSON datasources throw the exception\norg.apache.spark.sql.AnalysisException: Found duplicate column(s) in the data schema\nin read if they detect duplicate names in top-level columns as well in nested structures. The datasources take into account the SQL config\nspark.sql.caseSensitive\nwhile detecting column name duplicates.\nIn Spark 3.1, structs and maps are wrapped by the\n{}\nbrackets in casting them to strings. For instance, the\nshow()\naction and the\nCAST\nexpression use such brackets. In Spark 3.0 and earlier, the\n[]\nbrackets are used for the same purpose. To restore the behavior before Spark 3.1, you can set\nspark.sql.legacy.castComplexTypesToString.enabled\nto\ntrue\n.\nIn Spark 3.1, NULL elements of structures, arrays and maps are converted to “null” in casting them to strings. In Spark 3.0 or earlier, NULL elements are converted to empty strings. To restore the behavior before Spark 3.1, you can set\nspark.sql.legacy.castComplexTypesToString.enabled\nto\ntrue\n.\nIn Spark 3.1, when\nspark.sql.ansi.enabled\nis false, Spark always returns null if the sum of decimal type column overflows. In Spark 3.0 or earlier, in the case, the sum of decimal type column may return null or incorrect result, or even fails at runtime (depending on the actual query plan execution).\nIn Spark 3.1,\npath\noption cannot coexist when the following methods are called with path parameter(s):\nDataFrameReader.load()\n,\nDataFrameWriter.save()\n,\nDataStreamReader.load()\n, or\nDataStreamWriter.start()\n. In addition,\npaths\noption cannot coexist for\nDataFrameReader.load()\n. For example,\nspark.read.format(\"csv\").option(\"path\", \"/tmp\").load(\"/tmp2\")\nor\nspark.read.option(\"path\", \"/tmp\").csv(\"/tmp2\")\nwill throw\norg.apache.spark.sql.AnalysisException\n. In Spark version 3.0 and below,\npath\noption is overwritten if one path parameter is passed to above methods;\npath\noption is added to the overall paths if multiple path parameters are passed to\nDataFrameReader.load()\n. To restore the behavior before Spark 3.1, you can set\nspark.sql.legacy.pathOptionBehavior.enabled\nto\ntrue\n.\nIn Spark 3.1,\nIllegalArgumentException\nis returned for the incomplete interval literals, e.g.\nINTERVAL '1'\n,\nINTERVAL '1 DAY 2'\n, which are invalid. In Spark 3.0, these literals result in\nNULL\ns.\nIn Spark 3.1, we remove the built-in Hive 1.2. You need to migrate your custom SerDes to Hive 2.3. See\nHIVE-15167\nfor more details.\nIn Spark 3.1, loading and saving of timestamps from/to parquet files fails if the timestamps are before 1900-01-01 00:00:00Z, and loaded (saved) as the INT96 type. In Spark 3.0, the actions don’t fail but might lead to shifting of the input timestamps due to rebasing from/to Julian to/from Proleptic Gregorian calendar. To restore the behavior before Spark 3.1, you can set\nspark.sql.legacy.parquet.int96RebaseModeInRead\nor/and\nspark.sql.legacy.parquet.int96RebaseModeInWrite\nto\nLEGACY\n.\nIn Spark 3.1, the\nschema_of_json\nand\nschema_of_csv\nfunctions return the schema in the SQL format in which field names are quoted. In Spark 3.0, the function returns a catalog string without field quoting and in lower case.\nIn Spark 3.1, refreshing a table will trigger an uncache operation for all other caches that reference the table, even if the table itself is not cached. In Spark 3.0 the operation will only be triggered if the table itself is cached.\nIn Spark 3.1, creating or altering a permanent view will capture runtime SQL configs and store them as view properties. These configs will be applied during the parsing and analysis phases of the view resolution. To restore the behavior before Spark 3.1, you can set\nspark.sql.legacy.useCurrentConfigsForView\nto\ntrue\n.\nIn Spark 3.1, the temporary view will have same behaviors with the permanent view, i.e. capture and store runtime SQL configs, SQL text, catalog and namespace. The captured view properties will be applied during the parsing and analysis phases of the view resolution. To restore the behavior before Spark 3.1, you can set\nspark.sql.legacy.storeAnalyzedPlanForView\nto\ntrue\n.\nIn Spark 3.1, temporary view created via\nCACHE TABLE ... AS SELECT\nwill also have the same behavior with permanent view. In particular, when the temporary view is dropped, Spark will invalidate all its cache dependents, as well as the cache for the temporary view itself. This is different from Spark 3.0 and below, which only does the latter. To restore the previous behavior, you can set\nspark.sql.legacy.storeAnalyzedPlanForView\nto\ntrue\n.\nSince Spark 3.1, CHAR/CHARACTER and VARCHAR types are supported in the table schema. Table scan/insertion will respect the char/varchar semantic. If char/varchar is used in places other than table schema, an exception will be thrown (CAST is an exception that simply treats char/varchar as string like before). To restore the behavior before Spark 3.1, which treats them as STRING types and ignores a length parameter, e.g.\nCHAR(4)\n, you can set\nspark.sql.legacy.charVarcharAsString\nto\ntrue\n.\nIn Spark 3.1,\nAnalysisException\nis replaced by its sub-classes that are thrown for tables from Hive external catalog in the following situations:\nALTER TABLE .. ADD PARTITION\nthrows\nPartitionsAlreadyExistException\nif new partition exists already\nALTER TABLE .. DROP PARTITION\nthrows\nNoSuchPartitionsException\nfor not existing partitions\nUpgrading from Spark SQL 3.0.1 to 3.0.2\nIn Spark 3.0.2,\nAnalysisException\nis replaced by its sub-classes that are thrown for tables from Hive external catalog in the following situations:\nALTER TABLE .. ADD PARTITION\nthrows\nPartitionsAlreadyExistException\nif new partition exists already\nALTER TABLE .. DROP PARTITION\nthrows\nNoSuchPartitionsException\nfor not existing partitions\nIn Spark 3.0.2,\nPARTITION(col=null)\nis always parsed as a null literal in the partition spec. In Spark 3.0.1 or earlier, it is parsed as a string literal of its text representation, e.g., string “null”, if the partition column is string type. To restore the legacy behavior, you can set\nspark.sql.legacy.parseNullPartitionSpecAsStringLiteral\nas true.\nIn Spark 3.0.2, the output schema of\nSHOW DATABASES\nbecomes\nnamespace: string\n. In Spark version 3.0.1 and earlier, the schema was\ndatabaseName: string\n. Since Spark 3.0.2, you can restore the old schema by setting\nspark.sql.legacy.keepCommandOutputSchema\nto\ntrue\n.\nUpgrading from Spark SQL 3.0 to 3.0.1\nIn Spark 3.0, JSON datasource and JSON function\nschema_of_json\ninfer TimestampType from string values if they match to the pattern defined by the JSON option\ntimestampFormat\n. Since version 3.0.1, the timestamp type inference is disabled by default. Set the JSON option\ninferTimestamp\nto\ntrue\nto enable such type inference.\nIn Spark 3.0, when casting string to integral types(tinyint, smallint, int and bigint), datetime types(date, timestamp and interval) and boolean type, the leading and trailing characters (<= ASCII 32) will be trimmed. For example,\ncast('\\b1\\b' as int)\nresults\n1\n. Since Spark 3.0.1, only the leading and trailing whitespace ASCII characters will be trimmed. For example,\ncast('\\t1\\t' as int)\nresults\n1\nbut\ncast('\\b1\\b' as int)\nresults\nNULL\n.\nUpgrading from Spark SQL 2.4 to 3.0\nDataset/DataFrame APIs\nIn Spark 3.0, the Dataset and DataFrame API\nunionAll\nis no longer deprecated. It is an alias for\nunion\n.\nIn Spark 2.4 and below,\nDataset.groupByKey\nresults to a grouped dataset with key attribute is wrongly named as “value”, if the key is non-struct type, for example, int, string, array, etc. This is counterintuitive and makes the schema of aggregation queries unexpected. For example, the schema of\nds.groupByKey(...).count()\nis\n(value, count)\n. Since Spark 3.0, we name the grouping attribute to “key”. The old behavior is preserved under a newly added configuration\nspark.sql.legacy.dataset.nameNonStructGroupingKeyAsValue\nwith a default value of\nfalse\n.\nIn Spark 3.0, the column metadata will always be propagated in the API\nColumn.name\nand\nColumn.as\n. In Spark version 2.4 and earlier, the metadata of\nNamedExpression\nis set as the\nexplicitMetadata\nfor the new column at the time the API is called, it won’t change even if the underlying\nNamedExpression\nchanges metadata. To restore the behavior before Spark 3.0, you can use the API\nas(alias: String, metadata: Metadata)\nwith explicit metadata.\nWhen turning a Dataset to another Dataset, Spark will up cast the fields in the original Dataset to the type of corresponding fields in the target DataSet. In version 2.4 and earlier, this up cast is not very strict, e.g.\nSeq(\"str\").toDS.as[Int]\nfails, but\nSeq(\"str\").toDS.as[Boolean]\nworks and throw NPE during execution. In Spark 3.0, the up cast is stricter and turning String into something else is not allowed, i.e.\nSeq(\"str\").toDS.as[Boolean]\nwill fail during analysis. To restore the behavior before Spark 3.0, set\nspark.sql.legacy.doLooseUpcast\nto\ntrue\n.\nDDL Statements\nIn Spark 3.0, when inserting a value into a table column with a different data type, the type coercion is performed as per ANSI SQL standard. Certain unreasonable type conversions such as converting\nstring\nto\nint\nand\ndouble\nto\nboolean\nare disallowed. A runtime exception is thrown if the value is out-of-range for the data type of the column. In Spark version 2.4 and below, type conversions during table insertion are allowed as long as they are valid\nCast\n. When inserting an out-of-range value to an integral field, the low-order bits of the value is inserted(the same as Java/Scala numeric type casting). For example, if 257 is inserted to a field of byte type, the result is 1. The behavior is controlled by the option\nspark.sql.storeAssignmentPolicy\n, with a default value as “ANSI”. Setting the option as “Legacy” restores the previous behavior.\nThe\nADD JAR\ncommand previously returned a result set with the single value 0. It now returns an empty result set.\nSpark 2.4 and below: the\nSET\ncommand works without any warnings even if the specified key is for\nSparkConf\nentries and it has no effect because the command does not update\nSparkConf\n, but the behavior might confuse users. In 3.0, the command fails if a\nSparkConf\nkey is used. You can disable such a check by setting\nspark.sql.legacy.setCommandRejectsSparkCoreConfs\nto\nfalse\n.\nRefreshing a cached table would trigger a table uncache operation and then a table cache (lazily) operation. In Spark version 2.4 and below, the cache name and storage level are not preserved before the uncache operation. Therefore, the cache name and storage level could be changed unexpectedly. In Spark 3.0, cache name and storage level are first preserved for cache recreation. It helps to maintain a consistent cache behavior upon table refreshing.\nIn Spark 3.0, the properties listing below become reserved; commands fail if you specify reserved properties in places like\nCREATE DATABASE ... WITH DBPROPERTIES\nand\nALTER TABLE ... SET TBLPROPERTIES\n. You need their specific clauses to specify them, for example,\nCREATE DATABASE test COMMENT 'any comment' LOCATION 'some path'\n. You can set\nspark.sql.legacy.notReserveProperties\nto\ntrue\nto ignore the\nParseException\n, in this case, these properties will be silently removed, for example:\nSET DBPROPERTIES('location'='/tmp')\nwill have no effect. In Spark version 2.4 and below, these properties are neither reserved nor have side effects, for example,\nSET DBPROPERTIES('location'='/tmp')\ndo not change the location of the database but only create a headless property just like\n'a'='b'\n.\nProperty (case sensitive)\nDatabase Reserved\nTable Reserved\nRemarks\nprovider\nno\nyes\nFor tables, use the\nUSING\nclause to specify it. Once set, it can’t be changed.\nlocation\nyes\nyes\nFor databases and tables, use the\nLOCATION\nclause to specify it.\nowner\nyes\nyes\nFor databases and tables, it is determined by the user who runs spark and create the table.\nIn Spark 3.0, you can use\nADD FILE\nto add file directories as well. Earlier you could add only single files using this command. To restore the behavior of earlier versions, set\nspark.sql.legacy.addSingleFileInAddFile\nto\ntrue\n.\nIn Spark 3.0,\nSHOW TBLPROPERTIES\nthrows\nAnalysisException\nif the table does not exist. In Spark version 2.4 and below, this scenario caused\nNoSuchTableException\n.\nIn Spark 3.0,\nSHOW CREATE TABLE table_identifier\nalways returns Spark DDL, even when the given table is a Hive SerDe table. For generating Hive DDL, use\nSHOW CREATE TABLE table_identifier AS SERDE\ncommand instead.\nIn Spark 3.0, column of CHAR type is not allowed in non-Hive-Serde tables, and CREATE/ALTER TABLE commands will fail if CHAR type is detected. Please use STRING type instead. In Spark version 2.4 and below, CHAR type is treated as STRING type and the length parameter is simply ignored.\nUDFs and Built-in Functions\nIn Spark 3.0, the\ndate_add\nand\ndate_sub\nfunctions accepts only int, smallint, tinyint as the 2nd argument; fractional and non-literal strings are not valid anymore, for example:\ndate_add(cast('1964-05-23' as date), '12.34')\ncauses\nAnalysisException\n. Note that, string literals are still allowed, but Spark will throw\nAnalysisException\nif the string content is not a valid integer. In Spark version 2.4 and below, if the 2nd argument is fractional or string value, it is coerced to int value, and the result is a date value of\n1964-06-04\n.\nIn Spark 3.0, the function\npercentile_approx\nand its alias\napprox_percentile\nonly accept integral value with range in\n[1, 2147483647]\nas its 3rd argument\naccuracy\n, fractional and string types are disallowed, for example,\npercentile_approx(10.0, 0.2, 1.8D)\ncauses\nAnalysisException\n. In Spark version 2.4 and below, if\naccuracy\nis fractional or string value, it is coerced to an int value,\npercentile_approx(10.0, 0.2, 1.8D)\nis operated as\npercentile_approx(10.0, 0.2, 1)\nwhich results in\n10.0\n.\nIn Spark 3.0, an analysis exception is thrown when hash expressions are applied on elements of\nMapType\n. To restore the behavior before Spark 3.0, set\nspark.sql.legacy.allowHashOnMapType\nto\ntrue\n.\nIn Spark 3.0, when the\narray\n/\nmap\nfunction is called without any parameters, it returns an empty collection with\nNullType\nas element type. In Spark version 2.4 and below, it returns an empty collection with\nStringType\nas element type. To restore the behavior before Spark 3.0, you can set\nspark.sql.legacy.createEmptyCollectionUsingStringType\nto\ntrue\n.\nIn Spark 3.0, the\nfrom_json\nfunctions supports two modes -\nPERMISSIVE\nand\nFAILFAST\n. The modes can be set via the\nmode\noption. The default mode became\nPERMISSIVE\n. In previous versions, behavior of\nfrom_json\ndid not conform to either\nPERMISSIVE\nnor\nFAILFAST\n, especially in processing of malformed JSON records. For example, the JSON string\n{\"a\" 1}\nwith the schema\na INT\nis converted to\nnull\nby previous versions but Spark 3.0 converts it to\nRow(null)\n.\nIn Spark version 2.4 and below, you can create map values with map type key via built-in function such as\nCreateMap\n,\nMapFromArrays\n, etc. In Spark 3.0, it’s not allowed to create map values with map type key with these built-in functions. Users can use\nmap_entries\nfunction to convert map to array<struct<key, value» as a workaround. In addition, users can still read map values with map type key from data source or Java/Scala collections, though it is discouraged.\nIn Spark version 2.4 and below, you can create a map with duplicated keys via built-in functions like\nCreateMap\n,\nStringToMap\n, etc. The behavior of map with duplicated keys is undefined, for example, map look up respects the duplicated key appears first,\nDataset.collect\nonly keeps the duplicated key appears last,\nMapKeys\nreturns duplicated keys, etc. In Spark 3.0, Spark throws\nRuntimeException\nwhen duplicated keys are found. You can set\nspark.sql.mapKeyDedupPolicy\nto\nLAST_WIN\nto deduplicate map keys with last wins policy. Users may still read map values with duplicated keys from data sources which do not enforce it (for example, Parquet), the behavior is undefined.\nIn Spark 3.0, using\norg.apache.spark.sql.functions.udf(AnyRef, DataType)\nis not allowed by default. Remove the return type parameter to automatically switch to typed Scala udf is recommended, or set\nspark.sql.legacy.allowUntypedScalaUDF\nto true to keep using it. In Spark version 2.4 and below, if\norg.apache.spark.sql.functions.udf(AnyRef, DataType)\ngets a Scala closure with primitive-type argument, the returned UDF returns null if the input values is null. However, in Spark 3.0, the UDF returns the default value of the Java type if the input value is null. For example,\nval f = udf((x: Int) => x, IntegerType)\n,\nf($\"x\")\nreturns null in Spark 2.4 and below if column\nx\nis null, and return 0 in Spark 3.0. This behavior change is introduced because Spark 3.0 is built with Scala 2.12 by default.\nIn Spark 3.0, a higher-order function\nexists\nfollows the three-valued boolean logic, that is, if the\npredicate\nreturns any\nnull\ns and no\ntrue\nis obtained, then\nexists\nreturns\nnull\ninstead of\nfalse\n. For example,\nexists(array(1, null, 3), x -> x % 2 == 0)\nis\nnull\n. The previous behavior can be restored by setting\nspark.sql.legacy.followThreeValuedLogicInArrayExists\nto\nfalse\n.\nIn Spark 3.0, the\nadd_months\nfunction does not adjust the resulting date to a last day of month if the original date is a last day of months. For example,\nselect add_months(DATE'2019-02-28', 1)\nresults\n2019-03-28\n. In Spark version 2.4 and below, the resulting date is adjusted when the original date is a last day of months. For example, adding a month to\n2019-02-28\nresults in\n2019-03-31\n.\nIn Spark version 2.4 and below, the\ncurrent_timestamp\nfunction returns a timestamp with millisecond resolution only. In Spark 3.0, the function can return the result with microsecond resolution if the underlying clock available on the system offers such resolution.\nIn Spark 3.0, a 0-argument Java UDF is executed in the executor side identically with other UDFs. In Spark version 2.4 and below, the 0-argument Java UDF alone was executed in the driver side, and the result was propagated to executors, which might be more performant in some cases but caused inconsistency with a correctness issue in some cases.\nThe result of\njava.lang.Math\n’s\nlog\n,\nlog1p\n,\nexp\n,\nexpm1\n, and\npow\nmay vary across platforms. In Spark 3.0, the result of the equivalent SQL functions (including related SQL functions like\nLOG10\n) return values consistent with\njava.lang.StrictMath\n. In virtually all cases this makes no difference in the return value, and the difference is very small, but may not exactly match\njava.lang.Math\non x86 platforms in cases like, for example,\nlog(3.0)\n, whose value varies between\nMath.log()\nand\nStrictMath.log()\n.\nIn Spark 3.0, the\ncast\nfunction processes string literals such as ‘Infinity’, ‘+Infinity’, ‘-Infinity’, ‘NaN’, ‘Inf’, ‘+Inf’, ‘-Inf’ in a case-insensitive manner when casting the literals to\nDouble\nor\nFloat\ntype to ensure greater compatibility with other database systems. This behavior change is illustrated in the table below:\nOperation\nResult before Spark 3.0\nResult in Spark 3.0\nCAST(‘infinity’ AS DOUBLE)\nNULL\nDouble.PositiveInfinity\nCAST(‘+infinity’ AS DOUBLE)\nNULL\nDouble.PositiveInfinity\nCAST(‘inf’ AS DOUBLE)\nNULL\nDouble.PositiveInfinity\nCAST(‘inf’ AS DOUBLE)\nNULL\nDouble.PositiveInfinity\nCAST(‘-infinity’ AS DOUBLE)\nNULL\nDouble.NegativeInfinity\nCAST(‘-inf’ AS DOUBLE)\nNULL\nDouble.NegativeInfinity\nCAST(‘infinity’ AS FLOAT)\nNULL\nFloat.PositiveInfinity\nCAST(‘+infinity’ AS FLOAT)\nNULL\nFloat.PositiveInfinity\nCAST(‘inf’ AS FLOAT)\nNULL\nFloat.PositiveInfinity\nCAST(‘+inf’ AS FLOAT)\nNULL\nFloat.PositiveInfinity\nCAST(‘-infinity’ AS FLOAT)\nNULL\nFloat.NegativeInfinity\nCAST(‘-inf’ AS FLOAT)\nNULL\nFloat.NegativeInfinity\nCAST(‘nan’ AS DOUBLE)\nNULL\nDouble.NaN\nCAST(‘nan’ AS FLOAT)\nNULL\nFloat.NaN\nIn Spark 3.0, when casting interval values to string type, there is no “interval” prefix, for example,\n1 days 2 hours\n. In Spark version 2.4 and below, the string contains the “interval” prefix like\ninterval 1 days 2 hours\n.\nIn Spark 3.0, when casting string value to integral types(tinyint, smallint, int and bigint), datetime types(date, timestamp and interval) and boolean type, the leading and trailing whitespaces (<= ASCII 32) will be trimmed before converted to these type values, for example,\ncast(' 1\\t' as int)\nresults\n1\n,\ncast(' 1\\t' as boolean)\nresults\ntrue\n,\ncast('2019-10-10\\t as date)\nresults the date value\n2019-10-10\n. In Spark version 2.4 and below, when casting string to integrals and booleans, it does not trim the whitespaces from both ends; the foregoing results is\nnull\n, while to datetimes, only the trailing spaces (= ASCII 32) are removed.\nQuery Engine\nIn Spark version 2.4 and below, SQL queries such as\nFROM <table>\nor\nFROM <table> UNION ALL FROM <table>\nare supported by accident. In hive-style\nFROM <table> SELECT <expr>\n, the\nSELECT\nclause is not negligible. Neither Hive nor Presto support this syntax. These queries are treated as invalid in Spark 3.0.\nIn Spark 3.0, the interval literal syntax does not allow multiple from-to units anymore. For example,\nSELECT INTERVAL '1-1' YEAR TO MONTH '2-2' YEAR TO MONTH'\nthrows parser exception.\nIn Spark 3.0, numbers written in scientific notation(for example,\n1E2\n) would be parsed as Double. In Spark version 2.4 and below, they’re parsed as Decimal. To restore the behavior before Spark 3.0, you can set\nspark.sql.legacy.exponentLiteralAsDecimal.enabled\nto\ntrue\n.\nIn Spark 3.0, day-time interval strings are converted to intervals with respect to the\nfrom\nand\nto\nbounds. If an input string does not match to the pattern defined by specified bounds, the\nParseException\nexception is thrown. For example,\ninterval '2 10:20' hour to minute\nraises the exception because the expected format is\n[+|-]h[h]:[m]m\n. In Spark version 2.4, the\nfrom\nbound was not taken into account, and the\nto\nbound was used to truncate the resulted interval. For instance, the day-time interval string from the showed example is converted to\ninterval 10 hours 20 minutes\n. To restore the behavior before Spark 3.0, you can set\nspark.sql.legacy.fromDayTimeString.enabled\nto\ntrue\n.\nIn Spark 3.0, negative scale of decimal is not allowed by default, for example, data type of literal like\n1E10BD\nis\nDecimalType(11, 0)\n. In Spark version 2.4 and below, it was\nDecimalType(2, -9)\n. To restore the behavior before Spark 3.0, you can set\nspark.sql.legacy.allowNegativeScaleOfDecimal\nto\ntrue\n.\nIn Spark 3.0, the unary arithmetic operator plus(\n+\n) only accepts string, numeric and interval type values as inputs. Besides,\n+\nwith an integral string representation is coerced to a double value, for example,\n+'1'\nreturns\n1.0\n. In Spark version 2.4 and below, this operator is ignored. There is no type checking for it, thus, all type values with a\n+\nprefix are valid, for example,\n+ array(1, 2)\nis valid and results\n[1, 2]\n. Besides, there is no type coercion for it at all, for example, in Spark 2.4, the result of\n+'1'\nis string\n1\n.\nIn Spark 3.0, Dataset query fails if it contains ambiguous column reference that is caused by self join. A typical example:\nval df1 = ...; val df2 = df1.filter(...);\n, then\ndf1.join(df2, df1(\"a\") > df2(\"a\"))\nreturns an empty result which is quite confusing. This is because Spark cannot resolve Dataset column references that point to tables being self joined, and\ndf1(\"a\")\nis exactly the same as\ndf2(\"a\")\nin Spark. To restore the behavior before Spark 3.0, you can set\nspark.sql.analyzer.failAmbiguousSelfJoin\nto\nfalse\n.\nIn Spark 3.0,\nspark.sql.legacy.ctePrecedencePolicy\nis introduced to control the behavior for name conflicting in the nested WITH clause. By default value\nEXCEPTION\n, Spark throws an AnalysisException, it forces users to choose the specific substitution order they wanted. If set to\nCORRECTED\n(which is recommended), inner CTE definitions take precedence over outer definitions. For example, set the config to\nfalse\n,\nWITH t AS (SELECT 1), t2 AS (WITH t AS (SELECT 2) SELECT * FROM t) SELECT * FROM t2\nreturns\n2\n, while setting it to\nLEGACY\n, the result is\n1\nwhich is the behavior in version 2.4 and below.\nIn Spark 3.0, configuration\nspark.sql.crossJoin.enabled\nbecome internal configuration, and is true by default, so by default spark won’t raise exception on sql with implicit cross join.\nIn Spark version 2.4 and below, float/double -0.0 is semantically equal to 0.0, but -0.0 and 0.0 are considered as different values when used in aggregate grouping keys, window partition keys, and join keys. In Spark 3.0, this bug is fixed. For example,\nSeq(-0.0, 0.0).toDF(\"d\").groupBy(\"d\").count()\nreturns\n[(0.0, 2)]\nin Spark 3.0, and\n[(0.0, 1), (-0.0, 1)]\nin Spark 2.4 and below.\nIn Spark version 2.4 and below, invalid time zone ids are silently ignored and replaced by GMT time zone, for example, in the from_utc_timestamp function. In Spark 3.0, such time zone ids are rejected, and Spark throws\njava.time.DateTimeException\n.\nIn Spark 3.0, Proleptic Gregorian calendar is used in parsing, formatting, and converting dates and timestamps as well as in extracting sub-components like years, days and so on. Spark 3.0 uses Java 8 API classes from the\njava.time\npackages that are based on\nISO chronology\n. In Spark version 2.4 and below, those operations are performed using the hybrid calendar (\nJulian + Gregorian\n. The changes impact on the results for dates before October 15, 1582 (Gregorian) and affect on the following Spark 3.0 API:\nParsing/formatting of timestamp/date strings. This effects on CSV/JSON datasources and on the\nunix_timestamp\n,\ndate_format\n,\nto_unix_timestamp\n,\nfrom_unixtime\n,\nto_date\n,\nto_timestamp\nfunctions when patterns specified by users is used for parsing and formatting. In Spark 3.0, we define our own pattern strings in\nDatetime Patterns for Formatting and Parsing\n,\n which is implemented via\nDateTimeFormatter\nunder the hood. New implementation performs strict checking of its input. For example, the\n2015-07-22 10:00:00\ntimestamp cannot be parse if pattern is\nyyyy-MM-dd\nbecause the parser does not consume whole input. Another example is the\n31/01/2015 00:00\ninput cannot be parsed by the\ndd/MM/yyyy hh:mm\npattern because\nhh\nsupposes hours in the range\n1-12\n. In Spark version 2.4 and below,\njava.text.SimpleDateFormat\nis used for timestamp/date string conversions, and the supported patterns are described in\nSimpleDateFormat\n. The old behavior can be restored by setting\nspark.sql.legacy.timeParserPolicy\nto\nLEGACY\n.\nThe\nweekofyear\n,\nweekday\n,\ndayofweek\n,\ndate_trunc\n,\nfrom_utc_timestamp\n,\nto_utc_timestamp\n, and\nunix_timestamp\nfunctions use java.time API for calculation week number of year, day number of week as well for conversion from/to TimestampType values in UTC time zone.\nThe JDBC options\nlowerBound\nand\nupperBound\nare converted to TimestampType/DateType values in the same way as casting strings to TimestampType/DateType values. The conversion is based on Proleptic Gregorian calendar, and time zone defined by the SQL config\nspark.sql.session.timeZone\n. In Spark version 2.4 and below, the conversion is based on the hybrid calendar (Julian + Gregorian) and on default system time zone.\nFormatting\nTIMESTAMP\nand\nDATE\nliterals.\nCreating typed\nTIMESTAMP\nand\nDATE\nliterals from strings. In Spark 3.0, string conversion to typed\nTIMESTAMP\n/\nDATE\nliterals is performed via casting to\nTIMESTAMP\n/\nDATE\nvalues. For example,\nTIMESTAMP '2019-12-23 12:59:30'\nis semantically equal to\nCAST('2019-12-23 12:59:30' AS TIMESTAMP)\n. When the input string does not contain information about time zone, the time zone from the SQL config\nspark.sql.session.timeZone\nis used in that case. In Spark version 2.4 and below, the conversion is based on JVM system time zone. The different sources of the default time zone may change the behavior of typed\nTIMESTAMP\nand\nDATE\nliterals.\nIn Spark 3.0,\nTIMESTAMP\nliterals are converted to strings using the SQL config\nspark.sql.session.timeZone\n. In Spark version 2.4 and below, the conversion uses the default time zone of the Java virtual machine.\nIn Spark 3.0, Spark casts\nString\nto\nDate/Timestamp\nin binary comparisons with dates/timestamps. The previous behavior of casting\nDate/Timestamp\nto\nString\ncan be restored by setting\nspark.sql.legacy.typeCoercion.datetimeToString.enabled\nto\ntrue\n.\nIn Spark 3.0, special values are supported in conversion from strings to dates and timestamps. Those values are simply notational shorthands that are converted to ordinary date or timestamp values when read. The following string values are supported for dates:\nepoch [zoneId]\n- 1970-01-01\ntoday [zoneId]\n- the current date in the time zone specified by\nspark.sql.session.timeZone\nyesterday [zoneId]\n- the current date - 1\ntomorrow [zoneId]\n- the current date + 1\nnow\n- the date of running the current query. It has the same notion as today\nFor example\nSELECT date 'tomorrow' - date 'yesterday';\nshould output\n2\n. Here are special timestamp values:\nepoch [zoneId]\n- 1970-01-01 00:00:00+00 (Unix system time zero)\ntoday [zoneId]\n- midnight today\nyesterday [zoneId]\n- midnight yesterday\ntomorrow [zoneId]\n- midnight tomorrow\nnow\n- current query start time\nFor example\nSELECT timestamp 'tomorrow';\n.\nSince Spark 3.0, when using\nEXTRACT\nexpression to extract the second field from date/timestamp values, the result will be a\nDecimalType(8, 6)\nvalue with 2 digits for second part, and 6 digits for the fractional part with microsecond precision. e.g.\nextract(second from to_timestamp('2019-09-20 10:10:10.1'))\nresults\n10.100000\n.  In Spark version 2.4 and earlier, it returns an\nIntegerType\nvalue and the result for the former example is\n10\n.\nIn Spark 3.0, datetime pattern letter\nF\nis\naligned day of week in month\nthat represents the concept of the count of days within the period of a week where the weeks are aligned to the start of the month. In Spark version 2.4 and earlier, it is\nweek of month\nthat represents the concept of the count of weeks within the month where weeks start on a fixed day-of-week, e.g.\n2020-07-30\nis 30 days (4 weeks and 2 days) after the first day of the month, so\ndate_format(date '2020-07-30', 'F')\nreturns 2 in Spark 3.0, but as a week count in Spark 2.x, it returns 5 because it locates in the 5th week of July 2020, where week one is 2020-07-01 to 07-04.\nIn Spark 3.0, Spark will try to use built-in data source writer instead of Hive serde in\nCTAS\n. This behavior is effective only if\nspark.sql.hive.convertMetastoreParquet\nor\nspark.sql.hive.convertMetastoreOrc\nis enabled respectively for Parquet and ORC formats. To restore the behavior before Spark 3.0, you can set\nspark.sql.hive.convertMetastoreCtas\nto\nfalse\n.\nIn Spark 3.0, Spark will try to use built-in data source writer instead of Hive serde to process inserting into partitioned ORC/Parquet tables created by using the HiveSQL syntax. This behavior is effective only if\nspark.sql.hive.convertMetastoreParquet\nor\nspark.sql.hive.convertMetastoreOrc\nis enabled respectively for Parquet and ORC formats. To restore the behavior before Spark 3.0, you can set\nspark.sql.hive.convertInsertingPartitionedTable\nto\nfalse\n.\nData Sources\nIn Spark version 2.4 and below, when reading a Hive SerDe table with Spark native data sources(parquet/orc), Spark infers the actual file schema and update the table schema in metastore. In Spark 3.0, Spark doesn’t infer the schema anymore. This should not cause any problems to end users, but if it does, set\nspark.sql.hive.caseSensitiveInferenceMode\nto\nINFER_AND_SAVE\n.\nIn Spark version 2.4 and below, partition column value is converted as null if it can’t be casted to corresponding user provided schema. In 3.0, partition column value is validated with user provided schema. An exception is thrown if the validation fails. You can disable such validation by setting\nspark.sql.sources.validatePartitionColumns\nto\nfalse\n.\nIn Spark 3.0, if files or subdirectories disappear during recursive directory listing (that is, they appear in an intermediate listing but then cannot be read or listed during later phases of the recursive directory listing, due to either concurrent file deletions or object store consistency issues) then the listing will fail with an exception unless\nspark.sql.files.ignoreMissingFiles\nis\ntrue\n(default\nfalse\n). In previous versions, these missing files or subdirectories would be ignored. Note that this change of behavior only applies during initial table file listing (or during\nREFRESH TABLE\n), not during query execution: the net change is that\nspark.sql.files.ignoreMissingFiles\nis now obeyed during table file listing / query planning, not only at query execution time.\nIn Spark version 2.4 and below, the parser of JSON data source treats empty strings as null for some data types such as\nIntegerType\n. For\nFloatType\n,\nDoubleType\n,\nDateType\nand\nTimestampType\n, it fails on empty strings and throws exceptions. Spark 3.0 disallows empty strings and will throw an exception for data types except for\nStringType\nand\nBinaryType\n. The previous behavior of allowing an empty string can be restored by setting\nspark.sql.legacy.json.allowEmptyString.enabled\nto\ntrue\n.\nIn Spark version 2.4 and below, JSON datasource and JSON functions like\nfrom_json\nconvert a bad JSON record to a row with all\nnull\ns in the PERMISSIVE mode when specified schema is\nStructType\n. In Spark 3.0, the returned row can contain non-\nnull\nfields if some of JSON column values were parsed and converted to desired types successfully.\nIn Spark 3.0, JSON datasource and JSON function\nschema_of_json\ninfer TimestampType from string values if they match to the pattern defined by the JSON option\ntimestampFormat\n. Set JSON option\ninferTimestamp\nto\nfalse\nto disable such type inference.\nIn Spark version 2.4 and below, CSV datasource converts a malformed CSV string to a row with all\nnull\ns in the PERMISSIVE mode. In Spark 3.0, the returned row can contain non-\nnull\nfields if some of CSV column values were parsed and converted to desired types successfully.\nIn Spark 3.0, when Avro files are written with user provided schema, the fields are matched by field names between catalyst schema and Avro schema instead of positions.\nIn Spark 3.0, when Avro files are written with user provided non-nullable schema, even the catalyst schema is nullable, Spark is still able to write the files. However, Spark throws runtime NullPointerException if any of the records contains null.\nIn Spark version 2.4 and below, CSV datasource can detect encoding of input files automatically when the files have BOM at the beginning. For instance, CSV datasource can recognize UTF-8, UTF-16BE, UTF-16LE, UTF-32BE and UTF-32LE in the multi-line mode (the CSV option\nmultiLine\nis set to\ntrue\n). In Spark 3.0, CSV datasource reads input files in encoding specified via the CSV option\nencoding\nwhich has the default value of UTF-8. In this way, if file encoding doesn’t match to the encoding specified via the CSV option, Spark loads the file incorrectly. To solve the issue, users should either set correct encoding via the CSV option\nencoding\nor set the option to\nnull\nwhich fallbacks to encoding auto-detection as in Spark versions before 3.0.\nOthers\nIn Spark version 2.4, when a Spark session is created via\ncloneSession()\n, the newly created Spark session inherits its configuration from its parent\nSparkContext\neven though the same configuration may exist with a different value in its parent Spark session. In Spark 3.0, the configurations of a parent\nSparkSession\nhave a higher precedence over the parent\nSparkContext\n. You can restore the old behavior by setting\nspark.sql.legacy.sessionInitWithConfigDefaults\nto\ntrue\n.\nIn Spark 3.0, if\nhive.default.fileformat\nis not found in\nSpark SQL configuration\nthen it falls back to the\nhive-site.xml\nfile present in the\nHadoop configuration\nof\nSparkContext\n.\nIn Spark 3.0, we pad decimal numbers with trailing zeros to the scale of the column for\nspark-sql\ninterface, for example:\nQuery\nSpark 2.4\nSpark 3.0\nSELECT CAST(1 AS decimal(38, 18));\n1\n1.000000000000000000\nIn Spark 3.0, we upgraded the built-in Hive from 1.2 to 2.3 and it brings following impacts:\nYou may need to set\nspark.sql.hive.metastore.version\nand\nspark.sql.hive.metastore.jars\naccording to the version of the Hive metastore you want to connect to. For example: set\nspark.sql.hive.metastore.version\nto\n1.2.1\nand\nspark.sql.hive.metastore.jars\nto\nmaven\nif your Hive metastore version is 1.2.1.\nYou need to migrate your custom SerDes to Hive 2.3 or build your own Spark with\nhive-1.2\nprofile. See\nHIVE-15167\nfor more details.\nThe decimal string representation can be different between Hive 1.2 and Hive 2.3 when using\nTRANSFORM\noperator in SQL for script transformation, which depends on hive’s behavior. In Hive 1.2, the string representation omits trailing zeroes. But in Hive 2.3, it is always padded to 18 digits with trailing zeroes if necessary.\nUpgrading from Spark SQL 2.4.7 to 2.4.8\nIn Spark 2.4.8,\nAnalysisException\nis replaced by its sub-classes that are thrown for tables from Hive external catalog in the following situations:\nALTER TABLE .. ADD PARTITION\nthrows\nPartitionsAlreadyExistException\nif new partition exists already\nALTER TABLE .. DROP PARTITION\nthrows\nNoSuchPartitionsException\nfor not existing partitions\nUpgrading from Spark SQL 2.4.5 to 2.4.6\nIn Spark 2.4.6, the\nRESET\ncommand does not reset the static SQL configuration values to the default. It only clears the runtime SQL configuration values.\nUpgrading from Spark SQL 2.4.4 to 2.4.5\nSince Spark 2.4.5,\nTRUNCATE TABLE\ncommand tries to set back original permission and ACLs during re-creating the table/partition paths. To restore the behaviour of earlier versions, set\nspark.sql.truncateTable.ignorePermissionAcl.enabled\nto\ntrue\n.\nSince Spark 2.4.5,\nspark.sql.legacy.mssqlserver.numericMapping.enabled\nconfiguration is added in order to support the legacy MsSQLServer dialect mapping behavior using IntegerType and DoubleType for SMALLINT and REAL JDBC types, respectively. To restore the behaviour of 2.4.3 and earlier versions, set\nspark.sql.legacy.mssqlserver.numericMapping.enabled\nto\ntrue\n.\nUpgrading from Spark SQL 2.4.3 to 2.4.4\nSince Spark 2.4.4, according to\nMsSqlServer Guide\n, MsSQLServer JDBC Dialect uses ShortType and FloatType for SMALLINT and REAL, respectively. Previously, IntegerType and DoubleType is used.\nUpgrading from Spark SQL 2.4 to 2.4.1\nThe value of\nspark.executor.heartbeatInterval\n, when specified without units like “30” rather than “30s”, was\ninconsistently interpreted as both seconds and milliseconds in Spark 2.4.0 in different parts of the code.\nUnitless values are now consistently interpreted as milliseconds. Applications that set values like “30”\nneed to specify a value with units like “30s” now, to avoid being interpreted as milliseconds; otherwise,\nthe extremely short interval that results will likely cause applications to fail.\nUpgrading from Spark SQL 2.3 to 2.4\nIn Spark version 2.3 and earlier, the second parameter to array_contains function is implicitly promoted to the element type of first array type parameter. This type promotion can be lossy and may cause\narray_contains\nfunction to return wrong result. This problem has been addressed in 2.4 by employing a safer type promotion mechanism. This can cause some change in behavior and are illustrated in the table below.\nQuery\nSpark 2.3 or Prior\nSpark 2.4\nRemarks\nSELECT array_contains(array(1), 1.34D);\ntrue\nfalse\nIn Spark 2.4, left and right parameters are promoted to array type of double type and double type respectively.\nSELECT array_contains(array(1), '1');\ntrue\nAnalysisException\nis thrown.\nExplicit cast can be used in arguments to avoid the exception. In Spark 2.4,\nAnalysisException\nis thrown since integer type can not be promoted to string type in a loss-less manner.\nSELECT array_contains(array(1), 'anystring');\nnull\nAnalysisException\nis thrown.\nExplicit cast can be used in arguments to avoid the exception. In Spark 2.4,\nAnalysisException\nis thrown since integer type can not be promoted to string type in a loss-less manner.\nSince Spark 2.4, when there is a struct field in front of the IN operator before a subquery, the inner query must contain a struct field as well. In previous versions, instead, the fields of the struct were compared to the output of the inner query. For example, if\na\nis a\nstruct(a string, b int)\n, in Spark 2.4\na in (select (1 as a, 'a' as b) from range(1))\nis a valid query, while\na in (select 1, 'a' from range(1))\nis not. In previous version it was the opposite.\nIn versions 2.2.1+ and 2.3, if\nspark.sql.caseSensitive\nis set to true, then the\nCURRENT_DATE\nand\nCURRENT_TIMESTAMP\nfunctions incorrectly became case-sensitive and would resolve to columns (unless typed in lower case). In Spark 2.4 this has been fixed and the functions are no longer case-sensitive.\nSince Spark 2.4, Spark will evaluate the set operations referenced in a query by following a precedence rule as per the SQL standard. If the order is not specified by parentheses, set operations are performed from left to right with the exception that all INTERSECT operations are performed before any UNION, EXCEPT or MINUS operations. The old behaviour of giving equal precedence to all the set operations are preserved under a newly added configuration\nspark.sql.legacy.setopsPrecedence.enabled\nwith a default value of\nfalse\n. When this property is set to\ntrue\n, spark will evaluate the set operators from left to right as they appear in the query given no explicit ordering is enforced by usage of parenthesis.\nSince Spark 2.4, Spark will display table description column Last Access value as UNKNOWN when the value was Jan 01 1970.\nSince Spark 2.4, Spark maximizes the usage of a vectorized ORC reader for ORC files by default. To do that,\nspark.sql.orc.impl\nand\nspark.sql.orc.filterPushdown\nchange their default values to\nnative\nand\ntrue\nrespectively. ORC files created by native ORC writer cannot be read by some old Apache Hive releases. Use\nspark.sql.orc.impl=hive\nto create the files shared with Hive 2.1.1 and older.\nSince Spark 2.4, writing an empty dataframe to a directory launches at least one write task, even if physically the dataframe has no partition. This introduces a small behavior change that for self-describing file formats like Parquet and Orc, Spark creates a metadata-only file in the target directory when writing a 0-partition dataframe, so that schema inference can still work if users read that directory later. The new behavior is more reasonable and more consistent regarding writing empty dataframe.\nSince Spark 2.4, expression IDs in UDF arguments do not appear in column names. For example, a column name in Spark 2.4 is not\nUDF:f(col0 AS colA#28)\nbut\nUDF:f(col0 AS `colA`)\n.\nSince Spark 2.4, writing a dataframe with an empty or nested empty schema using any file formats (parquet, orc, json, text, csv etc.) is not allowed. An exception is thrown when attempting to write dataframes with empty schema.\nSince Spark 2.4, Spark compares a DATE type with a TIMESTAMP type after promotes both sides to TIMESTAMP. To set\nfalse\nto\nspark.sql.legacy.compareDateTimestampInTimestamp\nrestores the previous behavior. This option will be removed in Spark 3.0.\nSince Spark 2.4, creating a managed table with nonempty location is not allowed. An exception is thrown when attempting to create a managed table with nonempty location. To set\ntrue\nto\nspark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\nrestores the previous behavior. This option will be removed in Spark 3.0.\nSince Spark 2.4, renaming a managed table to existing location is not allowed. An exception is thrown when attempting to rename a managed table to existing location.\nSince Spark 2.4, the type coercion rules can automatically promote the argument types of the variadic SQL functions (e.g., IN/COALESCE) to the widest common type, no matter how the input arguments order. In prior Spark versions, the promotion could fail in some specific orders (e.g., TimestampType, IntegerType and StringType) and throw an exception.\nSince Spark 2.4, Spark has enabled non-cascading SQL cache invalidation in addition to the traditional cache invalidation mechanism. The non-cascading cache invalidation mechanism allows users to remove a cache without impacting its dependent caches. This new cache invalidation mechanism is used in scenarios where the data of the cache to be removed is still valid, e.g., calling unpersist() on a Dataset, or dropping a temporary view. This allows users to free up memory and keep the desired caches valid at the same time.\nIn version 2.3 and earlier, Spark converts Parquet Hive tables by default but ignores table properties like\nTBLPROPERTIES (parquet.compression 'NONE')\n. This happens for ORC Hive table properties like\nTBLPROPERTIES (orc.compress 'NONE')\nin case of\nspark.sql.hive.convertMetastoreOrc=true\n, too. Since Spark 2.4, Spark respects Parquet/ORC specific table properties while converting Parquet/ORC Hive tables. As an example,\nCREATE TABLE t(id int) STORED AS PARQUET TBLPROPERTIES (parquet.compression 'NONE')\nwould generate Snappy parquet files during insertion in Spark 2.3, and in Spark 2.4, the result would be uncompressed parquet files.\nSince Spark 2.0, Spark converts Parquet Hive tables by default for better performance. Since Spark 2.4, Spark converts ORC Hive tables by default, too. It means Spark uses its own ORC support by default instead of Hive SerDe. As an example,\nCREATE TABLE t(id int) STORED AS ORC\nwould be handled with Hive SerDe in Spark 2.3, and in Spark 2.4, it would be converted into Spark’s ORC data source table and ORC vectorization would be applied. To set\nfalse\nto\nspark.sql.hive.convertMetastoreOrc\nrestores the previous behavior.\nIn version 2.3 and earlier, CSV rows are considered as malformed if at least one column value in the row is malformed. CSV parser dropped such rows in the DROPMALFORMED mode or outputs an error in the FAILFAST mode. Since Spark 2.4, CSV row is considered as malformed only when it contains malformed column values requested from CSV datasource, other values can be ignored. As an example, CSV file contains the “id,name” header and one row “1234”. In Spark 2.4, selection of the id column consists of a row with one column value 1234 but in Spark 2.3 and earlier it is empty in the DROPMALFORMED mode. To restore the previous behavior, set\nspark.sql.csv.parser.columnPruning.enabled\nto\nfalse\n.\nSince Spark 2.4, File listing for compute statistics is done in parallel by default. This can be disabled by setting\nspark.sql.statistics.parallelFileListingInStatsComputation.enabled\nto\nFalse\n.\nSince Spark 2.4, Metadata files (e.g. Parquet summary files) and temporary files are not counted as data files when calculating table size during Statistics computation.\nSince Spark 2.4, empty strings are saved as quoted empty strings\n\"\"\n. In version 2.3 and earlier, empty strings are equal to\nnull\nvalues and do not reflect to any characters in saved CSV files. For example, the row of\n\"a\", null, \"\", 1\nwas written as\na,,,1\n. Since Spark 2.4, the same row is saved as\na,,\"\",1\n. To restore the previous behavior, set the CSV option\nemptyValue\nto empty (not quoted) string.\nSince Spark 2.4, The LOAD DATA command supports wildcard\n?\nand\n*\n, which match any one character, and zero or more characters, respectively. Example:\nLOAD DATA INPATH '/tmp/folder*/'\nor\nLOAD DATA INPATH '/tmp/part-?'\n. Special Characters like\nspace\nalso now work in paths. Example:\nLOAD DATA INPATH '/tmp/folder name/'\n.\nIn Spark version 2.3 and earlier, HAVING without GROUP BY is treated as WHERE. This means,\nSELECT 1 FROM range(10) HAVING true\nis executed as\nSELECT 1 FROM range(10) WHERE true\nand returns 10 rows. This violates SQL standard, and has been fixed in Spark 2.4. Since Spark 2.4, HAVING without GROUP BY is treated as a global aggregate, which means\nSELECT 1 FROM range(10) HAVING true\nwill return only one row. To restore the previous behavior, set\nspark.sql.legacy.parser.havingWithoutGroupByAsWhere\nto\ntrue\n.\nIn version 2.3 and earlier, when reading from a Parquet data source table, Spark always returns null for any column whose column names in Hive metastore schema and Parquet schema are in different letter cases, no matter whether\nspark.sql.caseSensitive\nis set to\ntrue\nor\nfalse\n. Since 2.4, when\nspark.sql.caseSensitive\nis set to\nfalse\n, Spark does case insensitive column name resolution between Hive metastore schema and Parquet schema, so even column names are in different letter cases, Spark returns corresponding column values. An exception is thrown if there is ambiguity, i.e. more than one Parquet column is matched. This change also applies to Parquet Hive tables when\nspark.sql.hive.convertMetastoreParquet\nis set to\ntrue\n.\nUpgrading from Spark SQL 2.2 to 2.3\nSince Spark 2.3, the queries from raw JSON/CSV files are disallowed when the referenced columns only include the internal corrupt record column (named\n_corrupt_record\nby default). For example,\nspark.read.schema(schema).json(file).filter($\"_corrupt_record\".isNotNull).count()\nand\nspark.read.schema(schema).json(file).select(\"_corrupt_record\").show()\n. Instead, you can cache or save the parsed results and then send the same query. For example,\nval df = spark.read.schema(schema).json(file).cache()\nand then\ndf.filter($\"_corrupt_record\".isNotNull).count()\n.\nThe\npercentile_approx\nfunction previously accepted numeric type input and output double type results. Now it supports date type, timestamp type and numeric types as input types. The result type is also changed to be the same as the input type, which is more reasonable for percentiles.\nSince Spark 2.3, the Join/Filter’s deterministic predicates that are after the first non-deterministic predicates are also pushed down/through the child operators, if possible. In prior Spark versions, these filters are not eligible for predicate pushdown.\nPartition column inference previously found incorrect common type for different inferred types, for example, previously it ended up with double type as the common type for double type and date type. Now it finds the correct common type for such conflicts. The conflict resolution follows the table below:\nInputA \\ InputB\nNullType\nIntegerType\nLongType\nDecimalType(38,0)*\nDoubleType\nDateType\nTimestampType\nStringType\nNullType\nNullType\nIntegerType\nLongType\nDecimalType(38,0)\nDoubleType\nDateType\nTimestampType\nStringType\nIntegerType\nIntegerType\nIntegerType\nLongType\nDecimalType(38,0)\nDoubleType\nStringType\nStringType\nStringType\nLongType\nLongType\nLongType\nLongType\nDecimalType(38,0)\nStringType\nStringType\nStringType\nStringType\nDecimalType(38,0)*\nDecimalType(38,0)\nDecimalType(38,0)\nDecimalType(38,0)\nDecimalType(38,0)\nStringType\nStringType\nStringType\nStringType\nDoubleType\nDoubleType\nDoubleType\nStringType\nStringType\nDoubleType\nStringType\nStringType\nStringType\nDateType\nDateType\nStringType\nStringType\nStringType\nStringType\nDateType\nTimestampType\nStringType\nTimestampType\nTimestampType\nStringType\nStringType\nStringType\nStringType\nTimestampType\nTimestampType\nStringType\nStringType\nStringType\nStringType\nStringType\nStringType\nStringType\nStringType\nStringType\nStringType\nNote that, for\nDecimalType(38,0)*\n, the table above intentionally does not cover all other combinations of scales and precisions because currently we only infer decimal type like\nBigInteger\n/\nBigInt\n. For example, 1.1 is inferred as double type.\nSince Spark 2.3, when either broadcast hash join or broadcast nested loop join is applicable, we prefer to broadcasting the table that is explicitly specified in a broadcast hint. For details, see the section\nJoin Strategy Hints for SQL Queries\nand\nSPARK-22489\n.\nSince Spark 2.3, when all inputs are binary,\nfunctions.concat()\nreturns an output as binary. Otherwise, it returns as a string. Until Spark 2.3, it always returns as a string despite of input types. To keep the old behavior, set\nspark.sql.function.concatBinaryAsString\nto\ntrue\n.\nSince Spark 2.3, when all inputs are binary, SQL\nelt()\nreturns an output as binary. Otherwise, it returns as a string. Until Spark 2.3, it always returns as a string despite of input types. To keep the old behavior, set\nspark.sql.function.eltOutputAsString\nto\ntrue\n.\nSince Spark 2.3, by default arithmetic operations between decimals return a rounded value if an exact representation is not possible (instead of returning NULL). This is compliant with SQL ANSI 2011 specification and Hive’s new behavior introduced in Hive 2.2 (HIVE-15331). This involves the following changes\nThe rules to determine the result type of an arithmetic operation have been updated. In particular, if the precision / scale needed are out of the range of available values, the scale is reduced up to 6, in order to prevent the truncation of the integer part of the decimals. All the arithmetic operations are affected by the change, i.e. addition (\n+\n), subtraction (\n-\n), multiplication (\n*\n), division (\n/\n), remainder (\n%\n) and positive modulus (\npmod\n).\nLiteral values used in SQL operations are converted to DECIMAL with the exact precision and scale needed by them.\nThe configuration\nspark.sql.decimalOperations.allowPrecisionLoss\nhas been introduced. It defaults to\ntrue\n, which means the new behavior described here; if set to\nfalse\n, Spark uses previous rules, i.e. it doesn’t adjust the needed scale to represent the values and it returns NULL if an exact representation of the value is not possible.\nUn-aliased subquery’s semantic has not been well defined with confusing behaviors. Since Spark 2.3, we invalidate such confusing cases, for example:\nSELECT v.i from (SELECT i FROM v)\n, Spark will throw an analysis exception in this case because users should not be able to use the qualifier inside a subquery. See\nSPARK-20690\nand\nSPARK-21335\nfor more details.\nWhen creating a\nSparkSession\nwith\nSparkSession.builder.getOrCreate()\n, if there is an existing\nSparkContext\n, the builder was trying to update the\nSparkConf\nof the existing\nSparkContext\nwith configurations specified to the builder, but the\nSparkContext\nis shared by all\nSparkSession\ns, so we should not update them. Since 2.3, the builder comes to not update the configurations. If you want to update them, you need to update them prior to creating a\nSparkSession\n.\nUpgrading from Spark SQL 2.1 to 2.2\nSpark 2.1.1 introduced a new configuration key:\nspark.sql.hive.caseSensitiveInferenceMode\n. It had a default setting of\nNEVER_INFER\n, which kept behavior identical to 2.1.0. However, Spark 2.2.0 changes this setting’s default value to\nINFER_AND_SAVE\nto restore compatibility with reading Hive metastore tables whose underlying file schema have mixed-case column names. With the\nINFER_AND_SAVE\nconfiguration value, on first access Spark will perform schema inference on any Hive metastore table for which it has not already saved an inferred schema. Note that schema inference can be a very time-consuming operation for tables with thousands of partitions. If compatibility with mixed-case column names is not a concern, you can safely set\nspark.sql.hive.caseSensitiveInferenceMode\nto\nNEVER_INFER\nto avoid the initial overhead of schema inference. Note that with the new default\nINFER_AND_SAVE\nsetting, the results of the schema inference are saved as a metastore key for future use. Therefore, the initial schema inference occurs only at a table’s first access.\nSince Spark 2.2.1 and 2.3.0, the schema is always inferred at runtime when the data source tables have the columns that exist in both partition schema and data schema. The inferred schema does not have the partitioned columns. When reading the table, Spark respects the partition values of these overlapping columns instead of the values stored in the data source files. In 2.2.0 and 2.1.x release, the inferred schema is partitioned but the data of the table is invisible to users (i.e., the result set is empty).\nSince Spark 2.2, view definitions are stored in a different way from prior versions. This may cause Spark unable to read views created by prior versions. In such cases, you need to recreate the views using\nALTER VIEW AS\nor\nCREATE OR REPLACE VIEW AS\nwith newer Spark versions.\nUpgrading from Spark SQL 2.0 to 2.1\nDatasource tables now store partition metadata in the Hive metastore. This means that Hive DDLs such as\nALTER TABLE PARTITION ... SET LOCATION\nare now available for tables created with the Datasource API.\nLegacy datasource tables can be migrated to this format via the\nMSCK REPAIR TABLE\ncommand. Migrating legacy tables is recommended to take advantage of Hive DDL support and improved planning performance.\nTo determine if a table has been migrated, look for the\nPartitionProvider: Catalog\nattribute when issuing\nDESCRIBE FORMATTED\non the table.\nChanges to\nINSERT OVERWRITE TABLE ... PARTITION ...\nbehavior for Datasource tables.\nIn prior Spark versions\nINSERT OVERWRITE\noverwrote the entire Datasource table, even when given a partition specification. Now only partitions matching the specification are overwritten.\nNote that this still differs from the behavior of Hive tables, which is to overwrite only partitions overlapping with newly inserted data.\nUpgrading from Spark SQL 1.6 to 2.0\nSparkSession\nis now the new entry point of Spark that replaces the old\nSQLContext\nand\nHiveContext\n. Note that the old SQLContext and HiveContext are kept for backward compatibility. A new\ncatalog\ninterface is accessible from\nSparkSession\n- existing API on databases and tables access such as\nlistTables\n,\ncreateExternalTable\n,\ndropTempView\n,\ncacheTable\nare moved here.\nDataset API and DataFrame API are unified. In Scala,\nDataFrame\nbecomes a type alias for\nDataset[Row]\n, while Java API users must replace\nDataFrame\nwith\nDataset<Row>\n. Both the typed\ntransformations (e.g.,\nmap\n,\nfilter\n, and\ngroupByKey\n) and untyped transformations (e.g.,\nselect\nand\ngroupBy\n) are available on the Dataset class. Since compile-time type-safety in\nPython and R is not a language feature, the concept of Dataset does not apply to these languages’\nAPIs. Instead,\nDataFrame\nremains the primary programming abstraction, which is analogous to the\nsingle-node data frame notion in these languages.\nDataset and DataFrame API\nunionAll\nhas been deprecated and replaced by\nunion\nDataset and DataFrame API\nexplode\nhas been deprecated, alternatively, use\nfunctions.explode()\nwith\nselect\nor\nflatMap\nDataset and DataFrame API\nregisterTempTable\nhas been deprecated and replaced by\ncreateOrReplaceTempView\nChanges to\nCREATE TABLE ... LOCATION\nbehavior for Hive tables.\nFrom Spark 2.0,\nCREATE TABLE ... LOCATION\nis equivalent to\nCREATE EXTERNAL TABLE ... LOCATION\nin order to prevent accidental dropping the existing data in the user-provided locations.\nThat means, a Hive table created in Spark SQL with the user-specified location is always a Hive external table.\nDropping external tables will not remove the data. Users are not allowed to specify the location for Hive managed tables.\nNote that this is different from the Hive behavior.\nAs a result,\nDROP TABLE\nstatements on those tables will not remove the data.\nspark.sql.parquet.cacheMetadata\nis no longer used.\nSee\nSPARK-13664\nfor details.\nUpgrading from Spark SQL 1.5 to 1.6\nFrom Spark 1.6, by default, the Thrift server runs in multi-session mode. Which means each JDBC/ODBC\nconnection owns a copy of their own SQL configuration and temporary function registry. Cached\ntables are still shared though. If you prefer to run the Thrift server in the old single-session\nmode, please set option\nspark.sql.hive.thriftServer.singleSession\nto\ntrue\n. You may either add\nthis option to\nspark-defaults.conf\n, or pass it to\nstart-thriftserver.sh\nvia\n--conf\n:\n./sbin/start-thriftserver.sh\n\\\n--conf\nspark.sql.hive.thriftServer.singleSession\n=\ntrue\n\\\n...\nFrom Spark 1.6, LongType casts to TimestampType expect seconds instead of microseconds. This\nchange was made to match the behavior of Hive 1.2 for more consistent type casting to TimestampType\nfrom numeric types. See\nSPARK-11724\nfor\ndetails.\nUpgrading from Spark SQL 1.4 to 1.5\nOptimized execution using manually managed memory (Tungsten) is now enabled by default, along with\ncode generation for expression evaluation. These features can both be disabled by setting\nspark.sql.tungsten.enabled\nto\nfalse\n.\nParquet schema merging is no longer enabled by default. It can be re-enabled by setting\nspark.sql.parquet.mergeSchema\nto\ntrue\n.\nIn-memory columnar storage partition pruning is on by default. It can be disabled by setting\nspark.sql.inMemoryColumnarStorage.partitionPruning\nto\nfalse\n.\nUnlimited precision decimal columns are no longer supported, instead Spark SQL enforces a maximum\nprecision of 38. When inferring schema from\nBigDecimal\nobjects, a precision of (38, 18) is now\nused. When no precision is specified in DDL then the default remains\nDecimal(10, 0)\n.\nTimestamps are now stored at a precision of 1us, rather than 1ns\nIn the\nsql\ndialect, floating point numbers are now parsed as decimal. HiveQL parsing remains\nunchanged.\nThe canonical name of SQL/DataFrame functions are now lower case (e.g., sum vs SUM).\nJSON data source will not automatically load new files that are created by other applications\n(i.e. files that are not inserted to the dataset through Spark SQL).\nFor a JSON persistent table (i.e. the metadata of the table is stored in Hive Metastore),\nusers can use\nREFRESH TABLE\nSQL command or\nHiveContext\n’s\nrefreshTable\nmethod\nto include those new files to the table. For a DataFrame representing a JSON dataset, users need to recreate\nthe DataFrame and the new DataFrame will include new files.\nUpgrading from Spark SQL 1.3 to 1.4\nDataFrame data reader/writer interface\nBased on user feedback, we created a new, more fluid API for reading data in (\nSQLContext.read\n)\nand writing data out (\nDataFrame.write\n),\nand deprecated the old APIs (e.g.,\nSQLContext.parquetFile\n,\nSQLContext.jsonFile\n).\nSee the API docs for\nSQLContext.read\n(\nScala\n,\nJava\n,\nPython\n) and\nDataFrame.write\n(\nScala\n,\nJava\n,\nPython\n) more information.\nDataFrame.groupBy retains grouping columns\nBased on user feedback, we changed the default behavior of\nDataFrame.groupBy().agg()\nto retain the\ngrouping columns in the resulting\nDataFrame\n. To keep the behavior in 1.3, set\nspark.sql.retainGroupColumns\nto\nfalse\n.\nimport\npyspark.sql.functions\nas\nfunc\n# In 1.3.x, in order for the grouping column \"department\" to show up,\n# it must be included explicitly as part of the agg function call.\ndf\n.\ngroupBy\n(\n\"\ndepartment\n\"\n).\nagg\n(\ndf\n[\n\"\ndepartment\n\"\n],\nfunc\n.\nmax\n(\n\"\nage\n\"\n),\nfunc\n.\nsum\n(\n\"\nexpense\n\"\n))\n# In 1.4+, grouping column \"department\" is included automatically.\ndf\n.\ngroupBy\n(\n\"\ndepartment\n\"\n).\nagg\n(\nfunc\n.\nmax\n(\n\"\nage\n\"\n),\nfunc\n.\nsum\n(\n\"\nexpense\n\"\n))\n# Revert to 1.3.x behavior (not retaining grouping column) by:\nsqlContext\n.\nsetConf\n(\n\"\nspark.sql.retainGroupColumns\n\"\n,\n\"\nfalse\n\"\n)\n// In 1.3.x, in order for the grouping column \"department\" to show up,\n// it must be included explicitly as part of the agg function call.\ndf\n.\ngroupBy\n(\n\"department\"\n).\nagg\n(\n$\n\"department\"\n,\nmax\n(\n\"age\"\n),\nsum\n(\n\"expense\"\n))\n// In 1.4+, grouping column \"department\" is included automatically.\ndf\n.\ngroupBy\n(\n\"department\"\n).\nagg\n(\nmax\n(\n\"age\"\n),\nsum\n(\n\"expense\"\n))\n// Revert to 1.3 behavior (not retaining grouping column) by:\nsqlContext\n.\nsetConf\n(\n\"spark.sql.retainGroupColumns\"\n,\n\"false\"\n)\n// In 1.3.x, in order for the grouping column \"department\" to show up,\n// it must be included explicitly as part of the agg function call.\ndf\n.\ngroupBy\n(\n\"department\"\n).\nagg\n(\ncol\n(\n\"department\"\n),\nmax\n(\n\"age\"\n),\nsum\n(\n\"expense\"\n));\n// In 1.4+, grouping column \"department\" is included automatically.\ndf\n.\ngroupBy\n(\n\"department\"\n).\nagg\n(\nmax\n(\n\"age\"\n),\nsum\n(\n\"expense\"\n));\n// Revert to 1.3 behavior (not retaining grouping column) by:\nsqlContext\n.\nsetConf\n(\n\"spark.sql.retainGroupColumns\"\n,\n\"false\"\n);\nBehavior change on DataFrame.withColumn\nPrior to 1.4, DataFrame.withColumn() supports adding a column only. The column will always be added\nas a new column with its specified name in the result DataFrame even if there may be any existing\ncolumns of the same name. Since 1.4, DataFrame.withColumn() supports adding a column of a different\nname from names of all existing columns or replacing existing columns of the same name.\nNote that this change is only for Scala API, not for PySpark and SparkR.\nUpgrading from Spark SQL 1.0-1.2 to 1.3\nIn Spark 1.3 we removed the “Alpha” label from Spark SQL and as part of this did a cleanup of the\navailable APIs. From Spark 1.3 onwards, Spark SQL will provide binary compatibility with other\nreleases in the 1.X series. This compatibility guarantee excludes APIs that are explicitly marked\nas unstable (i.e., DeveloperAPI or Experimental).\nRename of SchemaRDD to DataFrame\nThe largest change that users will notice when upgrading to Spark SQL 1.3 is that\nSchemaRDD\nhas\nbeen renamed to\nDataFrame\n. This is primarily because DataFrames no longer inherit from RDD\ndirectly, but instead provide most of the functionality that RDDs provide though their own\nimplementation. DataFrames can still be converted to RDDs by calling the\n.rdd\nmethod.\nIn Scala, there is a type alias from\nSchemaRDD\nto\nDataFrame\nto provide source compatibility for\nsome use cases. It is still recommended that users update their code to use\nDataFrame\ninstead.\nJava and Python users will need to update their code.\nUnification of the Java and Scala APIs\nPrior to Spark 1.3 there were separate Java compatible classes (\nJavaSQLContext\nand\nJavaSchemaRDD\n)\nthat mirrored the Scala API. In Spark 1.3 the Java API and Scala API have been unified. Users\nof either language should use\nSQLContext\nand\nDataFrame\n. In general these classes try to\nuse types that are usable from both languages (i.e.\nArray\ninstead of language-specific collections).\nIn some cases where no common type exists (e.g., for passing in closures or Maps) function overloading\nis used instead.\nAdditionally, the Java specific types API has been removed. Users of both Scala and Java should\nuse the classes present in\norg.apache.spark.sql.types\nto describe schema programmatically.\nIsolation of Implicit Conversions and Removal of dsl Package (Scala-only)\nMany of the code examples prior to Spark 1.3 started with\nimport sqlContext._\n, which brought\nall of the functions from sqlContext into scope. In Spark 1.3 we have isolated the implicit\nconversions for converting\nRDD\ns into\nDataFrame\ns into an object inside of the\nSQLContext\n.\nUsers should now write\nimport sqlContext.implicits._\n.\nAdditionally, the implicit conversions now only augment RDDs that are composed of\nProduct\ns (i.e.,\ncase classes or tuples) with a method\ntoDF\n, instead of applying automatically.\nWhen using function inside of the DSL (now replaced with the\nDataFrame\nAPI) users used to import\norg.apache.spark.sql.catalyst.dsl\n. Instead the public dataframe functions API should be used:\nimport org.apache.spark.sql.functions._\n.\nRemoval of the type aliases in org.apache.spark.sql for DataType (Scala-only)\nSpark 1.3 removes the type aliases that were present in the base sql package for\nDataType\n. Users\nshould instead import the classes in\norg.apache.spark.sql.types\nUDF Registration Moved to\nsqlContext.udf\n(Java & Scala)\nFunctions that are used to register UDFs, either for use in the DataFrame DSL or SQL, have been\nmoved into the udf object in\nSQLContext\n.\nsqlContext\n.\nudf\n.\nregister\n(\n\"strLen\"\n,\n(\ns\n:\nString\n)\n=>\ns\n.\nlength\n())\nsqlContext\n.\nudf\n().\nregister\n(\n\"strLen\"\n,\n(\nString\ns\n)\n->\ns\n.\nlength\n(),\nDataTypes\n.\nIntegerType\n);\nPython UDF registration is unchanged.\nCompatibility with Apache Hive\nSpark SQL is designed to be compatible with the Hive Metastore, SerDes and UDFs.\nCurrently, Hive SerDes and UDFs are based on built-in Hive,\nand Spark SQL can be connected to different versions of Hive Metastore\n(from 2.0.0 to 2.3.10 and 3.0.0 to 3.1.3. Also see\nInteracting with Different Versions of Hive Metastore\n).\nDeploying in Existing Hive Warehouses\nThe Spark SQL Thrift JDBC server is designed to be “out of the box” compatible with existing Hive\ninstallations. You do not need to modify your existing Hive Metastore or change the data placement\nor partitioning of your tables.\nSupported Hive Features\nSpark SQL supports the vast majority of Hive features, such as:\nHive query statements, including:\nSELECT\nGROUP BY\nORDER BY\nDISTRIBUTE BY\nCLUSTER BY\nSORT BY\nAll Hive operators, including:\nRelational operators (\n=\n,\n<=>\n,\n==\n,\n<>\n,\n<\n,\n>\n,\n>=\n,\n<=\n, etc)\nArithmetic operators (\n+\n,\n-\n,\n*\n,\n/\n,\n%\n, etc)\nLogical operators (\nAND\n,\nOR\n, etc)\nComplex type constructors\nMathematical functions (\nsign\n,\nln\n,\ncos\n, etc)\nString functions (\ninstr\n,\nlength\n,\nprintf\n, etc)\nUser defined functions (UDF)\nUser defined aggregation functions (UDAF)\nUser defined serialization formats (SerDes)\nWindow functions\nJoins\nJOIN\n{LEFT|RIGHT|FULL} OUTER JOIN\nLEFT SEMI JOIN\nLEFT ANTI JOIN\nCROSS JOIN\nUnions\nSub-queries\nSub-queries in the FROM Clause\nSELECT col FROM (SELECT a + b AS col FROM t1) t2\nSub-queries in WHERE Clause\nCorrelated or non-correlated IN and NOT IN statement in WHERE Clause\nSELECT col FROM t1 WHERE col IN (SELECT a FROM t2 WHERE t1.a = t2.a)\nSELECT col FROM t1 WHERE col IN (SELECT a FROM t2)\nCorrelated or non-correlated EXISTS and NOT EXISTS statement in WHERE Clause\nSELECT col FROM t1 WHERE EXISTS (SELECT t2.a FROM t2 WHERE t1.a = t2.a AND t2.a > 10)\nSELECT col FROM t1 WHERE EXISTS (SELECT t2.a FROM t2 WHERE t2.a > 10)\nNon-correlated IN and NOT IN statement in JOIN Condition\nSELECT t1.col FROM t1 JOIN t2 ON t1.a = t2.a AND t1.a IN (SELECT a FROM t3)\nNon-correlated EXISTS and NOT EXISTS statement in JOIN Condition\nSELECT t1.col FROM t1 JOIN t2 ON t1.a = t2.a AND EXISTS (SELECT * FROM t3 WHERE t3.a > 10)\nSampling\nExplain\nPartitioned tables including dynamic partition insertion\nView\nIf column aliases are not specified in view definition queries, both Spark and Hive will\ngenerate alias names, but in different ways. In order for Spark to be able to read views created\nby Hive, users should explicitly specify column aliases in view definition queries. As an\nexample, Spark cannot read\nv1\ncreated as below by Hive.\nCREATE VIEW v1 AS SELECT * FROM (SELECT c + 1 FROM (SELECT 1 c) t1) t2;\nInstead, you should create\nv1\nas below with column aliases explicitly specified.\nCREATE VIEW v1 AS SELECT * FROM (SELECT c + 1 AS inc_c FROM (SELECT 1 c) t1) t2;\nAll Hive DDL Functions, including:\nCREATE TABLE\nCREATE TABLE AS SELECT\nCREATE TABLE LIKE\nALTER TABLE\nMost Hive Data types, including:\nTINYINT\nSMALLINT\nINT\nBIGINT\nBOOLEAN\nFLOAT\nDOUBLE\nSTRING\nBINARY\nTIMESTAMP\nDATE\nARRAY<>\nMAP<>\nSTRUCT<>\nUnsupported Hive Functionality\nBelow is a list of Hive features that we don’t support yet. Most of these features are rarely used\nin Hive deployments.\nEsoteric Hive Features\nUNION\ntype\nUnique join\nColumn statistics collecting: Spark SQL does not piggyback scans to collect column statistics at\nthe moment and only supports populating the sizeInBytes field of the hive metastore.\nHive Input/Output Formats\nFile format for CLI: For results showing back to the CLI, Spark SQL only supports TextOutputFormat.\nHadoop archive\nHive Optimizations\nA handful of Hive optimizations are not yet included in Spark. Some of these (such as indexes) are\nless important due to Spark SQL’s in-memory computational model. Others are slotted for future\nreleases of Spark SQL.\nBlock-level bitmap indexes and virtual columns (used to build indexes)\nAutomatically determine the number of reducers for joins and groupbys: Currently, in Spark SQL, you\nneed to control the degree of parallelism post-shuffle using “\nSET spark.sql.shuffle.partitions=[num_tasks];\n”.\nMeta-data only query: For queries that can be answered by using only metadata, Spark SQL still\nlaunches tasks to compute the result.\nSkew data flag: Spark SQL does not follow the skew data flags in Hive.\nSTREAMTABLE\nhint in join: Spark SQL does not follow the\nSTREAMTABLE\nhint.\nMerge multiple small files for query results: if the result output contains multiple small files,\nHive can optionally merge the small files into fewer large files to avoid overflowing the HDFS\nmetadata. Spark SQL does not support that.\nHive UDF/UDTF/UDAF\nNot all the APIs of the Hive UDF/UDTF/UDAF are supported by Spark SQL. Below are the unsupported APIs:\ngetRequiredJars\nand\ngetRequiredFiles\n(\nUDF\nand\nGenericUDF\n) are functions to automatically\ninclude additional resources required by this UDF.\ninitialize(StructObjectInspector)\nin\nGenericUDTF\nis not supported yet. Spark SQL currently uses\na deprecated interface\ninitialize(ObjectInspector[])\nonly.\nconfigure\n(\nGenericUDF\n,\nGenericUDTF\n, and\nGenericUDAFEvaluator\n) is a function to initialize\nfunctions with\nMapredContext\n, which is inapplicable to Spark.\nclose\n(\nGenericUDF\nand\nGenericUDAFEvaluator\n) is a function to release associated resources.\nSpark SQL does not call this function when tasks finish.\nreset\n(\nGenericUDAFEvaluator\n) is a function to re-initialize aggregation for reusing the same aggregation.\nSpark SQL currently does not support the reuse of aggregation.\ngetWindowingEvaluator\n(\nGenericUDAFEvaluator\n) is a function to optimize aggregation by evaluating\nan aggregate over a fixed window.\nIncompatible Hive UDF\nBelow are the scenarios in which Hive and Spark generate different results:\nSQRT(n)\nIf n < 0, Hive returns null, Spark SQL returns NaN.\nACOS(n)\nIf n < -1 or n > 1, Hive returns null, Spark SQL returns NaN.\nASIN(n)\nIf n < -1 or n > 1, Hive returns null, Spark SQL returns NaN.\nCAST(n AS TIMESTAMP)\nIf n is integral numbers, Hive treats n as milliseconds, Spark SQL treats n as seconds."}
{"url": "https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nEvaluation Metrics - RDD-based API\nClassification model evaluation\nBinary classification\nThreshold tuning\nMulticlass classification\nLabel based metrics\nMultilabel classification\nRanking systems\nRegression model evaluation\nspark.mllib\ncomes with a number of machine learning algorithms that can be used to learn from and make predictions\non data. When these algorithms are applied to build machine learning models, there is a need to evaluate the performance\nof the model on some criteria, which depends on the application and its requirements.\nspark.mllib\nalso provides a\nsuite of metrics for the purpose of evaluating the performance of machine learning models.\nSpecific machine learning algorithms fall under broader types of machine learning applications like classification,\nregression, clustering, etc. Each of these types have well-established metrics for performance evaluation and those\nmetrics that are currently available in\nspark.mllib\nare detailed in this section.\nClassification model evaluation\nWhile there are many different types of classification algorithms, the evaluation of classification models all share\nsimilar principles. In a\nsupervised classification problem\n,\nthere exists a true output and a model-generated predicted output for each data point. For this reason, the results for\neach data point can be assigned to one of four categories:\nTrue Positive (TP) - label is positive and prediction is also positive\nTrue Negative (TN) - label is negative and prediction is also negative\nFalse Positive (FP) - label is negative but prediction is positive\nFalse Negative (FN) - label is positive but prediction is negative\nThese four numbers are the building blocks for most classifier evaluation metrics. A fundamental point when considering\nclassifier evaluation is that pure accuracy (i.e. was the prediction correct or incorrect) is not generally a good metric. The\nreason for this is because a dataset may be highly unbalanced. For example, if a model is designed to predict fraud from\na dataset where 95% of the data points are\nnot fraud\nand 5% of the data points are\nfraud\n, then a naive classifier\nthat predicts\nnot fraud\n, regardless of input, will be 95% accurate. For this reason, metrics like\nprecision and recall\nare typically used because they take into\naccount the\ntype\nof error. In most applications there is some desired balance between precision and recall, which can\nbe captured by combining the two into a single metric, called the\nF-measure\n.\nBinary classification\nBinary classifiers\nare used to separate the elements of a given\ndataset into one of two possible groups (e.g. fraud or not fraud) and is a special case of multiclass classification.\nMost binary classification metrics can be generalized to multiclass classification metrics.\nThreshold tuning\nIt is import to understand that many classification models actually output a “score” (often times a probability) for\neach class, where a higher score indicates higher likelihood. In the binary case, the model may output a probability for\neach class: $P(Y=1|X)$ and $P(Y=0|X)$. Instead of simply taking the higher probability, there may be some cases where\nthe model might need to be tuned so that it only predicts a class when the probability is very high (e.g. only block a\ncredit card transaction if the model predicts fraud with >90% probability). Therefore, there is a prediction\nthreshold\nwhich determines what the predicted class will be based on the probabilities that the model outputs.\nTuning the prediction threshold will change the precision and recall of the model and is an important part of model\noptimization. In order to visualize how precision, recall, and other metrics change as a function of the threshold it is\ncommon practice to plot competing metrics against one another, parameterized by threshold. A P-R curve plots (precision,\nrecall) points for different threshold values, while a\nreceiver operating characteristic\n, or ROC, curve\nplots (recall, false positive rate) points.\nAvailable metrics\nMetric\nDefinition\nPrecision (Positive Predictive Value)\n$PPV=\\frac{TP}{TP + FP}$\nRecall (True Positive Rate)\n$TPR=\\frac{TP}{P}=\\frac{TP}{TP + FN}$\nF-measure\n$F(\\beta) = \\left(1 + \\beta^2\\right) \\cdot \\left(\\frac{PPV \\cdot TPR}\n          {\\beta^2 \\cdot PPV + TPR}\\right)$\nReceiver Operating Characteristic (ROC)\n$FPR(T)=\\int^\\infty_{T} P_0(T)\\,dT \\\\ TPR(T)=\\int^\\infty_{T} P_1(T)\\,dT$\nArea Under ROC Curve\n$AUROC=\\int^1_{0} \\frac{TP}{P} d\\left(\\frac{FP}{N}\\right)$\nArea Under Precision-Recall Curve\n$AUPRC=\\int^1_{0} \\frac{TP}{TP+FP} d\\left(\\frac{TP}{P}\\right)$\nExamples\nThe following code snippets illustrate how to load a sample dataset, train a binary classification algorithm on the\ndata, and evaluate the performance of the algorithm by several binary evaluation metrics.\nRefer to the\nBinaryClassificationMetrics\nPython docs\nand\nLogisticRegressionWithLBFGS\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.classification\nimport\nLogisticRegressionWithLBFGS\nfrom\npyspark.mllib.evaluation\nimport\nBinaryClassificationMetrics\nfrom\npyspark.mllib.util\nimport\nMLUtils\n# Several of the methods available in scala are currently missing from pyspark\n# Load training data in LIBSVM format\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"\ndata/mllib/sample_binary_classification_data.txt\n\"\n)\n# Split data into training (60%) and test (40%)\ntraining\n,\ntest\n=\ndata\n.\nrandomSplit\n([\n0.6\n,\n0.4\n],\nseed\n=\n11\n)\ntraining\n.\ncache\n()\n# Run training algorithm to build the model\nmodel\n=\nLogisticRegressionWithLBFGS\n.\ntrain\n(\ntraining\n)\n# Compute raw scores on the test set\npredictionAndLabels\n=\ntest\n.\nmap\n(\nlambda\nlp\n:\n(\nfloat\n(\nmodel\n.\npredict\n(\nlp\n.\nfeatures\n)),\nlp\n.\nlabel\n))\n# Instantiate metrics object\nmetrics\n=\nBinaryClassificationMetrics\n(\npredictionAndLabels\n)\n# Area under precision-recall curve\nprint\n(\n\"\nArea under PR = %s\n\"\n%\nmetrics\n.\nareaUnderPR\n)\n# Area under ROC curve\nprint\n(\n\"\nArea under ROC = %s\n\"\n%\nmetrics\n.\nareaUnderROC\n)\nFind full example code at \"examples/src/main/python/mllib/binary_classification_metrics_example.py\" in the Spark repo.\nRefer to the\nLogisticRegressionWithLBFGS\nScala docs\nand\nBinaryClassificationMetrics\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.classification.LogisticRegressionWithLBFGS\nimport\norg.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nimport\norg.apache.spark.mllib.regression.LabeledPoint\nimport\norg.apache.spark.mllib.util.MLUtils\n// Load training data in LIBSVM format\nval\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_binary_classification_data.txt\"\n)\n// Split data into training (60%) and test (40%)\nval\nArray\n(\ntraining\n,\ntest\n)\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.6\n,\n0.4\n),\nseed\n=\n11L\n)\ntraining\n.\ncache\n()\n// Run training algorithm to build the model\nval\nmodel\n=\nnew\nLogisticRegressionWithLBFGS\n()\n.\nsetNumClasses\n(\n2\n)\n.\nrun\n(\ntraining\n)\n// Clear the prediction threshold so the model will return probabilities\nmodel\n.\nclearThreshold\n()\n// Compute raw scores on the test set\nval\npredictionAndLabels\n=\ntest\n.\nmap\n{\ncase\nLabeledPoint\n(\nlabel\n,\nfeatures\n)\n=>\nval\nprediction\n=\nmodel\n.\npredict\n(\nfeatures\n)\n(\nprediction\n,\nlabel\n)\n}\n// Instantiate metrics object\nval\nmetrics\n=\nnew\nBinaryClassificationMetrics\n(\npredictionAndLabels\n)\n// Precision by threshold\nval\nprecision\n=\nmetrics\n.\nprecisionByThreshold\n()\nprecision\n.\ncollect\n().\nforeach\n{\ncase\n(\nt\n,\np\n)\n=>\nprintln\n(\ns\n\"Threshold: $t, Precision: $p\"\n)\n}\n// Recall by threshold\nval\nrecall\n=\nmetrics\n.\nrecallByThreshold\n()\nrecall\n.\ncollect\n().\nforeach\n{\ncase\n(\nt\n,\nr\n)\n=>\nprintln\n(\ns\n\"Threshold: $t, Recall: $r\"\n)\n}\n// Precision-Recall Curve\nval\nPRC\n=\nmetrics\n.\npr\n()\n// F-measure\nval\nf1Score\n=\nmetrics\n.\nfMeasureByThreshold\n()\nf1Score\n.\ncollect\n().\nforeach\n{\ncase\n(\nt\n,\nf\n)\n=>\nprintln\n(\ns\n\"Threshold: $t, F-score: $f, Beta = 1\"\n)\n}\nval\nbeta\n=\n0.5\nval\nfScore\n=\nmetrics\n.\nfMeasureByThreshold\n(\nbeta\n)\nfScore\n.\ncollect\n().\nforeach\n{\ncase\n(\nt\n,\nf\n)\n=>\nprintln\n(\ns\n\"Threshold: $t, F-score: $f, Beta = 0.5\"\n)\n}\n// AUPRC\nval\nauPRC\n=\nmetrics\n.\nareaUnderPR\n()\nprintln\n(\ns\n\"Area under precision-recall curve = $auPRC\"\n)\n// Compute thresholds used in ROC and PR curves\nval\nthresholds\n=\nprecision\n.\nmap\n(\n_\n.\n_1\n)\n// ROC Curve\nval\nroc\n=\nmetrics\n.\nroc\n()\n// AUROC\nval\nauROC\n=\nmetrics\n.\nareaUnderROC\n()\nprintln\n(\ns\n\"Area under ROC = $auROC\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\" in the Spark repo.\nRefer to the\nLogisticRegressionModel\nJava docs\nand\nLogisticRegressionWithLBFGS\nJava docs\nfor details on the API.\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.api.java.*\n;\nimport\norg.apache.spark.mllib.classification.LogisticRegressionModel\n;\nimport\norg.apache.spark.mllib.classification.LogisticRegressionWithLBFGS\n;\nimport\norg.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n;\nimport\norg.apache.spark.mllib.regression.LabeledPoint\n;\nimport\norg.apache.spark.mllib.util.MLUtils\n;\nString\npath\n=\n\"data/mllib/sample_binary_classification_data.txt\"\n;\nJavaRDD\n<\nLabeledPoint\n>\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\npath\n).\ntoJavaRDD\n();\n// Split initial RDD into two... [60% training data, 40% testing data].\nJavaRDD\n<\nLabeledPoint\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]{\n0.6\n,\n0.4\n},\n11L\n);\nJavaRDD\n<\nLabeledPoint\n>\ntraining\n=\nsplits\n[\n0\n].\ncache\n();\nJavaRDD\n<\nLabeledPoint\n>\ntest\n=\nsplits\n[\n1\n];\n// Run training algorithm to build the model.\nLogisticRegressionModel\nmodel\n=\nnew\nLogisticRegressionWithLBFGS\n()\n.\nsetNumClasses\n(\n2\n)\n.\nrun\n(\ntraining\n.\nrdd\n());\n// Clear the prediction threshold so the model will return probabilities\nmodel\n.\nclearThreshold\n();\n// Compute raw scores on the test set.\nJavaPairRDD\n<\nObject\n,\nObject\n>\npredictionAndLabels\n=\ntest\n.\nmapToPair\n(\np\n->\nnew\nTuple2\n<>(\nmodel\n.\npredict\n(\np\n.\nfeatures\n()),\np\n.\nlabel\n()));\n// Get evaluation metrics.\nBinaryClassificationMetrics\nmetrics\n=\nnew\nBinaryClassificationMetrics\n(\npredictionAndLabels\n.\nrdd\n());\n// Precision by threshold\nJavaRDD\n<\nTuple2\n<\nObject\n,\nObject\n>>\nprecision\n=\nmetrics\n.\nprecisionByThreshold\n().\ntoJavaRDD\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Precision by threshold: \"\n+\nprecision\n.\ncollect\n());\n// Recall by threshold\nJavaRDD\n<?>\nrecall\n=\nmetrics\n.\nrecallByThreshold\n().\ntoJavaRDD\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Recall by threshold: \"\n+\nrecall\n.\ncollect\n());\n// F Score by threshold\nJavaRDD\n<?>\nf1Score\n=\nmetrics\n.\nfMeasureByThreshold\n().\ntoJavaRDD\n();\nSystem\n.\nout\n.\nprintln\n(\n\"F1 Score by threshold: \"\n+\nf1Score\n.\ncollect\n());\nJavaRDD\n<?>\nf2Score\n=\nmetrics\n.\nfMeasureByThreshold\n(\n2.0\n).\ntoJavaRDD\n();\nSystem\n.\nout\n.\nprintln\n(\n\"F2 Score by threshold: \"\n+\nf2Score\n.\ncollect\n());\n// Precision-recall curve\nJavaRDD\n<?>\nprc\n=\nmetrics\n.\npr\n().\ntoJavaRDD\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Precision-recall curve: \"\n+\nprc\n.\ncollect\n());\n// Thresholds\nJavaRDD\n<\nDouble\n>\nthresholds\n=\nprecision\n.\nmap\n(\nt\n->\nDouble\n.\nparseDouble\n(\nt\n.\n_1\n().\ntoString\n()));\n// ROC Curve\nJavaRDD\n<?>\nroc\n=\nmetrics\n.\nroc\n().\ntoJavaRDD\n();\nSystem\n.\nout\n.\nprintln\n(\n\"ROC curve: \"\n+\nroc\n.\ncollect\n());\n// AUPRC\nSystem\n.\nout\n.\nprintln\n(\n\"Area under precision-recall curve = \"\n+\nmetrics\n.\nareaUnderPR\n());\n// AUROC\nSystem\n.\nout\n.\nprintln\n(\n\"Area under ROC = \"\n+\nmetrics\n.\nareaUnderROC\n());\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/LogisticRegressionModel\"\n);\nLogisticRegressionModel\n.\nload\n(\nsc\n,\n\"target/tmp/LogisticRegressionModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\" in the Spark repo.\nMulticlass classification\nA\nmulticlass classification\ndescribes a classification\nproblem where there are $M \\gt 2$ possible labels for each data point (the case where $M=2$ is the binary\nclassification problem). For example, classifying handwriting samples to the digits 0 to 9, having 10 possible classes.\nFor multiclass metrics, the notion of positives and negatives is slightly different. Predictions and labels can still\nbe positive or negative, but they must be considered under the context of a particular class. Each label and prediction\ntake on the value of one of the multiple classes and so they are said to be positive for their particular class and negative\nfor all other classes. So, a true positive occurs whenever the prediction and the label match, while a true negative\noccurs when neither the prediction nor the label take on the value of a given class. By this convention, there can be\nmultiple true negatives for a given data sample. The extension of false negatives and false positives from the former\ndefinitions of positive and negative labels is straightforward.\nLabel based metrics\nOpposed to binary classification where there are only two possible labels, multiclass classification problems have many\npossible labels and so the concept of label-based metrics is introduced. Accuracy measures precision across all\nlabels -  the number of times any class was predicted correctly (true positives) normalized by the number of data\npoints. Precision by label considers only one class, and measures the number of time a specific label was predicted\ncorrectly normalized by the number of times that label appears in the output.\nAvailable metrics\nDefine the class, or label, set as\n\\[L = \\{\\ell_0, \\ell_1, \\ldots, \\ell_{M-1} \\}\\]\nThe true output vector $\\mathbf{y}$ consists of $N$ elements\n\\[\\mathbf{y}_0, \\mathbf{y}_1, \\ldots, \\mathbf{y}_{N-1} \\in L\\]\nA multiclass prediction algorithm generates a prediction vector $\\hat{\\mathbf{y}}$ of $N$ elements\n\\[\\hat{\\mathbf{y}}_0, \\hat{\\mathbf{y}}_1, \\ldots, \\hat{\\mathbf{y}}_{N-1} \\in L\\]\nFor this section, a modified delta function $\\hat{\\delta}(x)$ will prove useful\n\\[\\hat{\\delta}(x) = \\begin{cases}1 & \\text{if $x = 0$}, \\\\ 0 & \\text{otherwise}.\\end{cases}\\]\nMetric\nDefinition\nConfusion Matrix\n$C_{ij} = \\sum_{k=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_k-\\ell_i) \\cdot \\hat{\\delta}(\\hat{\\mathbf{y}}_k - \\ell_j)\\\\ \\\\\n         \\left( \\begin{array}{ccc}\n         \\sum_{k=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_k-\\ell_1) \\cdot \\hat{\\delta}(\\hat{\\mathbf{y}}_k - \\ell_1) & \\ldots &\n         \\sum_{k=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_k-\\ell_1) \\cdot \\hat{\\delta}(\\hat{\\mathbf{y}}_k - \\ell_N) \\\\\n         \\vdots & \\ddots & \\vdots \\\\\n         \\sum_{k=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_k-\\ell_N) \\cdot \\hat{\\delta}(\\hat{\\mathbf{y}}_k - \\ell_1) & \\ldots &\n         \\sum_{k=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_k-\\ell_N) \\cdot \\hat{\\delta}(\\hat{\\mathbf{y}}_k - \\ell_N)\n         \\end{array} \\right)$\nAccuracy\n$ACC = \\frac{TP}{TP + FP} = \\frac{1}{N}\\sum_{i=0}^{N-1} \\hat{\\delta}\\left(\\hat{\\mathbf{y}}_i -\n        \\mathbf{y}_i\\right)$\nPrecision by label\n$PPV(\\ell) = \\frac{TP}{TP + FP} =\n          \\frac{\\sum_{i=0}^{N-1} \\hat{\\delta}(\\hat{\\mathbf{y}}_i - \\ell) \\cdot \\hat{\\delta}(\\mathbf{y}_i - \\ell)}\n          {\\sum_{i=0}^{N-1} \\hat{\\delta}(\\hat{\\mathbf{y}}_i - \\ell)}$\nRecall by label\n$TPR(\\ell)=\\frac{TP}{P} =\n          \\frac{\\sum_{i=0}^{N-1} \\hat{\\delta}(\\hat{\\mathbf{y}}_i - \\ell) \\cdot \\hat{\\delta}(\\mathbf{y}_i - \\ell)}\n          {\\sum_{i=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_i - \\ell)}$\nF-measure by label\n$F(\\beta, \\ell) = \\left(1 + \\beta^2\\right) \\cdot \\left(\\frac{PPV(\\ell) \\cdot TPR(\\ell)}\n          {\\beta^2 \\cdot PPV(\\ell) + TPR(\\ell)}\\right)$\nWeighted precision\n$PPV_{w}= \\frac{1}{N} \\sum\\nolimits_{\\ell \\in L} PPV(\\ell)\n          \\cdot \\sum_{i=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_i-\\ell)$\nWeighted recall\n$TPR_{w}= \\frac{1}{N} \\sum\\nolimits_{\\ell \\in L} TPR(\\ell)\n          \\cdot \\sum_{i=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_i-\\ell)$\nWeighted F-measure\n$F_{w}(\\beta)= \\frac{1}{N} \\sum\\nolimits_{\\ell \\in L} F(\\beta, \\ell)\n          \\cdot \\sum_{i=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_i-\\ell)$\nExamples\nThe following code snippets illustrate how to load a sample dataset, train a multiclass classification algorithm on\nthe data, and evaluate the performance of the algorithm by several multiclass classification evaluation metrics.\nRefer to the\nMulticlassMetrics\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.classification\nimport\nLogisticRegressionWithLBFGS\nfrom\npyspark.mllib.util\nimport\nMLUtils\nfrom\npyspark.mllib.evaluation\nimport\nMulticlassMetrics\n# Load training data in LIBSVM format\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"\ndata/mllib/sample_multiclass_classification_data.txt\n\"\n)\n# Split data into training (60%) and test (40%)\ntraining\n,\ntest\n=\ndata\n.\nrandomSplit\n([\n0.6\n,\n0.4\n],\nseed\n=\n11\n)\ntraining\n.\ncache\n()\n# Run training algorithm to build the model\nmodel\n=\nLogisticRegressionWithLBFGS\n.\ntrain\n(\ntraining\n,\nnumClasses\n=\n3\n)\n# Compute raw scores on the test set\npredictionAndLabels\n=\ntest\n.\nmap\n(\nlambda\nlp\n:\n(\nfloat\n(\nmodel\n.\npredict\n(\nlp\n.\nfeatures\n)),\nlp\n.\nlabel\n))\n# Instantiate metrics object\nmetrics\n=\nMulticlassMetrics\n(\npredictionAndLabels\n)\n# Overall statistics\nprecision\n=\nmetrics\n.\nprecision\n(\n1.0\n)\nrecall\n=\nmetrics\n.\nrecall\n(\n1.0\n)\nf1Score\n=\nmetrics\n.\nfMeasure\n(\n1.0\n)\nprint\n(\n\"\nSummary Stats\n\"\n)\nprint\n(\n\"\nPrecision = %s\n\"\n%\nprecision\n)\nprint\n(\n\"\nRecall = %s\n\"\n%\nrecall\n)\nprint\n(\n\"\nF1 Score = %s\n\"\n%\nf1Score\n)\n# Statistics by class\nlabels\n=\ndata\n.\nmap\n(\nlambda\nlp\n:\nlp\n.\nlabel\n).\ndistinct\n().\ncollect\n()\nfor\nlabel\nin\nsorted\n(\nlabels\n):\nprint\n(\n\"\nClass %s precision = %s\n\"\n%\n(\nlabel\n,\nmetrics\n.\nprecision\n(\nlabel\n)))\nprint\n(\n\"\nClass %s recall = %s\n\"\n%\n(\nlabel\n,\nmetrics\n.\nrecall\n(\nlabel\n)))\nprint\n(\n\"\nClass %s F1 Measure = %s\n\"\n%\n(\nlabel\n,\nmetrics\n.\nfMeasure\n(\nlabel\n,\nbeta\n=\n1.0\n)))\n# Weighted stats\nprint\n(\n\"\nWeighted recall = %s\n\"\n%\nmetrics\n.\nweightedRecall\n)\nprint\n(\n\"\nWeighted precision = %s\n\"\n%\nmetrics\n.\nweightedPrecision\n)\nprint\n(\n\"\nWeighted F(1) Score = %s\n\"\n%\nmetrics\n.\nweightedFMeasure\n())\nprint\n(\n\"\nWeighted F(0.5) Score = %s\n\"\n%\nmetrics\n.\nweightedFMeasure\n(\nbeta\n=\n0.5\n))\nprint\n(\n\"\nWeighted false positive rate = %s\n\"\n%\nmetrics\n.\nweightedFalsePositiveRate\n)\nFind full example code at \"examples/src/main/python/mllib/multi_class_metrics_example.py\" in the Spark repo.\nRefer to the\nMulticlassMetrics\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.classification.LogisticRegressionWithLBFGS\nimport\norg.apache.spark.mllib.evaluation.MulticlassMetrics\nimport\norg.apache.spark.mllib.regression.LabeledPoint\nimport\norg.apache.spark.mllib.util.MLUtils\n// Load training data in LIBSVM format\nval\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_multiclass_classification_data.txt\"\n)\n// Split data into training (60%) and test (40%)\nval\nArray\n(\ntraining\n,\ntest\n)\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.6\n,\n0.4\n),\nseed\n=\n11L\n)\ntraining\n.\ncache\n()\n// Run training algorithm to build the model\nval\nmodel\n=\nnew\nLogisticRegressionWithLBFGS\n()\n.\nsetNumClasses\n(\n3\n)\n.\nrun\n(\ntraining\n)\n// Compute raw scores on the test set\nval\npredictionAndLabels\n=\ntest\n.\nmap\n{\ncase\nLabeledPoint\n(\nlabel\n,\nfeatures\n)\n=>\nval\nprediction\n=\nmodel\n.\npredict\n(\nfeatures\n)\n(\nprediction\n,\nlabel\n)\n}\n// Instantiate metrics object\nval\nmetrics\n=\nnew\nMulticlassMetrics\n(\npredictionAndLabels\n)\n// Confusion matrix\nprintln\n(\n\"Confusion matrix:\"\n)\nprintln\n(\nmetrics\n.\nconfusionMatrix\n)\n// Overall Statistics\nval\naccuracy\n=\nmetrics\n.\naccuracy\nprintln\n(\n\"Summary Statistics\"\n)\nprintln\n(\ns\n\"Accuracy = $accuracy\"\n)\n// Precision by label\nval\nlabels\n=\nmetrics\n.\nlabels\nlabels\n.\nforeach\n{\nl\n=>\nprintln\n(\ns\n\"Precision($l) = \"\n+\nmetrics\n.\nprecision\n(\nl\n))\n}\n// Recall by label\nlabels\n.\nforeach\n{\nl\n=>\nprintln\n(\ns\n\"Recall($l) = \"\n+\nmetrics\n.\nrecall\n(\nl\n))\n}\n// False positive rate by label\nlabels\n.\nforeach\n{\nl\n=>\nprintln\n(\ns\n\"FPR($l) = \"\n+\nmetrics\n.\nfalsePositiveRate\n(\nl\n))\n}\n// F-measure by label\nlabels\n.\nforeach\n{\nl\n=>\nprintln\n(\ns\n\"F1-Score($l) = \"\n+\nmetrics\n.\nfMeasure\n(\nl\n))\n}\n// Weighted stats\nprintln\n(\ns\n\"Weighted precision: ${metrics.weightedPrecision}\"\n)\nprintln\n(\ns\n\"Weighted recall: ${metrics.weightedRecall}\"\n)\nprintln\n(\ns\n\"Weighted F1 score: ${metrics.weightedFMeasure}\"\n)\nprintln\n(\ns\n\"Weighted false positive rate: ${metrics.weightedFalsePositiveRate}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\" in the Spark repo.\nRefer to the\nMulticlassMetrics\nJava docs\nfor details on the API.\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.api.java.*\n;\nimport\norg.apache.spark.mllib.classification.LogisticRegressionModel\n;\nimport\norg.apache.spark.mllib.classification.LogisticRegressionWithLBFGS\n;\nimport\norg.apache.spark.mllib.evaluation.MulticlassMetrics\n;\nimport\norg.apache.spark.mllib.regression.LabeledPoint\n;\nimport\norg.apache.spark.mllib.util.MLUtils\n;\nimport\norg.apache.spark.mllib.linalg.Matrix\n;\nString\npath\n=\n\"data/mllib/sample_multiclass_classification_data.txt\"\n;\nJavaRDD\n<\nLabeledPoint\n>\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\npath\n).\ntoJavaRDD\n();\n// Split initial RDD into two... [60% training data, 40% testing data].\nJavaRDD\n<\nLabeledPoint\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]{\n0.6\n,\n0.4\n},\n11L\n);\nJavaRDD\n<\nLabeledPoint\n>\ntraining\n=\nsplits\n[\n0\n].\ncache\n();\nJavaRDD\n<\nLabeledPoint\n>\ntest\n=\nsplits\n[\n1\n];\n// Run training algorithm to build the model.\nLogisticRegressionModel\nmodel\n=\nnew\nLogisticRegressionWithLBFGS\n()\n.\nsetNumClasses\n(\n3\n)\n.\nrun\n(\ntraining\n.\nrdd\n());\n// Compute raw scores on the test set.\nJavaPairRDD\n<\nObject\n,\nObject\n>\npredictionAndLabels\n=\ntest\n.\nmapToPair\n(\np\n->\nnew\nTuple2\n<>(\nmodel\n.\npredict\n(\np\n.\nfeatures\n()),\np\n.\nlabel\n()));\n// Get evaluation metrics.\nMulticlassMetrics\nmetrics\n=\nnew\nMulticlassMetrics\n(\npredictionAndLabels\n.\nrdd\n());\n// Confusion matrix\nMatrix\nconfusion\n=\nmetrics\n.\nconfusionMatrix\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Confusion matrix: \\n\"\n+\nconfusion\n);\n// Overall statistics\nSystem\n.\nout\n.\nprintln\n(\n\"Accuracy = \"\n+\nmetrics\n.\naccuracy\n());\n// Stats by labels\nfor\n(\nint\ni\n=\n0\n;\ni\n<\nmetrics\n.\nlabels\n().\nlength\n;\ni\n++)\n{\nSystem\n.\nout\n.\nformat\n(\n\"Class %f precision = %f\\n\"\n,\nmetrics\n.\nlabels\n()[\ni\n],\nmetrics\n.\nprecision\n(\nmetrics\n.\nlabels\n()[\ni\n]));\nSystem\n.\nout\n.\nformat\n(\n\"Class %f recall = %f\\n\"\n,\nmetrics\n.\nlabels\n()[\ni\n],\nmetrics\n.\nrecall\n(\nmetrics\n.\nlabels\n()[\ni\n]));\nSystem\n.\nout\n.\nformat\n(\n\"Class %f F1 score = %f\\n\"\n,\nmetrics\n.\nlabels\n()[\ni\n],\nmetrics\n.\nfMeasure\n(\nmetrics\n.\nlabels\n()[\ni\n]));\n}\n//Weighted stats\nSystem\n.\nout\n.\nformat\n(\n\"Weighted precision = %f\\n\"\n,\nmetrics\n.\nweightedPrecision\n());\nSystem\n.\nout\n.\nformat\n(\n\"Weighted recall = %f\\n\"\n,\nmetrics\n.\nweightedRecall\n());\nSystem\n.\nout\n.\nformat\n(\n\"Weighted F1 score = %f\\n\"\n,\nmetrics\n.\nweightedFMeasure\n());\nSystem\n.\nout\n.\nformat\n(\n\"Weighted false positive rate = %f\\n\"\n,\nmetrics\n.\nweightedFalsePositiveRate\n());\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/LogisticRegressionModel\"\n);\nLogisticRegressionModel\nsameModel\n=\nLogisticRegressionModel\n.\nload\n(\nsc\n,\n\"target/tmp/LogisticRegressionModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\" in the Spark repo.\nMultilabel classification\nA\nmultilabel classification\nproblem involves mapping\neach sample in a dataset to a set of class labels. In this type of classification problem, the labels are not\nmutually exclusive. For example, when classifying a set of news articles into topics, a single article might be both\nscience and politics.\nBecause the labels are not mutually exclusive, the predictions and true labels are now vectors of label\nsets\n, rather\nthan vectors of labels. Multilabel metrics, therefore, extend the fundamental ideas of precision, recall, etc. to\noperations on sets. For example, a true positive for a given class now occurs when that class exists in the predicted\nset and it exists in the true label set, for a specific data point.\nAvailable metrics\nHere we define a set $D$ of $N$ documents\n\\[D = \\left\\{d_0, d_1, ..., d_{N-1}\\right\\}\\]\nDefine $L_0, L_1, …, L_{N-1}$ to be a family of label sets and $P_0, P_1, …, P_{N-1}$\nto be a family of prediction sets where $L_i$ and $P_i$ are the label set and prediction set, respectively, that\ncorrespond to document $d_i$.\nThe set of all unique labels is given by\n\\[L = \\bigcup_{k=0}^{N-1} L_k\\]\nThe following definition of indicator function $I_A(x)$ on a set $A$ will be necessary\n\\[I_A(x) = \\begin{cases}1 & \\text{if $x \\in A$}, \\\\ 0 & \\text{otherwise}.\\end{cases}\\]\nMetric\nDefinition\nPrecision\n$\\frac{1}{N} \\sum_{i=0}^{N-1} \\frac{\\left|P_i \\cap L_i\\right|}{\\left|P_i\\right|}$\nRecall\n$\\frac{1}{N} \\sum_{i=0}^{N-1} \\frac{\\left|L_i \\cap P_i\\right|}{\\left|L_i\\right|}$\nAccuracy\n$\\frac{1}{N} \\sum_{i=0}^{N - 1} \\frac{\\left|L_i \\cap P_i \\right|}\n        {\\left|L_i\\right| + \\left|P_i\\right| - \\left|L_i \\cap P_i \\right|}$\nPrecision by label\n$PPV(\\ell)=\\frac{TP}{TP + FP}=\n          \\frac{\\sum_{i=0}^{N-1} I_{P_i}(\\ell) \\cdot I_{L_i}(\\ell)}\n          {\\sum_{i=0}^{N-1} I_{P_i}(\\ell)}$\nRecall by label\n$TPR(\\ell)=\\frac{TP}{P}=\n          \\frac{\\sum_{i=0}^{N-1} I_{P_i}(\\ell) \\cdot I_{L_i}(\\ell)}\n          {\\sum_{i=0}^{N-1} I_{L_i}(\\ell)}$\nF1-measure by label\n$F1(\\ell) = 2\n                            \\cdot \\left(\\frac{PPV(\\ell) \\cdot TPR(\\ell)}\n                            {PPV(\\ell) + TPR(\\ell)}\\right)$\nHamming Loss\n$\\frac{1}{N \\cdot \\left|L\\right|} \\sum_{i=0}^{N - 1} \\left|L_i\\right| + \\left|P_i\\right| - 2\\left|L_i\n          \\cap P_i\\right|$\nSubset Accuracy\n$\\frac{1}{N} \\sum_{i=0}^{N-1} I_{\\{L_i\\}}(P_i)$\nF1 Measure\n$\\frac{1}{N} \\sum_{i=0}^{N-1} 2 \\frac{\\left|P_i \\cap L_i\\right|}{\\left|P_i\\right| \\cdot \\left|L_i\\right|}$\nMicro precision\n$\\frac{TP}{TP + FP}=\\frac{\\sum_{i=0}^{N-1} \\left|P_i \\cap L_i\\right|}\n          {\\sum_{i=0}^{N-1} \\left|P_i \\cap L_i\\right| + \\sum_{i=0}^{N-1} \\left|P_i - L_i\\right|}$\nMicro recall\n$\\frac{TP}{TP + FN}=\\frac{\\sum_{i=0}^{N-1} \\left|P_i \\cap L_i\\right|}\n        {\\sum_{i=0}^{N-1} \\left|P_i \\cap L_i\\right| + \\sum_{i=0}^{N-1} \\left|L_i - P_i\\right|}$\nMicro F1 Measure\n$2 \\cdot \\frac{TP}{2 \\cdot TP + FP + FN}=2 \\cdot \\frac{\\sum_{i=0}^{N-1} \\left|P_i \\cap L_i\\right|}{2 \\cdot\n        \\sum_{i=0}^{N-1} \\left|P_i \\cap L_i\\right| + \\sum_{i=0}^{N-1} \\left|L_i - P_i\\right| + \\sum_{i=0}^{N-1}\n        \\left|P_i - L_i\\right|}$\nExamples\nThe following code snippets illustrate how to evaluate the performance of a multilabel classifier. The examples\nuse the fake prediction and label data for multilabel classification that is shown below.\nDocument predictions:\ndoc 0 - predict 0, 1 - class 0, 2\ndoc 1 - predict 0, 2 - class 0, 1\ndoc 2 - predict none - class 0\ndoc 3 - predict 2 - class 2\ndoc 4 - predict 2, 0 - class 2, 0\ndoc 5 - predict 0, 1, 2 - class 0, 1\ndoc 6 - predict 1 - class 1, 2\nPredicted classes:\nclass 0 - doc 0, 1, 4, 5 (total 4)\nclass 1 - doc 0, 5, 6 (total 3)\nclass 2 - doc 1, 3, 4, 5 (total 4)\nTrue classes:\nclass 0 - doc 0, 1, 2, 4, 5 (total 5)\nclass 1 - doc 1, 5, 6 (total 3)\nclass 2 - doc 0, 3, 4, 6 (total 4)\nRefer to the\nMultilabelMetrics\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.evaluation\nimport\nMultilabelMetrics\nscoreAndLabels\n=\nsc\n.\nparallelize\n([\n([\n0.0\n,\n1.0\n],\n[\n0.0\n,\n2.0\n]),\n([\n0.0\n,\n2.0\n],\n[\n0.0\n,\n1.0\n]),\n([],\n[\n0.0\n]),\n([\n2.0\n],\n[\n2.0\n]),\n([\n2.0\n,\n0.0\n],\n[\n2.0\n,\n0.0\n]),\n([\n0.0\n,\n1.0\n,\n2.0\n],\n[\n0.0\n,\n1.0\n]),\n([\n1.0\n],\n[\n1.0\n,\n2.0\n])])\n# Instantiate metrics object\nmetrics\n=\nMultilabelMetrics\n(\nscoreAndLabels\n)\n# Summary stats\nprint\n(\n\"\nRecall = %s\n\"\n%\nmetrics\n.\nrecall\n())\nprint\n(\n\"\nPrecision = %s\n\"\n%\nmetrics\n.\nprecision\n())\nprint\n(\n\"\nF1 measure = %s\n\"\n%\nmetrics\n.\nf1Measure\n())\nprint\n(\n\"\nAccuracy = %s\n\"\n%\nmetrics\n.\naccuracy\n)\n# Individual label stats\nlabels\n=\nscoreAndLabels\n.\nflatMap\n(\nlambda\nx\n:\nx\n[\n1\n]).\ndistinct\n().\ncollect\n()\nfor\nlabel\nin\nlabels\n:\nprint\n(\n\"\nClass %s precision = %s\n\"\n%\n(\nlabel\n,\nmetrics\n.\nprecision\n(\nlabel\n)))\nprint\n(\n\"\nClass %s recall = %s\n\"\n%\n(\nlabel\n,\nmetrics\n.\nrecall\n(\nlabel\n)))\nprint\n(\n\"\nClass %s F1 Measure = %s\n\"\n%\n(\nlabel\n,\nmetrics\n.\nf1Measure\n(\nlabel\n)))\n# Micro stats\nprint\n(\n\"\nMicro precision = %s\n\"\n%\nmetrics\n.\nmicroPrecision\n)\nprint\n(\n\"\nMicro recall = %s\n\"\n%\nmetrics\n.\nmicroRecall\n)\nprint\n(\n\"\nMicro F1 measure = %s\n\"\n%\nmetrics\n.\nmicroF1Measure\n)\n# Hamming loss\nprint\n(\n\"\nHamming loss = %s\n\"\n%\nmetrics\n.\nhammingLoss\n)\n# Subset accuracy\nprint\n(\n\"\nSubset accuracy = %s\n\"\n%\nmetrics\n.\nsubsetAccuracy\n)\nFind full example code at \"examples/src/main/python/mllib/multi_label_metrics_example.py\" in the Spark repo.\nRefer to the\nMultilabelMetrics\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.evaluation.MultilabelMetrics\nimport\norg.apache.spark.rdd.RDD\nval\nscoreAndLabels\n:\nRDD\n[(\nArray\n[\nDouble\n]\n,\nArray\n[\nDouble\n])]\n=\nsc\n.\nparallelize\n(\nSeq\n((\nArray\n(\n0.0\n,\n1.0\n),\nArray\n(\n0.0\n,\n2.0\n)),\n(\nArray\n(\n0.0\n,\n2.0\n),\nArray\n(\n0.0\n,\n1.0\n)),\n(\nArray\n.\nempty\n[\nDouble\n],\nArray\n(\n0.0\n)),\n(\nArray\n(\n2.0\n),\nArray\n(\n2.0\n)),\n(\nArray\n(\n2.0\n,\n0.0\n),\nArray\n(\n2.0\n,\n0.0\n)),\n(\nArray\n(\n0.0\n,\n1.0\n,\n2.0\n),\nArray\n(\n0.0\n,\n1.0\n)),\n(\nArray\n(\n1.0\n),\nArray\n(\n1.0\n,\n2.0\n))),\n2\n)\n// Instantiate metrics object\nval\nmetrics\n=\nnew\nMultilabelMetrics\n(\nscoreAndLabels\n)\n// Summary stats\nprintln\n(\ns\n\"Recall = ${metrics.recall}\"\n)\nprintln\n(\ns\n\"Precision = ${metrics.precision}\"\n)\nprintln\n(\ns\n\"F1 measure = ${metrics.f1Measure}\"\n)\nprintln\n(\ns\n\"Accuracy = ${metrics.accuracy}\"\n)\n// Individual label stats\nmetrics\n.\nlabels\n.\nforeach\n(\nlabel\n=>\nprintln\n(\ns\n\"Class $label precision = ${metrics.precision(label)}\"\n))\nmetrics\n.\nlabels\n.\nforeach\n(\nlabel\n=>\nprintln\n(\ns\n\"Class $label recall = ${metrics.recall(label)}\"\n))\nmetrics\n.\nlabels\n.\nforeach\n(\nlabel\n=>\nprintln\n(\ns\n\"Class $label F1-score = ${metrics.f1Measure(label)}\"\n))\n// Micro stats\nprintln\n(\ns\n\"Micro recall = ${metrics.microRecall}\"\n)\nprintln\n(\ns\n\"Micro precision = ${metrics.microPrecision}\"\n)\nprintln\n(\ns\n\"Micro F1 measure = ${metrics.microF1Measure}\"\n)\n// Hamming loss\nprintln\n(\ns\n\"Hamming loss = ${metrics.hammingLoss}\"\n)\n// Subset accuracy\nprintln\n(\ns\n\"Subset accuracy = ${metrics.subsetAccuracy}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\" in the Spark repo.\nRefer to the\nMultilabelMetrics\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.api.java.*\n;\nimport\norg.apache.spark.mllib.evaluation.MultilabelMetrics\n;\nimport\norg.apache.spark.SparkConf\n;\nList\n<\nTuple2\n<\ndouble\n[],\ndouble\n[]>>\ndata\n=\nArrays\n.\nasList\n(\nnew\nTuple2\n<>(\nnew\ndouble\n[]{\n0.0\n,\n1.0\n},\nnew\ndouble\n[]{\n0.0\n,\n2.0\n}),\nnew\nTuple2\n<>(\nnew\ndouble\n[]{\n0.0\n,\n2.0\n},\nnew\ndouble\n[]{\n0.0\n,\n1.0\n}),\nnew\nTuple2\n<>(\nnew\ndouble\n[]{},\nnew\ndouble\n[]{\n0.0\n}),\nnew\nTuple2\n<>(\nnew\ndouble\n[]{\n2.0\n},\nnew\ndouble\n[]{\n2.0\n}),\nnew\nTuple2\n<>(\nnew\ndouble\n[]{\n2.0\n,\n0.0\n},\nnew\ndouble\n[]{\n2.0\n,\n0.0\n}),\nnew\nTuple2\n<>(\nnew\ndouble\n[]{\n0.0\n,\n1.0\n,\n2.0\n},\nnew\ndouble\n[]{\n0.0\n,\n1.0\n}),\nnew\nTuple2\n<>(\nnew\ndouble\n[]{\n1.0\n},\nnew\ndouble\n[]{\n1.0\n,\n2.0\n})\n);\nJavaRDD\n<\nTuple2\n<\ndouble\n[],\ndouble\n[]>>\nscoreAndLabels\n=\nsc\n.\nparallelize\n(\ndata\n);\n// Instantiate metrics object\nMultilabelMetrics\nmetrics\n=\nnew\nMultilabelMetrics\n(\nscoreAndLabels\n.\nrdd\n());\n// Summary stats\nSystem\n.\nout\n.\nformat\n(\n\"Recall = %f\\n\"\n,\nmetrics\n.\nrecall\n());\nSystem\n.\nout\n.\nformat\n(\n\"Precision = %f\\n\"\n,\nmetrics\n.\nprecision\n());\nSystem\n.\nout\n.\nformat\n(\n\"F1 measure = %f\\n\"\n,\nmetrics\n.\nf1Measure\n());\nSystem\n.\nout\n.\nformat\n(\n\"Accuracy = %f\\n\"\n,\nmetrics\n.\naccuracy\n());\n// Stats by labels\nfor\n(\nint\ni\n=\n0\n;\ni\n<\nmetrics\n.\nlabels\n().\nlength\n-\n1\n;\ni\n++)\n{\nSystem\n.\nout\n.\nformat\n(\n\"Class %1.1f precision = %f\\n\"\n,\nmetrics\n.\nlabels\n()[\ni\n],\nmetrics\n.\nprecision\n(\nmetrics\n.\nlabels\n()[\ni\n]));\nSystem\n.\nout\n.\nformat\n(\n\"Class %1.1f recall = %f\\n\"\n,\nmetrics\n.\nlabels\n()[\ni\n],\nmetrics\n.\nrecall\n(\nmetrics\n.\nlabels\n()[\ni\n]));\nSystem\n.\nout\n.\nformat\n(\n\"Class %1.1f F1 score = %f\\n\"\n,\nmetrics\n.\nlabels\n()[\ni\n],\nmetrics\n.\nf1Measure\n(\nmetrics\n.\nlabels\n()[\ni\n]));\n}\n// Micro stats\nSystem\n.\nout\n.\nformat\n(\n\"Micro recall = %f\\n\"\n,\nmetrics\n.\nmicroRecall\n());\nSystem\n.\nout\n.\nformat\n(\n\"Micro precision = %f\\n\"\n,\nmetrics\n.\nmicroPrecision\n());\nSystem\n.\nout\n.\nformat\n(\n\"Micro F1 measure = %f\\n\"\n,\nmetrics\n.\nmicroF1Measure\n());\n// Hamming loss\nSystem\n.\nout\n.\nformat\n(\n\"Hamming loss = %f\\n\"\n,\nmetrics\n.\nhammingLoss\n());\n// Subset accuracy\nSystem\n.\nout\n.\nformat\n(\n\"Subset accuracy = %f\\n\"\n,\nmetrics\n.\nsubsetAccuracy\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\" in the Spark repo.\nRanking systems\nThe role of a ranking algorithm (often thought of as a\nrecommender system\n)\nis to return to the user a set of relevant items or documents based on some training data. The definition of relevance\nmay vary and is usually application specific. Ranking system metrics aim to quantify the effectiveness of these\nrankings or recommendations in various contexts. Some metrics compare a set of recommended documents to a ground truth\nset of relevant documents, while other metrics may incorporate numerical ratings explicitly.\nAvailable metrics\nA ranking system usually deals with a set of $M$ users\n\\[U = \\left\\{u_0, u_1, ..., u_{M-1}\\right\\}\\]\nEach user ($u_i$) having a set of $N_i$ ground truth relevant documents\n\\[D_i = \\left\\{d_0, d_1, ..., d_{N_i-1}\\right\\}\\]\nAnd a list of $Q_i$ recommended documents, in order of decreasing relevance\n\\[R_i = \\left[r_0, r_1, ..., r_{Q_i-1}\\right]\\]\nThe goal of the ranking system is to produce the most relevant set of documents for each user. The relevance of the\nsets and the effectiveness of the algorithms can be measured using the metrics listed below.\nIt is necessary to define a function which, provided a recommended document and a set of ground truth relevant\ndocuments, returns a relevance score for the recommended document.\n\\[rel_D(r) = \\begin{cases}1 & \\text{if $r \\in D$}, \\\\ 0 & \\text{otherwise}.\\end{cases}\\]\nMetric\nDefinition\nNotes\nPrecision at k\n$p(k)=\\frac{1}{M} \\sum_{i=0}^{M-1} {\\frac{1}{k} \\sum_{j=0}^{\\text{min}(Q_i, k) - 1} rel_{D_i}(R_i(j))}$\nPrecision at k\nis a measure of\n         how many of the first k recommended documents are in the set of true relevant documents averaged across all\n         users. In this metric, the order of the recommendations is not taken into account.\nMean Average Precision\n$MAP=\\frac{1}{M} \\sum_{i=0}^{M-1} {\\frac{1}{N_i} \\sum_{j=0}^{Q_i-1} \\frac{rel_{D_i}(R_i(j))}{j + 1}}$\nMAP\nis a measure of how\n         many of the recommended documents are in the set of true relevant documents, where the\n        order of the recommendations is taken into account (i.e. penalty for highly relevant documents is higher).\nNormalized Discounted Cumulative Gain\n$NDCG(k)=\\frac{1}{M} \\sum_{i=0}^{M-1} {\\frac{1}{IDCG(D_i, k)}\\sum_{j=0}^{n-1}\n          \\frac{rel_{D_i}(R_i(j))}{\\text{log}(j+2)}} \\\\\n        \\text{Where} \\\\\n        \\hspace{5 mm} n = \\text{min}\\left(\\text{max}\\left(Q_i, N_i\\right),k\\right) \\\\\n        \\hspace{5 mm} IDCG(D, k) = \\sum_{j=0}^{\\text{min}(\\left|D\\right|, k) - 1} \\frac{1}{\\text{log}(j+2)}$\nNDCG at k\nis a\n        measure of how many of the first k recommended documents are in the set of true relevant documents averaged\n        across all users. In contrast to precision at k, this metric takes into account the order of the recommendations\n        (documents are assumed to be in order of decreasing relevance).\nExamples\nThe following code snippets illustrate how to load a sample dataset, train an alternating least squares recommendation\nmodel on the data, and evaluate the performance of the recommender by several ranking metrics. A brief summary of the\nmethodology is provided below.\nMovieLens ratings are on a scale of 1-5:\n5: Must see\n4: Will enjoy\n3: It’s okay\n2: Fairly bad\n1: Awful\nSo we should not recommend a movie if the predicted rating is less than 3.\nTo map ratings to confidence scores, we use:\n5 -> 2.5\n4 -> 1.5\n3 -> 0.5\n2 -> -0.5\n1 -> -1.5.\nThis mappings means unobserved entries are generally between It’s okay and Fairly bad. The semantics of 0 in this\nexpanded world of non-positive weights are “the same as never having interacted at all.”\nRefer to the\nRegressionMetrics\nPython docs\nand\nRankingMetrics\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.recommendation\nimport\nALS\n,\nRating\nfrom\npyspark.mllib.evaluation\nimport\nRegressionMetrics\n# Read in the ratings data\nlines\n=\nsc\n.\ntextFile\n(\n\"\ndata/mllib/sample_movielens_data.txt\n\"\n)\ndef\nparseLine\n(\nline\n):\nfields\n=\nline\n.\nsplit\n(\n\"\n::\n\"\n)\nreturn\nRating\n(\nint\n(\nfields\n[\n0\n]),\nint\n(\nfields\n[\n1\n]),\nfloat\n(\nfields\n[\n2\n])\n-\n2.5\n)\nratings\n=\nlines\n.\nmap\n(\nlambda\nr\n:\nparseLine\n(\nr\n))\n# Train a model on to predict user-product ratings\nmodel\n=\nALS\n.\ntrain\n(\nratings\n,\n10\n,\n10\n,\n0.01\n)\n# Get predicted ratings on all existing user-product pairs\ntestData\n=\nratings\n.\nmap\n(\nlambda\np\n:\n(\np\n.\nuser\n,\np\n.\nproduct\n))\npredictions\n=\nmodel\n.\npredictAll\n(\ntestData\n).\nmap\n(\nlambda\nr\n:\n((\nr\n.\nuser\n,\nr\n.\nproduct\n),\nr\n.\nrating\n))\nratingsTuple\n=\nratings\n.\nmap\n(\nlambda\nr\n:\n((\nr\n.\nuser\n,\nr\n.\nproduct\n),\nr\n.\nrating\n))\nscoreAndLabels\n=\npredictions\n.\njoin\n(\nratingsTuple\n).\nmap\n(\nlambda\ntup\n:\ntup\n[\n1\n])\n# Instantiate regression metrics to compare predicted and actual ratings\nmetrics\n=\nRegressionMetrics\n(\nscoreAndLabels\n)\n# Root mean squared error\nprint\n(\n\"\nRMSE = %s\n\"\n%\nmetrics\n.\nrootMeanSquaredError\n)\n# R-squared\nprint\n(\n\"\nR-squared = %s\n\"\n%\nmetrics\n.\nr2\n)\nFind full example code at \"examples/src/main/python/mllib/ranking_metrics_example.py\" in the Spark repo.\nRefer to the\nRegressionMetrics\nScala docs\nand\nRankingMetrics\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.evaluation.\n{\nRankingMetrics\n,\nRegressionMetrics\n}\nimport\norg.apache.spark.mllib.recommendation.\n{\nALS\n,\nRating\n}\n// Read in the ratings data\nval\nratings\n=\nspark\n.\nread\n.\ntextFile\n(\n\"data/mllib/sample_movielens_data.txt\"\n).\nrdd\n.\nmap\n{\nline\n=>\nval\nfields\n=\nline\n.\nsplit\n(\n\"::\"\n)\nRating\n(\nfields\n(\n0\n).\ntoInt\n,\nfields\n(\n1\n).\ntoInt\n,\nfields\n(\n2\n).\ntoDouble\n-\n2.5\n)\n}.\ncache\n()\n// Map ratings to 1 or 0, 1 indicating a movie that should be recommended\nval\nbinarizedRatings\n=\nratings\n.\nmap\n(\nr\n=>\nRating\n(\nr\n.\nuser\n,\nr\n.\nproduct\n,\nif\n(\nr\n.\nrating\n>\n0\n)\n1.0\nelse\n0.0\n)).\ncache\n()\n// Summarize ratings\nval\nnumRatings\n=\nratings\n.\ncount\n()\nval\nnumUsers\n=\nratings\n.\nmap\n(\n_\n.\nuser\n).\ndistinct\n().\ncount\n()\nval\nnumMovies\n=\nratings\n.\nmap\n(\n_\n.\nproduct\n).\ndistinct\n().\ncount\n()\nprintln\n(\ns\n\"Got $numRatings ratings from $numUsers users on $numMovies movies.\"\n)\n// Build the model\nval\nnumIterations\n=\n10\nval\nrank\n=\n10\nval\nlambda\n=\n0.01\nval\nmodel\n=\nALS\n.\ntrain\n(\nratings\n,\nrank\n,\nnumIterations\n,\nlambda\n)\n// Define a function to scale ratings from 0 to 1\ndef\nscaledRating\n(\nr\n:\nRating\n)\n:\nRating\n=\n{\nval\nscaledRating\n=\nmath\n.\nmax\n(\nmath\n.\nmin\n(\nr\n.\nrating\n,\n1.0\n),\n0.0\n)\nRating\n(\nr\n.\nuser\n,\nr\n.\nproduct\n,\nscaledRating\n)\n}\n// Get sorted top ten predictions for each user and then scale from [0, 1]\nval\nuserRecommended\n=\nmodel\n.\nrecommendProductsForUsers\n(\n10\n).\nmap\n{\ncase\n(\nuser\n,\nrecs\n)\n=>\n(\nuser\n,\nrecs\n.\nmap\n(\nscaledRating\n))\n}\n// Assume that any movie a user rated 3 or higher (which maps to a 1) is a relevant document\n// Compare with top ten most relevant documents\nval\nuserMovies\n=\nbinarizedRatings\n.\ngroupBy\n(\n_\n.\nuser\n)\nval\nrelevantDocuments\n=\nuserMovies\n.\njoin\n(\nuserRecommended\n).\nmap\n{\ncase\n(\nuser\n,\n(\nactual\n,\npredictions\n))\n=>\n(\npredictions\n.\nmap\n(\n_\n.\nproduct\n),\nactual\n.\nfilter\n(\n_\n.\nrating\n>\n0.0\n).\nmap\n(\n_\n.\nproduct\n).\ntoArray\n)\n}\n// Instantiate metrics object\nval\nmetrics\n=\nnew\nRankingMetrics\n(\nrelevantDocuments\n)\n// Precision at K\nArray\n(\n1\n,\n3\n,\n5\n).\nforeach\n{\nk\n=>\nprintln\n(\ns\n\"Precision at $k = ${metrics.precisionAt(k)}\"\n)\n}\n// Mean average precision\nprintln\n(\ns\n\"Mean average precision = ${metrics.meanAveragePrecision}\"\n)\n// Mean average precision at k\nprintln\n(\ns\n\"Mean average precision at 2 = ${metrics.meanAveragePrecisionAt(2)}\"\n)\n// Normalized discounted cumulative gain\nArray\n(\n1\n,\n3\n,\n5\n).\nforeach\n{\nk\n=>\nprintln\n(\ns\n\"NDCG at $k = ${metrics.ndcgAt(k)}\"\n)\n}\n// Recall at K\nArray\n(\n1\n,\n3\n,\n5\n).\nforeach\n{\nk\n=>\nprintln\n(\ns\n\"Recall at $k = ${metrics.recallAt(k)}\"\n)\n}\n// Get predictions for each data point\nval\nallPredictions\n=\nmodel\n.\npredict\n(\nratings\n.\nmap\n(\nr\n=>\n(\nr\n.\nuser\n,\nr\n.\nproduct\n))).\nmap\n(\nr\n=>\n((\nr\n.\nuser\n,\nr\n.\nproduct\n),\nr\n.\nrating\n))\nval\nallRatings\n=\nratings\n.\nmap\n(\nr\n=>\n((\nr\n.\nuser\n,\nr\n.\nproduct\n),\nr\n.\nrating\n))\nval\npredictionsAndLabels\n=\nallPredictions\n.\njoin\n(\nallRatings\n).\nmap\n{\ncase\n((\nuser\n,\nproduct\n),\n(\npredicted\n,\nactual\n))\n=>\n(\npredicted\n,\nactual\n)\n}\n// Get the RMSE using regression metrics\nval\nregressionMetrics\n=\nnew\nRegressionMetrics\n(\npredictionsAndLabels\n)\nprintln\n(\ns\n\"RMSE = ${regressionMetrics.rootMeanSquaredError}\"\n)\n// R-squared\nprintln\n(\ns\n\"R-squared = ${regressionMetrics.r2}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\" in the Spark repo.\nRefer to the\nRegressionMetrics\nJava docs\nand\nRankingMetrics\nJava docs\nfor details on the API.\nimport\njava.util.*\n;\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.api.java.*\n;\nimport\norg.apache.spark.mllib.evaluation.RegressionMetrics\n;\nimport\norg.apache.spark.mllib.evaluation.RankingMetrics\n;\nimport\norg.apache.spark.mllib.recommendation.ALS\n;\nimport\norg.apache.spark.mllib.recommendation.MatrixFactorizationModel\n;\nimport\norg.apache.spark.mllib.recommendation.Rating\n;\nString\npath\n=\n\"data/mllib/sample_movielens_data.txt\"\n;\nJavaRDD\n<\nString\n>\ndata\n=\nsc\n.\ntextFile\n(\npath\n);\nJavaRDD\n<\nRating\n>\nratings\n=\ndata\n.\nmap\n(\nline\n->\n{\nString\n[]\nparts\n=\nline\n.\nsplit\n(\n\"::\"\n);\nreturn\nnew\nRating\n(\nInteger\n.\nparseInt\n(\nparts\n[\n0\n]),\nInteger\n.\nparseInt\n(\nparts\n[\n1\n]),\nDouble\n.\nparseDouble\n(\nparts\n[\n2\n])\n-\n2.5\n);\n});\nratings\n.\ncache\n();\n// Train an ALS model\nMatrixFactorizationModel\nmodel\n=\nALS\n.\ntrain\n(\nJavaRDD\n.\ntoRDD\n(\nratings\n),\n10\n,\n10\n,\n0.01\n);\n// Get top 10 recommendations for every user and scale ratings from 0 to 1\nJavaRDD\n<\nTuple2\n<\nObject\n,\nRating\n[]>>\nuserRecs\n=\nmodel\n.\nrecommendProductsForUsers\n(\n10\n).\ntoJavaRDD\n();\nJavaRDD\n<\nTuple2\n<\nObject\n,\nRating\n[]>>\nuserRecsScaled\n=\nuserRecs\n.\nmap\n(\nt\n->\n{\nRating\n[]\nscaledRatings\n=\nnew\nRating\n[\nt\n.\n_2\n().\nlength\n];\nfor\n(\nint\ni\n=\n0\n;\ni\n<\nscaledRatings\n.\nlength\n;\ni\n++)\n{\ndouble\nnewRating\n=\nMath\n.\nmax\n(\nMath\n.\nmin\n(\nt\n.\n_2\n()[\ni\n].\nrating\n(),\n1.0\n),\n0.0\n);\nscaledRatings\n[\ni\n]\n=\nnew\nRating\n(\nt\n.\n_2\n()[\ni\n].\nuser\n(),\nt\n.\n_2\n()[\ni\n].\nproduct\n(),\nnewRating\n);\n}\nreturn\nnew\nTuple2\n<>(\nt\n.\n_1\n(),\nscaledRatings\n);\n});\nJavaPairRDD\n<\nObject\n,\nRating\n[]>\nuserRecommended\n=\nJavaPairRDD\n.\nfromJavaRDD\n(\nuserRecsScaled\n);\n// Map ratings to 1 or 0, 1 indicating a movie that should be recommended\nJavaRDD\n<\nRating\n>\nbinarizedRatings\n=\nratings\n.\nmap\n(\nr\n->\n{\ndouble\nbinaryRating\n;\nif\n(\nr\n.\nrating\n()\n>\n0.0\n)\n{\nbinaryRating\n=\n1.0\n;\n}\nelse\n{\nbinaryRating\n=\n0.0\n;\n}\nreturn\nnew\nRating\n(\nr\n.\nuser\n(),\nr\n.\nproduct\n(),\nbinaryRating\n);\n});\n// Group ratings by common user\nJavaPairRDD\n<\nObject\n,\nIterable\n<\nRating\n>>\nuserMovies\n=\nbinarizedRatings\n.\ngroupBy\n(\nRating:\n:\nuser\n);\n// Get true relevant documents from all user ratings\nJavaPairRDD\n<\nObject\n,\nList\n<\nInteger\n>>\nuserMoviesList\n=\nuserMovies\n.\nmapValues\n(\ndocs\n->\n{\nList\n<\nInteger\n>\nproducts\n=\nnew\nArrayList\n<>();\nfor\n(\nRating\nr\n:\ndocs\n)\n{\nif\n(\nr\n.\nrating\n()\n>\n0.0\n)\n{\nproducts\n.\nadd\n(\nr\n.\nproduct\n());\n}\n}\nreturn\nproducts\n;\n});\n// Extract the product id from each recommendation\nJavaPairRDD\n<\nObject\n,\nList\n<\nInteger\n>>\nuserRecommendedList\n=\nuserRecommended\n.\nmapValues\n(\ndocs\n->\n{\nList\n<\nInteger\n>\nproducts\n=\nnew\nArrayList\n<>();\nfor\n(\nRating\nr\n:\ndocs\n)\n{\nproducts\n.\nadd\n(\nr\n.\nproduct\n());\n}\nreturn\nproducts\n;\n});\nJavaRDD\n<\nTuple2\n<\nList\n<\nInteger\n>,\nList\n<\nInteger\n>>>\nrelevantDocs\n=\nuserMoviesList\n.\njoin\n(\nuserRecommendedList\n).\nvalues\n();\n// Instantiate the metrics object\nRankingMetrics\n<\nInteger\n>\nmetrics\n=\nRankingMetrics\n.\nof\n(\nrelevantDocs\n);\n// Precision, NDCG and Recall at k\nInteger\n[]\nkVector\n=\n{\n1\n,\n3\n,\n5\n};\nfor\n(\nInteger\nk\n:\nkVector\n)\n{\nSystem\n.\nout\n.\nformat\n(\n\"Precision at %d = %f\\n\"\n,\nk\n,\nmetrics\n.\nprecisionAt\n(\nk\n));\nSystem\n.\nout\n.\nformat\n(\n\"NDCG at %d = %f\\n\"\n,\nk\n,\nmetrics\n.\nndcgAt\n(\nk\n));\nSystem\n.\nout\n.\nformat\n(\n\"Recall at %d = %f\\n\"\n,\nk\n,\nmetrics\n.\nrecallAt\n(\nk\n));\n}\n// Mean average precision\nSystem\n.\nout\n.\nformat\n(\n\"Mean average precision = %f\\n\"\n,\nmetrics\n.\nmeanAveragePrecision\n());\n//Mean average precision at k\nSystem\n.\nout\n.\nformat\n(\n\"Mean average precision at 2 = %f\\n\"\n,\nmetrics\n.\nmeanAveragePrecisionAt\n(\n2\n));\n// Evaluate the model using numerical ratings and regression metrics\nJavaRDD\n<\nTuple2\n<\nObject\n,\nObject\n>>\nuserProducts\n=\nratings\n.\nmap\n(\nr\n->\nnew\nTuple2\n<>(\nr\n.\nuser\n(),\nr\n.\nproduct\n()));\nJavaPairRDD\n<\nTuple2\n<\nInteger\n,\nInteger\n>,\nObject\n>\npredictions\n=\nJavaPairRDD\n.\nfromJavaRDD\n(\nmodel\n.\npredict\n(\nJavaRDD\n.\ntoRDD\n(\nuserProducts\n)).\ntoJavaRDD\n().\nmap\n(\nr\n->\nnew\nTuple2\n<>(\nnew\nTuple2\n<>(\nr\n.\nuser\n(),\nr\n.\nproduct\n()),\nr\n.\nrating\n())));\nJavaRDD\n<\nTuple2\n<\nObject\n,\nObject\n>>\nratesAndPreds\n=\nJavaPairRDD\n.\nfromJavaRDD\n(\nratings\n.\nmap\n(\nr\n->\nnew\nTuple2\n<\nTuple2\n<\nInteger\n,\nInteger\n>,\nObject\n>(\nnew\nTuple2\n<>(\nr\n.\nuser\n(),\nr\n.\nproduct\n()),\nr\n.\nrating\n())\n)).\njoin\n(\npredictions\n).\nvalues\n();\n// Create regression metrics object\nRegressionMetrics\nregressionMetrics\n=\nnew\nRegressionMetrics\n(\nratesAndPreds\n.\nrdd\n());\n// Root mean squared error\nSystem\n.\nout\n.\nformat\n(\n\"RMSE = %f\\n\"\n,\nregressionMetrics\n.\nrootMeanSquaredError\n());\n// R-squared\nSystem\n.\nout\n.\nformat\n(\n\"R-squared = %f\\n\"\n,\nregressionMetrics\n.\nr2\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\" in the Spark repo.\nRegression model evaluation\nRegression analysis\nis used when predicting a continuous output\nvariable from a number of independent variables.\nAvailable metrics\nMetric\nDefinition\nMean Squared Error (MSE)\n$MSE = \\frac{\\sum_{i=0}^{N-1} (\\mathbf{y}_i - \\hat{\\mathbf{y}}_i)^2}{N}$\nRoot Mean Squared Error (RMSE)\n$RMSE = \\sqrt{\\frac{\\sum_{i=0}^{N-1} (\\mathbf{y}_i - \\hat{\\mathbf{y}}_i)^2}{N}}$\nMean Absolute Error (MAE)\n$MAE=\\frac{1}{N}\\sum_{i=0}^{N-1} \\left|\\mathbf{y}_i - \\hat{\\mathbf{y}}_i\\right|$\nCoefficient of Determination $(R^2)$\n$R^2=1 - \\frac{MSE}{\\text{VAR}(\\mathbf{y}) \\cdot (N-1)}=1-\\frac{\\sum_{i=0}^{N-1}\n        (\\mathbf{y}_i - \\hat{\\mathbf{y}}_i)^2}{\\sum_{i=0}^{N-1}(\\mathbf{y}_i-\\bar{\\mathbf{y}})^2}$\nExplained Variance\n$1 - \\frac{\\text{VAR}(\\mathbf{y} - \\mathbf{\\hat{y}})}{\\text{VAR}(\\mathbf{y})}$"}
{"url": "https://spark.apache.org/docs/latest/streaming/structured-streaming-transform-with-state.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nStructured Streaming Programming Guide\nOverview\nGetting Started\nAPIs on DataFrames and Datasets\nPerformance Tips\nAdditional Information\nStructured Streaming Programming Guide\nOverview\nTransformWithState is the new arbitrary stateful operator in Structured Streaming since the Apache Spark 4.0 release. This operator is the next generation replacement for the old mapGroupsWithState/flatMapGroupsWithState API in Scala and the applyInPandasWithState API in Python for arbitrary stateful processing in Apache Spark.\nThis operator has support for an umbrella of features such as object-oriented stateful processor definition, composite types, automatic TTL based eviction, timers etc and can be used to build business-critical operational use-cases.\nLanguage Support\nTransformWithState\nis available in Scala, Java and Python. Note that in Python, the operator name is called\ntransformWithStateInPandas\nsimilar to other operators interacting with the Pandas interface in Apache Spark.\nComponents of a TransformWithState Query\nA transformWithState query typically consists of the following components:\nStateful Processor - A user-defined stateful processor that defines the stateful logic\nOutput Mode - Output mode for the query such as Append, Update etc\nTime Mode - Time mode for the query such as EventTime, ProcessingTime etc\nInitial State - An optional initial state batch dataframe used to pre-populate the state\nIn the following sections, we will go through the above components in more detail.\nDefining a Stateful Processor\nA stateful processor is the core of the user-defined logic used to operate on the input events. A stateful processor is defined by extending the StatefulProcessor class and implementing a few methods.\nA typical stateful processor deals with the following constructs:\nInput Records - Input records received by the stream\nState Variables - Zero or more class specific members used to store user state\nOutput Records - Output records produced by the processor. Zero or more output records may be produced by the processor.\nA stateful processor uses the object-oriented paradigm to define the stateful logic. The stateful logic is defined by implementing the following methods:\ninit\n- Initialize the stateful processor and define any state variables as needed\nhandleInputRows\n- Process input rows belonging to a grouping key and emit output if needed\nhandleExpiredTimer\n- Handle expired timers and emit output if needed\nclose\n- Perform any cleanup operations if needed\nhandleInitialState\n- Optionally handle the initial state batch dataframe\nThe methods above will be invoked by the Spark query engine when the operator is executed as part of a streaming query.\nNote also that not all types of operations are supported in each of the methods. For eg, users cannot register timers in the\ninit\nmethod. Similarly, they cannot operate on input rows in the\nhandleExpiredTimer\nmethod. The engine will detect unsupported/incompatible operations and fail the query, if needed.\nUsing the StatefulProcessorHandle\nMany operations within the methods above can be performed using the\nStatefulProcessorHandle\nobject. The\nStatefulProcessorHandle\nobject provides methods to interact with the underlying state store. This object can be retrieved within the StatefulProcessor by invoking the\ngetHandle\nmethod.\nUsing State Variables\nState variables are class specific members used to store user state. They need to be declared once and initialized within the\ninit\nmethod of the stateful processor.\nInitializing a state variable typically involves the following steps:\nProvide a unique name for the state variable (unique within the stateful processor definition)\nProvide a type for the state variable (ValueState, ListState, MapState) - depending on the type, the appropriate method on the handle needs to be invoked\nProvide a state encoder for the state variable (in Scala - this can be skipped if implicit encoders are available)\nProvide an optional TTL config for the state variable\nTypes of state variables\nState variables can be of the following types:\nValue State\nList State\nMap State\nSimilar to collections for popular programming languages, the state types could be used to model data structures optimized for various types of operations for the underlying storage layer. For example, appends are optimized for ListState and point lookups are optimized for MapState.\nProviding state encoders\nState encoders are used to serialize and deserialize the state variables. In Scala, the state encoders can be skipped if implicit encoders are available. In Java and Python, the state encoders need to be provided explicitly.\nBuilt-in encoders for primitives, case classes and Java Bean classes are provided by default via the Spark SQL encoders.\nProviding implicit encoders in Scala\nIn Scala, implicit encoders can be provided for case classes and primitive types. The\nimplicits\nobject is provided as part of the\nStatefulProcessor\nclass. Within the StatefulProcessor definition, the user can simply import implicits as\nimport implicits._\nand then they do not require to pass the encoder type explicitly.\nProviding TTL for state variables\nState variables can be configured with an optional TTL (Time-To-Live) value. The TTL value is used to automatically evict the state variable after the specified duration. The TTL value can be provided as a Duration.\nHandling input rows\nThe\nhandleInputRows\nmethod is used to process input rows belonging to a grouping key and emit output if needed. The method is invoked by the Spark query engine for each grouping key value received by the operator. If multiple rows belong to the same grouping key, the provided iterator will include all those rows.\nHandling expired timers\nWithin the\nhandleInputRows\nor\nhandleExpiredTimer\nmethods, the stateful processor can register timers to be triggered at a later time. The\nhandleExpiredTimer\nmethod is invoked by the Spark query engine when a timer set by the stateful processor has expired. This method is invoked once for each expired timer.\nHere are a few timer properties that are supported:\nMultiple timers associated with the same grouping key can be registered\nThe engine provides the ability to list/add/remove timers as needed\nTimers are also checkpointed as part of the query checkpoint and can be triggered on query restart as well.\nHandling initial state\nThe\nhandleInitialState\nmethod is used to optionally handle the initial state batch dataframe. The initial state batch dataframe is used to pre-populate the state for the stateful processor. The method is invoked by the Spark query engine when the initial state batch dataframe is available.\nThis method is only called once in the lifetime of the query. This is invoked before any input rows are processed by the stateful processor.\nPutting it all together\nHere is an example of a StatefulProcessor that implements a downtime detector. Each time a new value is seen for a given key, it updates the lastSeen state value, clears any existing timers, and resets a timer for the future.\nWhen a timer expires, the application emits the elapsed time since the last observed event for the key. It then sets a new timer to emit an update 10 seconds later.\nclass\nDownTimeDetector\n(\nStatefulProcessor\n):\ndef\ninit\n(\nself\n,\nhandle\n:\nStatefulProcessorHandle\n)\n->\nNone\n:\n# Define schema for the state value (timestamp)\nstate_schema\n=\nStructType\n([\nStructField\n(\n\"\nvalue\n\"\n,\nTimestampType\n(),\nTrue\n)])\nself\n.\nhandle\n=\nhandle\n# Initialize state to store the last seen timestamp for each key\nself\n.\nlast_seen\n=\nhandle\n.\ngetValueState\n(\n\"\nlast_seen\n\"\n,\nstate_schema\n)\ndef\nhandleExpiredTimer\n(\nself\n,\nkey\n,\ntimerValues\n,\nexpiredTimerInfo\n)\n->\nIterator\n[\npd\n.\nDataFrame\n]:\nlatest_from_existing\n=\nself\n.\nlast_seen\n.\nget\n()\n# Calculate downtime duration\ndowntime_duration\n=\ntimerValues\n.\ngetCurrentProcessingTimeInMs\n()\n-\nint\n(\nlatest_from_existing\n.\ntimestamp\n()\n*\n1000\n)\n# Register a new timer for 10 seconds in the future\nself\n.\nhandle\n.\nregisterTimer\n(\ntimerValues\n.\ngetCurrentProcessingTimeInMs\n()\n+\n10000\n)\n# Yield a DataFrame with the key and downtime duration\nyield\npd\n.\nDataFrame\n(\n{\n\"\nid\n\"\n:\nkey\n,\n\"\ntimeValues\n\"\n:\nstr\n(\ndowntime_duration\n),\n}\n)\ndef\nhandleInputRows\n(\nself\n,\nkey\n,\nrows\n,\ntimerValues\n)\n->\nIterator\n[\npd\n.\nDataFrame\n]:\n# Find the row with the maximum timestamp\nmax_row\n=\nmax\n((\ntuple\n(\npdf\n.\niloc\n[\n0\n])\nfor\npdf\nin\nrows\n),\nkey\n=\nlambda\nrow\n:\nrow\n[\n1\n])\n# Get the latest timestamp from existing state or use epoch start if not exists\nif\nself\n.\nlast_seen\n.\nexists\n():\nlatest_from_existing\n=\nself\n.\nlast_seen\n.\nget\n()\nelse\n:\nlatest_from_existing\n=\ndatetime\n.\nfromtimestamp\n(\n0\n)\n# If new data is more recent than existing state\nif\nlatest_from_existing\n<\nmax_row\n[\n1\n]:\n# Delete all existing timers\nfor\ntimer\nin\nself\n.\nhandle\n.\nlistTimers\n():\nself\n.\nhandle\n.\ndeleteTimer\n(\ntimer\n)\n# Update the last seen timestamp\nself\n.\nlast_seen\n.\nupdate\n((\nmax_row\n[\n1\n],))\n# Register a new timer for 5 seconds in the future\nself\n.\nhandle\n.\nregisterTimer\n(\ntimerValues\n.\ngetCurrentProcessingTimeInMs\n()\n+\n5000\n)\n# Yield an empty DataFrame\nyield\npd\n.\nDataFrame\n()\ndef\nclose\n(\nself\n)\n->\nNone\n:\n# No cleanup needed\npass\n// The (String, Timestamp) schema represents an (id, time). We want to do downtime\n// detection on every single unique sensor, where each sensor has a sensor ID.\nclass\nDowntimeDetector\n(\nduration\n:\nDuration\n)\nextends\nStatefulProcessor\n[\nString\n,\n(\nString\n,\nTimestamp\n)\n,\n(\nString\n,\nDuration\n)]\n{\n@transient\nprivate\nvar\n_lastSeen\n:\nValueState\n[\nTimestamp\n]\n=\n_\noverride\ndef\ninit\n(\noutputMode\n:\nOutputMode\n,\ntimeMode\n:\nTimeMode\n)\n:\nUnit\n=\n{\n_lastSeen\n=\ngetHandle\n.\ngetValueState\n[\nTimestamp\n](\n\"lastSeen\"\n,\nEncoders\n.\nTIMESTAMP\n,\nTTLConfig\n.\nNONE\n)\n}\n// The logic here is as follows: find the largest timestamp seen so far. Set a timer for\n// the duration later.\noverride\ndef\nhandleInputRows\n(\nkey\n:\nString\n,\ninputRows\n:\nIterator\n[(\nString\n,\nTimestamp\n)],\ntimerValues\n:\nTimerValues\n)\n:\nIterator\n[(\nString\n,\nDuration\n)]\n=\n{\nval\nlatestRecordFromNewRows\n=\ninputRows\n.\nmaxBy\n(\n_\n.\n_2\n.\ngetTime\n)\nval\nlatestTimestampFromExistingRows\n=\nif\n(\n_lastSeen\n.\nexists\n())\n{\n_lastSeen\n.\nget\n()\n}\nelse\n{\nnew\nTimestamp\n(\n0\n)\n}\nval\nlatestTimestampFromNewRows\n=\nlatestRecordFromNewRows\n.\n_2\nif\n(\nlatestTimestampFromNewRows\n.\nafter\n(\nlatestTimestampFromExistingRows\n))\n{\n// Cancel the one existing timer, since we have a new latest timestamp.\n// We call \"listTimers()\" just because we don't know ahead of time what\n// the timestamp of the existing timer is.\ngetHandle\n.\nlistTimers\n().\nforeach\n(\ntimer\n=>\ngetHandle\n.\ndeleteTimer\n(\ntimer\n))\n_lastSeen\n.\nupdate\n(\nlatestTimestampFromNewRows\n)\n// Use timerValues to schedule a timer using processing time.\ngetHandle\n.\nregisterTimer\n(\ntimerValues\n.\ngetCurrentProcessingTimeInMs\n()\n+\nduration\n.\ntoMillis\n)\n}\nelse\n{\n// No new latest timestamp, so no need to update state or set a timer.\n}\nIterator\n.\nempty\n}\noverride\ndef\nhandleExpiredTimer\n(\nkey\n:\nString\n,\ntimerValues\n:\nTimerValues\n,\nexpiredTimerInfo\n:\nExpiredTimerInfo\n)\n:\nIterator\n[(\nString\n,\nDuration\n)]\n=\n{\nval\nlatestTimestamp\n=\n_lastSeen\n.\nget\n()\nval\ndowntimeDuration\n=\nnew\nDuration\n(\ntimerValues\n.\ngetCurrentProcessingTimeInMs\n()\n-\nlatestTimestamp\n.\ngetTime\n)\n// Register another timer that will fire in 10 seconds.\n// Timers can be registered anywhere but init()\ngetHandle\n.\nregisterTimer\n(\ntimerValues\n.\ngetCurrentProcessingTimeInMs\n()\n+\n10000\n)\nIterator\n((\nkey\n,\ndowntimeDuration\n))\n}\n}\nUsing the StatefulProcessor in a streaming query\nNow that we have defined the\nStatefulProcessor\n, we can use it in a streaming query. The following code snippets show how to use the\nStatefulProcessor\nin a streaming query in Python and Scala.\nq\n=\n(\ndf\n.\ngroupBy\n(\n\"\nkey\n\"\n)\n.\ntransformWithStateInPandas\n(\nstatefulProcessor\n=\nDownTimeDetector\n(),\noutputStructType\n=\noutput_schema\n,\noutputMode\n=\n\"\nUpdate\n\"\n,\ntimeMode\n=\n\"\nNone\n\"\n,\n)\n.\nwriteStream\n...\nval\nquery\n=\ndf\n.\ngroupBy\n(\n\"key\"\n)\n.\ntransformWithState\n(\nstatefulProcessor\n=\nnew\nDownTimeDetector\n(),\noutputMode\n=\nOutputMode\n.\nUpdate\n,\ntimeMode\n=\nTimeMode\n.\nNone\n)\n.\nwriteStream\n...\nState Schema Evolution\nTransformWithState also allows for performing schema evolution of the managed state. There are 2 parts here:\nevolution across state variables\nevolution within a state variable\nNote that schema evolution is only supported on the value side. Key side state schema evolution is not supported.\nEvolution across state variables\nThis operator allows for state variables to be added and removed across different runs of the same streaming query. In order to remove a variable, we also need to inform the engine so that the underlying state can be purged. Users can achieve this by invoking the\ndeleteIfExists\nmethod for a given state variable within the\ninit\nmethod of the StatefulProcessor.\nEvolution within a state variable\nThis operator also allows for the state schema of a specific state variable to also be evolved. For example, if you are using a case class to store the state within a\nValueState\nvariable, then it’s possible for you to evolve this case class by adding/removing/widening fields.\nWe support such schema evolution only when the underlying encoding format is set to\nAvro\n. In order to enable this, please set the following Spark config as\nspark.conf.set(\"spark.sql.streaming.stateStore.encodingFormat\", \"avro\")\n.\nThe following evolution operations are supported within Avro rules:\nAdding a new field\nRemoving a field\nType widening\nReordering fields\nThe following evolution operations are not supported:\nRenaming a field\nType narrowing\nIntegration with State Data Source\nTransformWithState is a stateful operator that allows users to maintain arbitrary state across batches. In order to read this state, the user needs to provide some additional options in the state data source reader query.\nThis operator allows for multiple state variables to be used within the same query. However, because they could be of different composite types and encoding formats, they need to be read within a batch query one variable at a time.\nIn order to allow this, the user needs to specify the\nstateVarName\nfor the state variable they are interested in reading.\nTimers can read by setting the option\nreadRegisteredTimers\nto true. This will return all the registered timer across grouping keys.\nWe also allow for composite type variables to be read in 2 formats:\nFlattened: This is the default format where the composite types are flattened out into individual columns.\nNon-flattened: This is where the composite types are returned as a single column of Array or Map type in Spark SQL.\nDepending on your memory requirements, you can choose the format that best suits your use case.\nMore information about source options can be found\nhere\n."}
{"url": "https://spark.apache.org/docs/latest/streaming/structured-streaming-state-data-source.html", "content": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nStructured Streaming Programming Guide\nOverview\nGetting Started\nAPIs on DataFrames and Datasets\nPerformance Tips\nAdditional Information\nState Data Source Integration Guide\nState data source Guide in Structured Streaming (Experimental)\nOverview\nState data source provides functionality to manipulate the state from the checkpoint.\nAs of Spark 4.0, state data source provides the read functionality with a batch query. Additional functionalities including write is on the future roadmap.\nNOTE: this data source is currently marked as experimental - source options and the behavior (output) might be subject to change.\nReading state key-values from the checkpoint\nState data source enables reading key-value pairs from the state store in the checkpoint, via running a separate batch query.\nUsers can leverage the functionality to cover two major use cases described below:\nConstruct a test checking both output and the state. It is non-trivial to deduce the key-value of the state from the output, and having visibility of the state would be a huge win on testing.\nInvestigate an incident against stateful streaming query. If users observe the incorrect output and want to track how it came up, having visibility of the state would be required.\nUsers can read an instance of state store, which is matched to a single stateful operator in most cases. This means, users can expect that they can read the entire key-value pairs in the state for a single stateful operator.\nNote that there could be an exception, e.g. stream-stream join, which leverages multiple state store instances internally. The data source abstracts the internal representation away from users and\nprovides a user-friendly approach to read the state. See the section for stream-stream join for more details.\nReading the state store as batch queries (all defaults)\ndf\n=\nspark\n\\\n.\nread\n\\\n.\nformat\n(\n\"\nstatestore\n\"\n)\n\\\n.\nload\n(\n\"\n<checkpointLocation>\n\"\n)\nval\ndf\n=\nspark\n.\nread\n.\nformat\n(\n\"statestore\"\n)\n.\nload\n(\n\"<checkpointLocation>\"\n)\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\nread\n()\n.\nformat\n(\n\"statestore\"\n)\n.\nload\n(\n\"<checkpointLocation>\"\n);\nEach row in the source has the following schema:\nColumn\nType\nNote\nkey\nstruct (depends on the type for state key)\nvalue\nstruct (depends on the type for state value)\npartition_id\nint\nThe nested columns for key and value heavily depend on the input schema of the stateful operator as well as the type of operator.\nUsers are encouraged to query about the schema via df.schema() / df.printSchema() first to understand the type of output.\nThe following options must be set for the source.\nOption\nValue\nMeaning\npath\nstring\nSpecify the root directory of the checkpoint location. You can either specify the path via option(\"path\", `path`) or load(`path`).\nThe following configurations are optional:\nOption\nValue\nDefault\nMeaning\nbatchId\nnumeric value\nlatest committed batch\nRepresents the target batch to read from. This option is used when users want to perform time-travel. The batch should be committed but not yet cleaned up.\noperatorId\nnumeric value\n0\nRepresents the target operator to read from. This option is used when the query is using multiple stateful operators.\nstoreName\nstring\nDEFAULT\nRepresents the target state store name to read from. This option is used when the stateful operator uses multiple state store instances. It is not required except stream-stream join.\njoinSide\nstring (\"left\" or \"right\")\n(none)\nRepresents the target side to read from. This option is used when users want to read the state from stream-stream join.\nsnapshotStartBatchId\nnumeric value\nIf specified, force to read the snapshot at this batch ID, then changelogs will be replayed until 'batchId' or its default. Note that snapshot batch ID starts with 0 and equals to snapshot version ID minus 1. This option must be used together with 'snapshotPartitionId'.\nsnapshotPartitionId\nnumeric value\nIf specified, only this specific partition will be read. Note that partition ID starts with 0. This option must be used together with 'snapshotStartBatchId'.\nreadChangeFeed\nboolean\nfalse\nIf set to true, will read the change of state over microbatches. The output table schema will also differ. Details can be found in section\n\"Reading state changes over microbatches\"\n. Option 'changeStartBatchId' must be specified with this option. Option 'batchId', 'joinSide', 'snapshotStartBatchId' and 'snapshotPartitionId' cannot be used together with this option.\nchangeStartBatchId\nnumeric value\nRepresents the first batch to read in the read change feed mode. This option requires 'readChangeFeed' to be set to true.\nchangeEndBatchId\nnumeric value\nlatest commited batchId\nRepresents the last batch to read in the read change feed mode. This option requires 'readChangeFeed' to be set to true.\nstateVarName\nstring\nThe state variable name to read as part of this batch query. This is a required option if the transformWithState operator is used. Note that currently this option only applies to the transformWithState operator.\nreadRegisteredTimers\nboolean\nfalse\nIf true, the user can read registered timers used within the transformWithState operator. Note that currently this option only applies to the transformWithState operator. This option and the stateVarName option described above are mutually exclusive and only one of them can be used at a time.\nflattenCollectionTypes\nboolean\ntrue\nIf true, the collection types for state variables such as list state, map state etc are flattened out. If false, the values are provided as Array or Map type in Spark SQL. Note that currently this option only applies to the transformWithState operator.\nReading state for stream-stream join\nStructured Streaming implements the stream-stream join feature via leveraging multiple instances of state store internally.\nThese instances logically compose buffers to store the input rows for left and right.\nSince it is more obvious to users to reason about, the data source provides the option ‘joinSide’ to read the buffered input for specific side of the join.\nTo enable the functionality to read the internal state store instance directly, we also allow specifying the option ‘storeName’, with restriction that ‘storeName’ and ‘joinSide’ cannot be specified together.\nReading state for transformWithState\nTransformWithState is a stateful operator that allows users to maintain arbitrary state across batches. In order to read this state, the user needs to provide some additional options in the state data source reader query.\nThis operator allows for multiple state variables to be used within the same query. However, because they could be of different composite types and encoding formats, they need to be read within a batch query one variable at a time.\nIn order to allow this, the user needs to specify the\nstateVarName\nfor the state variable they are interested in reading.\nTimers can be read by setting the option\nreadRegisteredTimers\nto true. This will return all the registered timer across grouping keys.\nWe also allow for composite type variables to be read in 2 formats:\nFlattened: This is the default format where the composite types are flattened out into individual columns.\nNon-flattened: This is where the composite types are returned as a single column of Array or Map type in Spark SQL.\nDepending on your memory requirements, you can choose the format that best suits your use case.\nReading state changes over microbatches\nIf we want to understand the change of state store over microbatches instead of the whole state store at a particular microbatch, ‘readChangeFeed’ is the option to use.\nFor example, this is the code to read the change of state from batch 2 to the latest committed batch.\ndf\n=\nspark\n\\\n.\nread\n\\\n.\nformat\n(\n\"\nstatestore\n\"\n)\n\\\n.\noption\n(\n\"\nreadChangeFeed\n\"\n,\ntrue\n)\n\\\n.\noption\n(\n\"\nchangeStartBatchId\n\"\n,\n2\n)\n\\\n.\nload\n(\n\"\n<checkpointLocation>\n\"\n)\nval\ndf\n=\nspark\n.\nread\n.\nformat\n(\n\"statestore\"\n)\n.\noption\n(\n\"readChangeFeed\"\n,\ntrue\n)\n.\noption\n(\n\"changeStartBatchId\"\n,\n2\n)\n.\nload\n(\n\"<checkpointLocation>\"\n)\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\nread\n()\n.\nformat\n(\n\"statestore\"\n)\n.\noption\n(\n\"readChangeFeed\"\n,\ntrue\n)\n.\noption\n(\n\"changeStartBatchId\"\n,\n2\n)\n.\nload\n(\n\"<checkpointLocation>\"\n);\nThe output schema will also be different from the normal output.\nColumn\nType\nNote\nbatch_id\nlong\nchange_type\nstring\nThere are two possible values: 'update' and 'delete'. Update represents either inserting a non-existing key-value pair or updating an existing key with new value. The 'value' field will be null for delete records.\nkey\nstruct (depends on the type for state key)\nvalue\nstruct (depends on the type for state value)\npartition_id\nint\nState Metadata Source\nBefore querying the state from existing checkpoint via state data source, users would like to understand the information for the checkpoint, especially about state operator. This includes which operators and state store instances are available in the checkpoint, available range of batch IDs, etc.\nStructured Streaming provides a data source named “State metadata source” to provide the state-related metadata information from the checkpoint.\nNote: The metadata is constructed when the streaming query is running with Spark 4.0+. The existing checkpoint which has been running with lower Spark version does not have the metadata and will be unable to query/use with this metadata source. It is required to run the streaming query pointing the existing checkpoint in Spark 4.0+ to construct the metadata before querying.\nUsers can optionally provide the batchId to get the operator metadata at a point in time.\nCreating a State metadata store for Batch Queries\ndf\n=\nspark\n\\\n.\nread\n\\\n.\nformat\n(\n\"\nstate-metadata\n\"\n)\n\\\n.\nload\n(\n\"\n<checkpointLocation>\n\"\n)\nval\ndf\n=\nspark\n.\nread\n.\nformat\n(\n\"state-metadata\"\n)\n.\nload\n(\n\"<checkpointLocation>\"\n)\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\nread\n()\n.\nformat\n(\n\"state-metadata\"\n)\n.\nload\n(\n\"<checkpointLocation>\"\n);\nThe following options must be set for the source:\nOption\nValue\nMeaning\npath\nstring\nSpecify the root directory of the checkpoint location. You can either specify the path via option(\"path\", `path`) or load(`path`).\nThe following configurations are optional:\nOption\nValue\nDefault\nMeaning\nbatchId\nnumeric value\nLast committed batch if available, else 0\nOptional batchId used to retrieve operator metadata at that batch.\nEach row in the source has the following schema:\nColumn\nType\nNote\noperatorId\nint\noperatorName\nstring\nstateStoreName\nint\nnumPartitions\nint\nminBatchId\nint\nThe minimum batch ID available for querying state. The value could be invalid if the streaming query taking the checkpoint is running, as cleanup would run.\nmaxBatchId\nint\nThe maximum batch ID available for querying state. The value could be invalid if the streaming query taking the checkpoint is running, as the query will commit further batches.\noperatorProperties\nstring\nList of properties used by the operator encoded as JSON. Output generated here is operator dependent.\n_numColsPrefixKey\nint\nmetadata column (hidden unless specified with SELECT)\nOne of the major use cases of this data source is to identify the operatorId to query if the query has multiple stateful operators, e.g. stream-stream join followed by deduplication.\nThe column ‘operatorName’ helps users to identify the operatorId for given operator.\nAdditionally, if users want to query about an internal state store instance for a stateful operator (e.g. stream-stream join), the column ‘stateStoreName’ would be useful to determine the target."}
