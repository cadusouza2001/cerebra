{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nApache Spark - A Unified engine for large-scale data analytics\nApache Spark is a unified analytics engine for large-scale data processing.\n                  It provides high-level APIs in Java, Scala, Python and R,\n                  and an optimized e", "question": "What languages does Apache Spark provide high-level APIs in?", "answers": {"text": ["It provides high-level APIs in Java, Scala, Python and R"], "answer_start": [706]}}
{"context": "gine for large-scale data processing.\n                  It provides high-level APIs in Java, Scala, Python and R,\n                  and an optimized engine that supports general execution graphs.\n                  It also supports a rich set of higher-level tools including\nSpark SQL\nfor SQL and structured data processing,\npandas API on Spark\nfor pandas workloads,\nMLlib\nfor machine learning,\nGraphX\nfor graph processing,\n                   and\nStructured Streaming\nfor incremental computation and stream processing.\nDownloading\nGet Spark from the\ndownloads page\nof the project website. This documentation is for Spark version 4.0.0. Spark uses Hadoop’s client libraries for HDFS and YARN. Downloads are pre-packaged for a handful of popular Hadoop versions.\nUsers can also download a “Hadoop free” ", "question": "What does Spark use for HDFS and YARN?", "answers": {"text": ["Spark uses Hadoop’s client libraries for HDFS and YARN."], "answer_start": [635]}}
{"context": "hine — all you need is to have\njava\ninstalled on your system\nPATH\n, or the\nJAVA_HOME\nenvironment variable pointing to a Java installation.\nSpark runs on Java 17/21, Scala 2.13, Python 3.9+, and R 3.5+ (Deprecated).\nWhen using the Scala API, it is necessary for applications to use the same version of Scala that Spark was compiled for. Since Spark 4.0.0, it’s Scala 2.13.\nRunning the Examples and Shell\nSpark comes with several sample programs. Python, Scala, Java, and R examples are in the\nexamples/src/main\ndirectory.\nTo run Spark interactively in a Python interpreter, use\nbin/pyspark\n:\n./bin/pyspark --master \"local[2]\"\nSample applications are provided in Python. For example:\n./bin/spark-submit examples/src/main/python/pi.py 10\nTo run one of the Scala or Java sample programs, use\nbin/run-exam", "question": "Which Java versions does Spark run on?", "answers": {"text": ["Spark runs on Java 17/21, Scala 2.13, Python 3.9+, and R 3.5+ (Deprecated)."], "answer_start": [139]}}
{"context": "rovided in Python. For example:\n./bin/spark-submit examples/src/main/python/pi.py 10\nTo run one of the Scala or Java sample programs, use\nbin/run-example <class> [params]\nin the top-level Spark directory. (Behind the scenes, this\ninvokes the more general\nspark-submit\nscript\nfor\nlaunching applications). For example,\n./bin/run-example SparkPi 10\nYou can also run Spark interactively through a modified version of the Scala shell. This is a\ngreat way to learn the framework.\n./bin/spark-shell --master \"local[2]\"\nThe\n--master\noption specifies the\nmaster URL for a distributed cluster\n, or\nlocal\nto run\nlocally with one thread, or\nlocal[N]\nto run locally with N threads. You should start by using\nlocal\nfor testing. For a full list of options, run the Spark shell with the\n--help\noption.\nSince version ", "question": "How can you run Spark interactively?", "answers": {"text": ["./bin/spark-shell --master \"local[2]\""], "answer_start": [474]}}
{"context": "Hadoop YARN\nKubernetes\nWhere to Go from Here\nProgramming Guides:\nQuick Start\n: a quick introduction to the Spark API; start here!\nRDD Programming Guide\n: overview of Spark basics - RDDs (core but old API), accumulators, and broadcast variables\nSpark SQL, Datasets, and DataFrames\n: processing structured data with relational queries (newer API than RDDs)\nStructured Streaming\n: processing structured data streams with relation queries (using Datasets and DataFrames, newer API than DStreams)\nSpark Streaming\n: processing data streams using DStreams (old API)\nMLlib\n: applying machine learning algorithms\nGraphX\n: processing graphs\nSparkR (Deprecated)\n: processing data with Spark in R\nPySpark\n: processing data with Spark in Python\nSpark SQL CLI\n: processing data with SQL on the command line\nAPI Doc", "question": "What does the RDD Programming Guide provide an overview of?", "answers": {"text": ["overview of Spark basics - RDDs (core but old API), accumulators, and broadcast variables"], "answer_start": [154]}}
{"context": "\n: processing data with Spark in R\nPySpark\n: processing data with Spark in Python\nSpark SQL CLI\n: processing data with SQL on the command line\nAPI Docs:\nSpark Python API (Sphinx)\nSpark Scala API (Scaladoc)\nSpark Java API (Javadoc)\nSpark R API (Roxygen2)\nSpark SQL, Built-in Functions (MkDocs)\nDeployment Guides:\nCluster Overview\n: overview of concepts and components when running on a cluster\nSubmitting Applications\n: packaging and deploying applications\nDeployment modes:\nStandalone Deploy Mode\n: launch a standalone cluster quickly without a third-party cluster manager\nYARN\n: deploy Spark on top of Hadoop NextGen (YARN)\nKubernetes\n: deploy Spark apps on top of Kubernetes directly\nAmazon EC2\n: scripts that let you launch a cluster on EC2 in about 5 minutes\nSpark Kubernetes Operator\n:\nSparkApp\n", "question": "What does YARN allow you to do?", "answers": {"text": ["deploy Spark on top of Hadoop NextGen (YARN)"], "answer_start": [580]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nQuick Start\nInteractive Analysis with the Spark Shell\nBasics\nMore on Dataset Operations\nCaching\nSelf-Contained Applications\nWhere to Go from Here\nThis tutorial provides a quick introduction to using Spark. We will first introduce the API through Spark", "question": "What does this tutorial provide an introduction to?", "answers": {"text": ["using Spark"], "answer_start": [742]}}
{"context": "Contained Applications\nWhere to Go from Here\nThis tutorial provides a quick introduction to using Spark. We will first introduce the API through Spark’s\ninteractive shell (in Python or Scala),\nthen show how to write applications in Java, Scala, and Python.\nTo follow along with this guide, first, download a packaged release of Spark from the\nSpark website\n. Since we won’t be using HDFS,\nyou can download a package for any version of Hadoop.\nNote that, before Spark 2.0, the main programming interface of Spark was the Resilient Distributed Dataset (RDD). After Spark 2.0, RDDs are replaced by Dataset, which is strongly-typed like an RDD, but with richer optimizations under the hood. The RDD interface is still supported, and you can get a more detailed reference at the\nRDD programming guide\n. Ho", "question": "What replaced RDDs after Spark 2.0?", "answers": {"text": ["After Spark 2.0, RDDs are replaced by Dataset, which is strongly-typed like an RDD, but with richer optimizations under the hood."], "answer_start": [557]}}
{"context": "richer optimizations under the hood. The RDD interface is still supported, and you can get a more detailed reference at the\nRDD programming guide\n. However, we highly recommend you to switch to use Dataset, which has better performance than RDD. See the\nSQL programming guide\nto get more information about Dataset.\nInteractive Analysis with the Spark Shell\nBasics\nSpark’s shell provides a simple way to learn the API, as well as a powerful tool to analyze data interactively.\nIt is available in either Scala (which runs on the Java VM and is thus a good way to use existing Java libraries)\nor Python. Start it by running the following in the Spark directory:\n./bin/pyspark\nOr if PySpark is installed with pip in your current environment:\npyspark\nSpark’s primary abstraction is a distributed collectio", "question": "What are the two languages available for Spark's shell?", "answers": {"text": ["or Python. Start it by running the following in the Spark directory:"], "answer_start": [590]}}
{"context": "rectory:\n./bin/pyspark\nOr if PySpark is installed with pip in your current environment:\npyspark\nSpark’s primary abstraction is a distributed collection of items called a Dataset. Datasets can be created from Hadoop InputFormats (such as HDFS files) or by transforming other Datasets. Due to Python’s dynamic nature, we don’t need the Dataset to be strongly-typed in Python. As a result, all Datasets in Python are Dataset[Row], and we call it\nDataFrame\nto be consistent with the data frame concept in Pandas and R. Let’s make a new DataFrame from the text of the README file in the Spark source directory:\n>>>\ntextFile\n=\nspark\n.\nread\n.\ntext\n(\n\"\nREADME.md\n\"\n)\nYou can get values from DataFrame directly, by calling some actions, or transform the DataFrame to get a new one. For more details, please re", "question": "What is Spark’s primary abstraction?", "answers": {"text": ["Spark’s primary abstraction is a distributed collection of items called a Dataset."], "answer_start": [96]}}
{"context": "E.md\n\"\n)\nYou can get values from DataFrame directly, by calling some actions, or transform the DataFrame to get a new one. For more details, please read the\nAPI doc\n.\n>>>\ntextFile\n.\ncount\n()\n# Number of rows in this DataFrame\n126\n>>>\ntextFile\n.\nfirst\n()\n# First row in this DataFrame\nRow\n(\nvalue\n=\nu\n'\n# Apache Spark\n'\n)\nNow let’s transform this DataFrame to a new one. We call\nfilter\nto return a new DataFrame with a subset of the lines in the file.\n>>>\nlinesWithSpark\n=\ntextFile\n.\nfilter\n(\ntextFile\n.\nvalue\n.\ncontains\n(\n\"\nSpark\n\"\n))\nWe can chain together transformations and actions:\n>>>\ntextFile\n.\nfilter\n(\ntextFile\n.\nvalue\n.\ncontains\n(\n\"\nSpark\n\"\n)).\ncount\n()\n# How many lines contain \"Spark\"?\n15\n./bin/spark-shell\nSpark’s primary abstraction is a distributed collection of items called a Dataset.", "question": "How many lines contain \"Spark\"?", "answers": {"text": ["15"], "answer_start": [697]}}
{"context": ")).\ncount\n()\n# How many lines contain \"Spark\"?\n15\n./bin/spark-shell\nSpark’s primary abstraction is a distributed collection of items called a Dataset. Datasets can be created from Hadoop InputFormats (such as HDFS files) or by transforming other Datasets. Let’s make a new Dataset from the text of the README file in the Spark source directory:\nscala\n>\nval\ntextFile\n=\nspark\n.\nread\n.\ntextFile\n(\n\"README.md\"\n)\ntextFile\n:\norg.apache.spark.sql.Dataset\n[\nString\n]\n=\n[\nvalue:\nstring\n]\nYou can get values from Dataset directly, by calling some actions, or transform the Dataset to get a new one. For more details, please read the\nAPI doc\n.\nscala\n>\ntextFile\n.\ncount\n()\n// Number of items in this Dataset\nres0\n:\nLong\n=\n126\n// May be different from yours as README.md will change over time, similar to other ou", "question": "What is Spark’s primary abstraction?", "answers": {"text": ["Spark’s primary abstraction is a distributed collection of items called a Dataset."], "answer_start": [68]}}
{"context": ".\ncount\n()\n// Number of items in this Dataset\nres0\n:\nLong\n=\n126\n// May be different from yours as README.md will change over time, similar to other outputs\nscala\n>\ntextFile\n.\nfirst\n()\n// First item in this Dataset\nres1\n:\nString\n=\n#\nApache\nSpark\nNow let’s transform this Dataset into a new one. We call\nfilter\nto return a new Dataset with a subset of the items in the file.\nscala\n>\nval\nlinesWithSpark\n=\ntextFile\n.\nfilter\n(\nline\n=>\nline\n.\ncontains\n(\n\"Spark\"\n))\nlinesWithSpark\n:\norg.apache.spark.sql.Dataset\n[\nString\n]\n=\n[\nvalue:\nstring\n]\nWe can chain together transformations and actions:\nscala\n>\ntextFile\n.\nfilter\n(\nline\n=>\nline\n.\ncontains\n(\n\"Spark\"\n)).\ncount\n()\n// How many lines contain \"Spark\"?\nres3\n:\nLong\n=\n15\nMore on Dataset Operations\nDataset actions and transformations can be used for more co", "question": "How many lines contain \"Spark\"?", "answers": {"text": ["15"], "answer_start": [711]}}
{"context": ").\ncount\n()\n// How many lines contain \"Spark\"?\nres3\n:\nLong\n=\n15\nMore on Dataset Operations\nDataset actions and transformations can be used for more complex computations. Let’s say we want to find the line with the most words:\n>>>\nfrom\npyspark.sql\nimport\nfunctions\nas\nsf\n>>>\ntextFile\n.\nselect\n(\nsf\n.\nsize\n(\nsf\n.\nsplit\n(\ntextFile\n.\nvalue\n,\n\"\n\\s+\n\"\n)).\nname\n(\n\"\nnumWords\n\"\n)).\nagg\n(\nsf\n.\nmax\n(\nsf\n.\ncol\n(\n\"\nnumWords\n\"\n))).\ncollect\n()\n[\nRow\n(\nmax\n(\nnumWords\n)\n=\n15\n)]\nThis first maps a line to an integer value and aliases it as “numWords”, creating a new DataFrame.\nagg\nis called on that DataFrame to find the largest word count. The arguments to\nselect\nand\nagg\nare both\nColumn\n, we can use\ndf.colName\nto get a column from a DataFrame. We can also import pyspark.sql.functions, which provides a lot of c", "question": "What is the maximum number of words found in a line of the text file?", "answers": {"text": ["15"], "answer_start": [61]}}
{"context": "\nand\nagg\nare both\nColumn\n, we can use\ndf.colName\nto get a column from a DataFrame. We can also import pyspark.sql.functions, which provides a lot of convenient functions to build a new Column from an old one.\nOne common data flow pattern is MapReduce, as popularized by Hadoop. Spark can implement MapReduce flows easily:\n>>>\nwordCounts\n=\ntextFile\n.\nselect\n(\nsf\n.\nexplode\n(\nsf\n.\nsplit\n(\ntextFile\n.\nvalue\n,\n\"\n\\s+\n\"\n)).\nalias\n(\n\"\nword\n\"\n)).\ngroupBy\n(\n\"\nword\n\"\n).\ncount\n()\nHere, we use the\nexplode\nfunction in\nselect\n, to transform a Dataset of lines to a Dataset of words, and then combine\ngroupBy\nand\ncount\nto compute the per-word counts in the file as a DataFrame of 2 columns: “word” and “count”. To collect the word counts in our shell, we can call\ncollect\n:\n>>>\nwordCounts\n.\ncollect\n()\n[\nRow\n(\nwor", "question": "How can we get a column from a DataFrame?", "answers": {"text": ["df.colName"], "answer_start": [38]}}
{"context": "ions declared elsewhere. We’ll use\nMath.max()\nfunction to make this code easier to understand:\nscala\n>\nimport\njava.lang.Math\nimport\njava.lang.Math\nscala\n>\ntextFile\n.\nmap\n(\nline\n=>\nline\n.\nsplit\n(\n\" \"\n).\nsize\n).\nreduce\n((\na\n,\nb\n)\n=>\nMath\n.\nmax\n(\na\n,\nb\n))\nres5\n:\nInt\n=\n15\nOne common data flow pattern is MapReduce, as popularized by Hadoop. Spark can implement MapReduce flows easily:\nscala\n>\nval\nwordCounts\n=\ntextFile\n.\nflatMap\n(\nline\n=>\nline\n.\nsplit\n(\n\" \"\n)).\ngroupByKey\n(\nidentity\n).\ncount\n()\nwordCounts\n:\norg.apache.spark.sql.Dataset\n[(\nString\n,\nLong\n)]\n=\n[\nvalue:\nstring\n,\ncount\n(\n1\n)\n:\nbigint\n]\nHere, we call\nflatMap\nto transform a Dataset of lines to a Dataset of words, and then combine\ngroupByKey\nand\ncount\nto compute the per-word counts in the file as a Dataset of (String, Long) pairs. To col", "question": "What function is used to make the code easier to understand?", "answers": {"text": ["Math.max()"], "answer_start": [35]}}
{"context": "f you are building a packaged PySpark application or library you can add it to your setup.py file as:\ninstall_requires\n=\n[\n'\npyspark==4.0.0\n'\n]\nAs an example, we’ll create a simple Spark application,\nSimpleApp.py\n:\n\"\"\"\nSimpleApp.py\n\"\"\"\nfrom\npyspark.sql\nimport\nSparkSession\nlogFile\n=\n\"\nYOUR_SPARK_HOME/README.md\n\"\n# Should be some file on your system\nspark\n=\nSparkSession\n.\nbuilder\n.\nappName\n(\n\"\nSimpleApp\n\"\n).\ngetOrCreate\n()\nlogData\n=\nspark\n.\nread\n.\ntext\n(\nlogFile\n).\ncache\n()\nnumAs\n=\nlogData\n.\nfilter\n(\nlogData\n.\nvalue\n.\ncontains\n(\n'\na\n'\n)).\ncount\n()\nnumBs\n=\nlogData\n.\nfilter\n(\nlogData\n.\nvalue\n.\ncontains\n(\n'\nb\n'\n)).\ncount\n()\nprint\n(\n\"\nLines with a: %i, lines with b: %i\n\"\n%\n(\nnumAs\n,\nnumBs\n))\nspark\n.\nstop\n()\nThis program just counts the number of lines containing ‘a’ and the number containing ‘b’", "question": "How can you add PySpark as a dependency in a packaged application or library?", "answers": {"text": ["install_requires\n=\n[\n'\npyspark==4.0.0\n'\n]"], "answer_start": [102]}}
{"context": "run this application using the\nbin/spark-submit\nscript:\n# Use spark-submit to run your application\n$\nYOUR_SPARK_HOME/bin/spark-submit\n\\\n--master\n\"local[4]\"\n\\\nSimpleApp.py\n...\nLines with a: 46, Lines with b: 23\nIf you have PySpark pip installed into your environment (e.g.,\npip install pyspark\n), you can run your application with the regular Python interpreter or use the provided ‘spark-submit’ as you prefer.\n# Use the Python interpreter to run your application\n$\npython SimpleApp.py\n...\nLines with a: 46, Lines with b: 23\nWe’ll create a very simple Spark application in Scala–so simple, in fact, that it’s\nnamed\nSimpleApp.scala\n:\n/* SimpleApp.scala */\nimport\norg.apache.spark.sql.SparkSession\nobject\nSimpleApp\n{\ndef\nmain\n(\nargs\n:\nArray\n[\nString\n])\n:\nUnit\n=\n{\nval\nlogFile\n=\n\"YOUR_SPARK_HOME/README.", "question": "How can you run your application if you have PySpark pip installed?", "answers": {"text": ["you can run your application with the regular Python interpreter or use the provided ‘spark-submit’ as you prefer."], "answer_start": [296]}}
{"context": "a */\nimport\norg.apache.spark.sql.SparkSession\nobject\nSimpleApp\n{\ndef\nmain\n(\nargs\n:\nArray\n[\nString\n])\n:\nUnit\n=\n{\nval\nlogFile\n=\n\"YOUR_SPARK_HOME/README.md\"\n// Should be some file on your system\nval\nspark\n=\nSparkSession\n.\nbuilder\n.\nappName\n(\n\"Simple Application\"\n).\ngetOrCreate\n()\nval\nlogData\n=\nspark\n.\nread\n.\ntextFile\n(\nlogFile\n).\ncache\n()\nval\nnumAs\n=\nlogData\n.\nfilter\n(\nline\n=>\nline\n.\ncontains\n(\n\"a\"\n)).\ncount\n()\nval\nnumBs\n=\nlogData\n.\nfilter\n(\nline\n=>\nline\n.\ncontains\n(\n\"b\"\n)).\ncount\n()\nprintln\n(\ns\n\"Lines with a: $numAs, Lines with b: $numBs\"\n)\nspark\n.\nstop\n()\n}\n}\nNote that applications should define a\nmain()\nmethod instead of extending\nscala.App\n.\nSubclasses of\nscala.App\nmay not work correctly.\nThis program just counts the number of lines containing ‘a’ and the number containing ‘b’ in the\nSpar", "question": "What does the program count?", "answers": {"text": ["This program just counts the number of lines containing ‘a’ and the number containing ‘b’ in the"], "answer_start": [699]}}
{"context": "\nSubclasses of\nscala.App\nmay not work correctly.\nThis program just counts the number of lines containing ‘a’ and the number containing ‘b’ in the\nSpark README. Note that you’ll need to replace YOUR_SPARK_HOME with the location where Spark is\ninstalled. Unlike the earlier examples with the Spark shell, which initializes its own SparkSession,\nwe initialize a SparkSession as part of the program.\nWe call\nSparkSession.builder\nto construct a\nSparkSession\n, then set the application name, and finally call\ngetOrCreate\nto get the\nSparkSession\ninstance.\nOur application depends on the Spark API, so we’ll also include an sbt configuration file,\nbuild.sbt\n, which explains that Spark is a dependency. This file also adds a repository that\nSpark depends on:\nname\n:=\n\"Simple Project\"\nversion\n:=\n\"1.0\"\nscalaVe", "question": "How is a SparkSession initialized in this program?", "answers": {"text": ["we initialize a SparkSession as part of the program."], "answer_start": [343]}}
{"context": ", which explains that Spark is a dependency. This file also adds a repository that\nSpark depends on:\nname\n:=\n\"Simple Project\"\nversion\n:=\n\"1.0\"\nscalaVersion\n:=\n\"2.13.16\"\nlibraryDependencies\n+=\n\"org.apache.spark\"\n%%\n\"spark-sql\"\n%\n\"4.0.0\"\nFor sbt to work correctly, we’ll need to layout\nSimpleApp.scala\nand\nbuild.sbt\naccording to the typical directory structure. Once that is in place, we can create a JAR package\ncontaining the application’s code, then use the\nspark-submit\nscript to run our program.\n# Your directory layout should look like this\n$\nfind\n.\n.\n./build.sbt\n./src\n./src/main\n./src/main/scala\n./src/main/scala/SimpleApp.scala\n# Package a jar containing your application\n$\nsbt package\n...\n[\ninfo] Packaging\n{\n..\n}\n/\n{\n..\n}\n/target/scala-2.13/simple-project_2.13-1.0.jar\n# Use spark-submit to ", "question": "What version of Scala is being used in this project?", "answers": {"text": ["2.13.16"], "answer_start": [160]}}
{"context": " containing your application\n$\nsbt package\n...\n[\ninfo] Packaging\n{\n..\n}\n/\n{\n..\n}\n/target/scala-2.13/simple-project_2.13-1.0.jar\n# Use spark-submit to run your application\n$\nYOUR_SPARK_HOME/bin/spark-submit\n\\\n--class\n\"SimpleApp\"\n\\\n--master\n\"local[4]\"\n\\\ntarget/scala-2.13/simple-project_2.13-1.0.jar\n...\nLines with a: 46, Lines with b: 23\nThis example will use Maven to compile an application JAR, but any similar build system will work.\nWe’ll create a very simple Spark application,\nSimpleApp.java\n:\n/* SimpleApp.java */\nimport\norg.apache.spark.sql.SparkSession\n;\nimport\norg.apache.spark.sql.Dataset\n;\npublic\nclass\nSimpleApp\n{\npublic\nstatic\nvoid\nmain\n(\nString\n[]\nargs\n)\n{\nString\nlogFile\n=\n\"YOUR_SPARK_HOME/README.md\"\n;\n// Should be some file on your system\nSparkSession\nspark\n=\nSparkSession\n.\nbuilder\n", "question": "How can you run your application after packaging it with sbt?", "answers": {"text": ["Use spark-submit to run your application"], "answer_start": [130]}}
{"context": "(\nString\n[]\nargs\n)\n{\nString\nlogFile\n=\n\"YOUR_SPARK_HOME/README.md\"\n;\n// Should be some file on your system\nSparkSession\nspark\n=\nSparkSession\n.\nbuilder\n().\nappName\n(\n\"Simple Application\"\n).\ngetOrCreate\n();\nDataset\n<\nString\n>\nlogData\n=\nspark\n.\nread\n().\ntextFile\n(\nlogFile\n).\ncache\n();\nlong\nnumAs\n=\nlogData\n.\nfilter\n(\ns\n->\ns\n.\ncontains\n(\n\"a\"\n)).\ncount\n();\nlong\nnumBs\n=\nlogData\n.\nfilter\n(\ns\n->\ns\n.\ncontains\n(\n\"b\"\n)).\ncount\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Lines with a: \"\n+\nnumAs\n+\n\", lines with b: \"\n+\nnumBs\n);\nspark\n.\nstop\n();\n}\n}\nThis program just counts the number of lines containing ‘a’ and the number containing ‘b’ in the\nSpark README. Note that you’ll need to replace YOUR_SPARK_HOME with the location where Spark is\ninstalled. Unlike the earlier examples with the Spark shell, which initializes its", "question": "What file is used as input for counting lines containing 'a' and 'b'?", "answers": {"text": ["\"YOUR_SPARK_HOME/README.md\""], "answer_start": [38]}}
{"context": "ll need to replace YOUR_SPARK_HOME with the location where Spark is\ninstalled. Unlike the earlier examples with the Spark shell, which initializes its own SparkSession,\nwe initialize a SparkSession as part of the program.\nTo build the program, we also write a Maven\npom.xml\nfile that lists Spark as a dependency.\nNote that Spark artifacts are tagged with a Scala version.\n<project>\n<groupId>\nedu.berkeley\n</groupId>\n<artifactId>\nsimple-project\n</artifactId>\n<modelVersion>\n4.0.0\n</modelVersion>\n<name>\nSimple Project\n</name>\n<packaging>\njar\n</packaging>\n<version>\n1.0\n</version>\n<dependencies>\n<dependency>\n<!-- Spark dependency -->\n<groupId>\norg.apache.spark\n</groupId>\n<artifactId>\nspark-sql_2.13\n</artifactId>\n<version>\n4.0.0\n</version>\n<scope>\nprovided\n</scope>\n</dependency>\n</dependencies>\n</pr", "question": "What Scala version is the Spark artifact tagged with in the provided pom.xml file?", "answers": {"text": ["spark-sql_2.13"], "answer_start": [684]}}
{"context": "che.spark\n</groupId>\n<artifactId>\nspark-sql_2.13\n</artifactId>\n<version>\n4.0.0\n</version>\n<scope>\nprovided\n</scope>\n</dependency>\n</dependencies>\n</project>\nWe lay out these files according to the canonical Maven directory structure:\n$\nfind\n.\n./pom.xml\n./src\n./src/main\n./src/main/java\n./src/main/java/SimpleApp.java\nNow, we can package the application using Maven and execute it with\n./bin/spark-submit\n.\n# Package a JAR containing your application\n$\nmvn package\n...\n[\nINFO] Building jar:\n{\n..\n}\n/\n{\n..\n}\n/target/simple-project-1.0.jar\n# Use spark-submit to run your application\n$\nYOUR_SPARK_HOME/bin/spark-submit\n\\\n--class\n\"SimpleApp\"\n\\\n--master\n\"local[4]\"\n\\\ntarget/simple-project-1.0.jar\n...\nLines with a: 46, Lines with b: 23\nOther dependency management tools such as Conda and pip can be also us", "question": "How can the application be packaged using Maven?", "answers": {"text": ["mvn package"], "answer_start": [452]}}
{"context": "an run them as follows:\n# For Python examples, use spark-submit directly:\n./bin/spark-submit examples/src/main/python/pi.py\n# For Scala and Java, use run-example:\n./bin/run-example SparkPi\n# For R examples, use spark-submit directly:\n./bin/spark-submit examples/src/main/r/dataframe.R", "question": "How can Python examples be run?", "answers": {"text": ["use spark-submit directly:"], "answer_start": [47]}}
{"context": " in OpenStack Swift using the\nsame URI formats as in Hadoop. You can specify a path in Swift as input through a\nURI of the form\nswift://container.PROVIDER/path\n. You will also need to set your\nSwift security credentials, through\ncore-site.xml\nor via\nSparkContext.hadoopConfiguration\n.\nThe current Swift driver requires Swift to use the Keystone authentication method, or\nits Rackspace-specific predecessor.\nConfiguring Swift for Better Data Locality\nAlthough not mandatory, it is recommended to configure the proxy server of Swift with\nlist_endpoints\nto have better data locality. More information is\navailable here\n.\nDependencies\nThe Spark application should include\nhadoop-openstack\ndependency, which can\nbe done by including the\nhadoop-cloud\nmodule for the specific version of spark used.\nFor exam", "question": "How can Swift security credentials be set?", "answers": {"text": ["through\ncore-site.xml\nor via\nSparkContext.hadoopConfiguration"], "answer_start": [221]}}
{"context": "on should include\nhadoop-openstack\ndependency, which can\nbe done by including the\nhadoop-cloud\nmodule for the specific version of spark used.\nFor example, for Maven support, add the following to the\npom.xml\nfile:\n<dependencyManagement>\n...\n<dependency>\n<groupId>\norg.apache.spark\n</groupId>\n<artifactId>\nhadoop-cloud_2.13\n</artifactId>\n<version>\n${spark.version}\n</version>\n</dependency>\n...\n</dependencyManagement>\nConfiguration Parameters\nCreate\ncore-site.xml\nand place it inside Spark’s\nconf\ndirectory.\nThe main category of parameters that should be configured is the authentication parameters\nrequired by Keystone.\nThe following table contains a list of Keystone mandatory parameters.\nPROVIDER\ncan be\nany (alphanumeric) name.\nProperty Name\nMeaning\nRequired\nfs.swift.service.PROVIDER.auth.url\nKeys", "question": "What should be added to the pom.xml file for Maven support?", "answers": {"text": ["<dependency>\n<groupId>\norg.apache.spark\n</groupId>\n<artifactId>\nhadoop-cloud_2.13\n</artifactId>\n<version>\n${spark.version}\n</version>\n</dependency>"], "answer_start": [240]}}
{"context": "list of Keystone mandatory parameters.\nPROVIDER\ncan be\nany (alphanumeric) name.\nProperty Name\nMeaning\nRequired\nfs.swift.service.PROVIDER.auth.url\nKeystone Authentication URL\nMandatory\nfs.swift.service.PROVIDER.auth.endpoint.prefix\nKeystone endpoints prefix\nOptional\nfs.swift.service.PROVIDER.tenant\nTenant\nMandatory\nfs.swift.service.PROVIDER.username\nUsername\nMandatory\nfs.swift.service.PROVIDER.password\nPassword\nMandatory\nfs.swift.service.PROVIDER.http.port\nHTTP port\nMandatory\nfs.swift.service.PROVIDER.region\nKeystone region\nMandatory\nfs.swift.service.PROVIDER.public\nIndicates whether to use the public (off cloud) or private (in cloud; no transfer fees) endpoints\nMandatory\nFor example, assume\nPROVIDER=SparkTest\nand Keystone contains user\ntester\nwith password\ntesting\ndefined for tenant\ntest\n.", "question": "Quais parâmetros são obrigatórios para a configuração do Keystone?", "answers": {"text": ["Mandatory"], "answer_start": [174]}}
{"context": "fer fees) endpoints\nMandatory\nFor example, assume\nPROVIDER=SparkTest\nand Keystone contains user\ntester\nwith password\ntesting\ndefined for tenant\ntest\n. Then\ncore-site.xml\nshould include:\n<configuration>\n<property>\n<name>\nfs.swift.service.SparkTest.auth.url\n</name>\n<value>\nhttp://127.0.0.1:5000/v2.0/tokens\n</value>\n</property>\n<property>\n<name>\nfs.swift.service.SparkTest.auth.endpoint.prefix\n</name>\n<value>\nendpoints\n</value>\n</property>\n<name>\nfs.swift.service.SparkTest.http.port\n</name>\n<value>\n8080\n</value>\n</property>\n<property>\n<name>\nfs.swift.service.SparkTest.region\n</name>\n<value>\nRegionOne\n</value>\n</property>\n<property>\n<name>\nfs.swift.service.SparkTest.public\n</name>\n<value>\ntrue\n</value>\n</property>\n<property>\n<name>\nfs.swift.service.SparkTest.tenant\n</name>\n<value>\ntest\n</value>", "question": "What value should be defined for fs.swift.service.SparkTest.tenant in core-site.xml?", "answers": {"text": ["test"], "answer_start": [96]}}
{"context": "esting purposes when running Spark\nvia\nspark-shell\n.\nFor job submissions they should be provided via\nsparkContext.hadoopConfiguration\n.", "question": "How should configuration properties be provided for job submissions?", "answers": {"text": ["sparkContext.hadoopConfiguration"], "answer_start": [101]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nHardware Provisioning\nA common question received by Spark developers is how to configure hardware for it. While the right\nhardware will depend on the situation, we make the following recommendations.\nStorage Systems\nBecause most Spark jobs will likely", "question": "What is a common question received by Spark developers?", "answers": {"text": ["A common question received by Spark developers is how to configure hardware for it."], "answer_start": [571]}}
{"context": "duce.tasktracker.map.tasks.maximum\nand\nmapreduce.tasktracker.reduce.tasks.maximum\nfor number of tasks). Alternatively, you can run\nHadoop and Spark on a common cluster manager like\nHadoop YARN\n.\nIf this is not possible, run Spark on different nodes in the same local-area network as HDFS.\nFor low-latency data stores like HBase, it may be preferable to run computing jobs on different\nnodes than the storage system to avoid interference.\nLocal Disks\nWhile Spark can perform a lot of its computation in memory, it still uses local disks to store\ndata that doesn’t fit in RAM, as well as to preserve intermediate output between stages. We\nrecommend having\n4-8 disks\nper node, configured\nwithout\nRAID (just as separate mount points).\nIn Linux, mount the disks with the\nnoatime\noption\nto reduce unnecessa", "question": "How many disks per node are recommended for Spark?", "answers": {"text": ["4-8 disks"], "answer_start": [654]}}
{"context": "d likely provision at least\n8-16 cores\nper machine. Depending on the CPU\ncost of your workload, you may also need more: once data is in memory, most applications are\neither CPU- or network-bound.", "question": "What is a likely CPU core provision for each machine?", "answers": {"text": ["8-16 cores"], "answer_start": [28]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nRDD Programming Guide\nOverview\nLinking with Spark\nInitializing Spark\nUsing the Shell\nResilient Distributed Datasets (RDDs)\nParallelized Collections\nExternal Datasets\nRDD Operations\nBasics\nPassing Functions to Spark\nUnderstanding closures\nExample\nLocal", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars"], "answer_start": [46]}}
{"context": "it applications to a cluster.\nYou can also use\nbin/pyspark\nto launch an interactive Python shell.\nIf you wish to access HDFS data, you need to use a build of PySpark linking\nto your version of HDFS.\nPrebuilt packages\nare also available on the Spark homepage\nfor common HDFS versions.\nFinally, you need to import some Spark classes into your program. Add the following line:\nfrom\npyspark\nimport\nSparkContext\n,\nSparkConf\nPySpark requires the same minor version of Python in both driver and workers. It uses the default python version in PATH,\nyou can specify which version of Python you want to use by\nPYSPARK_PYTHON\n, for example:\n$ PYSPARK_PYTHON\n=\npython3.8 bin/pyspark\n$ PYSPARK_PYTHON\n=\n/path-to-your-pypy/pypy bin/spark-submit examples/src/main/python/pi.py\nSpark 4.0.0 is built and distributed t", "question": "How can you specify which version of Python to use with PySpark?", "answers": {"text": ["you can specify which version of Python you want to use by\nPYSPARK_PYTHON"], "answer_start": [541]}}
{"context": "ython3.8 bin/pyspark\n$ PYSPARK_PYTHON\n=\n/path-to-your-pypy/pypy bin/spark-submit examples/src/main/python/pi.py\nSpark 4.0.0 is built and distributed to work with Scala 2.13\nby default. (Spark can be built to work with other versions of Scala, too.) To write\napplications in Scala, you will need to use a compatible Scala version (e.g. 2.13.X).\nTo write a Spark application, you need to add a Maven dependency on Spark. Spark is available through Maven Central at:\ngroupId = org.apache.spark\nartifactId = spark-core_2.13\nversion = 4.0.0\nIn addition, if you wish to access an HDFS cluster, you need to add a dependency on\nhadoop-client\nfor your version of HDFS.\ngroupId = org.apache.hadoop\nartifactId = hadoop-client\nversion = <your-hdfs-version>\nFinally, you need to import some Spark classes into you", "question": "What is the default Scala version Spark 4.0.0 is built to work with?", "answers": {"text": ["Scala 2.13"], "answer_start": [162]}}
{"context": " of HDFS.\ngroupId = org.apache.hadoop\nartifactId = hadoop-client\nversion = <your-hdfs-version>\nFinally, you need to import some Spark classes into your program. Add the following lines:\nimport\norg.apache.spark.SparkContext\nimport\norg.apache.spark.SparkConf\n(Before Spark 1.3.0, you need to explicitly\nimport org.apache.spark.SparkContext._\nto enable essential implicit conversions.)\nSpark 4.0.0 supports\nlambda expressions\nfor concisely writing functions, otherwise you can use the classes in the\norg.apache.spark.api.java.function\npackage.\nNote that support for Java 7 was removed in Spark 2.2.0.\nTo write a Spark application in Java, you need to add a dependency on Spark. Spark is available through Maven Central at:\ngroupId = org.apache.spark\nartifactId = spark-core_2.13\nversion = 4.0.0\nIn addit", "question": "What versions of Spark are mentioned in the text regarding Java support?", "answers": {"text": ["Note that support for Java 7 was removed in Spark 2.2.0."], "answer_start": [541]}}
{"context": "d a dependency on Spark. Spark is available through Maven Central at:\ngroupId = org.apache.spark\nartifactId = spark-core_2.13\nversion = 4.0.0\nIn addition, if you wish to access an HDFS cluster, you need to add a dependency on\nhadoop-client\nfor your version of HDFS.\ngroupId = org.apache.hadoop\nartifactId = hadoop-client\nversion = <your-hdfs-version>\nFinally, you need to import some Spark classes into your program. Add the following lines:\nimport\norg.apache.spark.api.java.JavaSparkContext\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.SparkConf\n;\nInitializing Spark\nThe first thing a Spark program must do is to create a\nSparkContext\nobject, which tells Spark\nhow to access a cluster. To create a\nSparkContext\nyou first need to build a\nSparkConf\nobject\nthat contains informa", "question": "What are the Maven coordinates for the Spark core dependency?", "answers": {"text": ["groupId = org.apache.spark\nartifactId = spark-core_2.13\nversion = 4.0.0"], "answer_start": [70]}}
{"context": ". Inside the notebook, you can input the command\n%pylab inline\nas part of\nyour notebook before you start to try Spark from the Jupyter notebook.\nIn the Spark shell, a special interpreter-aware SparkContext is already created for you, in the\nvariable called\nsc\n. Making your own SparkContext will not work. You can set which master the\ncontext connects to using the\n--master\nargument, and you can add JARs to the classpath\nby passing a comma-separated list to the\n--jars\nargument. You can also add dependencies\n(e.g. Spark Packages) to your shell session by supplying a comma-separated list of Maven coordinates\nto the\n--packages\nargument. Any additional repositories where dependencies might exist (e.g. Sonatype)\ncan be passed to the\n--repositories\nargument. For example, to run\nbin/spark-shell\non e", "question": "What variable holds the pre-created SparkContext in the Spark shell?", "answers": {"text": ["sc"], "answer_start": [257]}}
{"context": "orkers or use a network-mounted shared file system.\nAll of Spark’s file-based input methods, including\ntextFile\n, support running on directories, compressed files, and wildcards as well. For example, you can use\ntextFile(\"/my/directory\")\n,\ntextFile(\"/my/directory/*.txt\")\n, and\ntextFile(\"/my/directory/*.gz\")\n.\nThe\ntextFile\nmethod also takes an optional second argument for controlling the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Note that you cannot have fewer partitions than blocks.\nApart from text files, Spark’s Python API also supports several other data formats:\nSparkContext.wholeTextFiles\nlets you read a di", "question": "What is the default block size in HDFS that Spark uses to create partitions when reading files?", "answers": {"text": ["blocks being 128MB by default in HDFS"], "answer_start": [492]}}
{"context": "itions than blocks.\nApart from text files, Spark’s Python API also supports several other data formats:\nSparkContext.wholeTextFiles\nlets you read a directory containing multiple small text files, and returns each of them as (filename, content) pairs. This is in contrast with\ntextFile\n, which would return one record per line in each file.\nRDD.saveAsPickleFile\nand\nSparkContext.pickleFile\nsupport saving an RDD in a simple format consisting of pickled Python objects. Batching is used on pickle serialization, with default batch size 10.\nSequenceFile and Hadoop Input/Output Formats\nNote\nthis feature is currently marked\nExperimental\nand is intended for advanced users. It may be replaced in future with read/write support based on Spark SQL, in which case Spark SQL is the preferred approach.\nWritab", "question": "What does SparkContext.wholeTextFiles return when reading a directory of text files?", "answers": {"text": ["returns each of them as (filename, content) pairs."], "answer_start": [200]}}
{"context": "for advanced users. It may be replaced in future with read/write support based on Spark SQL, in which case Spark SQL is the preferred approach.\nWritable Support\nPySpark SequenceFile support loads an RDD of key-value pairs within Java, converts Writables to base Java types, and pickles the\nresulting Java objects using\npickle\n. When saving an RDD of key-value pairs to SequenceFile,\nPySpark does the reverse. It unpickles Python objects into Java objects and then converts them to Writables. The following\nWritables are automatically converted:\nWritable Type\nPython Type\nText\nstr\nIntWritable\nint\nFloatWritable\nfloat\nDoubleWritable\nfloat\nBooleanWritable\nbool\nBytesWritable\nbytearray\nNullWritable\nNone\nMapWritable\ndict\nArrays are not handled out-of-the-box. Users need to specify custom\nArrayWritable\ns", "question": "What Python type is automatically converted from IntWritable?", "answers": {"text": ["int"], "answer_start": [437]}}
{"context": "extFile\n, support running on directories, compressed files, and wildcards as well. For example, you can use\ntextFile(\"/my/directory\")\n,\ntextFile(\"/my/directory/*.txt\")\n, and\ntextFile(\"/my/directory/*.gz\")\n. When multiple files are read, the order of the partitions depends on the order the files are returned from the filesystem. It may or may not, for example, follow the lexicographic ordering of the files by path. Within a partition, elements are ordered according to their order in the underlying file.\nThe\ntextFile\nmethod also takes an optional second argument for controlling the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number of partitions by passing a lar", "question": "What is the default block size used by Spark when creating partitions from a file?", "answers": {"text": ["blocks being 128MB by default in HDFS"], "answer_start": [689]}}
{"context": "cessible at the same path on worker nodes. Either copy the file to all workers or use a network-mounted shared file system.\nAll of Spark’s file-based input methods, including\ntextFile\n, support running on directories, compressed files, and wildcards as well. For example, you can use\ntextFile(\"/my/directory\")\n,\ntextFile(\"/my/directory/*.txt\")\n, and\ntextFile(\"/my/directory/*.gz\")\n.\nThe\ntextFile\nmethod also takes an optional second argument for controlling the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Note that you cannot have fewer partitions than blocks.\nApart from text files, Spark’s Java API also supports seve", "question": "What is the default block size in HDFS used by Spark when creating partitions for a file?", "answers": {"text": ["blocks being 128MB by default in HDFS"], "answer_start": [564]}}
{"context": "rtitions by passing a larger value. Note that you cannot have fewer partitions than blocks.\nApart from text files, Spark’s Java API also supports several other data formats:\nJavaSparkContext.wholeTextFiles\nlets you read a directory containing multiple small text files, and returns each of them as (filename, content) pairs. This is in contrast with\ntextFile\n, which would return one record per line in each file.\nFor\nSequenceFiles\n, use SparkContext’s\nsequenceFile[K, V]\nmethod where\nK\nand\nV\nare the types of key and values in the file. These should be subclasses of Hadoop’s\nWritable\ninterface, like\nIntWritable\nand\nText\n.\nFor other Hadoop InputFormats, you can use the\nJavaSparkContext.hadoopRDD\nmethod, which takes an arbitrary\nJobConf\nand input format class, key class and value class. Set these", "question": "How does JavaSparkContext read a directory containing multiple small text files?", "answers": {"text": ["lets you read a directory containing multiple small text files, and returns each of them as (filename, content) pairs."], "answer_start": [206]}}
{"context": "mats, you can use the\nJavaSparkContext.hadoopRDD\nmethod, which takes an arbitrary\nJobConf\nand input format class, key class and value class. Set these the same way you would for a Hadoop job with your input source. You can also use\nJavaSparkContext.newAPIHadoopRDD\nfor InputFormats based on the “new” MapReduce API (\norg.apache.hadoop.mapreduce\n).\nJavaRDD.saveAsObjectFile\nand\nJavaSparkContext.objectFile\nsupport saving an RDD in a simple format consisting of serialized Java objects. While this is not as efficient as specialized formats like Avro, it offers an easy way to save any RDD.\nRDD Operations\nRDDs support two types of operations:\ntransformations\n, which create a new dataset from an existing one, and\nactions\n, which return a value to the driver program after running a computation on the", "question": "What method can be used to create an RDD from a Hadoop JobConf?", "answers": {"text": ["JavaSparkContext.hadoopRDD"], "answer_start": [22]}}
{"context": "\n(\nMyFunctions\n.\nfunc1\n)\nNote that while it is also possible to pass a reference to a method in a class instance (as opposed to\na singleton object), this requires sending the object that contains that class along with the method.\nFor example, consider:\nclass\nMyClass\n{\ndef\nfunc1\n(\ns\n:\nString\n)\n:\nString\n=\n{\n...\n}\ndef\ndoStuff\n(\nrdd\n:\nRDD\n[\nString\n])\n:\nRDD\n[\nString\n]\n=\n{\nrdd\n.\nmap\n(\nfunc1\n)\n}\n}\nHere, if we create a new\nMyClass\ninstance and call\ndoStuff\non it, the\nmap\ninside there references the\nfunc1\nmethod\nof that\nMyClass\ninstance\n, so the whole object needs to be sent to the cluster. It is\nsimilar to writing\nrdd.map(x => this.func1(x))\n.\nIn a similar way, accessing fields of the outer object will reference the whole object:\nclass\nMyClass\n{\nval\nfield\n=\n\"Hello\"\ndef\ndoStuff\n(\nrdd\n:\nRDD\n[\nString", "question": "What happens when accessing fields of the outer object?", "answers": {"text": ["In a similar way, accessing fields of the outer object will reference the whole object:"], "answer_start": [644]}}
{"context": " the driver and will reference the same original\ncounter\n, and may actually update it.\nTo ensure well-defined behavior in these sorts of scenarios one should use an\nAccumulator\n. Accumulators in Spark are used specifically to provide a mechanism for safely updating a variable when execution is split up across worker nodes in a cluster. The Accumulators section of this guide discusses these in more detail.\nIn general, closures - constructs like loops or locally defined methods, should not be used to mutate some global state. Spark does not define or guarantee the behavior of mutations to objects referenced from outside of closures. Some code that does this may work in local mode, but that’s just by accident and such code will not behave as expected in distributed mode. Use an Accumulator in", "question": "What should one use to ensure well-defined behavior when updating a variable across worker nodes in a cluster?", "answers": {"text": ["Accumulators in Spark are used specifically to provide a mechanism for safely updating a variable when execution is split up across worker nodes in a cluster."], "answer_start": [179]}}
{"context": "ts on the driver, one can use the\ncollect()\nmethod to first bring the RDD to the driver node thus:\nrdd.collect().foreach(println)\n. This can cause the driver to run out of memory, though, because\ncollect()\nfetches the entire RDD to a single machine; if you only need to print a few elements of the RDD, a safer approach is to use the\ntake()\n:\nrdd.take(100).foreach(println)\n.\nWorking with Key-Value Pairs\nWhile most Spark operations work on RDDs containing any type of objects, a few special operations are\nonly available on RDDs of key-value pairs.\nThe most common ones are distributed “shuffle” operations, such as grouping or aggregating the elements\nby a key.\nIn Python, these operations work on RDDs containing built-in Python tuples such as\n(1, 2)\n.\nSimply create such tuples and then call your", "question": "What method can be used to bring an RDD to the driver node?", "answers": {"text": ["collect()"], "answer_start": [34]}}
{"context": "nts\nby a key.\nIn Python, these operations work on RDDs containing built-in Python tuples such as\n(1, 2)\n.\nSimply create such tuples and then call your desired operation.\nFor example, the following code uses the\nreduceByKey\noperation on key-value pairs to count how\nmany times each line of text occurs in a file:\nlines\n=\nsc\n.\ntextFile\n(\n\"\ndata.txt\n\"\n)\npairs\n=\nlines\n.\nmap\n(\nlambda\ns\n:\n(\ns\n,\n1\n))\ncounts\n=\npairs\n.\nreduceByKey\n(\nlambda\na\n,\nb\n:\na\n+\nb\n)\nWe could also use\ncounts.sortByKey()\n, for example, to sort the pairs alphabetically, and finally\ncounts.collect()\nto bring them back to the driver program as a list of objects.\nWhile most Spark operations work on RDDs containing any type of objects, a few special operations are\nonly available on RDDs of key-value pairs.\nThe most common ones are dis", "question": "What operation is used to count how many times each line of text occurs in a file?", "answers": {"text": ["reduceByKey"], "answer_start": [211]}}
{"context": "d RDD functions and special\nkey-value ones.\nFor example, the following code uses the\nreduceByKey\noperation on key-value pairs to count how\nmany times each line of text occurs in a file:\nJavaRDD\n<\nString\n>\nlines\n=\nsc\n.\ntextFile\n(\n\"data.txt\"\n);\nJavaPairRDD\n<\nString\n,\nInteger\n>\npairs\n=\nlines\n.\nmapToPair\n(\ns\n->\nnew\nTuple2\n(\ns\n,\n1\n));\nJavaPairRDD\n<\nString\n,\nInteger\n>\ncounts\n=\npairs\n.\nreduceByKey\n((\na\n,\nb\n)\n->\na\n+\nb\n);\nWe could also use\ncounts.sortByKey()\n, for example, to sort the pairs alphabetically, and finally\ncounts.collect()\nto bring them back to the driver program as an array of objects.\nNote:\nwhen using custom objects as the key in key-value pair operations, you must be sure that a\ncustom\nequals()\nmethod is accompanied with a matching\nhashCode()\nmethod.  For full details, see\nthe contra", "question": "What operation is used to count how many times each line of text occurs in a file?", "answers": {"text": ["reduceByKey"], "answer_start": [85]}}
{"context": "on\nfraction\nof the data, with or without replacement, using a given random number generator seed.\nunion\n(\notherDataset\n)\nReturn a new dataset that contains the union of the elements in the source dataset and the argument.\nintersection\n(\notherDataset\n)\nReturn a new RDD that contains the intersection of elements in the source dataset and the argument.\ndistinct\n([\nnumPartitions\n]))\nReturn a new dataset that contains the distinct elements of the source dataset.\ngroupByKey\n([\nnumPartitions\n])\nWhen called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs.\nNote:\nIf you are grouping in order to perform an aggregation (such as a sum or\n      average) over each key, using\nreduceByKey\nor\naggregateByKey\nwill yield much better\n      performance.\nNote:\nBy default, the level of pa", "question": "What should you use instead of `groupByKey` if you are grouping to perform an aggregation like a sum or average?", "answers": {"text": ["using\nreduceByKey\nor\naggregateByKey\nwill yield much better\n      performance."], "answer_start": [689]}}
{"context": "a sum or\n      average) over each key, using\nreduceByKey\nor\naggregateByKey\nwill yield much better\n      performance.\nNote:\nBy default, the level of parallelism in the output depends on the number of partitions of the parent RDD.\n      You can pass an optional\nnumPartitions\nargument to set a different number of tasks.\nreduceByKey\n(\nfunc\n, [\nnumPartitions\n])\nWhen called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function\nfunc\n, which must be of type (V,V) => V. Like in\ngroupByKey\n, the number of reduce tasks is configurable through an optional second argument.\naggregateByKey\n(\nzeroValue\n)(\nseqOp\n,\ncombOp\n, [\nnumPartitions\n])\nWhen called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs wher", "question": "What is the type of the reduce function 'func' used in reduceByKey?", "answers": {"text": ["(V,V) => V"], "answer_start": [541]}}
{"context": "nt.\naggregateByKey\n(\nzeroValue\n)(\nseqOp\n,\ncombOp\n, [\nnumPartitions\n])\nWhen called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral \"zero\" value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in\ngroupByKey\n, the number of reduce tasks is configurable through an optional second argument.\nsortByKey\n([\nascending\n], [\nnumPartitions\n])\nWhen called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean\nascending\nargument.\njoin\n(\notherDataset\n, [\nnumPartitions\n])\nWhen called on datasets of type (K, V) and ", "question": "What does aggregateByKey return when called on a dataset of (K, V) pairs?", "answers": {"text": ["returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral \"zero\" value."], "answer_start": [112]}}
{"context": "descending order, as specified in the boolean\nascending\nargument.\njoin\n(\notherDataset\n, [\nnumPartitions\n])\nWhen called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key.\n    Outer joins are supported through\nleftOuterJoin\n,\nrightOuterJoin\n, and\nfullOuterJoin\n.\ncogroup\n(\notherDataset\n, [\nnumPartitions\n])\nWhen called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable<V>, Iterable<W>)) tuples. This operation is also called\ngroupWith\n.\ncartesian\n(\notherDataset\n)\nWhen called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements).\npipe\n(\ncommand\n,\n[envVars]\n)\nPipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the\n", "question": "What does the 'cartesian' function return when called on datasets of types T and U?", "answers": {"text": ["returns a dataset of (T, U) pairs (all pairs of elements)."], "answer_start": [596]}}
{"context": "d associative so that it can be computed correctly in parallel.\ncollect\n()\nReturn all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data.\ncount\n()\nReturn the number of elements in the dataset.\nfirst\n()\nReturn the first element of the dataset (similar to take(1)).\ntake\n(\nn\n)\nReturn an array with the first\nn\nelements of the dataset.\ntakeSample\n(\nwithReplacement\n,\nnum\n, [\nseed\n])\nReturn an array with a random sample of\nnum\nelements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed.\ntakeOrdered\n(\nn\n,\n[ordering]\n)\nReturn the first\nn\nelements of the RDD using either their natural order or a custom comparator.\nsaveAsTextFile\n", "question": "What does the `count()` function do?", "answers": {"text": ["Return the number of elements in the dataset."], "answer_start": [269]}}
{"context": "r seed.\ntakeOrdered\n(\nn\n,\n[ordering]\n)\nReturn the first\nn\nelements of the RDD using either their natural order or a custom comparator.\nsaveAsTextFile\n(\npath\n)\nWrite the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file.\nsaveAsSequenceFile\n(\npath\n)\n(Java and Scala)\nWrite the elements of the dataset as a Hadoop SequenceFile in a given path in the local filesystem, HDFS or any other Hadoop-supported file system. This is available on RDDs of key-value pairs that implement Hadoop's Writable interface. In Scala, it is also\n   available on types that are implicitly convertible to Writable (Spark includes conv", "question": "What does the `saveAsTextFile` method do?", "answers": {"text": ["Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system."], "answer_start": [159]}}
{"context": "hat implement Hadoop's Writable interface. In Scala, it is also\n   available on types that are implicitly convertible to Writable (Spark includes conversions for basic types like Int, Double, String, etc).\nsaveAsObjectFile\n(\npath\n)\n(Java and Scala)\nWrite the elements of the dataset in a simple format using Java serialization, which can then be loaded using\nSparkContext.objectFile()\n.\ncountByKey\n()\nOnly available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key.\nforeach\n(\nfunc\n)\nRun a function\nfunc\non each element of the dataset. This is usually done for side effects such as updating an\nAccumulator\nor interacting with external storage systems.\nNote\n: modifying variables other than Accumulators outside of the\nforeach()\nmay result in undefined behavior. S", "question": "What does the `countByKey` function do?", "answers": {"text": ["Returns a hashmap of (K, Int) pairs with the count of each key."], "answer_start": [440]}}
{"context": " level\n, allowing you, for example,\nto persist the dataset on disk, persist it in memory but as serialized Java objects (to save space),\nreplicate it across nodes.\nThese levels are set by passing a\nStorageLevel\nobject (\nPython\n,\nScala\n,\nJava\n)\nto\npersist()\n. The\ncache()\nmethod is a shorthand for using the default storage level,\nwhich is\nStorageLevel.MEMORY_ONLY\n(store deserialized objects in memory). The full set of\nstorage levels is:\nStorage Level\nMeaning\nMEMORY_ONLY\nStore RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will\n    not be cached and will be recomputed on the fly each time they're needed. This is the default level.\nMEMORY_AND_DISK\nStore RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the\n   ", "question": "What is the default storage level used by the `cache()` method?", "answers": {"text": ["StorageLevel.MEMORY_ONLY"], "answer_start": [339]}}
{"context": "ntinue running tasks on the RDD without\nwaiting to recompute a lost partition.\nRemoving Data\nSpark automatically monitors cache usage on each node and drops out old data partitions in a\nleast-recently-used (LRU) fashion. If you would like to manually remove an RDD instead of waiting for\nit to fall out of the cache, use the\nRDD.unpersist()\nmethod. Note that this method does not\nblock by default. To block until resources are freed, specify\nblocking=true\nwhen calling this method.\nShared Variables\nNormally, when a function passed to a Spark operation (such as\nmap\nor\nreduce\n) is executed on a\nremote cluster node, it works on separate copies of all the variables used in the function. These\nvariables are copied to each machine, and no updates to the variables on the remote machine are\npropagated ", "question": "How can you manually remove an RDD instead of waiting for it to fall out of the cache?", "answers": {"text": ["use the\nRDD.unpersist()\nmethod."], "answer_start": [317]}}
{"context": ");\n// returns [1, 2, 3]\nAfter the broadcast variable is created, it should be used instead of the value\nv\nin any functions\nrun on the cluster so that\nv\nis not shipped to the nodes more than once. In addition, the object\nv\nshould not be modified after it is broadcast in order to ensure that all nodes get the same\nvalue of the broadcast variable (e.g. if the variable is shipped to a new node later).\nTo release the resources that the broadcast variable copied onto executors, call\n.unpersist()\n.\nIf the broadcast is used again afterwards, it will be re-broadcast. To permanently release all\nresources used by the broadcast variable, call\n.destroy()\n. The broadcast variable can’t be used\nafter that. Note that these methods do not block by default. To block until resources are freed,\nspecify\nblocki", "question": "What should be done to release the resources copied onto executors by a broadcast variable?", "answers": {"text": [".unpersist()"], "answer_start": [482]}}
{"context": ". The broadcast variable can’t be used\nafter that. Note that these methods do not block by default. To block until resources are freed,\nspecify\nblocking=true\nwhen calling them.\nAccumulators\nAccumulators are variables that are only “added” to through an associative and commutative operation and can\ntherefore be efficiently supported in parallel. They can be used to implement counters (as in\nMapReduce) or sums. Spark natively supports accumulators of numeric types, and programmers\ncan add support for new types.\nAs a user, you can create named or unnamed accumulators. As seen in the image below, a named accumulator (in this instance\ncounter\n) will display in the web UI for the stage that modifies that accumulator. Spark displays the value for each accumulator modified by a task in the “Tasks”", "question": "What does Spark display in the \"Tasks\" section of the web UI?", "answers": {"text": ["Spark displays the value for each accumulator modified by a task in the “Tasks”"], "answer_start": [721]}}
{"context": "V2\n();\n// Then, register it into spark context:\njsc\n.\nsc\n().\nregister\n(\nmyVectorAcc\n,\n\"MyVectorAcc1\"\n);\nNote that, when programmers define their own type of AccumulatorV2, the resulting type can be different than that of the elements added.\nWarning\n: When a Spark task finishes, Spark will try to merge the accumulated updates in this task to an accumulator.\nIf it fails, Spark will ignore the failure and still mark the task successful and continue to run other tasks. Hence,\na buggy accumulator will not impact a Spark job, but it may not get updated correctly although a Spark job is successful.\nFor accumulator updates performed inside\nactions only\n, Spark guarantees that each task’s update to the accumulator\nwill only be applied once, i.e. restarted tasks will not update the value. In transfo", "question": "What happens if Spark fails to merge accumulated updates from a task to an accumulator?", "answers": {"text": ["If it fails, Spark will ignore the failure and still mark the task successful and continue to run other tasks."], "answer_start": [359]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nTuning Spark\nData Serialization\nMemory Tuning\nMemory Management Overview\nDetermining Memory Consumption\nTuning Data Structures\nSerialized RDD Storage\nGarbage Collection Tuning\nOther Considerations\nLevel of Parallelism\nParallel Listing on Input Paths\nM", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars"], "answer_start": [46]}}
{"context": "on\nTuning Data Structures\nSerialized RDD Storage\nGarbage Collection Tuning\nOther Considerations\nLevel of Parallelism\nParallel Listing on Input Paths\nMemory Usage of Reduce Tasks\nBroadcasting Large Variables\nData Locality\nSummary\nBecause of the in-memory nature of most Spark computations, Spark programs can be bottlenecked\nby any resource in the cluster: CPU, network bandwidth, or memory.\nMost often, if the data fits in memory, the bottleneck is network bandwidth, but sometimes, you\nalso need to do some tuning, such as\nstoring RDDs in serialized form\n, to\ndecrease memory usage.\nThis guide will cover two main topics: data serialization, which is crucial for good network\nperformance and can also reduce memory use, and memory tuning. We also sketch several smaller topics.\nData Serialization\nSe", "question": "What can be done to decrease memory usage?", "answers": {"text": ["storing RDDs in serialized form"], "answer_start": [524]}}
{"context": "u’ll use in the program in advance\nfor best performance.\nYou can switch to using Kryo by initializing your job with a\nSparkConf\nand calling\nconf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n.\nThis setting configures the serializer used for not only shuffling data between worker\nnodes but also when serializing RDDs to disk.  The only reason Kryo is not the default is because of the custom\nregistration requirement, but we recommend trying it in any network-intensive application.\nSince Spark 2.0.0, we internally use Kryo serializer when shuffling RDDs with simple types, arrays of simple types, or string type.\nSpark automatically includes Kryo serializers for the many commonly-used core Scala classes covered\nin the AllScalaRegistrar from the\nTwitter chill\nlibrary.\nTo r", "question": "How can you switch to using Kryo?", "answers": {"text": ["You can switch to using Kryo by initializing your job with a\nSparkConf\nand calling\nconf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")"], "answer_start": [57]}}
{"context": "ize each object on the fly.\nWe highly recommend\nusing Kryo\nif you want to cache data in serialized form, as\nit leads to much smaller sizes than Java serialization (and certainly than raw Java objects).\nGarbage Collection Tuning\nJVM garbage collection can be a problem when you have large “churn” in terms of the RDDs\nstored by your program. (It is usually not a problem in programs that just read an RDD once\nand then run many operations on it.) When Java needs to evict old objects to make room for new ones, it will\nneed to trace through all your Java objects and find the unused ones. The main point to remember here is\nthat\nthe cost of garbage collection is proportional to the number of Java objects\n, so using data\nstructures with fewer objects (e.g. an array of\nInt\ns instead of a\nLinkedList\n)", "question": "What is the cost of garbage collection proportional to?", "answers": {"text": ["the cost of garbage collection is proportional to the number of Java objects"], "answer_start": [628]}}
{"context": "ther tune garbage collection, we first need to understand some basic information about memory management in the JVM:\nJava Heap space is divided into two regions Young and Old. The Young generation is meant to hold short-lived objects\nwhile the Old generation is intended for objects with longer lifetimes.\nThe Young generation is further divided into three regions [Eden, Survivor1, Survivor2].\nA simplified description of the garbage collection procedure: When Eden is full, a minor GC is run on Eden and objects\nthat are alive from Eden and Survivor1 are copied to Survivor2. The Survivor regions are swapped. If an object is old\nenough or Survivor2 is full, it is moved to Old. Finally, when Old is close to full, a full GC is invoked.\nThe goal of GC tuning in Spark is to ensure that only long-li", "question": "Como o espaço Java Heap é dividido?", "answers": {"text": ["Java Heap space is divided into two regions Young and Old."], "answer_start": [117]}}
{"context": "utors can be specified by setting\nspark.executor.defaultJavaOptions\nor\nspark.executor.extraJavaOptions\nin\na job’s configuration.\nOther Considerations\nLevel of Parallelism\nClusters will not be fully utilized unless you set the level of parallelism for each operation high\nenough. Spark automatically sets the number of “map” tasks to run on each file according to its size\n(though you can control it through optional parameters to\nSparkContext.textFile\n, etc), and for\ndistributed “reduce” operations, such as\ngroupByKey\nand\nreduceByKey\n, it uses the largest\nparent RDD’s number of partitions. You can pass the level of parallelism as a second argument\n(see the\nspark.PairRDDFunctions\ndocumentation),\nor set the config property\nspark.default.parallelism\nto change the default.\nIn general, we recommend", "question": "How does Spark determine the number of map tasks to run on each file?", "answers": {"text": ["Spark automatically sets the number of “map” tasks to run on each file according to its size"], "answer_start": [279]}}
{"context": "ata.  Spark builds its scheduling around\nthis general principle of data locality.\nData locality is how close data is to the code processing it.  There are several levels of\nlocality based on the data’s current location.  In order from closest to farthest:\nPROCESS_LOCAL\ndata is in the same JVM as the running code.  This is the best locality\npossible.\nNODE_LOCAL\ndata is on the same node.  Examples might be in HDFS on the same node, or in\nanother executor on the same node.  This is a little slower than\nPROCESS_LOCAL\nbecause the data\nhas to travel between processes.\nNO_PREF\ndata is accessed equally quickly from anywhere and has no locality preference.\nRACK_LOCAL\ndata is on the same rack of servers.  Data is on a different server on the same rack\nso needs to be sent over the network, typically ", "question": "What is data locality in the context of Spark scheduling?", "answers": {"text": ["Data locality is how close data is to the code processing it."], "answer_start": [82]}}
{"context": "ograms,\nswitching to Kryo serialization and persisting data in serialized form will solve most common\nperformance issues. Feel free to ask on the\nSpark mailing list\nabout other tuning best practices.", "question": "What is suggested to solve most common performance issues?", "answers": {"text": ["switching to Kryo serialization and persisting data in serialized form will solve most common"], "answer_start": [8]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)"], "answer_start": [46]}}
{"context": "ures\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nMachine Learning Library (MLlib) Guide\nMLlib is Spark’s machine learning (ML) library.\nIts goal is to make practical machine learning scalable and easy.\nAt a high level, it provides tools such as:\nML Algorithms: common learning algorithms such as classification, regression, clustering, and collaborative filtering\nFeaturization: feature extraction, transformation, dimensionality reduction, an", "question": "What is the main goal of MLlib?", "answers": {"text": ["Its goal is to make practical machine learning scalable and easy."], "answer_start": [493]}}
{"context": "as classification, regression, clustering, and collaborative filtering\nFeaturization: feature extraction, transformation, dimensionality reduction, and selection\nPipelines: tools for constructing, evaluating, and tuning ML Pipelines\nPersistence: saving and load algorithms, models, and Pipelines\nUtilities: linear algebra, statistics, data handling, etc.\nAnnouncement: DataFrame-based API is primary API\nThe MLlib RDD-based API is now in maintenance mode.\nAs of Spark 2.0, the\nRDD\n-based APIs in the\nspark.mllib\npackage have entered maintenance mode.\nThe primary Machine Learning API for Spark is now the\nDataFrame\n-based API in the\nspark.ml\npackage.\nWhat are the implications?\nMLlib will still support the RDD-based API in\nspark.mllib\nwith bug fixes.\nMLlib will not add new features to the RDD-based", "question": "What is the current status of the MLlib RDD-based API?", "answers": {"text": ["The MLlib RDD-based API is now in maintenance mode."], "answer_start": [404]}}
{"context": "\nWhat are the implications?\nMLlib will still support the RDD-based API in\nspark.mllib\nwith bug fixes.\nMLlib will not add new features to the RDD-based API.\nIn the Spark 2.x releases, MLlib will add features to the DataFrames-based API to reach feature parity with the RDD-based API.\nWhy is MLlib switching to the DataFrame-based API?\nDataFrames provide a more user-friendly API than RDDs.  The many benefits of DataFrames include Spark Datasources, SQL/DataFrame queries, Tungsten and Catalyst optimizations, and uniform APIs across languages.\nThe DataFrame-based API for MLlib provides a uniform API across ML algorithms and across multiple languages.\nDataFrames facilitate practical ML Pipelines, particularly feature transformations.  See the\nPipelines guide\nfor details.\nWhat is “Spark ML”?\n“Spar", "question": "Why is MLlib switching to the DataFrame-based API?", "answers": {"text": ["DataFrames provide a more user-friendly API than RDDs."], "answer_start": [334]}}
{"context": "s.\nDataFrames facilitate practical ML Pipelines, particularly feature transformations.  See the\nPipelines guide\nfor details.\nWhat is “Spark ML”?\n“Spark ML” is not an official name but occasionally used to refer to the MLlib DataFrame-based API.\nThis is majorly due to the\norg.apache.spark.ml\nScala package name used by the DataFrame-based API, \nand the “Spark ML Pipelines” term we used initially to emphasize the pipeline concept.\nIs MLlib deprecated?\nNo. MLlib includes both the RDD-based API and the DataFrame-based API.\nThe RDD-based API is now in maintenance mode.\nBut neither API is deprecated, nor MLlib as a whole.\nDependencies\nMLlib uses linear algebra packages\nBreeze\nand\ndev.ludovic.netlib\nfor optimised numerical processing\n1\n. Those packages may call native acceleration libraries such a", "question": "What is “Spark ML”?", "answers": {"text": ["“Spark ML” is not an official name but occasionally used to refer to the MLlib DataFrame-based API."], "answer_start": [145]}}
{"context": "in Python, you will need\nNumPy\nversion 1.4 or newer.\nHighlights in 3.0\nThe list below highlights some of the new features and enhancements added to MLlib in the\n3.0\nrelease of Spark:\nMultiple columns support was added to\nBinarizer\n(\nSPARK-23578\n),\nStringIndexer\n(\nSPARK-11215\n),\nStopWordsRemover\n(\nSPARK-29808\n) and PySpark\nQuantileDiscretizer\n(\nSPARK-22796\n).\nTree-Based Feature Transformation was added\n(\nSPARK-13677\n).\nTwo new evaluators\nMultilabelClassificationEvaluator\n(\nSPARK-16692\n) and\nRankingEvaluator\n(\nSPARK-28045\n) were added.\nSample weights support was added in\nDecisionTreeClassifier/Regressor\n(\nSPARK-19591\n),\nRandomForestClassifier/Regressor\n(\nSPARK-9478\n),\nGBTClassifier/Regressor\n(\nSPARK-9612\n),\nMulticlassClassificationEvaluator\n(\nSPARK-24101\n),\nRegressionEvaluator\n(\nSPARK-24102\n", "question": "Which versions of Spark were enhancements added to Binarizer, StringIndexer, StopWordsRemover, and PySpark QuantileDiscretizer?", "answers": {"text": ["3.0"], "answer_start": [67]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nSpark SQL, DataFrames and Datasets Guide\nSpark SQL is a Spark module", "question": "What is Spark SQL?", "answers": {"text": ["Spark SQL is a Spark module"], "answer_start": [773]}}
{"context": "PI\n,\nDataFrame\nis simply a type alias of\nDataset[Row]\n.\nWhile, in\nJava API\n, users need to use\nDataset<Row>\nto represent a\nDataFrame\n.\nThroughout this document, we will often refer to Scala/Java Datasets of\nRow\ns as DataFrames.", "question": "What is a DataFrame simply a type alias of?", "answers": {"text": ["Dataset[Row]"], "answer_start": [41]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nStructured Streaming Programming Guide\nOverview\nGetting Started\nAPIs on DataFrames and Datasets\nPerformance Tips\nAdditional Information\nStructured Streaming Programming Guide\nOverview\nStructured Streaming is a scalable and fault-tolerant stream proces", "question": "What is Structured Streaming?", "answers": {"text": ["Structured Streaming is a scalable and fault-tolerant stream proces"], "answer_start": [733]}}
{"context": "rmance Tips\nAdditional Information\nStructured Streaming Programming Guide\nOverview\nStructured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. You can express your streaming computation the same way you would express a batch computation on static data. The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive. You can use the\nDataset/DataFrame API\nin Scala, Java, Python or R to express streaming aggregations, event-time windows, stream-to-batch joins, etc. The computation is executed on the same optimized Spark SQL engine. Finally, the system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs. In shor", "question": "What is Structured Streaming built on?", "answers": {"text": ["Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine."], "answer_start": [83]}}
{"context": "d Spark SQL engine. Finally, the system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs. In short,\nStructured Streaming provides fast, scalable, fault-tolerant, end-to-end exactly-once stream processing without the user having to reason about streaming.\nInternally, by default, Structured Streaming queries are processed using a\nmicro-batch processing\nengine, which processes data streams as a series of small batch jobs thereby achieving end-to-end latencies as low as 100 milliseconds and exactly-once fault-tolerance guarantees. However, since Spark 2.3, we have introduced a new low-latency processing mode called\nContinuous Processing\n, which can achieve end-to-end latencies as low as 1 millisecond with at-least-once guarantees. Without ch", "question": "What are the fault-tolerance guarantees provided by Structured Streaming?", "answers": {"text": ["end-to-end exactly-once fault-tolerance guarantees"], "answer_start": [48]}}
{"context": "rocessing mode called\nContinuous Processing\n, which can achieve end-to-end latencies as low as 1 millisecond with at-least-once guarantees. Without changing the Dataset/DataFrame operations in your queries, you will be able to choose the mode based on your application requirements.\nIn this guide, we are going to walk you through the programming model and the APIs. We are going to explain the concepts mostly using the default micro-batch processing model, and then\nlater\ndiscuss Continuous Processing model. First, let’s start with a simple example of a Structured Streaming query - a streaming word count.", "question": "What latency can be achieved with Continuous Processing mode?", "answers": {"text": ["as low as 1 millisecond with at-least-once guarantees"], "answer_start": [85]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nGraphX Programming Guide\nOverview\nGetting Started\nThe Property Graph\nExample Property Graph\nGraph Operators\nSummary List of Operators\nProperty Operators\nStructural Operators\nJoin Operators\nNeighborhood Aggregation\nAggregate Messages (aggregateMessages", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars"], "answer_start": [46]}}
{"context": "a\nbipartite graph we might do the following:\nclass\nVertexProperty\n()\ncase\nclass\nUserProperty\n(\nval\nname\n:\nString\n)\nextends\nVertexProperty\ncase\nclass\nProductProperty\n(\nval\nname\n:\nString\n,\nval\nprice\n:\nDouble\n)\nextends\nVertexProperty\n// The graph might then have the type:\nvar\ngraph\n:\nGraph\n[\nVertexProperty\n,\nString\n]\n=\nnull\nLike RDDs, property graphs are immutable, distributed, and fault-tolerant.  Changes to the values or\nstructure of the graph are accomplished by producing a new graph with the desired changes.  Note\nthat substantial parts of the original graph (i.e., unaffected structure, attributes, and indices)\nare reused in the new graph reducing the cost of this inherently functional data structure.  The\ngraph is partitioned across the executors using a range of vertex partitioning heur", "question": "What is a characteristic of property graphs, similar to RDDs?", "answers": {"text": ["Like RDDs, property graphs are immutable, distributed, and fault-tolerant."], "answer_start": [323]}}
{"context": "ap\noperator, the property graph contains the following:\nclass\nGraph\n[\nVD\n,\nED\n]\n{\ndef\nmapVertices\n[\nVD2\n](\nmap\n:\n(\nVertexId\n,\nVD\n)\n=>\nVD2\n)\n:\nGraph\n[\nVD2\n,\nED\n]\ndef\nmapEdges\n[\nED2\n](\nmap\n:\nEdge\n[\nED\n]\n=>\nED2\n)\n:\nGraph\n[\nVD\n,\nED2\n]\ndef\nmapTriplets\n[\nED2\n](\nmap\n:\nEdgeTriplet\n[\nVD\n,\nED\n]\n=>\nED2\n)\n:\nGraph\n[\nVD\n,\nED2\n]\n}\nEach of these operators yields a new graph with the vertex or edge properties modified by the user\ndefined\nmap\nfunction.\nNote that in each case the graph structure is unaffected. This is a key feature of these operators\nwhich allows the resulting graph to reuse the structural indices of the original graph. The\nfollowing snippets are logically equivalent, but the first one does not preserve the structural\nindices and would not benefit from the GraphX system optimizations:\nval\nne", "question": "What is a key feature of the mapVertices, mapEdges, and mapTriplets operators?", "answers": {"text": ["Note that in each case the graph structure is unaffected."], "answer_start": [439]}}
{"context": "re logically equivalent, but the first one does not preserve the structural\nindices and would not benefit from the GraphX system optimizations:\nval\nnewVertices\n=\ngraph\n.\nvertices\n.\nmap\n{\ncase\n(\nid\n,\nattr\n)\n=>\n(\nid\n,\nmapUdf\n(\nid\n,\nattr\n))\n}\nval\nnewGraph\n=\nGraph\n(\nnewVertices\n,\ngraph\n.\nedges\n)\nInstead, use\nmapVertices\nto preserve the indices:\nval\nnewGraph\n=\ngraph\n.\nmapVertices\n((\nid\n,\nattr\n)\n=>\nmapUdf\n(\nid\n,\nattr\n))\nThese operators are often used to initialize the graph for a particular computation or project away\nunnecessary properties.  For example, given a graph with the out degrees as the vertex properties\n(we describe how to construct such a graph later), we initialize it for PageRank:\n// Given a graph where the vertex property is the out degree\nval\ninputGraph\n:\nGraph\n[\nInt\n,\nString\n]\n=", "question": "What operator should be used to preserve the indices when modifying vertex attributes?", "answers": {"text": ["mapVertices"], "answer_start": [306]}}
{"context": "0 (for which we have no information) connected to users\n// 4 (peter) and 5 (franklin).\ngraph\n.\ntriplets\n.\nmap\n(\ntriplet\n=>\ntriplet\n.\nsrcAttr\n.\n_1\n+\n\" is the \"\n+\ntriplet\n.\nattr\n+\n\" of \"\n+\ntriplet\n.\ndstAttr\n.\n_1\n).\ncollect\n.\nforeach\n(\nprintln\n(\n_\n))\n// Remove missing vertices as well as the edges to connected to them\nval\nvalidGraph\n=\ngraph\n.\nsubgraph\n(\nvpred\n=\n(\nid\n,\nattr\n)\n=>\nattr\n.\n_2\n!=\n\"Missing\"\n)\n// The valid subgraph will disconnect users 4 and 5 by removing user 0\nvalidGraph\n.\nvertices\n.\ncollect\n.\nforeach\n(\nprintln\n(\n_\n))\nvalidGraph\n.\ntriplets\n.\nmap\n(\ntriplet\n=>\ntriplet\n.\nsrcAttr\n.\n_1\n+\n\" is the \"\n+\ntriplet\n.\nattr\n+\n\" of \"\n+\ntriplet\n.\ndstAttr\n.\n_1\n).\ncollect\n.\nforeach\n(\nprintln\n(\n_\n))\nNote in the above example only the vertex predicate is provided.  The\nsubgraph\noperator defaults\nto\nt", "question": "What happens to users 4 and 5 after applying the subgraph operation?", "answers": {"text": ["The valid subgraph will disconnect users 4 and 5 by removing user 0"], "answer_start": [406]}}
{"context": " information about the neighborhood of each\nvertex.\nFor example, we might want to know the number of followers each user has or the average age of\nthe followers of each user.  Many iterative graph algorithms (e.g., PageRank, Shortest Path, and\nconnected components) repeatedly aggregate properties of neighboring vertices (e.g., current\nPageRank Value, shortest path to the source, and smallest reachable vertex id).\nTo improve performance the primary aggregation operator changed from\ngraph.mapReduceTriplets\nto the new\ngraph.AggregateMessages\n.  While the changes in the API are\nrelatively small, we provide a transition guide below.\nAggregate Messages (aggregateMessages)\nThe core aggregation operation in GraphX is\naggregateMessages\n.\nThis operator applies a user defined\nsendMsg\nfunction to each", "question": "What is the core aggregation operation in GraphX?", "answers": {"text": ["aggregateMessages"], "answer_start": [656]}}
{"context": "ntext\n(i.e., the source vertex attribute but not the destination vertex attribute).\nThe possible options for the\ntripletsFields\nare defined in\nTripletFields\nand\nthe default value is\nTripletFields.All\nwhich indicates that the user\ndefined\nsendMsg\nfunction may access any of the fields in the\nEdgeContext\n.\nThe\ntripletFields\nargument can be used to notify GraphX that only part of the\nEdgeContext\nwill be needed allowing GraphX to select an optimized join strategy.\nFor example if we are computing the average age of the followers of each user we would only require\nthe source field and so we would use\nTripletFields.Src\nto indicate that we\nonly require the source field\nIn earlier versions of GraphX we used byte code inspection to infer the\nTripletFields\nhowever we have found that bytecode inspectio", "question": "What is the default value for the tripletsFields argument?", "answers": {"text": ["TripletFields.All"], "answer_start": [182]}}
{"context": "Followers\n.\nmapValues\n(\n(\nid\n,\nvalue\n)\n=>\nvalue\nmatch\n{\ncase\n(\ncount\n,\ntotalAge\n)\n=>\ntotalAge\n/\ncount\n}\n)\n// Display the results\navgAgeOfOlderFollowers\n.\ncollect\n().\nforeach\n(\nprintln\n(\n_\n))\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\" in the Spark repo.\nThe\naggregateMessages\noperation performs optimally when the messages (and the sums of\nmessages) are constant sized (e.g., floats and addition instead of lists and concatenation).\nMap Reduce Triplets Transition Guide (Legacy)\nIn earlier versions of GraphX neighborhood aggregation was accomplished using the\nmapReduceTriplets\noperator:\nclass\nGraph\n[\nVD\n,\nED\n]\n{\ndef\nmapReduceTriplets\n[\nMsg\n](\nmap\n:\nEdgeTriplet\n[\nVD\n,\nED\n]\n=>\nIterator\n[(\nVertexId\n,\nMsg\n)],\nreduce\n:\n(\nMsg\n,\nM", "question": "Where can I find a full example code for the discussed operations?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\" in the Spark repo."], "answer_start": [191]}}
{"context": "phX behave the same way.\nWhen using a graph multiple times, make sure to call\nGraph.cache()\non it first.\nIn iterative computations,\nuncaching\nmay also be necessary for best performance. By default, cached RDDs and graphs will remain in memory until memory pressure forces them to be evicted in LRU order. For iterative computation, intermediate results from previous iterations will fill up the cache. Though they will eventually be evicted, the unnecessary data stored in memory will slow down garbage collection. It would be more efficient to uncache intermediate results as soon as they are no longer necessary. This involves materializing (caching and forcing) a graph or RDD every iteration, uncaching all other datasets, and only using the materialized dataset in future iterations. However, be", "question": "What should be done with intermediate results in iterative computations for better performance?", "answers": {"text": ["It would be more efficient to uncache intermediate results as soon as they are no longer necessary."], "answer_start": [515]}}
{"context": "g and forcing) a graph or RDD every iteration, uncaching all other datasets, and only using the materialized dataset in future iterations. However, because graphs are composed of multiple RDDs, it can be difficult to unpersist them correctly.\nFor iterative computation we recommend using the Pregel API, which correctly unpersists intermediate results.\nPregel API\nGraphs are inherently recursive data structures as properties of vertices depend on properties of\ntheir neighbors which in turn depend on properties of\ntheir\nneighbors.  As a\nconsequence many important graph algorithms iteratively recompute the properties of each vertex\nuntil a fixed-point condition is reached.  A range of graph-parallel abstractions have been proposed\nto express these iterative algorithms.  GraphX exposes a variant", "question": "What is recommended for iterative computation when correctly unpersisting intermediate results is a concern?", "answers": {"text": ["For iterative computation we recommend using the Pregel API, which correctly unpersists intermediate results."], "answer_start": [243]}}
{"context": "ssages\nmergeMsg\n.\nWe can use the Pregel operator to express computation such as single source\nshortest path in the following example.\nimport\norg.apache.spark.graphx.\n{\nGraph\n,\nVertexId\n}\nimport\norg.apache.spark.graphx.util.GraphGenerators\n// A graph with edge attributes containing distances\nval\ngraph\n:\nGraph\n[\nLong\n,\nDouble\n]\n=\nGraphGenerators\n.\nlogNormalGraph\n(\nsc\n,\nnumVertices\n=\n100\n).\nmapEdges\n(\ne\n=>\ne\n.\nattr\n.\ntoDouble\n)\nval\nsourceId\n:\nVertexId\n=\n42\n// The ultimate source\n// Initialize the graph such that all vertices except the root have distance infinity.\nval\ninitialGraph\n=\ngraph\n.\nmapVertices\n((\nid\n,\n_\n)\n=>\nif\n(\nid\n==\nsourceId\n)\n0.0\nelse\nDouble\n.\nPositiveInfinity\n)\nval\nsssp\n=\ninitialGraph\n.\npregel\n(\nDouble\n.\nPositiveInfinity\n)(\n(\nid\n,\ndist\n,\nnewDist\n)\n=>\nmath\n.\nmin\n(\ndist\n,\nnewDist\n", "question": "How is the initial graph initialized in the single source shortest path example?", "answers": {"text": ["Initialize the graph such that all vertices except the root have distance infinity."], "answer_start": [484]}}
{"context": "se\nDouble\n.\nPositiveInfinity\n)\nval\nsssp\n=\ninitialGraph\n.\npregel\n(\nDouble\n.\nPositiveInfinity\n)(\n(\nid\n,\ndist\n,\nnewDist\n)\n=>\nmath\n.\nmin\n(\ndist\n,\nnewDist\n),\n// Vertex Program\ntriplet\n=>\n{\n// Send Message\nif\n(\ntriplet\n.\nsrcAttr\n+\ntriplet\n.\nattr\n<\ntriplet\n.\ndstAttr\n)\n{\nIterator\n((\ntriplet\n.\ndstId\n,\ntriplet\n.\nsrcAttr\n+\ntriplet\n.\nattr\n))\n}\nelse\n{\nIterator\n.\nempty\n}\n},\n(\na\n,\nb\n)\n=>\nmath\n.\nmin\n(\na\n,\nb\n)\n// Merge Message\n)\nprintln\n(\nsssp\n.\nvertices\n.\ncollect\n().\nmkString\n(\n\"\\n\"\n))\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\" in the Spark repo.\nGraph Builders\nGraphX provides several ways of building a graph from a collection of vertices and edges in an RDD or on disk. None of the graph builders repartitions the graph’s edges by default; instead", "question": "Onde posso encontrar um exemplo de código completo para SSSP?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\" in the Spark repo."], "answer_start": [475]}}
{"context": "ng a graph from a collection of vertices and edges in an RDD or on disk. None of the graph builders repartitions the graph’s edges by default; instead, edges are left in their default partitions (such as their original blocks in HDFS).\nGraph.groupEdges\nrequires the graph to be repartitioned because it assumes identical edges will be colocated on the same partition, so you must call\nGraph.partitionBy\nbefore calling\ngroupEdges\n.\nobject\nGraphLoader\n{\ndef\nedgeListFile\n(\nsc\n:\nSparkContext\n,\npath\n:\nString\n,\ncanonicalOrientation\n:\nBoolean\n=\nfalse\n,\nminEdgePartitions\n:\nInt\n=\n1\n)\n:\nGraph\n[\nInt\n,\nInt\n]\n}\nGraphLoader.edgeListFile\nprovides a way to load a graph from a list of edges on disk. It parses an adjacency list of (source vertex ID, destination vertex ID) pairs of the following form, skipping c", "question": "What does GraphLoader.edgeListFile do?", "answers": {"text": ["provides a way to load a graph from a list of edges on disk."], "answer_start": [627]}}
{"context": "igned the default attribute.\nGraph.fromEdges\nallows creating a graph from only an RDD of edges, automatically creating any vertices mentioned by edges and assigning them the default value.\nGraph.fromEdgeTuples\nallows creating a graph from only an RDD of edge tuples, assigning the edges the value 1, and automatically creating any vertices mentioned by edges and assigning them the default value. It also supports deduplicating the edges; to deduplicate, pass\nSome\nof a\nPartitionStrategy\nas the\nuniqueEdges\nparameter (for example,\nuniqueEdges = Some(PartitionStrategy.RandomVertexCut)\n). A partition strategy is necessary to colocate identical edges on the same partition so they can be deduplicated.\nVertex and Edge RDDs\nGraphX exposes\nRDD\nviews of the vertices and edges stored within the graph.  H", "question": "What does Graph.fromEdgeTuples do when creating a graph?", "answers": {"text": ["allows creating a graph from only an RDD of edge tuples, assigning the edges the value 1, and automatically creating any vertices mentioned by edges and assigning them the default value."], "answer_start": [210]}}
{"context": "ernal index\ndef\nfilter\n(\npred\n:\nTuple2\n[\nVertexId\n,\nVD\n]\n=>\nBoolean\n)\n:\nVertexRDD\n[\nVD\n]\n// Transform the values without changing the ids (preserves the internal index)\ndef\nmapValues\n[\nVD2\n](\nmap\n:\nVD\n=>\nVD2\n)\n:\nVertexRDD\n[\nVD2\n]\ndef\nmapValues\n[\nVD2\n](\nmap\n:\n(\nVertexId\n,\nVD\n)\n=>\nVD2\n)\n:\nVertexRDD\n[\nVD2\n]\n// Show only vertices unique to this set based on their VertexId's\ndef\nminus\n(\nother\n:\nRDD\n[(\nVertexId\n,\nVD\n)])\n// Remove vertices from this set that appear in the other set\ndef\ndiff\n(\nother\n:\nVertexRDD\n[\nVD\n])\n:\nVertexRDD\n[\nVD\n]\n// Join operators that take advantage of the internal indexing to accelerate joins (substantially)\ndef\nleftJoin\n[\nVD2\n,\nVD3\n](\nother\n:\nRDD\n[(\nVertexId\n,\nVD2\n)])(\nf\n:\n(\nVertexId\n,\nVD\n,\nOption\n[\nVD2\n])\n=>\nVD3\n)\n:\nVertexRDD\n[\nVD3\n]\ndef\ninnerJoin\n[\nU\n,\nVD2\n](\nother\n:\n", "question": "What does the `filter` function do?", "answers": {"text": ["// Transform the values without changing the ids (preserves the internal index)"], "answer_start": [89]}}
{"context": "VD2\n,\nVD3\n](\nother\n:\nRDD\n[(\nVertexId\n,\nVD2\n)])(\nf\n:\n(\nVertexId\n,\nVD\n,\nOption\n[\nVD2\n])\n=>\nVD3\n)\n:\nVertexRDD\n[\nVD3\n]\ndef\ninnerJoin\n[\nU\n,\nVD2\n](\nother\n:\nRDD\n[(\nVertexId\n,\nU\n)])(\nf\n:\n(\nVertexId\n,\nVD\n,\nU\n)\n=>\nVD2\n)\n:\nVertexRDD\n[\nVD2\n]\n// Use the index on this RDD to accelerate a `reduceByKey` operation on the input RDD.\ndef\naggregateUsingIndex\n[\nVD2\n](\nother\n:\nRDD\n[(\nVertexId\n,\nVD2\n)],\nreduceFunc\n:\n(\nVD2\n,\nVD2\n)\n=>\nVD2\n)\n:\nVertexRDD\n[\nVD2\n]\n}\nNotice, for example,  how the\nfilter\noperator returns a\nVertexRDD\nVertexRDD\n.  Filter is actually\nimplemented using a\nBitSet\nthereby reusing the index and preserving the ability to do fast joins\nwith other\nVertexRDD\ns.  Likewise, the\nmapValues\noperators do not allow the\nmap\nfunction to\nchange the\nVertexId\nthereby enabling the same\nHashMap\ndata structures t", "question": "What does the filter operator return?", "answers": {"text": ["VertexRDD"], "answer_start": [97]}}
{"context": "extends\nRDD[Edge[ED]]\norganizes the edges in blocks partitioned using one\nof the various partitioning strategies defined in\nPartitionStrategy\n.  Within\neach partition, edge attributes and adjacency structure, are stored separately enabling maximum\nreuse when changing attribute values.\nThe three additional functions exposed by the\nEdgeRDD\nEdgeRDD\nare:\n// Transform the edge attributes while preserving the structure\ndef\nmapValues\n[\nED2\n](\nf\n:\nEdge\n[\nED\n]\n=>\nED2\n)\n:\nEdgeRDD\n[\nED2\n]\n// Reverse the edges reusing both attributes and structure\ndef\nreverse\n:\nEdgeRDD\n[\nED\n]\n// Join two `EdgeRDD`s partitioned using the same partitioning strategy.\ndef\ninnerJoin\n[\nED2\n,\nED3\n](\nother\n:\nEdgeRDD\n[\nED2\n])(\nf\n:\n(\nVertexId\n,\nVertexId\n,\nED\n,\nED2\n)\n=>\nED3\n)\n:\nEdgeRDD\n[\nED3\n]\nIn most applications we have found ", "question": "What does the `mapValues` function do?", "answers": {"text": ["Transform the edge attributes while preserving the structure"], "answer_start": [356]}}
{"context": "itting graphs along edges, GraphX partitions the graph along vertices which can\nreduce both the communication and storage overhead.  Logically, this corresponds to assigning edges\nto machines and allowing vertices to span multiple machines.  The exact method of assigning edges\ndepends on the\nPartitionStrategy\nand there are several tradeoffs to the\nvarious heuristics.  Users can choose between different strategies by repartitioning the graph with\nthe\nGraph.partitionBy\noperator.  The default partitioning strategy is to use\nthe initial partitioning of the edges as provided on graph construction.  However, users can easily\nswitch to 2D-partitioning or other heuristics included in GraphX.\nOnce the edges have been partitioned the key challenge to efficient graph-parallel computation is\nefficient", "question": "What determines the method of assigning edges in GraphX?", "answers": {"text": ["PartitionStrategy"], "answer_start": [293]}}
{"context": "ng or other heuristics included in GraphX.\nOnce the edges have been partitioned the key challenge to efficient graph-parallel computation is\nefficiently joining vertex attributes with the edges.  Because real-world graphs typically have more\nedges than vertices, we move vertex attributes to the edges.  Because not all partitions will\ncontain edges adjacent to all vertices we internally maintain a routing table which identifies where\nto broadcast vertices when implementing the join required for operations like\ntriplets\nand\naggregateMessages\n.\nGraph Algorithms\nGraphX includes a set of graph algorithms to simplify analytics tasks. The algorithms are contained in the\norg.apache.spark.graphx.lib\npackage and can be accessed directly as methods on\nGraph\nvia\nGraphOps\n. This section describes the a", "question": "What is the key challenge to efficient graph-parallel computation after edges have been partitioned?", "answers": {"text": ["efficiently joining vertex attributes with the edges."], "answer_start": [141]}}
{"context": "ing by more than a specified tolerance).\nGraphOps\nallows calling these algorithms directly as methods on\nGraph\n.\nGraphX also includes an example social network dataset that we can run PageRank on. A set of users is given in\ndata/graphx/users.txt\n, and a set of relationships between users is given in\ndata/graphx/followers.txt\n. We compute the PageRank of each user as follows:\nimport\norg.apache.spark.graphx.GraphLoader\n// Load the edges as a graph\nval\ngraph\n=\nGraphLoader\n.\nedgeListFile\n(\nsc\n,\n\"data/graphx/followers.txt\"\n)\n// Run PageRank\nval\nranks\n=\ngraph\n.\npageRank\n(\n0.0001\n).\nvertices\n// Join the ranks with the usernames\nval\nusers\n=\nsc\n.\ntextFile\n(\n\"data/graphx/users.txt\"\n).\nmap\n{\nline\n=>\nval\nfields\n=\nline\n.\nsplit\n(\n\",\"\n)\n(\nfields\n(\n0\n).\ntoLong\n,\nfields\n(\n1\n))\n}\nval\nranksByUsername\n=\nusers", "question": "Where are the edges of the social network dataset loaded from?", "answers": {"text": ["data/graphx/followers.txt"], "answer_start": [301]}}
{"context": "File\n(\n\"data/graphx/users.txt\"\n).\nmap\n{\nline\n=>\nval\nfields\n=\nline\n.\nsplit\n(\n\",\"\n)\n(\nfields\n(\n0\n).\ntoLong\n,\nfields\n(\n1\n))\n}\nval\nranksByUsername\n=\nusers\n.\njoin\n(\nranks\n).\nmap\n{\ncase\n(\nid\n,\n(\nusername\n,\nrank\n))\n=>\n(\nusername\n,\nrank\n)\n}\n// Print the result\nprintln\n(\nranksByUsername\n.\ncollect\n().\nmkString\n(\n\"\\n\"\n))\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\" in the Spark repo.\nConnected Components\nThe connected components algorithm labels each connected component of the graph with the ID of its lowest-numbered vertex. For example, in a social network, connected components can approximate clusters. GraphX contains an implementation of the algorithm in the\nConnectedComponents\nobject\n, and we compute the connected components of the exa", "question": "Where can I find the full example code for PageRank?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\" in the Spark repo."], "answer_start": [312]}}
{"context": "(\n1\n))\n}\nval\nccByUsername\n=\nusers\n.\njoin\n(\ncc\n).\nmap\n{\ncase\n(\nid\n,\n(\nusername\n,\ncc\n))\n=>\n(\nusername\n,\ncc\n)\n}\n// Print the result\nprintln\n(\nccByUsername\n.\ncollect\n().\nmkString\n(\n\"\\n\"\n))\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\" in the Spark repo.\nTriangle Counting\nA vertex is part of a triangle when it has two adjacent vertices with an edge between them. GraphX implements a triangle counting algorithm in the\nTriangleCount\nobject\nthat determines the number of triangles passing through each vertex, providing a measure of clustering. We compute the triangle count of the social network dataset from the\nPageRank section\n.\nNote that\nTriangleCount\nrequires the edges to be in canonical orientation (\nsrcId < dstId\n) and the ", "question": "O que o objeto TriangleCount faz?", "answers": {"text": ["that determines the number of triangles passing through each vertex, providing a measure of clustering."], "answer_start": [508]}}
{"context": "e\n(\n\"data/graphx/users.txt\"\n).\nmap\n{\nline\n=>\nval\nfields\n=\nline\n.\nsplit\n(\n\",\"\n)\n(\nfields\n(\n0\n).\ntoLong\n,\nfields\n(\n1\n))\n}\nval\ntriCountByUsername\n=\nusers\n.\njoin\n(\ntriCounts\n).\nmap\n{\ncase\n(\nid\n,\n(\nusername\n,\ntc\n))\n=>\n(\nusername\n,\ntc\n)\n}\n// Print the result\nprintln\n(\ntriCountByUsername\n.\ncollect\n().\nmkString\n(\n\"\\n\"\n))\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\" in the Spark repo.\nExamples\nSuppose I want to build a graph from some text files, restrict the graph\nto important relationships and users, run page-rank on the subgraph, and\nthen finally return attributes associated with the top users.  I can do\nall of this in just a few lines with GraphX:\nimport\norg.apache.spark.graphx.GraphLoader\n// Load my user data and parse into ", "question": "Where can I find the full example code for TriangleCountingExample?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\" in the Spark repo."], "answer_start": [315]}}
{"context": "\n.\n_1\n)).\nmkString\n(\n\"\\n\"\n))\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\" in the Spark repo.", "question": "Where can I find a full example code for the ComprehensiveExample?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\" in the Spark repo."], "answer_start": [29]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSparkR (R on Spark)\nOverview\nSparkDataFrame\nStarting Up: SparkSession\nStarting Up from RStudio\nCreating SparkDataFrames\nFrom local data frames\nFrom Data Sources\nFrom Hive tables\nSparkDataFrame Operations\nSelecting rows, columns\nGrouping, Aggregation\nO", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars"], "answer_start": [46]}}
{"context": "ng SparkDataFrames\nFrom local data frames\nFrom Data Sources\nFrom Hive tables\nSparkDataFrame Operations\nSelecting rows, columns\nGrouping, Aggregation\nOperating on Columns\nApplying User-Defined Function\nRun a given function on a large dataset using\ndapply\nor\ndapplyCollect\ndapply\ndapplyCollect\nRun a given function on a large dataset grouping by input column(s) and using\ngapply\nor\ngapplyCollect\ngapply\ngapplyCollect\nRun local R functions distributed using\nspark.lapply\nspark.lapply\nEager execution\nRunning SQL Queries from SparkR\nMachine Learning\nAlgorithms\nClassification\nRegression\nTree\nClustering\nCollaborative Filtering\nFrequent Pattern Mining\nStatistics\nModel persistence\nData type mapping between R and Spark\nStructured Streaming\nApache Arrow in SparkR\nEnsure Arrow Installed\nEnabling for Conver", "question": "What are some of the operations that can be performed on SparkDataFrames?", "answers": {"text": ["Selecting rows, columns\nGrouping, Aggregation\nOperating on Columns\nApplying User-Defined Function"], "answer_start": [103]}}
{"context": "tistics\nModel persistence\nData type mapping between R and Spark\nStructured Streaming\nApache Arrow in SparkR\nEnsure Arrow Installed\nEnabling for Conversion to/from R DataFrame,\ndapply\nand\ngapply\nSupported SQL Types\nR Function Name Conflicts\nMigration Guide\nSparkR is deprecated from Apache Spark 4.0.0 and will be removed in a future version.\nOverview\nSparkR is an R package that provides a light-weight frontend to use Apache Spark from R.\nIn Spark 4.0.0, SparkR provides a distributed data frame implementation that\nsupports operations like selection, filtering, aggregation etc. (similar to R data frames,\ndplyr\n) but on large datasets. SparkR also supports distributed\nmachine learning using MLlib.\nSparkDataFrame\nA SparkDataFrame is a distributed collection of data organized into named columns. ", "question": "What is a SparkDataFrame?", "answers": {"text": ["A SparkDataFrame is a distributed collection of data organized into named columns."], "answer_start": [717]}}
{"context": " supports distributed\nmachine learning using MLlib.\nSparkDataFrame\nA SparkDataFrame is a distributed collection of data organized into named columns. It is conceptually\nequivalent to a table in a relational database or a data frame in R, but with richer\noptimizations under the hood. SparkDataFrames can be constructed from a wide array of sources such as:\nstructured data files, tables in Hive, external databases, or existing local R data frames.\nAll of the examples on this page use sample data included in R or the Spark distribution and can be run using the\n./bin/sparkR\nshell.\nStarting Up: SparkSession\nThe entry point into SparkR is the\nSparkSession\nwhich connects your R program to a Spark cluster.\nYou can create a\nSparkSession\nusing\nsparkR.session\nand pass in options such as the applicatio", "question": "What is a SparkDataFrame conceptually equivalent to?", "answers": {"text": ["a table in a relational database or a data frame in R"], "answer_start": [183]}}
{"context": "ession\nwhich connects your R program to a Spark cluster.\nYou can create a\nSparkSession\nusing\nsparkR.session\nand pass in options such as the application name, any spark packages depended on, etc. Further, you can also work with SparkDataFrames via\nSparkSession\n. If you are working from the\nsparkR\nshell, the\nSparkSession\nshould already be created for you, and you would not need to call\nsparkR.session\n.\nsparkR.session\n()\nStarting Up from RStudio\nYou can also start SparkR from RStudio. You can connect your R program to a Spark cluster from\nRStudio, R shell, Rscript or other R IDEs. To start, make sure SPARK_HOME is set in environment\n(you can check\nSys.getenv\n),\nload the SparkR package, and call\nsparkR.session\nas below. It will check for the Spark installation, and, if not found, it will be do", "question": "How can you connect your R program to a Spark cluster?", "answers": {"text": ["which connects your R program to a Spark cluster."], "answer_start": [7]}}
{"context": "\n(\nnchar\n(\nSys.getenv\n(\n\"SPARK_HOME\"\n))\n<\n1\n)\n{\nSys.setenv\n(\nSPARK_HOME\n=\n\"/home/spark\"\n)\n}\nlibrary\n(\nSparkR\n,\nlib.loc\n=\nc\n(\nfile.path\n(\nSys.getenv\n(\n\"SPARK_HOME\"\n),\n\"R\"\n,\n\"lib\"\n)))\nsparkR.session\n(\nmaster\n=\n\"local[*]\"\n,\nsparkConfig\n=\nlist\n(\nspark.driver.memory\n=\n\"2g\"\n))\nThe following Spark driver properties can be set in\nsparkConfig\nwith\nsparkR.session\nfrom RStudio:\nProperty Name\nProperty group\nspark-submit\nequivalent\nspark.master\nApplication Properties\n--master\nspark.kerberos.keytab\nApplication Properties\n--keytab\nspark.kerberos.principal\nApplication Properties\n--principal\nspark.driver.memory\nApplication Properties\n--driver-memory\nspark.driver.extraClassPath\nRuntime Environment\n--driver-class-path\nspark.driver.extraJavaOptions\nRuntime Environment\n--driver-java-options\nspark.driver.extraL", "question": "Which Spark driver property corresponds to the --driver-memory argument in spark-submit?", "answers": {"text": ["spark.driver.memory"], "answer_start": [242]}}
{"context": "ver.extraClassPath\nRuntime Environment\n--driver-class-path\nspark.driver.extraJavaOptions\nRuntime Environment\n--driver-java-options\nspark.driver.extraLibraryPath\nRuntime Environment\n--driver-library-path\nCreating SparkDataFrames\nWith a\nSparkSession\n, applications can create\nSparkDataFrame\ns from a local R data frame, from a\nHive table\n, or from other\ndata sources\n.\nFrom local data frames\nThe simplest way to create a data frame is to convert a local R data frame into a SparkDataFrame. Specifically, we can use\nas.DataFrame\nor\ncreateDataFrame\nand pass in the local R data frame to create a SparkDataFrame. As an example, the following creates a\nSparkDataFrame\nbased using the\nfaithful\ndataset from R.\ndf\n<-\nas.DataFrame\n(\nfaithful\n)\n# Displays the first part of the SparkDataFrame\nhead\n(\ndf\n)\n##  e", "question": "How can a SparkDataFrame be created from a local R data frame?", "answers": {"text": ["Specifically, we can use\nas.DataFrame\nor\ncreateDataFrame\nand pass in the local R data frame to create a SparkDataFrame."], "answer_start": [488]}}
{"context": "rkDataFrame\nbased using the\nfaithful\ndataset from R.\ndf\n<-\nas.DataFrame\n(\nfaithful\n)\n# Displays the first part of the SparkDataFrame\nhead\n(\ndf\n)\n##  eruptions waiting\n##1     3.600      79\n##2     1.800      54\n##3     3.333      74\nFrom Data Sources\nSparkR supports operating on a variety of data sources through the\nSparkDataFrame\ninterface. This section describes the general methods for loading and saving data using Data Sources. You can check the Spark SQL programming guide for more\nspecific options\nthat are available for the built-in data sources.\nThe general method for creating SparkDataFrames from data sources is\nread.df\n. This method takes in the path for the file to load and the type of data source, and the currently active SparkSession will be used automatically.\nSparkR supports re", "question": "What method is generally used for creating SparkDataFrames from data sources?", "answers": {"text": ["read.df"], "answer_start": [626]}}
{"context": "urces using an example JSON input file. Note that the file that is used here is\nnot\na typical JSON file. Each line in the file must contain a separate, self-contained valid JSON object. For more information, please see\nJSON Lines text format, also called newline-delimited JSON\n. As a consequence, a regular multi-line JSON file will most often fail.\npeople\n<-\nread.df\n(\n\"./examples/src/main/resources/people.json\"\n,\n\"json\"\n)\nhead\n(\npeople\n)\n##  age    name\n##1  NA Michael\n##2  30    Andy\n##3  19  Justin\n# SparkR automatically infers the schema from the JSON file\nprintSchema\n(\npeople\n)\n# root\n#  |-- age: long (nullable = true)\n#  |-- name: string (nullable = true)\n# Similarly, multiple files can be read with read.json\npeople\n<-\nread.json\n(\nc\n(\n\"./examples/src/main/resources/people.json\"\n,\n\"./e", "question": "What does SparkR automatically infer from the JSON file?", "answers": {"text": ["SparkR automatically infers the schema from the JSON file"], "answer_start": [508]}}
{"context": " (nullable = true)\n# Similarly, multiple files can be read with read.json\npeople\n<-\nread.json\n(\nc\n(\n\"./examples/src/main/resources/people.json\"\n,\n\"./examples/src/main/resources/people2.json\"\n))\nThe data sources API natively supports CSV formatted input files. For more information please refer to SparkR\nread.df\nAPI documentation.\ndf\n<-\nread.df\n(\ncsvPath\n,\n\"csv\"\n,\nheader\n=\n\"true\"\n,\ninferSchema\n=\n\"true\"\n,\nna.strings\n=\n\"NA\"\n)\nThe data sources API can also be used to save out SparkDataFrames into multiple file formats. For example, we can save the SparkDataFrame from the previous example\nto a Parquet file using\nwrite.df\n.\nwrite.df\n(\npeople\n,\npath\n=\n\"people.parquet\"\n,\nsource\n=\n\"parquet\"\n,\nmode\n=\n\"overwrite\"\n)\nFrom Hive tables\nYou can also create SparkDataFrames from Hive tables. To do this we wi", "question": "How can multiple files be read using a function similar to read.json?", "answers": {"text": ["Similarly, multiple files can be read with read.json"], "answer_start": [21]}}
{"context": "=\n\"people.parquet\"\n,\nsource\n=\n\"parquet\"\n,\nmode\n=\n\"overwrite\"\n)\nFrom Hive tables\nYou can also create SparkDataFrames from Hive tables. To do this we will need to create a SparkSession with Hive support which can access tables in the Hive MetaStore. Note that Spark should have been built with\nHive support\nand more details can be found in the\nSQL programming guide\n. In SparkR, by default it will attempt to create a SparkSession with Hive support enabled (\nenableHiveSupport = TRUE\n).\nsparkR.session\n()\nsql\n(\n\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\"\n)\nsql\n(\n\"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src\"\n)\n# Queries can be expressed in HiveQL.\nresults\n<-\nsql\n(\n\"FROM src SELECT key, value\"\n)\n# results is now a SparkDataFrame\nhead\n(\nresults\n)\n##  key  ", "question": "How can SparkDataFrames be created from Hive tables?", "answers": {"text": ["To do this we will need to create a SparkSession with Hive support which can access tables in the Hive MetaStore."], "answer_start": [134]}}
{"context": "\"\n)\n# Queries can be expressed in HiveQL.\nresults\n<-\nsql\n(\n\"FROM src SELECT key, value\"\n)\n# results is now a SparkDataFrame\nhead\n(\nresults\n)\n##  key   value\n## 1 238 val_238\n## 2  86  val_86\n## 3 311 val_311\nSparkDataFrame Operations\nSparkDataFrames support a number of functions to do structured data processing.\nHere we include some basic examples and a complete list can be found in the\nAPI\ndocs:\nSelecting rows, columns\n# Create the SparkDataFrame\ndf\n<-\nas.DataFrame\n(\nfaithful\n)\n# Get basic information about the SparkDataFrame\ndf\n## SparkDataFrame[eruptions:double, waiting:double]\n# Select only the \"eruptions\" column\nhead\n(\nselect\n(\ndf\n,\ndf\n$\neruptions\n))\n##  eruptions\n##1     3.600\n##2     1.800\n##3     3.333\n# You can also pass in column name as strings\nhead\n(\nselect\n(\ndf\n,\n\"eruptions\"\n)", "question": "What does the text say SparkDataFrames support?", "answers": {"text": ["SparkDataFrames support a number of functions to do structured data processing."], "answer_start": [234]}}
{"context": "\neruptions\n))\n##  eruptions\n##1     3.600\n##2     1.800\n##3     3.333\n# You can also pass in column name as strings\nhead\n(\nselect\n(\ndf\n,\n\"eruptions\"\n))\n# Filter the SparkDataFrame to only retain rows with wait times shorter than 50 mins\nhead\n(\nfilter\n(\ndf\n,\ndf\n$\nwaiting\n<\n50\n))\n##  eruptions waiting\n##1     1.750      47\n##2     1.750      47\n##3     1.867      48\nGrouping, Aggregation\nSparkR data frames support a number of commonly used functions to aggregate data after grouping. For example, we can compute a histogram of the\nwaiting\ntime in the\nfaithful\ndataset as shown below\n# We use the `n` operator to count the number of times each waiting time appears\nhead\n(\nsummarize\n(\ngroupBy\n(\ndf\n,\ndf\n$\nwaiting\n),\ncount\n=\nn\n(\ndf\n$\nwaiting\n)))\n##  waiting count\n##1      70     4\n##2      67     1\n#", "question": "What operator is used to count the number of times each waiting time appears?", "answers": {"text": ["We use the `n` operator to count the number of times each waiting time appears"], "answer_start": [587]}}
{"context": " gear avg(mpg)\n##1  NA 140.8    4     22.8\n##2   4  75.7    4     30.4\n##3   8 400.0    3     19.2\n##4   8 318.0    3     15.5\n##5  NA 351.0   NA     15.8\n##6  NA 275.8   NA     16.3\nand\nrollup\n:\nhead\n(\nagg\n(\nrollup\n(\ndf\n,\n\"cyl\"\n,\n\"disp\"\n,\n\"gear\"\n),\navg\n(\ndf\n$\nmpg\n)))\n##  cyl  disp gear avg(mpg)\n##1   4  75.7    4     30.4\n##2   8 400.0    3     19.2\n##3   8 318.0    3     15.5\n##4   4  78.7   NA     32.4\n##5   8 304.0    3     15.2\n##6   4  79.0   NA     27.3\nOperating on Columns\nSparkR also provides a number of functions that can be directly applied to columns for data processing and during aggregation. The example below shows the use of basic arithmetic functions.\n# Convert waiting time from hours to seconds.\n# Note that we can assign this to a new column in the same SparkDataFrame\ndf\n$", "question": "What does SparkR provide for data processing and during aggregation?", "answers": {"text": ["SparkR also provides a number of functions that can be directly applied to columns for data processing and during aggregation."], "answer_start": [486]}}
{"context": "sic arithmetic functions.\n# Convert waiting time from hours to seconds.\n# Note that we can assign this to a new column in the same SparkDataFrame\ndf\n$\nwaiting_secs\n<-\ndf\n$\nwaiting\n*\n60\nhead\n(\ndf\n)\n##  eruptions waiting waiting_secs\n##1     3.600      79         4740\n##2     1.800      54         3240\n##3     3.333      74         4440\nApplying User-Defined Function\nIn SparkR, we support several kinds of User-Defined Functions:\nRun a given function on a large dataset using\ndapply\nor\ndapplyCollect\ndapply\nApply a function to each partition of a\nSparkDataFrame\n. The function to be applied to each partition of the\nSparkDataFrame\nand should have only one parameter, to which a\ndata.frame\ncorresponds to each partition will be passed. The output of function should be a\ndata.frame\n. Schema specifies", "question": "In SparkR, what functions are supported for User-Defined Functions?", "answers": {"text": ["Run a given function on a large dataset using\ndapply\nor\ndapplyCollect"], "answer_start": [431]}}
{"context": "100\n##2      69       5.067\n##3      71       5.033\n##4      87       5.000\n##5      63       4.933\n##6      89       4.900\ngapplyCollect\nLike\ngapply\n, applies a function to each partition of a\nSparkDataFrame\nand collect the result back to R data.frame. The output of the function should be a\ndata.frame\n. But, the schema is not required to be passed. Note that\ngapplyCollect\ncan fail if the output of UDF run on all the partition cannot be pulled to the driver and fit in driver memory.\n# Determine six waiting times with the largest eruption time in minutes.\nresult\n<-\ngapplyCollect\n(\ndf\n,\n\"waiting\"\n,\nfunction\n(\nkey\n,\nx\n)\n{\ny\n<-\ndata.frame\n(\nkey\n,\nmax\n(\nx\n$\neruptions\n))\ncolnames\n(\ny\n)\n<-\nc\n(\n\"waiting\"\n,\n\"max_eruption\"\n)\ny\n})\nhead\n(\nresult\n[\norder\n(\nresult\n$\nmax_eruption\n,\ndecreasing\n=\nTRUE\n),\n]", "question": "What does the function `gapplyCollect` do?", "answers": {"text": ["applies a function to each partition of a\nSparkDataFrame\nand collect the result back to R data.frame."], "answer_start": [152]}}
{"context": "should fit in a single machine. If that is not the case they can do something like\ndf <- createDataFrame(list)\nand then use\ndapply\n# Perform distributed training of multiple models with spark.lapply. Here, we pass\n# a read-only list of arguments which specifies family the generalized linear model should be.\nfamilies\n<-\nc\n(\n\"gaussian\"\n,\n\"poisson\"\n)\ntrain\n<-\nfunction\n(\nfamily\n)\n{\nmodel\n<-\nglm\n(\nSepal.Length\n~\nSepal.Width\n+\nSpecies\n,\niris\n,\nfamily\n=\nfamily\n)\nsummary\n(\nmodel\n)\n}\n# Return a list of model's summaries\nmodel.summaries\n<-\nspark.lapply\n(\nfamilies\n,\ntrain\n)\n# Print the summary of each model\nprint\n(\nmodel.summaries\n)\nEager execution\nIf eager execution is enabled, the data will be returned to R client immediately when the\nSparkDataFrame\nis created. By default, eager execution is not en", "question": "What is used to perform distributed training of multiple models?", "answers": {"text": ["spark.lapply"], "answer_start": [186]}}
{"context": " rows and up to 20 characters per column will be showed.\n# Start up spark session with eager execution enabled\nsparkR.session\n(\nmaster\n=\n\"local[*]\"\n,\nsparkConfig\n=\nlist\n(\nspark.sql.repl.eagerEval.enabled\n=\n\"true\"\n,\nspark.sql.repl.eagerEval.maxNumRows\n=\nas.integer\n(\n10\n)))\n# Create a grouped and sorted SparkDataFrame\ndf\n<-\ncreateDataFrame\n(\nfaithful\n)\ndf2\n<-\narrange\n(\nsummarize\n(\ngroupBy\n(\ndf\n,\ndf\n$\nwaiting\n),\ncount\n=\nn\n(\ndf\n$\nwaiting\n)),\n\"waiting\"\n)\n# Similar to R data.frame, displays the data returned, instead of SparkDataFrame class string\ndf2\n##+-------+-----+\n##|waiting|count|\n##+-------+-----+\n##|   43.0|    1|\n##|   45.0|    3|\n##|   46.0|    5|\n##|   47.0|    4|\n##|   48.0|    3|\n##|   49.0|    5|\n##|   50.0|    5|\n##|   51.0|    6|\n##|   52.0|    5|\n##|   53.0|    7|\n##+-------+---", "question": "What is the maximum number of rows displayed when eager evaluation is enabled?", "answers": {"text": ["10"], "answer_start": [266]}}
{"context": ".0|    5|\n##|   47.0|    4|\n##|   48.0|    3|\n##|   49.0|    5|\n##|   50.0|    5|\n##|   51.0|    6|\n##|   52.0|    5|\n##|   53.0|    7|\n##+-------+-----+\n##only showing top 10 rows\nNote that to enable eager execution in\nsparkR\nshell, add\nspark.sql.repl.eagerEval.enabled=true\nconfiguration property to the\n--conf\noption.\nRunning SQL Queries from SparkR\nA SparkDataFrame can also be registered as a temporary view in Spark SQL and that allows you to run SQL queries over its data.\nThe\nsql\nfunction enables applications to run SQL queries programmatically and returns the result as a\nSparkDataFrame\n.\n# Load a JSON file\npeople\n<-\nread.df\n(\n\"./examples/src/main/resources/people.json\"\n,\n\"json\"\n)\n# Register this SparkDataFrame as a temporary view.\ncreateOrReplaceTempView\n(\npeople\n,\n\"people\"\n)\n# SQL sta", "question": "How can eager execution be enabled in sparkR shell?", "answers": {"text": ["Note that to enable eager execution in\nsparkR\nshell, add\nspark.sql.repl.eagerEval.enabled=true\nconfiguration property to the\n--conf\noption."], "answer_start": [181]}}
{"context": "rvreg\n:\nAccelerated Failure Time (AFT) Survival  Model\nspark.glm\nor\nglm\n:\nGeneralized Linear Model (GLM)\nspark.isoreg\n:\nIsotonic Regression\nspark.lm\n:\nLinear Regression\nspark.fmRegressor\n:\nFactorization Machines regressor\nTree\nspark.decisionTree\n:\nDecision Tree for\nRegression\nand\nClassification\nspark.gbt\n:\nGradient Boosted Trees for\nRegression\nand\nClassification\nspark.randomForest\n:\nRandom Forest for\nRegression\nand\nClassification\nClustering\nspark.bisectingKmeans\n:\nBisecting k-means\nspark.gaussianMixture\n:\nGaussian Mixture Model (GMM)\nspark.kmeans\n:\nK-Means\nspark.lda\n:\nLatent Dirichlet Allocation (LDA)\nspark.powerIterationClustering (PIC)\n:\nPower Iteration Clustering (PIC)\nCollaborative Filtering\nspark.als\n:\nAlternating Least Squares (ALS)\nFrequent Pattern Mining\nspark.fpGrowth\n:\nFP-growth\n", "question": "What does spark.glm stand for?", "answers": {"text": ["Generalized Linear Model (GLM)"], "answer_start": [74]}}
{"context": "wer Iteration Clustering (PIC)\nCollaborative Filtering\nspark.als\n:\nAlternating Least Squares (ALS)\nFrequent Pattern Mining\nspark.fpGrowth\n:\nFP-growth\nspark.prefixSpan\n:\nPrefixSpan\nStatistics\nspark.kstest\n:\nKolmogorov-Smirnov Test\nUnder the hood, SparkR uses MLlib to train the model. Please refer to the corresponding section of MLlib user guide for example code.\nUsers can call\nsummary\nto print a summary of the fitted model,\npredict\nto make predictions on new data, and\nwrite.ml\n/\nread.ml\nto save/load fitted models.\nSparkR supports a subset of the available R formula operators for model fitting, including ‘~’, ‘.’, ‘:’, ‘+’, and ‘-‘.\nModel persistence\nThe following example shows how to save/load a MLlib model by SparkR.\ntraining\n<-\nread.df\n(\n\"data/mllib/sample_multiclass_classification_data.t", "question": "What does SparkR use under the hood to train models?", "answers": {"text": ["Under the hood, SparkR uses MLlib to train the model."], "answer_start": [230]}}
{"context": "(\ngaussianGLM2\n)\n# Check model prediction\ngaussianPredictions\n<-\npredict\n(\ngaussianGLM2\n,\ngaussianTestDF\n)\nhead\n(\ngaussianPredictions\n)\nunlink\n(\nmodelPath\n)\nFind full example code at \"examples/src/main/r/ml/ml.R\" in the Spark repo.\nData type mapping between R and Spark\nR\nSpark\nbyte\nbyte\ninteger\ninteger\nfloat\nfloat\ndouble\ndouble\nnumeric\ndouble\ncharacter\nstring\nstring\nstring\nbinary\nbinary\nraw\nbinary\nlogical\nboolean\nPOSIXct\ntimestamp\nPOSIXlt\ntimestamp\nDate\ndate\narray\narray\nlist\narray\nenv\nmap\nStructured Streaming\nSparkR supports the Structured Streaming API. Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. For more information see the R API on the\nStructured Streaming Programming Guide\n.\nApache Arrow in SparkR\nApache Arrow is an in-m", "question": "What does SparkR support in terms of stream processing?", "answers": {"text": ["Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine."], "answer_start": [561]}}
{"context": "for instance,\nSparkR::cume_dist(x)\nor\ndplyr::cume_dist(x)\n.\nYou can inspect the search path in R with\nsearch()\nMigration Guide\nThe migration guide is now archived\non this page\n.", "question": "How can you inspect the search path in R?", "answers": {"text": ["search()"], "answer_start": [102]}}
{"context": "ar\n\\\n1000\n# Run on a YARN cluster in cluster deploy mode\nexport\nHADOOP_CONF_DIR\n=\nXXX\n./bin/spark-submit\n\\\n--class\norg.apache.spark.examples.SparkPi\n\\\n--master\nyarn\n\\\n--deploy-mode\ncluster\n\\\n--executor-memory\n20G\n\\\n--num-executors\n50\n\\\n/path/to/examples.jar\n\\\n1000\n# Run a Python application on a Spark standalone cluster\n./bin/spark-submit\n\\\n--master\nspark://207.184.161.138:7077\n\\\nexamples/src/main/python/pi.py\n\\\n1000\n# Run on a Kubernetes cluster in cluster deploy mode\n./bin/spark-submit\n\\\n--class\norg.apache.spark.examples.SparkPi\n\\\n--master\nk8s://xx.yy.zz.ww:443\n\\\n--deploy-mode\ncluster\n\\\n--executor-memory\n20G\n\\\n--num-executors\n50\n\\\nhttp://path/to/examples.jar\n\\\n1000\nMaster URLs\nThe master URL passed to Spark can be in one of the following formats:\nMaster URL\nMeaning\nlocal\nRun Spark locall", "question": "What are the possible formats for the master URL passed to Spark?", "answers": {"text": ["local"], "answer_start": [778]}}
{"context": "th/to/examples.jar\n\\\n1000\nMaster URLs\nThe master URL passed to Spark can be in one of the following formats:\nMaster URL\nMeaning\nlocal\nRun Spark locally with one worker thread (i.e. no parallelism at all).\nlocal[K]\nRun Spark locally with K worker threads (ideally, set this to the number of cores on your machine).\nlocal[K,F]\nRun Spark locally with K worker threads and F maxFailures (see\nspark.task.maxFailures\nfor an explanation of this variable).\nlocal[*]\nRun Spark locally with as many worker threads as logical cores on your machine.\nlocal[*,F]\nRun Spark locally with as many worker threads as logical cores on your machine and F maxFailures.\nlocal-cluster[N,C,M]\nLocal-cluster mode is only for unit tests. It emulates a distributed cluster in a single JVM with N number of workers, C cores per w", "question": "Qual o significado de 'local[K]' como URL Master para o Spark?", "answers": {"text": ["Run Spark locally with K worker threads (ideally, set this to the number of cores on your machine)."], "answer_start": [214]}}
{"context": "al-cluster[N,C,M]\nLocal-cluster mode is only for unit tests. It emulates a distributed cluster in a single JVM with N number of workers, C cores per worker and M MiB of memory per worker.\nspark://HOST:PORT\nConnect to the given\nSpark standalone\n        cluster\nmaster. The port must be whichever one your master is configured to use, which is 7077 by default.\nspark://HOST1:PORT1,HOST2:PORT2\nConnect to the given\nSpark standalone\n        cluster with standby masters with Zookeeper\n. The list must have all the master hosts in the high availability cluster set up with Zookeeper. The port must be whichever each master is configured to use, which is 7077 by default.\nyarn\nConnect to a\nYARN\ncluster in\nclient\nor\ncluster\nmode depending on the value of\n--deploy-mode\n.\n        The cluster location will b", "question": "What is the default port for the Spark standalone cluster master?", "answers": {"text": ["which is 7077 by default."], "answer_start": [333]}}
{"context": "077 by default.\nyarn\nConnect to a\nYARN\ncluster in\nclient\nor\ncluster\nmode depending on the value of\n--deploy-mode\n.\n        The cluster location will be found based on the\nHADOOP_CONF_DIR\nor\nYARN_CONF_DIR\nvariable.\nk8s://HOST:PORT\nConnect to a\nKubernetes\ncluster in\nclient\nor\ncluster\nmode depending on the value of\n--deploy-mode\n.\n        The\nHOST\nand\nPORT\nrefer to the\nKubernetes API Server\n.\n        It connects using TLS by default. In order to force it to use an unsecured connection, you can use\nk8s://http://HOST:PORT\n.\nLoading Configuration from a File\nThe\nspark-submit\nscript can load default\nSpark configuration values\nfrom a\nproperties file and pass them on to your application. The file can be specified via the\n--properties-file\nparameter. When this is not specified, by default Spark will", "question": "How can Spark be forced to use an unsecured connection to a Kubernetes cluster?", "answers": {"text": ["k8s://http://HOST:PORT"], "answer_start": [500]}}
{"context": "to monitor and debug applications.", "question": "What is the purpose of the described functionality?", "answers": {"text": ["to monitor and debug applications."], "answer_start": [0]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark Streaming Programming Guide\nNote\nOverview\nA Quick Example\nBasic Concepts\nLinking\nInitializing StreamingContext\nDiscretized Streams (DStreams)\nInput DStreams and Receivers\nTransformations on DStreams\nOutput Operations on DStreams\nDataFrame and SQ", "question": "What are some of the programming guides available for Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)"], "answer_start": [46]}}
{"context": "apply Spark’s\nmachine learning\nand\ngraph processing\nalgorithms on data streams.\nInternally, it works as follows. Spark Streaming receives live input data streams and divides\nthe data into batches, which are then processed by the Spark engine to generate the final\nstream of results in batches.\nSpark Streaming provides a high-level abstraction called\ndiscretized stream\nor\nDStream\n,\nwhich represents a continuous stream of data. DStreams can be created either from input data\nstreams from sources such as Kafka, and Kinesis, or by applying high-level\noperations on other DStreams. Internally, a DStream is represented as a sequence of\nRDDs\n.\nThis guide shows you how to start writing Spark Streaming programs with DStreams. You can\nwrite Spark Streaming programs in Scala, Java or Python (introduced ", "question": "What does Spark Streaming divide the incoming data into?", "answers": {"text": ["batches"], "answer_start": [188]}}
{"context": "like. Let’s say we want to\ncount the number of words in text data received from a data server listening on a TCP\nsocket. All you need to\ndo is as follows.\nFirst, we import\nStreamingContext\n, which is the main entry point for all streaming functionality. We create a local StreamingContext with two execution threads, and batch interval of 1 second.\nfrom\npyspark\nimport\nSparkContext\nfrom\npyspark.streaming\nimport\nStreamingContext\n# Create a local StreamingContext with two working thread and batch interval of 1 second\nsc\n=\nSparkContext\n(\n\"\nlocal[2]\n\"\n,\n\"\nNetworkWordCount\n\"\n)\nssc\n=\nStreamingContext\n(\nsc\n,\n1\n)\nUsing this context, we can create a DStream that represents streaming data from a TCP\nsource, specified as hostname (e.g.\nlocalhost\n) and port (e.g.\n9999\n).\n# Create a DStream that will conn", "question": "What is the main entry point for all streaming functionality?", "answers": {"text": ["StreamingContext"], "answer_start": [172]}}
{"context": "cord in the source DStream. In this case,\neach line will be split into multiple words and the stream of words is represented as the\nwords\nDStream.  Next, we want to count these words.\n# Count each word in each batch\npairs\n=\nwords\n.\nmap\n(\nlambda\nword\n:\n(\nword\n,\n1\n))\nwordCounts\n=\npairs\n.\nreduceByKey\n(\nlambda\nx\n,\ny\n:\nx\n+\ny\n)\n# Print the first ten elements of each RDD generated in this DStream to the console\nwordCounts\n.\npprint\n()\nThe\nwords\nDStream is further mapped (one-to-one transformation) to a DStream of\n(word,\n1)\npairs, which is then reduced to get the frequency of words in each batch of data.\nFinally,\nwordCounts.pprint()\nwill print a few of the counts generated every second.\nNote that when these lines are executed, Spark Streaming only sets up the computation it\nwill perform when it is ", "question": "What transformation is used to get the frequency of words in each batch of data?", "answers": {"text": ["reduced to get the frequency of words in each batch of data."], "answer_start": [542]}}
{"context": "ing multiple new records from each record in the source DStream. In this case,\neach line will be split into multiple words and the stream of words is represented as the\nwords\nDStream.  Next, we want to count these words.\nimport\norg.apache.spark.streaming.StreamingContext._\n// not necessary since Spark 1.3\n// Count each word in each batch\nval\npairs\n=\nwords\n.\nmap\n(\nword\n=>\n(\nword\n,\n1\n))\nval\nwordCounts\n=\npairs\n.\nreduceByKey\n(\n_\n+\n_\n)\n// Print the first ten elements of each RDD generated in this DStream to the console\nwordCounts\n.\nprint\n()\nThe\nwords\nDStream is further mapped (one-to-one transformation) to a DStream of\n(word,\n1)\npairs, which is then reduced to get the frequency of words in each batch of data.\nFinally,\nwordCounts.print()\nwill print a few of the counts generated every second.\nNot", "question": "What is the next step after the words DStream is created?", "answers": {"text": ["Next, we want to count these words."], "answer_start": [185]}}
{"context": "entry point for all streaming\nfunctionality. We create a local StreamingContext with two execution threads, and a batch interval of 1 second.\nimport\norg.apache.spark.*\n;\nimport\norg.apache.spark.api.java.function.*\n;\nimport\norg.apache.spark.streaming.*\n;\nimport\norg.apache.spark.streaming.api.java.*\n;\nimport\nscala.Tuple2\n;\n// Create a local StreamingContext with two working thread and batch interval of 1 second\nSparkConf\nconf\n=\nnew\nSparkConf\n().\nsetMaster\n(\n\"local[2]\"\n).\nsetAppName\n(\n\"NetworkWordCount\"\n);\nJavaStreamingContext\njssc\n=\nnew\nJavaStreamingContext\n(\nconf\n,\nDurations\n.\nseconds\n(\n1\n));\nUsing this context, we can create a DStream that represents streaming data from a TCP\nsource, specified as hostname (e.g.\nlocalhost\n) and port (e.g.\n9999\n).\n// Create a DStream that will connect to hos", "question": "How is a local StreamingContext created, specifying the number of execution threads and batch interval?", "answers": {"text": ["We create a local StreamingContext with two execution threads, and a batch interval of 1 second."], "answer_start": [45]}}
{"context": "sformations have been setup, we finally call\nstart\nmethod.\njssc\n.\nstart\n();\n// Start the computation\njssc\n.\nawaitTermination\n();\n// Wait for the computation to terminate\nThe complete code can be found in the Spark Streaming example\nJavaNetworkWordCount\n.\nIf you have already\ndownloaded\nand\nbuilt\nSpark,\nyou can run this example as follows. You will first need to run Netcat\n(a small utility found in most Unix-like systems) as a data server by using\n$\nnc\n-lk\n9999\nThen, in a different terminal, you can start the example by using\n$\n./bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999\n$\n./bin/run-example streaming.NetworkWordCount localhost 9999\n$\n./bin/run-example streaming.JavaNetworkWordCount localhost 9999\nThen, any lines typed in the terminal running the ", "question": "How can you start the example if you have already downloaded and built Spark?", "answers": {"text": ["./bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999"], "answer_start": [532]}}
{"context": "Spark Streaming program, you will have to add the following dependency to your SBT or Maven project.\n<dependency>\n    <groupId>org.apache.spark</groupId>\n    <artifactId>spark-streaming_2.13</artifactId>\n    <version>4.0.0</version>\n    <scope>provided</scope>\n</dependency>\nlibraryDependencies += \"org.apache.spark\" % \"spark-streaming_2.13\" % \"4.0.0\" % \"provided\"\nFor ingesting data from sources like Kafka and Kinesis that are not present in the Spark\nStreaming core\n API, you will have to add the corresponding\nartifact\nspark-streaming-xyz_2.13\nto the dependencies. For example,\nsome of the common ones are as follows.\nSource\nArtifact\nKafka\nspark-streaming-kafka-0-10_2.13\nKinesis\nspark-streaming-kinesis-asl_2.13 [Amazon Software License]\nFor an up-to-date list, please refer to the\nMaven reposit", "question": "What dependency should be added to an SBT or Maven project for a Spark Streaming program?", "answers": {"text": ["<dependency>\n    <groupId>org.apache.spark</groupId>\n    <artifactId>spark-streaming_2.13</artifactId>\n    <version>4.0.0</version>\n    <scope>provided</scope>\n</dependency>\nlibraryDependencies += \"org.apache.spark\" % \"spark-streaming_2.13\" % \"4.0.0\" % \"provided\""], "answer_start": [101]}}
{"context": "our application to show on the cluster UI.\nmaster\nis a\nSpark or YARN cluster URL\n,\nor a special\n“local[*]”\nstring to run in local mode. In practice, when running on a cluster,\nyou will not want to hardcode\nmaster\nin the program,\nbut rather\nlaunch the application with\nspark-submit\nand\nreceive it there. However, for local testing and unit tests, you can pass “local[*]” to run Spark Streaming\nin-process (detects the number of cores in the local system).\nThe batch interval must be set based on the latency requirements of your application\nand available cluster resources. See the\nPerformance Tuning\nsection for more details.\nA\nStreamingContext\nobject can be created from a\nSparkConf\nobject.\nimport\norg.apache.spark._\nimport\norg.apache.spark.streaming._\nval\nconf\n=\nnew\nSparkConf\n().\nsetAppName\n(\nappN", "question": "What string can be passed to run Spark Streaming in-process for local testing?", "answers": {"text": ["“local[*]"], "answer_start": [96]}}
{"context": " a context is defined, you have to do the following.\nDefine the input sources by creating input DStreams.\nDefine the streaming computations by applying transformation and output operations to DStreams.\nStart receiving data and processing it using\nstreamingContext.start()\n.\nWait for the processing to be stopped (manually or due to any error) using\nstreamingContext.awaitTermination()\n.\nThe processing can be manually stopped using\nstreamingContext.stop()\n.\nPoints to remember:\nOnce a context has been started, no new streaming computations can be set up or added to it.\nOnce a context has been stopped, it cannot be restarted.\nOnly one StreamingContext can be active in a JVM at the same time.\nstop() on StreamingContext also stops the SparkContext. To stop only the StreamingContext, set the option", "question": "What happens when you call stop() on a StreamingContext?", "answers": {"text": ["stop() on StreamingContext also stops the SparkContext."], "answer_start": [695]}}
{"context": " simple directory can be monitored, such as\n\"hdfs://namenode:8040/logs/\"\n.\nAll files directly under such a path will be processed as they are discovered.\nA\nPOSIX glob pattern\ncan be supplied, such as\n\"hdfs://namenode:8040/logs/2017/*\"\n.\nHere, the DStream will consist of all files in the directories\nmatching the pattern.\nThat is: it is a pattern of directories, not of files in directories.\nAll files must be in the same data format.\nA file is considered part of a time period based on its modification time,\nnot its creation time.\nOnce processed, changes to a file within the current window will not cause the file to be reread.\nThat is:\nupdates are ignored\n.\nThe more files under a directory, the longer it will take to\nscan for changes — even if no files have been modified.\nIf a wildcard is used", "question": "What happens to updates made to a file within the current window after it has been processed?", "answers": {"text": ["updates are ignored"], "answer_start": [640]}}
{"context": "treamingContext\nfor Python,\nStreamingContext\nfor Scala,\nand\nJavaStreamingContext\nfor Java.\nAdvanced Sources\nPython API\nAs of Spark 4.0.0,\nout of these sources, Kafka and Kinesis are available in the Python API.\nThis category of sources requires interfacing with external non-Spark libraries, some of them with\ncomplex dependencies (e.g., Kafka). Hence, to minimize issues related to version conflicts\nof dependencies, the functionality to create DStreams from these sources has been moved to separate\nlibraries that can be\nlinked\nto explicitly when necessary.\nNote that these advanced sources are not available in the Spark shell, hence applications based on\nthese advanced sources cannot be tested in the shell. If you really want to use them in the Spark\nshell you will have to download the corresp", "question": "Which sources are available in the Python API as of Spark 4.0.0?", "answers": {"text": ["Kafka and Kinesis are available in the Python API."], "answer_start": [160]}}
{"context": "unt of each word\nseen in a text data stream. Here, the running count is the state and it is an integer. We\ndefine the update function as:\ndef\nupdateFunction\n(\nnewValues\n,\nrunningCount\n):\nif\nrunningCount\nis\nNone\n:\nrunningCount\n=\n0\nreturn\nsum\n(\nnewValues\n,\nrunningCount\n)\n# add the new values with the previous running count to get the new count\nThis is applied on a DStream containing words (say, the\npairs\nDStream containing\n(word,\n1)\npairs in the\nearlier example\n).\nrunningCounts\n=\npairs\n.\nupdateStateByKey\n(\nupdateFunction\n)\nThe update function will be called for each word, with\nnewValues\nhaving a sequence of 1’s (from\nthe\n(word, 1)\npairs) and the\nrunningCount\nhaving the previous count. For the complete\nPython code, take a look at the example\nstateful_network_wordcount.py\n.\ndef\nupdateFunction\n", "question": "What is the state in the running count example?", "answers": {"text": ["the state and it is an integer"], "answer_start": [72]}}
{"context": "ver,\nyou can easily use\ntransform\nto do this. This enables very powerful possibilities. For example,\none can do real-time data cleaning by joining the input data stream with precomputed\nspam information (maybe generated with Spark as well) and then filtering based on it.\nspamInfoRDD\n=\nsc\n.\npickleFile\n(...)\n# RDD containing spam information\n# join data stream with spam information to do data cleaning\ncleanedDStream\n=\nwordCounts\n.\ntransform\n(\nlambda\nrdd\n:\nrdd\n.\njoin\n(\nspamInfoRDD\n).\nfilter\n(...))\nval\nspamInfoRDD\n=\nssc\n.\nsparkContext\n.\nnewAPIHadoopRDD\n(...)\n// RDD containing spam information\nval\ncleanedDStream\n=\nwordCounts\n.\ntransform\n{\nrdd\n=>\nrdd\n.\njoin\n(\nspamInfoRDD\n).\nfilter\n(...)\n// join data stream with spam information to do data cleaning\n...\n}\nimport\norg.apache.spark.streaming.api.java", "question": "What is used to join the input data stream with precomputed spam information for real-time data cleaning?", "answers": {"text": ["transform"], "answer_start": [24]}}
{"context": "dd\n.\njoin\n(\nspamInfoRDD\n).\nfilter\n(...)\n// join data stream with spam information to do data cleaning\n...\n}\nimport\norg.apache.spark.streaming.api.java.*\n;\n// RDD containing spam information\nJavaPairRDD\n<\nString\n,\nDouble\n>\nspamInfoRDD\n=\njssc\n.\nsparkContext\n().\nnewAPIHadoopRDD\n(...);\nJavaPairDStream\n<\nString\n,\nInteger\n>\ncleanedDStream\n=\nwordCounts\n.\ntransform\n(\nrdd\n->\n{\nrdd\n.\njoin\n(\nspamInfoRDD\n).\nfilter\n(...);\n// join data stream with spam information to do data cleaning\n...\n});\nNote that the supplied function gets called in every batch interval. This allows you to do\ntime-varying RDD operations, that is, RDD operations, number of partitions, broadcast variables,\netc. can be changed between batches.\nWindow Operations\nSpark Streaming also provides\nwindowed computations\n, which allow you to a", "question": "What is the purpose of joining the data stream with spam information?", "answers": {"text": ["// join data stream with spam information to do data cleaning"], "answer_start": [40]}}
{"context": "The duration of the window (3 in the figure).\nsliding interval\n- The interval at which the window operation is performed (2 in\n the figure).\nThese two parameters must be multiples of the batch interval of the source DStream (1 in the\nfigure).\nLet’s illustrate the window operations with an example. Say, you want to extend the\nearlier example\nby generating word counts over the last 30 seconds of data,\nevery 10 seconds. To do this, we have to apply the\nreduceByKey\noperation on the\npairs\nDStream of\n(word, 1)\npairs over the last 30 seconds of data. This is done using the\noperation\nreduceByKeyAndWindow\n.\n# Reduce last 30 seconds of data, every 10 seconds\nwindowedWordCounts\n=\npairs\n.\nreduceByKeyAndWindow\n(\nlambda\nx\n,\ny\n:\nx\n+\ny\n,\nlambda\nx\n,\ny\n:\nx\n-\ny\n,\n30\n,\n10\n)\n// Reduce last 30 seconds of data, ", "question": "What operation is used to reduce the pairs DStream of (word, 1) pairs over the last 30 seconds of data?", "answers": {"text": ["reduceByKeyAndWindow"], "answer_start": [583]}}
{"context": "oinedStream\n=\nwindowedStream1\n.\njoin\n(\nwindowedStream2\n)\nval\nwindowedStream1\n=\nstream1\n.\nwindow\n(\nSeconds\n(\n20\n))\nval\nwindowedStream2\n=\nstream2\n.\nwindow\n(\nMinutes\n(\n1\n))\nval\njoinedStream\n=\nwindowedStream1\n.\njoin\n(\nwindowedStream2\n)\nJavaPairDStream\n<\nString\n,\nString\n>\nwindowedStream1\n=\nstream1\n.\nwindow\n(\nDurations\n.\nseconds\n(\n20\n));\nJavaPairDStream\n<\nString\n,\nString\n>\nwindowedStream2\n=\nstream2\n.\nwindow\n(\nDurations\n.\nminutes\n(\n1\n));\nJavaPairDStream\n<\nString\n,\nTuple2\n<\nString\n,\nString\n>>\njoinedStream\n=\nwindowedStream1\n.\njoin\n(\nwindowedStream2\n);\nStream-dataset joins\nThis has already been shown earlier while explain\nDStream.transform\noperation. Here is yet another example of joining a windowed stream with a dataset.\ndataset\n=\n...\n# some RDD\nwindowedStream\n=\nstream\n.\nwindow\n(\n20\n)\njoinedStream\n", "question": "What operation is demonstrated with the joining of a windowed stream with a dataset?", "answers": {"text": ["Stream-dataset joins"], "answer_start": [549]}}
{"context": "set\n));\nIn fact, you can also dynamically change the dataset you want to join against. The function provided to\ntransform\nis evaluated every batch interval and therefore will use the current dataset that\ndataset\nreference points to.\nThe complete list of DStream transformations is available in the API documentation. For the Python API,\nsee\nDStream\n.\nFor the Scala API, see\nDStream\nand\nPairDStreamFunctions\n.\nFor the Java API, see\nJavaDStream\nand\nJavaPairDStream\n.\nOutput Operations on DStreams\nOutput operations allow DStream’s data to be pushed out to external systems like a database or a file system.\nSince the output operations actually allow the transformed data to be consumed by external systems,\nthey trigger the actual execution of all the DStream transformations (similar to actions for RD", "question": "What triggers the actual execution of all DStream transformations?", "answers": {"text": ["they trigger the actual execution of all the DStream transformations"], "answer_start": [705]}}
{"context": ".suffix]\"\n.\nsaveAsObjectFiles\n(\nprefix\n, [\nsuffix\n])\nSave this DStream's contents as\nSequenceFiles\nof serialized Java objects. The file\n  name at each batch interval is generated based on\nprefix\nand\nsuffix\n:\n\"prefix-TIME_IN_MS[.suffix]\"\n.\nPython API\nThis is not available in\n  the Python API.\nsaveAsHadoopFiles\n(\nprefix\n, [\nsuffix\n])\nSave this DStream's contents as Hadoop files. The file name at each batch interval is\n  generated based on\nprefix\nand\nsuffix\n:\n\"prefix-TIME_IN_MS[.suffix]\"\n.\nPython API\nThis is not available in\n  the Python API.\nforeachRDD\n(\nfunc\n)\nThe most generic output operator that applies a function,\nfunc\n, to each RDD generated from\n  the stream. This function should push the data in each RDD to an external system, such as saving the RDD to\n  files, or writing it over the ", "question": "How are the file names generated when saving a DStream as SequenceFiles or Hadoop files?", "answers": {"text": ["\"prefix-TIME_IN_MS[.suffix]\""], "answer_start": [208]}}
{"context": " driver\nrdd\n.\nforeach\n{\nrecord\n=>\nconnection\n.\nsend\n(\nrecord\n)\n// executed at the worker\n}\n}\ndstream\n.\nforeachRDD\n(\nrdd\n->\n{\nConnection\nconnection\n=\ncreateNewConnection\n();\n// executed at the driver\nrdd\n.\nforeach\n(\nrecord\n->\n{\nconnection\n.\nsend\n(\nrecord\n);\n// executed at the worker\n});\n});\nThis is incorrect as this requires the connection object to be serialized and sent from the\ndriver to the worker. Such connection objects are rarely transferable across machines. This\nerror may manifest as serialization errors (connection object not serializable), initialization\nerrors (connection object needs to be initialized at the workers), etc. The correct solution is\nto create the connection object at the worker.\nHowever, this can lead to another common mistake - creating a new connection for every", "question": "What problem arises from requiring the connection object to be serialized and sent from the driver to the worker?", "answers": {"text": ["Such connection objects are rarely transferable across machines."], "answer_start": [405]}}
{"context": "rect solution is\nto create the connection object at the worker.\nHowever, this can lead to another common mistake - creating a new connection for every record.\nFor example,\ndef\nsendRecord\n(\nrecord\n):\nconnection\n=\ncreateNewConnection\n()\nconnection\n.\nsend\n(\nrecord\n)\nconnection\n.\nclose\n()\ndstream\n.\nforeachRDD\n(\nlambda\nrdd\n:\nrdd\n.\nforeach\n(\nsendRecord\n))\ndstream\n.\nforeachRDD\n{\nrdd\n=>\nrdd\n.\nforeach\n{\nrecord\n=>\nval\nconnection\n=\ncreateNewConnection\n()\nconnection\n.\nsend\n(\nrecord\n)\nconnection\n.\nclose\n()\n}\n}\ndstream\n.\nforeachRDD\n(\nrdd\n->\n{\nrdd\n.\nforeach\n(\nrecord\n->\n{\nConnection\nconnection\n=\ncreateNewConnection\n();\nconnection\n.\nsend\n(\nrecord\n);\nconnection\n.\nclose\n();\n});\n});\nTypically, creating a connection object has time and resource overheads. Therefore, creating and\ndestroying a connection object ", "question": "What is a common mistake when creating a connection object at the worker?", "answers": {"text": ["creating a new connection for every record."], "answer_start": [115]}}
{"context": "reachRDD\n{\nrdd\n=>\nrdd\n.\nforeachPartition\n{\npartitionOfRecords\n=>\nval\nconnection\n=\ncreateNewConnection\n()\npartitionOfRecords\n.\nforeach\n(\nrecord\n=>\nconnection\n.\nsend\n(\nrecord\n))\nconnection\n.\nclose\n()\n}\n}\ndstream\n.\nforeachRDD\n(\nrdd\n->\n{\nrdd\n.\nforeachPartition\n(\npartitionOfRecords\n->\n{\nConnection\nconnection\n=\ncreateNewConnection\n();\nwhile\n(\npartitionOfRecords\n.\nhasNext\n())\n{\nconnection\n.\nsend\n(\npartitionOfRecords\n.\nnext\n());\n}\nconnection\n.\nclose\n();\n});\n});\nThis amortizes the connection creation overheads over many records.\nFinally, this can be further optimized by reusing connection objects across multiple RDDs/batches.\nOne can maintain a static pool of connection objects than can be reused as\nRDDs of multiple batches are pushed to the external system, thus further reducing the overheads.\ndef", "question": "How can connection creation overheads be amortized over many records?", "answers": {"text": ["This amortizes the connection creation overheads over many records."], "answer_start": [458]}}
{"context": " pool of connection objects than can be reused as\nRDDs of multiple batches are pushed to the external system, thus further reducing the overheads.\ndef\nsendPartition\n(\niter\n):\n# ConnectionPool is a static, lazily initialized pool of connections\nconnection\n=\nConnectionPool\n.\ngetConnection\n()\nfor\nrecord\nin\niter\n:\nconnection\n.\nsend\n(\nrecord\n)\n# return to the pool for future reuse\nConnectionPool\n.\nreturnConnection\n(\nconnection\n)\ndstream\n.\nforeachRDD\n(\nlambda\nrdd\n:\nrdd\n.\nforeachPartition\n(\nsendPartition\n))\ndstream\n.\nforeachRDD\n{\nrdd\n=>\nrdd\n.\nforeachPartition\n{\npartitionOfRecords\n=>\n// ConnectionPool is a static, lazily initialized pool of connections\nval\nconnection\n=\nConnectionPool\n.\ngetConnection\n()\npartitionOfRecords\n.\nforeach\n(\nrecord\n=>\nconnection\n.\nsend\n(\nrecord\n))\nConnectionPool\n.\nreturnCo", "question": "What is ConnectionPool in the provided code?", "answers": {"text": ["ConnectionPool is a static, lazily initialized pool of connections"], "answer_start": [177]}}
{"context": "ns\nval\nconnection\n=\nConnectionPool\n.\ngetConnection\n()\npartitionOfRecords\n.\nforeach\n(\nrecord\n=>\nconnection\n.\nsend\n(\nrecord\n))\nConnectionPool\n.\nreturnConnection\n(\nconnection\n)\n// return to the pool for future reuse\n}\n}\ndstream\n.\nforeachRDD\n(\nrdd\n->\n{\nrdd\n.\nforeachPartition\n(\npartitionOfRecords\n->\n{\n// ConnectionPool is a static, lazily initialized pool of connections\nConnection\nconnection\n=\nConnectionPool\n.\ngetConnection\n();\nwhile\n(\npartitionOfRecords\n.\nhasNext\n())\n{\nconnection\n.\nsend\n(\npartitionOfRecords\n.\nnext\n());\n}\nConnectionPool\n.\nreturnConnection\n(\nconnection\n);\n// return to the pool for future reuse\n});\n});\nNote that the connections in the pool should be lazily created on demand and timed out if not used for a while. This achieves the most efficient sending of data to external systems", "question": "What should happen with the connections in the pool to achieve the most efficient sending of data?", "answers": {"text": ["This achieves the most efficient sending of data to external systems"], "answer_start": [732]}}
{"context": "he pool should be lazily created on demand and timed out if not used for a while. This achieves the most efficient sending of data to external systems.\nOther points to remember:\nDStreams are executed lazily by the output operations, just like RDDs are lazily executed by RDD actions. Specifically, RDD actions inside the DStream output operations force the processing of the received data. Hence, if your application does not have any output operation, or has output operations like\ndstream.foreachRDD()\nwithout any RDD action inside them, then nothing will get executed. The system will simply receive the data and discard it.\nBy default, output operations are executed one-at-a-time. And they are executed in the order they are defined in the application.\nDataFrame and SQL Operations\nYou can easil", "question": "What happens if a DStream application lacks output operations or has output operations without any RDD actions?", "answers": {"text": ["then nothing will get executed. The system will simply receive the data and discard it."], "answer_start": [540]}}
{"context": "rations are executed one-at-a-time. And they are executed in the order they are defined in the application.\nDataFrame and SQL Operations\nYou can easily use\nDataFrames and SQL\noperations on streaming data. You have to create a SparkSession using the SparkContext that the StreamingContext is using. Furthermore, this has to be done such that it can be restarted on driver failures. This is done by creating a lazily instantiated singleton instance of SparkSession. This is shown in the following example. It modifies the earlier\nword count example\nto generate word counts using DataFrames and SQL. Each RDD is converted to a DataFrame, registered as a temporary table and then queried using SQL.\n# Lazily instantiated global instance of SparkSession\ndef\ngetSparkSessionInstance\n(\nsparkConf\n):\nif\n(\n\"\ns", "question": "How are operations executed on streaming data?", "answers": {"text": ["rations are executed one-at-a-time. And they are executed in the order they are defined in the application."], "answer_start": [0]}}
{"context": "\n,\nJavaRow\n.\nclass\n);\n// Creates a temporary view using the DataFrame\nwordsDataFrame\n.\ncreateOrReplaceTempView\n(\n\"words\"\n);\n// Do word count on table using SQL and print it\nDataFrame\nwordCountsDataFrame\n=\nspark\n.\nsql\n(\n\"select word, count(*) as total from words group by word\"\n);\nwordCountsDataFrame\n.\nshow\n();\n});\nSee the full\nsource code\n.\nYou can also run SQL queries on tables defined on streaming data from a different thread (that is, asynchronous to the running StreamingContext). Just make sure that you set the StreamingContext to remember a sufficient amount of streaming data such that the query can run. Otherwise the StreamingContext, which is unaware of any of the asynchronous SQL queries, will delete off old streaming data before the query can complete. For example, if you want to q", "question": "What should you ensure when running SQL queries on tables defined on streaming data from a different thread?", "answers": {"text": ["Just make sure that you set the StreamingContext to remember a sufficient amount of streaming data such that the query can run."], "answer_start": [488]}}
{"context": "ich is unaware of any of the asynchronous SQL queries, will delete off old streaming data before the query can complete. For example, if you want to query the last batch, but your query can take 5 minutes to run, then call\nstreamingContext.remember(Minutes(5))\n(in Scala, or equivalent in other languages).\nSee the\nDataFrames and SQL\nguide to learn more about DataFrames.\nMLlib Operations\nYou can also easily use machine learning algorithms provided by\nMLlib\n. First of all, there are streaming machine learning algorithms (e.g.\nStreaming Linear Regression\n,\nStreaming KMeans\n, etc.) which can simultaneously learn from the streaming data as well as apply the model on the streaming data. Beyond these, for a much larger class of machine learning algorithms, you can learn a learning model offline (i", "question": "What should you call to remember a batch for 5 minutes in Scala?", "answers": {"text": ["streamingContext.remember(Minutes(5))"], "answer_start": [223]}}
{"context": "il later). Metadata includes:\nConfiguration\n- The configuration that was used to create the streaming application.\nDStream operations\n- The set of DStream operations that define the streaming application.\nIncomplete batches\n- Batches whose jobs are queued but have not completed yet.\nData checkpointing\n- Saving of the generated RDDs to reliable storage. This is necessary\nin some\nstateful\ntransformations that combine data across multiple batches. In such\ntransformations, the generated RDDs depend on RDDs of previous batches, which causes the length\nof the dependency chain to keep increasing with time. To avoid such unbounded increases in recovery\n time (proportional to dependency chain), intermediate RDDs of stateful transformations are periodically\ncheckpointed\nto reliable storage (e.g. HDF", "question": "What is the purpose of data checkpointing?", "answers": {"text": ["Saving of the generated RDDs to reliable storage."], "answer_start": [305]}}
{"context": "d after failure, it will re-create a StreamingContext\nfrom the checkpoint data in the checkpoint directory.\nThis behavior is made simple by using\nStreamingContext.getOrCreate\n. This is used as follows.\n# Function to create and setup a new StreamingContext\ndef\nfunctionToCreateContext\n():\nsc\n=\nSparkContext\n(...)\n# new context\nssc\n=\nStreamingContext\n(...)\nlines\n=\nssc\n.\nsocketTextStream\n(...)\n# create DStreams\n...\nssc\n.\ncheckpoint\n(\ncheckpointDirectory\n)\n# set checkpoint directory\nreturn\nssc\n# Get StreamingContext from checkpoint data or create a new one\ncontext\n=\nStreamingContext\n.\ngetOrCreate\n(\ncheckpointDirectory\n,\nfunctionToCreateContext\n)\n# Do additional setup on context that needs to be done,\n# irrespective of whether it is being started or restarted\ncontext\n.\n...\n# Start the context\ncon", "question": "How is a StreamingContext recreated from checkpoint data?", "answers": {"text": ["StreamingContext.getOrCreate"], "answer_start": [146]}}
{"context": "heckpoint data and start the\n computation by using\nStreamingContext.getOrCreate(checkpointDirectory, None)\n.\nThis behavior is made simple by using\nStreamingContext.getOrCreate\n. This is used as follows.\n// Function to create and setup a new StreamingContext\ndef\nfunctionToCreateContext\n()\n:\nStreamingContext\n=\n{\nval\nssc\n=\nnew\nStreamingContext\n(...)\n// new context\nval\nlines\n=\nssc\n.\nsocketTextStream\n(...)\n// create DStreams\n...\nssc\n.\ncheckpoint\n(\ncheckpointDirectory\n)\n// set checkpoint directory\nssc\n}\n// Get StreamingContext from checkpoint data or create a new one\nval\ncontext\n=\nStreamingContext\n.\ngetOrCreate\n(\ncheckpointDirectory\n,\nfunctionToCreateContext\n_\n)\n// Do additional setup on context that needs to be done,\n// irrespective of whether it is being started or restarted\ncontext\n.\n...\n// S", "question": "How is a StreamingContext obtained or created?", "answers": {"text": ["StreamingContext.getOrCreate(checkpointDirectory, None)"], "answer_start": [51]}}
{"context": "OrCreate\none also needs to ensure that the driver process gets\nrestarted automatically on failure. This can only be done by the deployment infrastructure that is\nused to run the application. This is further discussed in the\nDeployment\nsection.\nNote that checkpointing of RDDs incurs the cost of saving to reliable storage.\nThis may cause an increase in the processing time of those batches where RDDs get checkpointed.\nHence, the interval of\ncheckpointing needs to be set carefully. At small batch sizes (say 1 second), checkpointing every\nbatch may significantly reduce operation throughput. Conversely, checkpointing too infrequently\ncauses the lineage and task sizes to grow, which may have detrimental effects. For stateful\ntransformations that require RDD checkpointing, the default interval is ", "question": "What is a potential consequence of checkpointing RDDs too infrequently?", "answers": {"text": ["causes the lineage and task sizes to grow, which may have detrimental effects."], "answer_start": [636]}}
{"context": "List Broadcast\nexcludeList\n=\ngetWordExcludeList\n(\nrdd\n.\ncontext\n)\n# Get or register the droppedWordsCounter Accumulator\ndroppedWordsCounter\n=\ngetDroppedWordsCounter\n(\nrdd\n.\ncontext\n)\n# Use excludeList to drop words and use droppedWordsCounter to count them\ndef\nfilterFunc\n(\nwordCount\n):\nif\nwordCount\n[\n0\n]\nin\nexcludeList\n.\nvalue\n:\ndroppedWordsCounter\n.\nadd\n(\nwordCount\n[\n1\n])\nFalse\nelse\n:\nTrue\ncounts\n=\n\"\nCounts at time %s %s\n\"\n%\n(\ntime\n,\nrdd\n.\nfilter\n(\nfilterFunc\n).\ncollect\n())\nwordCounts\n.\nforeachRDD\n(\necho\n)\nSee the full\nsource code\n.\nobject\nWordExcludeList\n{\n@volatile\nprivate\nvar\ninstance\n:\nBroadcast\n[\nSeq\n[\nString\n]]\n=\nnull\ndef\ngetInstance\n(\nsc\n:\nSparkContext\n)\n:\nBroadcast\n[\nSeq\n[\nString\n]]\n=\n{\nif\n(\ninstance\n==\nnull\n)\n{\nsynchronized\n{\nif\n(\ninstance\n==\nnull\n)\n{\nval\nwordExcludeList\n=\nSeq\n(\n", "question": "What is used to drop words and count them in the provided code?", "answers": {"text": ["excludeList to drop words and use droppedWordsCounter to count them"], "answer_start": [189]}}
{"context": "edWordsCounter\"\n);\n}\n}\n}\nreturn\ninstance\n;\n}\n}\nwordCounts\n.\nforeachRDD\n((\nrdd\n,\ntime\n)\n->\n{\n// Get or register the excludeList Broadcast\nBroadcast\n<\nList\n<\nString\n>>\nexcludeList\n=\nJavaWordExcludeList\n.\ngetInstance\n(\nnew\nJavaSparkContext\n(\nrdd\n.\ncontext\n()));\n// Get or register the droppedWordsCounter Accumulator\nLongAccumulator\ndroppedWordsCounter\n=\nJavaDroppedWordsCounter\n.\ngetInstance\n(\nnew\nJavaSparkContext\n(\nrdd\n.\ncontext\n()));\n// Use excludeList to drop words and use droppedWordsCounter to count them\nString\ncounts\n=\nrdd\n.\nfilter\n(\nwordCount\n->\n{\nif\n(\nexcludeList\n.\nvalue\n().\ncontains\n(\nwordCount\n.\n_1\n()))\n{\ndroppedWordsCounter\n.\nadd\n(\nwordCount\n.\n_2\n());\nreturn\nfalse\n;\n}\nelse\n{\nreturn\ntrue\n;\n}\n}).\ncollect\n().\ntoString\n();\nString\noutput\n=\n\"Counts at time \"\n+\ntime\n+\n\" \"\n+\ncounts\n;\n}\nSee t", "question": "What is used to drop words and count them in the provided code snippet?", "answers": {"text": ["excludeList to drop words and use droppedWordsCounter to count them"], "answer_start": [442]}}
{"context": "Count\n.\n_2\n());\nreturn\nfalse\n;\n}\nelse\n{\nreturn\ntrue\n;\n}\n}).\ncollect\n().\ntoString\n();\nString\noutput\n=\n\"Counts at time \"\n+\ntime\n+\n\" \"\n+\ncounts\n;\n}\nSee the full\nsource code\n.\nDeploying Applications\nThis section discusses the steps to deploy a Spark Streaming application.\nRequirements\nTo run Spark Streaming applications, you need to have the following.\nCluster with a cluster manager\n- This is the general requirement of any Spark application,\nand discussed in detail in the\ndeployment guide\n.\nPackage the application JAR\n- You have to compile your streaming application into a JAR.\nIf you are using\nspark-submit\nto start the\napplication, then you will not need to provide Spark and Spark Streaming in the JAR. However,\nif your application uses\nadvanced sources\n(e.g. Kafka),\nthen you will have to pack", "question": "What is a general requirement for any Spark application?", "answers": {"text": ["Cluster with a cluster manager"], "answer_start": [351]}}
{"context": "g.receiver.writeAheadLog.enable\nto\ntrue\n. However, these stronger semantics may\ncome at the cost of the receiving throughput of individual receivers. This can be corrected by\nrunning\nmore receivers in parallel\nto increase aggregate throughput. Additionally, it is recommended that the replication of the\nreceived data within Spark be disabled when the write-ahead log is enabled as the log is already\nstored in a replicated storage system. This can be done by setting the storage level for the\ninput stream to\nStorageLevel.MEMORY_AND_DISK_SER\n. While using S3 (or any file system that\ndoes not support flushing) for\nwrite-ahead logs\n, please remember to enable\nspark.streaming.driver.writeAheadLog.closeFileAfterWrite\nand\nspark.streaming.receiver.writeAheadLog.closeFileAfterWrite\n. See\nSpark Streami", "question": "What storage level is recommended when the write-ahead log is enabled?", "answers": {"text": ["StorageLevel.MEMORY_AND_DISK_SER"], "answer_start": [510]}}
{"context": "\nget\n(\n0\n),\nkafkaStreams\n.\nsubList\n(\n1\n,\nkafkaStreams\n.\nsize\n()));\nunifiedStream\n.\nprint\n();\nAnother parameter that should be considered is the receiver’s block interval,\nwhich is determined by the\nconfiguration parameter\nspark.streaming.blockInterval\n. For most receivers, the received data is coalesced together into\nblocks of data before storing inside Spark’s memory. The number of blocks in each batch\ndetermines the number of tasks that will be used to process\nthe received data in a map-like transformation. The number of tasks per receiver per batch will be\napproximately (batch interval / block interval). For example, a block interval of 200 ms will\ncreate 10 tasks per 2 second batches. If the number of tasks is too low (that is, less than the number\nof cores per machine), then it will b", "question": "How is the number of tasks per receiver per batch approximately determined?", "answers": {"text": ["(batch interval / block interval)."], "answer_start": [580]}}
{"context": "0 ms will\ncreate 10 tasks per 2 second batches. If the number of tasks is too low (that is, less than the number\nof cores per machine), then it will be inefficient as all available cores will not be used to\nprocess the data. To increase the number of tasks for a given batch interval, reduce the\nblock interval. However, the recommended minimum value of block interval is about 50 ms,\nbelow which the task launching overheads may be a problem.\nAn alternative to receiving data with multiple input streams / receivers is to explicitly repartition\nthe input data stream (using\ninputStream.repartition(<number of partitions>)\n).\nThis distributes the received batches of data across the specified number of machines in the cluster\nbefore further processing.\nFor direct stream, please refer to\nSpark Strea", "question": "What is the recommended minimum value for the block interval?", "answers": {"text": ["the recommended minimum value of block interval is about 50 ms"], "answer_start": [321]}}
{"context": "t the\nspark.default.parallelism\nconfiguration property\nto change the default.\nData Serialization\nThe overheads of data serialization can be reduced by tuning the serialization formats. In the case of streaming, there are two types of data that are being serialized.\nInput data\n: By default, the input data received through Receivers is stored in the executors’ memory with\nStorageLevel.MEMORY_AND_DISK_SER_2\n. That is, the data is serialized into bytes to reduce GC overheads, and replicated for tolerating executor failures. Also, the data is kept first in memory, and spilled over to disk only if the memory is insufficient to hold all of the input data necessary for the streaming computation. This serialization obviously has overheads – the receiver must deserialize the received data and re-ser", "question": "What StorageLevel is used by default for input data received through Receivers?", "answers": {"text": ["StorageLevel.MEMORY_AND_DISK_SER_2"], "answer_start": [373]}}
{"context": " data necessary for the streaming computation. This serialization obviously has overheads – the receiver must deserialize the received data and re-serialize it using Spark’s serialization format.\nPersisted RDDs generated by Streaming Operations\n: RDDs generated by streaming computations may be persisted in memory. For example, window operations persist data in memory as they would be processed multiple times. However, unlike the Spark Core default of\nStorageLevel.MEMORY_ONLY\n, persisted RDDs generated by streaming computations are persisted with\nStorageLevel.MEMORY_ONLY_SER\n(i.e. serialized) by default to minimize GC overheads.\nIn both cases, using Kryo serialization can reduce both CPU and memory overheads. See the\nSpark Tuning Guide\nfor more details. For Kryo, consider registering custom", "question": "What StorageLevel is used by default for persisted RDDs generated by streaming computations?", "answers": {"text": ["StorageLevel.MEMORY_ONLY_SER"], "answer_start": [552]}}
{"context": " using Kryo serialization can reduce both CPU and memory overheads. See the\nSpark Tuning Guide\nfor more details. For Kryo, consider registering custom classes, and disabling object reference tracking (see Kryo-related configurations in the\nConfiguration Guide\n).\nIn specific cases where the amount of data that needs to be retained for the streaming application is not large, it may be feasible to persist data (both types) as deserialized objects without incurring excessive GC overheads. For example, if you are using batch intervals of a few seconds and no window operations, then you can try disabling serialization in persisted data by explicitly setting the storage level accordingly. This would reduce the CPU overheads due to serialization, potentially improving performance without too much ", "question": "What can be done to potentially improve performance when persisting data in a streaming application with small data volumes?", "answers": {"text": ["you can try disabling serialization in persisted data by explicitly setting the storage level accordingly."], "answer_start": [584]}}
{"context": "elay due to\ntemporary data rate increases may be fine as long as the delay reduces back to a low value\n(i.e., less than batch size).\nMemory Tuning\nTuning the memory usage and GC behavior of Spark applications has been discussed in great detail\nin the\nTuning Guide\n. It is strongly recommended that you read that. In this section, we discuss a few tuning parameters specifically in the context of Spark Streaming applications.\nThe amount of cluster memory required by a Spark Streaming application depends heavily on the type of transformations used. For example, if you want to use a window operation on the last 10 minutes of data, then your cluster should have sufficient memory to hold 10 minutes worth of data in memory. Or if you want to use\nupdateStateByKey\nwith a large number of keys, then th", "question": "What should your cluster have sufficient memory to hold if you want to use a window operation on the last 10 minutes of data?", "answers": {"text": ["your cluster should have sufficient memory to hold 10 minutes worth of data in memory."], "answer_start": [638]}}
{"context": " should have sufficient memory to hold 10 minutes worth of data in memory. Or if you want to use\nupdateStateByKey\nwith a large number of keys, then the necessary memory  will be high. On the contrary, if you want to do a simple map-filter-store operation, then the necessary memory will be low.\nIn general, since the data received through receivers is stored with StorageLevel.MEMORY_AND_DISK_SER_2, the data that does not fit in memory will spill over to the disk. This may reduce the performance of the streaming application, and hence it is advised to provide sufficient memory as required by your streaming application. Its best to try and see the memory usage on a small scale and estimate accordingly.\nAnother aspect of memory tuning is garbage collection. For a streaming application that requ", "question": "What happens to data that doesn't fit in memory when using receivers?", "answers": {"text": ["the data that does not fit in memory will spill over to the disk."], "answer_start": [400]}}
{"context": "e memory usage on a small scale and estimate accordingly.\nAnother aspect of memory tuning is garbage collection. For a streaming application that requires low latency, it is undesirable to have large pauses caused by JVM Garbage Collection.\nThere are a few parameters that can help you tune the memory usage and GC overheads:\nPersistence Level of DStreams\n: As mentioned earlier in the\nData Serialization\nsection, the input data and RDDs are by default persisted as serialized bytes. This reduces both the memory usage and GC overheads, compared to deserialized persistence. Enabling Kryo serialization further reduces serialized sizes and memory usage. Further reduction in memory usage can be achieved with compression (see the Spark configuration\nspark.rdd.compress\n), at the cost of CPU time.\nCle", "question": "What can further reduce serialized sizes and memory usage?", "answers": {"text": ["Enabling Kryo serialization further reduces serialized sizes and memory usage."], "answer_start": [575]}}
{"context": "ge. Further reduction in memory usage can be achieved with compression (see the Spark configuration\nspark.rdd.compress\n), at the cost of CPU time.\nClearing old data\n: By default, all input data and persisted RDDs generated by DStream transformations are automatically cleared. Spark Streaming decides when to clear the data based on the transformations that are used. For example, if you are using a window operation of 10 minutes, then Spark Streaming will keep around the last 10 minutes of data, and actively throw away older data.\nData can be retained for a longer duration (e.g. interactively querying older data) by setting\nstreamingContext.remember\n.\nOther tips\n: To further reduce GC overheads, here are some more tips to try.\nPersist RDDs using the\nOFF_HEAP\nstorage level. See more detail in", "question": "How does Spark Streaming decide when to clear data?", "answers": {"text": ["Spark Streaming decides when to clear the data based on the transformations that are used."], "answer_start": [277]}}
{"context": "account. The receivers are allocated to executors in a round robin fashion.\nWhen data is received from a stream source, the receiver creates blocks of data.  A new block of data is generated every blockInterval milliseconds. N blocks of data are created during the batchInterval where N = batchInterval/blockInterval. These blocks are distributed by the BlockManager of the current executor to the block managers of other executors. After that, the Network Input Tracker running on the driver is informed about the block locations for further processing.\nAn RDD is created on the driver for the blocks created during the batchInterval. The blocks generated during the batchInterval are partitions of the RDD. Each partition is a task in spark. blockInterval== batchinterval would mean that a single p", "question": "How are receivers allocated to executors?", "answers": {"text": ["The receivers are allocated to executors in a round robin fashion."], "answer_start": [9]}}
{"context": "erated during the batchInterval are partitions of the RDD. Each partition is a task in spark. blockInterval== batchinterval would mean that a single partition is created and probably it is processed locally.\nThe map tasks on the blocks are processed in the executors (one that received the block, and another where the block was replicated) that has the blocks irrespective of block interval, unless non-local scheduling kicks in.\nHaving a bigger blockinterval means bigger blocks. A high value of\nspark.locality.wait\nincreases the chance of processing a block on the local node. A balance needs to be found out between these two parameters to ensure that the bigger blocks are processed locally.\nInstead of relying on batchInterval and blockInterval, you can define the number of partitions by calli", "question": "What does it mean if blockInterval is equal to batchInterval?", "answers": {"text": ["blockInterval== batchinterval would mean that a single partition is created and probably it is processed locally."], "answer_start": [94]}}
{"context": "his, you can union two dstreams. This will ensure that a single unionRDD is formed for the two RDDs of the dstreams. This unionRDD is then considered as a single job. However, the partitioning of the RDDs is not impacted.\nIf the batch processing time is more than batchinterval then obviously the receiver’s memory will start filling up and will end up in throwing exceptions (most probably BlockNotFoundException). Currently, there is  no way to pause the receiver. Using SparkConf configuration\nspark.streaming.receiver.maxRate\n, rate of receiver can be limited.\nFault-tolerance Semantics\nIn this section, we will discuss the behavior of Spark Streaming applications in the event\nof failures.\nBackground\nTo understand the semantics provided by Spark Streaming, let us remember the basic fault-toler", "question": "What configuration can be used to limit the rate of a receiver in Spark Streaming?", "answers": {"text": ["spark.streaming.receiver.maxRate"], "answer_start": [497]}}
{"context": "times. This is stronger than\nat-most once\nas it ensures that no data will be lost. But there may be duplicates.\nExactly once\n: Each record will be processed exactly once - no data will be lost and no data will be processed multiple times. This is obviously the strongest guarantee of the three.\nBasic Semantics\nIn any stream processing system, broadly speaking, there are three steps in processing the data.\nReceiving the data\n: The data is received from sources using Receivers or otherwise.\nTransforming the data\n: The received data is transformed using DStream and RDD transformations.\nPushing out the data\n: The final transformed data is pushed out to external systems like file systems, databases, dashboards, etc.\nIf a streaming application has to achieve end-to-end exactly-once guarantees, th", "question": "What is the strongest guarantee among the three processing guarantees?", "answers": {"text": ["This is obviously the strongest guarantee of the three."], "answer_start": [239]}}
{"context": "\nexactly once\n. Read for more details.\nWith Files\nIf all of the input data is already present in a fault-tolerant file system like\nHDFS, Spark Streaming can always recover from any failure and process all of the data. This gives\nexactly-once\nsemantics, meaning all of the data will be processed exactly once no matter what fails.\nWith Receiver-based Sources\nFor input sources based on receivers, the fault-tolerance semantics depend on both the failure\nscenario and the type of receiver.\nAs we discussed\nearlier\n, there are two types of receivers:\nReliable Receiver\n- These receivers acknowledge reliable sources only after ensuring that\n  the received data has been replicated. If such a receiver fails, the source will not receive\n  acknowledgment for the buffered (unreplicated) data. Therefore, i", "question": "What does exactly-once semantics mean in the context of Spark Streaming?", "answers": {"text": ["meaning all of the data will be processed exactly once no matter what fails."], "answer_start": [253]}}
{"context": "e fails,\nthen besides these losses, all of the past data that was received and replicated in memory will be\nlost. This will affect the results of the stateful transformations.\nTo avoid this loss of past received data, Spark 1.2 introduced\nwrite\nahead logs\nwhich save the received data to fault-tolerant storage. With the\nwrite-ahead logs\nenabled\nand reliable receivers, there is zero data loss. In terms of semantics, it provides an at-least once guarantee.\nThe following table summarizes the semantics under failures:\nDeployment Scenario\nWorker Failure\nDriver Failure\nSpark 1.1 or earlier,\nOR\nSpark 1.2 or later without write-ahead logs\nBuffered data lost with unreliable receivers\nZero data loss with reliable receivers\nAt-least once semantics\nBuffered data lost with unreliable receivers\nPast data", "question": "What happens with buffered data in Spark 1.1 or earlier, or Spark 1.2 or later without write-ahead logs, in case of a worker failure?", "answers": {"text": ["Buffered data lost with unreliable receivers"], "answer_start": [638]}}
{"context": "a lost with unreliable receivers\nZero data loss with reliable receivers\nAt-least once semantics\nBuffered data lost with unreliable receivers\nPast data lost with all receivers\nUndefined semantics\nSpark 1.2 or later with write-ahead logs\nZero data loss with reliable receivers\nAt-least once semantics\nZero data loss with reliable receivers and files\nAt-least once semantics\nWith Kafka Direct API\nIn Spark 1.3, we have introduced a new Kafka Direct API, which can ensure that all the Kafka data is received by Spark Streaming exactly once. Along with this, if you implement exactly-once output operation, you can achieve end-to-end exactly-once guarantees. This approach is further discussed in the\nKafka Integration Guide\n.\nSemantics of output operations\nOutput operations (like\nforeachRDD\n) have\nat-le", "question": "In Spark 1.3, what new API was introduced to ensure Kafka data is received by Spark Streaming exactly once?", "answers": {"text": ["In Spark 1.3, we have introduced a new Kafka Direct API, which can ensure that all the Kafka data is received by Spark Streaming exactly once."], "answer_start": [394]}}
{"context": "es. This approach is further discussed in the\nKafka Integration Guide\n.\nSemantics of output operations\nOutput operations (like\nforeachRDD\n) have\nat-least once\nsemantics, that is,\nthe transformed data may get written to an external entity more than once in\nthe event of a worker failure. While this is acceptable for saving to file systems using the\nsaveAs***Files\noperations (as the file will simply get overwritten with the same data),\nadditional effort may be necessary to achieve exactly-once semantics. There are two approaches.\nIdempotent updates\n: Multiple attempts always write the same data. For example,\nsaveAs***Files\nalways writes the same data to the generated files.\nTransactional updates\n: All updates are made transactionally so that updates are made exactly once atomically. One way t", "question": "What is the semantic guarantee provided by output operations like foreachRDD?", "answers": {"text": ["at-least once"], "answer_start": [145]}}
{"context": "m.foreachRDD { (rdd, time) =>\n  rdd.foreachPartition { partitionIterator =>\n    val partitionId = TaskContext.get.partitionId()\n    val uniqueId = generateUniqueId(time.milliseconds, partitionId)\n    // use this uniqueId to transactionally commit the data in partitionIterator\n  }\n}\nWhere to Go from Here\nAdditional guides\nKafka Integration Guide\nKinesis Integration Guide\nCustom Receiver Guide\nThird-party DStream data sources can be found in\nThird Party Projects\nAPI documentation\nPython docs\nStreamingContext\nand\nDStream\nScala docs\nStreamingContext\nand\nDStream\nKafkaUtils\n,\nKinesisUtils\nJava docs\nJavaStreamingContext\n,\nJavaDStream\nand\nJavaPairDStream\nKafkaUtils\n,\nKinesisUtils\nMore examples in\nPython\nand\nScala\nand\nJava\nPaper\nand\nvideo\ndescribing Spark Streaming.", "question": "What is used to transactionally commit the data in partitionIterator?", "answers": {"text": ["use this uniqueId to transactionally commit the data in partitionIterator"], "answer_start": [203]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nPerformance Tuning\nDistributed SQL Engine\nRunning the Thrift JDBC/ODBC server\nRunning the Spark SQL CLI\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nSpark ", "question": "Which programming languages have API documentation available for Spark?", "answers": {"text": ["Python\nScala\nJava\nR"], "answer_start": [266]}}
{"context": "rift JDBC/ODBC server\nRunning the Spark SQL CLI\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nSpark SQL CLI\nSpark SQL Command Line Options\nThe hiverc File\nPath interpretation\nSupported comment types\nSpark SQL CLI Interactive Shell Commands\nExamples\nThe Spark SQL CLI is a convenient interactive command tool to run the Hive metastore service and execute SQL\nqueries input from the command line. Note that the Spark SQL CLI cannot talk to the Thrift JDBC server.\nTo start the Spark SQL CLI, run the following in the Spark directory:\n./bin/spark-sql\nConfiguration of Hive is done by placing your\nhive-site.xml\n,\ncore-site.xml\nand\nhdfs-site.xml\nfiles in\nconf/\n.\nSpark SQL Command Line Options\nYou may run\n./bin/spark-sql --help\nfor a complete list of al", "question": "How do you start the Spark SQL CLI?", "answers": {"text": ["./bin/spark-sql"], "answer_start": [582]}}
{"context": "ite.xml\n,\ncore-site.xml\nand\nhdfs-site.xml\nfiles in\nconf/\n.\nSpark SQL Command Line Options\nYou may run\n./bin/spark-sql --help\nfor a complete list of all available options.\nCLI options:\n -d,--define <key=value>          Variable substitution to apply to Hive\n                                  commands. e.g. -d A=B or --define A=B\n    --database <databasename>     Specify the database to use\n -e <quoted-query-string>         SQL from command line\n -f <filename>                    SQL from files\n -H,--help                        Print help information\n    --hiveconf <property=value>   Use value for given property\n    --hivevar <key=value>         Variable substitution to apply to Hive\n                                  commands. e.g. --hivevar A=B\n -i <filename>                    Initialization", "question": "How can you get a complete list of all available Spark SQL command line options?", "answers": {"text": ["./bin/spark-sql --help"], "answer_start": [102]}}
{"context": "Variable substitution to apply to Hive\n                                  commands. e.g. --hivevar A=B\n -i <filename>                    Initialization SQL file\n -S,--silent                      Silent mode in interactive shell\n -v,--verbose                     Verbose mode (echo executed SQL to the\n                                  console)\nThe hiverc File\nWhen invoked without the\n-i\n, the Spark SQL CLI will attempt to load\n$HIVE_HOME/bin/.hiverc\nand\n$HOME/.hiverc\nas initialization files.\nPath interpretation\nSpark SQL CLI supports running SQL from initialization script file(\n-i\n) or normal SQL file(\n-f\n), If path url don’t have a scheme component, the path will be handled as local file.\nFor example:\n/path/to/spark-sql-cli.sql\nequals to\nfile:///path/to/spark-sql-cli.sql\n. User also can use ", "question": "What files does the Spark SQL CLI attempt to load as initialization files when invoked without the -i option?", "answers": {"text": ["$HIVE_HOME/bin/.hiverc\nand\n$HOME/.hiverc"], "answer_start": [428]}}
{"context": "\n/* This is a comment contains ;\n*/\nSELECT\n1\n;\nHowever, if ‘;’ is the end of the line, it terminates the SQL statement. The example above will be terminated into\n/* This is a comment contains\nand\n*/ SELECT 1\n, Spark will submit these two commands separated and throw parser error (\nunclosed bracketed comment\nand\nSyntax error at or near '*/'\n).\nCommand\nDescription\nquit\nor\nexit\nExits the interactive shell.\n!<command>\nExecutes a shell command from the Spark SQL CLI shell.\ndfs <HDFS dfs command>\nExecutes a HDFS\ndfs command\nfrom the Spark SQL CLI shell.\n<query string>\nExecutes a Spark SQL query and prints results to standard output.\nsource <filepath>\nExecutes a script file inside the CLI.\nExamples\nExample of running a query from the command line:\n./bin/spark-sql -e 'SELECT COL FROM TBL'\nExample ", "question": "What happens if ‘;’ is the end of the line in a Spark SQL statement?", "answers": {"text": ["However, if ‘;’ is the end of the line, it terminates the SQL statement."], "answer_start": [47]}}
{"context": "h>\nExecutes a script file inside the CLI.\nExamples\nExample of running a query from the command line:\n./bin/spark-sql -e 'SELECT COL FROM TBL'\nExample of setting Hive configuration variables:\n./bin/spark-sql -e 'SELECT COL FROM TBL' --hiveconf hive.exec.scratchdir=/home/my/hive_scratch\nExample of setting Hive configuration variables and using it in the SQL query:\n./bin/spark-sql -e 'SELECT ${hiveconf:aaa}' --hiveconf aaa=bbb --hiveconf hive.exec.scratchdir=/home/my/hive_scratch\nspark-sql> SELECT ${aaa};\nbbb\nExample of setting Hive variables substitution:\n./bin/spark-sql --hivevar aaa=bbb --define ccc=ddd\nspark-sql> SELECT ${aaa}, ${ccc};\nbbb ddd\nExample of dumping data out from a query into a file using silent mode:\n./bin/spark-sql -S -e 'SELECT COL FROM TBL' > result.txt\nExample of running", "question": "How can you run a query from the command line using spark-sql?", "answers": {"text": ["./bin/spark-sql -e 'SELECT COL FROM TBL'"], "answer_start": [101]}}
{"context": "dd\nExample of dumping data out from a query into a file using silent mode:\n./bin/spark-sql -S -e 'SELECT COL FROM TBL' > result.txt\nExample of running a script non-interactively:\n./bin/spark-sql -f /path/to/spark-sql-script.sql\nExample of running an initialization script before entering interactive mode:\n./bin/spark-sql -i /path/to/spark-sql-init.sql\nExample of entering interactive mode:\n./bin/spark-sql\nspark-sql> SELECT 1;\n1\nspark-sql> -- This is a simple comment.\nspark-sql> SELECT 1;\n1\nExample of entering interactive mode with escape\n;\nin comment:\n./bin/spark-sql\nspark-sql>/* This is a comment contains \\\\;\n         > It won't be terminated by \\\\; */\n         > SELECT 1;\n1", "question": "How can you run a script non-interactively?", "answers": {"text": ["./bin/spark-sql -f /path/to/spark-sql-script.sql"], "answer_start": [179]}}
{"context": "by \\\\; */\n         > SELECT 1;\n1", "question": "Qual comando SQL é executado no exemplo fornecido?", "answers": {"text": ["SELECT 1;"], "answer_start": [21]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nRunning Spark on YARN\nSecurity\nLaunching Spark on YARN\nAdding Other JARs\nPreparations\nConfiguration\nDebugging your Application\nSpark Properties\nAvailable patterns for SHS custom executor log URL\nResource Allocation and Configuration Overview\nStage Lev", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars"], "answer_start": [46]}}
{"context": "tures like authentication are not enabled by default. When deploying a cluster that is open to the internet\nor an untrusted network, it’s important to secure access to the cluster to prevent unauthorized applications\nfrom running on the cluster.\nPlease see\nSpark Security\nand the specific security sections in this doc before running Spark.\nLaunching Spark on YARN\nApache Hadoop does not support Java 17 as of 3.4.1, while Apache Spark requires at least Java 17 since 4.0.0, so a different JDK should be configured for Spark applications.\nPlease refer to\nConfiguring different JDKs for Spark Applications\nfor details.\nEnsure that\nHADOOP_CONF_DIR\nor\nYARN_CONF_DIR\npoints to the directory which contains the (client side) configuration files for the Hadoop cluster.\nThese configs are used to write to H", "question": "What is required for Spark applications regarding Java versions, given Hadoop's limitations?", "answers": {"text": ["Apache Hadoop does not support Java 17 as of 3.4.1, while Apache Spark requires at least Java 17 since 4.0.0, so a different JDK should be configured for Spark applications."], "answer_start": [365]}}
{"context": "ARN_CONF_DIR\npoints to the directory which contains the (client side) configuration files for the Hadoop cluster.\nThese configs are used to write to HDFS and connect to the YARN ResourceManager. The\nconfiguration contained in this directory will be distributed to the YARN cluster so that all\ncontainers used by the application use the same configuration. If the configuration references\nJava system properties or environment variables not managed by YARN, they should also be set in the\nSpark application’s configuration (driver, executors, and the AM when running in client mode).\nThere are two deploy modes that can be used to launch Spark applications on YARN. In\ncluster\nmode, the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can", "question": "What does ARN_CONF_DIR point to?", "answers": {"text": ["the directory which contains the (client side) configuration files for the Hadoop cluster."], "answer_start": [23]}}
{"context": "tions on YARN. In\ncluster\nmode, the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can go away after initiating the application. In\nclient\nmode, the driver runs in the client process, and the application master is only used for requesting resources from YARN.\nUnlike other cluster managers supported by Spark in which the master’s address is specified in the\n--master\nparameter, in YARN mode the ResourceManager’s address is picked up from the Hadoop configuration.\nThus, the\n--master\nparameter is\nyarn\n.\nTo launch a Spark application in\ncluster\nmode:\n$ ./bin/spark-submit --class path.to.your.Class --master yarn --deploy-mode cluster [options] <app jar> [app options]\nFor example:\n$ ./bin/spark-submit --class org.apache.spark.example", "question": "How is the Spark driver launched in cluster mode when using YARN?", "answers": {"text": ["the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can go away after initiating the application."], "answer_start": [32]}}
{"context": "to.your.Class --master yarn --deploy-mode cluster [options] <app jar> [app options]\nFor example:\n$ ./bin/spark-submit --class org.apache.spark.examples.SparkPi \\\n    --master yarn \\\n    --deploy-mode cluster \\\n    --driver-memory 4g \\\n    --executor-memory 2g \\\n    --executor-cores 1 \\\n    --queue thequeue \\\n    examples/jars/spark-examples*.jar \\\n    10\nThe above starts a YARN client program which starts the default Application Master. Then SparkPi will be run as a child thread of Application Master. The client will periodically poll the Application Master for status updates and display them in the console. The client will exit once your application has finished running.  Refer to the\nDebugging your Application\nsection below for how to see driver and executor logs.\nTo launch a Spark appli", "question": "What happens after the YARN client program starts the default Application Master?", "answers": {"text": ["Then SparkPi will be run as a child thread of Application Master."], "answer_start": [441]}}
{"context": "rs\nand upload it to the distributed cache.\nConfiguration\nMost of the configs are the same for Spark on YARN as for other deployment modes. See the\nconfiguration page\nfor more information on those.  These are configs that are specific to Spark on YARN.\nDebugging your Application\nIn YARN terminology, executors and application masters run inside “containers”. YARN has two modes for handling container logs after an application has completed. If log aggregation is turned on (with the\nyarn.log-aggregation-enable\nconfig), container logs are copied to HDFS and deleted on the local machine. These logs can be viewed from anywhere on the cluster with the\nyarn logs\ncommand.\nyarn logs -applicationId <app ID>\nwill print out the contents of all log files from all containers from the given application. Yo", "question": "How can container logs be viewed after an application has completed in YARN?", "answers": {"text": ["These logs can be viewed from anywhere on the cluster with the\nyarn logs\ncommand."], "answer_start": [589]}}
{"context": "here are the options:\nupload a custom\nlog4j2.properties\nusing\nspark-submit\n, by adding it to the\n--files\nlist of files\nto be uploaded with the application.\nadd\n-Dlog4j.configurationFile=<location of configuration file>\nto\nspark.driver.extraJavaOptions\n(for the driver) or\nspark.executor.extraJavaOptions\n(for executors). Note that if using a file,\nthe\nfile:\nprotocol should be explicitly provided, and the file needs to exist locally on all\nthe nodes.\nupdate the\n$SPARK_CONF_DIR/log4j2.properties\nfile and it will be automatically uploaded along\nwith the other configurations. Note that other 2 options has higher priority than this option if\nmultiple options are specified.\nNote that for the first option, both executors and the application master will share the same\nlog4j configuration, which may ", "question": "How can you upload a custom log4j2.properties file using spark-submit?", "answers": {"text": ["upload a custom\nlog4j2.properties\nusing\nspark-submit\n, by adding it to the\n--files\nlist of files\nto be uploaded with the application."], "answer_start": [22]}}
{"context": "e options are specified.\nNote that for the first option, both executors and the application master will share the same\nlog4j configuration, which may cause issues when they run on the same node (e.g. trying to write\nto the same log file).\nIf you need a reference to the proper location to put log files in the YARN so that YARN can properly display and aggregate them, use\nspark.yarn.app.container.log.dir\nin your\nlog4j2.properties\n. For example,\nappender.file_appender.fileName=${sys:spark.yarn.app.container.log.dir}/spark.log\n. For streaming applications, configuring\nRollingFileAppender\nand setting file location to YARN’s log directory will avoid disk overflow caused by large log files, and logs can be accessed using YARN’s log utility.\nTo use a custom metrics.properties for the application m", "question": "What property can be used to specify the proper location for log files in YARN?", "answers": {"text": ["spark.yarn.app.container.log.dir"], "answer_start": [373]}}
{"context": "d disk overflow caused by large log files, and logs can be accessed using YARN’s log utility.\nTo use a custom metrics.properties for the application master and executors, update the\n$SPARK_CONF_DIR/metrics.properties\nfile. It will automatically be uploaded with other configurations, so you don’t need to specify it manually with\n--files\n.\nSpark Properties\nProperty Name\nDefault\nMeaning\nSince Version\nspark.yarn.am.memory\n512m\nAmount of memory to use for the YARN Application Master in client mode, in the same format as JVM memory strings (e.g.\n512m\n,\n2g\n).\n    In cluster mode, use\nspark.driver.memory\ninstead.\nUse lower-case suffixes, e.g.\nk\n,\nm\n,\ng\n,\nt\n, and\np\n, for kibi-, mebi-, gibi-, tebi-, and pebibytes, respectively.\n1.3.0\nspark.yarn.am.resource.{resource-type}.amount\n(none)\nAmount of res", "question": "What is the default amount of memory to use for the YARN Application Master in client mode?", "answers": {"text": ["512m"], "answer_start": [422]}}
{"context": "\ng\n,\nt\n, and\np\n, for kibi-, mebi-, gibi-, tebi-, and pebibytes, respectively.\n1.3.0\nspark.yarn.am.resource.{resource-type}.amount\n(none)\nAmount of resource to use for the YARN Application Master in client mode.\n    In cluster mode, use\nspark.yarn.driver.resource.<resource-type>.amount\ninstead.\n    Please note that this feature can be used only with YARN 3.0+\n    For reference, see\nYARN Resource Model documentation\nExample:\n    To request GPU resources from YARN, use:\nspark.yarn.am.resource.yarn.io/gpu.amount\n3.0.0\nspark.yarn.applicationType\nSPARK\nDefines more specific application types, e.g.\nSPARK\n,\nSPARK-SQL\n,\nSPARK-STREAMING\n,\nSPARK-MLLIB\nand\nSPARK-GRAPH\n. Please be careful not to exceed 20 characters.\n3.1.0\nspark.yarn.driver.resource.{resource-type}.amount\n(none)\nAmount of resource to u", "question": "What should be used instead of spark.yarn.am.resource.{resource-type}.amount in cluster mode?", "answers": {"text": ["spark.yarn.driver.resource.<resource-type>.amount"], "answer_start": [236]}}
{"context": "nd\nSPARK-GRAPH\n. Please be careful not to exceed 20 characters.\n3.1.0\nspark.yarn.driver.resource.{resource-type}.amount\n(none)\nAmount of resource to use for the YARN Application Master in cluster mode.\n    Please note that this feature can be used only with YARN 3.0+\n    For reference, see\nYARN Resource Model documentation\nExample:\n    To request GPU resources from YARN, use:\nspark.yarn.driver.resource.yarn.io/gpu.amount\n3.0.0\nspark.yarn.executor.resource.{resource-type}.amount\n(none)\nAmount of resource to use per executor process.\n    Please note that this feature can be used only with YARN 3.0+\n    For reference, see\nYARN Resource Model documentation\nExample:\n    To request GPU resources from YARN, use:\nspark.yarn.executor.resource.yarn.io/gpu.amount\n3.0.0\nspark.yarn.resourceGpuDeviceNam", "question": "What is the purpose of spark.yarn.driver.resource.{resource-type}.amount?", "answers": {"text": ["Amount of resource to use for the YARN Application Master in cluster mode."], "answer_start": [127]}}
{"context": "umentation\nExample:\n    To request GPU resources from YARN, use:\nspark.yarn.executor.resource.yarn.io/gpu.amount\n3.0.0\nspark.yarn.resourceGpuDeviceName\nyarn.io/gpu\nSpecify the mapping of the Spark resource type of\ngpu\nto the YARN resource\n    representing a GPU. By default YARN uses\nyarn.io/gpu\nbut if YARN has been\n    configured with a custom resource type, this allows remapping it.\n    Applies when using the\nspark.{driver/executor}.resource.gpu.*\nconfigs.\n3.2.1\nspark.yarn.resourceFpgaDeviceName\nyarn.io/fpga\nSpecify the mapping of the Spark resource type of\nfpga\nto the YARN resource\n    representing a FPGA. By default YARN uses\nyarn.io/fpga\nbut if YARN has been\n    configured with a custom resource type, this allows remapping it.\n    Applies when using the\nspark.{driver/executor}.resource", "question": "What is the default YARN resource type used for GPUs?", "answers": {"text": ["yarn.io/gpu"], "answer_start": [94]}}
{"context": "but if YARN has been\n    configured with a custom resource type, this allows remapping it.\n    Applies when using the\nspark.{driver/executor}.resource.fpga.*\nconfigs.\n3.2.1\nspark.yarn.am.cores\n1\nNumber of cores to use for the YARN Application Master in client mode.\n    In cluster mode, use\nspark.driver.cores\ninstead.\n1.3.0\nspark.yarn.am.waitTime\n100s\nOnly used in\ncluster\nmode. Time for the YARN Application Master to wait for the\n    SparkContext to be initialized.\n1.3.0\nspark.yarn.submit.file.replication\nThe default HDFS replication (usually\n3\n)\nHDFS replication level for the files uploaded into HDFS for the application. These include things like the Spark jar, the app jar, and any distributed cache files/archives.\n0.8.1\nspark.yarn.stagingDir\nCurrent user's home directory in the filesystem", "question": "What is the default HDFS replication level for files uploaded into HDFS for the application?", "answers": {"text": ["3"], "answer_start": [167]}}
{"context": "like the Spark jar, the app jar, and any distributed cache files/archives.\n0.8.1\nspark.yarn.stagingDir\nCurrent user's home directory in the filesystem\nStaging directory used while submitting applications.\n2.0.0\nspark.yarn.preserve.staging.files\nfalse\nSet to\ntrue\nto preserve the staged files (Spark jar, app jar, distributed cache files) at the end of the job rather than delete them.\n1.1.0\nspark.yarn.scheduler.heartbeat.interval-ms\n3000\nThe interval in ms in which the Spark application master heartbeats into the YARN ResourceManager.\n    The value is capped at half the value of YARN's configuration for the expiry interval, i.e.\nyarn.am.liveness-monitor.expiry-interval-ms\n.\n0.8.1\nspark.yarn.scheduler.initial-allocation.interval\n200ms\nThe initial interval in which the Spark application master ", "question": "What does setting spark.yarn.preserve.staging.files to true do?", "answers": {"text": ["to preserve the staged files (Spark jar, app jar, distributed cache files) at the end of the job rather than delete them."], "answer_start": [263]}}
{"context": "p://\n). Defaults to not being set since the history server is an optional service. This address is given to the YARN ResourceManager when the Spark application finishes to link the application from the ResourceManager UI to the Spark history server UI.\n    For this property, YARN properties can be used as variables, and these are substituted by Spark at runtime. For example, if the Spark history server runs on the same node as the YARN ResourceManager, it can be set to\n${hadoopconf-yarn.resourcemanager.hostname}:18080\n.\n1.0.0\nspark.yarn.dist.archives\n(none)\nComma separated list of archives to be extracted into the working directory of each executor.\n1.0.0\nspark.yarn.dist.files\n(none)\nComma-separated list of files to be placed in the working directory of each executor.\n1.0.0\nspark.yarn.dist", "question": "What is the default value for spark.yarn.history.server.address?", "answers": {"text": ["Defaults to not being set since the history server is an optional service."], "answer_start": [8]}}
{"context": "ted to download resources for all the schemes.\n2.3.0\nspark.executor.instances\n2\nThe number of executors for static allocation. With\nspark.dynamicAllocation.enabled\n, the initial set of executors will be at least this large.\n1.0.0\nspark.yarn.am.memoryOverhead\nAM memory * 0.10, with minimum of 384\nSame as\nspark.driver.memoryOverhead\n, but for the YARN Application Master in client mode.\n1.3.0\nspark.yarn.queue\ndefault\nThe name of the YARN queue to which the application is submitted.\n1.0.0\nspark.yarn.jars\n(none)\nList of libraries containing Spark code to distribute to YARN containers.\n    By default, Spark on YARN will use Spark jars installed locally, but the Spark jars can also be\n    in a world-readable location on HDFS. This allows YARN to cache it on nodes so that it doesn't\n    need to be", "question": "What is the default value for spark.yarn.queue?", "answers": {"text": ["default"], "answer_start": [410]}}
{"context": "traJavaOptions\n(none)\nA string of extra JVM options to pass to the YARN Application Master in client mode.\n  In cluster mode, use\nspark.driver.extraJavaOptions\ninstead. Note that it is illegal\n  to set maximum heap size (-Xmx) settings with this option. Maximum heap size settings can be set\n  with\nspark.yarn.am.memory\n1.3.0\nspark.yarn.am.extraLibraryPath\n(none)\nSet a special library path to use when launching the YARN Application Master in client mode.\n1.4.0\nspark.yarn.populateHadoopClasspath\nFor\nwith-hadoop\nSpark distribution, this is set to false;\n    for\nno-hadoop\ndistribution, this is set to true.\nWhether to populate Hadoop classpath from\nyarn.application.classpath\nand\nmapreduce.application.classpath\nNote that if this is set to\nfalse\n,\n    it requires a\nwith-Hadoop\nSpark distribution t", "question": "What should be used instead of traJavaOptions in cluster mode?", "answers": {"text": ["spark.driver.extraJavaOptions"], "answer_start": [130]}}
{"context": "\nyarn.application.classpath\nand\nmapreduce.application.classpath\nNote that if this is set to\nfalse\n,\n    it requires a\nwith-Hadoop\nSpark distribution that bundles Hadoop runtime or\n    user has to provide a Hadoop installation separately.\n2.4.6\nspark.yarn.maxAppAttempts\nyarn.resourcemanager.am.max-attempts\nin YARN\nThe maximum number of attempts that will be made to submit the application.\n  It should be no larger than the global number of max attempts in the YARN configuration.\n1.3.0\nspark.yarn.am.attemptFailuresValidityInterval\n(none)\nDefines the validity interval for AM failure tracking.\n  If the AM has been running for at least the defined interval, the AM failure count will be reset.\n  This feature is not enabled if not configured.\n1.6.0\nspark.yarn.am.clientModeTreatDisconnectAsFailed\nf", "question": "What happens if spark.yarn.am.attemptFailuresValidityInterval is not configured?", "answers": {"text": ["This feature is not enabled if not configured."], "answer_start": [698]}}
{"context": "s of FAILED. This will allow the caller to decide if it was truly a failure. Note that if\n  this config is set and the user just terminate the client application badly it may show a status of FAILED when it wasn't really FAILED.\n3.3.0\nspark.yarn.am.clientModeExitOnError\nfalse\nIn yarn-client mode, when this is true, if driver got application report with final status of KILLED or FAILED,\n  driver will stop corresponding SparkContext and exit program with code 1.\n  Note, if this is true and called from another application, it will terminate the parent application as well.\n3.3.0\nspark.yarn.am.tokenConfRegex\n(none)\nThe value of this config is a regex expression used to grep a list of config entries from the job's configuration file (e.g., hdfs-site.xml)\n    and send to RM, which uses them when ", "question": "What happens in yarn-client mode when spark.yarn.am.clientModeExitOnError is true and the driver receives a final status of KILLED or FAILED?", "answers": {"text": ["driver will stop corresponding SparkContext and exit program with code 1."], "answer_start": [391]}}
{"context": " configs from the job's local configuration files. This config is very similar to\nmapreduce.job.send-token-conf\n. Please check YARN-5910 for more details.\n3.3.0\nspark.yarn.submit.waitAppCompletion\ntrue\nIn YARN cluster mode, controls whether the client waits to exit until the application completes.\n  If set to\ntrue\n, the client process will stay alive reporting the application's status.\n  Otherwise, the client process will exit after submission.\n1.4.0\nspark.yarn.am.nodeLabelExpression\n(none)\nA YARN node label expression that restricts the set of nodes AM will be scheduled on.\n  Only versions of YARN greater than or equal to 2.6 support node label expressions, so when\n  running against earlier versions, this property will be ignored.\n1.6.0\nspark.yarn.executor.nodeLabelExpression\n(none)\nA YAR", "question": "What does spark.yarn.submit.waitAppCompletion control in YARN cluster mode?", "answers": {"text": ["In YARN cluster mode, controls whether the client waits to exit until the application completes."], "answer_start": [202]}}
{"context": "bel expressions, so when\n  running against earlier versions, this property will be ignored.\n1.6.0\nspark.yarn.executor.nodeLabelExpression\n(none)\nA YARN node label expression that restricts the set of nodes executors will be scheduled on.\n  Only versions of YARN greater than or equal to 2.6 support node label expressions, so when\n  running against earlier versions, this property will be ignored.\n1.4.0\nspark.yarn.tags\n(none)\nComma-separated list of strings to pass through as YARN application tags appearing\n  in YARN ApplicationReports, which can be used for filtering when querying YARN apps.\n1.5.0\nspark.yarn.priority\n(none)\nApplication priority for YARN to define pending applications ordering policy, those with higher\n  integer value have a better opportunity to be activated. Currently, YARN", "question": "What does the property spark.yarn.executor.nodeLabelExpression do?", "answers": {"text": ["A YARN node label expression that restricts the set of nodes executors will be scheduled on."], "answer_start": [145]}}
{"context": "reference to some environment variable exported by\n  YARN (and, thus, visible to Spark containers).\nFor example, if the gateway node has Hadoop libraries installed on\n/disk1/hadoop\n, and\n  the location of the Hadoop install is exported by YARN as the\nHADOOP_HOME\nenvironment variable, setting this value to\n/disk1/hadoop\nand the replacement path to\n$HADOOP_HOME\nwill make sure that paths used to launch remote processes properly\n  reference the local YARN configuration.\n1.5.0\nspark.yarn.config.replacementPath\n(none)\nSee\nspark.yarn.config.gatewayPath\n.\n1.5.0\nspark.yarn.rolledLog.includePattern\n(none)\nJava Regex to filter the log files which match the defined include pattern\n  and those log files will be aggregated in a rolling fashion.\n  This will be used with YARN's rolling log aggregation, to", "question": "What environment variable is used to export the location of the Hadoop install?", "answers": {"text": ["HADOOP_HOME"], "answer_start": [251]}}
{"context": "the defined include pattern\n  and those log files will be aggregated in a rolling fashion.\n  This will be used with YARN's rolling log aggregation, to enable this feature in YARN side\nyarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds\nshould be\n  configured in yarn-site.xml. The Spark log4j appender needs be changed to use\n  FileAppender or another appender that can handle the files being removed while it is running. Based\n  on the file name configured in the log4j configuration (like spark.log), the user should set the\n  regex (spark*) to include all the log files that need to be aggregated.\n2.0.0\nspark.yarn.rolledLog.excludePattern\n(none)\nJava Regex to filter the log files which match the defined exclude pattern\n  and those log files will not be aggregated in a rolling fas", "question": "Where should the configuration `yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds` be set?", "answers": {"text": ["configured in yarn-site.xml."], "answer_start": [262]}}
{"context": "attern\n(none)\nJava Regex to filter the log files which match the defined exclude pattern\n  and those log files will not be aggregated in a rolling fashion. If the log file\n  name matches both the include and the exclude pattern, this file will be excluded eventually.\n2.0.0\nspark.yarn.executor.launch.excludeOnFailure.enabled\nfalse\nFlag to enable exclusion of nodes having YARN resource allocation problems.\n  The error limit for excluding can be configured by\nspark.excludeOnFailure.application.maxFailedExecutorsPerNode\n.\n2.4.0\nspark.yarn.exclude.nodes\n(none)\nComma-separated list of YARN node names which are excluded from resource allocation.\n3.0.0\nspark.yarn.metrics.namespace\n(none)\nThe root namespace for AM metrics reporting.\n  If it is not set then the YARN application ID is used.\n2.4.0\nspa", "question": "What is the purpose of the flag spark.yarn.executor.launch.excludeOnFailure.enabled?", "answers": {"text": ["Flag to enable exclusion of nodes having YARN resource allocation problems."], "answer_start": [332]}}
{"context": ".0\nspark.yarn.metrics.namespace\n(none)\nThe root namespace for AM metrics reporting.\n  If it is not set then the YARN application ID is used.\n2.4.0\nspark.yarn.report.interval\n1s\nInterval between reports of the current Spark job status in cluster mode.\n0.9.0\nspark.yarn.report.loggingFrequency\n30\nMaximum number of application reports processed until the next application status\n    is logged. If there is a change of state, the application status will be logged regardless\n    of the number of application reports processed.\n3.5.0\nspark.yarn.clientLaunchMonitorInterval\n1s\nInterval between requests for status the client mode AM when starting the app.\n2.3.0\nspark.yarn.includeDriverLogsLink\nfalse\nIn cluster mode, whether the client application report includes links to the driver\n    container's logs", "question": "What is the default value for spark.yarn.includeDriverLogsLink?", "answers": {"text": ["false"], "answer_start": [690]}}
{"context": "after\n    the External Shuffle Service restarts.\n3.5.0\nAvailable patterns for SHS custom executor log URL\nPattern\nMeaning\n{{HTTP_SCHEME}}\nhttp://\nor\nhttps://\naccording to YARN HTTP policy. (Configured via\nyarn.http.policy\n)\n{{NM_HOST}}\nThe \"host\" of node where container was run.\n{{NM_PORT}}\nThe \"port\" of node manager where container was run.\n{{NM_HTTP_PORT}}\nThe \"port\" of node manager's http server where container was run.\n{{NM_HTTP_ADDRESS}}\nHttp URI of the node on which the container is allocated.\n{{CLUSTER_ID}}\nThe cluster ID of Resource Manager. (Configured via\nyarn.resourcemanager.cluster-id\n)\n{{CONTAINER_ID}}\nThe ID of container.\n{{USER}}\nSPARK_USER\non system environment.\n{{FILE_NAME}}\nstdout\n,\nstderr\n.\nFor example, suppose you would like to point log url link to Job History Server d", "question": "What does {{NM_HOST}} represent in the SHS custom executor log URL patterns?", "answers": {"text": ["The \"host\" of node where container was run."], "answer_start": [236]}}
{"context": "}}\nSPARK_USER\non system environment.\n{{FILE_NAME}}\nstdout\n,\nstderr\n.\nFor example, suppose you would like to point log url link to Job History Server directly instead of let NodeManager http server redirects it, you can configure\nspark.history.custom.executor.log.url\nas below:\n{{HTTP_SCHEME}}<JHS_HOST>:<JHS_PORT>/jobhistory/logs/{{NM_HOST}}:{{NM_PORT}}/{{CONTAINER_ID}}/{{CONTAINER_ID}}/{{USER}}/{{FILE_NAME}}?start=-4096\nNOTE: you need to replace\n<JHS_HOST>\nand\n<JHS_PORT>\nwith actual value.\nResource Allocation and Configuration Overview\nPlease make sure to have read the Custom Resource Scheduling and Configuration Overview section on the\nconfiguration page\n. This section only talks about the YARN specific aspects of resource scheduling.\nYARN needs to be configured to support any resources th", "question": "How can you directly point a log URL link to the Job History Server instead of using NodeManager redirects?", "answers": {"text": ["you can configure\nspark.history.custom.executor.log.url"], "answer_start": [211]}}
{"context": "/executor}.resource.\n).\nFor example, the user wants to request 2 GPUs for each executor. The user can just specify\nspark.executor.resource.gpu.amount=2\nand Spark will handle requesting\nyarn.io/gpu\nresource type from YARN.\nIf the user has a user defined YARN resource, lets call it\nacceleratorX\nthen the user must specify\nspark.yarn.executor.resource.acceleratorX.amount=2\nand\nspark.executor.resource.acceleratorX.amount=2\n.\nYARN does not tell Spark the addresses of the resources allocated to each container. For that reason, the user must specify a discovery script that gets run by the executor on startup to discover what resources are available to that executor. You can find an example scripts in\nexamples/src/main/scripts/getGpusResources.sh\n. The script must have execute permissions set and t", "question": "How does a user request 2 GPUs for each executor in Spark?", "answers": {"text": ["spark.executor.resource.gpu.amount=2\nand Spark will handle requesting\nyarn.io/gpu\nresource type from YARN."], "answer_start": [115]}}
{"context": "o that executor. You can find an example scripts in\nexamples/src/main/scripts/getGpusResources.sh\n. The script must have execute permissions set and the user should setup permissions to not allow malicious users to modify it. The script should write to STDOUT a JSON string in the format of the ResourceInformation class. This has the resource name and an array of resource addresses available to just that executor.\nStage Level Scheduling Overview\nStage level scheduling is supported on YARN:\nWhen dynamic allocation is disabled: It allows users to specify different task resource requirements at the stage level and will use the same executors requested at startup.\nWhen dynamic allocation is enabled: It allows users to specify task and executor resource requirements at the stage level and will r", "question": "What should the script write to STDOUT?", "answers": {"text": ["a JSON string in the format of the ResourceInformation class"], "answer_start": [260]}}
{"context": "ested at startup.\nWhen dynamic allocation is enabled: It allows users to specify task and executor resource requirements at the stage level and will request the extra executors.\nOne thing to note that is YARN specific is that each ResourceProfile requires a different container priority on YARN. The mapping is simply the ResourceProfile id becomes the priority, on YARN lower numbers are higher priority. This means that profiles created earlier will have a higher priority in YARN. Normally this won’t matter as Spark finishes one stage before starting another one, the only case this might have an affect is in a job server type scenario, so its something to keep in mind.\nNote there is a difference in the way custom resources are handled between the base default profile and custom ResourceProfi", "question": "What is the relationship between ResourceProfile id and priority on YARN?", "answers": {"text": ["The mapping is simply the ResourceProfile id becomes the priority, on YARN lower numbers are higher priority."], "answer_start": [296]}}
{"context": "omething to keep in mind.\nNote there is a difference in the way custom resources are handled between the base default profile and custom ResourceProfiles. To allow for the user to request YARN containers with extra resources without Spark scheduling on them, the user can specify resources via the\nspark.yarn.executor.resource.\nconfig. Those configs are only used in the base default profile though and do not get propagated into any other custom ResourceProfiles. This is because there would be no way to remove them if you wanted a stage to not have them. This results in your default profile getting custom resources defined in\nspark.yarn.executor.resource.\nplus spark defined resources of GPU or FPGA. Spark converts GPU and FPGA resources into the YARN built in types\nyarn.io/gpu\n) and\nyarn.io/f", "question": "Where are the configurations `spark.yarn.executor.resource.config` used?", "answers": {"text": ["Those configs are only used in the base default profile though and do not get propagated into any other custom ResourceProfiles."], "answer_start": [336]}}
{"context": "this will be linked to by the name\nappSees.txt\n, and your application should use the name as\nappSees.txt\nto reference it when running on YARN.\nThe\n--jars\noption allows the\nSparkContext.addJar\nfunction to work if you are using it with local files and running in\ncluster\nmode. It does not need to be used if you are using it with HDFS, HTTP, HTTPS, or FTP files.\nKerberos\nStandard Kerberos support in Spark is covered in the\nSecurity\npage.\nIn YARN mode, when accessing Hadoop file systems, aside from the default file system in the hadoop\nconfiguration, Spark will also automatically obtain delegation tokens for the service hosting the\nstaging directory of the Spark application.\nYARN-specific Kerberos Configuration\nProperty Name\nDefault\nMeaning\nSince Version\nspark.kerberos.keytab\n(none)\nThe full pa", "question": "When running on YARN, how should your application reference the file named 'appSees.txt'?", "answers": {"text": ["appSees.txt"], "answer_start": [35]}}
{"context": "REST authentication via the system properties\nsun.security.krb5.debug\nand\nsun.security.spnego.debug=true\n-Dsun.security.krb5.debug=true -Dsun.security.spnego.debug=true\nAll these options can be enabled in the Application Master:\nspark.yarn.appMasterEnv.HADOOP_JAAS_DEBUG true\nspark.yarn.am.extraJavaOptions -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug=true\nFinally, if the log level for\norg.apache.spark.deploy.yarn.Client\nis set to\nDEBUG\n, the log\nwill include a list of all tokens obtained, and their expiry details\nConfiguring the External Shuffle Service\nTo start the Spark Shuffle Service on each\nNodeManager\nin your YARN cluster, follow these\ninstructions:\nBuild Spark with the\nYARN profile\n. Skip this step if you are using a\npre-packaged distribution.\nLocate the\nspark-<version>", "question": "How can you enable debugging options for REST authentication?", "answers": {"text": ["sun.security.krb5.debug\nand\nsun.security.spnego.debug=true"], "answer_start": [46]}}
{"context": "follow these\ninstructions:\nBuild Spark with the\nYARN profile\n. Skip this step if you are using a\npre-packaged distribution.\nLocate the\nspark-<version>-yarn-shuffle.jar\n. This should be under\n$SPARK_HOME/common/network-yarn/target/scala-<version>\nif you are building Spark yourself, and under\nyarn\nif you are using a distribution.\nAdd this jar to the classpath of all\nNodeManager\ns in your cluster.\nIn the\nyarn-site.xml\non each node, add\nspark_shuffle\nto\nyarn.nodemanager.aux-services\n,\nthen set\nyarn.nodemanager.aux-services.spark_shuffle.class\nto\norg.apache.spark.network.yarn.YarnShuffleService\n.\nIncrease\nNodeManager's\nheap size by setting\nYARN_HEAPSIZE\n(1000 by default) in\netc/hadoop/yarn-env.sh\nto avoid garbage collection issues during shuffle.\nRestart all\nNodeManager\ns in your cluster.\nThe f", "question": "Where is the spark-<version>-yarn-shuffle.jar located when building Spark yourself?", "answers": {"text": ["$SPARK_HOME/common/network-yarn/target/scala-<version>"], "answer_start": [191]}}
{"context": "APSIZE\n(1000 by default) in\netc/hadoop/yarn-env.sh\nto avoid garbage collection issues during shuffle.\nRestart all\nNodeManager\ns in your cluster.\nThe following extra configuration options are available when the shuffle service is running on YARN:\nProperty Name\nDefault\nMeaning\nSince Version\nspark.yarn.shuffle.stopOnFailure\nfalse\nWhether to stop the NodeManager when there's a failure in the Spark Shuffle Service's\n    initialization. This prevents application failures caused by running containers on\n    NodeManagers where the Spark Shuffle Service is not running.\n2.1.0\nspark.yarn.shuffle.service.metrics.namespace\nsparkShuffleService\nThe namespace to use when emitting shuffle service metrics into Hadoop metrics2 system of the\n    NodeManager.\n3.2.0\nspark.yarn.shuffle.service.logs.namespace\n(no", "question": "What is the default value for the property spark.yarn.shuffle.stopOnFailure?", "answers": {"text": ["false"], "answer_start": [323]}}
{"context": "e to use when emitting shuffle service metrics into Hadoop metrics2 system of the\n    NodeManager.\n3.2.0\nspark.yarn.shuffle.service.logs.namespace\n(not set)\nA namespace which will be appended to the class name when forming the logger name to use for\n    emitting logs from the YARN shuffle service, like\norg.apache.spark.network.yarn.YarnShuffleService.logsNamespaceValue\n. Since some logging frameworks\n    may expect the logger name to look like a class name, it's generally recommended to provide a value which\n    would be a valid Java package or class name and not include spaces.\n3.3.0\nspark.shuffle.service.db.backend\nROCKSDB\nWhen work-preserving restart is enabled in YARN, this is used to specify the disk-base store used\n    in shuffle service state store, supports `ROCKSDB` and `LEVELDB` ", "question": "What disk-base stores are supported for the shuffle service state store when work-preserving restart is enabled in YARN?", "answers": {"text": ["supports `ROCKSDB` and `LEVELDB`"], "answer_start": [767]}}
{"context": "ing restart is enabled in YARN, this is used to specify the disk-base store used\n    in shuffle service state store, supports `ROCKSDB` and `LEVELDB` (deprecated) with `ROCKSDB` as default value.\n    The original data store in `RocksDB/LevelDB` will not be automatically converted to another kind\n    of storage now. The original data store will be retained and the new type data store will be\n    created when switching storage types.\n3.4.0\nPlease note that the instructions above assume that the default shuffle service name,\nspark_shuffle\n, has been used. It is possible to use any name here, but the values used in the\nYARN NodeManager configurations must match the value of\nspark.shuffle.service.name\nin the\nSpark application.\nThe shuffle service will, by default, take all of its configurations", "question": "What are the supported disk-base stores for the shuffle service state store in YARN?", "answers": {"text": ["supports `ROCKSDB` and `LEVELDB` (deprecated) with `ROCKSDB` as default value."], "answer_start": [117]}}
{"context": "ems\nmust be unset.\nUsing the Spark History Server to replace the Spark Web UI\nIt is possible to use the Spark History Server application page as the tracking URL for running\napplications when the application UI is disabled. This may be desirable on secure clusters, or to\nreduce the memory usage of the Spark driver. To set up tracking through the Spark History Server,\ndo the following:\nOn the application side, set\nspark.yarn.historyServer.allowTracking=true\nin Spark’s\nconfiguration. This will tell Spark to use the history server’s URL as the tracking URL if\nthe application’s UI is disabled.\nOn the Spark History Server, add\norg.apache.spark.deploy.yarn.YarnProxyRedirectFilter\nto the list of filters in the\nspark.ui.filters\nconfiguration.\nBe aware that the history server information may not be", "question": "What configuration setting on the application side enables tracking through the Spark History Server?", "answers": {"text": ["spark.yarn.historyServer.allowTracking=true"], "answer_start": [417]}}
{"context": "park-x-config\nyarn.nodemanager.aux-services.spark_shuffle_y.classpath\n=\n/path/to/spark-y-path/*:/path/to/spark-y-config\nThe two\nspark-*-config\ndirectories each contain one file,\nspark-shuffle-site.xml\n. These are XML\nfiles in the\nHadoop Configuration format\nwhich each contain a few configurations to adjust the port number and metrics name prefix used:\n<configuration>\n<property>\n<name>\nspark.shuffle.service.port\n</name>\n<value>\n7001\n</value>\n</property>\n<property>\n<name>\nspark.yarn.shuffle.service.metrics.namespace\n</name>\n<value>\nsparkShuffleServiceX\n</value>\n</property>\n</configuration>\nThe values should both be different for the two different services.\nThen, in the configuration of the Spark applications, one should be configured with:\nspark.shuffle.service.name\n=\nspark_shuffle_x\nspark.s", "question": "What is the value of spark.shuffle.service.port in the spark-shuffle-site.xml file?", "answers": {"text": ["7001"], "answer_start": [431]}}
{"context": "nt services.\nThen, in the configuration of the Spark applications, one should be configured with:\nspark.shuffle.service.name\n=\nspark_shuffle_x\nspark.shuffle.service.port\n=\n7001\nand one should be configured with:\nspark.shuffle.service.name\n=\nspark_shuffle_y\nspark.shuffle.service.port\n=\n<other value>\nConfiguring different JDKs for Spark Applications\nIn some cases it may be desirable to use a different JDK from YARN node manager to run Spark applications,\nthis can be achieved by setting the\nJAVA_HOME\nenvironment variable for YARN containers and the\nspark-submit\nprocess.\nNote that, Spark assumes that all JVM processes runs in one application use the same version of JDK, otherwise,\nyou may encounter JDK serialization issues.\nTo configure a Spark application to use a JDK which has been pre-insta", "question": "What environment variable can be set to use a different JDK for Spark applications?", "answers": {"text": ["JAVA_HOME"], "answer_start": [493]}}
{"context": "1 to run\na Spark application, prepare a JDK 21 tarball\nopenjdk-21.tar.gz\nand untar it to\n/opt\non the local node,\nthen submit a Spark application:\n$ export JAVA_HOME=/opt/openjdk-21\n$ ./bin/spark-submit --class path.to.your.Class \\\n    --master yarn \\\n    --archives path/to/openjdk-21.tar.gz \\\n    --conf spark.yarn.appMasterEnv.JAVA_HOME=./openjdk-21.tar.gz/openjdk-21 \\\n    --conf spark.executorEnv.JAVA_HOME=./openjdk-21.tar.gz/openjdk-21 \\\n    <app jar> [app options]", "question": "How should the JAVA_HOME environment variable be set to run a Spark application with JDK 21?", "answers": {"text": ["$ export JAVA_HOME=/opt/openjdk-21"], "answer_start": [146]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark Standalone Mode\nSecurity\nInstalling Spark Standalone to a Cluster\nStarting a Cluster Manually\nCluster Launch Scripts\nResource Allocation and Configuration Overview\nConnecting an Application to the Cluster\nClient Properties\nLaunching Spark Applic", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars"], "answer_start": [46]}}
{"context": "ur provided\nlaunch scripts\n. It is also possible to run these daemons on a single machine for testing.\nSecurity\nSecurity features like authentication are not enabled by default. When deploying a cluster that is open to the internet\nor an untrusted network, it’s important to secure access to the cluster to prevent unauthorized applications\nfrom running on the cluster.\nPlease see\nSpark Security\nand the specific security sections in this doc before running Spark.\nInstalling Spark Standalone to a Cluster\nTo install Spark Standalone mode, you simply place a compiled version of Spark on each node on the cluster. You can obtain pre-built versions of Spark with each release or\nbuild it yourself\n.\nStarting a Cluster Manually\nYou can start a standalone master server by executing:\n./sbin/start-master", "question": "How can you start a standalone master server?", "answers": {"text": ["./sbin/start-master"], "answer_start": [781]}}
{"context": " Spark with each release or\nbuild it yourself\n.\nStarting a Cluster Manually\nYou can start a standalone master server by executing:\n./sbin/start-master.sh\nOnce started, the master will print out a\nspark://HOST:PORT\nURL for itself, which you can use to connect workers to it,\nor pass as the “master” argument to\nSparkContext\n. You can also find this URL on\nthe master’s web UI, which is\nhttp://localhost:8080\nby default.\nSimilarly, you can start one or more workers and connect them to the master via:\n./sbin/start-worker.sh <master-spark-URL>\nOnce you have started a worker, look at the master’s web UI (\nhttp://localhost:8080\nby default).\nYou should see the new node listed there, along with its number of CPUs and memory (minus one gigabyte left for the OS).\nFinally, the following configuration opt", "question": "How can you start a standalone master server?", "answers": {"text": ["./sbin/start-master.sh"], "answer_start": [131]}}
{"context": "see the new node listed there, along with its number of CPUs and memory (minus one gigabyte left for the OS).\nFinally, the following configuration options can be passed to the master and worker:\nArgument\nMeaning\n-h HOST\n,\n--host HOST\nHostname to listen on\n-p PORT\n,\n--port PORT\nPort for service to listen on (default: 7077 for master, random for worker)\n--webui-port PORT\nPort for web UI (default: 8080 for master, 8081 for worker)\n-c CORES\n,\n--cores CORES\nTotal CPU cores to allow Spark applications to use on the machine (default: all available); only on worker\n-m MEM\n,\n--memory MEM\nTotal amount of memory to allow Spark applications to use on the machine, in a format like 1000M or 2G (default: your machine's total RAM minus 1 GiB); only on worker\n-d DIR\n,\n--work-dir DIR\nDirectory to use for sc", "question": "What is the default port for the master service to listen on?", "answers": {"text": ["default: 7077 for master"], "answer_start": [309]}}
{"context": " machine, in a format like 1000M or 2G (default: your machine's total RAM minus 1 GiB); only on worker\n-d DIR\n,\n--work-dir DIR\nDirectory to use for scratch space and job output logs (default: SPARK_HOME/work); only on worker\n--properties-file FILE\nPath to a custom Spark properties file to load (default: conf/spark-defaults.conf)\nCluster Launch Scripts\nTo launch a Spark standalone cluster with the launch scripts, you should create a file called conf/workers in your Spark directory,\nwhich must contain the hostnames of all the machines where you intend to start Spark workers, one per line.\nIf conf/workers does not exist, the launch scripts defaults to a single machine (localhost), which is useful for testing.\nNote, the master machine accesses each of the worker machines via ssh. By default, s", "question": "What is the default directory used for scratch space and job output logs?", "answers": {"text": ["SPARK_HOME/work"], "answer_start": [192]}}
{"context": "lts to a single machine (localhost), which is useful for testing.\nNote, the master machine accesses each of the worker machines via ssh. By default, ssh is run in parallel and requires password-less (using a private key) access to be setup.\nIf you do not have a password-less setup, you can set the environment variable SPARK_SSH_FOREGROUND and serially provide a password for each worker.\nOnce you’ve set up this file, you can launch or stop your cluster with the following shell scripts, based on Hadoop’s deploy scripts, and available in\nSPARK_HOME/sbin\n:\nsbin/start-master.sh\n- Starts a master instance on the machine the script is executed on.\nsbin/start-workers.sh\n- Starts a worker instance on each machine specified in the\nconf/workers\nfile.\nsbin/start-worker.sh\n- Starts a worker instance on", "question": "Where are the shell scripts for launching or stopping a cluster located?", "answers": {"text": ["SPARK_HOME/sbin"], "answer_start": [541]}}
{"context": "bin/start-workers.sh\n- Starts a worker instance on each machine specified in the\nconf/workers\nfile.\nsbin/start-worker.sh\n- Starts a worker instance on the machine the script is executed on.\nsbin/start-connect-server.sh\n- Starts a Spark Connect server on the machine the script is executed on.\nsbin/start-all.sh\n- Starts both a master and a number of workers as described above.\nsbin/stop-master.sh\n- Stops the master that was started via the\nsbin/start-master.sh\nscript.\nsbin/stop-worker.sh\n- Stops all worker instances on the machine the script is executed on.\nsbin/stop-workers.sh\n- Stops all worker instances on the machines specified in the\nconf/workers\nfile.\nsbin/stop-connect-server.sh\n- Stops all Spark Connect server instances on the machine the script is executed on.\nsbin/stop-all.sh\n- Stop", "question": "What does sbin/start-worker.sh do?", "answers": {"text": ["- Starts a worker instance on the machine the script is executed on."], "answer_start": [121]}}
{"context": "workers\nfile.\nsbin/stop-connect-server.sh\n- Stops all Spark Connect server instances on the machine the script is executed on.\nsbin/stop-all.sh\n- Stops both the master and the workers as described above.\nNote that these scripts must be executed on the machine you want to run the Spark master on, not your local machine.\nYou can optionally configure the cluster further by setting environment variables in\nconf/spark-env.sh\n. Create this file by starting with the\nconf/spark-env.sh.template\n, and\ncopy it to all your worker machines\nfor the settings to take effect. The following settings are available:\nEnvironment Variable\nMeaning\nSPARK_MASTER_HOST\nBind the master to a specific hostname or IP address, for example a public one.\nSPARK_MASTER_PORT\nStart the master on a different port (default: 7077", "question": "What is the purpose of sbin/stop-connect-server.sh?", "answers": {"text": ["Stops all Spark Connect server instances on the machine the script is executed on."], "answer_start": [44]}}
{"context": "\nBind the master to a specific hostname or IP address, for example a public one.\nSPARK_MASTER_PORT\nStart the master on a different port (default: 7077).\nSPARK_MASTER_WEBUI_PORT\nPort for the master web UI (default: 8080).\nSPARK_MASTER_OPTS\nConfiguration properties that apply only to the master in the form \"-Dx=y\" (default: none). See below for a list of possible options.\nSPARK_LOCAL_DIRS\nDirectory to use for \"scratch\" space in Spark, including map output files and RDDs that get\n    stored on disk. This should be on a fast, local disk in your system. It can also be a\n    comma-separated list of multiple directories on different disks.\nSPARK_LOG_DIR\nWhere log files are stored. (default: SPARK_HOME/logs).\nSPARK_LOG_MAX_FILES\nThe maximum number of log files (default: 5).\nSPARK_PID_DIR\nWhere pid", "question": "What is the default port for the master web UI?", "answers": {"text": ["default: 8080"], "answer_start": [205]}}
{"context": "_DIR\nWhere log files are stored. (default: SPARK_HOME/logs).\nSPARK_LOG_MAX_FILES\nThe maximum number of log files (default: 5).\nSPARK_PID_DIR\nWhere pid files are stored. (default: /tmp).\nSPARK_WORKER_CORES\nTotal number of cores to allow Spark applications to use on the machine (default: all available cores).\nSPARK_WORKER_MEMORY\nTotal amount of memory to allow Spark applications to use on the machine, e.g.\n1000m\n,\n2g\n(default: total memory minus 1 GiB); note that each application's\nindividual\nmemory is configured using its\nspark.executor.memory\nproperty.\nSPARK_WORKER_PORT\nStart the Spark worker on a specific port (default: random).\nSPARK_WORKER_WEBUI_PORT\nPort for the worker web UI (default: 8081).\nSPARK_WORKER_DIR\nDirectory to run applications in, which will include both logs and scratch sp", "question": "What is the default directory for storing log files?", "answers": {"text": ["SPARK_HOME/logs"], "answer_start": [43]}}
{"context": "_WEBUI_PORT\nPort for the worker web UI (default: 8081).\nSPARK_WORKER_DIR\nDirectory to run applications in, which will include both logs and scratch space (default: SPARK_HOME/work).\nSPARK_WORKER_OPTS\nConfiguration properties that apply only to the worker in the form \"-Dx=y\" (default: none). See below for a list of possible options.\nSPARK_DAEMON_MEMORY\nMemory to allocate to the Spark master and worker daemons themselves (default: 1g).\nSPARK_DAEMON_JAVA_OPTS\nJVM options for the Spark master and worker daemons themselves in the form \"-Dx=y\" (default: none).\nSPARK_DAEMON_CLASSPATH\nClasspath for the Spark master and worker daemons themselves (default: none).\nSPARK_PUBLIC_DNS\nThe public DNS name of the Spark master and workers (default: none).\nNote:\nThe launch scripts do not currently support Wi", "question": "Qual é a porta padrão para a interface web do worker?", "answers": {"text": ["8081"], "answer_start": [49]}}
{"context": "ult: none).\nSPARK_PUBLIC_DNS\nThe public DNS name of the Spark master and workers (default: none).\nNote:\nThe launch scripts do not currently support Windows. To run a Spark cluster on Windows, start the master and workers by hand.\nSPARK_MASTER_OPTS supports the following system properties:\nProperty Name\nDefault\nMeaning\nSince Version\nspark.master.ui.port\n8080\nSpecifies the port number of the Master Web UI endpoint.\n1.1.0\nspark.master.ui.title\n(None)\nSpecifies the title of the Master UI page. If unset,\nSpark Master at 'master url'\nis used by default.\n4.0.0\nspark.master.ui.decommission.allow.mode\nLOCAL\nSpecifies the behavior of the Master Web UI's /workers/kill endpoint. Possible choices\n    are:\nLOCAL\nmeans allow this endpoint from IP's that are local to the machine running\n    the Master,\nDE", "question": "What is the default value for spark.master.ui.title?", "answers": {"text": ["(None)"], "answer_start": [445]}}
{"context": "4.0.0\nspark.master.rest.port\n6066\nSpecifies the port number of the Master REST API endpoint.\n1.3.0\nspark.master.rest.filters\n(None)\nComma separated list of filter class names to apply to the Master REST API.\n4.0.0\nspark.master.useAppNameAsAppId.enabled\nfalse\n(Experimental) If true, Spark master uses the user-provided appName for appId.\n4.0.0\nspark.deploy.retainedApplications\n200\nThe maximum number of completed applications to display. Older applications will be dropped from the UI to maintain this limit.\n0.8.0\nspark.deploy.retainedDrivers\n200\nThe maximum number of completed drivers to display. Older drivers will be dropped from the UI to maintain this limit.\n1.1.0\nspark.deploy.spreadOutDrivers\ntrue\nWhether the standalone cluster manager should spread drivers out across nodes or try\n    to ", "question": "What does spark.master.rest.port specify?", "answers": {"text": ["Specifies the port number of the Master REST API endpoint."], "answer_start": [34]}}
{"context": "tain this limit.\n1.1.0\nspark.deploy.spreadOutDrivers\ntrue\nWhether the standalone cluster manager should spread drivers out across nodes or try\n    to consolidate them onto as few nodes as possible. Spreading out is usually better for\n    data locality in HDFS, but consolidating is more efficient for compute-intensive workloads.\n4.0.0\nspark.deploy.spreadOutApps\ntrue\nWhether the standalone cluster manager should spread applications out across nodes or try\n    to consolidate them onto as few nodes as possible. Spreading out is usually better for\n    data locality in HDFS, but consolidating is more efficient for compute-intensive workloads.\n0.6.1\nspark.deploy.defaultCores\nInt.MaxValue\nDefault number of cores to give to applications in Spark's standalone mode if they don't\n    set\nspark.cores.m", "question": "What is the default number of cores given to applications in Spark's standalone mode?", "answers": {"text": ["Int.MaxValue"], "answer_start": [677]}}
{"context": " experiences more than\nspark.deploy.maxExecutorRetries\nfailures in a row, no executors\n    successfully start running in between those failures, and the application has no running\n    executors then the standalone cluster manager will remove the application and mark it as failed.\n    To disable this automatic removal, set\nspark.deploy.maxExecutorRetries\nto\n-1\n.\n1.6.3\nspark.deploy.maxDrivers\nInt.MaxValue\nThe maximum number of running drivers.\n4.0.0\nspark.deploy.appNumberModulo\n(None)\nThe modulo for app number. By default, the next of\napp-yyyyMMddHHmmss-9999\nis\napp-yyyyMMddHHmmss-10000\n. If we have 10000 as modulo, it will be\napp-yyyyMMddHHmmss-0000\n.\n    In most cases, the prefix\napp-yyyyMMddHHmmss\nis increased already during creating 10000 applications.\n4.0.0\nspark.deploy.driverIdPattern\nd", "question": "What value can be set for spark.deploy.maxExecutorRetries to disable automatic application removal?", "answers": {"text": ["-1"], "answer_start": [359]}}
{"context": "-0000\n.\n    In most cases, the prefix\napp-yyyyMMddHHmmss\nis increased already during creating 10000 applications.\n4.0.0\nspark.deploy.driverIdPattern\ndriver-%s-%04d\nThe pattern for driver ID generation based on Java\nString.format\nmethod.\n    The default value is\ndriver-%s-%04d\nwhich represents the existing driver id string, e.g.,\ndriver-20231031224459-0019\n. Please be careful to generate unique IDs.\n4.0.0\nspark.deploy.appIdPattern\napp-%s-%04d\nThe pattern for app ID generation based on Java\nString.format\nmethod.\n    The default value is\napp-%s-%04d\nwhich represents the existing app id string, e.g.,\napp-20231031224509-0008\n. Please be careful to generate unique IDs.\n4.0.0\nspark.worker.timeout\n60\nNumber of seconds after which the standalone deploy master considers a worker lost if it\n    recei", "question": "What is the default value for spark.deploy.appIdPattern?", "answers": {"text": ["app-%s-%04d"], "answer_start": [434]}}
{"context": " output of the script should be formatted like the\nResourceInformation\nclass.\n3.0.0\nspark.worker.resourcesFile\n(none)\nPath to resources file which is used to find various resources while worker starting up.\n    The content of resources file should be formatted like\n[{\"id\":{\"componentName\": \"spark.worker\", \"resourceName\":\"gpu\"}, \"addresses\":[\"0\",\"1\",\"2\"]}]\n.\n    If a particular resource is not found in the resources file, the discovery script would be used to\n    find that resource. If the discovery script also does not find the resources, the worker will fail\n    to start up.\n3.0.0\nSPARK_WORKER_OPTS supports the following system properties:\nProperty Name\nDefault\nMeaning\nSince Version\nspark.worker.initialRegistrationRetries\n6\nThe number of retries to reconnect in short intervals (between 5 ", "question": "What is the expected format of the resources file content?", "answers": {"text": ["[{\"id\":{\"componentName\": \"spark.worker\", \"resourceName\":\"gpu\"}, \"addresses\":[\"0\",\"1\",\"2\"]}]"], "answer_start": [266]}}
{"context": "roperty Name\nDefault\nMeaning\nSince Version\nspark.worker.initialRegistrationRetries\n6\nThe number of retries to reconnect in short intervals (between 5 and 15 seconds).\n4.0.0\nspark.worker.maxRegistrationRetries\n16\nThe max number of retries to reconnect.\n    After\nspark.worker.initialRegistrationRetries\nattempts, the interval is between\n    30 and 90 seconds.\n4.0.0\nspark.worker.cleanup.enabled\ntrue\nEnable periodic cleanup of worker / application directories.  Note that this only affects standalone\n    mode, as YARN works differently. Only the directories of stopped applications are cleaned up.\n    This should be enabled if\nspark.shuffle.service.db.enabled\nis \"true\"\n1.0.0\nspark.worker.cleanup.interval\n1800 (30 minutes)\nControls the interval, in seconds, at which the worker cleans up old applic", "question": "Qual é o número padrão de tentativas de reconexão em intervalos curtos (entre 5 e 15 segundos)?", "answers": {"text": ["6"], "answer_start": [83]}}
{"context": "db.enabled\nis \"true\"\n1.0.0\nspark.worker.cleanup.interval\n1800 (30 minutes)\nControls the interval, in seconds, at which the worker cleans up old application work dirs\n    on the local machine.\n1.0.0\nspark.worker.cleanup.appDataTtl\n604800 (7 days, 7 * 24 * 3600)\nThe number of seconds to retain application work directories on each worker.  This is a Time To Live\n    and should depend on the amount of available disk space you have.  Application logs and jars are\n    downloaded to each application work dir.  Over time, the work dirs can quickly fill up disk space,\n    especially if you run jobs very frequently.\n1.0.0\nspark.shuffle.service.db.enabled\ntrue\nStore External Shuffle service state on local disk so that when the external shuffle service is restarted, it will\n    automatically reload in", "question": "What is the purpose of spark.worker.cleanup.appDataTtl?", "answers": {"text": ["The number of seconds to retain application work directories on each worker."], "answer_start": [261]}}
{"context": "e unique IDs\n4.0.0\nResource Allocation and Configuration Overview\nPlease make sure to have read the Custom Resource Scheduling and Configuration Overview section on the\nconfiguration page\n. This section only talks about the Spark Standalone specific aspects of resource scheduling.\nSpark Standalone has 2 parts, the first is configuring the resources for the Worker, the second is the resource allocation for a specific application.\nThe user must configure the Workers to have a set of resources available so that it can assign them out to Executors. The\nspark.worker.resource.{resourceName}.amount\nis used to control the amount of each resource the worker has allocated. The user must also specify either\nspark.worker.resourcesFile\nor\nspark.worker.resource.{resourceName}.discoveryScript\nto specify ", "question": "How are resources configured for Workers in Spark Standalone?", "answers": {"text": ["The user must configure the Workers to have a set of resources available so that it can assign them out to Executors."], "answer_start": [433]}}
{"context": "worker has allocated. The user must also specify either\nspark.worker.resourcesFile\nor\nspark.worker.resource.{resourceName}.discoveryScript\nto specify how the Worker discovers the resources its assigned. See the descriptions above for each of those to see which method works best for your setup.\nThe second part is running an application on Spark Standalone. The only special case from the standard Spark resource configs is when you are running the Driver in client mode. For a Driver in client mode, the user can specify the resources it uses via\nspark.driver.resourcesFile\nor\nspark.driver.resource.{resourceName}.discoveryScript\n. If the Driver is running on the same host as other Drivers, please make sure the resources file or discovery script only returns resources that do not conflict with ot", "question": "How does the Worker discover the resources it's assigned?", "answers": {"text": ["to specify how the Worker discovers the resources its assigned."], "answer_start": [139]}}
{"context": "running on the same host as other Drivers, please make sure the resources file or discovery script only returns resources that do not conflict with other Drivers running on the same node.\nNote, the user does not need to specify a discovery script when submitting an application as the Worker will start each Executor with the resources it allocates to it.\nConnecting an Application to the Cluster\nTo run an application on the Spark cluster, simply pass the\nspark://IP:PORT\nURL of the master as to the\nSparkContext\nconstructor\n.\nTo run an interactive Spark shell against the cluster, run the following command:\n./bin/spark-shell --master spark://IP:PORT\nYou can also pass an option\n--total-executor-cores <numCores>\nto control the number of cores that spark-shell uses on the cluster.\nClient Propertie", "question": "How can you run an interactive Spark shell against the cluster?", "answers": {"text": ["./bin/spark-shell --master spark://IP:PORT"], "answer_start": [610]}}
{"context": "RT\nYou can also pass an option\n--total-executor-cores <numCores>\nto control the number of cores that spark-shell uses on the cluster.\nClient Properties\nSpark applications supports the following configuration properties specific to standalone mode:\nProperty Name\nDefault Value\nMeaning\nSince Version\nspark.standalone.submit.waitAppCompletion\nfalse\nIn standalone cluster mode, controls whether the client waits to exit until the application completes.\n  If set to\ntrue\n, the client process will stay alive polling the driver's status.\n  Otherwise, the client process will exit after submission.\n3.1.0\nLaunching Spark Applications\nSpark Protocol\nThe\nspark-submit\nscript\nprovides the most straightforward way to\nsubmit a compiled Spark application to the cluster. For standalone clusters, Spark currently\n", "question": "What does the property 'spark.standalone.submit.waitAppCompletion' control in standalone cluster mode?", "answers": {"text": ["In standalone cluster mode, controls whether the client waits to exit until the application completes."], "answer_start": [346]}}
{"context": "r the values of Spark properties and environment variables.\n1.3.0\nkill\nPOST\nKill a single Spark driver.\n1.3.0\nkillall\nPOST\nKill all running Spark drivers.\n4.0.0\nstatus\nGET\nCheck the status of a Spark job.\n1.3.0\nclear\nPOST\nClear the completed drivers and applications.\n4.0.0\nThe following is a\ncurl\nCLI command example with the\npi.py\nand REST API.\n$\ncurl\n-XPOST\nhttp://IP:PORT/v1/submissions/create\n\\\n--header\n\"Content-Type:application/json;charset=UTF-8\"\n\\\n--data\n'{\n  \"appResource\": \"\",\n  \"sparkProperties\": {\n    \"spark.master\": \"spark://master:7077\",\n    \"spark.app.name\": \"Spark Pi\",\n    \"spark.driver.memory\": \"1g\",\n    \"spark.driver.cores\": \"1\",\n    \"spark.jars\": \"\"\n  },\n  \"clientSparkVersion\": \"\",\n  \"mainClass\": \"org.apache.spark.deploy.SparkSubmit\",\n  \"environmentVariables\": { },\n  \"action", "question": "What is the content type header used in the curl command example?", "answers": {"text": ["Content-Type:application/json;charset=UTF-8"], "answer_start": [410]}}
{"context": ",\n    \"spark.jars\": \"\"\n  },\n  \"clientSparkVersion\": \"\",\n  \"mainClass\": \"org.apache.spark.deploy.SparkSubmit\",\n  \"environmentVariables\": { },\n  \"action\": \"CreateSubmissionRequest\",\n  \"appArgs\": [ \"/opt/spark/examples/src/main/python/pi.py\", \"10\" ]\n}'\nThe following is the response from the REST API for the above\ncreate\nrequest.\n{\n\"action\"\n:\n\"CreateSubmissionResponse\"\n,\n\"message\"\n:\n\"Driver successfully submitted as driver-20231124153531-0000\"\n,\n\"serverSparkVersion\"\n:\n\"4.0.0\"\n,\n\"submissionId\"\n:\n\"driver-20231124153531-0000\"\n,\n\"success\"\n:\ntrue\n}\nWhen Spark master requires HTTP\nAuthorization\nheader via\nspark.master.rest.filters=org.apache.spark.ui.JWSFilter\nand\nspark.org.apache.spark.ui.JWSFilter.param.secretKey=BASE64URL-ENCODED-KEY\nconfigurations,\ncurl\nCLI command can provide the required heade", "question": "What is the serverSparkVersion reported in the CreateSubmissionResponse?", "answers": {"text": ["4.0.0"], "answer_start": [470]}}
{"context": "rabs all the cores available\non the worker by default, in which case only one executor per application may be launched on each\nworker during one single schedule iteration.\nStage Level Scheduling Overview\nStage level scheduling is supported on Standalone:\nWhen dynamic allocation is disabled: It allows users to specify different task resource requirements at the stage level and will use the same executors requested at startup.\nWhen dynamic allocation is enabled: Currently, when the Master allocates executors for one application, it will schedule based on the order of the ResourceProfile ids for multiple ResourceProfiles. The ResourceProfile with smaller id will be scheduled firstly. Normally this won’t matter as Spark finishes one stage before starting another one, the only case this might h", "question": "What happens when dynamic allocation is enabled in Standalone mode?", "answers": {"text": ["Currently, when the Master allocates executors for one application, it will schedule based on the order of the ResourceProfile ids for multiple ResourceProfiles. The ResourceProfile with smaller id will be scheduled firstly."], "answer_start": [465]}}
{"context": "h smaller id will be scheduled firstly. Normally this won’t matter as Spark finishes one stage before starting another one, the only case this might have an affect is in a job server type scenario, so its something to keep in mind. For scheduling, we will only take executor memory and executor cores from built-in executor resources and all other custom resources from a ResourceProfile, other built-in executor resources such as offHeap and memoryOverhead won’t take any effect. The base default profile will be created based on the spark configs when you submit an application. Executor memory and executor cores from the base default profile can be propagated to custom ResourceProfiles, but all other custom resources can not be propagated.\nCaveats\nAs mentioned in\nDynamic Resource Allocation\n, ", "question": "What resources are used for scheduling?", "answers": {"text": ["For scheduling, we will only take executor memory and executor cores from built-in executor resources and all other custom resources from a ResourceProfile, other built-in executor resources such as offHeap and memoryOverhead won’t take any effect."], "answer_start": [232]}}
{"context": "be propagated to custom ResourceProfiles, but all other custom resources can not be propagated.\nCaveats\nAs mentioned in\nDynamic Resource Allocation\n, if cores for each executor is not explicitly specified with dynamic allocation enabled, spark will possibly acquire much more executors than expected. So you are recommended to explicitly set executor cores for each resource profile when using stage level scheduling.\nMonitoring and Logging\nSpark’s standalone mode offers a web-based user interface to monitor the cluster. The master and each worker has its own web UI that shows cluster and job statistics. By default, you can access the web UI for the master at port 8080. The port can be changed either in the configuration file or via command-line options.\nIn addition, detailed log output for ea", "question": "What is the default port to access the web UI for the Spark master?", "answers": {"text": ["By default, you can access the web UI for the master at port 8080."], "answer_start": [608]}}
{"context": "the master at port 8080. The port can be changed either in the configuration file or via command-line options.\nIn addition, detailed log output for each job is also written to the work directory of each worker node (\nSPARK_HOME/work\nby default). You will see two files for each job,\nstdout\nand\nstderr\n, with all output it wrote to its console.\nRunning Alongside Hadoop\nYou can run Spark alongside your existing Hadoop cluster by just launching it as a separate service on the same machines. To access Hadoop data from Spark, just use an hdfs:// URL (typically\nhdfs://<namenode>:9000/path\n, but you can find the right URL on your Hadoop Namenode’s web UI). Alternatively, you can set up a separate cluster for Spark, and still have it access HDFS over the network; this will be slower than disk-local ", "question": "Where is the detailed log output for each job written by default?", "answers": {"text": ["SPARK_HOME/work"], "answer_start": [217]}}
{"context": " UI). Alternatively, you can set up a separate cluster for Spark, and still have it access HDFS over the network; this will be slower than disk-local access, but may not be a concern if you are still running in the same local area network (e.g. you place a few Spark machines on each rack that you have Hadoop on).\nConfiguring Ports for Network Security\nGenerally speaking, a Spark cluster and its services are not deployed on the public internet.\nThey are generally private services, and should only be accessible within the network of the\norganization that deploys Spark. Access to the hosts and ports used by Spark services should\nbe limited to origin hosts that need to access the services.\nThis is particularly important for clusters using the standalone resource manager, as they do\nnot support", "question": "What is a potential drawback of setting up a separate Spark cluster that accesses HDFS over the network?", "answers": {"text": ["this will be slower than disk-local access"], "answer_start": [114]}}
{"context": "igin hosts that need to access the services.\nThis is particularly important for clusters using the standalone resource manager, as they do\nnot support fine-grained access control in a way that other resource managers do.\nFor a complete list of ports to configure, see the\nsecurity page\n.\nHigh Availability\nBy default, standalone scheduling clusters are resilient to Worker failures (insofar as Spark itself is resilient to losing work by moving it to other workers). However, the scheduler uses a Master to make scheduling decisions, and this (by default) creates a single point of failure: if the Master crashes, no new applications can be created. In order to circumvent this, we have two high availability schemes, detailed below.\nStandby Masters with ZooKeeper\nOverview\nUtilizing ZooKeeper to pro", "question": "What is a single point of failure in standalone scheduling clusters?", "answers": {"text": ["the scheduler uses a Master to make scheduling decisions, and this (by default) creates a single point of failure: if the Master crashes, no new applications can be created."], "answer_start": [476]}}
{"context": "In order to circumvent this, we have two high availability schemes, detailed below.\nStandby Masters with ZooKeeper\nOverview\nUtilizing ZooKeeper to provide leader election and some state storage, you can launch multiple Masters in your cluster connected to the same ZooKeeper instance. One will be elected “leader” and the others will remain in standby mode. If the current leader dies, another Master will be elected, recover the old Master’s state, and then resume scheduling. The entire recovery process (from the time the first leader goes down) should take between 1 and 2 minutes. Note that this delay only affects scheduling\nnew\napplications – applications that were already running during Master failover are unaffected.\nLearn more about getting started with ZooKeeper\nhere\n.\nConfiguration\nIn ", "question": "How long should the entire recovery process take when the current leader Master dies?", "answers": {"text": ["The entire recovery process (from the time the first leader goes down) should take between 1 and 2 minutes."], "answer_start": [478]}}
{"context": "applications that were already running during Master failover are unaffected.\nLearn more about getting started with ZooKeeper\nhere\n.\nConfiguration\nIn order to enable this recovery mode, you can set\nSPARK_DAEMON_JAVA_OPTS\nin spark-env by configuring\nspark.deploy.recoveryMode\nand related\nspark.deploy.zookeeper.*\nconfigurations.\nPossible gotcha: If you have multiple Masters in your cluster but fail to correctly configure the Masters to use ZooKeeper, the Masters will fail to discover each other and think they’re all leaders. This will not lead to a healthy cluster state (as all Masters will schedule independently).\nDetails\nAfter you have a ZooKeeper cluster set up, enabling high availability is straightforward. Simply start multiple Master processes on different nodes with the same ZooKeeper ", "question": "What happens to applications running during Master failover?", "answers": {"text": ["applications that were already running during Master failover are unaffected."], "answer_start": [0]}}
{"context": "eper cluster set up, enabling high availability is straightforward. Simply start multiple Master processes on different nodes with the same ZooKeeper configuration (ZooKeeper URL and directory). Masters can be added and removed at any time.\nIn order to schedule new applications or add Workers to the cluster, they need to know the IP address of the current leader. This can be accomplished by simply passing in a list of Masters where you used to pass in a single one. For example, you might start your SparkContext pointing to\nspark://host1:port1,host2:port2\n. This would cause your SparkContext to try registering with both Masters – if\nhost1\ngoes down, this configuration would still be correct as we’d find the new leader,\nhost2\n.\nThere’s an important distinction to be made between “registering", "question": "How can new applications or Workers be scheduled or added to the cluster?", "answers": {"text": ["This can be accomplished by simply passing in a list of Masters where you used to pass in a single one."], "answer_start": [366]}}
{"context": " down, this configuration would still be correct as we’d find the new leader,\nhost2\n.\nThere’s an important distinction to be made between “registering with a Master” and normal operation. When starting up, an application or Worker needs to be able to find and register with the current lead Master. Once it successfully registers, though, it is “in the system” (i.e., stored in ZooKeeper). If failover occurs, the new leader will contact all previously registered applications and Workers to inform them of the change in leadership, so they need not even have known of the existence of the new Master at startup.\nDue to this property, new Masters can be created at any time, and the only thing you need to worry about is that\nnew\napplications and Workers can find it to register with in case it becom", "question": "What happens after an application or Worker successfully registers with the lead Master?", "answers": {"text": ["Once it successfully registers, though, it is “in the system” (i.e., stored in ZooKeeper)."], "answer_start": [299]}}
{"context": " be created at any time, and the only thing you need to worry about is that\nnew\napplications and Workers can find it to register with in case it becomes the leader. Once registered, you’re taken care of.\nSingle-Node Recovery with Local File System\nOverview\nZooKeeper is the best way to go for production-level high availability, but if you just want to be able to restart the Master if it goes down, FILESYSTEM mode can take care of it. When applications and Workers register, they have enough state written to the provided directory so that they can be recovered upon a restart of the Master process.\nConfiguration\nIn order to enable this recovery mode, you can set SPARK_DAEMON_JAVA_OPTS in spark-env using this configuration:\nSystem property\nDefault Value\nMeaning\nSince Version\nspark.deploy.recove", "question": "What mode can be used to restart the Master if it goes down, offering a simpler solution than ZooKeeper for high availability?", "answers": {"text": ["FILESYSTEM mode can take care of it."], "answer_start": [400]}}
{"context": "ode, you can set SPARK_DAEMON_JAVA_OPTS in spark-env using this configuration:\nSystem property\nDefault Value\nMeaning\nSince Version\nspark.deploy.recoveryMode\nNONE\nThe recovery mode setting to recover submitted Spark jobs with cluster mode when it failed and relaunches. Set to\n      FILESYSTEM to enable file-system-based single-node recovery mode,\n      ROCKSDB to enable RocksDB-based single-node recovery mode,\n      ZOOKEEPER to use Zookeeper-based recovery mode, and\n      CUSTOM to provide a customer provider class via additional `spark.deploy.recoveryMode.factory` configuration.\n      NONE is the default value which disables this recovery mode.\n0.8.1\nspark.deploy.recoveryDirectory\n\"\"\nThe directory in which Spark will store recovery state, accessible from the Master's perspective.\n      No", "question": "What is the default value for spark.deploy.recoveryMode?", "answers": {"text": ["NONE"], "answer_start": [157]}}
{"context": "de.\n0.8.1\nspark.deploy.recoveryDirectory\n\"\"\nThe directory in which Spark will store recovery state, accessible from the Master's perspective.\n      Note that the directory should be clearly manually if\nspark.deploy.recoveryMode\nor\nspark.deploy.recoveryCompressionCodec\nis changed.\n0.8.1\nspark.deploy.recoveryCompressionCodec\n(none)\nA compression codec for persistence engines. none (default), lz4, lzf, snappy, and zstd. Currently, only FILESYSTEM mode supports this configuration.\n4.0.0\nspark.deploy.recoveryTimeout\n(none)\nThe timeout for recovery process. The default value is the same with\nspark.worker.timeout\n.\n4.0.0\nspark.deploy.recoveryMode.factory\n\"\"\nA class to implement\nStandaloneRecoveryModeFactory\ninterface\n1.2.0\nspark.deploy.zookeeper.url\nNone\nWhen\nspark.deploy.recoveryMode\nis set to Z", "question": "What is the default value for spark.deploy.recoveryTimeout?", "answers": {"text": ["(none)"], "answer_start": [325]}}
{"context": " suboptimal for certain development or experimental purposes. In particular, killing a master via stop-master.sh does not clean up its recovery state, so whenever you start a new Master, it will enter recovery mode. This could increase the startup time by up to 1 minute if it needs to wait for all previously-registered Workers/clients to timeout.\nWhile it’s not officially supported, you could mount an NFS directory as the recovery directory. If the original Master node dies completely, you could then start a Master on a different node, which would correctly recover all previously registered Workers/applications (equivalent to ZooKeeper recovery). Future applications will have to be able to find the new Master, however, in order to register.", "question": "What happens if a master is killed via stop-master.sh?", "answers": {"text": ["killing a master via stop-master.sh does not clean up its recovery state, so whenever you start a new Master, it will enter recovery mode."], "answer_start": [77]}}
{"context": "ry). Future applications will have to be able to find the new Master, however, in order to register.", "question": "What will future applications need to be able to do?", "answers": {"text": ["find the new Master, however, in order to register."], "answer_start": [49]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nPerformance Tuning\nCaching Data\nTuning Partitions\nLeveraging Statistics\nOptimizing the Join Strategy\nAdaptive Query Execution\nStorage Partition Join\nDistributed SQL Engine\nPySpark Usage Guide for Pandas wit", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars"], "answer_start": [46]}}
{"context": "ging Statistics\nOptimizing the Join Strategy\nAdaptive Query Execution\nStorage Partition Join\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nPerformance Tuning\nSpark offers many techniques for tuning the performance of DataFrame or SQL workloads. Those techniques, broadly speaking, include caching data, altering how datasets are partitioned, selecting the optimal join strategy, and providing the optimizer with additional information it can use to build more efficient execution plans.\nCaching Data\nTuning Partitions\nCoalesce Hints\nLeveraging Statistics\nOptimizing the Join Strategy\nAutomatically Broadcasting Joins\nJoin Strategy Hints\nAdaptive Query Execution\nCoalescing Post Shuffle Partitions\nSplitting skewed shuffle parti", "question": "What are some techniques Spark offers for tuning performance of DataFrame or SQL workloads?", "answers": {"text": ["Those techniques, broadly speaking, include caching data, altering how datasets are partitioned, selecting the optimal join strategy, and providing the optimizer with additional information it can use to build more efficient execution plans."], "answer_start": [318]}}
{"context": "ataFrame.unpersist()\nto remove the table from memory.\nConfiguration of in-memory caching can be done via\nspark.conf.set\nor by running\nSET key=value\ncommands using SQL.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.inMemoryColumnarStorage.compressed\ntrue\nWhen set to true, Spark SQL will automatically select a compression codec for each column based\n    on statistics of the data.\n1.0.1\nspark.sql.inMemoryColumnarStorage.batchSize\n10000\nControls the size of batches for columnar caching. Larger batch sizes can improve memory utilization\n    and compression, but risk OOMs when caching data.\n1.1.1\nTuning Partitions\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.files.maxPartitionBytes\n134217728 (128 MB)\nThe maximum number of bytes to pack into a single partition when reading files.", "question": "What is the default value for spark.sql.files.maxPartitionBytes?", "answers": {"text": ["134217728 (128 MB)"], "answer_start": [702]}}
{"context": "ing\nSince Version\nspark.sql.files.maxPartitionBytes\n134217728 (128 MB)\nThe maximum number of bytes to pack into a single partition when reading files.\n      This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.\n2.0.0\nspark.sql.files.openCostInBytes\n4194304 (4 MB)\nThe estimated cost to open a file, measured by the number of bytes that could be scanned in the same\n      time. This is used when putting multiple files into a partition. It is better to over-estimate,\n      then the partitions with small files will be faster than partitions with bigger files (which is\n      scheduled first). This configuration is effective only when using file-based sources such as Parquet,\n      JSON and ORC.\n2.0.0\nspark.sql.files.minPartitionNum\nDefault Parallelism\nT", "question": "When is the configuration 'spark.sql.files.maxPartitionBytes' effective?", "answers": {"text": ["This configuration is effective only when using file-based sources such as Parquet, JSON and ORC."], "answer_start": [157]}}
{"context": " files. The “COALESCE” hint only has a partition number as a\nparameter. The “REPARTITION” hint has a partition number, columns, or both/neither of them as parameters.\nThe “REPARTITION_BY_RANGE” hint must have column names and a partition number is optional. The “REBALANCE”\nhint has an initial partition number, columns, or both/neither of them as parameters.\nSELECT\n/*+ COALESCE(3) */\n*\nFROM\nt\n;\nSELECT\n/*+ REPARTITION(3) */\n*\nFROM\nt\n;\nSELECT\n/*+ REPARTITION(c) */\n*\nFROM\nt\n;\nSELECT\n/*+ REPARTITION(3, c) */\n*\nFROM\nt\n;\nSELECT\n/*+ REPARTITION */\n*\nFROM\nt\n;\nSELECT\n/*+ REPARTITION_BY_RANGE(c) */\n*\nFROM\nt\n;\nSELECT\n/*+ REPARTITION_BY_RANGE(3, c) */\n*\nFROM\nt\n;\nSELECT\n/*+ REBALANCE */\n*\nFROM\nt\n;\nSELECT\n/*+ REBALANCE(3) */\n*\nFROM\nt\n;\nSELECT\n/*+ REBALANCE(c) */\n*\nFROM\nt\n;\nSELECT\n/*+ REBALANCE(3, c) */\n*", "question": "Quais parâmetros o hint “REPARTITION” pode ter?", "answers": {"text": ["The “REPARTITION” hint has a partition number, columns, or both/neither of them as parameters."], "answer_start": [72]}}
{"context": "ROM\nt\n;\nSELECT\n/*+ REBALANCE */\n*\nFROM\nt\n;\nSELECT\n/*+ REBALANCE(3) */\n*\nFROM\nt\n;\nSELECT\n/*+ REBALANCE(c) */\n*\nFROM\nt\n;\nSELECT\n/*+ REBALANCE(3, c) */\n*\nFROM\nt\n;\nFor more details please refer to the documentation of\nPartitioning Hints\n.\nLeveraging Statistics\nApache Spark’s ability to choose the best execution plan among many possible options is determined in part by its estimates of how many rows will be output by every node in the execution plan (read, filter, join, etc.). Those estimates in turn are based on statistics that are made available to Spark in one of several ways:\nData source\n: Statistics that Spark reads directly from the underlying data source, like the counts and min/max values in the metadata of Parquet files. These statistics are maintained by the underlying data source.\nCa", "question": "How are Spark's estimates of row output determined?", "answers": {"text": ["Apache Spark’s ability to choose the best execution plan among many possible options is determined in part by its estimates of how many rows will be output by every node in the execution plan (read, filter, join, etc.)."], "answer_start": [257]}}
{"context": "s for a table that will be broadcast to all worker nodes when\n      performing a join. By setting this value to -1, broadcasting can be disabled.\n1.1.0\nspark.sql.broadcastTimeout\n300\nTimeout in seconds for the broadcast wait time in broadcast joins\n1.3.0\nJoin Strategy Hints\nThe join strategy hints, namely\nBROADCAST\n,\nMERGE\n,\nSHUFFLE_HASH\nand\nSHUFFLE_REPLICATE_NL\n,\ninstruct Spark to use the hinted strategy on each specified relation when joining them with another\nrelation. For example, when the\nBROADCAST\nhint is used on table ‘t1’, broadcast join (either\nbroadcast hash join or broadcast nested loop join depending on whether there is any equi-join key)\nwith ‘t1’ as the build side will be prioritized by Spark even if the size of table ‘t1’ suggested\nby the statistics is above the configuratio", "question": "What happens when the spark.sql.broadcastTimeout value is set to -1?", "answers": {"text": ["By setting this value to -1, broadcasting can be disabled."], "answer_start": [87]}}
{"context": "e\na specific strategy may not support all join types.\nspark\n.\ntable\n(\n\"\nsrc\n\"\n).\njoin\n(\nspark\n.\ntable\n(\n\"\nrecords\n\"\n).\nhint\n(\n\"\nbroadcast\n\"\n),\n\"\nkey\n\"\n).\nshow\n()\nspark\n.\ntable\n(\n\"src\"\n).\njoin\n(\nspark\n.\ntable\n(\n\"records\"\n).\nhint\n(\n\"broadcast\"\n),\n\"key\"\n).\nshow\n()\nspark\n.\ntable\n(\n\"src\"\n).\njoin\n(\nspark\n.\ntable\n(\n\"records\"\n).\nhint\n(\n\"broadcast\"\n),\n\"key\"\n).\nshow\n();\nsrc\n<-\nsql\n(\n\"SELECT * FROM src\"\n)\nrecords\n<-\nsql\n(\n\"SELECT * FROM records\"\n)\nhead\n(\njoin\n(\nsrc\n,\nhint\n(\nrecords\n,\n\"broadcast\"\n),\nsrc\n$\nkey\n==\nrecords\n$\nkey\n))\n-- We accept BROADCAST, BROADCASTJOIN and MAPJOIN for broadcast hint\nSELECT\n/*+ BROADCAST(r) */\n*\nFROM\nrecords\nr\nJOIN\nsrc\ns\nON\nr\n.\nkey\n=\ns\n.\nkey\nFor more details please refer to the documentation of\nJoin Hints\n.\nAdaptive Query Execution\nAdaptive Query Execution (AQE) is an opt", "question": "What join hints are accepted?", "answers": {"text": ["We accept BROADCAST, BROADCASTJOIN and MAPJOIN for broadcast hint"], "answer_start": [526]}}
{"context": "r\n.\nkey\n=\ns\n.\nkey\nFor more details please refer to the documentation of\nJoin Hints\n.\nAdaptive Query Execution\nAdaptive Query Execution (AQE) is an optimization technique in Spark SQL that makes use of the runtime statistics to choose the most efficient query execution plan, which is enabled by default since Apache Spark 3.2.0. Spark SQL can turn on and off AQE by\nspark.sql.adaptive.enabled\nas an umbrella configuration.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.adaptive.enabled\ntrue\nWhen true, enable adaptive query execution, which re-optimizes the query plan in the middle of query execution, based on accurate runtime statistics.\n1.6.0\nCoalescing Post Shuffle Partitions\nThis feature coalesces the post shuffle partitions based on the map output statistics when both\nspark.sql.adap", "question": "What does the configuration 'spark.sql.adaptive.enabled' control?", "answers": {"text": ["When true, enable adaptive query execution, which re-optimizes the query plan in the middle of query execution, based on accurate runtime statistics."], "answer_start": [499]}}
{"context": ".6.0\nCoalescing Post Shuffle Partitions\nThis feature coalesces the post shuffle partitions based on the map output statistics when both\nspark.sql.adaptive.enabled\nand\nspark.sql.adaptive.coalescePartitions.enabled\nconfigurations are true. This feature simplifies the tuning of shuffle partition number when running queries. You do not need to set a proper shuffle partition number to fit your dataset. Spark can pick the proper shuffle partition number at runtime once you set a large enough initial number of shuffle partitions via\nspark.sql.adaptive.coalescePartitions.initialPartitionNum\nconfiguration.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.adaptive.coalescePartitions.enabled\ntrue\nWhen true and\nspark.sql.adaptive.enabled\nis true, Spark will coalesce contiguous shuffle partitions ", "question": "What happens when both spark.sql.adaptive.enabled and spark.sql.adaptive.coalescePartitions.enabled configurations are true?", "answers": {"text": ["This feature coalesces the post shuffle partitions based on the map output statistics"], "answer_start": [40]}}
{"context": "ion only has an effect when\nspark.sql.adaptive.enabled\nand\nspark.sql.adaptive.coalescePartitions.enabled\nare both enabled.\n3.0.0\nspark.sql.adaptive.advisoryPartitionSizeInBytes\n64 MB\nThe advisory size in bytes of the shuffle partition during adaptive optimization (when\nspark.sql.adaptive.enabled\nis true). It takes effect when Spark coalesces small shuffle partitions or splits skewed shuffle partition.\n3.0.0\nSplitting skewed shuffle partitions\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled\ntrue\nWhen true and\nspark.sql.adaptive.enabled\nis true, Spark will optimize the skewed shuffle partitions in RebalancePartitions and split them to smaller ones according to the target size (specified by\nspark.sql.adaptive.advisoryPartitionSizeInByt", "question": "When does the advisory partition size during adaptive optimization take effect?", "answers": {"text": ["It takes effect when Spark coalesces small shuffle partitions or splits skewed shuffle partition."], "answer_start": [307]}}
{"context": "rk.sql.adaptive.localShuffleReader.enabled\ntrue\nWhen true and\nspark.sql.adaptive.enabled\nis true, Spark tries to use local shuffle reader to read the shuffle data when the shuffle partitioning is not needed, for example, after converting sort-merge join to broadcast-hash join.\n3.0.0\nConverting sort-merge join to shuffled hash join\nAQE converts sort-merge join to shuffled hash join when all post shuffle partitions are smaller than the threshold configured in\nspark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold\n.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold\n0\nConfigures the maximum size in bytes per partition that can be allowed to build local hash map. If this value is not smaller than\nspark.sql.adaptive.advisoryPartitionSizeInBytes\n", "question": "What does Spark attempt to use when both 'spark.sql.adaptive.localShuffleReader.enabled' and 'spark.sql.adaptive.enabled' are true?", "answers": {"text": ["Spark tries to use local shuffle reader to read the shuffle data when the shuffle partitioning is not needed, for example, after converting sort-merge join to broadcast-hash join."], "answer_start": [98]}}
{"context": ".sql.adaptive.skewJoin.enabled\nconfigurations are enabled.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.adaptive.skewJoin.enabled\ntrue\nWhen true and\nspark.sql.adaptive.enabled\nis true, Spark dynamically handles skew in sort-merge join by splitting (and replicating if needed) skewed partitions.\n3.0.0\nspark.sql.adaptive.skewJoin.skewedPartitionFactor\n5.0\nA partition is considered as skewed if its size is larger than this factor multiplying the median partition size and also larger than\nspark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\n.\n3.0.0\nspark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\n256MB\nA partition is considered as skewed if its size in bytes is larger than this threshold and also larger than\nspark.sql.adaptive.skewJoin.skewedPartitionFactor\nmultiplyin", "question": "What happens when both spark.sql.adaptive.skewJoin.enabled and spark.sql.adaptive.enabled are set to true?", "answers": {"text": ["Spark dynamically handles skew in sort-merge join by splitting (and replicating if needed) skewed partitions."], "answer_start": [194]}}
{"context": "sidered as skewed if its size in bytes is larger than this threshold and also larger than\nspark.sql.adaptive.skewJoin.skewedPartitionFactor\nmultiplying the median partition size. Ideally, this config should be set larger than\nspark.sql.adaptive.advisoryPartitionSizeInBytes\n.\n3.0.0\nspark.sql.adaptive.forceOptimizeSkewedJoin\nfalse\nWhen true, force enable OptimizeSkewedJoin, which is an adaptive rule to optimize skewed joins to avoid straggler tasks, even if it introduces extra shuffle.\n3.3.0\nAdvanced Customization\nYou can control the details of how AQE works by providing your own cost evaluator class or by excluding AQE optimizer rules.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.adaptive.optimizer.excludedRules\n(none)\nConfigures a list of rules to be disabled in the adaptive optim", "question": "What does setting 'spark.sql.adaptive.forceOptimizeSkewedJoin' to true do?", "answers": {"text": ["When true, force enable OptimizeSkewedJoin, which is an adaptive rule to optimize skewed joins to avoid straggler tasks, even if it introduces extra shuffle."], "answer_start": [331]}}
{"context": " shuffle phase.\nThis is a generalization of the concept of Bucket Joins, which is only applicable for\nbucketed\ntables, to tables partitioned by functions registered in FunctionCatalog. Storage Partition Joins are currently supported for compatible V2 DataSources.\nThe following SQL properties enable Storage Partition Join in different join queries with various optimizations.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.sources.v2.bucketing.enabled\nfalse\nWhen true, try to eliminate shuffle by using the partitioning reported by a compatible V2 data source.\n3.3.0\nspark.sql.sources.v2.bucketing.pushPartValues.enabled\ntrue\nWhen enabled, try to eliminate shuffle if one side of the join has missing partition values from the other side. This config requires\nspark.sql.sources.v2.bucketing.e", "question": "What does the property 'spark.sql.sources.v2.bucketing.enabled' do when set to true?", "answers": {"text": ["When true, try to eliminate shuffle by using the partitioning reported by a compatible V2 data source."], "answer_start": [466]}}
{"context": "y to eliminate shuffle if one side of the join has missing partition values from the other side. This config requires\nspark.sql.sources.v2.bucketing.enabled\nto be true.\n3.4.0\nspark.sql.requireAllClusterKeysForCoPartition\ntrue\nWhen true, require the join or MERGE keys to be same and in the same order as the partition keys to eliminate shuffle. Hence, set to\nfalse\nin this situation to eliminate shuffle.\n3.4.0\nspark.sql.sources.v2.bucketing.partiallyClusteredDistribution.enabled\nfalse\nWhen true, and when the join is not a full outer join, enable skew optimizations to handle partitions with large amounts of data when avoiding shuffle. One side will be chosen as the big table based on table statistics, and the splits on this side will be partially-clustered. The splits of the other side will be", "question": "What does setting spark.sql.requireAllClusterKeysForCoPartition to true require?", "answers": {"text": ["When true, require the join or MERGE keys to be same and in the same order as the partition keys to eliminate shuffle."], "answer_start": [226]}}
{"context": "ll be chosen as the big table based on table statistics, and the splits on this side will be partially-clustered. The splits of the other side will be grouped and replicated to match. This config requires both\nspark.sql.sources.v2.bucketing.enabled\nand\nspark.sql.sources.v2.bucketing.pushPartValues.enabled\nto be true.\n3.4.0\nspark.sql.sources.v2.bucketing.allowJoinKeysSubsetOfPartitionKeys.enabled\nfalse\nWhen enabled, try to avoid shuffle if join or MERGE condition does not include all partition columns. This config requires both\nspark.sql.sources.v2.bucketing.enabled\nand\nspark.sql.sources.v2.bucketing.pushPartValues.enabled\nto be true, and\nspark.sql.requireAllClusterKeysForCoPartition\nto be false.\n4.0.0\nspark.sql.sources.v2.bucketing.allowCompatibleTransforms.enabled\nfalse\nWhen enabled, try ", "question": "What configurations are required for `spark.sql.sources.v2.bucketing.allowJoinKeysSubsetOfPartitionKeys.enabled` to function?", "answers": {"text": ["This config requires both\nspark.sql.sources.v2.bucketing.enabled\nand\nspark.sql.sources.v2.bucketing.pushPartValues.enabled\nto be true, and\nspark.sql.requireAllClusterKeysForCoPartition\nto be false."], "answer_start": [507]}}
{"context": "k.sql.requireAllClusterKeysForCoPartition\nto be false.\n4.0.0\nspark.sql.sources.v2.bucketing.allowCompatibleTransforms.enabled\nfalse\nWhen enabled, try to avoid shuffle if partition transforms are compatible but not identical. This config requires both\nspark.sql.sources.v2.bucketing.enabled\nand\nspark.sql.sources.v2.bucketing.pushPartValues.enabled\nto be true.\n4.0.0\nspark.sql.sources.v2.bucketing.shuffle.enabled\nfalse\nWhen enabled, try to avoid shuffle on one side of the join, by recognizing the partitioning reported by a V2 data source on the other side.\n4.0.0\nIf Storage Partition Join is performed, the query plan will not contain Exchange nodes prior to the join.\nThe following example uses Iceberg (\nhttps://iceberg.apache.org/docs/latest/spark-getting-started/\n), a Spark V2 DataSource that ", "question": "What is the effect of enabling spark.sql.sources.v2.bucketing.allowCompatibleTransforms.enabled?", "answers": {"text": ["When enabled, try to avoid shuffle if partition transforms are compatible but not identical."], "answer_start": [132]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMonitoring and Instrumentation\nWeb Interfaces\nViewing After the Fact\nEnvironment Variables\nApplying compaction on rolling event log files\nSpark History Server Configuration Options\nREST API\nExecutor Task Metrics\nExecutor Metrics\nAPI Versioning Policy\n", "question": "What are some of the programming guides available for Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)"], "answer_start": [46]}}
{"context": "ompaction on rolling event log files\nSpark History Server Configuration Options\nREST API\nExecutor Task Metrics\nExecutor Metrics\nAPI Versioning Policy\nMetrics\nList of available metrics providers\nComponent instance = Driver\nComponent instance = Executor\nSource = JVM Source\nComponent instance = applicationMaster\nComponent instance = master\nComponent instance = ApplicationSource\nComponent instance = worker\nComponent instance = shuffleService\nAdvanced Instrumentation\nThere are several ways to monitor Spark applications: web UIs, metrics, and external instrumentation.\nWeb Interfaces\nEvery SparkContext launches a\nWeb UI\n, by default on port 4040, that\ndisplays useful information about the application. This includes:\nA list of scheduler stages and tasks\nA summary of RDD sizes and memory usage\nEnvi", "question": "What is launched by every SparkContext by default?", "answers": {"text": ["Web UI"], "answer_start": [614]}}
{"context": "rk to log Spark events that encode the information displayed\nin the UI to persisted storage.\nViewing After the Fact\nIt is still possible to construct the UI of an application through Spark’s history server,\nprovided that the application’s event logs exist.\nYou can start the history server by executing:\n./sbin/start-history-server.sh\nThis creates a web interface at\nhttp://<server-url>:18080\nby default, listing incomplete\nand completed applications and attempts.\nWhen using the file-system provider class (see\nspark.history.provider\nbelow), the base logging\ndirectory must be supplied in the\nspark.history.fs.logDirectory\nconfiguration option,\nand should contain sub-directories that each represents an application’s event logs.\nThe spark jobs themselves must be configured to log events, and to lo", "question": "How can you start the Spark history server?", "answers": {"text": ["./sbin/start-history-server.sh"], "answer_start": [304]}}
{"context": "should contain sub-directories that each represents an application’s event logs.\nThe spark jobs themselves must be configured to log events, and to log them to the same shared,\nwritable directory. For example, if the server was configured with a log directory of\nhdfs://namenode/shared/spark-logs\n, then the client-side options would be:\nspark.eventLog.enabled true\nspark.eventLog.dir hdfs://namenode/shared/spark-logs\nThe history server can be configured as follows:\nEnvironment Variables\nEnvironment Variable\nMeaning\nSPARK_DAEMON_MEMORY\nMemory to allocate to the history server (default: 1g).\nSPARK_DAEMON_JAVA_OPTS\nJVM options for the history server (default: none).\nSPARK_DAEMON_CLASSPATH\nClasspath for the history server (default: none).\nSPARK_PUBLIC_DNS\nThe public address for the history serve", "question": "What is the default memory allocation for the history server?", "answers": {"text": ["Memory to allocate to the history server (default: 1g)."], "answer_start": [539]}}
{"context": "ng\nspark.eventLog.rolling.enabled\nand\nspark.eventLog.rolling.maxFileSize\nwould\nlet you have rolling event log files instead of single huge event log file which may help some scenarios on its own,\nbut it still doesn’t help you reducing the overall size of logs.\nSpark History Server can apply compaction on the rolling event log files to reduce the overall size of\nlogs, via setting the configuration\nspark.history.fs.eventLog.rolling.maxFilesToRetain\non the\nSpark History Server.\nDetails will be described below, but please note in prior that compaction is LOSSY operation.\nCompaction will discard some events which will be no longer seen on UI - you may want to check which events will be discarded\nbefore enabling the option.\nWhen the compaction happens, the History Server lists all the available ", "question": "What configuration setting on the Spark History Server allows compaction on rolling event log files to reduce overall log size?", "answers": {"text": ["spark.history.fs.eventLog.rolling.maxFilesToRetain"], "answer_start": [400]}}
{"context": "event log files if figures out not a lot of space\nwould be reduced during compaction. For streaming query we normally expect compaction\nwill run as each micro-batch will trigger one or more jobs which will be finished shortly, but compaction won’t run\nin many cases for batch query.\nPlease also note that this is a new feature introduced in Spark 3.0, and may not be completely stable. Under some circumstances,\nthe compaction may exclude more events than you expect, leading some UI issues on History Server for the application.\nUse it with caution.\nSpark History Server Configuration Options\nSecurity options for the Spark History Server are covered more detail in the\nSecurity\npage.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.history.provider\norg.apache.spark.deploy.history.FsHistoryProvid", "question": "What is the default value for the property 'spark.history.provider'?", "answers": {"text": ["org.apache.spark.deploy.history.FsHistoryProvid"], "answer_start": [753]}}
{"context": "d more detail in the\nSecurity\npage.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.history.provider\norg.apache.spark.deploy.history.FsHistoryProvider\nName of the class implementing the application history backend. Currently there is only\n    one implementation, provided by Spark, which looks for application logs stored in the\n    file system.\n1.1.0\nspark.history.fs.logDirectory\nfile:/tmp/spark-events\nFor the filesystem history provider, the URL to the directory containing application event\n    logs to load. This can be a local\nfile://\npath,\n    an HDFS path\nhdfs://namenode/shared/spark-logs\nor that of an alternative filesystem supported by the Hadoop APIs.\n1.1.0\nspark.history.fs.update.interval\n10s\nThe period at which the filesystem history provider checks for new or\n      updated logs ", "question": "What is the default value for the property spark.history.fs.logDirectory?", "answers": {"text": ["file:/tmp/spark-events"], "answer_start": [384]}}
{"context": "in the cache,\n      it will have to be loaded from disk if it is accessed from the UI.\n1.0.0\nspark.history.ui.maxApplications\nInt.MaxValue\nThe number of applications to display on the history summary page. Application UIs are still\n      available by accessing their URLs directly even if they are not displayed on the history summary page.\n2.0.1\nspark.history.ui.port\n18080\nThe port to which the web interface of the history server binds.\n1.0.0\nspark.history.kerberos.enabled\nfalse\nIndicates whether the history server should use kerberos to login. This is required\n      if the history server is accessing HDFS files on a secure Hadoop cluster.\n1.0.1\nspark.history.kerberos.principal\n(none)\nWhen\nspark.history.kerberos.enabled=true\n, specifies kerberos principal name for the History Server.\n1.0.1\n", "question": "What is the default value for spark.history.ui.port?", "answers": {"text": ["18080"], "answer_start": [369]}}
{"context": "vent log directory.\n      Spark tries to clean up the completed attempt logs to maintain the log directory under this limit.\n      This should be smaller than the underlying file system limit like\n      `dfs.namenode.fs-limits.max-directory-items` in HDFS.\n3.0.0\nspark.history.fs.endEventReparseChunkSize\n1m\nHow many bytes to parse at the end of log files looking for the end event.\n      This is used to speed up generation of application listings by skipping unnecessary\n      parts of event log files. It can be disabled by setting this config to 0.\n2.4.0\nspark.history.fs.inProgressOptimization.enabled\ntrue\nEnable optimized handling of in-progress logs. This option may leave finished\n      applications that fail to rename their event logs listed as in-progress.\n2.4.0\nspark.history.fs.driverlo", "question": "What does spark.history.fs.endEventReparseChunkSize configure?", "answers": {"text": ["How many bytes to parse at the end of log files looking for the end event."], "answer_start": [308]}}
{"context": "ss logs. This option may leave finished\n      applications that fail to rename their event logs listed as in-progress.\n2.4.0\nspark.history.fs.driverlog.cleaner.enabled\nspark.history.fs.cleaner.enabled\nSpecifies whether the History Server should periodically clean up driver logs from storage.\n3.0.0\nspark.history.fs.driverlog.cleaner.interval\nspark.history.fs.cleaner.interval\nWhen\nspark.history.fs.driverlog.cleaner.enabled=true\n, specifies how often the filesystem driver log cleaner checks for files to delete.\n      Files are only deleted if they are older than\nspark.history.fs.driverlog.cleaner.maxAge\n3.0.0\nspark.history.fs.driverlog.cleaner.maxAge\nspark.history.fs.cleaner.maxAge\nWhen\nspark.history.fs.driverlog.cleaner.enabled=true\n, driver log files older than this will be deleted when the", "question": "What condition must be met for the filesystem driver log cleaner to check for files to delete?", "answers": {"text": ["spark.history.fs.driverlog.cleaner.enabled=true"], "answer_start": [382]}}
{"context": "axAge\nspark.history.fs.cleaner.maxAge\nWhen\nspark.history.fs.driverlog.cleaner.enabled=true\n, driver log files older than this will be deleted when the driver log cleaner runs.\n3.0.0\nspark.history.fs.numReplayThreads\n25% of available cores\nNumber of threads that will be used by history server to process event logs.\n2.0.0\nspark.history.store.maxDiskUsage\n10g\nMaximum disk usage for the local directory where the cache application history information\n      are stored.\n2.3.0\nspark.history.store.path\n(none)\nLocal directory where to cache application history data. If set, the history\n        server will store application data on disk instead of keeping it in memory. The data\n        written to disk will be re-used in the event of a history server restart.\n2.3.0\nspark.history.store.serializer\nJSON\n", "question": "What is the maximum disk usage for the local directory where the cache application history information are stored?", "answers": {"text": ["10g"], "answer_start": [355]}}
{"context": "ng it in memory. The data\n        written to disk will be re-used in the event of a history server restart.\n2.3.0\nspark.history.store.serializer\nJSON\nSerializer for writing/reading in-memory UI objects to/from disk-based KV Store; JSON or PROTOBUF.\n        JSON serializer is the only choice before Spark 3.4.0, thus it is the default value.\n        PROTOBUF serializer is fast and compact, compared to the JSON serializer.\n3.4.0\nspark.history.custom.executor.log.url\n(none)\nSpecifies custom spark executor log URL for supporting external log service instead of using cluster\n        managers' application log URLs in the history server. Spark will support some path variables via patterns\n        which can vary on cluster manager. Please check the documentation for your cluster manager to\n        ", "question": "What are the available serializer options for writing/reading in-memory UI objects to/from disk-based KV Store?", "answers": {"text": ["JSON or PROTOBUF."], "answer_start": [231]}}
{"context": "upport some path variables via patterns\n        which can vary on cluster manager. Please check the documentation for your cluster manager to\n        see which patterns are supported, if any. This configuration has no effect on a live application, it only\n        affects the history server.\nFor now, only YARN mode supports this configuration\n3.0.0\nspark.history.custom.executor.log.url.applyIncompleteApplication\ntrue\nSpecifies whether to apply custom spark executor log URL to incomplete applications as well.\n        If executor logs for running applications should be provided as origin log URLs, set this to `false`.\n        Please note that incomplete applications may include applications which didn't shutdown gracefully.\n        Even this is set to `true`, this configuration has no effect ", "question": "What does setting `spark.history.custom.executor.log.url.applyIncompleteApplication` to `false` do?", "answers": {"text": ["If executor logs for running applications should be provided as origin log URLs, set this to `false`."], "answer_start": [521]}}
{"context": "ng event logs. HybridStore will first write data\n      to an in-memory store and having a background thread that dumps data to a disk store after the writing\n      to in-memory store is completed.\n3.1.0\nspark.history.store.hybridStore.maxMemoryUsage\n2g\nMaximum memory space that can be used to create HybridStore. The HybridStore co-uses the heap memory,\n      so the heap memory should be increased through the memory option for SHS if the HybridStore is enabled.\n3.1.0\nspark.history.store.hybridStore.diskBackend\nROCKSDB\nSpecifies a disk-based store used in hybrid store; ROCKSDB or LEVELDB (deprecated).\n3.3.0\nspark.history.fs.update.batchSize\nInt.MaxValue\nSpecifies the batch size for updating new eventlog files.\n      This controls each scan process to be completed within a reasonable time, an", "question": "What are the possible disk-based stores used in a hybrid store?", "answers": {"text": ["ROCKSDB or LEVELDB (deprecated)."], "answer_start": [574]}}
{"context": ".MaxValue\nSpecifies the batch size for updating new eventlog files.\n      This controls each scan process to be completed within a reasonable time, and such\n      prevent the initial scan from running too long and blocking new eventlog files to\n      be scanned in time in large environments.\n3.4.0\nNote that in all of these UIs, the tables are sortable by clicking their headers,\nmaking it easy to identify slow tasks, data skew, etc.\nNote\nThe history server displays both completed and incomplete Spark jobs. If an application makes\nmultiple attempts after failures, the failed attempts will be displayed, as well as any ongoing\nincomplete attempt or the final successful attempt.\nIncomplete applications are only updated intermittently. The time between updates is defined\nby the interval between ", "question": "What does the MaxValue setting specify?", "answers": {"text": ["Specifies the batch size for updating new eventlog files."], "answer_start": [10]}}
{"context": "hon using the\nwith SparkContext() as sc:\nconstruct\nto handle the Spark Context setup and tear down.\nREST API\nIn addition to viewing the metrics in the UI, they are also available as JSON.  This gives developers\nan easy way to create new visualizations and monitoring tools for Spark.  The JSON is available for\nboth running applications, and in the history server.  The endpoints are mounted at\n/api/v1\n.  For example,\nfor the history server, they would typically be accessible at\nhttp://<server-url>:18080/api/v1\n, and\nfor a running application, at\nhttp://localhost:4040/api/v1\n.\nIn the API, an application is referenced by its application ID,\n[app-id]\n.\nWhen running on YARN, each application may have multiple attempts, but there are attempt IDs\nonly for applications in cluster mode, not applicat", "question": "Where are the REST API endpoints mounted?", "answers": {"text": ["/api/v1"], "answer_start": [395]}}
{"context": "=[date]\nearliest end date/time to list.\n?maxEndDate=[date]\nlatest end date/time to list.\n?limit=[limit]\nlimits the number of applications listed.\nExamples:\n?minDate=2015-02-10\n?minDate=2015-02-03T16:42:40.000GMT\n?maxDate=2015-02-11T20:41:30.000GMT\n?minEndDate=2015-02-12\n?minEndDate=2015-02-12T09:15:10.000GMT\n?maxEndDate=2015-02-14T16:30:45.000GMT\n?limit=10\n/applications/[app-id]/jobs\nA list of all jobs for a given application.\n?status=[running|succeeded|failed|unknown]\nlist only jobs in the specific state.\n/applications/[app-id]/jobs/[job-id]\nDetails for the given job.\n/applications/[app-id]/stages\nA list of all stages for a given application.\n?status=[active|complete|pending|failed]\nlist only stages in the given state.\n?details=true\nlists all stages with the task data.\n?taskStatus=[RUNNIN", "question": "What does the ?limit parameter do?", "answers": {"text": ["limits the number of applications listed."], "answer_start": [104]}}
{"context": "ery parameter quantiles takes effect only when\nwithSummaries=true\n. Default value is\n0.0,0.25,0.5,0.75,1.0\n.\n/applications/[app-id]/stages/[stage-id]\nA list of all attempts for the given stage.\n?details=true\nlists all attempts with the task data for the given stage.\n?taskStatus=[RUNNING|SUCCESS|FAILED|KILLED|PENDING]\nlists only those tasks with the specified task status. Query parameter taskStatus takes effect only when\ndetails=true\n. This also supports multiple\ntaskStatus\nsuch as\n?details=true&taskStatus=SUCCESS&taskStatus=FAILED\nwhich will return all tasks matching any of specified task status.\n?withSummaries=true\nlists task metrics distribution and executor metrics distribution of each attempt.\n?quantiles=0.0,0.25,0.5,0.75,1.0\nsummarize the metrics with the given quantiles. Query parame", "question": "When does the query parameter quantiles take effect?", "answers": {"text": ["ery parameter quantiles takes effect only when\nwithSummaries=true"], "answer_start": [0]}}
{"context": "ution and executor metrics distribution of each attempt.\n?quantiles=0.0,0.25,0.5,0.75,1.0\nsummarize the metrics with the given quantiles. Query parameter quantiles takes effect only when\nwithSummaries=true\n. Default value is\n0.0,0.25,0.5,0.75,1.0\n.\nExample:\n?details=true\n?details=true&taskStatus=RUNNING\n?withSummaries=true\n?details=true&withSummaries=true&quantiles=0.01,0.5,0.99\n/applications/[app-id]/stages/[stage-id]/[stage-attempt-id]\nDetails for the given stage attempt.\n?details=true\nlists all task data for the given stage attempt.\n?taskStatus=[RUNNING|SUCCESS|FAILED|KILLED|PENDING]\nlists only those tasks with the specified task status. Query parameter taskStatus takes effect only when\ndetails=true\n. This also supports multiple\ntaskStatus\nsuch as\n?details=true&taskStatus=SUCCESS&taskSt", "question": "What is the default value for the quantiles parameter?", "answers": {"text": ["0.0,0.25,0.5,0.75,1.0"], "answer_start": [68]}}
{"context": "iles=0.01,0.5,0.99\n/applications/[app-id]/stages/[stage-id]/[stage-attempt-id]/taskSummary\nSummary metrics of all tasks in the given stage attempt.\n?quantiles\nsummarize the metrics with the given quantiles.\nExample:\n?quantiles=0.01,0.5,0.99\n/applications/[app-id]/stages/[stage-id]/[stage-attempt-id]/taskList\nA list of all tasks for the given stage attempt.\n?offset=[offset]&length=[len]\nlist tasks in the given range.\n?sortBy=[runtime|-runtime]\nsort the tasks.\n?status=[running|success|killed|failed|unknown]\nlist only tasks in the state.\nExample:\n?offset=10&length=50&sortBy=runtime&status=running\n/applications/[app-id]/executors\nA list of all active executors for the given application.\n/applications/[app-id]/executors/[executor-id]/threads\nStack traces of all the threads running within the gi", "question": "What parameters can be used with the /applications/[app-id]/stages/[stage-id]/[stage-attempt-id]/taskList endpoint to list tasks in a given range?", "answers": {"text": ["?offset=[offset]&length=[len]"], "answer_start": [359]}}
{"context": "tive executors for the given application.\n/applications/[app-id]/executors/[executor-id]/threads\nStack traces of all the threads running within the given active executor.\n      Not available via the history server.\n/applications/[app-id]/allexecutors\nA list of all(active and dead) executors for the given application.\n/applications/[app-id]/storage/rdd\nA list of stored RDDs for the given application.\n/applications/[app-id]/storage/rdd/[rdd-id]\nDetails for the storage status of a given RDD.\n/applications/[base-app-id]/logs\nDownload the event logs for all attempts of the given application as files within\n    a zip file.\n/applications/[base-app-id]/[attempt-id]/logs\nDownload the event logs for a specific application attempt as a zip file.\n/applications/[app-id]/streaming/statistics\nStatistics ", "question": "What does the endpoint /applications/[app-id]/storage/rdd/[rdd-id] provide?", "answers": {"text": ["Details for the storage status of a given RDD."], "answer_start": [447]}}
{"context": "d]/[attempt-id]/logs\nDownload the event logs for a specific application attempt as a zip file.\n/applications/[app-id]/streaming/statistics\nStatistics for the streaming context.\n/applications/[app-id]/streaming/receivers\nA list of all streaming receivers.\n/applications/[app-id]/streaming/receivers/[stream-id]\nDetails of the given receiver.\n/applications/[app-id]/streaming/batches\nA list of all retained batches.\n/applications/[app-id]/streaming/batches/[batch-id]\nDetails of the given batch.\n/applications/[app-id]/streaming/batches/[batch-id]/operations\nA list of all output operations of the given batch.\n/applications/[app-id]/streaming/batches/[batch-id]/operations/[outputOp-id]\nDetails of the given operation and given batch.\n/applications/[app-id]/sql\nA list of all queries for a given appli", "question": "What does the endpoint /applications/[app-id]/streaming/statistics provide?", "answers": {"text": ["Statistics for the streaming context."], "answer_start": [139]}}
{"context": "[batch-id]/operations/[outputOp-id]\nDetails of the given operation and given batch.\n/applications/[app-id]/sql\nA list of all queries for a given application.\n?details=[true (default) | false]\nlists/hides details of Spark plan nodes.\n?planDescription=[true (default) | false]\nenables/disables Physical\nplanDescription\non demand when Physical Plan size is high.\n?offset=[offset]&length=[len]\nlists queries in the given range.\n/applications/[app-id]/sql/[execution-id]\nDetails for the given query.\n?details=[true (default) | false]\nlists/hides metric details in addition to given query details.\n?planDescription=[true (default) | false]\nenables/disables Physical\nplanDescription\non demand for the given query when Physical Plan size is high.\n/applications/[app-id]/environment\nEnvironment details of the", "question": "What does the parameter '?details=[true (default) | false]' do when used with '/applications/[app-id]/sql'?", "answers": {"text": ["lists/hides details of Spark plan nodes."], "answer_start": [192]}}
{"context": "trics\nThe REST API exposes the values of the Task Metrics collected by Spark executors with the granularity\nof task execution. The metrics can be used for performance troubleshooting and workload characterization.\nA list of the available metrics, with a short description:\nSpark Executor Task Metric name\nShort description\nexecutorRunTime\nElapsed time the executor spent running this task. This includes time fetching shuffle data.\n    The value is expressed in milliseconds.\nexecutorCpuTime\nCPU time the executor spent running this task. This includes time fetching shuffle data.\n    The value is expressed in nanoseconds.\nexecutorDeserializeTime\nElapsed time spent to deserialize this task. The value is expressed in milliseconds.\nexecutorDeserializeCpuTime\nCPU time taken on the executor to deseri", "question": "Em quais unidades o valor de executorRunTime é expresso?", "answers": {"text": ["The value is expressed in milliseconds."], "answer_start": [436]}}
{"context": "apsed time spent to deserialize this task. The value is expressed in milliseconds.\nexecutorDeserializeCpuTime\nCPU time taken on the executor to deserialize this task. The value is expressed\n    in nanoseconds.\nresultSize\nThe number of bytes this task transmitted back to the driver as the TaskResult.\njvmGCTime\nElapsed time the JVM spent in garbage collection while executing this task.\n    The value is expressed in milliseconds.\nConcurrentGCCount\nThis metric returns the total number of collections that have occurred.\n        It only applies when the Java Garbage collector is G1 Concurrent GC.\nConcurrentGCTime\nThis metric returns the approximate accumulated collection elapsed time in milliseconds.\n        It only applies when the Java Garbage collector is G1 Concurrent GC.\nresultSerialization", "question": "What unit is used to express the value of 'jvmGCTime'?", "answers": {"text": ["The value is expressed in milliseconds."], "answer_start": [43]}}
{"context": "QL jobs, this only tracks all\n         unsafe operators and ExternalSort.\ninputMetrics.*\nMetrics related to reading data from\norg.apache.spark.rdd.HadoopRDD\nor from persisted data.\n.bytesRead\nTotal number of bytes read.\n.recordsRead\nTotal number of records read.\noutputMetrics.*\nMetrics related to writing data externally (e.g. to a distributed filesystem),\n    defined only in tasks with output.\n.bytesWritten\nTotal number of bytes written\n.recordsWritten\nTotal number of records written\nshuffleReadMetrics.*\nMetrics related to shuffle read operations.\n.recordsRead\nNumber of records read in shuffle operations\n.remoteBlocksFetched\nNumber of remote blocks fetched in shuffle operations\n.localBlocksFetched\nNumber of local (as opposed to read from a remote executor) blocks fetched\n    in shuffle ope", "question": "What does .bytesRead represent?", "answers": {"text": ["Total number of bytes read."], "answer_start": [192]}}
{"context": "e read operations, as opposed to\n    being read into memory, which is the default behavior.\n.fetchWaitTime\nTime the task spent waiting for remote shuffle blocks.\n        This only includes the time blocking on shuffle input data.\n        For instance if block B is being fetched while the task is still not finished\n        processing block A, it is not considered to be blocking on block B.\n        The value is expressed in milliseconds.\nshuffleWriteMetrics.*\nMetrics related to operations writing shuffle data.\n.bytesWritten\nNumber of bytes written in shuffle operations\n.recordsWritten\nNumber of records written in shuffle operations\n.writeTime\nTime spent blocking on writes to disk or buffer cache. The value is expressed\n     in nanoseconds.\nExecutor Metrics\nExecutor-level metrics are sent fro", "question": "What unit is used to express the .fetchWaitTime value?", "answers": {"text": ["The value is expressed in milliseconds."], "answer_start": [400]}}
{"context": "ime spent blocking on writes to disk or buffer cache. The value is expressed\n     in nanoseconds.\nExecutor Metrics\nExecutor-level metrics are sent from each executor to the driver as part of the Heartbeat to describe the performance metrics of Executor itself like JVM heap memory, GC information.\nExecutor metric values and their measured memory peak values per executor are exposed via the REST API in JSON format and in Prometheus format.\nThe JSON end point is exposed at:\n/applications/[app-id]/executors\n, and the Prometheus endpoint at:\n/metrics/executors/prometheus\n.\nIn addition, aggregated per-stage peak values of the executor memory metrics are written to the event log if\nspark.eventLog.logStageExecutorMetrics\nis true.\nExecutor memory metrics are also exposed via the Spark metrics syste", "question": "Where are executor metric values exposed via the REST API?", "answers": {"text": ["/applications/[app-id]/executors"], "answer_start": [476]}}
{"context": "s are written to the event log if\nspark.eventLog.logStageExecutorMetrics\nis true.\nExecutor memory metrics are also exposed via the Spark metrics system based on the\nDropwizard metrics library\n.\nA list of the available metrics, with a short description:\nExecutor Level Metric name\nShort description\nrddBlocks\nRDD blocks in the block manager of this executor.\nmemoryUsed\nStorage memory used by this executor.\ndiskUsed\nDisk space used for RDD storage by this executor.\ntotalCores\nNumber of cores available in this executor.\nmaxTasks\nMaximum number of tasks that can run concurrently in this executor.\nactiveTasks\nNumber of tasks currently executing.\nfailedTasks\nNumber of tasks that have failed in this executor.\ncompletedTasks\nNumber of tasks that have completed in this executor.\ntotalTasks\nTotal numb", "question": "What metric represents the number of tasks currently executing?", "answers": {"text": ["activeTasks\nNumber of tasks currently executing."], "answer_start": [598]}}
{"context": "ledTasks\nNumber of tasks that have failed in this executor.\ncompletedTasks\nNumber of tasks that have completed in this executor.\ntotalTasks\nTotal number of tasks (running, failed and completed) in this executor.\ntotalDuration\nElapsed time the JVM spent executing tasks in this executor.\n    The value is expressed in milliseconds.\ntotalGCTime\nElapsed time the JVM spent in garbage collection summed in this executor.\n    The value is expressed in milliseconds.\ntotalInputBytes\nTotal input bytes summed in this executor.\ntotalShuffleRead\nTotal shuffle read bytes summed in this executor.\ntotalShuffleWrite\nTotal shuffle write bytes summed in this executor.\nmaxMemory\nTotal amount of memory available for storage, in bytes.\nmemoryMetrics.*\nCurrent value of memory metrics:\n.usedOnHeapStorageMemory\nUsed", "question": "In what unit is the total duration and total GC time expressed?", "answers": {"text": ["The value is expressed in milliseconds."], "answer_start": [291]}}
{"context": "utor.\nmaxMemory\nTotal amount of memory available for storage, in bytes.\nmemoryMetrics.*\nCurrent value of memory metrics:\n.usedOnHeapStorageMemory\nUsed on heap memory currently for storage, in bytes.\n.usedOffHeapStorageMemory\nUsed off heap memory currently for storage, in bytes.\n.totalOnHeapStorageMemory\nTotal available on heap memory for storage, in bytes. This amount can vary over time,  on the MemoryManager implementation.\n.totalOffHeapStorageMemory\nTotal available off heap memory for storage, in bytes. This amount can vary over time, depending on the MemoryManager implementation.\npeakMemoryMetrics.*\nPeak value of memory (and GC) metrics:\n.JVMHeapMemory\nPeak memory usage of the heap that is used for object allocation.\n    The heap consists of one or more memory pools. The used and commit", "question": "What does '.usedOnHeapStorageMemory' represent?", "answers": {"text": ["Used on heap memory currently for storage, in bytes."], "answer_start": [146]}}
{"context": "JVMHeapMemory\nPeak memory usage of the heap that is used for object allocation.\n    The heap consists of one or more memory pools. The used and committed size of the returned memory usage is the sum of those values of all heap memory pools whereas the init and max size of the returned memory usage represents the setting of the heap memory which may not be the sum of those of all heap memory pools.\n    The amount of used memory in the returned memory usage is the amount of memory occupied by both live objects and garbage objects that have not been collected, if any.\n.JVMOffHeapMemory\nPeak memory usage of non-heap memory that is used by the Java virtual machine. The non-heap memory consists of one or more memory pools. The used and committed size of the returned memory usage is the sum of th", "question": "What does the used memory in the returned memory usage represent?", "answers": {"text": ["The amount of used memory in the returned memory usage is the amount of memory occupied by both live objects and garbage objects that have not been collected, if any."], "answer_start": [405]}}
{"context": "a virtual machine. The non-heap memory consists of one or more memory pools. The used and committed size of the returned memory usage is the sum of those values of all non-heap memory pools whereas the init and max size of the returned memory usage represents the setting of the non-heap memory which may not be the sum of those of all non-heap memory pools.\n.OnHeapExecutionMemory\nPeak on heap execution memory in use, in bytes.\n.OffHeapExecutionMemory\nPeak off heap execution memory in use, in bytes.\n.OnHeapStorageMemory\nPeak on heap storage memory in use, in bytes.\n.OffHeapStorageMemory\nPeak off heap storage memory in use, in bytes.\n.OnHeapUnifiedMemory\nPeak on heap memory (execution and storage).\n.OffHeapUnifiedMemory\nPeak off heap memory (execution and storage).\n.DirectPoolMemory\nPeak memo", "question": "What does the returned memory usage's init and max size represent?", "answers": {"text": ["the setting of the non-heap memory which may not be the sum of those of all non-heap memory pools"], "answer_start": [260]}}
{"context": "is does not\n      include pages which have not been demand-loaded in,\n      or which are swapped out. Enabled if spark.executor.processTreeMetrics.enabled is true.\n.ProcessTreePythonVMemory\nVirtual memory size for Python in bytes. Enabled if spark.executor.processTreeMetrics.enabled is true.\n.ProcessTreePythonRSSMemory\nResident Set Size for Python. Enabled if spark.executor.processTreeMetrics.enabled is true.\n.ProcessTreeOtherVMemory\nVirtual memory size for other kind of process in bytes. Enabled if spark.executor.processTreeMetrics.enabled is true.\n.ProcessTreeOtherRSSMemory\nResident Set Size for other kind of process. Enabled if spark.executor.processTreeMetrics.enabled is true.\n.MinorGCCount\nTotal minor GC count. For example, the garbage collector is one of     Copy, PS Scavenge, ParNew", "question": "When are the metrics .ProcessTreePythonVMemory, .ProcessTreePythonRSSMemory, .ProcessTreeOtherVMemory, and .ProcessTreeOtherRSSMemory enabled?", "answers": {"text": ["Enabled if spark.executor.processTreeMetrics.enabled is true."], "answer_start": [102]}}
{"context": "tor.processTreeMetrics.enabled is true.\n.MinorGCCount\nTotal minor GC count. For example, the garbage collector is one of     Copy, PS Scavenge, ParNew, G1 Young Generation and so on.\n.MinorGCTime\nElapsed total minor GC time.\n    The value is expressed in milliseconds.\n.MajorGCCount\nTotal major GC count. For example, the garbage collector is one of     MarkSweepCompact, PS MarkSweep, ConcurrentMarkSweep, G1 Old Generation and so on.\n.MajorGCTime\nElapsed total major GC time.\n    The value is expressed in milliseconds.\nThe computation of RSS and Vmem are based on\nproc(5)\nAPI Versioning Policy\nThese endpoints have been strongly versioned to make it easier to develop applications on top.\n In particular, Spark guarantees:\nEndpoints will never be removed from one version\nIndividual fields will ne", "question": "What is the unit of measurement for MinorGCTime and MajorGCTime?", "answers": {"text": ["The value is expressed in milliseconds."], "answer_start": [229]}}
{"context": "it easier to develop applications on top.\n In particular, Spark guarantees:\nEndpoints will never be removed from one version\nIndividual fields will never be removed for any given endpoint\nNew endpoints may be added\nNew fields may be added to existing endpoints\nNew versions of the api may be added in the future as a separate endpoint (e.g.,\napi/v2\n).  New versions are\nnot\nrequired to be backwards compatible.\nApi versions may be dropped, but only after at least one minor release of co-existing with a new api version.\nNote that even when examining the UI of running applications, the\napplications/[app-id]\nportion is\nstill required, though there is only one application available.  E.g. to see the list of jobs for the\nrunning app, you would go to\nhttp://localhost:4040/api/v1/applications/[app-id", "question": "What is guaranteed by Spark regarding endpoints?", "answers": {"text": ["Endpoints will never be removed from one version"], "answer_start": [76]}}
{"context": "s only one application available.  E.g. to see the list of jobs for the\nrunning app, you would go to\nhttp://localhost:4040/api/v1/applications/[app-id]/jobs\n.  This is to\nkeep the paths consistent in both modes.\nMetrics\nSpark has a configurable metrics system based on the\nDropwizard Metrics Library\n.\nThis allows users to report Spark metrics to a variety of sinks including HTTP, JMX, and CSV\nfiles. The metrics are generated by sources embedded in the Spark code base. They\nprovide instrumentation for specific activities and Spark components.\nThe metrics system is configured via a configuration file that Spark expects to be present\nat\n$SPARK_HOME/conf/metrics.properties\n. A custom file location can be specified via the\nspark.metrics.conf\nconfiguration property\n.\nInstead of using the configur", "question": "Where does Spark expect the metrics configuration file to be located?", "answers": {"text": ["$SPARK_HOME/conf/metrics.properties"], "answer_start": [641]}}
{"context": "ME/conf/metrics.properties\n. A custom file location can be specified via the\nspark.metrics.conf\nconfiguration property\n.\nInstead of using the configuration file, a set of configuration parameters with prefix\nspark.metrics.conf.\ncan be used.\nBy default, the root namespace used for driver or executor metrics is\nthe value of\nspark.app.id\n. However, often times, users want to be able to track the metrics\nacross apps for driver and executors, which is hard to do with application ID\n(i.e.\nspark.app.id\n) since it changes with every invocation of the app. For such use cases,\na custom namespace can be specified for metrics reporting using\nspark.metrics.namespace\nconfiguration property.\nIf, say, users wanted to set the metrics namespace to the name of the application, they\ncan set the\nspark.metrics.", "question": "How can a custom namespace be specified for metrics reporting?", "answers": {"text": ["a custom namespace can be specified for metrics reporting using\nspark.metrics.namespace\nconfiguration property."], "answer_start": [574]}}
{"context": "s.namespace\nconfiguration property.\nIf, say, users wanted to set the metrics namespace to the name of the application, they\ncan set the\nspark.metrics.namespace\nproperty to a value like\n${spark.app.name}\n. This value is\nthen expanded appropriately by Spark and is used as the root namespace of the metrics system.\nNon-driver and executor metrics are never prefixed with\nspark.app.id\n, nor does the\nspark.metrics.namespace\nproperty have any such affect on such metrics.\nSpark’s metrics are decoupled into different\ninstances\ncorresponding to Spark components. Within each instance, you can configure a\nset of sinks to which metrics are reported. The following instances are currently supported:\nmaster\n: The Spark standalone master process.\napplications\n: A component within the master which reports on", "question": "What can users set the metrics namespace to?", "answers": {"text": ["the name of the application"], "answer_start": [90]}}
{"context": "llowing instances are currently supported:\nmaster\n: The Spark standalone master process.\napplications\n: A component within the master which reports on various applications.\nworker\n: A Spark standalone worker process.\nexecutor\n: A Spark executor.\ndriver\n: The Spark driver process (the process in which your SparkContext is created).\nshuffleService\n: The Spark shuffle service.\napplicationMaster\n: The Spark ApplicationMaster when running on YARN.\nEach instance can report to zero or more\nsinks\n. Sinks are contained in the\norg.apache.spark.metrics.sink\npackage:\nConsoleSink\n: Logs metrics information to the console.\nCSVSink\n: Exports metrics data to CSV files at regular intervals.\nJmxSink\n: Registers metrics for viewing in a JMX console.\nMetricsServlet\n: Adds a servlet within the existing Spark U", "question": "What does the ConsoleSink do?", "answers": {"text": ["Logs metrics information to the console."], "answer_start": [576]}}
{"context": " CSV files at regular intervals.\nJmxSink\n: Registers metrics for viewing in a JMX console.\nMetricsServlet\n: Adds a servlet within the existing Spark UI to serve metrics data as JSON data.\nPrometheusServlet\n: (Experimental) Adds a servlet within the existing Spark UI to serve metrics data in Prometheus format.\nGraphiteSink\n: Sends metrics to a Graphite node.\nSlf4jSink\n: Sends metrics to slf4j as log entries.\nStatsdSink\n: Sends metrics to a StatsD node.\nThe Prometheus Servlet mirrors the JSON data exposed by the\nMetrics Servlet\nand the REST API, but in a time-series format. The following are the equivalent Prometheus Servlet endpoints.\nComponent\nPort\nJSON End Point\nPrometheus End Point\nMaster\n8080\n/metrics/master/json/\n/metrics/master/prometheus/\nMaster\n8080\n/metrics/applications/json/\n/metr", "question": "What format does the Prometheus Servlet serve metrics data in?", "answers": {"text": ["in a time-series format"], "answer_start": [554]}}
{"context": "onf.\nfollowed by the configuration\ndetails, i.e. the parameters take the following form:\nspark.metrics.conf.[instance|*].sink.[sink_name].[parameter_name]\n.\nThis example shows a list of Spark configuration parameters for a Graphite sink:\n\"spark.metrics.conf.*.sink.graphite.class\"=\"org.apache.spark.metrics.sink.GraphiteSink\"\n\"spark.metrics.conf.*.sink.graphite.host\"=\"graphiteEndPoint_hostName>\"\n\"spark.metrics.conf.*.sink.graphite.port\"=<graphite_listening_port>\n\"spark.metrics.conf.*.sink.graphite.period\"=10\n\"spark.metrics.conf.*.sink.graphite.unit\"=seconds\n\"spark.metrics.conf.*.sink.graphite.prefix\"=\"optional_prefix\"\n\"spark.metrics.conf.*.sink.graphite.regex\"=\"optional_regex_to_send_matching_metrics\"\nDefault values of the Spark metrics configuration are as follows:\n\"*.sink.servlet.class\" = ", "question": "What is the general form of Spark configuration parameters?", "answers": {"text": ["spark.metrics.conf.[instance|*].sink.[sink_name].[parameter_name]"], "answer_start": [89]}}
{"context": ".graphite.regex\"=\"optional_regex_to_send_matching_metrics\"\nDefault values of the Spark metrics configuration are as follows:\n\"*.sink.servlet.class\" = \"org.apache.spark.metrics.sink.MetricsServlet\"\n\"*.sink.servlet.path\" = \"/metrics/json\"\n\"master.sink.servlet.path\" = \"/metrics/master/json\"\n\"applications.sink.servlet.path\" = \"/metrics/applications/json\"\nAdditional sources can be configured using the metrics configuration file or the configuration\nparameter\nspark.metrics.conf.[component_name].source.jvm.class=[source_name]\n. At present the\nJVM source is the only available optional source. For example the following configuration parameter\nactivates the JVM source:\n\"spark.metrics.conf.*.source.jvm.class\"=\"org.apache.spark.metrics.source.JvmSource\"\nList of available metrics providers\nMetrics used", "question": "What is the default class for the metrics sink servlet?", "answers": {"text": ["org.apache.spark.metrics.sink.MetricsServlet"], "answer_start": [151]}}
{"context": "s the JVM source:\n\"spark.metrics.conf.*.source.jvm.class\"=\"org.apache.spark.metrics.source.JvmSource\"\nList of available metrics providers\nMetrics used by Spark are of multiple types: gauge, counter, histogram, meter and timer,\nsee\nDropwizard library documentation for details\n.\nThe following list of components and metrics reports the name and some details about the available metrics,\ngrouped per component instance and source namespace.\nThe most common time of metrics used in Spark instrumentation are gauges and counters.\nCounters can be recognized as they have the\n.count\nsuffix. Timers, meters and histograms are annotated\nin the list, the rest of the list elements are metrics of type gauge.\nThe large majority of metrics are active as soon as their parent component instance is configured,\nso", "question": "What are the common types of metrics used in Spark instrumentation?", "answers": {"text": ["The most common time of metrics used in Spark instrumentation are gauges and counters."], "answer_start": [439]}}
{"context": " of the list elements are metrics of type gauge.\nThe large majority of metrics are active as soon as their parent component instance is configured,\nsome metrics require also to be enabled via an additional configuration parameter, the details are\nreported in the list.\nComponent instance = Driver\nThis is the component with the largest amount of instrumented metrics\nnamespace=BlockManager\ndisk.diskSpaceUsed_MB\nmemory.maxMem_MB\nmemory.maxOffHeapMem_MB\nmemory.maxOnHeapMem_MB\nmemory.memUsed_MB\nmemory.offHeapMemUsed_MB\nmemory.onHeapMemUsed_MB\nmemory.remainingMem_MB\nmemory.remainingOffHeapMem_MB\nmemory.remainingOnHeapMem_MB\nnamespace=HiveExternalCatalog\nnote:\nthese metrics are conditional to a configuration parameter:\nspark.metrics.staticSources.enabled\n(default is true)\nfileCacheHits.count\nfiles", "question": "What configuration parameter conditionally enables some metrics?", "answers": {"text": ["spark.metrics.staticSources.enabled"], "answer_start": [721]}}
{"context": "alog\nnote:\nthese metrics are conditional to a configuration parameter:\nspark.metrics.staticSources.enabled\n(default is true)\nfileCacheHits.count\nfilesDiscovered.count\nhiveClientCalls.count\nparallelListingJobCount.count\npartitionsFetched.count\nnamespace=CodeGenerator\nnote:\nthese metrics are conditional to a configuration parameter:\nspark.metrics.staticSources.enabled\n(default is true)\ncompilationTime (histogram)\ngeneratedClassSize (histogram)\ngeneratedMethodSize (histogram)\nsourceCodeSize (histogram)\nnamespace=DAGScheduler\njob.activeJobs\njob.allJobs\nmessageProcessingTime (timer)\nstage.failedStages\nstage.runningStages\nstage.waitingStages\nnamespace=LiveListenerBus\nlistenerProcessingTime.org.apache.spark.HeartbeatReceiver (timer)\nlistenerProcessingTime.org.apache.spark.scheduler.EventLoggingLi", "question": "What is the default value of the configuration parameter that affects the metrics listed?", "answers": {"text": ["(default is true)"], "answer_start": [107]}}
{"context": "spark.metrics.appStatusSource.enabled\n(default is true)\nstages.failedStages.count\nstages.skippedStages.count\nstages.completedStages.count\ntasks.blackListedExecutors.count // deprecated use excludedExecutors instead\ntasks.excludedExecutors.count\ntasks.completedTasks.count\ntasks.failedTasks.count\ntasks.killedTasks.count\ntasks.skippedTasks.count\ntasks.unblackListedExecutors.count // deprecated use unexcludedExecutors instead\ntasks.unexcludedExecutors.count\njobs.succeededJobs\njobs.failedJobs\njobDuration\nnamespace=AccumulatorSource\nnote:\nUser-configurable sources to attach accumulators to metric system\nDoubleAccumulatorSource\nLongAccumulatorSource\nnamespace=spark.streaming\nnote:\nThis applies to Spark Structured Streaming only. Conditional to a configuration\nparameter:\nspark.sql.streaming.metric", "question": "What is the default value for spark.metrics.appStatusSource.enabled?", "answers": {"text": ["(default is true)"], "answer_start": [38]}}
{"context": "ce.enabled\n(default is true)\nThis source contains memory-related metrics. A full list of available metrics in this\nnamespace can be found in the corresponding entry for the Executor component instance.\nnamespace=ExecutorAllocationManager\nnote:\nthese metrics are only emitted when using dynamic allocation. Conditional to a configuration\nparameter\nspark.dynamicAllocation.enabled\n(default is false)\nexecutors.numberExecutorsToAdd\nexecutors.numberExecutorsPendingToRemove\nexecutors.numberAllExecutors\nexecutors.numberTargetExecutors\nexecutors.numberMaxNeededExecutors\nexecutors.numberDecommissioningExecutors\nexecutors.numberExecutorsGracefullyDecommissioned.count\nexecutors.numberExecutorsDecommissionUnfinished.count\nexecutors.numberExecutorsExitedUnexpectedly.count\nexecutors.numberExecutorsKilledBy", "question": "What is the default value for the configuration parameter spark.dynamicAllocation.enabled?", "answers": {"text": ["(default is false)"], "answer_start": [379]}}
{"context": "sioned.count\nexecutors.numberExecutorsDecommissionUnfinished.count\nexecutors.numberExecutorsExitedUnexpectedly.count\nexecutors.numberExecutorsKilledByDriver.count\nnamespace=plugin.<Plugin Class Name>\nOptional namespace(s). Metrics in this namespace are defined by user-supplied code, and\nconfigured using the Spark plugin API. See “Advanced Instrumentation” below for how to load\ncustom plugins into Spark.\nComponent instance = Executor\nThese metrics are exposed by Spark executors.\nnamespace=executor (metrics are of type counter or gauge)\nnotes:\nspark.executor.metrics.fileSystemSchemes\n(default:\nfile,hdfs\n) determines the exposed file system metrics.\nbytesRead.count\nbytesWritten.count\ncpuTime.count\ndeserializeCpuTime.count\ndeserializeTime.count\ndiskBytesSpilled.count\nfilesystem.file.largeRead_", "question": "What determines the exposed file system metrics?", "answers": {"text": ["spark.executor.metrics.fileSystemSchemes"], "answer_start": [548]}}
{"context": "shuffleRecordsRead.count\nshuffleRecordsWritten.count\nshuffleRemoteBlocksFetched.count\nshuffleRemoteBytesRead.count\nshuffleRemoteBytesReadToDisk.count\nshuffleTotalBytesRead.count\nshuffleWriteTime.count\nMetrics related to push-based shuffle:\nshuffleCorruptMergedBlockChunks\nshuffleMergedFetchFallbackCount\nshuffleMergedRemoteBlocksFetched\nshuffleMergedLocalBlocksFetched\nshuffleMergedRemoteChunksFetched\nshuffleMergedLocalChunksFetched\nshuffleMergedRemoteBytesRead\nshuffleMergedLocalBytesRead\nshuffleRemoteReqsDuration\nshuffleMergedRemoteReqsDuration\nsucceededTasks.count\nthreadpool.activeTasks\nthreadpool.completeTasks\nthreadpool.currentPool_size\nthreadpool.maxPool_size\nthreadpool.startedTasks\nnamespace=ExecutorMetrics\nnotes:\nThese metrics are conditional to a configuration parameter:\nspark.metrics", "question": "Quais métricas estão relacionadas ao shuffle baseado em push?", "answers": {"text": ["Metrics related to push-based shuffle:"], "answer_start": [201]}}
{"context": "adpool.maxPool_size\nthreadpool.startedTasks\nnamespace=ExecutorMetrics\nnotes:\nThese metrics are conditional to a configuration parameter:\nspark.metrics.executorMetricsSource.enabled\n(default value is true)\nExecutorMetrics are updated as part of heartbeat processes scheduled\n for the executors and for the driver at regular intervals:\nspark.executor.heartbeatInterval\n(default value is 10 seconds)\nAn optional faster polling mechanism is available for executor memory metrics,\n it can be activated by setting a polling interval (in milliseconds) using the configuration parameter\nspark.executor.metrics.pollingInterval\nJVMHeapMemory\nJVMOffHeapMemory\nOnHeapExecutionMemory\nOnHeapStorageMemory\nOnHeapUnifiedMemory\nOffHeapExecutionMemory\nOffHeapStorageMemory\nOffHeapUnifiedMemory\nDirectPoolMemory\nMappedP", "question": "What configuration parameter enables ExecutorMetrics?", "answers": {"text": ["spark.metrics.executorMetricsSource.enabled"], "answer_start": [137]}}
{"context": "nHeapExecutionMemory\nOnHeapStorageMemory\nOnHeapUnifiedMemory\nOffHeapExecutionMemory\nOffHeapStorageMemory\nOffHeapUnifiedMemory\nDirectPoolMemory\nMappedPoolMemory\nMinorGCCount\nMinorGCTime\nMajorGCCount\nMajorGCTime\n“ProcessTree*” metric counters:\nProcessTreeJVMVMemory\nProcessTreeJVMRSSMemory\nProcessTreePythonVMemory\nProcessTreePythonRSSMemory\nProcessTreeOtherVMemory\nProcessTreeOtherRSSMemory\nnote:\n“ProcessTree\n” metrics are collected only under certain conditions.\nThe conditions are the logical AND of the following:\n/proc\nfilesystem exists,\nspark.executor.processTreeMetrics.enabled=true\n.\n“ProcessTree\n” metrics report 0 when those conditions are not met.\nnamespace=JVMCPU\njvmCpuTime\nnamespace=NettyBlockTransfer\nshuffle-client.usedDirectMemory\nshuffle-client.usedHeapMemory\nshuffle-server.usedDire", "question": "Under what conditions are \"ProcessTree\" metrics collected?", "answers": {"text": ["The conditions are the logical AND of the following:\n/proc\nfilesystem exists,\nspark.executor.processTreeMetrics.enabled=true"], "answer_start": [464]}}
{"context": "ot met.\nnamespace=JVMCPU\njvmCpuTime\nnamespace=NettyBlockTransfer\nshuffle-client.usedDirectMemory\nshuffle-client.usedHeapMemory\nshuffle-server.usedDirectMemory\nshuffle-server.usedHeapMemory\nnamespace=HiveExternalCatalog\nnote:\nthese metrics are conditional to a configuration parameter:\nspark.metrics.staticSources.enabled\n(default is true)\nfileCacheHits.count\nfilesDiscovered.count\nhiveClientCalls.count\nparallelListingJobCount.count\npartitionsFetched.count\nnamespace=CodeGenerator\nnote:\nthese metrics are conditional to a configuration parameter:\nspark.metrics.staticSources.enabled\n(default is true)\ncompilationTime (histogram)\ngeneratedClassSize (histogram)\ngeneratedMethodSize (histogram)\nsourceCodeSize (histogram)\nnamespace=plugin.<Plugin Class Name>\nOptional namespace(s). Metrics in this names", "question": "What configuration parameter makes the HiveExternalCatalog metrics conditional?", "answers": {"text": ["spark.metrics.staticSources.enabled"], "answer_start": [285]}}
{"context": "urce is available for driver and executor instances and is also available for other instances.\nThis source provides information on JVM metrics using the\nDropwizard/Codahale Metric Sets for JVM instrumentation\nand in particular the metric sets BufferPoolMetricSet, GarbageCollectorMetricSet and MemoryUsageGaugeSet.\nComponent instance = applicationMaster\nNote: applies when running on YARN\nnumContainersPendingAllocate\nnumExecutorsFailed\nnumExecutorsRunning\nnumLocalityAwareTasks\nnumReleasedContainers\nComponent instance = master\nNote: applies when running in Spark standalone as master\nworkers\naliveWorkers\napps\nwaitingApps\nComponent instance = ApplicationSource\nNote: applies when running in Spark standalone as master\nstatus\nruntime_ms\ncores\nComponent instance = worker\nNote: applies when running i", "question": "Which metric sets are used for JVM instrumentation?", "answers": {"text": ["BufferPoolMetricSet, GarbageCollectorMetricSet and MemoryUsageGaugeSet."], "answer_start": [243]}}
{"context": "cationSource\nNote: applies when running in Spark standalone as master\nstatus\nruntime_ms\ncores\nComponent instance = worker\nNote: applies when running in Spark standalone as worker\nexecutors\ncoresUsed\nmemUsed_MB\ncoresFree\nmemFree_MB\nComponent instance = shuffleService\nNote: applies to the shuffle service\nblockTransferRate (meter) - rate of blocks being transferred\nblockTransferMessageRate (meter) - rate of block transfer messages,\ni.e. if batch fetches are enabled, this represents number of batches rather than number of blocks\nblockTransferRateBytes (meter)\nblockTransferAvgSize_1min (gauge - 1-minute moving average)\nnumActiveConnections.count\nnumRegisteredConnections.count\nnumCaughtExceptions.count\nopenBlockRequestLatencyMillis (timer)\nregisterExecutorRequestLatencyMillis (timer)\nfetchMerged", "question": "What does the 'blockTransferMessageRate' meter represent when batch fetches are enabled?", "answers": {"text": ["i.e. if batch fetches are enabled, this represents number of batches rather than number of blocks"], "answer_start": [433]}}
{"context": "umRegisteredConnections.count\nnumCaughtExceptions.count\nopenBlockRequestLatencyMillis (timer)\nregisterExecutorRequestLatencyMillis (timer)\nfetchMergedBlocksMetaLatencyMillis (timer)\nfinalizeShuffleMergeLatencyMillis (timer)\nregisteredExecutorsSize\nshuffle-server.usedDirectMemory\nshuffle-server.usedHeapMemory\nnote:\nthe metrics below apply when the server side configuration\nspark.shuffle.push.server.mergedShuffleFileManagerImpl\nis set to\norg.apache.spark.network.shuffle.MergedShuffleFileManager\nfor Push-Based Shuffle\nblockBytesWritten - size of the pushed block data written to file in bytes\nblockAppendCollisions - number of shuffle push blocks collided in shuffle services\nas another block for the same reduce partition were being written\nlateBlockPushes - number of shuffle push blocks that ar", "question": "Quais métricas se aplicam quando a configuração do lado do servidor spark.shuffle.push.server.mergedShuffleFileManagerImpl está definida como org.apache.spark.network.shuffle.MergedShuffleFileManager?", "answers": {"text": ["the metrics below apply when the server side configuration\nspark.shuffle.push.server.mergedShuffleFileManagerImpl\nis set to\norg.apache.spark.network.shuffle.MergedShuffleFileManager"], "answer_start": [316]}}
{"context": "stat\nfor reporting time-series statistics and\njconsole\nfor visually exploring various JVM\nproperties are useful for those comfortable with JVM internals.\nSpark also provides a plugin API so that custom instrumentation code can be added to Spark\napplications. There are two configuration keys available for loading plugins into Spark:\nspark.plugins\nspark.plugins.defaultList\nBoth take a comma-separated list of class names that implement the\norg.apache.spark.api.plugin.SparkPlugin\ninterface. The two names exist so that it’s\npossible for one list to be placed in the Spark default config file, allowing users to\neasily add other plugins from the command line without overwriting the config file’s list. Duplicate\nplugins are ignored.", "question": "What are the two configuration keys available for loading plugins into Spark?", "answers": {"text": ["spark.plugins\nspark.plugins.defaultList"], "answer_start": [334]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)"], "answer_start": [46]}}
{"context": "ures\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nBasic Statistics\n\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\wv}{\\mathbf{w}}\n\\newcommand{\\av}{\\mathbf{\\alpha}}\n\\newcommand{\\bv}{\\mathbf{b}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\id}{\\mathbf{I}}\n\\newcommand{\\ind}{\\mathbf{1}}\n\\newcommand{\\0}{\\mathbf{0}}\n\\newcommand{\\unit}{\\mathbf{e}}\n\\newcommand{\\one}{\\mathb", "question": "Quais são alguns dos tópicos avançados abordados no texto?", "answers": {"text": ["Advanced topics"], "answer_start": [121]}}
{"context": ".\nThe output will be a DataFrame that contains the correlation matrix of the column of vectors.\nfrom\npyspark.ml.linalg\nimport\nVectors\nfrom\npyspark.ml.stat\nimport\nCorrelation\ndata\n=\n[(\nVectors\n.\nsparse\n(\n4\n,\n[(\n0\n,\n1.0\n),\n(\n3\n,\n-\n2.0\n)]),),\n(\nVectors\n.\ndense\n([\n4.0\n,\n5.0\n,\n0.0\n,\n3.0\n]),),\n(\nVectors\n.\ndense\n([\n6.0\n,\n7.0\n,\n0.0\n,\n8.0\n]),),\n(\nVectors\n.\nsparse\n(\n4\n,\n[(\n0\n,\n9.0\n),\n(\n3\n,\n1.0\n)]),)]\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\n[\n\"\nfeatures\n\"\n])\nr1\n=\nCorrelation\n.\ncorr\n(\ndf\n,\n\"\nfeatures\n\"\n).\nhead\n()\nprint\n(\n\"\nPearson correlation matrix:\n\\n\n\"\n+\nstr\n(\nr1\n[\n0\n]))\nr2\n=\nCorrelation\n.\ncorr\n(\ndf\n,\n\"\nfeatures\n\"\n,\n\"\nspearman\n\"\n).\nhead\n()\nprint\n(\n\"\nSpearman correlation matrix:\n\\n\n\"\n+\nstr\n(\nr2\n[\n0\n]))\nFind full example code at \"examples/src/main/python/ml/correlation_example.py\" in the Spark repo.\nCo", "question": "What does the `Correlation.corr` function calculate?", "answers": {"text": ["The output will be a DataFrame that contains the correlation matrix of the column of vectors."], "answer_start": [2]}}
{"context": "0.0\n,\n8.0\n)),\nRowFactory\n.\ncreate\n(\nVectors\n.\nsparse\n(\n4\n,\nnew\nint\n[]{\n0\n,\n3\n},\nnew\ndouble\n[]{\n9.0\n,\n1.0\n}))\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"features\"\n,\nnew\nVectorUDT\n(),\nfalse\n,\nMetadata\n.\nempty\n()),\n});\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nRow\nr1\n=\nCorrelation\n.\ncorr\n(\ndf\n,\n\"features\"\n).\nhead\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Pearson correlation matrix:\\n\"\n+\nr1\n.\nget\n(\n0\n).\ntoString\n());\nRow\nr2\n=\nCorrelation\n.\ncorr\n(\ndf\n,\n\"features\"\n,\n\"spearman\"\n).\nhead\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Spearman correlation matrix:\\n\"\n+\nr2\n.\nget\n(\n0\n).\ntoString\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\" in the Spark repo.\nHypothesis testing\nHypothesis testing is a powerf", "question": "Where can the full example code be found?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\" in the Spark repo."], "answer_start": [624]}}
{"context": "\nRefer to the\nChiSquareTest\nPython docs\nfor details on the API.\nfrom\npyspark.ml.linalg\nimport\nVectors\nfrom\npyspark.ml.stat\nimport\nChiSquareTest\ndata\n=\n[(\n0.0\n,\nVectors\n.\ndense\n(\n0.5\n,\n10.0\n)),\n(\n0.0\n,\nVectors\n.\ndense\n(\n1.5\n,\n20.0\n)),\n(\n1.0\n,\nVectors\n.\ndense\n(\n1.5\n,\n30.0\n)),\n(\n0.0\n,\nVectors\n.\ndense\n(\n3.5\n,\n30.0\n)),\n(\n0.0\n,\nVectors\n.\ndense\n(\n3.5\n,\n40.0\n)),\n(\n1.0\n,\nVectors\n.\ndense\n(\n3.5\n,\n40.0\n))]\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\n[\n\"\nlabel\n\"\n,\n\"\nfeatures\n\"\n])\nr\n=\nChiSquareTest\n.\ntest\n(\ndf\n,\n\"\nfeatures\n\"\n,\n\"\nlabel\n\"\n).\nhead\n()\nprint\n(\n\"\npValues:\n\"\n+\nstr\n(\nr\n.\npValues\n))\nprint\n(\n\"\ndegreesOfFreedom:\n\"\n+\nstr\n(\nr\n.\ndegreesOfFreedom\n))\nprint\n(\n\"\nstatistics:\n\"\n+\nstr\n(\nr\n.\nstatistics\n))\nFind full example code at \"examples/src/main/python/ml/chi_square_test_example.py\" in the Spark repo.\nRefer to", "question": "Where can I find a full example code for the ChiSquareTest?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/chi_square_test_example.py\" in the Spark repo."], "answer_start": [690]}}
{"context": "statistics:\n\"\n+\nstr\n(\nr\n.\nstatistics\n))\nFind full example code at \"examples/src/main/python/ml/chi_square_test_example.py\" in the Spark repo.\nRefer to the\nChiSquareTest\nScala docs\nfor details on the API.\nimport\norg.apache.spark.ml.linalg.\n{\nVector\n,\nVectors\n}\nimport\norg.apache.spark.ml.stat.ChiSquareTest\nval\ndata\n=\nSeq\n(\n(\n0.0\n,\nVectors\n.\ndense\n(\n0.5\n,\n10.0\n)),\n(\n0.0\n,\nVectors\n.\ndense\n(\n1.5\n,\n20.0\n)),\n(\n1.0\n,\nVectors\n.\ndense\n(\n1.5\n,\n30.0\n)),\n(\n0.0\n,\nVectors\n.\ndense\n(\n3.5\n,\n30.0\n)),\n(\n0.0\n,\nVectors\n.\ndense\n(\n3.5\n,\n40.0\n)),\n(\n1.0\n,\nVectors\n.\ndense\n(\n3.5\n,\n40.0\n))\n)\nval\ndf\n=\ndata\n.\ntoDF\n(\n\"label\"\n,\n\"features\"\n)\nval\nchi\n=\nChiSquareTest\n.\ntest\n(\ndf\n,\n\"features\"\n,\n\"label\"\n).\nhead\n()\nprintln\n(\ns\n\"pValues = ${chi.getAs[Vector](0)}\"\n)\nprintln\n(\ns\n\"degreesOfFreedom ${chi.getSeq[Int](1).mkString(\"\n[\n", "question": "Where can I find a full example code for the ChiSquareTest?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/chi_square_test_example.py\" in the Spark repo."], "answer_start": [40]}}
{"context": "f\n,\n\"features\"\n,\n\"label\"\n).\nhead\n()\nprintln\n(\ns\n\"pValues = ${chi.getAs[Vector](0)}\"\n)\nprintln\n(\ns\n\"degreesOfFreedom ${chi.getSeq[Int](1).mkString(\"\n[\n\"\n,\n\"\n,\n\"\n,\n\"\n]\n\")}\"\n)\nprintln\n(\ns\n\"statistics ${chi.getAs[Vector](2)}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\" in the Spark repo.\nRefer to the\nChiSquareTest\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.ml.stat.ChiSquareTest\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.*\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n", "question": "Where can I find a full example code for ChiSquareTest?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\" in the Spark repo."], "answer_start": [224]}}
{"context": "ct\n(\nsummarizer\n.\nsummary\n(\ndf\n.\nfeatures\n,\ndf\n.\nweight\n)).\nshow\n(\ntruncate\n=\nFalse\n)\n# compute statistics for multiple metrics without weight\ndf\n.\nselect\n(\nsummarizer\n.\nsummary\n(\ndf\n.\nfeatures\n)).\nshow\n(\ntruncate\n=\nFalse\n)\n# compute statistics for single metric \"mean\" with weight\ndf\n.\nselect\n(\nSummarizer\n.\nmean\n(\ndf\n.\nfeatures\n,\ndf\n.\nweight\n)).\nshow\n(\ntruncate\n=\nFalse\n)\n# compute statistics for single metric \"mean\" without weight\ndf\n.\nselect\n(\nSummarizer\n.\nmean\n(\ndf\n.\nfeatures\n)).\nshow\n(\ntruncate\n=\nFalse\n)\nFind full example code at \"examples/src/main/python/ml/summarizer_example.py\" in the Spark repo.\nThe following example demonstrates using\nSummarizer\nto compute the mean and variance for a vector column of the input dataframe, with and without a weight column.\nimport\norg.apache.spark.ml.", "question": "Where can I find a full example code for using Summarizer?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/summarizer_example.py\" in the Spark repo."], "answer_start": [513]}}
{"context": "\nSummarizer\nto compute the mean and variance for a vector column of the input dataframe, with and without a weight column.\nimport\norg.apache.spark.ml.linalg.\n{\nVector\n,\nVectors\n}\nimport\norg.apache.spark.ml.stat.Summarizer\nval\ndata\n=\nSeq\n(\n(\nVectors\n.\ndense\n(\n2.0\n,\n3.0\n,\n5.0\n),\n1.0\n),\n(\nVectors\n.\ndense\n(\n4.0\n,\n6.0\n,\n7.0\n),\n2.0\n)\n)\nval\ndf\n=\ndata\n.\ntoDF\n(\n\"features\"\n,\n\"weight\"\n)\nval\n(\nmeanVal\n,\nvarianceVal\n)\n=\ndf\n.\nselect\n(\nmetrics\n(\n\"mean\"\n,\n\"variance\"\n)\n.\nsummary\n(\n$\n\"features\"\n,\n$\n\"weight\"\n).\nas\n(\n\"summary\"\n))\n.\nselect\n(\n\"summary.mean\"\n,\n\"summary.variance\"\n)\n.\nas\n[(\nVector\n,\nVector\n)].\nfirst\n()\nprintln\n(\ns\n\"with weight: mean = ${meanVal}, variance = ${varianceVal}\"\n)\nval\n(\nmeanVal2\n,\nvarianceVal2\n)\n=\ndf\n.\nselect\n(\nmean\n(\n$\n\"features\"\n),\nvariance\n(\n$\n\"features\"\n))\n.\nas\n[(\nVector\n,\nVector\n)]", "question": "What is computed for a vector column of the input dataframe?", "answers": {"text": ["to compute the mean and variance for a vector column of the input dataframe, with and without a weight column."], "answer_start": [12]}}
{"context": "iance = ${varianceVal}\"\n)\nval\n(\nmeanVal2\n,\nvarianceVal2\n)\n=\ndf\n.\nselect\n(\nmean\n(\n$\n\"features\"\n),\nvariance\n(\n$\n\"features\"\n))\n.\nas\n[(\nVector\n,\nVector\n)].\nfirst\n()\nprintln\n(\ns\n\"without weight: mean = ${meanVal2}, sum = ${varianceVal2}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/SummarizerExample.scala\" in the Spark repo.\nThe following example demonstrates using\nSummarizer\nto compute the mean and variance for a vector column of the input dataframe, with and without a weight column.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.linalg.Vector\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.ml.stat.Summarizer\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimpo", "question": "Where can I find a full example code for using Summarizer?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/SummarizerExample.scala\" in the Spark repo."], "answer_start": [235]}}
{"context": "ght: mean = \"\n+\nresult2\n.<\nVector\n>\ngetAs\n(\n0\n).\ntoString\n()\n+\n\", variance = \"\n+\nresult2\n.<\nVector\n>\ngetAs\n(\n1\n).\ntoString\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaSummarizerExample.java\" in the Spark repo.", "question": "Where can I find a full example code for the JavaSummarizerExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaSummarizerExample.java\" in the Spark repo."], "answer_start": [128]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nRunning Spark on Kubernetes\nSecurity\nUser Identity\nVolume Mounts\nPrerequisites\nHow it works\nSubmitting Applications to Kubernetes\nDocker Images\nCluster Mode\nClient Mode\nClient Mode Networking\nClient Mode Executor Pod Garbage Collection\nAuthentication ", "question": "Quais são alguns dos modos de envio de aplicações para Kubernetes?", "answers": {"text": ["Cluster Mode\nClient Mode"], "answer_start": [693]}}
{"context": "g Applications to Kubernetes\nDocker Images\nCluster Mode\nClient Mode\nClient Mode Networking\nClient Mode Executor Pod Garbage Collection\nAuthentication Parameters\nIPv4 and IPv6\nDependency Management\nSecret Management\nPod Template\nUsing Kubernetes Volumes\nPVC-oriented executor pod allocation\nLocal Storage\nUsing RAM for local storage\nIntrospection and Debugging\nAccessing Logs\nAccessing Driver UI\nDebugging\nKubernetes Features\nConfiguration File\nContexts\nNamespaces\nRBAC\nSpark Application Management\nFuture Work\nConfiguration\nSpark Properties\nPod template properties\nPod Metadata\nPod Spec\nContainer spec\nResource Allocation and Configuration Overview\nResource Level Scheduling Overview\nPriority Scheduling\nCustomized Kubernetes Schedulers for Spark on Kubernetes\nUsing Volcano as Customized Scheduler f", "question": "What topics are covered regarding Kubernetes features?", "answers": {"text": ["Kubernetes Features"], "answer_start": [405]}}
{"context": "ed network, it’s important to secure access to the cluster to prevent unauthorized applications\nfrom running on the cluster.\nPlease see\nSpark Security\nand the specific security sections in this doc before running Spark.\nUser Identity\nImages built from the project provided Dockerfiles contain a default\nUSER\ndirective with a default UID of\n185\n.  This means that the resulting images will be running the Spark processes as this UID inside the container. Security conscious deployments should consider providing custom images with\nUSER\ndirectives specifying their desired unprivileged UID and GID.  The resulting UID should include the root group in its supplementary groups in order to be able to run the Spark executables.  Users building their own images with the provided\ndocker-image-tool.sh\nscri", "question": "What UID do images built from the project's Dockerfiles contain by default?", "answers": {"text": ["185"], "answer_start": [340]}}
{"context": "ts supplementary groups in order to be able to run the Spark executables.  Users building their own images with the provided\ndocker-image-tool.sh\nscript can use the\n-u <uid>\noption to specify the desired UID.\nAlternatively the\nPod Template\nfeature can be used to add a\nSecurity Context\nwith a\nrunAsUser\nto the pods that Spark submits.  This can be used to override the\nUSER\ndirectives in the images themselves.  Please bear in mind that this requires cooperation from your users and as such may not be a suitable solution for shared environments.  Cluster administrators should use\nPod Security Policies\nif they wish to limit the users that pods may run as.\nVolume Mounts\nAs described later in this document under\nUsing Kubernetes Volumes\nSpark on K8S provides configuration options that allow for mo", "question": "How can users building their own images specify the desired UID?", "answers": {"text": ["Users building their own images with the provided\ndocker-image-tool.sh\nscript can use the\n-u <uid>\noption to specify the desired UID."], "answer_start": [75]}}
{"context": "sts\nlogs and remains in “completed” state in the Kubernetes API until it’s eventually garbage collected or manually cleaned up.\nNote that in the completed state, the driver pod does\nnot\nuse any computational or memory resources.\nThe driver and executor pod scheduling is handled by Kubernetes. Communication to the Kubernetes API is done via fabric8. It is possible to schedule the\ndriver and executor pods on a subset of available nodes through a\nnode selector\nusing the configuration property for it. It will be possible to use more advanced\nscheduling hints like\nnode/pod affinities\nin a future release.\nSubmitting Applications to Kubernetes\nDocker Images\nKubernetes requires users to supply images that can be deployed into containers within pods. The images are built to\nbe run in a container ru", "question": "How is driver and executor pod scheduling handled?", "answers": {"text": ["The driver and executor pod scheduling is handled by Kubernetes."], "answer_start": [229]}}
{"context": "er\ncommand line argument to\nspark-submit\nor by setting\nspark.master\nin the application’s configuration, must be a URL with the format\nk8s://<api_server_host>:<k8s-apiserver-port>\n. The port must always be specified, even if it’s the HTTPS port 443. Prefixing the\nmaster string with\nk8s://\nwill cause the Spark application to launch on the Kubernetes cluster, with the API server\nbeing contacted at\napi_server_url\n. If no HTTP protocol is specified in the URL, it defaults to\nhttps\n. For example,\nsetting the master to\nk8s://example.com:443\nis equivalent to setting it to\nk8s://https://example.com:443\n, but to\nconnect without TLS on a different port, the master would be set to\nk8s://http://example.com:8080\n.\nIn Kubernetes mode, the Spark application name that is specified by\nspark.app.name\nor the\n", "question": "What is the format of the URL required for the spark.master configuration when using Kubernetes?", "answers": {"text": ["k8s://<api_server_host>:<k8s-apiserver-port>"], "answer_start": [134]}}
{"context": "from the executors by a stable hostname. When deploying your headless service, ensure that\nthe service’s label selector will only match the driver pod and no other pods; it is recommended to assign your driver\npod a sufficiently unique label and to use that label in the label selector of the headless service. Specify the driver’s\nhostname via\nspark.driver.host\nand your spark driver’s port to\nspark.driver.port\n.\nClient Mode Executor Pod Garbage Collection\nIf you run your Spark driver in a pod, it is highly recommended to set\nspark.kubernetes.driver.pod.name\nto the name of that pod.\nWhen this property is set, the Spark scheduler will deploy the executor pods with an\nOwnerReference\n, which in turn will\nensure that once the driver pod is deleted from the cluster, all of the application’s execu", "question": "What property should be set to the name of the Spark driver pod?", "answers": {"text": ["spark.kubernetes.driver.pod.name"], "answer_start": [530]}}
{"context": "od.name\nis not set when your application is\nactually running in a pod, keep in mind that the executor pods may not be properly deleted from the cluster when the\napplication exits. The Spark scheduler attempts to delete these pods, but if the network request to the API server fails\nfor any reason, these pods will remain in the cluster. The executor processes should exit when they cannot reach the\ndriver, so the executor pods should not consume compute resources (cpu and memory) in the cluster after your application\nexits.\nYou may use\nspark.kubernetes.executor.podNamePrefix\nto fully control the executor pod names.\nWhen this property is set, it’s highly recommended to make it unique across all jobs in the same namespace.\nAuthentication Parameters\nUse the exact prefix\nspark.kubernetes.authenti", "question": "What should you use to fully control the executor pod names?", "answers": {"text": ["spark.kubernetes.executor.podNamePrefix"], "answer_start": [539]}}
{"context": "s highly recommended to make it unique across all jobs in the same namespace.\nAuthentication Parameters\nUse the exact prefix\nspark.kubernetes.authenticate\nfor Kubernetes authentication parameters in client mode.\nIPv4 and IPv6\nStarting with 3.4.0, Spark supports additionally IPv6-only environment via\nIPv4/IPv6 dual-stack network\nfeature which enables the allocation of both IPv4 and IPv6 addresses to Pods and Services.\nAccording to the K8s cluster capability,\nspark.kubernetes.driver.service.ipFamilyPolicy\nand\nspark.kubernetes.driver.service.ipFamilies\ncan be one of\nSingleStack\n,\nPreferDualStack\n,\nand\nRequireDualStack\nand one of\nIPv4\n,\nIPv6\n,\nIPv4,IPv6\n, and\nIPv6,IPv4\nrespectively.\nBy default, Spark uses\nspark.kubernetes.driver.service.ipFamilyPolicy=SingleStack\nand\nspark.kubernetes.driver.se", "question": "What prefix should be used for Kubernetes authentication parameters in client mode?", "answers": {"text": ["spark.kubernetes.authenticate"], "answer_start": [125]}}
{"context": "iners, add the following options to the\nspark-submit\ncommand:\n--conf spark.kubernetes.driver.secrets.spark-secret=/etc/secrets\n--conf spark.kubernetes.executor.secrets.spark-secret=/etc/secrets\nTo use a secret through an environment variable use the following options to the\nspark-submit\ncommand:\n--conf spark.kubernetes.driver.secretKeyRef.ENV_NAME=name:key\n--conf spark.kubernetes.executor.secretKeyRef.ENV_NAME=name:key\nPod Template\nKubernetes allows defining pods from\ntemplate files\n.\nSpark users can similarly use template files to define the driver or executor pod configurations that Spark configurations do not support.\nTo do so, specify the spark properties\nspark.kubernetes.driver.podTemplateFile\nand\nspark.kubernetes.executor.podTemplateFile\nto point to files accessible to the\nspark-subm", "question": "How can Spark users define driver or executor pod configurations that Spark configurations do not support?", "answers": {"text": ["Spark users can similarly use template files to define the driver or executor pod configurations that Spark configurations do not support."], "answer_start": [490]}}
{"context": "ated to volume mounts.\nTo mount a volume of any of the types above into the driver pod, use the following configuration property:\n--conf spark.kubernetes.driver.volumes.[VolumeType].[VolumeName].mount.path=<mount path>\n--conf spark.kubernetes.driver.volumes.[VolumeType].[VolumeName].mount.readOnly=<true|false>\n--conf spark.kubernetes.driver.volumes.[VolumeType].[VolumeName].mount.subPath=<mount subPath>\nSpecifically,\nVolumeType\ncan be one of the following values:\nhostPath\n,\nemptyDir\n,\nnfs\nand\npersistentVolumeClaim\n.\nVolumeName\nis the name you want to use for the volume under the\nvolumes\nfield in the pod specification.\nEach supported type of volumes may have some specific configuration options, which can be specified using configuration properties of the following form:\nspark.kubernetes.dri", "question": "What configuration property is used to mount a volume into the driver pod?", "answers": {"text": ["--conf spark.kubernetes.driver.volumes.[VolumeType].[VolumeName].mount.path=<mount path>"], "answer_start": [130]}}
{"context": "olumes may have some specific configuration options, which can be specified using configuration properties of the following form:\nspark.kubernetes.driver.volumes.[VolumeType].[VolumeName].options.[OptionName]=<value>\nFor example, the server and path of a\nnfs\nwith volume name\nimages\ncan be specified using the following properties:\nspark.kubernetes.driver.volumes.nfs.images.options.server=example.com\nspark.kubernetes.driver.volumes.nfs.images.options.path=/data\nAnd, the claim name of a\npersistentVolumeClaim\nwith volume name\ncheckpointpvc\ncan be specified using the following property:\nspark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.options.claimName=check-point-pvc-claim\nThe configuration properties for mounting volumes into the executor pods use prefix\nspark.kubernetes.ex", "question": "How can the server and path of an NFS volume named 'images' be specified?", "answers": {"text": ["spark.kubernetes.driver.volumes.nfs.images.options.server=example.com\nspark.kubernetes.driver.volumes.nfs.images.options.path=/data"], "answer_start": [332]}}
{"context": "intpvc.options.claimName=check-point-pvc-claim\nThe configuration properties for mounting volumes into the executor pods use prefix\nspark.kubernetes.executor.\ninstead of\nspark.kubernetes.driver.\n.\nFor example, you can mount a dynamically-created persistent volume claim per executor by using\nOnDemand\nas a claim name and\nstorageClass\nand\nsizeLimit\noptions like the following. This is useful in case of\nDynamic Allocation\n.\nspark.kubernetes.executor.volumes.persistentVolumeClaim.data.options.claimName=OnDemand\nspark.kubernetes.executor.volumes.persistentVolumeClaim.data.options.storageClass=gp\nspark.kubernetes.executor.volumes.persistentVolumeClaim.data.options.sizeLimit=500Gi\nspark.kubernetes.executor.volumes.persistentVolumeClaim.data.mount.path=/data\nspark.kubernetes.executor.volumes.persiste", "question": "What claim name can be used to mount a dynamically-created persistent volume claim per executor?", "answers": {"text": ["OnDemand"], "answer_start": [291]}}
{"context": ".data.options.sizeLimit=500Gi\nspark.kubernetes.executor.volumes.persistentVolumeClaim.data.mount.path=/data\nspark.kubernetes.executor.volumes.persistentVolumeClaim.data.mount.readOnly=false\nFor a complete list of available options for each supported type of volumes, please refer to the\nSpark Properties\nsection below.\nPVC-oriented executor pod allocation\nSince disks are one of the important resource types, Spark driver provides a fine-grained control\nvia a set of configurations. For example, by default, on-demand PVCs are owned by executors and\nthe lifecycle of PVCs are tightly coupled with its owner executors.\nHowever, on-demand PVCs can be owned by driver and reused by another executors during the Spark job’s\nlifetime with the following options. This reduces the overhead of PVC creation a", "question": "What is the default ownership of on-demand PVCs?", "answers": {"text": ["by default, on-demand PVCs are owned by executors"], "answer_start": [496]}}
{"context": "wned by driver and reused by another executors during the Spark job’s\nlifetime with the following options. This reduces the overhead of PVC creation and deletion.\nspark.kubernetes.driver.ownPersistentVolumeClaim=true\nspark.kubernetes.driver.reusePersistentVolumeClaim=true\nIn addition, since Spark 3.4, Spark driver is able to do PVC-oriented executor allocation which means\nSpark counts the total number of created PVCs which the job can have, and holds on a new executor creation\nif the driver owns the maximum number of PVCs. This helps the transition of the existing PVC from one executor\nto another executor.\nspark.kubernetes.driver.waitToReusePersistentVolumeClaim=true\nLocal Storage\nSpark supports using volumes to spill data during shuffles and other operations. To use a volume as local stor", "question": "What options reduce the overhead of PVC creation and deletion?", "answers": {"text": ["This reduces the overhead of PVC creation and deletion."], "answer_start": [107]}}
{"context": "ersistentVolumeClaim=true\nLocal Storage\nSpark supports using volumes to spill data during shuffles and other operations. To use a volume as local storage, the volume’s name should starts with\nspark-local-dir-\n, for example:\n--conf spark.kubernetes.driver.volumes.[VolumeType].spark-local-dir-[VolumeName].mount.path=<mount path>\n--conf spark.kubernetes.driver.volumes.[VolumeType].spark-local-dir-[VolumeName].mount.readOnly=false\nSpecifically, you can use persistent volume claims if the jobs require large shuffle and sorting operations in executors.\nspark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.claimName=OnDemand\nspark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.storageClass=gp\nspark.kubernetes.executor.volumes.persistentVolu", "question": "What should the name of a volume start with to be used as local storage in Spark?", "answers": {"text": ["spark-local-dir-"], "answer_start": [192]}}
{"context": "and\nspark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.storageClass=gp\nspark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.sizeLimit=500Gi\nspark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.mount.path=/data\nspark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.mount.readOnly=false\nTo enable shuffle data recovery feature via the built-in\nKubernetesLocalDiskShuffleDataIO\nplugin, we need to have the following. You may want to enable\nspark.kubernetes.driver.waitToReusePersistentVolumeClaim\nadditionally.\nspark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.mount.path=/data/spark-x/executor-x\nspark.shuffle.sort.io.plugin.class=org.apache.spark.shuffle.KubernetesLocalDiskSh", "question": "What is the mount path for spark-local-dir-1?", "answers": {"text": ["/data"], "answer_start": [285]}}
{"context": "entVolumeClaim.spark-local-dir-1.mount.path=/data/spark-x/executor-x\nspark.shuffle.sort.io.plugin.class=org.apache.spark.shuffle.KubernetesLocalDiskShuffleDataIO\nIf no volume is set as local storage, Spark uses temporary scratch space to spill data to disk during shuffles and other operations. When using Kubernetes as the resource manager the pods will be created with an\nemptyDir\nvolume mounted for each directory listed in\nspark.local.dir\nor the environment variable\nSPARK_LOCAL_DIRS\n.  If no directories are explicitly specified then a default directory is created and configured appropriately.\nemptyDir\nvolumes use the ephemeral storage feature of Kubernetes and do not persist beyond the life of the pod.\nUsing RAM for local storage\nemptyDir\nvolumes use the nodes backing storage for ephemeral", "question": "What happens when no volume is set as local storage?", "answers": {"text": ["If no volume is set as local storage, Spark uses temporary scratch space to spill data to disk during shuffles and other operations."], "answer_start": [162]}}
{"context": " of Kubernetes and do not persist beyond the life of the pod.\nUsing RAM for local storage\nemptyDir\nvolumes use the nodes backing storage for ephemeral storage by default, this behaviour may not be appropriate for some compute environments.  For example if you have diskless nodes with remote storage mounted over a network, having lots of executors doing IO to this remote storage may actually degrade performance.\nIn this case it may be desirable to set\nspark.kubernetes.local.dirs.tmpfs=true\nin your configuration which will cause the\nemptyDir\nvolumes to be configured as\ntmpfs\ni.e. RAM backed volumes.  When configured like this Spark’s local storage usage will count towards your pods memory usage therefore you may wish to increase your memory requests by increasing the value of\nspark.{driver,e", "question": "What happens when spark.kubernetes.local.dirs.tmpfs is set to true?", "answers": {"text": ["in your configuration which will cause the\nemptyDir\nvolumes to be configured as\ntmpfs\ni.e. RAM backed volumes."], "answer_start": [494]}}
{"context": "ard\nif installed on\nthe cluster.\nWhen there exists a log collection system, you can expose it at Spark Driver\nExecutors\ntab UI. For example,\nspark.ui.custom.executor.log.url='https://log-server/log?appId=&execId='\nYou can add additional custom variables to this url template, populated with the values of existing executor environment variables like\nspark.executorEnv.SPARK_EXECUTOR_ATTRIBUTE_YOUR_VAR='$(EXISTING_EXECUTOR_ENV_VAR)'\nspark.ui.custom.executor.log.url='https://log-server/log?appId=&execId=&your_var='\nAccessing Driver UI\nThe UI associated with any application can be accessed locally using\nkubectl port-forward\n.\n$\nkubectl port-forward <driver-pod-name> 4040:4040\nThen, the Spark driver UI can be accessed on\nhttp://localhost:4040\n.\nSince Apache Spark 4.0.0, Driver UI provides a way t", "question": "How can you access the Spark driver UI locally?", "answers": {"text": ["kubectl port-forward <driver-pod-name> 4040:4040\nThen, the Spark driver UI can be accessed on\nhttp://localhost:4040"], "answer_start": [630]}}
{"context": " <driver-pod-name> 4040:4040\nThen, the Spark driver UI can be accessed on\nhttp://localhost:4040\n.\nSince Apache Spark 4.0.0, Driver UI provides a way to see driver logs via a new configuration.\nspark.driver.log.localDir=/tmp\nThen, the Spark driver UI can be accessed on\nhttp://localhost:4040/logs/\n.\nOptionally, the layout of log is configured by the following.\nspark.driver.log.layout=\"%m%n%ex\"\nDebugging\nThere may be several kinds of failures. If the Kubernetes API server rejects the request made from spark-submit, or the\nconnection is refused for a different reason, the submission logic should indicate the error encountered. However, if there\nare errors during the running of the application, often, the best way to investigate may be through the Kubernetes CLI.\nTo get some basic information a", "question": "How can the Spark driver UI be accessed?", "answers": {"text": ["http://localhost:4040"], "answer_start": [74]}}
{"context": "re errors during the running of the application, often, the best way to investigate may be through the Kubernetes CLI.\nTo get some basic information about the scheduling decisions made around the driver pod, you can run:\n$\nkubectl describe pod <spark-driver-pod>\nIf the pod has encountered a runtime error, the status can be probed further using:\n$\nkubectl logs <spark-driver-pod>\nStatus and logs of failed executor pods can be checked in similar ways. Finally, deleting the driver pod will clean up the entire spark\napplication, including all executors, associated service, etc. The driver pod can be thought of as the Kubernetes representation of\nthe Spark application.\nKubernetes Features\nConfiguration File\nYour Kubernetes config file typically lives under\n.kube/config\nin your home directory or ", "question": "How can you investigate errors during the running of a Spark application?", "answers": {"text": ["the Kubernetes CLI."], "answer_start": [99]}}
{"context": "usters and/or user identities.  By default Spark on Kubernetes will use your current context (which can be checked by running\nkubectl config current-context\n) when doing the initial auto-configuration of the Kubernetes client.\nIn order to use an alternative context users can specify the desired context via the Spark configuration property\nspark.kubernetes.context\ne.g.\nspark.kubernetes.context=minikube\n.\nNamespaces\nKubernetes has the concept of\nnamespaces\n.\nNamespaces are ways to divide cluster resources between multiple users (via resource quota). Spark on Kubernetes can\nuse namespaces to launch Spark applications. This can be made use of through the\nspark.kubernetes.namespace\nconfiguration.\nKubernetes allows using\nResourceQuota\nto set limits on\nresources, number of objects, etc on individ", "question": "How can users specify an alternative Kubernetes context for Spark?", "answers": {"text": ["In order to use an alternative context users can specify the desired context via the Spark configuration property\nspark.kubernetes.context"], "answer_start": [227]}}
{"context": "t via the spark-submit CLI tool in cluster mode.\nUsers can kill a job by providing the submission ID that is printed when submitting their job.\nThe submission ID follows the format\nnamespace:driver-pod-name\n.\nIf user omits the namespace then the namespace set in current k8s context is used.\nFor example if user has set a specific namespace as follows\nkubectl config set-context minikube --namespace=spark\nthen the\nspark\nnamespace will be used by default. On the other hand, if there is no namespace added to the specific context\nthen all namespaces will be considered by default. That means operations will affect all Spark applications matching the given submission ID regardless of namespace.\nMoreover, spark-submit for application management uses the same backend code that is used for submitting", "question": "What happens if a user omits the namespace when killing a job?", "answers": {"text": ["If user omits the namespace then the namespace set in current k8s context is used."], "answer_start": [209]}}
{"context": ".168.2.8:8443\nThe above will kill all application with the specific prefix.\nUser can specify the grace period for pod termination via the\nspark.kubernetes.appKillPodDeletionGracePeriod\nproperty,\nusing\n--conf\nas means to provide it (default value for all K8s pods is\n30 secs\n).\nFuture Work\nThere are several Spark on Kubernetes features that are currently being worked on or planned to be worked on. Those features are expected to eventually make it into future versions of the spark-kubernetes integration.\nSome of these include:\nExternal Shuffle Service\nJob Queues and Resource Management\nConfiguration\nSee the\nconfiguration page\nfor information on Spark configurations.  The following configurations are specific to Spark on Kubernetes.\nSpark Properties\nProperty Name\nDefault\nMeaning\nSince Version\n", "question": "What is the default grace period for Kubernetes pod termination?", "answers": {"text": ["30 secs"], "answer_start": [266]}}
{"context": "Spark configurations.  The following configurations are specific to Spark on Kubernetes.\nSpark Properties\nProperty Name\nDefault\nMeaning\nSince Version\nspark.kubernetes.context\n(none)\nThe context from the user Kubernetes configuration file used for the initial\n    auto-configuration of the Kubernetes client library.  When not specified then\n    the users current context is used.\nNB:\nMany of the\n    auto-configured settings can be overridden by the use of other Spark\n    configuration properties e.g.\nspark.kubernetes.namespace\n.\n3.0.0\nspark.kubernetes.driver.master\nhttps://kubernetes.default.svc\nThe internal Kubernetes master (API server) address to be used for driver to request executors or\n    'local[*]' for driver-pod-only mode.\n3.0.0\nspark.kubernetes.namespace\ndefault\nThe namespace that w", "question": "What is the default value for spark.kubernetes.namespace?", "answers": {"text": ["default"], "answer_start": [588]}}
{"context": "s to be used for driver to request executors or\n    'local[*]' for driver-pod-only mode.\n3.0.0\nspark.kubernetes.namespace\ndefault\nThe namespace that will be used for running the driver and executor pods.\n2.3.0\nspark.kubernetes.container.image\n(none)\nContainer image to use for the Spark application.\n    This is usually of the form\nexample.com/repo/spark:v1.0.0\n.\n    This configuration is required and must be provided by the user, unless explicit\n    images are provided for each different container type.\n2.3.0\nspark.kubernetes.driver.container.image\n(value of spark.kubernetes.container.image)\nCustom container image to use for the driver.\n2.3.0\nspark.kubernetes.executor.container.image\n(value of spark.kubernetes.container.image)\nCustom container image to use for executors.\n2.3.0\nspark.kuberne", "question": "What is the default value for spark.kubernetes.namespace?", "answers": {"text": ["default"], "answer_start": [122]}}
{"context": "ntication options, this is expected to be the exact string value of the token to use for\n    the authentication. In client mode, use\nspark.kubernetes.authenticate.oauthToken\ninstead.\n2.3.0\nspark.kubernetes.authenticate.submission.oauthTokenFile\n(none)\nPath to the OAuth token file containing the token to use when authenticating against the Kubernetes API server when starting the driver.\n    This file must be located on the submitting machine's disk. Specify this as a path as opposed to a URI (i.e. do not\n    provide a scheme). In client mode, use\nspark.kubernetes.authenticate.oauthTokenFile\ninstead.\n2.3.0\nspark.kubernetes.authenticate.driver.caCertFile\n(none)\nPath to the CA cert file for connecting to the Kubernetes API server over TLS from the driver pod when requesting\n    executors. This", "question": "What should be used instead of spark.kubernetes.authenticate.submission.oauthTokenFile in client mode?", "answers": {"text": ["spark.kubernetes.authenticate.oauthTokenFile"], "answer_start": [552]}}
{"context": "aCertFile\n(none)\nPath to the CA cert file for connecting to the Kubernetes API server over TLS from the driver pod when requesting\n    executors. This file must be located on the submitting machine's disk, and will be uploaded to the driver pod.\n    Specify this as a path as opposed to a URI (i.e. do not provide a scheme). In client mode, use\nspark.kubernetes.authenticate.caCertFile\ninstead.\n2.3.0\nspark.kubernetes.authenticate.driver.clientKeyFile\n(none)\nPath to the client key file for authenticating against the Kubernetes API server from the driver pod when requesting\n    executors. This file must be located on the submitting machine's disk, and will be uploaded to the driver pod as\n    a Kubernetes secret. Specify this as a path as opposed to a URI (i.e. do not provide a scheme).\n    In ", "question": "What should be specified for the CA cert file when connecting to the Kubernetes API server over TLS from the driver pod?", "answers": {"text": ["Specify this as a path as opposed to a URI (i.e. do not provide a scheme)."], "answer_start": [250]}}
{"context": " and will be uploaded to the driver pod as\n    a Kubernetes secret. Specify this as a path as opposed to a URI (i.e. do not provide a scheme).\n    In client mode, use\nspark.kubernetes.authenticate.clientKeyFile\ninstead.\n2.3.0\nspark.kubernetes.authenticate.driver.clientCertFile\n(none)\nPath to the client cert file for authenticating against the Kubernetes API server from the driver pod when\n    requesting executors. This file must be located on the submitting machine's disk, and will be uploaded to the\n    driver pod as a Kubernetes secret. Specify this as a path as opposed to a URI (i.e. do not provide a scheme).\n    In client mode, use\nspark.kubernetes.authenticate.clientCertFile\ninstead.\n2.3.0\nspark.kubernetes.authenticate.driver.oauthToken\n(none)\nOAuth token to use when authenticating ag", "question": "What should be specified for driver authentication instead of a URI?", "answers": {"text": ["Specify this as a path as opposed to a URI (i.e. do not provide a scheme)."], "answer_start": [68]}}
{"context": "osed to a URI (i.e. do not provide a scheme). In client mode, use\nspark.kubernetes.authenticate.caCertFile\ninstead.\n2.3.0\nspark.kubernetes.authenticate.driver.mounted.clientKeyFile\n(none)\nPath to the client key file for authenticating against the Kubernetes API server from the driver pod when requesting\n    executors. This path must be accessible from the driver pod.\n    Specify this as a path as opposed to a URI (i.e. do not provide a scheme). In client mode, use\nspark.kubernetes.authenticate.clientKeyFile\ninstead.\n2.3.0\nspark.kubernetes.authenticate.driver.mounted.clientCertFile\n(none)\nPath to the client cert file for authenticating against the Kubernetes API server from the driver pod when\n    requesting executors. This path must be accessible from the driver pod.\n    Specify this as a ", "question": "What should be specified for the client key file path when authenticating against the Kubernetes API server from the driver pod?", "answers": {"text": ["Specify this as a path as opposed to a URI (i.e. do not provide a scheme)."], "answer_start": [374]}}
{"context": " the Kubernetes API server from the driver pod when\n    requesting executors. This path must be accessible from the driver pod.\n    Specify this as a path as opposed to a URI (i.e. do not provide a scheme). In client mode, use\nspark.kubernetes.authenticate.clientCertFile\ninstead.\n2.3.0\nspark.kubernetes.authenticate.driver.mounted.oauthTokenFile\n(none)\nPath to the file containing the OAuth token to use when authenticating against the Kubernetes API server from the driver pod when\n    requesting executors. This path must be accessible from the driver pod.\n    Note that unlike the other authentication options, this file must contain the exact string value of the token to use\n    for the authentication. In client mode, use\nspark.kubernetes.authenticate.oauthTokenFile\ninstead.\n2.3.0\nspark.kuber", "question": "What should be specified as a path when configuring Kubernetes authentication?", "answers": {"text": ["Specify this as a path as opposed to a URI (i.e. do not provide a scheme)."], "answer_start": [132]}}
{"context": "ring value of the token to use\n    for the authentication. In client mode, use\nspark.kubernetes.authenticate.oauthTokenFile\ninstead.\n2.3.0\nspark.kubernetes.authenticate.driver.serviceAccountName\ndefault\nService account that is used when running the driver pod. The driver pod uses this service account when requesting\n    executor pods from the API server. Note that this cannot be specified alongside a CA cert file, client key file,\n    client cert file, and/or OAuth token. In client mode, use\nspark.kubernetes.authenticate.serviceAccountName\ninstead.\n2.3.0\nspark.kubernetes.authenticate.executor.serviceAccountName\n(value of spark.kubernetes.authenticate.driver.serviceAccountName)\nService account that is used when running the executor pod.\n    If this parameter is not setup, the fallback logic", "question": "What service account is used when running the driver pod?", "answers": {"text": ["default"], "answer_start": [195]}}
{"context": "enticate.driver.serviceAccountName)\nService account that is used when running the executor pod.\n    If this parameter is not setup, the fallback logic will use the driver's service account.\n3.1.0\nspark.kubernetes.authenticate.caCertFile\n(none)\nIn client mode, path to the CA cert file for connecting to the Kubernetes API server over TLS when\n    requesting executors. Specify this as a path as opposed to a URI (i.e. do not provide a scheme).\n2.4.0\nspark.kubernetes.authenticate.clientKeyFile\n(none)\nIn client mode, path to the client key file for authenticating against the Kubernetes API server\n    when requesting executors. Specify this as a path as opposed to a URI (i.e. do not provide a scheme).\n2.4.0\nspark.kubernetes.authenticate.clientCertFile\n(none)\nIn client mode, path to the client cer", "question": "What is used when running the executor pod if the service account parameter is not setup?", "answers": {"text": ["the driver's service account"], "answer_start": [160]}}
{"context": "s.authenticate.oauthTokenFile\n(none)\nIn client mode, path to the file containing the OAuth token to use when authenticating against the Kubernetes API\n    server when requesting executors.\n2.4.0\nspark.kubernetes.driver.label.[LabelName]\n(none)\nAdd the label specified by\nLabelName\nto the driver pod.\n    For example,\nspark.kubernetes.driver.label.something=true\n.\n    Note that Spark also adds its own labels to the driver pod\n    for bookkeeping purposes.\n2.3.0\nspark.kubernetes.driver.annotation.[AnnotationName]\n(none)\nAdd the Kubernetes\nannotation\nspecified by\nAnnotationName\nto the driver pod.\n    For example,\nspark.kubernetes.driver.annotation.something=true\n.\n2.3.0\nspark.kubernetes.driver.service.label.[LabelName]\n(none)\nAdd the Kubernetes\nlabel\nspecified by\nLabelName\nto the driver service", "question": "What does the property spark.kubernetes.driver.label.[LabelName] do?", "answers": {"text": ["Add the label specified by\nLabelName\nto the driver pod."], "answer_start": [244]}}
{"context": ".something=true\n.\n2.3.0\nspark.kubernetes.driver.service.label.[LabelName]\n(none)\nAdd the Kubernetes\nlabel\nspecified by\nLabelName\nto the driver service.\n    For example,\nspark.kubernetes.driver.service.label.something=true\n.\n    Note that Spark also adds its own labels to the driver service\n    for bookkeeping purposes.\n3.4.0\nspark.kubernetes.driver.service.annotation.[AnnotationName]\n(none)\nAdd the Kubernetes\nannotation\nspecified by\nAnnotationName\nto the driver service.\n    For example,\nspark.kubernetes.driver.service.annotation.something=true\n.\n3.0.0\nspark.kubernetes.executor.label.[LabelName]\n(none)\nAdd the label specified by\nLabelName\nto the executor pods.\n    For example,\nspark.kubernetes.executor.label.something=true\n.\n    Note that Spark also adds its own labels to the executor pod\n ", "question": "What is an example of setting a Kubernetes label for the driver service?", "answers": {"text": ["spark.kubernetes.driver.service.label.something=true"], "answer_start": [169]}}
{"context": "Type\ntype to the driver pod on the path specified in the value. For example,\nspark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.path=/checkpoint\n.\n2.4.0\nspark.kubernetes.driver.volumes.[VolumeType].[VolumeName].mount.subPath\n(none)\nSpecifies a\nsubpath\nto be mounted from the volume into the driver pod.\nspark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.subPath=checkpoint\n.\n3.0.0\nspark.kubernetes.driver.volumes.[VolumeType].[VolumeName].mount.readOnly\n(none)\nSpecify if the mounted volume is read only or not. For example,\nspark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.readOnly=false\n.\n2.4.0\nspark.kubernetes.driver.volumes.[VolumeType].[VolumeName].options.[OptionName]\n(none)\nConfigure\nKubernetes Volume\noptions passed to ", "question": "What does spark.kubernetes.driver.volumes.[VolumeType].[VolumeName].mount.readOnly configure?", "answers": {"text": ["Specify if the mounted volume is read only or not."], "answer_start": [507]}}
{"context": "lumes.persistentVolumeClaim.checkpointpvc.label.foo=bar\n.\n4.0.0\nspark.kubernetes.driver.volumes.[VolumeType].[VolumeName].annotation.[AnnotationName]\n(none)\nConfigure\nKubernetes Volume\nannotations passed to the Kubernetes with\nAnnotationName\nas key having specified value, must conform with Kubernetes annotations format. For example,\nspark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.annotation.foo=bar\n.\n4.0.0\nspark.kubernetes.executor.volumes.[VolumeType].[VolumeName].mount.path\n(none)\nAdd the\nKubernetes Volume\nnamed\nVolumeName\nof the\nVolumeType\ntype to the executor pod on the path specified in the value. For example,\nspark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.path=/checkpoint\n.\n2.4.0\nspark.kubernetes.executor.volumes.[VolumeType].[VolumeNa", "question": "What is an example of configuring Kubernetes Volume annotations?", "answers": {"text": ["spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.annotation.foo=bar"], "answer_start": [335]}}
{"context": "actor that will allocate memory to non-JVM memory, which includes off-heap memory allocations, non-JVM tasks, various systems processes, and\ntmpfs\n-based local directories when\nspark.kubernetes.local.dirs.tmpfs\nis\ntrue\n. For JVM-based jobs this value will default to 0.10 and 0.40 for non-JVM jobs.\n    This is done as non-JVM tasks need more non-JVM heap space and such tasks commonly fail with \"Memory Overhead Exceeded\" errors. This preempts this error with a higher default.\n    This will be overridden by the value set by\nspark.driver.memoryOverheadFactor\nand\nspark.executor.memoryOverheadFactor\nexplicitly.\n2.4.0\nspark.kubernetes.pyspark.pythonVersion\n\"3\"\nThis sets the major Python version of the docker image used to run the driver and executor containers.\n   It can be only \"3\". This configu", "question": "What value will the memory allocation default to for JVM-based jobs?", "answers": {"text": ["0.10 and 0.40 for non-JVM jobs."], "answer_start": [267]}}
{"context": "Version\n\"3\"\nThis sets the major Python version of the docker image used to run the driver and executor containers.\n   It can be only \"3\". This configuration was deprecated from Spark 3.1.0, and is effectively no-op.\n   Users should set 'spark.pyspark.python' and 'spark.pyspark.driver.python' configurations or\n   'PYSPARK_PYTHON' and 'PYSPARK_DRIVER_PYTHON' environment variables.\n2.4.0\nspark.kubernetes.kerberos.krb5.path\n(none)\nSpecify the local location of the krb5.conf file to be mounted on the driver and executors for Kerberos interaction.\n   It is important to note that the KDC defined needs to be visible from inside the containers.\n3.0.0\nspark.kubernetes.kerberos.krb5.configMapName\n(none)\nSpecify the name of the ConfigMap, containing the krb5.conf file, to be mounted on the driver and ", "question": "What is the only allowed value for the 'version' configuration?", "answers": {"text": ["It can be only \"3\"."], "answer_start": [118]}}
{"context": "ainer name to be used as a basis for the driver in the given\npod template\n.\n   For example\nspark.kubernetes.driver.podTemplateContainerName=spark-driver\n3.0.0\nspark.kubernetes.executor.podTemplateFile\n(none)\nSpecify the local file that contains the executor\npod template\n. For example\nspark.kubernetes.executor.podTemplateFile=/path/to/executor-pod-template.yaml\n3.0.0\nspark.kubernetes.executor.podTemplateContainerName\n(none)\nSpecify the container name to be used as a basis for the executor in the given\npod template\n.\n   For example\nspark.kubernetes.executor.podTemplateContainerName=spark-executor\n3.0.0\nspark.kubernetes.executor.deleteOnTermination\ntrue\nSpecify whether executor pods should be deleted in case of failure or normal termination.\n3.0.0\nspark.kubernetes.executor.checkAllContainers\n", "question": "What is an example of how to specify the driver container name?", "answers": {"text": ["spark.kubernetes.driver.podTemplateContainerName=spark-driver"], "answer_start": [91]}}
{"context": "ion\ntrue\nSpecify whether executor pods should be deleted in case of failure or normal termination.\n3.0.0\nspark.kubernetes.executor.checkAllContainers\ntrue\nSpecify whether executor pods should be check all containers (including sidecars) or only the executor container when determining the pod status.\n3.1.0\nspark.kubernetes.submission.connectionTimeout\n10000\nConnection timeout in milliseconds for the kubernetes client to use for starting the driver.\n3.0.0\nspark.kubernetes.submission.requestTimeout\n10000\nRequest timeout in milliseconds for the kubernetes client to use for starting the driver.\n3.0.0\nspark.kubernetes.trust.certificates\nfalse\nIf set to true then client can submit to kubernetes cluster only with token.\n3.2.0\nspark.kubernetes.driver.connectionTimeout\n10000\nConnection timeout in mi", "question": "What does spark.kubernetes.submission.connectionTimeout configure?", "answers": {"text": ["Connection timeout in milliseconds for the kubernetes client to use for starting the driver."], "answer_start": [359]}}
{"context": "t to true then client can submit to kubernetes cluster only with token.\n3.2.0\nspark.kubernetes.driver.connectionTimeout\n10000\nConnection timeout in milliseconds for the kubernetes client in driver to use when requesting executors.\n3.0.0\nspark.kubernetes.driver.requestTimeout\n10000\nRequest timeout in milliseconds for the kubernetes client in driver to use when requesting executors.\n3.0.0\nspark.kubernetes.appKillPodDeletionGracePeriod\n(none)\nSpecify the grace period in seconds when deleting a Spark application using spark-submit.\n3.0.0\nspark.kubernetes.dynamicAllocation.deleteGracePeriod\n5s\nHow long to wait for executors to shut down gracefully before a forceful kill.\n3.0.0\nspark.kubernetes.file.upload.path\n(none)\nPath to store files at the spark submit side in cluster mode. For example:\nspa", "question": "What is the connection timeout in milliseconds for the kubernetes client in driver when requesting executors?", "answers": {"text": ["10000"], "answer_start": [120]}}
{"context": " before a forceful kill.\n3.0.0\nspark.kubernetes.file.upload.path\n(none)\nPath to store files at the spark submit side in cluster mode. For example:\nspark.kubernetes.file.upload.path=s3a://<s3-bucket>/path\nFile should specified as\nfile://path/to/file\nor absolute path.\n3.0.0\nspark.kubernetes.executor.decommissionLabel\n(none)\nLabel to be applied to pods which are exiting or being decommissioned. Intended for use\n    with pod disruption budgets, deletion costs, and similar.\n3.3.0\nspark.kubernetes.executor.decommissionLabelValue\n(none)\nValue to be applied with the label when\nspark.kubernetes.executor.decommissionLabel\nis enabled.\n3.3.0\nspark.kubernetes.executor.scheduler.name\n(none)\nSpecify the scheduler name for each executor pod.\n3.0.0\nspark.kubernetes.driver.scheduler.name\n(none)\nSpecify the ", "question": "How should a file be specified for spark.kubernetes.file.upload.path?", "answers": {"text": ["File should specified as\nfile://path/to/file\nor absolute path."], "answer_start": [204]}}
{"context": "or allocation for this\n    application. Those newly requested executors which are unknown by Kubernetes yet are\n    also counted into this limit as they will change into pending PODs by time.\n    This limit is independent from the resource profiles as it limits the sum of all\n    allocation for all the used resource profiles.\n3.2.0\nspark.kubernetes.allocation.pods.allocator\ndirect\nAllocator to use for pods. Possible values are\ndirect\n(the default)\n    and\nstatefulset\n, or a full class name of a class implementing `AbstractPodsAllocator`.\n    Future version may add Job or replicaset. This is a developer API and may change\n    or be removed at anytime.\n3.3.0\nspark.kubernetes.allocation.executor.timeout\n600s\nTime to wait before a newly created executor POD request, which does not reached\n    ", "question": "What are the possible values for spark.kubernetes.allocation.pods.allocator?", "answers": {"text": ["direct\n(the default)\n    and\nstatefulset"], "answer_start": [431]}}
{"context": "ing. This should be used carefully.\n3.3.0\nspark.kubernetes.executor.eventProcessingInterval\n1s\nInterval between successive inspection of executor events sent from the Kubernetes API.\n2.4.0\nspark.kubernetes.executor.rollInterval\n0s\nInterval between executor roll operations. It's disabled by default with `0s`.\n3.3.0\nspark.kubernetes.executor.minTasksPerExecutorBeforeRolling\n0\nThe minimum number of tasks per executor before rolling.\n    Spark will not roll executors whose total number of tasks is smaller\n    than this configuration. The default value is zero.\n3.3.0\nspark.kubernetes.executor.rollPolicy\nOUTLIER\nExecutor roll policy: Valid values are ID, ADD_TIME, TOTAL_GC_TIME,\n    TOTAL_DURATION, FAILED_TASKS, and OUTLIER (default).\n    When executor roll happens, Spark uses this policy to cho", "question": "What is the default value for spark.kubernetes.executor.minTasksPerExecutorBeforeRolling?", "answers": {"text": ["The default value is zero."], "answer_start": [536]}}
{"context": "re ID, ADD_TIME, TOTAL_GC_TIME,\n    TOTAL_DURATION, FAILED_TASKS, and OUTLIER (default).\n    When executor roll happens, Spark uses this policy to choose\n    an executor and decommission it. The built-in policies are based on executor summary\n    and newly started executors are protected by spark.kubernetes.executor.minTasksPerExecutorBeforeRolling.\n    ID policy chooses an executor with the smallest executor ID.\n    ADD_TIME policy chooses an executor with the smallest add-time.\n    TOTAL_GC_TIME policy chooses an executor with the biggest total task GC time.\n    TOTAL_DURATION policy chooses an executor with the biggest total task time.\n    AVERAGE_DURATION policy chooses an executor with the biggest average task time.\n    FAILED_TASKS policy chooses an executor with the most number of f", "question": "Which policy chooses an executor with the biggest total task time?", "answers": {"text": ["TOTAL_DURATION policy chooses an executor with the biggest total task time."], "answer_start": [571]}}
{"context": " AVERAGE_DURATION policy chooses an executor with the biggest average task time.\n    FAILED_TASKS policy chooses an executor with the most number of failed tasks.\n    OUTLIER policy chooses an executor with outstanding statistics which is bigger than\n    at least two standard deviation from the mean in average task time,\n    total task time, total task GC time, and the number of failed tasks if exists.\n    If there is no outlier, it works like TOTAL_DURATION policy.\n3.3.0\nPod template properties\nSee the below table for the full list of pod specifications that will be overwritten by spark.\nPod Metadata\nPod metadata key\nModified value\nDescription\nname\nValue of\nspark.kubernetes.driver.pod.name\nThe driver pod name will be overwritten with either the configured or default value of\nspark.kuberne", "question": "What does the OUTLIER policy do when no outlier is found?", "answers": {"text": ["If there is no outlier, it works like TOTAL_DURATION policy."], "answer_start": [410]}}
{"context": "on\nname\nValue of\nspark.kubernetes.driver.pod.name\nThe driver pod name will be overwritten with either the configured or default value of\nspark.kubernetes.driver.pod.name\n. The executor pod names will be unaffected.\nnamespace\nValue of\nspark.kubernetes.namespace\nSpark makes strong assumptions about the driver and executor namespaces. Both driver and executor namespaces will\n    be replaced by either the configured or default spark conf value.\nlabels\nAdds the labels from\nspark.kubernetes.{driver,executor}.label.*\nSpark will add additional labels specified by the spark configuration.\nannotations\nAdds the annotations from\nspark.kubernetes.{driver,executor}.annotation.*\nSpark will add additional annotations specified by the spark configuration.\nPod Spec\nPod spec key\nModified value\nDescription\nim", "question": "What happens to the driver and executor namespaces in Spark when using Kubernetes?", "answers": {"text": ["Both driver and executor namespaces will\n    be replaced by either the configured or default spark conf value."], "answer_start": [334]}}
{"context": "executor}.annotation.*\nSpark will add additional annotations specified by the spark configuration.\nPod Spec\nPod spec key\nModified value\nDescription\nimagePullSecrets\nAdds image pull secrets from\nspark.kubernetes.container.image.pullSecrets\nAdditional pull secrets will be added from the spark configuration to both executor pods.\nnodeSelector\nAdds node selectors from\nspark.kubernetes.node.selector.*\nAdditional node selectors will be added from the spark configuration to both executor pods.\nrestartPolicy\n\"never\"\nSpark assumes that both drivers and executors never restart.\nserviceAccount\nValue of\nspark.kubernetes.authenticate.driver.serviceAccountName\nSpark will override\nserviceAccount\nwith the value of the spark configuration for only\n    driver pods, and only if the spark configuration is spe", "question": "What does Spark assume about drivers and executors?", "answers": {"text": ["Spark assumes that both drivers and executors never restart."], "answer_start": [514]}}
{"context": "Name\nSpark will override\nserviceAccount\nwith the value of the spark configuration for only\n    driver pods, and only if the spark configuration is specified. Executor pods will remain unaffected.\nserviceAccountName\nValue of\nspark.kubernetes.authenticate.driver.serviceAccountName\nSpark will override\nserviceAccountName\nwith the value of the spark configuration for only\n    driver pods, and only if the spark configuration is specified. Executor pods will remain unaffected.\nvolumes\nAdds volumes from\nspark.kubernetes.{driver,executor}.volumes.[VolumeType].[VolumeName].mount.path\nSpark will add volumes as specified by the spark conf, as well as additional volumes necessary for passing\n    spark conf and pod template files.\nContainer spec\nThe following affect the driver and executor containers. A", "question": "What happens to executor pods when Spark overrides serviceAccountName?", "answers": {"text": ["Executor pods will remain unaffected."], "answer_start": [158]}}
{"context": "itional volumes necessary for passing\n    spark conf and pod template files.\nContainer spec\nThe following affect the driver and executor containers. All other containers in the pod spec will be unaffected.\nContainer spec key\nModified value\nDescription\nenv\nAdds env variables from\nspark.kubernetes.driverEnv.[EnvironmentVariableName]\nSpark will add driver env variables from\nspark.kubernetes.driverEnv.[EnvironmentVariableName]\n, and\n    executor env variables from\nspark.executorEnv.[EnvironmentVariableName]\n.\nimage\nValue of\nspark.kubernetes.{driver,executor}.container.image\nThe image will be defined by the spark configurations.\nimagePullPolicy\nValue of\nspark.kubernetes.container.image.pullPolicy\nSpark will override the pull policy for both driver and executors.\nname\nSee description\nThe contain", "question": "What configuration keys define the image used for the driver and executor containers?", "answers": {"text": ["spark.kubernetes.{driver,executor}.container.image"], "answer_start": [526]}}
{"context": "esource scheduling.\nThe user is responsible to properly configuring the Kubernetes cluster to have the resources available and ideally isolate each resource per container so that a resource is not shared between multiple containers. If the resource is not isolated the user is responsible for writing a discovery script so that the resource is not shared between containers. See the Kubernetes documentation for specifics on configuring Kubernetes with\ncustom resources\n.\nSpark automatically handles translating the Spark configs\nspark.{driver/executor}.resource.{resourceType}\ninto the kubernetes configs as long as the Kubernetes resource type follows the Kubernetes device plugin format of\nvendor-domain/resourcetype\n. The user must specify the vendor using the\nspark.{driver/executor}.resource.{r", "question": "What format must Kubernetes resource types follow for Spark to automatically translate Spark configs?", "answers": {"text": ["vendor-domain/resourcetype"], "answer_start": [693]}}
{"context": "ows the Kubernetes device plugin format of\nvendor-domain/resourcetype\n. The user must specify the vendor using the\nspark.{driver/executor}.resource.{resourceType}.vendor\nconfig. The user does not need to explicitly add anything if you are using Pod templates. For reference and an example, you can see the Kubernetes documentation for scheduling\nGPUs\n. Spark only supports setting the resource limits.\nKubernetes does not tell Spark the addresses of the resources allocated to each container. For that reason, the user must specify a discovery script that gets run by the executor on startup to discover what resources are available to that executor. You can find an example scripts in\nexamples/src/main/scripts/getGpusResources.sh\n. The script must have execute permissions set and the user should s", "question": "Where can users find an example discovery script?", "answers": {"text": ["examples/src/main/scripts/getGpusResources.sh"], "answer_start": [686]}}
{"context": "ftserver\n-Pkubernetes\n-Pvolcano\nUsage\nSpark on Kubernetes allows using Volcano as a custom scheduler. Users can use Volcano to\nsupport more advanced resource scheduling: queue scheduling, resource reservation, priority scheduling, and more.\nTo use Volcano as a custom scheduler the user needs to specify the following configuration options:\n# Specify volcano scheduler and PodGroup template\n--conf\nspark.kubernetes.scheduler.name\n=\nvolcano\n--conf\nspark.kubernetes.scheduler.volcano.podGroupTemplateFile\n=\n/path/to/podgroup-template.yaml\n# Specify driver/executor VolcanoFeatureStep\n--conf\nspark.kubernetes.driver.pod.featureSteps\n=\norg.apache.spark.deploy.k8s.features.VolcanoFeatureStep\n--conf\nspark.kubernetes.executor.pod.featureSteps\n=\norg.apache.spark.deploy.k8s.features.VolcanoFeatureStep\nVolc", "question": "How can a user specify Volcano as a custom scheduler in Spark on Kubernetes?", "answers": {"text": ["# Specify volcano scheduler and PodGroup template\n--conf\nspark.kubernetes.scheduler.name\n=\nvolcano\n--conf\nspark.kubernetes.scheduler.volcano.podGroupTemplateFile\n=\n/path/to/podgroup-template.yaml"], "answer_start": [341]}}
{"context": "ache.github.io/yunikorn-release\nhelm repo update\nhelm\ninstall\nyunikorn yunikorn/yunikorn\n--namespace\nyunikorn\n--version\n1.6.3\n--create-namespace\n--set\nembedAdmissionController\n=\nfalse\nThe above steps will install YuniKorn v1.6.3 on an existing Kubernetes cluster.\nGet started\nSubmit Spark jobs with the following extra options:\n--conf\nspark.kubernetes.scheduler.name\n=\nyunikorn\n--conf\nspark.kubernetes.driver.label.queue\n=\nroot.default\n--conf\nspark.kubernetes.executor.label.queue\n=\nroot.default\n--conf\nspark.kubernetes.driver.annotation.yunikorn.apache.org/app-id\n={{\nAPP_ID\n}}\n--conf\nspark.kubernetes.executor.annotation.yunikorn.apache.org/app-id\n={{\nAPP_ID\n}}\nNote that {{APP_ID}} is the built-in variable that will be substituted with Spark job ID automatically.\nWith the above configuration, th", "question": "What version of YuniKorn will be installed using the provided steps?", "answers": {"text": ["1.6.3"], "answer_start": [120]}}
{"context": "={{\nAPP_ID\n}}\nNote that {{APP_ID}} is the built-in variable that will be substituted with Spark job ID automatically.\nWith the above configuration, the job will be scheduled by YuniKorn scheduler instead of the default Kubernetes scheduler.\nStage Level Scheduling Overview\nStage level scheduling is supported on Kubernetes:\nWhen dynamic allocation is disabled: It allows users to specify different task resource requirements at the stage level and will use the same executors requested at startup.\nWhen dynamic allocation is enabled: It allows users to specify task and executor resource requirements at the stage level and will request the extra executors. This also requires\nspark.dynamicAllocation.shuffleTracking.enabled\nto be enabled since Kubernetes doesn’t support an external shuffle service ", "question": "What is the built-in variable that will be substituted with Spark job ID automatically?", "answers": {"text": ["{{APP_ID}}"], "answer_start": [24]}}
{"context": "cutors. This also requires\nspark.dynamicAllocation.shuffleTracking.enabled\nto be enabled since Kubernetes doesn’t support an external shuffle service at this time. The order in which containers for different profiles is requested from Kubernetes is not guaranteed. Note that since dynamic allocation on Kubernetes requires the shuffle tracking feature, this means that executors from previous stages that used a different ResourceProfile may not idle timeout due to having shuffle data on them. This could result in using more cluster resources and in the worst case if there are no remaining resources on the Kubernetes cluster then Spark could potentially hang. You may consider looking at config\nspark.dynamicAllocation.shuffleTracking.timeout\nto set a timeout, but that could result in data havin", "question": "What is a potential consequence of dynamic allocation on Kubernetes when executors from previous stages have shuffle data?", "answers": {"text": ["This could result in using more cluster resources and in the worst case if there are no remaining resources on the Kubernetes cluster then Spark could potentially hang."], "answer_start": [495]}}
{"context": "ntially hang. You may consider looking at config\nspark.dynamicAllocation.shuffleTracking.timeout\nto set a timeout, but that could result in data having to be recomputed if the shuffle data is really needed.\nNote, there is a difference in the way pod template resources are handled between the base default profile and custom ResourceProfiles. Any resources specified in the pod template file will only be used with the base default profile. If you create custom ResourceProfiles be sure to include all necessary resources there since the resources from the template file will not be propagated to custom ResourceProfiles.", "question": "What happens to resources specified in the pod template file when custom ResourceProfiles are created?", "answers": {"text": ["If you create custom ResourceProfiles be sure to include all necessary resources there since the resources from the template file will not be propagated to custom ResourceProfiles."], "answer_start": [441]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark Security\nSpark Security: Things You Need To Know\nSpark RPC (Communication protocol between Spark processes)\nAuthentication\nYARN\nKubernetes\nNetwork Encryption\nSSL Encryption (Preferred)\nAES-based Encryption (Legacy)\nLocal Storage Encryption\nWeb U", "question": "What communication protocol is used between Spark processes?", "answers": {"text": ["Spark RPC (Communication protocol between Spark processes)"], "answer_start": [604]}}
{"context": "k processes)\nAuthentication\nYARN\nKubernetes\nNetwork Encryption\nSSL Encryption (Preferred)\nAES-based Encryption (Legacy)\nLocal Storage Encryption\nWeb UI\nAuthentication and Authorization\nSpark History Server ACLs\nSSL Configuration\nPreparing the key stores\nYARN mode\nStandalone mode\nHTTP Security Headers\nConfiguring Ports for Network Security\nStandalone mode only\nAll cluster managers\nKerberos\nLong-Running Applications\nUsing a Keytab\nUsing a ticket cache\nSecure Interaction with Kubernetes\nEvent Logging\nPersisting driver logs in client mode\nSpark Security: Things You Need To Know\nSecurity features like authentication are not enabled by default. When deploying a cluster that is open to the internet\nor an untrusted network, it’s important to secure access to the cluster to prevent unauthorized app", "question": "What type of encryption is preferred?", "answers": {"text": ["SSL Encryption (Preferred)"], "answer_start": [63]}}
{"context": "n. The secret is propagated to executor pods using environment variables. This means\nthat any user that can list pods in the namespace where the Spark application is running can\nalso see their authentication secret. Access control rules should be properly set up by the\nKubernetes admin to ensure that Spark authentication is secure.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.authenticate\nfalse\nWhether Spark authenticates its internal connections.\n1.0.0\nspark.authenticate.secret\nNone\nThe secret key used authentication. See above for when this configuration should be set.\n1.0.0\nAlternatively, one can mount authentication secrets using files and Kubernetes secrets that\nthe user mounts into their pods.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.authenticate.secret.file\nNone\nPath p", "question": "How is the secret propagated to executor pods?", "answers": {"text": ["The secret is propagated to executor pods using environment variables."], "answer_start": [3]}}
{"context": "es TLS (aka SSL) encryption via Netty’s support for SSL. Enabling SSL\nrequires keys and certificates to be properly configured. SSL is standardized and considered more\nsecure.\nThe legacy method is an AES-based encryption mechanism relying on a shared secret. This requires\nRPC authentication to also be enabled. This method uses a bespoke protocol and it is recommended\nto use SSL instead.\nOne may prefer to use the SSL based encryption in scenarios where compliance mandates the usage\nof specific protocols; or to leverage the security of a more standard encryption library. However,\nthe AES based encryption is simpler to configure and may be preferred if the only requirement\nis that data be encrypted in transit.\nIf both options are enabled in the configuration, the SSL based RPC encryption take", "question": "What is the legacy encryption method based on?", "answers": {"text": ["an AES-based encryption mechanism relying on a shared secret."], "answer_start": [197]}}
{"context": "ble for configuring this feature.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.network.crypto.enabled\nfalse\nEnable AES-based RPC encryption, including the new authentication protocol added in 2.2.0.\n2.2.0\nspark.network.crypto.cipher\nAES/CTR/NoPadding\nCipher mode to use. Defaults \"AES/CTR/NoPadding\" for backward compatibility, which is not authenticated. \n    Recommended to use \"AES/GCM/NoPadding\", which is an authenticated encryption mode.\n4.0.0, 3.5.2, 3.4.4\nspark.network.crypto.authEngineVersion\n1\nVersion of AES-based RPC encryption to use. Valid versions are 1 or 2. Version 2 is recommended.\n4.0.0\nspark.network.crypto.config.*\nNone\nConfiguration values for the commons-crypto library, such as which cipher implementations to\n    use. The config name should be the name of commons-cryp", "question": "What is the default cipher mode used for spark.network.crypto.cipher?", "answers": {"text": ["AES/CTR/NoPadding"], "answer_start": [238]}}
{"context": "s created explicitly by the user.\nThe following settings cover enabling encryption for data written to disk:\nProperty Name\nDefault\nMeaning\nSince Version\nspark.io.encryption.enabled\nfalse\nEnable local disk I/O encryption. Currently supported by all modes. It's strongly\n    recommended that RPC encryption be enabled when using this feature.\n2.1.0\nspark.io.encryption.keySizeBits\n128\nIO encryption key size in bits. Supported values are 128, 192 and 256.\n2.1.0\nspark.io.encryption.keygen.algorithm\nHmacSHA1\nThe algorithm to use when generating the IO encryption key. The supported algorithms are\n    described in the KeyGenerator section of the Java Cryptography Architecture Standard Algorithm\n    Name Documentation.\n2.1.0\nspark.io.encryption.commons.config.*\nNone\nConfiguration values for the commo", "question": "What is the default value for the property 'spark.io.encryption.enabled'?", "answers": {"text": ["false"], "answer_start": [181]}}
{"context": "rd (\n*\n) added to specific ACL\nmeans that all users will have the respective privilege. By default, only the user submitting the\napplication is added to the ACLs.\nGroup membership is established by using a configurable group mapping provider. The mapper is\nconfigured using the\nspark.user.groups.mapping\nconfig option, described in the table\nbelow.\nThe following options control the authentication of Web UIs:\nProperty Name\nDefault\nMeaning\nSince Version\nspark.ui.allowFramingFrom\nSAMEORIGIN\nAllow framing for a specific named URI via\nX-Frame-Options\n. By default, allow only from the same origin.\n1.6.0\nspark.ui.filters\nNone\nSpark supports HTTP\nAuthorization\nheader with a cryptographically signed\n    JSON Web Token via\norg.apache.spark.ui.JWSFilter\n.\nSee the\nSpark UI\nconfiguration for how to confi", "question": "What is the default value for the spark.ui.allowFramingFrom property?", "answers": {"text": ["SAMEORIGIN"], "answer_start": [480]}}
{"context": " group mapping service defined by the trait\norg.apache.spark.security.GroupMappingServiceProvider\n, which can be configured by\n    this property.\nBy default, a Unix shell-based implementation is used, which collects this information\n    from the host OS.\nNote:\nThis implementation supports only Unix/Linux-based environments.\n    Windows environment is currently\nnot\nsupported. However, a new platform/protocol can\n    be supported by implementing the trait mentioned above.\n2.0.0\nOn YARN, the view and modify ACLs are provided to the YARN service when submitting applications, and\ncontrol who has the respective privileges via YARN interfaces.\nSpark History Server ACLs\nAuthentication for the SHS Web UI is enabled the same way as for regular applications, using\nservlet filters.\nTo enable authoriza", "question": "What operating systems are supported by the default Unix shell-based implementation for group mapping?", "answers": {"text": ["This implementation supports only Unix/Linux-based environments."], "answer_start": [261]}}
{"context": "lowing table describes the SSL configuration namespaces:\nConfig Namespace\nComponent\nspark.ssl\nThe default SSL configuration. These values will apply to all namespaces below, unless\n      explicitly overridden at the namespace level.\nspark.ssl.ui\nSpark application Web UI\nspark.ssl.standalone\nStandalone Master / Worker Web UI\nspark.ssl.historyServer\nHistory Server Web UI\nspark.ssl.rpc\nSpark RPC communication\nThe full breakdown of available SSL options can be found below. The\n${ns}\nplaceholder should be\nreplaced with one of the above namespaces.\nProperty Name\nDefault\nMeaning\nSupported Namespaces\n${ns}.enabled\nfalse\nEnables SSL. When enabled,\n${ns}.ssl.protocol\nis required.\nui,standalone,historyServer,rpc\n${ns}.port\nNone\nThe port where the SSL service will listen on.\nThe port must be defined w", "question": "Para quais namespaces o SSL pode ser habilitado usando a propriedade `${ns}.enabled`?", "answers": {"text": ["ui,standalone,historyServer,rpc"], "answer_start": [679]}}
{"context": "   of the Java security guide. The list for Java 17 can be found at\nthis\npage.\nNote: If not set, the default cipher suite for the JRE will be used.\nui,standalone,historyServer,rpc\n${ns}.keyPassword\nNone\nThe password to the private key in the key store.\nui,standalone,historyServer,rpc\n${ns}.keyStore\nNone\nPath to the key store file. The path can be absolute or relative to the directory in which the\n      process is started.\nui,standalone,historyServer,rpc\n${ns}.keyStorePassword\nNone\nPassword to the key store.\nui,standalone,historyServer,rpc\n${ns}.keyStoreType\nJKS\nThe type of the key store.\nui,standalone,historyServer\n${ns}.protocol\nNone\nTLS protocol to use. The protocol must be supported by JVM.\nThe reference list of protocols can be found in the \"Additional JSSE Standard Names\"\n      sectio", "question": "What is the default cipher suite used if not set?", "answers": {"text": ["If not set, the default cipher suite for the JRE will be used."], "answer_start": [85]}}
{"context": "tocol to use. The protocol must be supported by JVM.\nThe reference list of protocols can be found in the \"Additional JSSE Standard Names\"\n      section of the Java security guide. For Java 17, the list can be found at\nthis\npage.\nui,standalone,historyServer,rpc\n${ns}.needClientAuth\nfalse\nWhether to require client authentication.\nui,standalone,historyServer\n${ns}.trustStore\nNone\nPath to the trust store file. The path can be absolute or relative to the directory in which\n      the process is started.\nui,standalone,historyServer,rpc\n${ns}.trustStorePassword\nNone\nPassword for the trust store.\nui,standalone,historyServer,rpc\n${ns}.trustStoreType\nJKS\nThe type of the trust store.\nui,standalone,historyServer\n${ns}.openSSLEnabled\nfalse\nWhether to use OpenSSL for cryptographic operations instead of t", "question": "What is the default value for the property `${ns}.needClientAuth`?", "answers": {"text": ["false"], "answer_start": [282]}}
{"context": "S\nThe type of the trust store.\nui,standalone,historyServer\n${ns}.openSSLEnabled\nfalse\nWhether to use OpenSSL for cryptographic operations instead of the JDK SSL provider.\n      This setting requires the `certChain` and `privateKey` settings to be set.\n      This takes precedence over the `keyStore` and `trustStore` settings if both are specified.\n      If the OpenSSL library is not available at runtime, we will fall back to the JDK provider.\nrpc\n${ns}.privateKey\nNone\nPath to the private key file in PEM format. The path can be absolute or relative to the \n      directory in which the process is started. \n      This setting is required when using the OpenSSL implementation.\nrpc\n${ns}.privateKeyPassword\nNone\nThe password to the above private key file in PEM format.\nrpc\n${ns}.certChain\nNone\nPa", "question": "What is required when using the OpenSSL implementation?", "answers": {"text": ["This setting requires the `certChain` and `privateKey` settings to be set."], "answer_start": [177]}}
{"context": "ng the OpenSSL implementation.\nrpc\n${ns}.privateKeyPassword\nNone\nThe password to the above private key file in PEM format.\nrpc\n${ns}.certChain\nNone\nPath to the certificate chain file in PEM format. The path can be absolute or relative to the \n      directory in which the process is started. \n      This setting is required when using the OpenSSL implementation.\nrpc\n${ns}.trustStoreReloadingEnabled\nfalse\nWhether the trust store should be reloaded periodically.\n      This setting is mostly only useful in standalone deployments, not k8s or yarn deployments.\nrpc\n${ns}.trustStoreReloadIntervalMs\n10000\nThe interval at which the trust store should be reloaded (in milliseconds).\n      This setting is mostly only useful in standalone deployments, not k8s or yarn deployments.\nrpc\nSpark also supports ", "question": "When is the 'trustStoreReloadingEnabled' setting most useful?", "answers": {"text": ["This setting is mostly only useful in standalone deployments, not k8s or yarn deployments."], "answer_start": [469]}}
{"context": "sed by Spark, like:\n<property>\n    <name>hadoop.security.credential.provider.path</name>\n    <value>jceks://hdfs@nn1.example.com:9001/user/backup/ssl.jceks</value>\n  </property>\nOr via SparkConf “spark.hadoop.hadoop.security.credential.provider.path=jceks://hdfs@nn1.example.com:9001/user/backup/ssl.jceks”.\nPreparing the key stores\nKey stores can be generated by\nkeytool\nprogram. The reference documentation for this tool for\nJava 17 is\nhere\n.\nThe most basic steps to configure the key stores and the trust store for a Spark Standalone\ndeployment mode is as follows:\nGenerate a key pair for each node\nExport the public key of the key pair to a file on each node\nImport all exported public keys into a single trust store\nDistribute the trust store to the cluster nodes\nYARN mode\nTo provide a local tr", "question": "How can key stores be generated?", "answers": {"text": ["Key stores can be generated by\nkeytool\nprogram."], "answer_start": [333]}}
{"context": "\nHTTP Security Headers\nApache Spark can be configured to include HTTP headers to aid in preventing Cross Site Scripting\n(XSS), Cross-Frame Scripting (XFS), MIME-Sniffing, and also to enforce HTTP Strict Transport\nSecurity.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.ui.xXssProtection\n1; mode=block\nValue for HTTP X-XSS-Protection response header. You can choose appropriate value\n    from below:\n0\n(Disables XSS filtering)\n1\n(Enables XSS filtering. If a cross-site scripting attack is detected,\n        the browser will sanitize the page.)\n1; mode=block\n(Enables XSS filtering. The browser will prevent rendering\n        of the page if an attack is detected.)\n2.3.0\nspark.ui.xContentTypeOptions.enabled\ntrue\nWhen enabled, X-Content-Type-Options HTTP response header will be set to \"nosniff\".\n2", "question": "What value is set for the HTTP X-XSS-Protection response header by default?", "answers": {"text": ["1; mode=block"], "answer_start": [291]}}
{"context": "ck is detected.)\n2.3.0\nspark.ui.xContentTypeOptions.enabled\ntrue\nWhen enabled, X-Content-Type-Options HTTP response header will be set to \"nosniff\".\n2.3.0\nspark.ui.strictTransportSecurity\nNone\nValue for HTTP Strict Transport Security (HSTS) Response Header. You can choose appropriate\n    value from below and set\nexpire-time\naccordingly. This option is only used when\n    SSL/TLS is enabled.\nmax-age=<expire-time>\nmax-age=<expire-time>; includeSubDomains\nmax-age=<expire-time>; preload\n2.3.0\nConfiguring Ports for Network Security\nGenerally speaking, a Spark cluster and its services are not deployed on the public internet.\nThey are generally private services, and should only be accessible within the network of the\norganization that deploys Spark. Access to the hosts and ports used by Spark serv", "question": "What HTTP response header will be set when spark.ui.xContentTypeOptions.enabled is true?", "answers": {"text": ["X-Content-Type-Options HTTP response header will be set to \"nosniff\"."], "answer_start": [79]}}
{"context": "te services, and should only be accessible within the network of the\norganization that deploys Spark. Access to the hosts and ports used by Spark services should\nbe limited to origin hosts that need to access the services.\nHowever, like the REST Submission port, Spark also supports HTTP\nAuthorization\nheader\nwith a cryptographically signed JSON Web Token (JWT) for all UI ports.\nTo use it, a user needs to configure\nspark.ui.filters=org.apache.spark.ui.JWSFilter\nand\nspark.org.apache.spark.ui.JWSFilter.param.secretKey=BASE64URL-ENCODED-KEY\n.\nBelow are the primary ports that Spark uses for its communication and how to\nconfigure those ports.\nStandalone mode only\nFrom\nTo\nDefault Port\nPurpose\nConfiguration\n    Setting\nNotes\nBrowser\nStandalone Master\n8080\nWeb UI\nspark.master.ui.port /\nSPARK_MASTER_", "question": "How can a user enable HTTP Authorization with a JWT for Spark UI ports?", "answers": {"text": ["To use it, a user needs to configure\nspark.ui.filters=org.apache.spark.ui.JWSFilter\nand\nspark.org.apache.spark.ui.JWSFilter.param.secretKey=BASE64URL-ENCODED-KEY"], "answer_start": [380]}}
{"context": "lone mode only\nFrom\nTo\nDefault Port\nPurpose\nConfiguration\n    Setting\nNotes\nBrowser\nStandalone Master\n8080\nWeb UI\nspark.master.ui.port /\nSPARK_MASTER_WEBUI_PORT\nJetty-based. Standalone mode only.\nBrowser\nStandalone Worker\n8081\nWeb UI\nspark.worker.ui.port /\nSPARK_WORKER_WEBUI_PORT\nJetty-based. Standalone mode only.\nDriver /\nStandalone Worker\nStandalone Master\n7077\nSubmit job to cluster /\nJoin cluster\nSPARK_MASTER_PORT\nSet to \"0\" to choose a port randomly. Standalone mode only.\nExternal Service\nStandalone Master\n6066\nSubmit job to cluster via REST API\nspark.master.rest.port\nUse\nspark.master.rest.enabled\nto enable/disable this service. Standalone mode only.\nStandalone Master\nStandalone Worker\n(random)\nSchedule executors\nSPARK_WORKER_PORT\nSet to \"0\" to choose a port randomly. Standalone mode o", "question": "What is the default port for the Standalone Master Web UI?", "answers": {"text": ["8080"], "answer_start": [102]}}
{"context": "e mode only.\nStandalone Master\nStandalone Worker\n(random)\nSchedule executors\nSPARK_WORKER_PORT\nSet to \"0\" to choose a port randomly. Standalone mode only.\nAll cluster managers\nFrom\nTo\nDefault Port\nPurpose\nConfiguration\n    Setting\nNotes\nBrowser\nApplication\n4040\nWeb UI\nspark.ui.port\nJetty-based\nBrowser\nHistory Server\n18080\nWeb UI\nspark.history.ui.port\nJetty-based\nExecutor /\nStandalone Master\nDriver\n(random)\nConnect to application /\nNotify executor state changes\nspark.driver.port\nSet to \"0\" to choose a port randomly.\nExecutor / Driver\nExecutor / Driver\n(random)\nBlock Manager port\nspark.blockManager.port\nRaw socket via ServerSocketChannel\nKerberos\nSpark supports submitting applications in environments that use Kerberos for authentication.\nIn most cases, Spark relies on the credentials of the ", "question": "What is the default port for the History Server Web UI?", "answers": {"text": ["18080"], "answer_start": [318]}}
{"context": "os\nSpark supports submitting applications in environments that use Kerberos for authentication.\nIn most cases, Spark relies on the credentials of the current logged in user when authenticating\nto Kerberos-aware services. Such credentials can be obtained by logging in to the configured KDC\nwith tools like\nkinit\n.\nWhen talking to Hadoop-based services, Spark needs to obtain delegation tokens so that non-local\nprocesses can authenticate. Spark ships with support for HDFS and other Hadoop file systems, Hive\nand HBase.\nWhen using a Hadoop filesystem (such HDFS or WebHDFS), Spark will acquire the relevant tokens\nfor the service hosting the user’s home directory.\nAn HBase token will be obtained if HBase is in the application’s classpath, and the HBase\nconfiguration has Kerberos authentication tur", "question": "What does Spark rely on when authenticating to Kerberos-aware services?", "answers": {"text": ["In most cases, Spark relies on the credentials of the current logged in user when authenticating"], "answer_start": [96]}}
{"context": "ntials for services when security is enabled.\n    By default, credentials for all supported services are retrieved when those services are\n    configured, but it's possible to disable that behavior if it somehow conflicts with the\n    application being run.\n2.3.0\nspark.kerberos.access.hadoopFileSystems\n(none)\nA comma-separated list of secure Hadoop filesystems your Spark application is going to access. For\n    example,\nspark.kerberos.access.hadoopFileSystems=hdfs://nn1.com:8032,hdfs://nn2.com:8032,\n    webhdfs://nn3.com:50070\n. The Spark application must have access to the filesystems listed\n    and Kerberos must be properly configured to be able to access them (either in the same realm\n    or in a trusted realm). Spark acquires security tokens for each of the filesystems so that\n    the S", "question": "What is the purpose of the spark.kerberos.access.hadoopFileSystems property?", "answers": {"text": ["A comma-separated list of secure Hadoop filesystems your Spark application is going to access."], "answer_start": [311]}}
{"context": "park needs to obtain delegation tokens\nso that non-local processes can authenticate. These delegation tokens in Kubernetes are stored in Secrets that are\nshared by the Driver and its Executors. As such, there are three ways of submitting a Kerberos job:\nIn all cases you must define the environment variable:\nHADOOP_CONF_DIR\nor\nspark.kubernetes.hadoop.configMapName.\nIt also important to note that the KDC needs to be visible from inside the containers.\nIf a user wishes to use a remote HADOOP_CONF directory, that contains the Hadoop configuration files, this could be\nachieved by setting\nspark.kubernetes.hadoop.configMapName\nto a pre-existing ConfigMap.\nSubmitting with a $kinit that stores a TGT in the Local Ticket Cache:\n/usr/bin/kinit\n-kt\n<keytab_file> <username>/<krb5 realm>\n/opt/spark/bin/s", "question": "What environment variable must be defined in all cases when submitting a Kerberos job?", "answers": {"text": ["HADOOP_CONF_DIR"], "answer_start": [309]}}
{"context": "igMap.\nSubmitting with a $kinit that stores a TGT in the Local Ticket Cache:\n/usr/bin/kinit\n-kt\n<keytab_file> <username>/<krb5 realm>\n/opt/spark/bin/spark-submit\n\\\n--deploy-mode\ncluster\n\\\n--class\norg.apache.spark.examples.HdfsTest\n\\\n--master\nk8s://<KUBERNETES_MASTER_ENDPOINT>\n\\\n--conf\nspark.executor.instances\n=\n1\n\\\n--conf\nspark.app.name\n=\nspark-hdfs\n\\\n--conf\nspark.kubernetes.container.image\n=\nspark:latest\n\\\n--conf\nspark.kubernetes.kerberos.krb5.path\n=\n/etc/krb5.conf\n\\\nlocal\n:///opt/spark/examples/jars/spark-examples_<VERSION>.jar\n\\\n<HDFS_FILE_LOCATION>\nSubmitting with a local Keytab and Principal\n/opt/spark/bin/spark-submit\n\\\n--deploy-mode\ncluster\n\\\n--class\norg.apache.spark.examples.HdfsTest\n\\\n--master\nk8s://<KUBERNETES_MASTER_ENDPOINT>\n\\\n--conf\nspark.executor.instances\n=\n1\n\\\n--conf\nspark.", "question": "What command is used for submitting with a $kinit that stores a TGT in the Local Ticket Cache?", "answers": {"text": ["/usr/bin/kinit\n-kt\n<keytab_file> <username>/<krb5 realm>"], "answer_start": [77]}}
{"context": "uster\n\\\n--class\norg.apache.spark.examples.HdfsTest\n\\\n--master\nk8s://<KUBERNETES_MASTER_ENDPOINT>\n\\\n--conf\nspark.executor.instances\n=\n1\n\\\n--conf\nspark.app.name\n=\nspark-hdfs\n\\\n--conf\nspark.kubernetes.container.image\n=\nspark:latest\n\\\n--conf\nspark.kerberos.keytab\n=\n<KEYTAB_FILE>\n\\\n--conf\nspark.kerberos.principal\n=\n<PRINCIPAL>\n\\\n--conf\nspark.kubernetes.kerberos.krb5.path\n=\n/etc/krb5.conf\n\\\nlocal\n:///opt/spark/examples/jars/spark-examples_<VERSION>.jar\n\\\n<HDFS_FILE_LOCATION>\nSubmitting with pre-populated secrets, that contain the Delegation Token, already existing within the namespace\n/opt/spark/bin/spark-submit\n\\\n--deploy-mode\ncluster\n\\\n--class\norg.apache.spark.examples.HdfsTest\n\\\n--master\nk8s://<KUBERNETES_MASTER_ENDPOINT>\n\\\n--conf\nspark.executor.instances\n=\n1\n\\\n--conf\nspark.app.name\n=\nspark-h", "question": "What is the value set for `spark.app.name`?", "answers": {"text": ["spark-hdfs"], "answer_start": [161]}}
{"context": "g.apache.spark.examples.HdfsTest\n\\\n--master\nk8s://<KUBERNETES_MASTER_ENDPOINT>\n\\\n--conf\nspark.executor.instances\n=\n1\n\\\n--conf\nspark.app.name\n=\nspark-hdfs\n\\\n--conf\nspark.kubernetes.container.image\n=\nspark:latest\n\\\n--conf\nspark.kubernetes.kerberos.tokenSecret.name\n=\n<SECRET_TOKEN_NAME>\n\\\n--conf\nspark.kubernetes.kerberos.tokenSecret.itemKey\n=\n<SECRET_ITEM_KEY>\n\\\n--conf\nspark.kubernetes.kerberos.krb5.path\n=\n/etc/krb5.conf\n\\\nlocal\n:///opt/spark/examples/jars/spark-examples_<VERSION>.jar\n\\\n<HDFS_FILE_LOCATION>\n3b. Submitting like in (3) however specifying a pre-created krb5 ConfigMap and pre-created\nHADOOP_CONF_DIR\nConfigMap\n/opt/spark/bin/spark-submit\n\\\n--deploy-mode\ncluster\n\\\n--class\norg.apache.spark.examples.HdfsTest\n\\\n--master\nk8s://<KUBERNETES_MASTER_ENDPOINT>\n\\\n--conf\nspark.executor.instan", "question": "What is the value assigned to `spark.app.name`?", "answers": {"text": ["spark-hdfs"], "answer_start": [143]}}
{"context": "bmit\n\\\n--deploy-mode\ncluster\n\\\n--class\norg.apache.spark.examples.HdfsTest\n\\\n--master\nk8s://<KUBERNETES_MASTER_ENDPOINT>\n\\\n--conf\nspark.executor.instances\n=\n1\n\\\n--conf\nspark.app.name\n=\nspark-hdfs\n\\\n--conf\nspark.kubernetes.container.image\n=\nspark:latest\n\\\n--conf\nspark.kubernetes.kerberos.tokenSecret.name\n=\n<SECRET_TOKEN_NAME>\n\\\n--conf\nspark.kubernetes.kerberos.tokenSecret.itemKey\n=\n<SECRET_ITEM_KEY>\n\\\n--conf\nspark.kubernetes.hadoop.configMapName\n=\n<HCONF_CONFIG_MAP_NAME>\n\\\n--conf\nspark.kubernetes.kerberos.krb5.configMapName\n=\n<KRB_CONFIG_MAP_NAME>\n\\\nlocal\n:///opt/spark/examples/jars/spark-examples_<VERSION>.jar\n\\\n<HDFS_FILE_LOCATION>\nEvent Logging\nIf your applications are using event logging, the directory where the event logs go\n(\nspark.eventLog.dir\n) should be manually created with proper ", "question": "Qual imagem de contêiner do Kubernetes o Spark deve usar?", "answers": {"text": ["spark:latest"], "answer_start": [239]}}
{"context": "ing\nIf your applications are using event logging, the directory where the event logs go\n(\nspark.eventLog.dir\n) should be manually created with proper permissions. To secure the log files,\nthe directory permissions should be set to\ndrwxrwxrwxt\n. The owner and group of the directory\nshould correspond to the super user who is running the Spark History Server.\nThis will allow all users to write to the directory but will prevent unprivileged users from\nreading, removing or renaming a file unless they own it. The event log files will be created by\nSpark with permissions such that only the user and group have read and write access.\nPersisting driver logs in client mode\nIf your applications persist driver logs in client mode by enabling\nspark.driver.log.persistToDfs.enabled\n,\nthe directory where t", "question": "What directory permissions should be set to secure the log files?", "answers": {"text": ["drwxrwxrwxt"], "answer_start": [231]}}
{"context": " logs in client mode\nIf your applications persist driver logs in client mode by enabling\nspark.driver.log.persistToDfs.enabled\n,\nthe directory where the driver logs go (\nspark.driver.log.dfsDir\n) should be manually created with proper\npermissions. To secure the log files, the directory permissions should be set to\ndrwxrwxrwxt\n. The owner\nand group of the directory should correspond to the super user who is running the Spark History Server.\nThis will allow all users to write to the directory but will prevent unprivileged users from\nreading, removing or renaming a file unless they own it. The driver log files will be created by\nSpark with permissions such that only the user and group have read and write access.", "question": "What directory permissions should be set to secure the log files?", "answers": {"text": ["drwxrwxrwxt"], "answer_start": [316]}}
{"context": "ssions such that only the user and group have read and write access.", "question": "What access permissions are granted to the user and group?", "answers": {"text": ["only the user and group have read and write access."], "answer_start": [17]}}
{"context": "easier to understand\nthe components involved. Read through the\napplication submission guide\nto learn about launching applications on a cluster.\nComponents\nSpark applications run as independent sets of processes on a cluster, coordinated by the\nSparkContext\nobject in your main program (called the\ndriver program\n).\nSpecifically, to run on a cluster, the SparkContext can connect to several types of\ncluster managers\n(either Spark’s own standalone cluster manager, YARN or Kubernetes), which allocate resources across\napplications. Once connected, Spark acquires\nexecutors\non nodes in the cluster, which are\nprocesses that run computations and store data for your application.\nNext, it sends your application code (defined by JAR or Python files passed to SparkContext) to\nthe executors. Finally, Spar", "question": "What are the types of cluster managers that the SparkContext can connect to?", "answers": {"text": ["either Spark’s own standalone cluster manager, YARN or Kubernetes"], "answer_start": [417]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark Configuration\nSpark Properties\nDynamically Loading Spark Properties\nViewing Spark Properties\nAvailable Properties\nApplication Properties\nRuntime Environment\nShuffle Behavior\nSpark UI\nCompression and Serialization\nMemory Management\nExecution Beha", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars"], "answer_start": [46]}}
{"context": "ailable Properties\nApplication Properties\nRuntime Environment\nShuffle Behavior\nSpark UI\nCompression and Serialization\nMemory Management\nExecution Behavior\nExecutor Metrics\nNetworking\nScheduling\nBarrier Execution Mode\nDynamic Allocation\nThread Configurations\nSpark Connect\nServer Configuration\nSecurity\nSpark SQL\nRuntime SQL Configuration\nStatic SQL Configuration\nSpark Streaming\nSparkR (deprecated)\nGraphX\nCluster Managers\nYARN\nKubernetes\nStandalone Mode\nEnvironment Variables\nConfiguring Logging\nPlain Text Logging\nStructured Logging\nQuerying Structured Logs with Spark SQL\nOverriding configuration directory\nInheriting Hadoop Cluster Configuration\nCustom Hadoop/Hive Configuration\nCustom Resource Scheduling and Configuration Overview\nStage Level Scheduling Overview\nPush-based shuffle overview\nExt", "question": "Quais tópicos são abordados na documentação?", "answers": {"text": ["Application Properties\nRuntime Environment\nShuffle Behavior\nSpark UI\nCompression and Serialization\nMemory Management\nExecution Behavior\nExecutor Metrics\nNetworking\nScheduling\nBarrier Execution Mode\nDynamic Allocation\nThread Configurations\nSpark Connect\nServer Configuration\nSecurity\nSpark SQL\nRuntime SQL Configuration\nStatic SQL Configuration\nSpark Streaming\nSparkR (deprecated)\nGraphX\nCluster Managers\nYARN\nKubernetes\nStandalone Mode\nEnvironment Variables\nConfiguring Logging\nPlain Text Logging\nStructured Logging\nQuerying Structured Logs with Spark SQL\nOverriding configuration directory\nInheriting Hadoop Cluster Configuration\nCustom Hadoop/Hive Configuration\nCustom Resource Scheduling and Configuration Overview\nStage Level Scheduling Overview\nPush-based shuffle overview"], "answer_start": [19]}}
{"context": "etAppName\n(\n\"CountingSheep\"\n)\nval\nsc\n=\nnew\nSparkContext\n(\nconf\n)\nNote that we can have more than 1 thread in local mode, and in cases like Spark Streaming, we may\nactually require more than 1 thread to prevent any sort of starvation issues.\nProperties that specify some time duration should be configured with a unit of time.\nThe following format is accepted:\n25ms (milliseconds)\n5s (seconds)\n10m or 10min (minutes)\n3h (hours)\n5d (days)\n1y (years)\nProperties that specify a byte size should be configured with a unit of size.\nThe following format is accepted:\n1b (bytes)\n1k or 1kb (kibibytes = 1024 bytes)\n1m or 1mb (mebibytes = 1024 kibibytes)\n1g or 1gb (gibibytes = 1024 mebibytes)\n1t or 1tb (tebibytes = 1024 gibibytes)\n1p or 1pb (pebibytes = 1024 tebibytes)\nWhile numbers without units are genera", "question": "What units are accepted for specifying a time duration?", "answers": {"text": ["25ms (milliseconds)\n5s (seconds)\n10m or 10min (minutes)\n3h (hours)\n5d (days)\n1y (years)"], "answer_start": [360]}}
{"context": "hen configurations are specified via the\n--conf/-c\nflags,\nbin/spark-submit\nwill also read\nconfiguration options from\nconf/spark-defaults.conf\n, in which each line consists of a key and\na value separated by whitespace. For example:\nspark.master            spark://5.6.7.8:7077\nspark.executor.memory   4g\nspark.eventLog.enabled  true\nspark.serializer        org.apache.spark.serializer.KryoSerializer\nIn addition, a property file with Spark configurations can be passed to\nbin/spark-submit\nvia\n--properties-file\nparameter. When this is set, Spark will no longer load configurations from\nconf/spark-defaults.conf\nunless another parameter\n--load-spark-defaults\nis provided.\nAny values specified as flags or in the properties file will be passed on to the application\nand merged with those specified throu", "question": "From which file does bin/spark-submit read configuration options by default?", "answers": {"text": ["conf/spark-defaults.conf"], "answer_start": [117]}}
{"context": "rties have been set correctly. Note\nthat only values explicitly specified through\nspark-defaults.conf\n,\nSparkConf\n, or the command\nline will appear. For all other configuration properties, you can assume the default value is used.\nAvailable Properties\nMost of the properties that control internal settings have reasonable default values. Some\nof the most common options to set are:\nApplication Properties\nProperty Name\nDefault\nMeaning\nSince Version\nspark.app.name\n(none)\nThe name of your application. This will appear in the UI and in log data.\n0.9.0\nspark.driver.cores\n1\nNumber of cores to use for the driver process, only in cluster mode.\n1.3.0\nspark.driver.maxResultSize\n1g\nLimit of total size of serialized results of all partitions for each Spark action (e.g.\n    collect) in bytes. Should be at", "question": "What does spark.app.name represent?", "answers": {"text": ["The name of your application. This will appear in the UI and in log data."], "answer_start": [471]}}
{"context": "ed. For instance, Windows does not support resource limiting and actual\n    resource is not limited on MacOS.\n2.4.0\nspark.executor.memoryOverhead\nexecutorMemory *\nspark.executor.memoryOverheadFactor\n, with minimum of\nspark.executor.minMemoryOverhead\nAmount of additional memory to be allocated per executor process, in MiB unless otherwise specified.\n    This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc.\n    This tends to grow with the executor size (typically 6-10%). This option is currently supported on YARN and Kubernetes.\nNote:\nAdditional memory includes PySpark executor memory\n    (when\nspark.executor.pyspark.memory\nis not configured) and memory used by other\n    non-executor processes running in the same container. The maximum memo", "question": "What does spark.executor.memoryOverhead account for?", "answers": {"text": ["This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc."], "answer_start": [355]}}
{"context": "urceName}.amount\n0\nAmount of a particular resource type to use per executor process.\n    If this is used, you must also specify the\nspark.executor.resource.{resourceName}.discoveryScript\nfor the executor to find the resource on startup.\n3.0.0\nspark.executor.resource.{resourceName}.discoveryScript\nNone\nA script for the executor to run to discover a particular resource type. This should\n    write to STDOUT a JSON string in the format of the ResourceInformation class. This has a\n    name and an array of addresses.\n3.0.0\nspark.executor.resource.{resourceName}.vendor\nNone\nVendor of the resources to use for the executors. This option is currently\n    only supported on Kubernetes and is actually both the vendor and domain following\n    the Kubernetes device plugin naming convention. (e.g. For GPU", "question": "What format should the script for discovering a resource type write to STDOUT?", "answers": {"text": ["a JSON string in the format of the ResourceInformation class. This has a\n    name and an array of addresses."], "answer_start": [408]}}
{"context": "d master URL's\n.\n0.9.0\nspark.submit.deployMode\nclient\nThe deploy mode of Spark driver program, either \"client\" or \"cluster\",\n    Which means to launch driver program locally (\"client\")\n    or remotely (\"cluster\") on one of the nodes inside the cluster.\n1.5.0\nspark.log.callerContext\n(none)\nApplication information that will be written into Yarn RM log/HDFS audit log when running on Yarn/HDFS.\n    Its length depends on the Hadoop configuration\nhadoop.caller.context.max.size\n. It should be concise,\n    and typically can have up to 50 characters.\n2.2.0\nspark.log.level\n(none)\nWhen set, overrides any user-defined log settings as if calling\nSparkContext.setLogLevel()\nat Spark startup. Valid log levels include: \"ALL\", \"DEBUG\", \"ERROR\", \"FATAL\", \"INFO\", \"OFF\", \"TRACE\", \"WARN\".\n3.5.0\nspark.driver.sup", "question": "What are the valid log levels for spark.log.level?", "answers": {"text": ["Valid log levels include: \"ALL\", \"DEBUG\", \"ERROR\", \"FATAL\", \"INFO\", \"OFF\", \"TRACE\", \"WARN\"."], "answer_start": [686]}}
{"context": "ext.setLogLevel()\nat Spark startup. Valid log levels include: \"ALL\", \"DEBUG\", \"ERROR\", \"FATAL\", \"INFO\", \"OFF\", \"TRACE\", \"WARN\".\n3.5.0\nspark.driver.supervise\nfalse\nIf true, restarts the driver automatically if it fails with a non-zero exit status.\n    Only has effect in Spark standalone mode.\n1.3.0\nspark.driver.timeout\n0min\nA timeout for Spark driver in minutes. 0 means infinite. For the positive time value,\n    terminate the driver with the exit code 124 if it runs after timeout duration. To use,\n    it's required to set\nspark.plugins\nwith\norg.apache.spark.deploy.DriverTimeoutPlugin\n.\n4.0.0\nspark.driver.log.localDir\n(none)\nSpecifies a local directory to write driver logs and enable Driver Log UI Tab.\n4.0.0\nspark.driver.log.dfsDir\n(none)\nBase directory in which Spark driver logs are synced,", "question": "What are the valid log levels for ext.setLogLevel()?", "answers": {"text": ["\"ALL\", \"DEBUG\", \"ERROR\", \"FATAL\", \"INFO\", \"OFF\", \"TRACE\", \"WARN\""], "answer_start": [62]}}
{"context": "lts.\n3.0.0\nspark.decommission.enabled\nfalse\nWhen decommission enabled, Spark will try its best to shut down the executor gracefully.\n    Spark will try to migrate all the RDD blocks (controlled by\nspark.storage.decommission.rddBlocks.enabled\n)\n    and shuffle blocks (controlled by\nspark.storage.decommission.shuffleBlocks.enabled\n) from the decommissioning\n    executor to a remote executor when\nspark.storage.decommission.enabled\nis enabled.\n    With decommission enabled, Spark will also decommission an executor instead of killing when\nspark.dynamicAllocation.enabled\nenabled.\n3.1.0\nspark.executor.decommission.killInterval\n(none)\nDuration after which a decommissioned executor will be killed forcefully by an outside (e.g. non-spark) service.\n3.1.0\nspark.executor.decommission.forceKillTimeout\n(", "question": "What happens when decommission is enabled regarding executor shutdown?", "answers": {"text": ["When decommission enabled, Spark will try its best to shut down the executor gracefully."], "answer_start": [44]}}
{"context": "which a decommissioned executor will be killed forcefully by an outside (e.g. non-spark) service.\n3.1.0\nspark.executor.decommission.forceKillTimeout\n(none)\nDuration after which a Spark will force a decommissioning executor to exit.\n    This should be set to a high value in most situations as low values will prevent block migrations from having enough time to complete.\n3.2.0\nspark.executor.decommission.signal\nPWR\nThe signal that used to trigger the executor to start decommission.\n3.2.0\nspark.executor.maxNumFailures\nnumExecutors * 2, with minimum of 3\nThe maximum number of executor failures before failing the application.\n    This configuration only takes effect on YARN and Kubernetes.\n3.5.0\nspark.executor.failuresValidityInterval\n(none)\nInterval after which executor failures will be conside", "question": "What is the purpose of spark.executor.decommission.forceKillTimeout?", "answers": {"text": ["Duration after which a Spark will force a decommissioning executor to exit."], "answer_start": [156]}}
{"context": " only takes effect on YARN and Kubernetes.\n3.5.0\nspark.executor.failuresValidityInterval\n(none)\nInterval after which executor failures will be considered independent and\n    not accumulate towards the attempt count.\n    This configuration only takes effect on YARN and Kubernetes.\n3.5.0\nApart from these, the following properties are also available, and may be useful in some situations:\nRuntime Environment\nProperty Name\nDefault\nMeaning\nSince Version\nspark.driver.extraClassPath\n(none)\nExtra classpath entries to prepend to the classpath of the driver.\nNote:\nIn client mode, this config must not be set through the\nSparkConf\ndirectly in your application, because the driver JVM has already started at that point.\n    Instead, please set this through the\n--driver-class-path\ncommand line option or in", "question": "Where does the configuration 'spark.executor.failuresValidityInterval' take effect?", "answers": {"text": ["This configuration only takes effect on YARN and Kubernetes."], "answer_start": [220]}}
{"context": "tion, because the driver JVM has already started at that point.\n    Instead, please set this through the\n--driver-class-path\ncommand line option or in\n    your default properties file.\n1.0.0\nspark.driver.defaultJavaOptions\n(none)\nA string of default JVM options to prepend to\nspark.driver.extraJavaOptions\n.\n    This is intended to be set by administrators.\n\n    For instance, GC settings or other logging.\n    Note that it is illegal to set maximum heap size (-Xmx) settings with this option. Maximum heap\n    size settings can be set with\nspark.driver.memory\nin the cluster mode and through\n    the\n--driver-memory\ncommand line option in the client mode.\nNote:\nIn client mode, this config must not be set through the\nSparkConf\ndirectly in your application, because the driver JVM has already starte", "question": "What should you not set maximum heap size with?", "answers": {"text": ["Note that it is illegal to set maximum heap size (-Xmx) settings with this option."], "answer_start": [411]}}
{"context": "r\n    backwards-compatibility with older versions of Spark. Users typically should not need to set\n    this option.\n1.0.0\nspark.executor.defaultJavaOptions\n(none)\nA string of default JVM options to prepend to\nspark.executor.extraJavaOptions\n.\n    This is intended to be set by administrators.\n\n    For instance, GC settings or other logging.\n    Note that it is illegal to set Spark properties or maximum heap size (-Xmx) settings with this\n    option. Spark properties should be set using a SparkConf object or the spark-defaults.conf file\n    used with the spark-submit script. Maximum heap size settings can be set with spark.executor.memory.\n\n    The following symbols, if present will be interpolated:  will be replaced by\n    application ID and  will be replaced by executor ID. For example, to", "question": "What should administrators use to set GC settings or other logging options?", "answers": {"text": ["This is intended to be set by administrators."], "answer_start": [247]}}
{"context": " The following symbols, if present will be interpolated:  will be replaced by\n    application ID and  will be replaced by executor ID. For example, to enable\n    verbose gc logging to a file named for the executor ID of the app in /tmp, pass a 'value' of:\n-verbose:gc -Xloggc:/tmp/-.gc\n3.0.0\nspark.executor.extraJavaOptions\n(none)\nA string of extra JVM options to pass to executors. This is intended to be set by users.\n\n    For instance, GC settings or other logging.\n    Note that it is illegal to set Spark properties or maximum heap size (-Xmx) settings with this\n    option. Spark properties should be set using a SparkConf object or the spark-defaults.conf file\n    used with the spark-submit script. Maximum heap size settings can be set with spark.executor.memory.\n\n    The following symbols,", "question": "What should be used to set maximum heap size settings?", "answers": {"text": ["Maximum heap size settings can be set with spark.executor.memory."], "answer_start": [707]}}
{"context": "efaults.conf file\n    used with the spark-submit script. Maximum heap size settings can be set with spark.executor.memory.\n\n    The following symbols, if present will be interpolated:  will be replaced by\n    application ID and  will be replaced by executor ID. For example, to enable\n    verbose gc logging to a file named for the executor ID of the app in /tmp, pass a 'value' of:\n-verbose:gc -Xloggc:/tmp/-.gc\nspark.executor.defaultJavaOptions\nwill be prepended to this configuration.\n1.0.0\nspark.executor.extraLibraryPath\n(none)\nSet a special library path to use when launching executor JVM's.\n1.0.0\nspark.executor.logs.rolling.maxRetainedFiles\n-1\nSets the number of latest rolling log files that are going to be retained by the system.\n    Older log files will be deleted. Disabled by default.\n1", "question": "What is the default value for spark.executor.logs.rolling.maxRetainedFiles?", "answers": {"text": ["-1"], "answer_start": [649]}}
{"context": "tomatic cleaning of old logs.\n1.1.0\nspark.executor.userClassPathFirst\nfalse\n(Experimental) Same functionality as\nspark.driver.userClassPathFirst\n, but\n    applied to executor instances.\n1.3.0\nspark.executorEnv.[EnvironmentVariableName]\n(none)\nAdd the environment variable specified by\nEnvironmentVariableName\nto the Executor\n    process. The user can specify multiple of these to set multiple environment variables.\n0.9.0\nspark.redaction.regex\n(?i)secret|password|token|access[.]?key\nRegex to decide which Spark configuration properties and environment variables in driver and\n    executor environments contain sensitive information. When this regex matches a property key or\n    value, the value is redacted from the environment UI and various logs like YARN and event logs.\n2.1.2\nspark.redaction.st", "question": "What is the purpose of spark.redaction.regex?", "answers": {"text": ["Regex to decide which Spark configuration properties and environment variables in driver and\n    executor environments contain sensitive information."], "answer_start": [484]}}
{"context": "matches a property key or\n    value, the value is redacted from the environment UI and various logs like YARN and event logs.\n2.1.2\nspark.redaction.string.regex\n(none)\nRegex to decide which parts of strings produced by Spark contain sensitive information.\n    When this regex matches a string part, that string part is replaced by a dummy value.\n    This is currently used to redact the output of SQL explain commands.\n2.2.0\nspark.python.profile\nfalse\nEnable profiling in Python worker, the profile result will show up by\nsc.show_profiles()\n,\n    or it will be displayed before the driver exits. It also can be dumped into disk by\nsc.dump_profiles(path)\n. If some of the profile results had been displayed manually,\n    they will not be displayed automatically before driver exiting.\n\n    By default ", "question": "What happens when the `spark.redaction.string.regex` matches a string part?", "answers": {"text": ["When this regex matches a string part, that string part is replaced by a dummy value."], "answer_start": [260]}}
{"context": "arated list of files to be placed in the working directory of each executor. Globs are allowed.\n1.0.0\nspark.submit.pyFiles\nComma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps. Globs are allowed.\n1.0.1\nspark.jars\nComma-separated list of jars to include on the driver and executor classpaths. Globs are allowed.\n0.9.0\nspark.jars.packages\nComma-separated list of Maven coordinates of jars to include on the driver and executor\n    classpaths. The coordinates should be groupId:artifactId:version. If\nspark.jars.ivySettings\nis given artifacts will be resolved according to the configuration in the file, otherwise artifacts\n    will be searched for in the local maven repo, then maven central and finally any additional remote\n    repositories given by the command", "question": "What is the format for specifying Maven coordinates of jars to include on the driver and executor classpaths?", "answers": {"text": ["groupId:artifactId:version"], "answer_start": [506]}}
{"context": "de, this file will also be localized to the remote driver for dependency\n    resolution within\nSparkContext#addJar\n2.2.0\nspark.jars.repositories\nComma-separated list of additional remote repositories to search for the maven coordinates\n    given with\n--packages\nor\nspark.jars.packages\n.\n2.3.0\nspark.archives\nComma-separated list of archives to be extracted into the working directory of each executor.\n    .jar, .tar.gz, .tgz and .zip are supported. You can specify the directory name to unpack via\n    adding\n#\nafter the file name to unpack, for example,\nfile.zip#directory\n.\n    This configuration is experimental.\n3.1.0\nspark.pyspark.driver.python\nPython binary executable to use for PySpark in driver.\n    (default is\nspark.pyspark.python\n)\n2.1.0\nspark.pyspark.python\nPython binary executable to ", "question": "What file types are supported for spark.archives?", "answers": {"text": [".jar, .tar.gz, .tgz and .zip are supported."], "answer_start": [406]}}
{"context": "\nPython binary executable to use for PySpark in driver.\n    (default is\nspark.pyspark.python\n)\n2.1.0\nspark.pyspark.python\nPython binary executable to use for PySpark in both driver and executors.\n2.1.0\nShuffle Behavior\nProperty Name\nDefault\nMeaning\nSince Version\nspark.reducer.maxSizeInFlight\n48m\nMaximum size of map outputs to fetch simultaneously from each reduce task, in MiB unless\n    otherwise specified. Since each output requires us to create a buffer to receive it, this\n    represents a fixed memory overhead per reduce task, so keep it small unless you have a\n    large amount of memory.\n1.4.0\nspark.reducer.maxReqsInFlight\nInt.MaxValue\nThis configuration limits the number of remote requests to fetch blocks at any given point.\n    When the number of hosts in the cluster increase, it mig", "question": "What is the default value for spark.reducer.maxSizeInFlight?", "answers": {"text": ["48m"], "answer_start": [293]}}
{"context": "\n32k\nSize of the in-memory buffer for each shuffle file input stream, in KiB unless otherwise\n    specified. These buffers use off-heap buffers and are related to the number of files in\n    the shuffle file. Too large buffers should be avoided.\n4.0.0\nspark.shuffle.unsafe.file.output.buffer\n32k\nDeprecated since Spark 4.0, please use\nspark.shuffle.localDisk.file.output.buffer\n.\n2.3.0\nspark.shuffle.localDisk.file.output.buffer\n32k\nThe file system for this buffer size after each partition is written in all local disk shuffle writers.\n    In KiB unless otherwise specified.\n4.0.0\nspark.shuffle.spill.diskWriteBufferSize\n1024 * 1024\nThe buffer size, in bytes, to use when writing the sorted records to an on-disk file.\n2.3.0\nspark.shuffle.io.maxRetries\n3\n(Netty only) Fetches that fail due to IO-rela", "question": "What is the size of the in-memory buffer for each shuffle file input stream?", "answers": {"text": ["32k"], "answer_start": [1]}}
{"context": " there are still outstanding fetch requests but no traffic no the channel\n    for at least\nconnectionTimeout\n.\n1.2.0\nspark.shuffle.io.connectionCreationTimeout\nvalue of\nspark.shuffle.io.connectionTimeout\nTimeout for establishing a connection between the shuffle servers and clients.\n3.2.0\nspark.shuffle.service.enabled\nfalse\nEnables the external shuffle service. This service preserves the shuffle files written by\n    executors e.g. so that executors can be safely removed, or so that shuffle fetches can continue in\n    the event of executor failure. The external shuffle service must be set up in order to enable it. See\ndynamic allocation\n    configuration and setup documentation\nfor more information.\n1.2.0\nspark.shuffle.service.port\n7337\nPort on which the external shuffle service will run.\n1.", "question": "What does spark.shuffle.service.enabled control?", "answers": {"text": ["Enables the external shuffle service. This service preserves the shuffle files written by\n    executors e.g. so that executors can be safely removed, or so that shuffle fetches can continue in\n    the event of executor failure."], "answer_start": [325]}}
{"context": "figuration and setup documentation\nfor more information.\n1.2.0\nspark.shuffle.service.port\n7337\nPort on which the external shuffle service will run.\n1.2.0\nspark.shuffle.service.name\nspark_shuffle\nThe configured name of the Spark shuffle service the client should communicate with.\n    This must match the name used to configure the Shuffle within the YARN NodeManager configuration\n    (\nyarn.nodemanager.aux-services\n). Only takes effect\n    when\nspark.shuffle.service.enabled\nis set to true.\n3.2.0\nspark.shuffle.service.index.cache.size\n100m\nCache entries limited to the specified memory footprint, in bytes unless otherwise specified.\n2.3.0\nspark.shuffle.service.removeShuffle\ntrue\nWhether to use the ExternalShuffleService for deleting shuffle blocks for\n    deallocated executors when the shuffle", "question": "What is the purpose of the 'spark.shuffle.service.port' configuration?", "answers": {"text": ["Port on which the external shuffle service will run."], "answer_start": [95]}}
{"context": "0\nspark.shuffle.registration.timeout\n5000\nTimeout in milliseconds for registration to the external shuffle service.\n2.3.0\nspark.shuffle.registration.maxAttempts\n3\nWhen we fail to register to the external shuffle service, we will retry for maxAttempts times.\n2.3.0\nspark.shuffle.reduceLocality.enabled\ntrue\nWhether to compute locality preferences for reduce tasks.\n1.5.0\nspark.shuffle.mapOutput.minSizeForBroadcast\n512k\nThe size at which we use Broadcast to send the map output statuses to the executors.\n2.0.0\nspark.shuffle.detectCorrupt\ntrue\nWhether to detect any corruption in fetched blocks.\n2.2.0\nspark.shuffle.detectCorrupt.useExtraMemory\nfalse\nIf enabled, part of a compressed/encrypted stream will be de-compressed/de-crypted by using extra memory\n    to detect early corruption. Any IOExcepti", "question": "What is the purpose of the 'spark.shuffle.registration.timeout' property?", "answers": {"text": ["Timeout in milliseconds for registration to the external shuffle service."], "answer_start": [42]}}
{"context": "lishing a connection for fetching files in Spark RPC environments.\n3.2.0\nspark.shuffle.checksum.enabled\ntrue\nWhether to calculate the checksum of shuffle data. If enabled, Spark will calculate the checksum values for each partition\n    data within the map output file and store the values in a checksum file on the disk. When there's shuffle data corruption\n    detected, Spark will try to diagnose the cause (e.g., network issue, disk issue, etc.) of the corruption by using the checksum file.\n3.2.0\nspark.shuffle.checksum.algorithm\nADLER32\nThe algorithm is used to calculate the shuffle checksum. Currently, it only supports built-in algorithms of JDK, e.g., ADLER32, CRC32 and CRC32C.\n3.2.0\nspark.shuffle.service.fetch.rdd.enabled\nfalse\nWhether to use the ExternalShuffleService for fetching disk ", "question": "What algorithms are currently supported for calculating the shuffle checksum?", "answers": {"text": ["Currently, it only supports built-in algorithms of JDK, e.g., ADLER32, CRC32 and CRC32C."], "answer_start": [599]}}
{"context": "JDK, e.g., ADLER32, CRC32 and CRC32C.\n3.2.0\nspark.shuffle.service.fetch.rdd.enabled\nfalse\nWhether to use the ExternalShuffleService for fetching disk persisted RDD blocks.\n    In case of dynamic allocation if this feature is enabled executors having only disk\n    persisted blocks are considered idle after\nspark.dynamicAllocation.executorIdleTimeout\nand will be released accordingly.\n3.0.0\nspark.shuffle.service.db.enabled\ntrue\nWhether to use db in ExternalShuffleService. Note that this only affects standalone mode.\n3.0.0\nspark.shuffle.service.db.backend\nROCKSDB\nSpecifies a disk-based store used in shuffle service local db. Setting as ROCKSDB or LEVELDB (deprecated).\n3.4.0\nSpark UI\nProperty Name\nDefault\nMeaning\nSince Version\nspark.eventLog.logBlockUpdates.enabled\nfalse\nWhether to log events f", "question": "What does spark.shuffle.service.db.backend specify?", "answers": {"text": ["Specifies a disk-based store used in shuffle service local db. Setting as ROCKSDB or LEVELDB (deprecated)."], "answer_start": [566]}}
{"context": "park will still not force the file to use erasure coding, it\n    will simply use filesystem defaults.\n3.0.0\nspark.eventLog.dir\nfile:///tmp/spark-events\nBase directory in which Spark events are logged, if\nspark.eventLog.enabled\nis true.\n    Within this base directory, Spark creates a sub-directory for each application, and logs the\n    events specific to the application in this directory. Users may want to set this to\n    a unified location like an HDFS directory so history files can be read by the history server.\n1.0.0\nspark.eventLog.enabled\nfalse\nWhether to log Spark events, useful for reconstructing the Web UI after the application has\n    finished.\n1.0.0\nspark.eventLog.overwrite\nfalse\nWhether to overwrite any existing files.\n1.0.0\nspark.eventLog.buffer.kb\n100k\nBuffer size to use when wr", "question": "What is the purpose of spark.eventLog.dir?", "answers": {"text": ["Base directory in which Spark events are logged, if\nspark.eventLog.enabled\nis true."], "answer_start": [152]}}
{"context": "finished.\n1.0.0\nspark.eventLog.overwrite\nfalse\nWhether to overwrite any existing files.\n1.0.0\nspark.eventLog.buffer.kb\n100k\nBuffer size to use when writing to output streams, in KiB unless otherwise specified.\n1.0.0\nspark.eventLog.rolling.enabled\nfalse\nWhether rolling over event log files is enabled. If set to true, it cuts down each event\n    log file to the configured size.\n3.0.0\nspark.eventLog.rolling.maxFileSize\n128m\nWhen\nspark.eventLog.rolling.enabled=true\n, specifies the max size of event log file before it's rolled over.\n3.0.0\nspark.ui.dagGraph.retainedRootRDDs\nInt.MaxValue\nHow many DAG graph nodes the Spark UI and status APIs remember before garbage collecting.\n2.1.0\nspark.ui.groupSQLSubExecutionEnabled\ntrue\nWhether to group sub executions together in SQL UI when they belong to the", "question": "What is the default value of spark.eventLog.overwrite?", "answers": {"text": ["false"], "answer_start": [41]}}
{"context": " before garbage collecting.\n2.1.0\nspark.ui.groupSQLSubExecutionEnabled\ntrue\nWhether to group sub executions together in SQL UI when they belong to the same root execution\n3.4.0\nspark.ui.enabled\ntrue\nWhether to run the web UI for the Spark application.\n1.1.1\nspark.ui.store.path\nNone\nLocal directory where to cache application information for live UI.\n    By default this is not set, meaning all application information will be kept in memory.\n3.4.0\nspark.ui.killEnabled\ntrue\nAllows jobs and stages to be killed from the web UI.\n1.0.0\nspark.ui.threadDumpsEnabled\ntrue\nWhether to show a link for executor thread dumps in Stages and Executor pages.\n1.2.0\nspark.ui.threadDump.flamegraphEnabled\ntrue\nWhether to render the Flamegraph for executor thread dumps.\n4.0.0\nspark.ui.heapHistogramEnabled\ntrue\nWhet", "question": "What does spark.ui.store.path configure?", "answers": {"text": ["Local directory where to cache application information for live UI.\n    By default this is not set, meaning all application information will be kept in memory."], "answer_start": [283]}}
{"context": "0\nspark.ui.threadDump.flamegraphEnabled\ntrue\nWhether to render the Flamegraph for executor thread dumps.\n4.0.0\nspark.ui.heapHistogramEnabled\ntrue\nWhether to show a link for executor heap histogram in Executor page.\n3.5.0\nspark.ui.liveUpdate.period\n100ms\nHow often to update live entities. -1 means \"never update\" when replaying applications,\n    meaning only the last write will happen. For live applications, this avoids a few\n    operations that we can live without when rapidly processing incoming task events.\n2.3.0\nspark.ui.liveUpdate.minFlushPeriod\n1s\nMinimum time elapsed before stale UI data is flushed. This avoids UI staleness when incoming\n    task events are not fired frequently.\n2.4.2\nspark.ui.port\n4040\nPort for your application's dashboard, which shows memory and workload data.\n0.7.0", "question": "How often does the Spark UI update live entities?", "answers": {"text": ["100ms"], "answer_start": [248]}}
{"context": "\n    task events are not fired frequently.\n2.4.2\nspark.ui.port\n4040\nPort for your application's dashboard, which shows memory and workload data.\n0.7.0\nspark.ui.retainedJobs\n1000\nHow many jobs the Spark UI and status APIs remember before garbage collecting.\n    This is a target maximum, and fewer elements may be retained in some circumstances.\n1.2.0\nspark.ui.retainedStages\n1000\nHow many stages the Spark UI and status APIs remember before garbage collecting.\n    This is a target maximum, and fewer elements may be retained in some circumstances.\n0.9.0\nspark.ui.retainedTasks\n100000\nHow many tasks in one stage the Spark UI and status APIs remember before garbage collecting.\n    This is a target maximum, and fewer elements may be retained in some circumstances.\n2.0.1\nspark.ui.reverseProxy\nfalse\n", "question": "What does spark.ui.port configure?", "answers": {"text": ["Port for your application's dashboard, which shows memory and workload data."], "answer_start": [68]}}
{"context": " before garbage collecting.\n    This is a target maximum, and fewer elements may be retained in some circumstances.\n2.0.1\nspark.ui.reverseProxy\nfalse\nEnable running Spark Master as reverse proxy for worker and application UIs. In this mode, Spark master will reverse proxy the worker and application UIs to enable access without requiring direct access to their hosts. Use it with caution, as worker and application UI will not be accessible directly, you will only be able to access them through spark master/proxy public URL. This setting affects all the workers and application UIs running in the cluster and must be set on all the workers, drivers and masters.\n2.1.0\nspark.ui.reverseProxyUrl\nIf the Spark UI should be served through another front-end reverse proxy, this is the URL\n    for access", "question": "What does spark.ui.reverseProxy do?", "answers": {"text": ["Enable running Spark Master as reverse proxy for worker and application UIs. In this mode, Spark master will reverse proxy the worker and application UIs to enable access without requiring direct access to their hosts."], "answer_start": [150]}}
{"context": "park UI's own\n    address. This should be only the address of the server, without any prefix paths for the\n    application; the prefix should be set either by the proxy server itself (by adding the\nX-Forwarded-Context\nrequest header), or by setting the proxy base in the Spark\n    app's configuration.\n3.0.0\nspark.ui.showConsoleProgress\nfalse\nShow the progress bar in the console. The progress bar shows the progress of stages\n    that run for longer than 500ms. If multiple stages run at the same time, multiple\n    progress bars will be displayed on the same line.\nNote:\nIn shell environment, the default value of spark.ui.showConsoleProgress is true.\n1.2.1\nspark.ui.consoleProgress.update.interval\n200\nAn interval in milliseconds to update the progress bar in the console.\n2.1.0\nspark.ui.custom.ex", "question": "What is the default value of spark.ui.showConsoleProgress in a shell environment?", "answers": {"text": ["In shell environment, the default value of spark.ui.showConsoleProgress is true."], "answer_start": [573]}}
{"context": "ue.\n1.2.1\nspark.ui.consoleProgress.update.interval\n200\nAn interval in milliseconds to update the progress bar in the console.\n2.1.0\nspark.ui.custom.executor.log.url\n(none)\nSpecifies custom spark executor log URL for supporting external log service instead of using cluster\n    managers' application log URLs in Spark UI. Spark will support some path variables via patterns\n    which can vary on cluster manager. Please check the documentation for your cluster manager to\n    see which patterns are supported, if any.\nPlease note that this configuration also replaces original log urls in event log,\n    which will be also effective when accessing the application on history server. The new log urls must be\n    permanent, otherwise you might have dead link for executor log urls.\nFor now, only YARN a", "question": "What does spark.ui.consoleProgress.update.interval configure?", "answers": {"text": ["An interval in milliseconds to update the progress bar in the console."], "answer_start": [55]}}
{"context": " application on history server. The new log urls must be\n    permanent, otherwise you might have dead link for executor log urls.\nFor now, only YARN and K8s cluster manager supports this configuration\n3.0.0\nspark.ui.prometheus.enabled\ntrue\nExpose executor metrics at /metrics/executors/prometheus at driver web page.\n3.0.0\nspark.worker.ui.retainedExecutors\n1000\nHow many finished executors the Spark UI and status APIs remember before garbage collecting.\n1.5.0\nspark.worker.ui.retainedDrivers\n1000\nHow many finished drivers the Spark UI and status APIs remember before garbage collecting.\n1.5.0\nspark.sql.ui.retainedExecutions\n1000\nHow many finished executions the Spark UI and status APIs remember before garbage collecting.\n1.5.0\nspark.streaming.ui.retainedBatches\n1000\nHow many finished batches th", "question": "How many finished executors the Spark UI and status APIs remember before garbage collecting?", "answers": {"text": ["How many finished executors the Spark UI and status APIs remember before garbage collecting."], "answer_start": [362]}}
{"context": "executions the Spark UI and status APIs remember before garbage collecting.\n1.5.0\nspark.streaming.ui.retainedBatches\n1000\nHow many finished batches the Spark UI and status APIs remember before garbage collecting.\n1.0.0\nspark.ui.retainedDeadExecutors\n100\nHow many dead executors the Spark UI and status APIs remember before garbage collecting.\n2.0.0\nspark.ui.filters\nNone\nComma separated list of filter class names to apply to the Spark Web UI. The filter should be a\n    standard\njavax servlet Filter\n.\nFilter parameters can also be specified in the configuration, by setting config entries\n    of the form\nspark.<class name of filter>.param.<param name>=<value>\nFor example:\nspark.ui.filters=com.test.filter1\nspark.com.test.filter1.param.name1=foo\nspark.com.test.filter1.param.name2=bar\n1.0.0\nspark.", "question": "How many finished batches the Spark UI and status APIs remember before garbage collecting.", "answers": {"text": ["1000"], "answer_start": [117]}}
{"context": "ame>=<value>\nFor example:\nspark.ui.filters=com.test.filter1\nspark.com.test.filter1.param.name1=foo\nspark.com.test.filter1.param.name2=bar\n1.0.0\nspark.ui.requestHeaderSize\n8k\nThe maximum allowed size for a HTTP request header, in bytes unless otherwise specified.\n    This setting applies for the Spark History Server too.\n2.2.3\nspark.ui.timelineEnabled\ntrue\nWhether to display event timeline data on UI pages.\n3.4.0\nspark.ui.timeline.executors.maximum\n250\nThe maximum number of executors shown in the event timeline.\n3.2.0\nspark.ui.timeline.jobs.maximum\n500\nThe maximum number of jobs shown in the event timeline.\n3.2.0\nspark.ui.timeline.stages.maximum\n500\nThe maximum number of stages shown in the event timeline.\n3.2.0\nspark.ui.timeline.tasks.maximum\n1000\nThe maximum number of tasks shown in the e", "question": "What is the maximum allowed size for a HTTP request header?", "answers": {"text": ["8k"], "answer_start": [171]}}
{"context": "um\n500\nThe maximum number of stages shown in the event timeline.\n3.2.0\nspark.ui.timeline.tasks.maximum\n1000\nThe maximum number of tasks shown in the event timeline.\n1.4.0\nspark.appStatusStore.diskStoreDir\nNone\nLocal directory where to store diagnostic information of SQL executions. This configuration is only for live UI.\n3.4.0\nCompression and Serialization\nProperty Name\nDefault\nMeaning\nSince Version\nspark.broadcast.compress\ntrue\nWhether to compress broadcast variables before sending them. Generally a good idea.\n    Compression will use\nspark.io.compression.codec\n.\n0.6.0\nspark.checkpoint.dir\n(none)\nSet the default directory for checkpointing. It can be overwritten by\n    SparkContext.setCheckpointDir.\n4.0.0\nspark.checkpoint.compress\nfalse\nWhether to compress RDD checkpoints. Generally a goo", "question": "What does spark.broadcast.compress control?", "answers": {"text": ["Whether to compress broadcast variables before sending them. Generally a good idea."], "answer_start": [433]}}
{"context": " and\norg.apache.spark.io.ZStdCompressionCodec\n.\n0.8.0\nspark.io.compression.lz4.blockSize\n32k\nBlock size used in LZ4 compression, in the case when LZ4 compression codec\n    is used. Lowering this block size will also lower shuffle memory usage when LZ4 is used.\n    Default unit is bytes, unless otherwise specified. This configuration only applies to\nspark.io.compression.codec\n.\n1.4.0\nspark.io.compression.snappy.blockSize\n32k\nBlock size in Snappy compression, in the case when Snappy compression codec is used.\n    Lowering this block size will also lower shuffle memory usage when Snappy is used.\n    Default unit is bytes, unless otherwise specified. This configuration only applies\n    to\nspark.io.compression.codec\n.\n1.4.0\nspark.io.compression.zstd.level\n1\nCompression level for Zstd compressio", "question": "What is the default unit for block size configurations in compression codecs?", "answers": {"text": ["Default unit is bytes, unless otherwise specified."], "answer_start": [265]}}
{"context": "configuration only applies to\nspark.io.compression.codec\n.\n2.3.0\nspark.io.compression.zstd.bufferPool.enabled\ntrue\nIf true, enable buffer pool of ZSTD JNI library.\n3.2.0\nspark.io.compression.zstd.workers\n0\nThread size spawned to compress in parallel when using Zstd. When value is 0\n    no worker is spawned, it works in single-threaded mode. When value > 0, it triggers\n    asynchronous mode, corresponding number of threads are spawned. More workers improve\n    performance, but also increase memory cost.\n4.0.0\nspark.io.compression.lzf.parallel.enabled\nfalse\nWhen true, LZF compression will use multiple threads to compress data in parallel.\n4.0.0\nspark.kryo.classesToRegister\n(none)\nIf you use Kryo serialization, give a comma-separated list of custom class names to register\n    with Kryo.\n    S", "question": "What happens when the value of spark.io.compression.zstd.workers is 0?", "answers": {"text": ["no worker is spawned, it works in single-threaded mode."], "answer_start": [287]}}
{"context": "eption\n    if an unregistered class is serialized. If set to false (the default), Kryo will write\n    unregistered class names along with each object. Writing class names can cause\n    significant performance overhead, so enabling this option can enforce strictly that a\n    user has not omitted classes from registration.\n1.1.0\nspark.kryo.registrator\n(none)\nIf you use Kryo serialization, give a comma-separated list of classes that register your custom classes with Kryo. This\n    property is useful if you need to register your classes in a custom way, e.g. to specify a custom\n    field serializer. Otherwise\nspark.kryo.classesToRegister\nis simpler. It should be\n    set to classes that extend\nKryoRegistrator\n.\n    See the\ntuning guide\nfor more details.\n0.5.0\nspark.kryo.unsafe\ntrue\nWhether to u", "question": "What does the property spark.kryo.registrator do?", "answers": {"text": ["If you use Kryo serialization, give a comma-separated list of classes that register your custom classes with Kryo."], "answer_start": [359]}}
{"context": "er. It should be\n    set to classes that extend\nKryoRegistrator\n.\n    See the\ntuning guide\nfor more details.\n0.5.0\nspark.kryo.unsafe\ntrue\nWhether to use unsafe based Kryo serializer. Can be\n    substantially faster by using Unsafe Based IO.\n2.1.0\nspark.kryoserializer.buffer.max\n64m\nMaximum allowable size of Kryo serialization buffer, in MiB unless otherwise specified.\n    This must be larger than any object you attempt to serialize and must be less than 2048m.\n    Increase this if you get a \"buffer limit exceeded\" exception inside Kryo.\n1.4.0\nspark.kryoserializer.buffer\n64k\nInitial size of Kryo's serialization buffer, in KiB unless otherwise specified.\n    Note that there will be one buffer\nper core\non each worker. This buffer will grow up to\nspark.kryoserializer.buffer.max\nif needed.\n1.4.", "question": "What is the maximum allowable size of Kryo serialization buffer?", "answers": {"text": ["64m"], "answer_start": [279]}}
{"context": " default it will reset the serializer every 100 objects.\n1.0.0\nMemory Management\nProperty Name\nDefault\nMeaning\nSince Version\nspark.memory.fraction\n0.6\nFraction of (heap space - 300MB) used for execution and storage. The lower this is, the\n    more frequently spills and cached data eviction occur. The purpose of this config is to set\n    aside memory for internal metadata, user data structures, and imprecise size estimation\n    in the case of sparse, unusually large records. Leaving this at the default value is\n    recommended. For more detail, including important information about correctly tuning JVM\n    garbage collection when increasing this value, see\nthis description\n.\n1.6.0\nspark.memory.storageFraction\n0.5\nAmount of storage memory immune to eviction, expressed as a fraction of the si", "question": "What is the default value for spark.memory.fraction?", "answers": {"text": ["0.6"], "answer_start": [147]}}
{"context": "ollection.\nThis context cleaner triggers cleanups only when weak references are garbage collected.\n    In long-running applications with large driver JVMs, where there is little memory pressure\n    on the driver, this may happen very occasionally or not at all. Not cleaning at all may\n    lead to executors running out of disk space after a while.\n1.6.0\nspark.cleaner.referenceTracking\ntrue\nEnables or disables context cleaning.\n1.0.0\nspark.cleaner.referenceTracking.blocking\ntrue\nControls whether the cleaning thread should block on cleanup tasks (other than shuffle, which is controlled by\nspark.cleaner.referenceTracking.blocking.shuffle\nSpark property).\n1.0.0\nspark.cleaner.referenceTracking.blocking.shuffle\nfalse\nControls whether the cleaning thread should block on shuffle cleanup tasks.\n1.1.", "question": "What does the property spark.cleaner.referenceTracking control?", "answers": {"text": ["Enables or disables context cleaning."], "answer_start": [392]}}
{"context": "operty).\n1.0.0\nspark.cleaner.referenceTracking.blocking.shuffle\nfalse\nControls whether the cleaning thread should block on shuffle cleanup tasks.\n1.1.1\nspark.cleaner.referenceTracking.cleanCheckpoints\nfalse\nControls whether to clean checkpoint files if the reference is out of scope.\n1.4.0\nExecution Behavior\nProperty Name\nDefault\nMeaning\nSince Version\nspark.broadcast.blockSize\n4m\nSize of each piece of a block for\nTorrentBroadcastFactory\n, in KiB unless otherwise\n    specified. Too large a value decreases parallelism during broadcast (makes it slower); however,\n    if it is too small,\nBlockManager\nmight take a performance hit.\n0.5.0\nspark.broadcast.checksum\ntrue\nWhether to enable checksum for broadcast. If enabled, broadcasts will include a checksum, which can\n    help detect corrupted block", "question": "Qual o tamanho padrão de cada pedaço de um bloco para TorrentBroadcastFactory?", "answers": {"text": ["4m"], "answer_start": [379]}}
{"context": "cast.checksum\ntrue\nWhether to enable checksum for broadcast. If enabled, broadcasts will include a checksum, which can\n    help detect corrupted blocks, at the cost of computing and sending a little more data. It's possible\n    to disable it if the network has other mechanisms to guarantee data won't be corrupted during broadcast.\n2.1.1\nspark.broadcast.UDFCompressionThreshold\n1 * 1024 * 1024\nThe threshold at which user-defined functions (UDFs) and Python RDD commands are compressed by broadcast in bytes unless otherwise specified.\n3.0.0\nspark.executor.cores\n1 in YARN mode, all the available cores on the worker in standalone mode.\nThe number of cores to use on each executor.\n1.0.0\nspark.default.parallelism\nFor distributed shuffle operations like\nreduceByKey\nand\njoin\n, the\n    largest number", "question": "What is the purpose of enabling checksum for broadcast?", "answers": {"text": ["help detect corrupted blocks, at the cost of computing and sending a little more data."], "answer_start": [123]}}
{"context": "memory mapping very small blocks. In general,\n    memory mapping has high overhead for blocks close to or below the page size of the operating system.\n0.9.2\nspark.storage.decommission.enabled\nfalse\nWhether to decommission the block manager when decommissioning executor.\n3.1.0\nspark.storage.decommission.shuffleBlocks.enabled\ntrue\nWhether to transfer shuffle blocks during block manager decommissioning. Requires a migratable shuffle resolver\n    (like sort based shuffle).\n3.1.0\nspark.storage.decommission.shuffleBlocks.maxThreads\n8\nMaximum number of threads to use in migrating shuffle files.\n3.1.0\nspark.storage.decommission.rddBlocks.enabled\ntrue\nWhether to transfer RDD blocks during block manager decommissioning.\n3.1.0\nspark.storage.decommission.fallbackStorage.path\n(none)\nThe location for fa", "question": "What does spark.storage.decommission.shuffleBlocks.enabled control?", "answers": {"text": ["Whether to transfer shuffle blocks during block manager decommissioning."], "answer_start": [331]}}
{"context": "\nWhether to transfer RDD blocks during block manager decommissioning.\n3.1.0\nspark.storage.decommission.fallbackStorage.path\n(none)\nThe location for fallback storage during block manager decommissioning. For example,\ns3a://spark-storage/\n.\n    In case of empty, fallback storage is disabled. The storage should be managed by TTL because Spark will not clean it up.\n3.1.0\nspark.storage.decommission.fallbackStorage.cleanUp\nfalse\nIf true, Spark cleans up its fallback storage data during shutting down.\n3.2.0\nspark.storage.decommission.shuffleBlocks.maxDiskSize\n(none)\nMaximum disk space to use to store shuffle blocks before rejecting remote shuffle blocks.\n    Rejecting remote shuffle blocks means that an executor will not receive any shuffle migrations,\n    and if there are no other executors avai", "question": "What happens if the `spark.storage.decommission.fallbackStorage.path` is empty?", "answers": {"text": ["In case of empty, fallback storage is disabled."], "answer_start": [243]}}
{"context": "ocks.\n    Rejecting remote shuffle blocks means that an executor will not receive any shuffle migrations,\n    and if there are no other executors available for migration then shuffle blocks will be lost unless\nspark.storage.decommission.fallbackStorage.path\nis configured.\n3.2.0\nspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\n1\nThe file output committer algorithm version, valid algorithm version number: 1 or 2.\n    Note that 2 may cause a correctness issue like MAPREDUCE-7282.\n2.2.0\nExecutor Metrics\nProperty Name\nDefault\nMeaning\nSince Version\nspark.eventLog.logStageExecutorMetrics\nfalse\nWhether to write per-stage peaks of executor metrics (for each executor) to the event log.\nNote:\nThe metrics are polled (collected) and sent in the executor heartbeat,\n    and this is always don", "question": "What are the valid algorithm version numbers for spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version?", "answers": {"text": ["1 or 2."], "answer_start": [419]}}
{"context": "ts (thus at the heartbeat interval,\n    specified by\nspark.executor.heartbeatInterval\n).\n    If positive, the polling is done at this interval.\n3.0.0\nspark.eventLog.gcMetrics.youngGenerationGarbageCollectors\nCopy,PS Scavenge,ParNew,G1 Young Generation\nNames of supported young generation garbage collector. A name usually is the return of GarbageCollectorMXBean.getName.\n    The built-in young generation garbage collectors are Copy,PS Scavenge,ParNew,G1 Young Generation.\n3.0.0\nspark.eventLog.gcMetrics.oldGenerationGarbageCollectors\nMarkSweepCompact,PS MarkSweep,ConcurrentMarkSweep,G1 Old Generation\nNames of supported old generation garbage collector. A name usually is the return of GarbageCollectorMXBean.getName.\n    The built-in old generation garbage collectors are MarkSweepCompact,PS MarkS", "question": "What are the built-in young generation garbage collectors?", "answers": {"text": ["The built-in young generation garbage collectors are Copy,PS Scavenge,ParNew,G1 Young Generation."], "answer_start": [375]}}
{"context": "ctor. A name usually is the return of GarbageCollectorMXBean.getName.\n    The built-in old generation garbage collectors are MarkSweepCompact,PS MarkSweep,ConcurrentMarkSweep,G1 Old Generation.\n3.0.0\nspark.executor.metrics.fileSystemSchemes\nfile,hdfs\nThe file system schemes to report in executor metrics.\n3.1.0\nNetworking\nProperty Name\nDefault\nMeaning\nSince Version\nspark.rpc.message.maxSize\n128\nMaximum message size (in MiB) to allow in \"control plane\" communication; generally only applies to map\n    output size information sent between executors and the driver. Increase this if you are running\n    jobs with many thousands of map and reduce tasks and see messages about the RPC message size.\n2.0.0\nspark.blockManager.port\n(random)\nPort for all block managers to listen on. These exist on both t", "question": "What is the default value for spark.rpc.message.maxSize?", "answers": {"text": ["128"], "answer_start": [393]}}
{"context": "ks and see messages about the RPC message size.\n2.0.0\nspark.blockManager.port\n(random)\nPort for all block managers to listen on. These exist on both the driver and the executors.\n1.1.0\nspark.driver.blockManager.port\n(value of spark.blockManager.port)\nDriver-specific port for the block manager to listen on, for cases where it cannot use the same\n    configuration as executors.\n2.1.0\nspark.driver.bindAddress\n(value of spark.driver.host)\nHostname or IP address where to bind listening sockets. This config overrides the SPARK_LOCAL_IP\n    environment variable (see below).\nIt also allows a different address from the local one to be advertised to executors or external systems.\n    This is useful, for example, when running containers with bridged networking. For this to properly work,\n    the diff", "question": "What does spark.driver.bindAddress configure?", "answers": {"text": ["Hostname or IP address where to bind listening sockets."], "answer_start": [439]}}
{"context": "ecutors or external systems.\n    This is useful, for example, when running containers with bridged networking. For this to properly work,\n    the different ports used by the driver (RPC, block manager and UI) need to be forwarded from the\n    container's host.\n2.1.0\nspark.driver.host\n(local hostname)\nHostname or IP address for the driver.\n    This is used for communicating with the executors and the standalone Master.\n0.7.0\nspark.driver.port\n(random)\nPort for the driver to listen on.\n    This is used for communicating with the executors and the standalone Master.\n0.7.0\nspark.rpc.io.backLog\n64\nLength of the accept queue for the RPC server. For large applications, this value may\n    need to be increased, so that incoming connections are not dropped when a large number of\n    connections arri", "question": "Para que é usado o spark.driver.host?", "answers": {"text": ["This is used for communicating with the executors and the standalone Master."], "answer_start": [345]}}
{"context": "0.0\nspark.rpc.io.connectionTimeout\nvalue of\nspark.network.timeout\nTimeout for the established connections between RPC peers to be marked as idled and closed\n    if there are outstanding RPC requests but no traffic on the channel for at least\nconnectionTimeout\n.\n1.2.0\nspark.rpc.io.connectionCreationTimeout\nvalue of\nspark.rpc.io.connectionTimeout\nTimeout for establishing a connection between RPC peers.\n3.2.0\nScheduling\nProperty Name\nDefault\nMeaning\nSince Version\nspark.cores.max\n(not set)\nWhen running on a\nstandalone deploy cluster\n, \n    the maximum amount of CPU cores to request for the application from\n    across the cluster (not from each machine). If not set, the default will be\nspark.deploy.defaultCores\non Spark's standalone cluster manager.\n0.6.0\nspark.locality.wait\n3s\nHow long to wait", "question": "What is the meaning of spark.cores.max when running on a standalone deploy cluster?", "answers": {"text": ["the maximum amount of CPU cores to request for the application from\n    across the cluster (not from each machine)."], "answer_start": [542]}}
{"context": "inRegisteredResourcesRatio\n0.8 for KUBERNETES mode; 0.8 for YARN mode; 0.0 for standalone mode\nThe minimum ratio of registered resources (registered resources / total expected resources)\n    (resources are executors in yarn mode and Kubernetes mode, CPU cores in standalone mode)\n    to wait for before scheduling begins. Specified as a double between 0.0 and 1.0.\n    Regardless of whether the minimum ratio of resources has been reached,\n    the maximum amount of time it will wait before scheduling begins is controlled by config\nspark.scheduler.maxRegisteredResourcesWaitingTime\n.\n1.1.1\nspark.scheduler.mode\nFIFO\nThe\nscheduling mode\nbetween\n    jobs submitted to the same SparkContext. Can be set to\nFAIR\nto use fair sharing instead of queueing jobs one after another. Useful for\n    multi-user s", "question": "What is the valid range for the inRegisteredResourcesRatio configuration?", "answers": {"text": ["Specified as a double between 0.0 and 1.0."], "answer_start": [322]}}
{"context": "obs submitted to the same SparkContext. Can be set to\nFAIR\nto use fair sharing instead of queueing jobs one after another. Useful for\n    multi-user services.\n0.8.0\nspark.scheduler.revive.interval\n1s\nThe interval length for the scheduler to revive the worker resource offers to run tasks.\n0.8.1\nspark.scheduler.listenerbus.eventqueue.capacity\n10000\nThe default capacity for event queues. Spark will try to initialize an event queue\n    using capacity specified by\nspark.scheduler.listenerbus.eventqueue.queueName.capacity\nfirst. If it's not configured, Spark will use the default capacity specified by this\n    config. Note that capacity must be greater than 0. Consider increasing value (e.g. 20000)\n    if listener events are dropped. Increasing this value may result in the driver using more memor", "question": "What is the default capacity for event queues in Spark?", "answers": {"text": ["10000"], "answer_start": [343]}}
{"context": "bus.eventqueue.capacity\nCapacity for appStatus event queue, which hold events for internal application status listeners.\n    Consider increasing value, if the listener events corresponding to appStatus queue are dropped.\n    Increasing this value may result in the driver using more memory.\n3.0.0\nspark.scheduler.listenerbus.eventqueue.executorManagement.capacity\nspark.scheduler.listenerbus.eventqueue.capacity\nCapacity for executorManagement event queue in Spark listener bus, which hold events for internal\n    executor management listeners. Consider increasing value if the listener events corresponding to\n    executorManagement queue are dropped. Increasing this value may result in the driver using more memory.\n3.0.0\nspark.scheduler.listenerbus.eventqueue.eventLog.capacity\nspark.scheduler.li", "question": "What should be considered if listener events corresponding to the appStatus queue are dropped?", "answers": {"text": ["Consider increasing value, if the listener events corresponding to appStatus queue are dropped."], "answer_start": [125]}}
{"context": "d. Increasing this value may result in the driver using more memory.\n3.0.0\nspark.scheduler.listenerbus.eventqueue.eventLog.capacity\nspark.scheduler.listenerbus.eventqueue.capacity\nCapacity for eventLog queue in Spark listener bus, which hold events for Event logging listeners\n    that write events to eventLogs. Consider increasing value if the listener events corresponding to eventLog queue\n    are dropped. Increasing this value may result in the driver using more memory.\n3.0.0\nspark.scheduler.listenerbus.eventqueue.streams.capacity\nspark.scheduler.listenerbus.eventqueue.capacity\nCapacity for streams queue in Spark listener bus, which hold events for internal streaming listener.\n    Consider increasing value if the listener events corresponding to streams queue are dropped. Increasing\n    ", "question": "What might happen if you increase the value of spark.scheduler.listenerbus.eventqueue.eventLog.capacity?", "answers": {"text": ["Increasing this value may result in the driver using more memory."], "answer_start": [3]}}
{"context": "stage.\n3.1.0\nspark.scheduler.excludeOnFailure.unschedulableTaskSetTimeout\n120s\nThe timeout in seconds to wait to acquire a new executor and schedule a task before aborting a\n    TaskSet which is unschedulable because all executors are excluded due to task failures.\n2.4.1\nspark.standalone.submit.waitAppCompletion\nfalse\nIf set to true, Spark will merge ResourceProfiles when different profiles are specified in RDDs that get combined into a single stage.\n    When they are merged, Spark chooses the maximum of each resource and creates a new ResourceProfile.\n    The default of false results in Spark throwing an exception if multiple different ResourceProfiles are found in RDDs going into the same stage.\n3.1.0\nspark.excludeOnFailure.enabled\nfalse\nIf set to \"true\", prevent Spark from scheduling ta", "question": "What happens when Spark encounters multiple different ResourceProfiles in RDDs going into the same stage with spark.standalone.submit.waitAppCompletion set to false?", "answers": {"text": ["The default of false results in Spark throwing an exception if multiple different ResourceProfiles are found in RDDs going into the same stage."], "answer_start": [563]}}
{"context": "g new tasks.\n2.1.0\nspark.excludeOnFailure.task.maxTaskAttemptsPerExecutor\n1\n(Experimental) For a given task, how many times it can be retried on one executor before the\n    executor is excluded for that task.\n2.1.0\nspark.excludeOnFailure.task.maxTaskAttemptsPerNode\n2\n(Experimental) For a given task, how many times it can be retried on one node, before the entire\n    node is excluded for that task.\n2.1.0\nspark.excludeOnFailure.stage.maxFailedTasksPerExecutor\n2\n(Experimental) How many different tasks must fail on one executor, within one stage, before the\n    executor is excluded for that stage.\n2.1.0\nspark.excludeOnFailure.stage.maxFailedExecutorsPerNode\n2\n(Experimental) How many different executors are marked as excluded for a given stage, before\n    the entire node is marked as failed for", "question": "For a given task, how many times can it be retried on one executor before the executor is excluded for that task?", "answers": {"text": ["For a given task, how many times it can be retried on one executor before the\n    executor is excluded for that task."], "answer_start": [91]}}
{"context": "re\n    running slowly in a stage, they will be re-launched.\n0.6.0\nspark.speculation.interval\n100ms\nHow often Spark will check for tasks to speculate.\n0.6.0\nspark.speculation.multiplier\n3\nHow many times slower a task is than the median to be considered for speculation.\n0.6.0\nspark.speculation.quantile\n0.9\nFraction of tasks which must be complete before speculation is enabled for a particular stage.\n0.6.0\nspark.speculation.minTaskRuntime\n100ms\nMinimum amount of time a task runs before being considered for speculation.\n    This can be used to avoid launching speculative copies of tasks that are very short.\n3.2.0\nspark.speculation.task.duration.threshold\nNone\nTask duration after which scheduler would try to speculative run the task. If provided, tasks\n    would be speculatively run if current ", "question": "What is the purpose of spark.speculation.minTaskRuntime?", "answers": {"text": ["Minimum amount of time a task runs before being considered for speculation.\n    This can be used to avoid launching speculative copies of tasks that are very short."], "answer_start": [446]}}
{"context": " successfulTaskDurations.median or\nspark.speculation.minTaskRuntime\n).\n3.4.0\nspark.task.cpus\n1\nNumber of cores to allocate for each task.\n0.5.0\nspark.task.resource.{resourceName}.amount\n1\nAmount of a particular resource type to allocate for each task, note that this can be a double.\n    If this is specified you must also provide the executor config\nspark.executor.resource.{resourceName}.amount\nand any corresponding discovery configs\n    so that your executors are created with that resource type. In addition to whole amounts,\n    a fractional amount (for example, 0.25, which means 1/4th of a resource) may be specified.\n    Fractional amounts must be less than or equal to 0.5, or in other words, the minimum amount of\n    resource sharing is 2 tasks per resource. Additionally, fractional amou", "question": "What is the maximum fractional amount allowed for a resource when using spark.task.resource.{resourceName}.amount?", "answers": {"text": ["Fractional amounts must be less than or equal to 0.5"], "answer_start": [630]}}
{"context": "ust be less than or equal to 0.5, or in other words, the minimum amount of\n    resource sharing is 2 tasks per resource. Additionally, fractional amounts are floored\n    in order to assign resource slots (e.g. a 0.2222 configuration, or 1/0.2222 slots will become\n    4 tasks/resource, not 5).\n3.0.0\nspark.task.maxFailures\n4\nNumber of continuous failures of any particular task before giving up on the job.\n    The total number of failures spread across different tasks will not cause the job\n    to fail; a particular task has to fail this number of attempts continuously.\n    If any attempt succeeds, the failure count for the task will be reset.\n    Should be greater than or equal to 1. Number of allowed retries = this value - 1.\n0.8.0\nspark.task.reaper.enabled\nfalse\nEnables monitoring of kille", "question": "What is the relationship between spark.task.maxFailures and the number of allowed retries?", "answers": {"text": ["Should be greater than or equal to 1. Number of allowed retries = this value - 1."], "answer_start": [653]}}
{"context": "leDataIO\nwho's\nShuffleDriverComponents\nsupports reliable storage.\n    The following configurations are also relevant:\nspark.dynamicAllocation.minExecutors\n,\nspark.dynamicAllocation.maxExecutors\n, and\nspark.dynamicAllocation.initialExecutors\nspark.dynamicAllocation.executorAllocationRatio\n1.2.0\nspark.dynamicAllocation.executorIdleTimeout\n60s\nIf dynamic allocation is enabled and an executor has been idle for more than this duration,\n    the executor will be removed. For more detail, see this\ndescription\n.\n1.2.0\nspark.dynamicAllocation.cachedExecutorIdleTimeout\ninfinity\nIf dynamic allocation is enabled and an executor which has cached data blocks has been idle for more than this duration,\n    the executor will be removed. For more details, see this\ndescription\n.\n1.4.0\nspark.dynamicAllocation.", "question": "What happens if an executor has been idle for more than the duration specified by spark.dynamicAllocation.executorIdleTimeout when dynamic allocation is enabled?", "answers": {"text": ["the executor will be removed."], "answer_start": [439]}}
{"context": "s has been idle for more than this duration,\n    the executor will be removed. For more details, see this\ndescription\n.\n1.4.0\nspark.dynamicAllocation.initialExecutors\nspark.dynamicAllocation.minExecutors\nInitial number of executors to run if dynamic allocation is enabled.\nIf\n--num-executors\n(or\nspark.executor.instances\n) is set and larger than this value, it will\n    be used as the initial number of executors.\n1.3.0\nspark.dynamicAllocation.maxExecutors\ninfinity\nUpper bound for the number of executors if dynamic allocation is enabled.\n1.2.0\nspark.dynamicAllocation.minExecutors\n0\nLower bound for the number of executors if dynamic allocation is enabled.\n1.2.0\nspark.dynamicAllocation.executorAllocationRatio\n1\nBy default, the dynamic allocation will request enough executors to maximize the\n    ", "question": "What is the default value for spark.dynamicAllocation.executorAllocationRatio?", "answers": {"text": ["1"], "answer_start": [120]}}
{"context": "enabled.\n1.2.0\nspark.dynamicAllocation.executorAllocationRatio\n1\nBy default, the dynamic allocation will request enough executors to maximize the\n    parallelism according to the number of tasks to process. While this minimizes the\n    latency of the job, with small tasks this setting can waste a lot of resources due to\n    executor allocation overhead, as some executor might not even do any work.\n    This setting allows to set a ratio that will be used to reduce the number of\n    executors w.r.t. full parallelism.\n    Defaults to 1.0 to give maximum parallelism.\n    0.5 will divide the target number of executors by 2\n    The target number of executors computed by the dynamicAllocation can still be overridden\n    by the\nspark.dynamicAllocation.minExecutors\nand\nspark.dynamicAllocation.maxEx", "question": "What does a setting of 0.5 do to the target number of executors computed by dynamic allocation?", "answers": {"text": ["0.5 will divide the target number of executors by 2"], "answer_start": [574]}}
{"context": "\nspark.dynamicAllocation.shuffleTracking.enabled\ntrue\nEnables shuffle file tracking for executors, which allows dynamic allocation\n    without the need for an external shuffle service. This option will try to keep alive executors\n    that are storing shuffle data for active jobs.\n3.0.0\nspark.dynamicAllocation.shuffleTracking.timeout\ninfinity\nWhen shuffle tracking is enabled, controls the timeout for executors that are holding shuffle\n    data. The default value means that Spark will rely on the shuffles being garbage collected to be\n    able to release executors. If for some reason garbage collection is not cleaning up shuffles\n    quickly enough, this option can be used to control when to time out executors even when they are\n    storing shuffle data.\n3.0.0\nThread Configurations\nDepending", "question": "What does enabling shuffle file tracking allow for dynamic allocation to function without?", "answers": {"text": ["without the need for an external shuffle service"], "answer_start": [135]}}
{"context": "roperty names except\nspark.{driver|executor}.rpc.netty.dispatcher.numThreads\n, which is only for RPC module.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.{driver|executor}.rpc.io.serverThreads\nFall back on\nspark.rpc.io.serverThreads\nNumber of threads used in the server thread pool\n1.6.0\nspark.{driver|executor}.rpc.io.clientThreads\nFall back on\nspark.rpc.io.clientThreads\nNumber of threads used in the client thread pool\n1.6.0\nspark.{driver|executor}.rpc.netty.dispatcher.numThreads\nFall back on\nspark.rpc.netty.dispatcher.numThreads\nNumber of threads used in RPC message dispatcher thread pool\n3.0.0\nThe default value for number of thread-related config keys is the minimum of the number of cores requested for\nthe driver or executor, or, in the absence of that value, the number of cores avai", "question": "Qual é a versão em que a propriedade spark.{driver|executor}.rpc.io.serverThreads foi introduzida?", "answers": {"text": ["1.6.0"], "answer_start": [287]}}
{"context": "ect server. The value can be\nclassic\nor\nconnect\n.\n4.0.0\nspark.connect.grpc.binding.address\n(none)\nAddress for Spark Connect server to bind.\n4.0.0\nspark.connect.grpc.binding.port\n15002\nPort for Spark Connect server to bind.\n3.4.0\nspark.connect.grpc.port.maxRetries\n0\nThe max port retry attempts for the gRPC server binding. By default, it's set to 0, and the server will fail fast in case of port conflicts.\n4.0.0\nspark.connect.grpc.interceptor.classes\n(none)\nComma separated list of class names that must implement the\nio.grpc.ServerInterceptor\ninterface\n3.4.0\nspark.connect.grpc.arrow.maxBatchSize\n4m\nWhen using Apache Arrow, limit the maximum size of one arrow batch that can be sent from server side to client side. Currently, we conservatively use 70% of it because the size is not accurate but e", "question": "What is the default value for the max port retry attempts for the gRPC server binding?", "answers": {"text": ["0"], "answer_start": [52]}}
{"context": "of one arrow batch that can be sent from server side to client side. Currently, we conservatively use 70% of it because the size is not accurate but estimated.\n3.4.0\nspark.connect.grpc.maxInboundMessageSize\n134217728\nSets the maximum inbound message size for the gRPC requests. Requests with a larger payload will fail.\n3.4.0\nspark.connect.extensions.relation.classes\n(none)\nComma separated list of classes that implement the trait\norg.apache.spark.sql.connect.plugin.RelationPlugin\nto support custom\nRelation types in proto.\n3.4.0\nspark.connect.extensions.expression.classes\n(none)\nComma separated list of classes that implement the trait\norg.apache.spark.sql.connect.plugin.ExpressionPlugin\nto support custom\nExpression types in proto.\n3.4.0\nspark.connect.extensions.command.classes\n(none)\nComma se", "question": "What does spark.connect.grpc.maxInboundMessageSize set?", "answers": {"text": ["Sets the maximum inbound message size for the gRPC requests. Requests with a larger payload will fail."], "answer_start": [217]}}
{"context": "tacktrace.enabled` is true.\n3.5.0\nspark.sql.connect.ui.retainedSessions\n200\nThe number of client sessions kept in the Spark Connect UI history.\n3.5.0\nspark.sql.connect.ui.retainedStatements\n200\nThe number of statements kept in the Spark Connect UI history.\n3.5.0\nspark.sql.connect.enrichError.enabled\ntrue\nWhen true, it enriches errors with full exception messages and optionally server-side stacktrace on the client side via an additional RPC.\n4.0.0\nspark.sql.connect.serverStacktrace.enabled\ntrue\nWhen true, it sets the server-side stacktrace in the user-facing Spark exception.\n4.0.0\nspark.connect.grpc.maxMetadataSize\n1024\nSets the maximum size of metadata fields. For instance, it restricts metadata fields in `ErrorInfo`.\n4.0.0\nspark.connect.progress.reportInterval\n2s\nThe interval at which the", "question": "What does `spark.sql.connect.serverStacktrace.enabled` do when set to true?", "answers": {"text": ["When true, it sets the server-side stacktrace in the user-facing Spark exception."], "answer_start": [499]}}
{"context": "to create\nSparkSession\n.\nAlso, they can be set and queried by SET commands and reset to their initial values by RESET command,\nor by\nSparkSession.conf\n’s setter and getter methods in runtime.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.adaptive.advisoryPartitionSizeInBytes\n(value of\nspark.sql.adaptive.shuffle.targetPostShuffleInputSize\n)\nThe advisory size in bytes of the shuffle partition during adaptive optimization (when spark.sql.adaptive.enabled is true). It takes effect when Spark coalesces small shuffle partitions or splits skewed shuffle partition.\n3.0.0\nspark.sql.adaptive.autoBroadcastJoinThreshold\n(none)\nConfigures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 broadcasting can be disab", "question": "What does spark.sql.adaptive.advisoryPartitionSizeInBytes control?", "answers": {"text": ["The advisory size in bytes of the shuffle partition during adaptive optimization (when spark.sql.adaptive.enabled is true). It takes effect when Spark coalesces small shuffle partitions or splits skewed shuffle partition."], "answer_start": [350]}}
{"context": "mum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 broadcasting can be disabled. The default value is same with spark.sql.autoBroadcastJoinThreshold. Note that, this config is used only in adaptive framework.\n3.2.0\nspark.sql.adaptive.coalescePartitions.enabled\ntrue\nWhen true and 'spark.sql.adaptive.enabled' is true, Spark will coalesce contiguous shuffle partitions according to the target size (specified by 'spark.sql.adaptive.advisoryPartitionSizeInBytes'), to avoid too many small tasks.\n3.0.0\nspark.sql.adaptive.coalescePartitions.initialPartitionNum\n(none)\nThe initial number of shuffle partitions before coalescing. If not set, it equals to spark.sql.shuffle.partitions. This configuration only has an effect when 'sp", "question": "What happens when the 'spark.sql.adaptive.coalescePartitions.enabled' config is set to true and 'spark.sql.adaptive.enabled' is also true?", "answers": {"text": ["When true and 'spark.sql.adaptive.enabled' is true, Spark will coalesce contiguous shuffle partitions according to the target size (specified by 'spark.sql.adaptive.advisoryPartitionSizeInBytes'), to avoid too many small tasks."], "answer_start": [340]}}
{"context": " number of shuffle partitions before coalescing. If not set, it equals to spark.sql.shuffle.partitions. This configuration only has an effect when 'spark.sql.adaptive.enabled' and 'spark.sql.adaptive.coalescePartitions.enabled' are both true.\n3.0.0\nspark.sql.adaptive.coalescePartitions.minPartitionSize\n1MB\nThe minimum size of shuffle partitions after coalescing. This is useful when the adaptively calculated target size is too small during partition coalescing.\n3.2.0\nspark.sql.adaptive.coalescePartitions.parallelismFirst\ntrue\nWhen true, Spark does not respect the target size specified by 'spark.sql.adaptive.advisoryPartitionSizeInBytes' (default 64MB) when coalescing contiguous shuffle partitions, but adaptively calculate the target size according to the default parallelism of the Spark clu", "question": "What happens when 'spark.sql.adaptive.coalescePartitions.parallelismFirst' is set to true?", "answers": {"text": ["When true, Spark does not respect the target size specified by 'spark.sql.adaptive.advisoryPartitionSizeInBytes' (default 64MB) when coalescing contiguous shuffle partitions, but adaptively calculate the target size according to the default parallelism of the Spark clu"], "answer_start": [531]}}
{"context": "lt 64MB) when coalescing contiguous shuffle partitions, but adaptively calculate the target size according to the default parallelism of the Spark cluster. The calculated size is usually smaller than the configured target size. This is to maximize the parallelism and avoid performance regressions when enabling adaptive query execution. It's recommended to set this config to false on a busy cluster to make resource utilization more efficient (not many small tasks).\n3.2.0\nspark.sql.adaptive.customCostEvaluatorClass\n(none)\nThe custom cost evaluator class to be used for adaptive execution. If not being set, Spark will use its own SimpleCostEvaluator by default.\n3.2.0\nspark.sql.adaptive.enabled\ntrue\nWhen true, enable adaptive query execution, which re-optimizes the query plan in the middle of q", "question": "What does Spark use as its default cost evaluator for adaptive execution if no custom class is set?", "answers": {"text": ["Spark will use its own SimpleCostEvaluator by default."], "answer_start": [611]}}
{"context": "tor by default.\n3.2.0\nspark.sql.adaptive.enabled\ntrue\nWhen true, enable adaptive query execution, which re-optimizes the query plan in the middle of query execution, based on accurate runtime statistics.\n1.6.0\nspark.sql.adaptive.forceOptimizeSkewedJoin\nfalse\nWhen true, force enable OptimizeSkewedJoin even if it introduces extra shuffle.\n3.3.0\nspark.sql.adaptive.localShuffleReader.enabled\ntrue\nWhen true and 'spark.sql.adaptive.enabled' is true, Spark tries to use local shuffle reader to read the shuffle data when the shuffle partitioning is not needed, for example, after converting sort-merge join to broadcast-hash join.\n3.0.0\nspark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold\n0b\nConfigures the maximum size in bytes per partition that can be allowed to build local hash map. If this val", "question": "What happens when 'spark.sql.adaptive.enabled' is set to true?", "answers": {"text": ["When true, enable adaptive query execution, which re-optimizes the query plan in the middle of query execution, based on accurate runtime statistics."], "answer_start": [54]}}
{"context": "ve.maxShuffledHashJoinLocalMapThreshold\n0b\nConfigures the maximum size in bytes per partition that can be allowed to build local hash map. If this value is not smaller than spark.sql.adaptive.advisoryPartitionSizeInBytes and all the partition size are not larger than this config, join selection prefer to use shuffled hash join instead of sort merge join regardless of the value of spark.sql.join.preferSortMergeJoin.\n3.2.0\nspark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled\ntrue\nWhen true and 'spark.sql.adaptive.enabled' is true, Spark will optimize the skewed shuffle partitions in RebalancePartitions and split them to smaller ones according to the target size (specified by 'spark.sql.adaptive.advisoryPartitionSizeInBytes'), to avoid data skew.\n3.2.0\nspark.sql.adaptive.optimizer.ex", "question": "What happens when 'spark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled' is true and 'spark.sql.adaptive.enabled' is true?", "answers": {"text": ["When true and 'spark.sql.adaptive.enabled' is true, Spark will optimize the skewed shuffle partitions in RebalancePartitions and split them to smaller ones according to the target size (specified by 'spark.sql.adaptive.advisoryPartitionSizeInBytes'), to avoid data skew."], "answer_start": [492]}}
{"context": "cording to the target size (specified by 'spark.sql.adaptive.advisoryPartitionSizeInBytes'), to avoid data skew.\n3.2.0\nspark.sql.adaptive.optimizer.excludedRules\n(none)\nConfigures a list of rules to be disabled in the adaptive optimizer, in which the rules are specified by their rule names and separated by comma. The optimizer will log the rules that have indeed been excluded.\n3.1.0\nspark.sql.adaptive.rebalancePartitionsSmallPartitionFactor\n0.2\nA partition will be merged during splitting if its size is small than this factor multiply spark.sql.adaptive.advisoryPartitionSizeInBytes.\n3.3.0\nspark.sql.adaptive.skewJoin.enabled\ntrue\nWhen true and 'spark.sql.adaptive.enabled' is true, Spark dynamically handles skew in shuffled join (sort-merge and shuffled hash) by splitting (and replicating if ", "question": "What happens when 'spark.sql.adaptive.skewJoin.enabled' is true and 'spark.sql.adaptive.enabled' is also true?", "answers": {"text": ["Spark dynamically handles skew in shuffled join (sort-merge and shuffled hash) by splitting (and replicating if"], "answer_start": [688]}}
{"context": "'spark.sql.adaptive.enabled' is true, Spark dynamically handles skew in shuffled join (sort-merge and shuffled hash) by splitting (and replicating if needed) skewed partitions.\n3.0.0\nspark.sql.adaptive.skewJoin.skewedPartitionFactor\n5.0\nA partition is considered as skewed if its size is larger than this factor multiplying the median partition size and also larger than 'spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes'\n3.0.0\nspark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\n256MB\nA partition is considered as skewed if its size in bytes is larger than this threshold and also larger than 'spark.sql.adaptive.skewJoin.skewedPartitionFactor' multiplying the median partition size. Ideally this config should be set larger than 'spark.sql.adaptive.advisoryPartitionSizeInBytes'.", "question": "Como o Spark lida com dados enviesados em junções embaralhadas?", "answers": {"text": ["Spark dynamically handles skew in shuffled join (sort-merge and shuffled hash) by splitting (and replicating if needed) skewed partitions."], "answer_start": [38]}}
{"context": "rk will throw an exception at runtime instead of returning null results when the inputs to a SQL operator/function are invalid. For full details of this dialect, you can find them in the section \"ANSI Compliance\" of Spark's documentation. Some ANSI dialect features may be not from the ANSI SQL standard directly, but their behaviors align with ANSI SQL's style\n3.0.0\nspark.sql.ansi.enforceReservedKeywords\nfalse\nWhen true and 'spark.sql.ansi.enabled' is true, the Spark SQL parser enforces the ANSI reserved keywords and forbids SQL queries that use reserved keywords as alias names and/or identifiers for table, view, function, etc.\n3.3.0\nspark.sql.ansi.relationPrecedence\nfalse\nWhen true and 'spark.sql.ansi.enabled' is true, JOIN takes precedence over comma when combining relation. For example,\n", "question": "What happens when 'spark.sql.ansi.enforceReservedKeywords' is set to true and 'spark.sql.ansi.enabled' is also true?", "answers": {"text": ["the Spark SQL parser enforces the ANSI reserved keywords and forbids SQL queries that use reserved keywords as alias names and/or identifiers for table, view, function, etc."], "answer_start": [461]}}
{"context": ".ansi.relationPrecedence\nfalse\nWhen true and 'spark.sql.ansi.enabled' is true, JOIN takes precedence over comma when combining relation. For example,\nt1, t2 JOIN t3\nshould result to\nt1 X (t2 X t3)\n. If the config is false, the result is\n(t1 X t2) X t3\n.\n3.4.0\nspark.sql.autoBroadcastJoinThreshold\n10MB\nConfigures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 broadcasting can be disabled.\n1.1.0\nspark.sql.avro.compression.codec\nsnappy\nCompression codec used in writing of AVRO files. Supported codecs: uncompressed, deflate, snappy, bzip2, xz and zstandard. Default codec is snappy.\n2.4.0\nspark.sql.avro.deflate.level\n-1\nCompression level for the deflate codec used in writing of AVRO files. Valid value must be i", "question": "What happens when 'spark.sql.ansi.enabled' is true and '.ansi.relationPrecedence' is true regarding JOIN precedence?", "answers": {"text": ["JOIN takes precedence over comma when combining relation."], "answer_start": [79]}}
{"context": "lt codec is snappy.\n2.4.0\nspark.sql.avro.deflate.level\n-1\nCompression level for the deflate codec used in writing of AVRO files. Valid value must be in the range of from 1 to 9 inclusive or -1. The default value is -1 which corresponds to 6 level in the current implementation.\n2.4.0\nspark.sql.avro.filterPushdown.enabled\ntrue\nWhen true, enable filter pushdown to Avro datasource.\n3.1.0\nspark.sql.avro.xz.level\n6\nCompression level for the xz codec used in writing of AVRO files. Valid value must be in the range of from 1 to 9 inclusive The default value is 6.\n4.0.0\nspark.sql.avro.zstandard.bufferPool.enabled\nfalse\nIf true, enable buffer pool of ZSTD JNI library when writing of AVRO files\n4.0.0\nspark.sql.avro.zstandard.level\n3\nCompression level for the zstandard codec used in writing of AVRO fil", "question": "What is the default value for spark.sql.avro.deflate.level?", "answers": {"text": ["The default value is -1 which corresponds to 6 level in the current implementation."], "answer_start": [194]}}
{"context": "TD JNI library when writing of AVRO files\n4.0.0\nspark.sql.avro.zstandard.level\n3\nCompression level for the zstandard codec used in writing of AVRO files.\n4.0.0\nspark.sql.binaryOutputStyle\n(none)\nThe output style used display binary data. Valid values are 'UTF-8', 'BASIC', 'BASE64', 'HEX', and 'HEX_DISCRETE'.\n4.0.0\nspark.sql.broadcastTimeout\n300\nTimeout in seconds for the broadcast wait time in broadcast joins.\n1.3.0\nspark.sql.bucketing.coalesceBucketsInJoin.enabled\nfalse\nWhen true, if two bucketed tables with the different number of buckets are joined, the side with a bigger number of buckets will be coalesced to have the same number of buckets as the other side. Bigger number of buckets is divisible by the smaller number of buckets. Bucket coalescing is applied to sort-merge joins and shu", "question": "What is the purpose of the 'spark.sql.avro.zstandard.level' configuration?", "answers": {"text": ["Compression level for the zstandard codec used in writing of AVRO files."], "answer_start": [81]}}
{"context": "ts as the other side. Bigger number of buckets is divisible by the smaller number of buckets. Bucket coalescing is applied to sort-merge joins and shuffled hash join. Note: Coalescing bucketed table can avoid unnecessary shuffling in join, but it also reduces parallelism and could possibly cause OOM for shuffled hash join.\n3.1.0\nspark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio\n4\nThe ratio of the number of two buckets being coalesced should be less than or equal to this value for bucket coalescing to be applied. This configuration only has an effect when 'spark.sql.bucketing.coalesceBucketsInJoin.enabled' is set to true.\n3.1.0\nspark.sql.catalog.spark_catalog\nbuiltin\nA catalog implementation that will be used as the v2 interface to Spark's built-in v1 catalog: spark_catalog. This cat", "question": "What is the purpose of 'spark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio'?", "answers": {"text": ["The ratio of the number of two buckets being coalesced should be less than or equal to this value for bucket coalescing to be applied."], "answer_start": [390]}}
{"context": "l.catalog.spark_catalog\nbuiltin\nA catalog implementation that will be used as the v2 interface to Spark's built-in v1 catalog: spark_catalog. This catalog shares its identifier namespace with the spark_catalog and must be consistent with it; for example, if a table can be loaded by the spark_catalog, this catalog must also return the table metadata. To delegate operations to the spark_catalog, implementations can extend 'CatalogExtension'. The value should be either 'builtin' which represents the spark's builit-in V2SessionCatalog, or a fully qualified class name of the catalog implementation.\n3.0.0\nspark.sql.cbo.enabled\nfalse\nEnables CBO for estimation of plan statistics when set true.\n2.2.0\nspark.sql.cbo.joinReorder.dp.star.filter\nfalse\nApplies star-join filter heuristics to cost based j", "question": "What is the purpose of l.catalog.spark_catalog?", "answers": {"text": ["A catalog implementation that will be used as the v2 interface to Spark's built-in v1 catalog: spark_catalog."], "answer_start": [32]}}
{"context": " estimation of plan statistics when set true.\n2.2.0\nspark.sql.cbo.joinReorder.dp.star.filter\nfalse\nApplies star-join filter heuristics to cost based join enumeration.\n2.2.0\nspark.sql.cbo.joinReorder.dp.threshold\n12\nThe maximum number of joined nodes allowed in the dynamic programming algorithm.\n2.2.0\nspark.sql.cbo.joinReorder.enabled\nfalse\nEnables join reorder in CBO.\n2.2.0\nspark.sql.cbo.planStats.enabled\nfalse\nWhen true, the logical plan will fetch row counts and column statistics from catalog.\n3.0.0\nspark.sql.cbo.starSchemaDetection\nfalse\nWhen true, it enables join reordering based on star schema detection.\n2.2.0\nspark.sql.charAsVarchar\nfalse\nWhen true, Spark replaces CHAR type with VARCHAR type in CREATE/REPLACE/ALTER TABLE commands, so that newly created/updated tables will not have CH", "question": "What does setting 'spark.sql.cbo.planStats.enabled' to true do?", "answers": {"text": ["When true, the logical plan will fetch row counts and column statistics from catalog."], "answer_start": [415]}}
{"context": "se\nWhen true, Spark replaces CHAR type with VARCHAR type in CREATE/REPLACE/ALTER TABLE commands, so that newly created/updated tables will not have CHAR type columns/fields. Existing tables with CHAR type columns/fields are not affected by this config.\n3.3.0\nspark.sql.chunkBase64String.enabled\ntrue\nWhether to truncate string generated by the\nBase64\nfunction. When true, base64 strings generated by the base64 function are chunked into lines of at most 76 characters. When false, the base64 strings are not chunked.\n3.5.2\nspark.sql.cli.print.header\nfalse\nWhen set to true, spark-sql CLI prints the names of the columns in query output.\n3.2.0\nspark.sql.columnNameOfCorruptRecord\n_corrupt_record\nThe name of internal column for storing raw/un-parsed JSON and CSV records that fail to parse.\n1.2.0\nspar", "question": "What happens when spark.sql.charTypesToVarchar.enabled is set to true?", "answers": {"text": ["When true, Spark replaces CHAR type with VARCHAR type in CREATE/REPLACE/ALTER TABLE commands, so that newly created/updated tables will not have CHAR type columns/fields."], "answer_start": [3]}}
{"context": "ql.columnNameOfCorruptRecord\n_corrupt_record\nThe name of internal column for storing raw/un-parsed JSON and CSV records that fail to parse.\n1.2.0\nspark.sql.csv.filterPushdown.enabled\ntrue\nWhen true, enable filter pushdown to CSV datasource.\n3.0.0\nspark.sql.datetime.java8API.enabled\nfalse\nIf the configuration property is set to true, java.time.Instant and java.time.LocalDate classes of Java 8 API are used as external types for Catalyst's TimestampType and DateType. If it is set to false, java.sql.Timestamp and java.sql.Date are used for the same purpose.\n3.0.0\nspark.sql.debug.maxToStringFields\n25\nMaximum number of fields of sequence-like entries can be converted to strings in debug output. Any elements beyond the limit will be dropped and replaced by a  \"... N more fields\" placeholder.\n3.0.", "question": "What happens when spark.sql.debug.maxToStringFields is set?", "answers": {"text": ["Maximum number of fields of sequence-like entries can be converted to strings in debug output. Any elements beyond the limit will be dropped and replaced by a  \"... N more fields\" placeholder."], "answer_start": [603]}}
{"context": "es can be converted to strings in debug output. Any elements beyond the limit will be dropped and replaced by a  \"... N more fields\" placeholder.\n3.0.0\nspark.sql.defaultCacheStorageLevel\nMEMORY_AND_DISK\nThe default storage level of\ndataset.cache()\n,\ncatalog.cacheTable()\nand sql query\nCACHE TABLE t\n.\n4.0.0\nspark.sql.defaultCatalog\nspark_catalog\nName of the default catalog. This will be the current catalog if users have not explicitly set the current catalog yet.\n3.0.0\nspark.sql.error.messageFormat\nPRETTY\nWhen PRETTY, the error message consists of textual representation of error class, message and query context. The MINIMAL and STANDARD formats are pretty JSON formats where STANDARD includes an additional JSON field\nmessage\n. This configuration property influences on error messages of Thrift", "question": "What happens to elements beyond the limit when converting to strings in debug output?", "answers": {"text": ["Any elements beyond the limit will be dropped and replaced by a  \"... N more fields\" placeholder."], "answer_start": [48]}}
{"context": " are pretty JSON formats where STANDARD includes an additional JSON field\nmessage\n. This configuration property influences on error messages of Thrift Server and SQL CLI while running queries.\n3.4.0\nspark.sql.execution.arrow.enabled\nfalse\n(Deprecated since Spark 3.0, please set 'spark.sql.execution.arrow.pyspark.enabled'.)\n2.3.0\nspark.sql.execution.arrow.fallback.enabled\ntrue\n(Deprecated since Spark 3.0, please set 'spark.sql.execution.arrow.pyspark.fallback.enabled'.)\n2.4.0\nspark.sql.execution.arrow.localRelationThreshold\n48MB\nWhen converting Arrow batches to Spark DataFrame, local collections are used in the driver side if the byte size of Arrow batches is smaller than this threshold. Otherwise, the Arrow batches are sent and deserialized to Spark internal rows in the executors.\n3.4.0\nsp", "question": "What happens when the byte size of Arrow batches is smaller than the 'spark.sql.execution.arrow.localRelationThreshold'?", "answers": {"text": ["local collections are used in the driver side"], "answer_start": [584]}}
{"context": "Arrow batches is smaller than this threshold. Otherwise, the Arrow batches are sent and deserialized to Spark internal rows in the executors.\n3.4.0\nspark.sql.execution.arrow.maxRecordsPerBatch\n10000\nWhen using Apache Arrow, limit the maximum number of records that can be written to a single ArrowRecordBatch in memory. This configuration is not effective for the grouping API such as DataFrame(.cogroup).groupby.applyInPandas because each group becomes each ArrowRecordBatch. If set to zero or negative there is no limit. See also spark.sql.execution.arrow.maxBytesPerBatch. If both are set, each batch is created when any condition of both is met.\n2.3.0\nspark.sql.execution.arrow.pyspark.enabled\n(value of\nspark.sql.execution.arrow.enabled\n)\nWhen true, make use of Apache Arrow for columnar data tr", "question": "What happens when the number of records exceeds the spark.sql.execution.arrow.maxRecordsPerBatch threshold?", "answers": {"text": ["Otherwise, the Arrow batches are sent and deserialized to Spark internal rows in the executors."], "answer_start": [46]}}
{"context": "2.3.0\nspark.sql.execution.arrow.pyspark.enabled\n(value of\nspark.sql.execution.arrow.enabled\n)\nWhen true, make use of Apache Arrow for columnar data transfers in PySpark. This optimization applies to: 1. pyspark.sql.DataFrame.toPandas. 2. pyspark.sql.SparkSession.createDataFrame when its input is a Pandas DataFrame or a NumPy ndarray. The following data type is unsupported: ArrayType of TimestampType.\n3.0.0\nspark.sql.execution.arrow.pyspark.fallback.enabled\n(value of\nspark.sql.execution.arrow.fallback.enabled\n)\nWhen true, optimizations enabled by 'spark.sql.execution.arrow.pyspark.enabled' will fallback automatically to non-optimized implementations if an error occurs.\n3.0.0\nspark.sql.execution.arrow.pyspark.selfDestruct.enabled\nfalse\n(Experimental) When true, make use of Apache Arrow's sel", "question": "What does setting 'spark.sql.execution.arrow.pyspark.enabled' to true do?", "answers": {"text": ["When true, make use of Apache Arrow for columnar data transfers in PySpark."], "answer_start": [94]}}
{"context": "ations if an error occurs.\n3.0.0\nspark.sql.execution.arrow.pyspark.selfDestruct.enabled\nfalse\n(Experimental) When true, make use of Apache Arrow's self-destruct and split-blocks options for columnar data transfers in PySpark, when converting from Arrow to Pandas. This reduces memory usage at the cost of some CPU time. This optimization applies to: pyspark.sql.DataFrame.toPandas when 'spark.sql.execution.arrow.pyspark.enabled' is set.\n3.2.0\nspark.sql.execution.arrow.sparkr.enabled\nfalse\nWhen true, make use of Apache Arrow for columnar data transfers in SparkR. This optimization applies to: 1. createDataFrame when its input is an R DataFrame 2. collect 3. dapply 4. gapply The following data types are unsupported: FloatType, BinaryType, ArrayType, StructType and MapType.\n3.0.0\nspark.sql.execu", "question": "What does enabling 'spark.sql.execution.arrow.pyspark.selfDestruct.enabled' do?", "answers": {"text": ["When true, make use of Apache Arrow's self-destruct and split-blocks options for columnar data transfers in PySpark, when converting from Arrow to Pandas. This reduces memory usage at the cost of some CPU time."], "answer_start": [109]}}
{"context": " collect 3. dapply 4. gapply The following data types are unsupported: FloatType, BinaryType, ArrayType, StructType and MapType.\n3.0.0\nspark.sql.execution.arrow.transformWithStateInPandas.maxRecordsPerBatch\n10000\nWhen using TransformWithStateInPandas, limit the maximum number of state records that can be written to a single ArrowRecordBatch in memory.\n4.0.0\nspark.sql.execution.arrow.useLargeVarTypes\nfalse\nWhen using Apache Arrow, use large variable width vectors for string and binary types. Regular string and binary types have a 2GiB limit for a column in a single record batch. Large variable types remove this limitation at the cost of higher memory usage per value.\n3.5.0\nspark.sql.execution.interruptOnCancel\ntrue\nWhen true, all running tasks will be interrupted if one cancels a query.\n4.0", "question": "What is the effect of setting spark.sql.execution.arrow.useLargeVarTypes to true?", "answers": {"text": ["Large variable types remove this limitation at the cost of higher memory usage per value."], "answer_start": [585]}}
{"context": " memory usage per value.\n3.5.0\nspark.sql.execution.interruptOnCancel\ntrue\nWhen true, all running tasks will be interrupted if one cancels a query.\n4.0.0\nspark.sql.execution.pandas.inferPandasDictAsMap\nfalse\nWhen true, spark.createDataFrame will infer dict from Pandas DataFrame as a MapType. When false, spark.createDataFrame infers dict from Pandas DataFrame as a StructType which is default inferring from PyArrow.\n4.0.0\nspark.sql.execution.pandas.structHandlingMode\nlegacy\nThe conversion mode of struct type when creating pandas DataFrame. When \"legacy\", 1. when Arrow optimization is disabled, convert to Row object, 2. when Arrow optimization is enabled, convert to dict or raise an Exception if there are duplicated nested field names. When \"row\", convert to Row object regardless of Arrow opti", "question": "What happens when spark.sql.execution.interruptOnCancel is set to true?", "answers": {"text": ["When true, all running tasks will be interrupted if one cancels a query."], "answer_start": [74]}}
{"context": " enabled, convert to dict or raise an Exception if there are duplicated nested field names. When \"row\", convert to Row object regardless of Arrow optimization. When \"dict\", convert to dict and use suffixed key names, e.g., a_0, a_1, if there are duplicated nested field names, regardless of Arrow optimization.\n3.5.0\nspark.sql.execution.pandas.udf.buffer.size\n(value of\nspark.buffer.size\n)\nSame as\nspark.buffer.size\nbut only applies to Pandas UDF executions. If it is not set, the fallback is\nspark.buffer.size\n. Note that Pandas execution requires more than 4 bytes. Lowering this value could make small Pandas UDF batch iterated and pipelined; however, it might degrade performance. See SPARK-27870.\n3.0.0\nspark.sql.execution.pyspark.udf.faulthandler.enabled\n(value of\nspark.python.worker.faulthand", "question": "What happens if duplicated nested field names are encountered when converting to a dictionary?", "answers": {"text": ["convert to dict and use suffixed key names, e.g., a_0, a_1, if there are duplicated nested field names, regardless of Arrow optimization."], "answer_start": [173]}}
{"context": "ver, it might degrade performance. See SPARK-27870.\n3.0.0\nspark.sql.execution.pyspark.udf.faulthandler.enabled\n(value of\nspark.python.worker.faulthandler.enabled\n)\nSame as spark.python.worker.faulthandler.enabled for Python execution with DataFrame and SQL. It can change during runtime.\n4.0.0\nspark.sql.execution.pyspark.udf.hideTraceback.enabled\nfalse\nWhen true, only show the message of the exception from Python UDFs, hiding the stack trace. If this is enabled, simplifiedTraceback has no effect.\n4.0.0\nspark.sql.execution.pyspark.udf.idleTimeoutSeconds\n(value of\nspark.python.worker.idleTimeoutSeconds\n)\nSame as spark.python.worker.idleTimeoutSeconds for Python execution with DataFrame and SQL. It can change during runtime.\n4.0.0\nspark.sql.execution.pyspark.udf.simplifiedTraceback.enabled\ntru", "question": "What does enabling spark.sql.execution.pyspark.udf.hideTraceback.enabled do?", "answers": {"text": ["When true, only show the message of the exception from Python UDFs, hiding the stack trace."], "answer_start": [354]}}
{"context": "conds for Python execution with DataFrame and SQL. It can change during runtime.\n4.0.0\nspark.sql.execution.pyspark.udf.simplifiedTraceback.enabled\ntrue\nWhen true, the traceback from Python UDFs is simplified. It hides the Python worker, (de)serialization, etc from PySpark in tracebacks, and only shows the exception messages from UDFs. Note that this works only with CPython 3.7+.\n3.1.0\nspark.sql.execution.python.udf.buffer.size\n(value of\nspark.buffer.size\n)\nSame as\nspark.buffer.size\nbut only applies to Python UDF executions. If it is not set, the fallback is\nspark.buffer.size\n.\n4.0.0\nspark.sql.execution.python.udf.maxRecordsPerBatch\n100\nWhen using Python UDFs, limit the maximum number of records that can be batched for serialization/deserialization.\n4.0.0\nspark.sql.execution.pythonUDF.arrow", "question": "What does enabling 'spark.sql.execution.pyspark.udf.simplifiedTraceback.enabled' do?", "answers": {"text": ["When true, the traceback from Python UDFs is simplified. It hides the Python worker, (de)serialization, etc from PySpark in tracebacks, and only shows the exception messages from UDFs."], "answer_start": [152]}}
{"context": "sing Python UDFs, limit the maximum number of records that can be batched for serialization/deserialization.\n4.0.0\nspark.sql.execution.pythonUDF.arrow.concurrency.level\n(none)\nThe level of concurrency to execute Arrow-optimized Python UDF. This can be useful if Python UDFs use I/O intensively.\n4.0.0\nspark.sql.execution.pythonUDF.arrow.enabled\nfalse\nEnable Arrow optimization in regular Python UDFs. This optimization can only be enabled when the given function takes at least one argument.\n3.4.0\nspark.sql.execution.pythonUDTF.arrow.enabled\nfalse\nEnable Arrow optimization for Python UDTFs.\n3.5.0\nspark.sql.execution.topKSortFallbackThreshold\n2147483632\nIn SQL queries with a SORT followed by a LIMIT like 'SELECT x FROM t ORDER BY y LIMIT m', if m is under this threshold, do a top-K sort in memor", "question": "What does spark.sql.execution.pythonUDF.arrow.enabled control?", "answers": {"text": ["Enable Arrow optimization in regular Python UDFs. This optimization can only be enabled when the given function takes at least one argument."], "answer_start": [351]}}
{"context": "83632\nIn SQL queries with a SORT followed by a LIMIT like 'SELECT x FROM t ORDER BY y LIMIT m', if m is under this threshold, do a top-K sort in memory, otherwise do a global sort which spills to disk if necessary.\n2.4.0\nspark.sql.extendedExplainProviders\n(none)\nA comma-separated list of classes that implement the org.apache.spark.sql.ExtendedExplainGenerator trait. If provided, Spark will print extended plan information from the providers in explain plan and in the UI\n4.0.0\nspark.sql.files.ignoreCorruptFiles\nfalse\nWhether to ignore corrupt files. If true, the Spark jobs will continue to run when encountering corrupted files and the contents that have been read will still be returned. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.\n2.1.1\nsp", "question": "What happens when a SQL query with SORT and LIMIT has a limit 'm' under a certain threshold?", "answers": {"text": ["do a top-K sort in memory, otherwise do a global sort which spills to disk if necessary."], "answer_start": [126]}}
{"context": "that have been read will still be returned. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.\n2.1.1\nspark.sql.files.ignoreInvalidPartitionPaths\nfalse\nWhether to ignore invalid partition paths that do not match <column>=<value>. When the option is enabled, table with two partition directories 'table/invalid' and 'table/col=1' will only load the latter directory and ignore the invalid partition\n4.0.0\nspark.sql.files.ignoreMissingFiles\nfalse\nWhether to ignore missing files. If true, the Spark jobs will continue to run when encountering missing files and the contents that have been read will still be returned. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.\n2.3.0\nspark.sql.files.maxPartitionBytes\n", "question": "When is the configuration `spark.sql.files.ignoreInvalidPartitionPaths` effective?", "answers": {"text": ["This configuration is effective only when using file-based sources such as Parquet, JSON and ORC."], "answer_start": [44]}}
{"context": "e returned. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.\n2.3.0\nspark.sql.files.maxPartitionBytes\n128MB\nThe maximum number of bytes to pack into a single partition when reading files. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.\n2.0.0\nspark.sql.files.maxPartitionNum\n(none)\nThe suggested (not guaranteed) maximum number of split file partitions. If it is set, Spark will rescale each partition to make the number of partitions is close to this value if the initial number of partitions exceeds this value. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.\n3.5.0\nspark.sql.files.maxRecordsPerFile\n0\nMaximum number of records to write out to a singl", "question": "Para quais tipos de fontes a configuração spark.sql.files.maxPartitionBytes é eficaz?", "answers": {"text": ["This configuration is effective only when using file-based sources such as Parquet, JSON and ORC."], "answer_start": [12]}}
{"context": "en using file-based sources such as Parquet, JSON and ORC.\n3.5.0\nspark.sql.files.maxRecordsPerFile\n0\nMaximum number of records to write out to a single file. If this value is zero or negative, there is no limit.\n2.2.0\nspark.sql.files.minPartitionNum\n(none)\nThe suggested (not guaranteed) minimum number of split file partitions. If not set, the default value is\nspark.sql.leafNodeDefaultParallelism\n. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.\n3.1.0\nspark.sql.function.concatBinaryAsString\nfalse\nWhen this option is set to false and all inputs are binary,\nfunctions.concat\nreturns an output as binary. Otherwise, it returns as a string.\n2.3.0\nspark.sql.function.eltOutputAsString\nfalse\nWhen this option is set to false and all inputs are binary,", "question": "What is the default value for spark.sql.files.minPartitionNum if not set?", "answers": {"text": ["spark.sql.leafNodeDefaultParallelism"], "answer_start": [362]}}
{"context": "l.hive.convertMetastoreParquet\nor\nspark.sql.hive.convertMetastoreOrc\nis true, the built-in ORC/Parquet writer is usedto process inserting into partitioned ORC/Parquet tables created by using the HiveSQL syntax.\n3.0.0\nspark.sql.hive.convertInsertingUnpartitionedTable\ntrue\nWhen set to true, and\nspark.sql.hive.convertMetastoreParquet\nor\nspark.sql.hive.convertMetastoreOrc\nis true, the built-in ORC/Parquet writer is usedto process inserting into unpartitioned ORC/Parquet tables created by using the HiveSQL syntax.\n4.0.0\nspark.sql.hive.convertMetastoreCtas\ntrue\nWhen set to true,  Spark will try to use built-in data source writer instead of Hive serde in CTAS. This flag is effective only if\nspark.sql.hive.convertMetastoreParquet\nor\nspark.sql.hive.convertMetastoreOrc\nis enabled respectively for Pa", "question": "Under what condition is the built-in ORC/Parquet writer used to process inserting into unpartitioned ORC/Parquet tables created by using the HiveSQL syntax?", "answers": {"text": ["When set to true, and\nspark.sql.hive.convertMetastoreParquet\nor\nspark.sql.hive.convertMetastoreOrc\nis true, the built-in ORC/Parquet writer is usedto process inserting into unpartitioned ORC/Parquet tables created by using the HiveSQL syntax."], "answer_start": [272]}}
{"context": "ng the HiveQL syntax, instead of Hive serde.\n2.0.0\nspark.sql.hive.convertMetastoreParquet\ntrue\nWhen set to true, the built-in Parquet reader and writer are used to process parquet tables created by using the HiveQL syntax, instead of Hive serde.\n1.1.1\nspark.sql.hive.convertMetastoreParquet.mergeSchema\nfalse\nWhen true, also tries to merge possibly different but compatible Parquet schemas in different Parquet data files. This configuration is only effective when \"spark.sql.hive.convertMetastoreParquet\" is true.\n1.3.1\nspark.sql.hive.dropPartitionByName.enabled\nfalse\nWhen true, Spark will get partition name rather than partition object to drop partition, which can improve the performance of drop partition.\n3.4.0\nspark.sql.hive.filesourcePartitionFileCacheSize\n262144000\nWhen nonzero, enable cac", "question": "What happens when 'spark.sql.hive.convertMetastoreParquet' is set to true?", "answers": {"text": ["When set to true, the built-in Parquet reader and writer are used to process parquet tables created by using the HiveQL syntax, instead of Hive serde."], "answer_start": [95]}}
{"context": "rtition, which can improve the performance of drop partition.\n3.4.0\nspark.sql.hive.filesourcePartitionFileCacheSize\n262144000\nWhen nonzero, enable caching of partition file metadata in memory. All tables share a cache that can use up to specified num bytes for file metadata. This conf only has an effect when hive filesource partition management is enabled.\n2.1.1\nspark.sql.hive.manageFilesourcePartitions\ntrue\nWhen true, enable metastore partition management for file source tables as well. This includes both datasource and converted Hive tables. When partition management is enabled, datasource tables store partition in the Hive metastore, and use the metastore to prune partitions during query planning when spark.sql.hive.metastorePartitionPruning is set to true.\n2.1.1\nspark.sql.hive.metastor", "question": "What happens when spark.sql.hive.manageFilesourcePartitions is set to true?", "answers": {"text": ["When true, enable metastore partition management for file source tables as well. This includes both datasource and converted Hive tables. When partition management is enabled, datasource tables store partition in the Hive metastore, and use the metastore to prune partitions during query planning when spark.sql.hive.metastorePartitionPruning is set to true."], "answer_start": [412]}}
{"context": "se the metastore to prune partitions during query planning when spark.sql.hive.metastorePartitionPruning is set to true.\n2.1.1\nspark.sql.hive.metastorePartitionPruning\ntrue\nWhen true, some predicates will be pushed down into the Hive metastore so that unmatching partitions can be eliminated earlier.\n1.5.0\nspark.sql.hive.metastorePartitionPruningFallbackOnException\nfalse\nWhether to fallback to get all partitions from Hive metastore and perform partition pruning on Spark client side, when encountering MetaException from the metastore. Note that Spark query performance may degrade if this is enabled and there are many partitions to be listed. If this is disabled, Spark will fail the query instead.\n3.3.0\nspark.sql.hive.metastorePartitionPruningFastFallback\nfalse\nWhen this config is enabled, if", "question": "What happens when spark.sql.hive.metastorePartitionPruning is set to true?", "answers": {"text": ["When true, some predicates will be pushed down into the Hive metastore so that unmatching partitions can be eliminated earlier."], "answer_start": [173]}}
{"context": " this is disabled, Spark will fail the query instead.\n3.3.0\nspark.sql.hive.metastorePartitionPruningFastFallback\nfalse\nWhen this config is enabled, if the predicates are not supported by Hive or Spark does fallback due to encountering MetaException from the metastore, Spark will instead prune partitions by getting the partition names first and then evaluating the filter expressions on the client side. Note that the predicates with TimeZoneAwareExpression is not supported.\n3.3.0\nspark.sql.hive.thriftServer.async\ntrue\nWhen set to true, Hive Thrift server executes SQL queries in an asynchronous way.\n1.5.0\nspark.sql.icu.caseMappings.enabled\ntrue\nWhen enabled we use the ICU library (instead of the JVM) to implement case mappings for strings under UTF8_BINARY collation.\n4.0.0\nspark.sql.inMemoryC", "question": "What happens when spark.sql.hive.metastorePartitionPruningFastFallback is enabled and the metastore encounters a MetaException?", "answers": {"text": ["Spark will instead prune partitions by getting the partition names first and then evaluating the filter expressions on the client side."], "answer_start": [269]}}
{"context": "When enabled we use the ICU library (instead of the JVM) to implement case mappings for strings under UTF8_BINARY collation.\n4.0.0\nspark.sql.inMemoryColumnarStorage.batchSize\n10000\nControls the size of batches for columnar caching.  Larger batch sizes can improve memory utilization and compression, but risk OOMs when caching data.\n1.1.1\nspark.sql.inMemoryColumnarStorage.compressed\ntrue\nWhen set to true Spark SQL will automatically select a compression codec for each column based on statistics of the data.\n1.0.1\nspark.sql.inMemoryColumnarStorage.enableVectorizedReader\ntrue\nEnables vectorized reader for columnar caching.\n2.3.1\nspark.sql.inMemoryColumnarStorage.hugeVectorReserveRatio\n1.2\nWhen spark.sql.inMemoryColumnarStorage.hugeVectorThreshold <= 0 or the required memory is smaller than spa", "question": "What does setting spark.sql.inMemoryColumnarStorage.compressed to true do?", "answers": {"text": ["When set to true Spark SQL will automatically select a compression codec for each column based on statistics of the data."], "answer_start": [389]}}
{"context": "yColumnarStorage.hugeVectorReserveRatio\n1.2\nWhen spark.sql.inMemoryColumnarStorage.hugeVectorThreshold <= 0 or the required memory is smaller than spark.sql.inMemoryColumnarStorage.hugeVectorThreshold, spark reserves required memory * 2 memory; otherwise, spark reserves required memory * this ratio memory, and will release this column vector memory before reading the next batch rows.\n4.0.0\nspark.sql.inMemoryColumnarStorage.hugeVectorThreshold\n-1b\nWhen the required memory is larger than this, spark reserves required memory * spark.sql.inMemoryColumnarStorage.hugeVectorReserveRatio memory next time and release this column vector memory before reading the next batch rows. -1 means disabling the optimization.\n4.0.0\nspark.sql.json.filterPushdown.enabled\ntrue\nWhen true, enable filter pushdown to", "question": "What happens when the required memory is larger than spark.sql.inMemoryColumnarStorage.hugeVectorThreshold?", "answers": {"text": ["When the required memory is larger than this, spark reserves required memory * spark.sql.inMemoryColumnarStorage.hugeVectorReserveRatio memory next time and release this column vector memory before reading the next batch rows."], "answer_start": [451]}}
{"context": "eading the next batch rows. -1 means disabling the optimization.\n4.0.0\nspark.sql.json.filterPushdown.enabled\ntrue\nWhen true, enable filter pushdown to JSON datasource.\n3.1.0\nspark.sql.json.useUnsafeRow\nfalse\nWhen set to true, use UnsafeRow to represent struct result in the JSON parser. It can be overwritten by the JSON option\nuseUnsafeRow\n.\n4.0.0\nspark.sql.jsonGenerator.ignoreNullFields\ntrue\nWhether to ignore null fields when generating JSON objects in JSON data source and JSON functions such as to_json. If false, it generates null for null fields in JSON objects.\n3.0.0\nspark.sql.leafNodeDefaultParallelism\n(none)\nThe default parallelism of Spark SQL leaf nodes that produce data, such as the file scan node, the local data scan node, the range node, etc. The default value of this config is '", "question": "What does setting `spark.sql.json.filterPushdown.enabled` to true do?", "answers": {"text": ["When true, enable filter pushdown to JSON datasource."], "answer_start": [114]}}
{"context": "ark SQL leaf nodes that produce data, such as the file scan node, the local data scan node, the range node, etc. The default value of this config is 'SparkContext#defaultParallelism'.\n3.2.0\nspark.sql.mapKeyDedupPolicy\nEXCEPTION\nThe policy to deduplicate map keys in builtin function: CreateMap, MapFromArrays, MapFromEntries, StringToMap, MapConcat and TransformKeys. When EXCEPTION, the query fails if duplicated map keys are detected. When LAST_WIN, the map key that is inserted at last takes precedence.\n3.0.0\nspark.sql.maven.additionalRemoteRepositories\nhttps://maven-central.storage-download.googleapis.com/maven2/\nA comma-delimited string config of the optional additional remote Maven mirror repositories. This is only used for downloading Hive jars in IsolatedClientLoader if the default Mave", "question": "What happens when duplicated map keys are detected with the policy set to EXCEPTION?", "answers": {"text": ["When EXCEPTION, the query fails if duplicated map keys are detected."], "answer_start": [368]}}
{"context": "g of the optional additional remote Maven mirror repositories. This is only used for downloading Hive jars in IsolatedClientLoader if the default Maven Central repo is unreachable.\n3.0.0\nspark.sql.maxMetadataStringLength\n100\nMaximum number of characters to output for a metadata string. e.g. file location in\nDataSourceScanExec\n, every value will be abbreviated if exceed length.\n3.1.0\nspark.sql.maxPlanStringLength\n2147483632\nMaximum number of characters to output for a plan string.  If the plan is longer, further output will be truncated.  The default setting always generates a full plan.  Set this to a lower value such as 8k if plan strings are taking up too much memory or are causing OutOfMemory errors in the driver or UI processes.\n3.0.0\nspark.sql.maxSinglePartitionBytes\n128m\nThe maximum ", "question": "What is the default value for spark.sql.maxPlanStringLength?", "answers": {"text": ["2147483632"], "answer_start": [416]}}
{"context": "e taking up too much memory or are causing OutOfMemory errors in the driver or UI processes.\n3.0.0\nspark.sql.maxSinglePartitionBytes\n128m\nThe maximum number of bytes allowed for a single partition. Otherwise, The planner will introduce shuffle to improve parallelism.\n3.4.0\nspark.sql.operatorPipeSyntaxEnabled\ntrue\nIf true, enable operator pipe syntax for Apache Spark SQL. This uses the operator pipe marker |> to indicate separation between clauses of SQL in a manner that describes the sequence of steps that the query performs in a composable fashion.\n4.0.0\nspark.sql.optimizer.avoidCollapseUDFWithExpensiveExpr\ntrue\nWhether to avoid collapsing projections that would duplicate expensive expressions in UDFs.\n4.0.0\nspark.sql.optimizer.collapseProjectAlwaysInline\nfalse\nWhether to always collapse ", "question": "What does spark.sql.maxSinglePartitionBytes control?", "answers": {"text": ["The maximum number of bytes allowed for a single partition. Otherwise, The planner will introduce shuffle to improve parallelism."], "answer_start": [138]}}
{"context": "rojections that would duplicate expensive expressions in UDFs.\n4.0.0\nspark.sql.optimizer.collapseProjectAlwaysInline\nfalse\nWhether to always collapse two adjacent projections and inline expressions even if it causes extra duplication.\n3.3.0\nspark.sql.optimizer.dynamicPartitionPruning.enabled\ntrue\nWhen true, we will generate predicate for partition column when it's used as join key\n3.0.0\nspark.sql.optimizer.enableCsvExpressionOptimization\ntrue\nWhether to optimize CSV expressions in SQL optimizer. It includes pruning unnecessary columns from from_csv.\n3.2.0\nspark.sql.optimizer.enableJsonExpressionOptimization\ntrue\nWhether to optimize JSON expressions in SQL optimizer. It includes pruning unnecessary columns from from_json, simplifying from_json + to_json, to_json + named_struct(from_json.col", "question": "What does the configuration 'spark.sql.optimizer.collapseProjectAlwaysInline' control?", "answers": {"text": ["Whether to always collapse two adjacent projections and inline expressions even if it causes extra duplication."], "answer_start": [123]}}
{"context": "ssions in SQL optimizer. It includes pruning unnecessary columns from from_json, simplifying from_json + to_json, to_json + named_struct(from_json.col1, from_json.col2, ....).\n3.1.0\nspark.sql.optimizer.excludedRules\n(none)\nConfigures a list of rules to be disabled in the optimizer, in which the rules are specified by their rule names and separated by comma. It is not guaranteed that all the rules in this configuration will eventually be excluded, as some rules are necessary for correctness. The optimizer will log the rules that have indeed been excluded.\n2.4.0\nspark.sql.optimizer.runtime.bloomFilter.applicationSideScanSizeThreshold\n10GB\nByte size threshold of the Bloom filter application side plan's aggregated scan size. Aggregated scan byte size of the Bloom filter application side needs ", "question": "What does the configuration 'spark.sql.optimizer.excludedRules' control?", "answers": {"text": ["Configures a list of rules to be disabled in the optimizer, in which the rules are specified by their rule names and separated by comma."], "answer_start": [223]}}
{"context": "size threshold of the Bloom filter application side plan's aggregated scan size. Aggregated scan byte size of the Bloom filter application side needs to be over this value to inject a bloom filter.\n3.3.0\nspark.sql.optimizer.runtime.bloomFilter.creationSideThreshold\n10MB\nSize threshold of the bloom filter creation side plan. Estimated size needs to be under this value to try to inject bloom filter.\n3.3.0\nspark.sql.optimizer.runtime.bloomFilter.enabled\ntrue\nWhen true and if one side of a shuffle join has a selective predicate, we attempt to insert a bloom filter in the other side to reduce the amount of shuffle data.\n3.3.0\nspark.sql.optimizer.runtime.bloomFilter.expectedNumItems\n1000000\nThe default number of expected items for the runtime bloomfilter\n3.3.0\nspark.sql.optimizer.runtime.bloomFi", "question": "What is the default number of expected items for the runtime bloomfilter?", "answers": {"text": ["1000000"], "answer_start": [686]}}
{"context": "untime.bloomFilter.expectedNumItems\n1000000\nThe default number of expected items for the runtime bloomfilter\n3.3.0\nspark.sql.optimizer.runtime.bloomFilter.maxNumBits\n67108864\nThe max number of bits to use for the runtime bloom filter\n3.3.0\nspark.sql.optimizer.runtime.bloomFilter.maxNumItems\n4000000\nThe max allowed number of expected items for the runtime bloom filter\n3.3.0\nspark.sql.optimizer.runtime.bloomFilter.numBits\n8388608\nThe default number of bits to use for the runtime bloom filter\n3.3.0\nspark.sql.optimizer.runtime.rowLevelOperationGroupFilter.enabled\ntrue\nEnables runtime group filtering for group-based row-level operations. Data sources that replace groups of data (e.g. files, partitions) may prune entire groups using provided data source filters when planning a row-level operatio", "question": "What is the default number of expected items for the runtime bloomfilter?", "answers": {"text": ["The default number of expected items for the runtime bloomfilter"], "answer_start": [44]}}
{"context": "ces that replace groups of data (e.g. files, partitions) may prune entire groups using provided data source filters when planning a row-level operation scan. However, such filtering is limited as not all expressions can be converted into data source filters and some expressions can only be evaluated by Spark (e.g. subqueries). Since rewriting groups is expensive, Spark can execute a query at runtime to find what records match the condition of the row-level operation. The information about matching records will be passed back to the row-level operation scan, allowing data sources to discard groups that don't have to be rewritten.\n3.4.0\nspark.sql.optimizer.runtimeFilter.number.threshold\n10\nThe total number of injected runtime filters (non-DPP) for a single query. This is to prevent driver OO", "question": "What limits the filtering capabilities when planning a row-level operation scan?", "answers": {"text": ["However, such filtering is limited as not all expressions can be converted into data source filters and some expressions can only be evaluated by Spark (e.g. subqueries)."], "answer_start": [158]}}
{"context": "ql.optimizer.runtimeFilter.number.threshold\n10\nThe total number of injected runtime filters (non-DPP) for a single query. This is to prevent driver OOMs with too many Bloom filters.\n3.3.0\nspark.sql.orc.aggregatePushdown\nfalse\nIf true, aggregates will be pushed down to ORC for optimization. Support MIN, MAX and COUNT as aggregate expression. For MIN/MAX, support boolean, integer, float and date type. For COUNT, support all data types. If statistics is missing from any ORC file footer, exception would be thrown.\n3.3.0\nspark.sql.orc.columnarReaderBatchSize\n4096\nThe number of rows to include in a orc vectorized reader batch. The number should be carefully chosen to minimize overhead and avoid OOMs in reading data.\n2.4.0\nspark.sql.orc.columnarWriterBatchSize\n1024\nThe number of rows to include i", "question": "What is the purpose of the `ql.optimizer.runtimeFilter.number.threshold` property?", "answers": {"text": ["This is to prevent driver OOMs with too many Bloom filters."], "answer_start": [122]}}
{"context": "carefully chosen to minimize overhead and avoid OOMs in reading data.\n2.4.0\nspark.sql.orc.columnarWriterBatchSize\n1024\nThe number of rows to include in a orc vectorized writer batch. The number should be carefully chosen to minimize overhead and avoid OOMs in writing data.\n3.4.0\nspark.sql.orc.compression.codec\nzstd\nSets the compression codec used when writing ORC files. If either\ncompression\nor\norc.compress\nis specified in the table-specific options/properties, the precedence would be\ncompression\n,\norc.compress\n,\nspark.sql.orc.compression.codec\n. Acceptable values include: none, uncompressed, snappy, zlib, lzo, zstd, lz4, brotli.\n2.3.0\nspark.sql.orc.enableNestedColumnVectorizedReader\ntrue\nEnables vectorized orc decoding for nested column.\n3.2.0\nspark.sql.orc.enableVectorizedReader\ntrue\nEna", "question": "What are the acceptable values for spark.sql.orc.compression.codec?", "answers": {"text": ["none, uncompressed, snappy, zlib, lzo, zstd, lz4, brotli."], "answer_start": [580]}}
{"context": "sql.orc.enableNestedColumnVectorizedReader\ntrue\nEnables vectorized orc decoding for nested column.\n3.2.0\nspark.sql.orc.enableVectorizedReader\ntrue\nEnables vectorized orc decoding.\n2.3.0\nspark.sql.orc.filterPushdown\ntrue\nWhen true, enable filter pushdown for ORC files.\n1.4.0\nspark.sql.orc.mergeSchema\nfalse\nWhen true, the Orc data source merges schemas collected from all data files, otherwise the schema is picked from a random data file.\n3.0.0\nspark.sql.orderByOrdinal\ntrue\nWhen true, the ordinal numbers are treated as the position in the select list. When false, the ordinal numbers in order/sort by clause are ignored.\n2.0.0\nspark.sql.parquet.aggregatePushdown\nfalse\nIf true, aggregates will be pushed down to Parquet for optimization. Support MIN, MAX and COUNT as aggregate expression. For MIN", "question": "What does `spark.sql.orc.filterPushdown` do when set to true?", "answers": {"text": ["When true, enable filter pushdown for ORC files."], "answer_start": [220]}}
{"context": "gregatePushdown\nfalse\nIf true, aggregates will be pushed down to Parquet for optimization. Support MIN, MAX and COUNT as aggregate expression. For MIN/MAX, support boolean, integer, float and date type. For COUNT, support all data types. If statistics is missing from any Parquet file footer, exception would be thrown.\n3.3.0\nspark.sql.parquet.binaryAsString\nfalse\nSome other Parquet-producing systems, in particular Impala and older versions of Spark SQL, do not differentiate between binary data and strings when writing out the Parquet schema. This flag tells Spark SQL to interpret binary data as a string to provide compatibility with these systems.\n1.1.1\nspark.sql.parquet.columnarReaderBatchSize\n4096\nThe number of rows to include in a parquet vectorized reader batch. The number should be car", "question": "What aggregate expressions are supported when pushing down aggregates to Parquet for optimization?", "answers": {"text": ["Support MIN, MAX and COUNT as aggregate expression."], "answer_start": [91]}}
{"context": "ems.\n1.1.1\nspark.sql.parquet.columnarReaderBatchSize\n4096\nThe number of rows to include in a parquet vectorized reader batch. The number should be carefully chosen to minimize overhead and avoid OOMs in reading data.\n2.4.0\nspark.sql.parquet.compression.codec\nsnappy\nSets the compression codec used when writing Parquet files. If either\ncompression\nor\nparquet.compression\nis specified in the table-specific options/properties, the precedence would be\ncompression\n,\nparquet.compression\n,\nspark.sql.parquet.compression.codec\n. Acceptable values include: none, uncompressed, snappy, gzip, lzo, brotli, lz4, lz4_raw, zstd.\n1.1.1\nspark.sql.parquet.enableNestedColumnVectorizedReader\ntrue\nEnables vectorized Parquet decoding for nested columns (e.g., struct, list, map). Requires spark.sql.parquet.enableVec", "question": "What are the acceptable values for the `spark.sql.parquet.compression.codec` property?", "answers": {"text": ["none, uncompressed, snappy, gzip, lzo, brotli, lz4, lz4_raw, zstd."], "answer_start": [551]}}
{"context": "stedColumnVectorizedReader\ntrue\nEnables vectorized Parquet decoding for nested columns (e.g., struct, list, map). Requires spark.sql.parquet.enableVectorizedReader to be enabled.\n3.3.0\nspark.sql.parquet.enableVectorizedReader\ntrue\nEnables vectorized parquet decoding.\n2.0.0\nspark.sql.parquet.fieldId.read.enabled\nfalse\nField ID is a native field of the Parquet schema spec. When enabled, Parquet readers will use field IDs (if present) in the requested Spark schema to look up Parquet fields instead of using column names\n3.3.0\nspark.sql.parquet.fieldId.read.ignoreMissing\nfalse\nWhen the Parquet file doesn't have any field IDs but the Spark read schema is using field IDs to read, we will silently return nulls when this flag is enabled, or error otherwise.\n3.3.0\nspark.sql.parquet.fieldId.write.ena", "question": "What does `spark.sql.parquet.enableVectorizedReader` do?", "answers": {"text": ["Enables vectorized parquet decoding."], "answer_start": [231]}}
{"context": "ema is using field IDs to read, we will silently return nulls when this flag is enabled, or error otherwise.\n3.3.0\nspark.sql.parquet.fieldId.write.enabled\ntrue\nField ID is a native field of the Parquet schema spec. When enabled, Parquet writers will populate the field Id metadata (if present) in the Spark schema to the Parquet schema.\n3.3.0\nspark.sql.parquet.filterPushdown\ntrue\nEnables Parquet filter push-down optimization when set to true.\n1.2.0\nspark.sql.parquet.inferTimestampNTZ.enabled\ntrue\nWhen enabled, Parquet timestamp columns with annotation isAdjustedToUTC = false are inferred as TIMESTAMP_NTZ type during schema inference. Otherwise, all the Parquet timestamp columns are inferred as TIMESTAMP_LTZ types. Note that Spark writes the output schema into Parquet's footer metadata on fil", "question": "What happens when the `spark.sql.parquet.fieldId.write.enabled` flag is enabled?", "answers": {"text": ["When enabled, Parquet writers will populate the field Id metadata (if present) in the Spark schema to the Parquet schema."], "answer_start": [215]}}
{"context": " all the Parquet timestamp columns are inferred as TIMESTAMP_LTZ types. Note that Spark writes the output schema into Parquet's footer metadata on file writing and leverages it on file reading. Thus this configuration only affects the schema inference on Parquet files which are not written by Spark.\n3.4.0\nspark.sql.parquet.int96AsTimestamp\ntrue\nSome Parquet-producing systems, in particular Impala, store Timestamp into INT96. Spark would also store Timestamp as INT96 because we need to avoid precision lost of the nanoseconds field. This flag tells Spark SQL to interpret INT96 data as a timestamp to provide compatibility with these systems.\n1.3.0\nspark.sql.parquet.int96TimestampConversion\nfalse\nThis controls whether timestamp adjustments should be applied to INT96 data when converting to tim", "question": "What does the flag `spark.sql.parquet.int96AsTimestamp` tell Spark SQL to do?", "answers": {"text": ["This flag tells Spark SQL to interpret INT96 data as a timestamp to provide compatibility with these systems."], "answer_start": [537]}}
{"context": ".0\nspark.sql.parquet.int96TimestampConversion\nfalse\nThis controls whether timestamp adjustments should be applied to INT96 data when converting to timestamps, for data written by Impala.  This is necessary because Impala stores INT96 data with a different timezone offset than Hive & Spark.\n2.3.0\nspark.sql.parquet.mergeSchema\nfalse\nWhen true, the Parquet data source merges schemas collected from all data files, otherwise the schema is picked from the summary file or a random data file if no summary file is available.\n1.5.0\nspark.sql.parquet.outputTimestampType\nINT96\nSets which Parquet timestamp type to use when Spark writes data to Parquet files. INT96 is a non-standard but commonly used timestamp type in Parquet. TIMESTAMP_MICROS is a standard timestamp type in Parquet, which stores number", "question": "What does spark.sql.parquet.outputTimestampType control?", "answers": {"text": ["Sets which Parquet timestamp type to use when Spark writes data to Parquet files. INT96 is a non-standard but commonly used timestamp type in Parquet. TIMESTAMP_MICROS is a standard timestamp type in Parquet, which stores number"], "answer_start": [572]}}
{"context": "es. INT96 is a non-standard but commonly used timestamp type in Parquet. TIMESTAMP_MICROS is a standard timestamp type in Parquet, which stores number of microseconds from the Unix epoch. TIMESTAMP_MILLIS is also standard, but with millisecond precision, which means Spark has to truncate the microsecond portion of its timestamp value.\n2.3.0\nspark.sql.parquet.recordLevelFilter.enabled\nfalse\nIf true, enables Parquet's native record-level filtering using the pushed down filters. This configuration only has an effect when 'spark.sql.parquet.filterPushdown' is enabled and the vectorized reader is not used. You can ensure the vectorized reader is not used by setting 'spark.sql.parquet.enableVectorizedReader' to false.\n2.3.0\nspark.sql.parquet.respectSummaryFiles\nfalse\nWhen true, we make assumptio", "question": "What does 'spark.sql.parquet.recordLevelFilter.enabled' do when set to true?", "answers": {"text": ["If true, enables Parquet's native record-level filtering using the pushed down filters."], "answer_start": [393]}}
{"context": "ot used by setting 'spark.sql.parquet.enableVectorizedReader' to false.\n2.3.0\nspark.sql.parquet.respectSummaryFiles\nfalse\nWhen true, we make assumption that all part-files of Parquet are consistent with summary files and we will ignore them when merging schema. Otherwise, if this is false, which is the default, we will merge all part-files. This should be considered as expert-only option, and shouldn't be enabled before knowing what it means exactly.\n1.5.0\nspark.sql.parquet.writeLegacyFormat\nfalse\nIf true, data will be written in a way of Spark 1.4 and earlier. For example, decimal values will be written in Apache Parquet's fixed-length byte array format, which other systems such as Apache Hive and Apache Impala use. If false, the newer format in Parquet will be used. For example, decimals", "question": "What happens when 'spark.sql.parquet.respectSummaryFiles' is set to false?", "answers": {"text": ["Otherwise, if this is false, which is the default, we will merge all part-files."], "answer_start": [262]}}
{"context": "array format, which other systems such as Apache Hive and Apache Impala use. If false, the newer format in Parquet will be used. For example, decimals will be written in int-based format. If Parquet output is intended for use with systems that do not support this newer format, set to true.\n1.6.0\nspark.sql.parser.quotedRegexColumnNames\nfalse\nWhen true, quoted Identifiers (using backticks) in SELECT statement are interpreted as regular expressions.\n2.3.0\nspark.sql.pivotMaxValues\n10000\nWhen doing a pivot without specifying values for the pivot column this is the maximum number of (distinct) values that will be collected without error.\n1.6.0\nspark.sql.planner.pythonExecution.memory\n(none)\nSpecifies the memory allocation for executing Python code in Spark driver, in MiB. When set, it caps the m", "question": "What happens when spark.sql.parser.quotedRegexColumnNames is set to true?", "answers": {"text": ["When true, quoted Identifiers (using backticks) in SELECT statement are interpreted as regular expressions."], "answer_start": [343]}}
{"context": "k.sql.planner.pythonExecution.memory\n(none)\nSpecifies the memory allocation for executing Python code in Spark driver, in MiB. When set, it caps the memory for Python execution to the specified amount. If not set, Spark will not limit Python's memory usage and it is up to the application to avoid exceeding the overhead memory space shared with other non-JVM processes.\nNote: Windows does not support resource limiting and actual resource is not limited on MacOS.\n4.0.0\nspark.sql.preserveCharVarcharTypeInfo\nfalse\nWhen true, Spark does not replace CHAR/VARCHAR types the STRING type, which is the default behavior of Spark 3.0 and earlier versions. This means the length checks for CHAR/VARCHAR types is enforced and CHAR type is also properly padded.\n4.0.0\nspark.sql.pyspark.inferNestedDictAsStruct", "question": "What happens when the 'k.sql.planner.pythonExecution.memory' property is not set?", "answers": {"text": ["If not set, Spark will not limit Python's memory usage and it is up to the application to avoid exceeding the overhead memory space shared with other non-JVM processes."], "answer_start": [202]}}
{"context": "This means the length checks for CHAR/VARCHAR types is enforced and CHAR type is also properly padded.\n4.0.0\nspark.sql.pyspark.inferNestedDictAsStruct.enabled\nfalse\nPySpark's SparkSession.createDataFrame infers the nested dict as a map by default. When it set to true, it infers the nested dict as a struct.\n3.3.0\nspark.sql.pyspark.jvmStacktrace.enabled\nfalse\nWhen true, it shows the JVM stacktrace in the user-facing PySpark exception together with Python stacktrace. By default, it is disabled to hide JVM stacktrace and shows a Python-friendly exception only. Note that this is independent from log level settings.\n3.0.0\nspark.sql.pyspark.plotting.max_rows\n1000\nThe visual limit on plots. If set to 1000 for top-n-based plots (pie, bar, barh), the first 1000 data points will be used for plotting.", "question": "What does setting 'spark.sql.pyspark.inferNestedDictAsStruct.enabled' to true do?", "answers": {"text": ["When it set to true, it infers the nested dict as a struct."], "answer_start": [248]}}
{"context": ".max_rows\n1000\nThe visual limit on plots. If set to 1000 for top-n-based plots (pie, bar, barh), the first 1000 data points will be used for plotting. For sampled-based plots (scatter, area, line), 1000 data points will be randomly sampled.\n4.0.0\nspark.sql.pyspark.udf.profiler\n(none)\nConfigure the Python/Pandas UDF profiler by enabling or disabling it with the option to choose between \"perf\" and \"memory\" types, or unsetting the config disables the profiler. This is disabled by default.\n4.0.0\nspark.sql.readSideCharPadding\ntrue\nWhen true, Spark applies string padding when reading CHAR type columns/fields, in addition to the write-side padding. This config is true by default to better enforce CHAR type semantic in cases such as external tables.\n3.4.0\nspark.sql.redaction.options.regex\n(?i)url\n", "question": "What is the default behavior of the Python/Pandas UDF profiler?", "answers": {"text": ["This is disabled by default."], "answer_start": [462]}}
{"context": "This config is true by default to better enforce CHAR type semantic in cases such as external tables.\n3.4.0\nspark.sql.redaction.options.regex\n(?i)url\nRegex to decide which keys in a Spark SQL command's options map contain sensitive information. The values of options whose names that match this regex will be redacted in the explain output. This redaction is applied on top of the global redaction configuration defined by spark.redaction.regex.\n2.2.2\nspark.sql.redaction.string.regex\n(value of\nspark.redaction.string.regex\n)\nRegex to decide which parts of strings produced by Spark contain sensitive information. When this regex matches a string part, that string part is replaced by a dummy value. This is currently used to redact the output of SQL explain commands. When this conf is not set, the ", "question": "What is the purpose of spark.sql.redaction.options.regex?", "answers": {"text": ["Regex to decide which keys in a Spark SQL command's options map contain sensitive information."], "answer_start": [150]}}
{"context": "t, that string part is replaced by a dummy value. This is currently used to redact the output of SQL explain commands. When this conf is not set, the value from\nspark.redaction.string.regex\nis used.\n2.3.0\nspark.sql.repl.eagerEval.enabled\nfalse\nEnables eager evaluation or not. When true, the top K rows of Dataset will be displayed if and only if the REPL supports the eager evaluation. Currently, the eager evaluation is supported in PySpark and SparkR. In PySpark, for the notebooks like Jupyter, the HTML table (generated by\nrepr_html\n) will be returned. For plain Python REPL, the returned outputs are formatted like dataframe.show(). In SparkR, the returned outputs are showed similar to R data.frame would.\n2.4.0\nspark.sql.repl.eagerEval.maxNumRows\n20\nThe max number of rows that are returned b", "question": "What happens when spark.sql.repl.eagerEval.enabled is set to true?", "answers": {"text": ["When true, the top K rows of Dataset will be displayed if and only if the REPL supports the eager evaluation."], "answer_start": [277]}}
{"context": "the returned outputs are showed similar to R data.frame would.\n2.4.0\nspark.sql.repl.eagerEval.maxNumRows\n20\nThe max number of rows that are returned by eager evaluation. This only takes effect when spark.sql.repl.eagerEval.enabled is set to true. The valid range of this config is from 0 to (Int.MaxValue - 1), so the invalid config like negative and greater than (Int.MaxValue - 1) will be normalized to 0 and (Int.MaxValue - 1).\n2.4.0\nspark.sql.repl.eagerEval.truncate\n20\nThe max number of characters for each cell that is returned by eager evaluation. This only takes effect when spark.sql.repl.eagerEval.enabled is set to true.\n2.4.0\nspark.sql.scripting.enabled\nfalse\nSQL Scripting feature is under development and its use should be done under this feature flag. SQL Scripting enables users to wr", "question": "What is the valid range for the spark.sql.repl.eagerEval.maxNumRows configuration?", "answers": {"text": ["The valid range of this config is from 0 to (Int.MaxValue - 1)"], "answer_start": [247]}}
{"context": "ripting.enabled\nfalse\nSQL Scripting feature is under development and its use should be done under this feature flag. SQL Scripting enables users to write procedural SQL including control flow and error handling.\n4.0.0\nspark.sql.session.localRelationCacheThreshold\n67108864\nThe threshold for the size in bytes of local relations to be cached at the driver side after serialization.\n3.5.0\nspark.sql.session.timeZone\n(value of local timezone)\nThe ID of session local timezone in the format of either region-based zone IDs or zone offsets. Region IDs must have the form 'area/city', such as 'America/Los_Angeles'. Zone offsets must be in the format '(+|-)HH', '(+|-)HH:mm' or '(+|-)HH:mm:ss', e.g '-08', '+01:00' or '-13:33:33'. Also 'UTC' and 'Z' are supported as aliases of '+00:00'. Other short names ", "question": "What is the purpose of the spark.sql.session.localRelationCacheThreshold configuration?", "answers": {"text": ["The threshold for the size in bytes of local relations to be cached at the driver side after serialization."], "answer_start": [273]}}
{"context": ")HH', '(+|-)HH:mm' or '(+|-)HH:mm:ss', e.g '-08', '+01:00' or '-13:33:33'. Also 'UTC' and 'Z' are supported as aliases of '+00:00'. Other short names are not recommended to use because they can be ambiguous.\n2.2.0\nspark.sql.shuffle.partitions\n200\nThe default number of partitions to use when shuffling data for joins or aggregations. Note: For structured streaming, this configuration cannot be changed between query restarts from the same checkpoint location.\n1.1.0\nspark.sql.shuffleDependency.fileCleanup.enabled\nfalse\nWhen enabled, shuffle files will be cleaned up at the end of Spark Connect SQL executions.\n4.0.0\nspark.sql.shuffleDependency.skipMigration.enabled\nfalse\nWhen enabled, shuffle dependencies for a Spark Connect SQL execution are marked at the end of the execution, and they will not", "question": "What is the default number of partitions used when shuffling data for joins or aggregations in Spark SQL?", "answers": {"text": ["200"], "answer_start": [243]}}
{"context": "Migration.enabled\nfalse\nWhen enabled, shuffle dependencies for a Spark Connect SQL execution are marked at the end of the execution, and they will not be migrated during decommissions.\n4.0.0\nspark.sql.shuffledHashJoinFactor\n3\nThe shuffle hash join can be selected if the data size of small side multiplied by this factor is still smaller than the large side.\n3.3.0\nspark.sql.sources.bucketing.autoBucketedScan.enabled\ntrue\nWhen true, decide whether to do bucketed scan on input tables based on query plan automatically. Do not use bucketed scan if 1. query does not have operators to utilize bucketing (e.g. join, group-by, etc), or 2. there's an exchange operator between these operators and table scan. Note when 'spark.sql.sources.bucketing.enabled' is set to false, this configuration does not ta", "question": "What happens when 'Migration.enabled' is set to true?", "answers": {"text": ["When enabled, shuffle dependencies for a Spark Connect SQL execution are marked at the end of the execution, and they will not be migrated during decommissions."], "answer_start": [24]}}
{"context": "hange operator between these operators and table scan. Note when 'spark.sql.sources.bucketing.enabled' is set to false, this configuration does not take any effect.\n3.1.0\nspark.sql.sources.bucketing.enabled\ntrue\nWhen false, we will treat bucketed table as normal table\n2.0.0\nspark.sql.sources.bucketing.maxBuckets\n100000\nThe maximum number of buckets allowed.\n2.4.0\nspark.sql.sources.default\nparquet\nThe default data source to use in input/output.\n1.3.0\nspark.sql.sources.parallelPartitionDiscovery.threshold\n32\nThe maximum number of paths allowed for listing files at driver side. If the number of detected paths exceeds this value during partition discovery, it tries to list the files with another Spark distributed job. This configuration is effective only when using file-based sources such as P", "question": "What happens when 'spark.sql.sources.bucketing.enabled' is set to false?", "answers": {"text": ["When false, we will treat bucketed table as normal table"], "answer_start": [212]}}
{"context": "discovery, it tries to list the files with another Spark distributed job. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.\n1.5.0\nspark.sql.sources.partitionColumnTypeInference.enabled\ntrue\nWhen true, automatically infer the data types for partitioned columns.\n1.5.0\nspark.sql.sources.partitionOverwriteMode\nSTATIC\nWhen INSERT OVERWRITE a partitioned data source table, we currently support 2 modes: static and dynamic. In static mode, Spark deletes all the partitions that match the partition specification(e.g. PARTITION(a=1,b)) in the INSERT statement, before overwriting. In dynamic mode, Spark doesn't delete partitions ahead, and only overwrite those partitions that have data written into it at runtime. By default we use static mode to keep the", "question": "What are the two supported modes when INSERT OVERWRITE a partitioned data source table?", "answers": {"text": ["static and dynamic"], "answer_start": [448]}}
{"context": "sn't delete partitions ahead, and only overwrite those partitions that have data written into it at runtime. By default we use static mode to keep the same behavior of Spark prior to 2.3. Note that this config doesn't affect Hive serde tables, as they are always overwritten with dynamic mode. This can also be set as an output option for a data source using key partitionOverwriteMode (which takes precedence over this setting), e.g. dataframe.write.option(\"partitionOverwriteMode\", \"dynamic\").save(path).\n2.3.0\nspark.sql.sources.v2.bucketing.allowCompatibleTransforms.enabled\nfalse\nWhether to allow storage-partition join in the case where the partition transforms are compatible but not identical.  This config requires both spark.sql.sources.v2.bucketing.enabled and spark.sql.sources.v2.bucketin", "question": "What happens when the key partitionOverwriteMode is used as an output option for a data source?", "answers": {"text": ["This can also be set as an output option for a data source using key partitionOverwriteMode (which takes precedence over this setting), e.g. dataframe.write.option(\"partitionOverwriteMode\", \"dynamic\").save(path)."], "answer_start": [294]}}
{"context": "ition transforms are compatible but not identical.  This config requires both spark.sql.sources.v2.bucketing.enabled and spark.sql.sources.v2.bucketing.pushPartValues.enabled to be enabled and spark.sql.sources.v2.bucketing.partiallyClusteredDistribution.enabled to be disabled.\n4.0.0\nspark.sql.sources.v2.bucketing.allowJoinKeysSubsetOfPartitionKeys.enabled\nfalse\nWhether to allow storage-partition join in the case where join keys are a subset of the partition keys of the source tables. At planning time, Spark will group the partitions by only those keys that are in the join keys. This is currently enabled only if spark.sql.requireAllClusterKeysForDistribution is false.\n4.0.0\nspark.sql.sources.v2.bucketing.enabled\nfalse\nSimilar to spark.sql.sources.bucketing.enabled, this config is used to e", "question": "What does spark.sql.sources.v2.bucketing.enabled control?", "answers": {"text": ["Similar to spark.sql.sources.bucketing.enabled, this config is used to e"], "answer_start": [728]}}
{"context": "sForDistribution is false.\n4.0.0\nspark.sql.sources.v2.bucketing.enabled\nfalse\nSimilar to spark.sql.sources.bucketing.enabled, this config is used to enable bucketing for V2 data sources. When turned on, Spark will recognize the specific distribution reported by a V2 data source through SupportsReportPartitioning, and will try to avoid shuffle if necessary.\n3.3.0\nspark.sql.sources.v2.bucketing.partiallyClusteredDistribution.enabled\nfalse\nDuring a storage-partitioned join, whether to allow input partitions to be partially clustered, when both sides of the join are of KeyGroupedPartitioning. At planning time, Spark will pick the side with less data size based on table statistics, group and replicate them to match the other side. This is an optimization on skew join and can help to reduce data", "question": "What does spark.sql.sources.v2.bucketing.enabled configure?", "answers": {"text": ["this config is used to enable bucketing for V2 data sources. When turned on, Spark will recognize the specific distribution reported by a V2 data source through SupportsReportPartitioning, and will try to avoid shuffle if necessary."], "answer_start": [126]}}
{"context": "ata size based on table statistics, group and replicate them to match the other side. This is an optimization on skew join and can help to reduce data skewness when certain partitions are assigned large amount of data. This config requires both spark.sql.sources.v2.bucketing.enabled and spark.sql.sources.v2.bucketing.pushPartValues.enabled to be enabled\n3.4.0\nspark.sql.sources.v2.bucketing.partition.filter.enabled\nfalse\nWhether to filter partitions when running storage-partition join. When enabled, partitions without matches on the other side can be omitted for scanning, if allowed by the join type. This config requires both spark.sql.sources.v2.bucketing.enabled and spark.sql.sources.v2.bucketing.pushPartValues.enabled to be enabled.\n4.0.0\nspark.sql.sources.v2.bucketing.pushPartValues.ena", "question": "What is required for the spark.sql.sources.v2.bucketing.partition.filter.enabled config to function?", "answers": {"text": ["This config requires both spark.sql.sources.v2.bucketing.enabled and spark.sql.sources.v2.bucketing.pushPartValues.enabled to be enabled"], "answer_start": [219]}}
{"context": ".v2.bucketing.enabled and spark.sql.sources.v2.bucketing.pushPartValues.enabled to be enabled.\n4.0.0\nspark.sql.sources.v2.bucketing.pushPartValues.enabled\ntrue\nWhether to pushdown common partition values when spark.sql.sources.v2.bucketing.enabled is enabled. When turned on, if both sides of a join are of KeyGroupedPartitioning and if they share compatible partition keys, even if they don't have the exact same partition values, Spark will calculate a superset of partition values and pushdown that info to scan nodes, which will use empty partitions for the missing partition values on either side. This could help to eliminate unnecessary shuffles\n3.4.0\nspark.sql.sources.v2.bucketing.shuffle.enabled\nfalse\nDuring a storage-partitioned join, whether to allow to shuffle only one side. When only ", "question": "What happens when spark.sql.sources.v2.bucketing.pushPartValues.enabled is turned on and both sides of a join are of KeyGroupedPartitioning with compatible partition keys?", "answers": {"text": ["Spark will calculate a superset of partition values and pushdown that info to scan nodes, which will use empty partitions for the missing partition values on either side."], "answer_start": [432]}}
{"context": "es\n3.4.0\nspark.sql.sources.v2.bucketing.shuffle.enabled\nfalse\nDuring a storage-partitioned join, whether to allow to shuffle only one side. When only one side is KeyGroupedPartitioning, if the conditions are met, spark will only shuffle the other side. This optimization will reduce the amount of data that needs to be shuffle. This config requires spark.sql.sources.v2.bucketing.enabled to be enabled\n4.0.0\nspark.sql.sources.v2.bucketing.sorting.enabled\nfalse\nWhen turned on, Spark will recognize the specific distribution reported by a V2 data source through SupportsReportPartitioning, and will try to avoid a shuffle if possible when sorting by those columns. This config requires spark.sql.sources.v2.bucketing.enabled to be enabled.\n4.0.0\nspark.sql.stackTracesInDataFrameContext\n1\nThe number of", "question": "What is the purpose of the configuration 'spark.sql.sources.v2.bucketing.shuffle.enabled'?", "answers": {"text": ["During a storage-partitioned join, whether to allow to shuffle only one side."], "answer_start": [62]}}
{"context": "hose columns. This config requires spark.sql.sources.v2.bucketing.enabled to be enabled.\n4.0.0\nspark.sql.stackTracesInDataFrameContext\n1\nThe number of non-Spark stack traces in the captured DataFrame query context.\n4.0.0\nspark.sql.statistics.fallBackToHdfs\nfalse\nWhen true, it will fall back to HDFS if the table statistics are not available from table metadata. This is useful in determining if a table is small enough to use broadcast joins. This flag is effective only for non-partitioned Hive tables. For non-partitioned data source tables, it will be automatically recalculated if table statistics are not available. For partitioned data source and partitioned Hive tables, It is 'spark.sql.defaultSizeInBytes' if table statistics are not available.\n2.0.0\nspark.sql.statistics.histogram.enabled\n", "question": "What happens when spark.sql.statistics.fallBackToHdfs is set to true?", "answers": {"text": ["When true, it will fall back to HDFS if the table statistics are not available from table metadata."], "answer_start": [263]}}
{"context": "and partitioned Hive tables, It is 'spark.sql.defaultSizeInBytes' if table statistics are not available.\n2.0.0\nspark.sql.statistics.histogram.enabled\nfalse\nGenerates histograms when computing column statistics if enabled. Histograms can provide better estimation accuracy. Currently, Spark only supports equi-height histogram. Note that collecting histograms takes extra cost. For example, collecting column statistics usually takes only one table scan, but generating equi-height histogram will cause an extra table scan.\n2.3.0\nspark.sql.statistics.size.autoUpdate.enabled\nfalse\nEnables automatic update for table size once table's data is changed. Note that if the total number of files of the table is very large, this can be expensive and slow down data change commands.\n2.3.0\nspark.sql.statistic", "question": "What does 'spark.sql.statistics.histogram.enabled' control?", "answers": {"text": ["Generates histograms when computing column statistics if enabled."], "answer_start": [156]}}
{"context": "Note that if the total number of files of the table is very large, this can be expensive and slow down data change commands.\n2.3.0\nspark.sql.statistics.updatePartitionStatsInAnalyzeTable.enabled\nfalse\nWhen this config is enabled, Spark will also update partition statistics in analyze table command (i.e., ANALYZE TABLE .. COMPUTE STATISTICS [NOSCAN]). Note the command will also become more expensive. When this config is disabled, Spark will only update table level statistics.\n4.0.0\nspark.sql.storeAssignmentPolicy\nANSI\nWhen inserting a value into a column with different data type, Spark will perform type coercion. Currently, we support 3 policies for the type coercion rules: ANSI, legacy and strict. With ANSI policy, Spark performs the type coercion as per ANSI SQL. In practice, the behavior", "question": "What happens when the 'spark.sql.statistics.updatePartitionStatsInAnalyzeTable.enabled' config is enabled?", "answers": {"text": ["When this config is enabled, Spark will also update partition statistics in analyze table command (i.e., ANALYZE TABLE .. COMPUTE STATISTICS [NOSCAN])."], "answer_start": [201]}}
{"context": "ing\ndouble\nto\nint\nor\ndecimal\nto\ndouble\nis not allowed.\n3.0.0\nspark.sql.streaming.checkpointLocation\n(none)\nThe default location for storing checkpoint data for streaming queries.\n2.0.0\nspark.sql.streaming.continuous.epochBacklogQueueSize\n10000\nThe max number of entries to be stored in queue to wait for late epochs. If this parameter is exceeded by the size of the queue, stream will stop with an error.\n3.0.0\nspark.sql.streaming.disabledV2Writers\nA comma-separated list of fully qualified data source register class names for which StreamWriteSupport is disabled. Writes to these sources will fall back to the V1 Sinks.\n2.3.1\nspark.sql.streaming.fileSource.cleaner.numThreads\n1\nNumber of threads used in the file source completed file cleaner.\n3.0.0\nspark.sql.streaming.forceDeleteTempCheckpointLoc", "question": "What is the default location for storing checkpoint data for streaming queries?", "answers": {"text": ["(none)\nThe default location for storing checkpoint data for streaming queries."], "answer_start": [100]}}
{"context": "leSource.cleaner.numThreads\n1\nNumber of threads used in the file source completed file cleaner.\n3.0.0\nspark.sql.streaming.forceDeleteTempCheckpointLocation\nfalse\nWhen true, enable temporary checkpoint locations force delete.\n3.0.0\nspark.sql.streaming.metricsEnabled\nfalse\nWhether Dropwizard/Codahale metrics will be reported for active streaming queries.\n2.0.2\nspark.sql.streaming.multipleWatermarkPolicy\nmin\nPolicy to calculate the global watermark value when there are multiple watermark operators in a streaming query. The default value is 'min' which chooses the minimum watermark reported across multiple operators. Other alternative value is 'max' which chooses the maximum across multiple operators. Note: This configuration cannot be changed between query restarts from the same checkpoint lo", "question": "What is the default value for the policy to calculate the global watermark value when there are multiple watermark operators in a streaming query?", "answers": {"text": ["The default value is 'min' which chooses the minimum watermark reported across multiple operators."], "answer_start": [522]}}
{"context": "ax' which chooses the maximum across multiple operators. Note: This configuration cannot be changed between query restarts from the same checkpoint location.\n2.4.0\nspark.sql.streaming.noDataMicroBatches.enabled\ntrue\nWhether streaming micro-batch engine will execute batches without data for eager state management for stateful streaming queries.\n2.4.1\nspark.sql.streaming.numRecentProgressUpdates\n100\nThe number of progress updates to retain for a streaming query\n2.1.1\nspark.sql.streaming.sessionWindow.merge.sessions.in.local.partition\nfalse\nWhen true, streaming session window sorts and merge sessions in local partition prior to shuffle. This is to reduce the rows to shuffle, but only beneficial when there're lots of rows in a batch being assigned to same sessions.\n3.2.0\nspark.sql.streaming.st", "question": "What does spark.sql.streaming.noDataMicroBatches.enabled control?", "answers": {"text": ["Whether streaming micro-batch engine will execute batches without data for eager state management for stateful streaming queries."], "answer_start": [216]}}
{"context": "to reduce the rows to shuffle, but only beneficial when there're lots of rows in a batch being assigned to same sessions.\n3.2.0\nspark.sql.streaming.stateStore.encodingFormat\nunsaferow\nThe encoding format used for stateful operators to store information in the state store\n4.0.0\nspark.sql.streaming.stateStore.stateSchemaCheck\ntrue\nWhen true, Spark will validate the state schema against schema on existing state and fail query if it's incompatible.\n3.1.0\nspark.sql.streaming.stopActiveRunOnRestart\ntrue\nRunning multiple runs of the same streaming query concurrently is not supported. If we find a concurrent active run for a streaming query (in the same or different SparkSessions on the same cluster) and this flag is true, we will stop the old streaming query run to start the new one.\n3.0.0\nspark.", "question": "What does the 'spark.sql.streaming.stateStore.stateSchemaCheck' configuration option do?", "answers": {"text": ["When true, Spark will validate the state schema against schema on existing state and fail query if it's incompatible."], "answer_start": [331]}}
{"context": "ame or different SparkSessions on the same cluster) and this flag is true, we will stop the old streaming query run to start the new one.\n3.0.0\nspark.sql.streaming.stopTimeout\n0\nHow long to wait in milliseconds for the streaming execution thread to stop when calling the streaming query's stop() method. 0 or negative values wait indefinitely.\n3.0.0\nspark.sql.streaming.transformWithState.stateSchemaVersion\n3\nThe version of the state schema used by the transformWithState operator\n4.0.0\nspark.sql.thriftServer.interruptOnCancel\n(value of\nspark.sql.execution.interruptOnCancel\n)\nWhen true, all running tasks will be interrupted if one cancels a query. When false, all running tasks will remain until finished.\n3.2.0\nspark.sql.thriftServer.queryTimeout\n0ms\nSet a query duration timeout in seconds in T", "question": "What happens when spark.sql.streaming.stopTimeout is set to 0 or a negative value?", "answers": {"text": ["0 or negative values wait indefinitely."], "answer_start": [304]}}
{"context": ". When false, all running tasks will remain until finished.\n3.2.0\nspark.sql.thriftServer.queryTimeout\n0ms\nSet a query duration timeout in seconds in Thrift Server. If the timeout is set to a positive value, a running query will be cancelled automatically when the timeout is exceeded, otherwise the query continues to run till completion. If timeout values are set for each statement via\njava.sql.Statement.setQueryTimeout\nand they are smaller than this configuration value, they take precedence. If you set this timeout and prefer to cancel the queries right away without waiting task to finish, consider enabling spark.sql.thriftServer.interruptOnCancel together.\n3.1.0\nspark.sql.thriftserver.scheduler.pool\n(none)\nSet a Fair Scheduler pool for a JDBC client session.\n1.1.1\nspark.sql.thriftserver.u", "question": "What happens if the query timeout in Thrift Server is set to a positive value?", "answers": {"text": ["If the timeout is set to a positive value, a running query will be cancelled automatically when the timeout is exceeded, otherwise the query continues to run till completion."], "answer_start": [164]}}
{"context": "ancel together.\n3.1.0\nspark.sql.thriftserver.scheduler.pool\n(none)\nSet a Fair Scheduler pool for a JDBC client session.\n1.1.1\nspark.sql.thriftserver.ui.retainedSessions\n200\nThe number of SQL client sessions kept in the JDBC/ODBC web UI history.\n1.4.0\nspark.sql.thriftserver.ui.retainedStatements\n200\nThe number of SQL statements kept in the JDBC/ODBC web UI history.\n1.4.0\nspark.sql.timeTravelTimestampKey\ntimestampAsOf\nThe option name to specify the time travel timestamp when reading a table.\n4.0.0\nspark.sql.timeTravelVersionKey\nversionAsOf\nThe option name to specify the time travel table version when reading a table.\n4.0.0\nspark.sql.timestampType\nTIMESTAMP_LTZ\nConfigures the default timestamp type of Spark SQL, including SQL DDL, Cast clause, type literal and the schema inference of data sou", "question": "What is the default timestamp type of Spark SQL?", "answers": {"text": ["TIMESTAMP_LTZ"], "answer_start": [653]}}
{"context": "pe\nTIMESTAMP_LTZ\nConfigures the default timestamp type of Spark SQL, including SQL DDL, Cast clause, type literal and the schema inference of data sources. Setting the configuration as TIMESTAMP_NTZ will use TIMESTAMP WITHOUT TIME ZONE as the default type while putting it as TIMESTAMP_LTZ will use TIMESTAMP WITH LOCAL TIME ZONE. Before the 3.4.0 release, Spark only supports the TIMESTAMP WITH LOCAL TIME ZONE type.\n3.4.0\nspark.sql.transposeMaxValues\n500\nWhen doing a transpose without specifying values for the index column this is the maximum number of values that will be transposed without error.\n4.0.0\nspark.sql.tvf.allowMultipleTableArguments.enabled\nfalse\nWhen true, allows multiple table arguments for table-valued functions, receiving the cartesian product of all the rows of these tables.", "question": "What does setting the configuration as TIMESTAMP_LTZ do?", "answers": {"text": ["will use TIMESTAMP WITH LOCAL TIME ZONE."], "answer_start": [290]}}
{"context": ".enabled\nfalse\nWhen true, allows multiple table arguments for table-valued functions, receiving the cartesian product of all the rows of these tables.\n3.5.0\nspark.sql.ui.explainMode\nformatted\nConfigures the query explain mode used in the Spark SQL UI. The value can be 'simple', 'extended', 'codegen', 'cost', or 'formatted'. The default value is 'formatted'.\n3.1.0\nspark.sql.variable.substitute\ntrue\nThis enables substitution using syntax like\n${var}\n,\n${system:var}\n, and\n${env:var}\n.\n2.0.0\nStatic SQL Configuration\nStatic SQL configurations are cross-session, immutable Spark SQL configurations. They can be set with final values by the config file\nand command-line options with\n--conf/-c\nprefixed, or by setting\nSparkConf\nthat are used to create\nSparkSession\n.\nExternal users can query the static", "question": "What does spark.sql.ui.explainMode configure?", "answers": {"text": ["Configures the query explain mode used in the Spark SQL UI."], "answer_start": [192]}}
{"context": "e\nand command-line options with\n--conf/-c\nprefixed, or by setting\nSparkConf\nthat are used to create\nSparkSession\n.\nExternal users can query the static sql config values via\nSparkSession.conf\nor via set command, e.g.\nSET spark.sql.extensions;\n, but cannot set/unset them.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.cache.serializer\norg.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer\nThe name of a class that implements org.apache.spark.sql.columnar.CachedBatchSerializer. It will be used to translate SQL data into a format that can more efficiently be cached. The underlying API is subject to change so use with caution. Multiple classes cannot be specified. The class must have a no-arg constructor.\n3.1.0\nspark.sql.catalog.spark_catalog.defaultDatabase\ndefault\nThe defa", "question": "What is the default value for the property spark.sql.catalog.spark_catalog.defaultDatabase?", "answers": {"text": ["default"], "answer_start": [768]}}
{"context": "Multiple classes cannot be specified. The class must have a no-arg constructor.\n3.1.0\nspark.sql.catalog.spark_catalog.defaultDatabase\ndefault\nThe default database for session catalog.\n3.4.0\nspark.sql.event.truncate.length\n2147483647\nThreshold of SQL length beyond which it will be truncated before adding to event. Defaults to no truncation. If set to 0, callsite will be logged instead.\n3.0.0\nspark.sql.extensions\n(none)\nA comma-separated list of classes that implement Function1[SparkSessionExtensions, Unit] used to configure Spark Session extensions. The classes must have a no-args constructor. If multiple extensions are specified, they are applied in the specified order. For the case of rules and planner strategies, they are applied in the specified order. For the case of parsers, the last ", "question": "What is the requirement for the constructor of classes used for Spark Session extensions?", "answers": {"text": ["The classes must have a no-args constructor."], "answer_start": [555]}}
{"context": "lied in the specified order. For the case of rules and planner strategies, they are applied in the specified order. For the case of parsers, the last parser is used and each parser can delegate to its predecessor. For the case of function name conflicts, the last registered function name is used.\n2.2.0\nspark.sql.extensions.test.loadFromCp\ntrue\nFlag that determines if we should load extensions from the classpath using the SparkSessionExtensionsProvider mechanism. This is a test only flag.\nspark.sql.hive.metastore.barrierPrefixes\nA comma separated list of class prefixes that should explicitly be reloaded for each version of Hive that Spark SQL is communicating with. For example, Hive UDFs that are declared in a prefix that typically would be shared (i.e.\norg.apache.spark.*\n).\n1.4.0\nspark.sql", "question": "What determines if Spark SQL should load extensions from the classpath?", "answers": {"text": ["Flag that determines if we should load extensions from the classpath using the SparkSessionExtensionsProvider mechanism."], "answer_start": [346]}}
{"context": "s.path\nin comma separated format. Support both local or remote paths.The provided jars\n  should be the same version as\nspark.sql.hive.metastore.version\n.\n4. A classpath in the standard format for both Hive and Hadoop. The provided jars\n  should be the same version as\nspark.sql.hive.metastore.version\n.\n1.4.0\nspark.sql.hive.metastore.jars.path\nComma-separated paths of the jars that used to instantiate the HiveMetastoreClient.\nThis configuration is useful only when\nspark.sql.hive.metastore.jars\nis set as\npath\n.\nThe paths can be any of the following format:\n1. file://path/to/jar/foo.jar\n2. hdfs://nameservice/path/to/jar/foo.jar\n3. /path/to/jar/ (path without URI scheme follow conf\nfs.defaultFS\n's URI schema)\n4. [http/https/ftp]://path/to/jar/foo.jar\nNote that 1, 2, and 3 support wildcard. For ", "question": "Quais formatos de caminho são suportados para os jars que serão usados para instanciar o HiveMetastoreClient?", "answers": {"text": ["1. file://path/to/jar/foo.jar\n2. hdfs://nameservice/path/to/jar/foo.jar\n3. /path/to/jar/ (path without URI scheme follow conf\nfs.defaultFS\n's URI schema)\n4. [http/https/ftp]://path/to/jar/foo.jar"], "answer_start": [560]}}
{"context": "path without URI scheme follow conf\nfs.defaultFS\n's URI schema)\n4. [http/https/ftp]://path/to/jar/foo.jar\nNote that 1, 2, and 3 support wildcard. For example:\n1. file://path/to/jar/\n,file://path2/to/jar/\n/\n.jar\n2. hdfs://nameservice/path/to/jar/\n,hdfs://nameservice2/path/to/jar/\n/\n.jar\n3.1.0\nspark.sql.hive.metastore.sharedPrefixes\ncom.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc\nA comma separated list of class prefixes that should be loaded using the classloader that is shared between Spark SQL and a specific version of Hive. An example of classes that should be shared is JDBC drivers that are needed to talk to the metastore. Other classes that need to be shared are those that interact with classes that are already shared. For example, custom appenders that are used by log", "question": "What is an example of classes that should be shared between Spark SQL and Hive?", "answers": {"text": ["JDBC drivers that are needed to talk to the metastore"], "answer_start": [596]}}
{"context": "of the Spark distribution bundled with. Note that, this a read-only conf and only used to report the built-in hive version. If you want a different metastore client for Spark to call, please refer to spark.sql.hive.metastore.version.\n1.1.1\nspark.sql.metadataCacheTTLSeconds\n-1000ms\nTime-to-live (TTL) value for the metadata caches: partition file metadata cache and session catalog cache. This configuration only has an effect when this value having a positive value (> 0). It also requires setting 'spark.sql.catalogImplementation' to\nhive\n, setting 'spark.sql.hive.filesourcePartitionFileCacheSize' > 0 and setting 'spark.sql.hive.manageFilesourcePartitions' to\ntrue\nto be applied to the partition file metadata cache.\n3.1.0\nspark.sql.queryExecutionListeners\n(none)\nList of class names implementing", "question": "What is the effect of setting 'spark.sql.catalogImplementation' to hive?", "answers": {"text": ["This configuration only has an effect when this value having a positive value (> 0). It also requires setting 'spark.sql.catalogImplementation' to\nhive"], "answer_start": [389]}}
{"context": "artitions' to\ntrue\nto be applied to the partition file metadata cache.\n3.1.0\nspark.sql.queryExecutionListeners\n(none)\nList of class names implementing QueryExecutionListener that will be automatically added to newly created sessions. The classes should have either a no-arg constructor, or a constructor that expects a SparkConf argument.\n2.3.0\nspark.sql.sources.disabledJdbcConnProviderList\nConfigures a list of JDBC connection providers, which are disabled. The list contains the name of the JDBC connection providers separated by comma.\n3.1.0\nspark.sql.streaming.streamingQueryListeners\n(none)\nList of class names implementing StreamingQueryListener that will be automatically added to newly created sessions. The classes should have either a no-arg constructor, or a constructor that expects a Sp", "question": "What is the purpose of spark.sql.queryExecutionListeners?", "answers": {"text": ["List of class names implementing QueryExecutionListener that will be automatically added to newly created sessions. The classes should have either a no-arg constructor, or a constructor that expects a SparkConf argument."], "answer_start": [118]}}
{"context": "o retain in the Spark UI.\n1.5.0\nspark.sql.warehouse.dir\n(value of\n$PWD/spark-warehouse\n)\nThe default location for managed databases and tables.\n2.0.0\nSpark Streaming\nProperty Name\nDefault\nMeaning\nSince Version\nspark.streaming.backpressure.enabled\nfalse\nEnables or disables Spark Streaming's internal backpressure mechanism (since 1.5).\n    This enables the Spark Streaming to control the receiving rate based on the\n    current batch scheduling delays and processing times so that the system receives\n    only as fast as the system can process. Internally, this dynamically sets the\n    maximum receiving rate of receivers. This rate is upper bounded by the values\nspark.streaming.receiver.maxRate\nand\nspark.streaming.kafka.maxRatePerPartition\nif they are set (see below).\n1.5.0\nspark.streaming.backp", "question": "What does spark.streaming.backpressure.enabled control?", "answers": {"text": ["Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5)."], "answer_start": [253]}}
{"context": "ceiver.maxRate\nnot set\nMaximum rate (number of records per second) at which each receiver will receive data.\n    Effectively, each stream will consume at most this number of records per second.\n    Setting this configuration to 0 or a negative number will put no limit on the rate.\n    See the\ndeployment guide\nin the Spark Streaming programming guide for mode details.\n1.0.2\nspark.streaming.receiver.writeAheadLog.enable\nfalse\nEnable write-ahead logs for receivers. All the input data received through receivers\n    will be saved to write-ahead logs that will allow it to be recovered after driver failures.\n    See the\ndeployment guide\nin the Spark Streaming programming guide for more details.\n1.2.1\nspark.streaming.unpersist\ntrue\nForce RDDs generated and persisted by Spark Streaming to be automa", "question": "What happens when the 'ceiver.maxRate' configuration is set to 0 or a negative number?", "answers": {"text": ["Setting this configuration to 0 or a negative number will put no limit on the rate."], "answer_start": [198]}}
{"context": "diately.\n1.4.0\nspark.streaming.kafka.maxRatePerPartition\nnot set\nMaximum rate (number of records per second) at which data will be read from each Kafka\n    partition when using the new Kafka direct stream API. See the\nKafka Integration guide\nfor more details.\n1.3.0\nspark.streaming.kafka.minRatePerPartition\n1\nMinimum rate (number of records per second) at which data will be read from each Kafka\n    partition when using the new Kafka direct stream API.\n2.4.0\nspark.streaming.ui.retainedBatches\n1000\nHow many batches the Spark Streaming UI and status APIs remember before garbage collecting.\n1.0.0\nspark.streaming.driver.writeAheadLog.closeFileAfterWrite\nfalse\nWhether to close the file after writing a write-ahead log record on the driver. Set this to 'true'\n    when you want to use S3 (or any fil", "question": "What does spark.streaming.ui.retainedBatches control?", "answers": {"text": ["How many batches the Spark Streaming UI and status APIs remember before garbage collecting."], "answer_start": [501]}}
{"context": "ackend to handle RPC calls from SparkR package.\n1.4.0\nspark.r.command\nRscript\nExecutable for executing R scripts in cluster modes for both driver and workers.\n1.5.3\nspark.r.driver.command\nspark.r.command\nExecutable for executing R scripts in client modes for driver. Ignored in cluster modes.\n1.5.3\nspark.r.shell.command\nR\nExecutable for executing sparkR shell in client modes for driver. Ignored in cluster modes. It is the same as environment variable\nSPARKR_DRIVER_R\n, but take precedence over it.\nspark.r.shell.command\nis used for sparkR shell while\nspark.r.driver.command\nis used for running R script.\n2.1.0\nspark.r.backendConnectionTimeout\n6000\nConnection timeout set by R process on its connection to RBackend in seconds.\n2.1.0\nspark.r.heartBeatInterval\n100\nInterval for heartbeats sent from S", "question": "What is the purpose of spark.r.command?", "answers": {"text": ["Executable for executing R scripts in cluster modes for both driver and workers."], "answer_start": [78]}}
{"context": "\nConnection timeout set by R process on its connection to RBackend in seconds.\n2.1.0\nspark.r.heartBeatInterval\n100\nInterval for heartbeats sent from SparkR backend to R process to prevent connection timeout.\n2.1.0\nGraphX\nProperty Name\nDefault\nMeaning\nSince Version\nspark.graphx.pregel.checkpointInterval\n-1\nCheckpoint interval for graph and message in Pregel. It used to avoid stackOverflowError due to long lineage chains\n  after lots of iterations. The checkpoint is disabled by default.\n2.2.0\nCluster Managers\nEach cluster manager in Spark has additional configuration options. Configurations\ncan be found on the pages for each mode:\nYARN\nKubernetes\nStandalone Mode\nEnvironment Variables\nCertain Spark settings can be configured through environment variables, which are read from the\nconf/spark-en", "question": "What is the default value for spark.graphx.pregel.checkpointInterval?", "answers": {"text": ["-1"], "answer_start": [304]}}
{"context": "ation\nspark.log.structuredLogging.enabled\nto\ntrue\n(default is\nfalse\n). For additional customization, copy\nlog4j2-json-layout.properties.template\nto\nconf/log4j2.properties\nand adjust as needed.\nQuerying Structured Logs with Spark SQL\nTo query structured logs in JSON format, use the following code snippet:\nPython:\nfrom\npyspark.logger\nimport\nSPARK_LOG_SCHEMA\nlogDf\n=\nspark\n.\nread\n.\nschema\n(\nSPARK_LOG_SCHEMA\n).\njson\n(\n\"\npath/to/logs\n\"\n)\nScala:\nimport\norg.apache.spark.util.LogUtils.SPARK_LOG_SCHEMA\nval\nlogDf\n=\nspark\n.\nread\n.\nschema\n(\nSPARK_LOG_SCHEMA\n).\njson\n(\n\"path/to/logs\"\n)\nNote\n: If you’re using the interactive shell (pyspark shell or spark-shell), you can omit the import statement in the code because SPARK_LOG_SCHEMA is already available in the shell’s context.\nOverriding configuration dire", "question": "How can structured logs in JSON format be queried using Spark SQL?", "answers": {"text": ["To query structured logs in JSON format, use the following code snippet:"], "answer_start": [233]}}
{"context": "te.xml\n, which sets the default filesystem name.\nThe location of these configuration files varies across Hadoop versions, but\na common location is inside of\n/etc/hadoop/conf\n. Some tools create\nconfigurations on-the-fly, but offer a mechanism to download copies of them.\nTo make these files visible to Spark, set\nHADOOP_CONF_DIR\nin\n$SPARK_HOME/conf/spark-env.sh\nto a location containing the configuration files.\nCustom Hadoop/Hive Configuration\nIf your Spark application is interacting with Hadoop, Hive, or both, there are probably Hadoop/Hive\nconfiguration files in Spark’s classpath.\nMultiple running applications might require different Hadoop/Hive client side configurations.\nYou can copy and modify\nhdfs-site.xml\n,\ncore-site.xml\n,\nyarn-site.xml\n,\nhive-site.xml\nin\nSpark’s classpath for each app", "question": "Where can you set the location containing the configuration files to make them visible to Spark?", "answers": {"text": ["set\nHADOOP_CONF_DIR\nin\n$SPARK_HOME/conf/spark-env.sh\nto a location containing the configuration files."], "answer_start": [309]}}
{"context": "ve client side configurations.\nYou can copy and modify\nhdfs-site.xml\n,\ncore-site.xml\n,\nyarn-site.xml\n,\nhive-site.xml\nin\nSpark’s classpath for each application. In a Spark cluster running on YARN, these configuration\nfiles are set cluster-wide, and cannot safely be changed by the application.\nThe better choice is to use spark hadoop properties in the form of\nspark.hadoop.*\n, and use\nspark hive properties in the form of\nspark.hive.*\n.\nFor example, adding configuration “spark.hadoop.abc.def=xyz” represents adding hadoop property “abc.def=xyz”,\nand adding configuration “spark.hive.abc=xyz” represents adding hive property “hive.abc=xyz”.\nThey can be considered as same as normal spark properties which can be set in\n$SPARK_HOME/conf/spark-defaults.conf\nIn some cases, you may want to avoid hard-co", "question": "How can Hadoop properties be added in Spark?", "answers": {"text": ["For example, adding configuration “spark.hadoop.abc.def=xyz” represents adding hadoop property “abc.def=xyz”"], "answer_start": [437]}}
{"context": "\\\n--conf\nspark.hadoop.abc.def\n=\nxyz\n\\\n--conf\nspark.hive.abc\n=\nxyz\n  myApp.jar\nCustom Resource Scheduling and Configuration Overview\nGPUs and other accelerators have been widely used for accelerating special workloads, e.g.,\ndeep learning and signal processing. Spark now supports requesting and scheduling generic resources, such as GPUs, with a few caveats. The current implementation requires that the resource have addresses that can be allocated by the scheduler. It requires your cluster manager to support and be properly configured with the resources.\nThere are configurations available to request resources for the driver:\nspark.driver.resource.{resourceName}.amount\n, request resources for the executor(s):\nspark.executor.resource.{resourceName}.amount\nand specify the requirements for each ", "question": "How can resources be requested for the driver in Spark?", "answers": {"text": ["spark.driver.resource.{resourceName}.amount"], "answer_start": [631]}}
{"context": "ce.{resourceName}.amount\n, request resources for the executor(s):\nspark.executor.resource.{resourceName}.amount\nand specify the requirements for each task:\nspark.task.resource.{resourceName}.amount\n. The\nspark.driver.resource.{resourceName}.discoveryScript\nconfig is required on YARN, Kubernetes and a client side Driver on Spark Standalone.\nspark.executor.resource.{resourceName}.discoveryScript\nconfig is required for YARN and Kubernetes. Kubernetes also requires\nspark.driver.resource.{resourceName}.vendor\nand/or\nspark.executor.resource.{resourceName}.vendor\n. See the config descriptions above for more information on each.\nSpark will use the configurations specified to first request containers with the corresponding resources from the cluster manager. Once it gets the container, Spark launch", "question": "Which configurations are required on YARN, Kubernetes and a client side Driver on Spark Standalone?", "answers": {"text": ["spark.driver.resource.{resourceName}.discoveryScript"], "answer_start": [204]}}
{"context": "he user can see the resources assigned with the SparkContext\nresources\ncall. It’s then up to the user to use the assigned addresses to do the processing they want or pass those into the ML/AI framework they are using.\nSee your cluster manager specific page for requirements and details on each of -\nYARN\n,\nKubernetes\nand\nStandalone Mode\n. It is currently not available with local mode. And please also note that local-cluster mode with multiple workers is not supported(see Standalone documentation).\nStage Level Scheduling Overview\nThe stage level scheduling feature allows users to specify task and executor resource requirements at the stage level. This allows for different stages to run with executors that have different resources. A prime example of this is one ETL stage runs with executors w", "question": "What does the stage level scheduling feature allow users to do?", "answers": {"text": ["The stage level scheduling feature allows users to specify task and executor resource requirements at the stage level."], "answer_start": [533]}}
{"context": ". This allows for different stages to run with executors that have different resources. A prime example of this is one ETL stage runs with executors with just CPUs, the next stage is an ML stage that needs GPUs. Stage level scheduling allows for user to request different executors that have GPUs when the ML stage runs rather then having to acquire executors with GPUs at the start of the application and them be idle while the ETL stage is being run.\nThis is only available for the RDD API in Scala, Java, and Python.  It is available on YARN, Kubernetes and Standalone when dynamic allocation is enabled. When dynamic allocation is disabled, it allows users to specify different task resource requirements at stage level, and this is supported on YARN, Kubernetes and Standalone cluster right now.", "question": "What is an example of how stage level scheduling is beneficial?", "answers": {"text": ["A prime example of this is one ETL stage runs with executors with just CPUs, the next stage is an ML stage that needs GPUs."], "answer_start": [88]}}
{"context": "t tasks into an executor that require a different ResourceProfile than the executor was created with. Executors that are not in use will idle timeout with the dynamic allocation logic. The default configuration for this feature is to only allow one ResourceProfile per stage. If the user associates more then 1 ResourceProfile to an RDD, Spark will throw an exception by default. See config\nspark.scheduler.resource.profileMergeConflicts\nto control that behavior. The current merge strategy Spark implements when\nspark.scheduler.resource.profileMergeConflicts\nis enabled is a simple max of each resource within the conflicting ResourceProfiles. Spark will create a new ResourceProfile with the max of each of the resources.\nPush-based shuffle overview\nPush-based shuffle helps improve the reliability", "question": "What happens when a user associates more than 1 ResourceProfile to an RDD?", "answers": {"text": ["If the user associates more then 1 ResourceProfile to an RDD, Spark will throw an exception by default."], "answer_start": [276]}}
{"context": " will create a new ResourceProfile with the max of each of the resources.\nPush-based shuffle overview\nPush-based shuffle helps improve the reliability and performance of spark shuffle. It takes a best-effort approach to push the shuffle blocks generated by the map tasks to remote external shuffle services to be merged per shuffle partition. Reduce tasks fetch a combination of merged shuffle partitions and original shuffle blocks as their input data, resulting in converting small random disk reads by external shuffle services into large sequential reads. Possibility of better data locality for reduce tasks additionally helps minimize network IO. Push-based shuffle takes priority over batch fetch for some scenarios, like partition coalesce when merged output is available.\nPush-based shuffle ", "question": "What does push-based shuffle aim to improve?", "answers": {"text": ["improve the reliability and performance of spark shuffle."], "answer_start": [127]}}
{"context": "O. Push-based shuffle takes priority over batch fetch for some scenarios, like partition coalesce when merged output is available.\nPush-based shuffle improves performance for long running jobs/queries which involves large disk I/O during shuffle. Currently it is not well suited for jobs/queries which runs quickly dealing with lesser amount of shuffle data. This will be further improved in the future releases.\nCurrently push-based shuffle is only supported for Spark on YARN with external shuffle service.\nExternal Shuffle service(server) side configuration options\nProperty Name\nDefault\nMeaning\nSince Version\nspark.shuffle.push.server.mergedShuffleFileManagerImpl\norg.apache.spark.network.shuffle.\nNoOpMergedShuffleFileManager\nClass name of the implementation of\nMergedShuffleFileManager\nthat man", "question": "For which Spark deployment is push-based shuffle currently supported?", "answers": {"text": ["Currently push-based shuffle is only supported for Spark on YARN with external shuffle service."], "answer_start": [413]}}
{"context": " multiple small shuffle blocks. Fetching the complete merged shuffle file in a single disk I/O increases the memory requirements for both the clients and the external shuffle services. Instead, the external shuffle service serves the merged file in\nMB-sized chunks\n.\nThis configuration controls how big a chunk can get. A corresponding index file for each merged shuffle file will be generated indicating chunk boundaries.\nSetting this too high would increase the memory requirements on both the clients and the external shuffle service.\nSetting this too low would increase the overall number of RPC requests to external shuffle service unnecessarily.\n3.2.0\nspark.shuffle.push.server.mergedIndexCacheSize\n100m\nThe maximum size of cache in memory which could be used in push-based shuffle for storing ", "question": "What happens if the chunk size is set too low?", "answers": {"text": ["Setting this too low would increase the overall number of RPC requests to external shuffle service unnecessarily."], "answer_start": [538]}}
{"context": ".\n3.2.0\nspark.shuffle.push.server.mergedIndexCacheSize\n100m\nThe maximum size of cache in memory which could be used in push-based shuffle for storing merged index files. This cache is in addition to the one configured via\nspark.shuffle.service.index.cache.size\n.\n3.2.0\nClient side configuration options\nProperty Name\nDefault\nMeaning\nSince Version\nspark.shuffle.push.enabled\nfalse\nSet to true to enable push-based shuffle on the client side and works in conjunction with the server side flag\nspark.shuffle.push.server.mergedShuffleFileManagerImpl\n.\n3.2.0\nspark.shuffle.push.finalize.timeout\n10s\nThe amount of time driver waits in seconds, after all mappers have finished for a given shuffle map stage, before it sends merge finalize requests to remote external shuffle services. This gives the externa", "question": "What does spark.shuffle.push.enabled do?", "answers": {"text": ["Set to true to enable push-based shuffle on the client side and works in conjunction with the server side flag\nspark.shuffle.push.server.mergedShuffleFileManagerImpl"], "answer_start": [380]}}
{"context": "ppers have finished for a given shuffle map stage, before it sends merge finalize requests to remote external shuffle services. This gives the external shuffle services extra time to merge blocks. Setting this too long could potentially lead to performance regression.\n3.2.0\nspark.shuffle.push.maxRetainedMergerLocations\n500\nMaximum number of merger locations cached for push-based shuffle. Currently, merger locations are hosts of external shuffle services responsible for handling pushed blocks, merging them and serving merged blocks for later shuffle fetch.\n3.2.0\nspark.shuffle.push.mergersMinThresholdRatio\n0.05\nRatio used to compute the minimum number of shuffle merger locations required for a stage based on the number of partitions for the reducer stage. For example, a reduce stage which ha", "question": "What does spark.shuffle.push.maxRetainedMergerLocations control?", "answers": {"text": ["Maximum number of merger locations cached for push-based shuffle."], "answer_start": [325]}}
{"context": " number of shuffle merger locations required for a stage based on the number of partitions for the reducer stage. For example, a reduce stage which has 100 partitions and uses the default value 0.05 requires at least 5 unique merger locations to enable push-based shuffle.\n3.2.0\nspark.shuffle.push.mergersMinStaticThreshold\n5\nThe static threshold for number of shuffle push merger locations should be available in order to enable push-based shuffle for a stage. Note this config works in conjunction with\nspark.shuffle.push.mergersMinThresholdRatio\n. Maximum of\nspark.shuffle.push.mergersMinStaticThreshold\nand\nspark.shuffle.push.mergersMinThresholdRatio\nratio number of mergers needed to enable push-based shuffle for a stage. For example: with 1000 partitions for the child stage with spark.shuffle", "question": "What is the purpose of spark.shuffle.push.mergersMinStaticThreshold?", "answers": {"text": ["The static threshold for number of shuffle push merger locations should be available in order to enable push-based shuffle for a stage."], "answer_start": [326]}}
{"context": "atio\nratio number of mergers needed to enable push-based shuffle for a stage. For example: with 1000 partitions for the child stage with spark.shuffle.push.mergersMinStaticThreshold as 5 and spark.shuffle.push.mergersMinThresholdRatio set to 0.05, we would need at least 50 mergers to enable push-based shuffle for that stage.\n3.2.0\nspark.shuffle.push.numPushThreads\n(none)\nSpecify the number of threads in the block pusher pool. These threads assist in creating connections and pushing blocks to remote external shuffle services.\n    By default, the threadpool size is equal to the number of spark executor cores.\n3.2.0\nspark.shuffle.push.maxBlockSizeToPush\n1m\nThe max size of an individual block to push to the remote external shuffle services. Blocks larger than this threshold are not pushed to b", "question": "What is the default threadpool size for spark.shuffle.push.numPushThreads?", "answers": {"text": ["By default, the threadpool size is equal to the number of spark executor cores."], "answer_start": [535]}}
{"context": "zeToPush\n1m\nThe max size of an individual block to push to the remote external shuffle services. Blocks larger than this threshold are not pushed to be merged remotely. These shuffle blocks will be fetched in the original manner.\nSetting this too high would result in more blocks to be pushed to remote external shuffle services but those are already efficiently fetched with the existing mechanisms resulting in additional overhead of pushing the large blocks to remote external shuffle services. It is recommended to set\nspark.shuffle.push.maxBlockSizeToPush\nlesser than\nspark.shuffle.push.maxBlockBatchSize\nconfig's value.\nSetting this too low would result in lesser number of blocks getting merged and directly fetched from mapper external shuffle service results in higher small random reads aff", "question": "What happens to shuffle blocks larger than the 'spark.shuffle.push.maxBlockSizeToPush' threshold?", "answers": {"text": ["Blocks larger than this threshold are not pushed to be merged remotely. These shuffle blocks will be fetched in the original manner."], "answer_start": [97]}}
{"context": "ld result in lesser number of blocks getting merged and directly fetched from mapper external shuffle service results in higher small random reads affecting overall disk I/O performance.\n3.2.0\nspark.shuffle.push.maxBlockBatchSize\n3m\nThe max size of a batch of shuffle blocks to be grouped into a single push request. Default is set to\n3m\nin order to keep it slightly higher than\nspark.storage.memoryMapThreshold\ndefault which is\n2m\nas it is very likely that each batch of block gets memory mapped which incurs higher overhead.\n3.2.0\nspark.shuffle.push.merge.finalizeThreads\n8\nNumber of threads used by driver to finalize shuffle merge. Since it could potentially take seconds for a large shuffle to finalize,\n    having multiple threads helps driver to handle concurrent shuffle merge finalize reques", "question": "What is the default value for spark.shuffle.push.maxBlockBatchSize?", "answers": {"text": ["3m"], "answer_start": [230]}}
{"context": " potentially take seconds for a large shuffle to finalize,\n    having multiple threads helps driver to handle concurrent shuffle merge finalize requests when push-based shuffle is enabled.\n3.3.0\nspark.shuffle.push.minShuffleSizeToWait\n500m\nDriver will wait for merge finalization to complete only if total shuffle data size is more than this threshold. If total shuffle size is less, driver will immediately finalize the shuffle output.\n3.3.0\nspark.shuffle.push.minCompletedPushRatio\n1.0\nFraction of minimum map partitions that should be push complete before driver starts shuffle merge finalization during push based shuffle.\n3.3.0", "question": "What is the default value for spark.shuffle.push.minCompletedPushRatio?", "answers": {"text": ["1.0"], "answer_start": [484]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nIntegration with Cloud Infrastructures\nIntroduction\nImportant: Cloud Object Stores are Not Real Filesystems\nConsistency\nInstallation\nAuthenticating\nConfiguring\nRecommended settings for writing to object stores\nParquet I/O Settings\nORC I/O Settings\nSpa", "question": "What are some of the programming guides available for Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)"], "answer_start": [46]}}
{"context": "ystems\nConsistency\nInstallation\nAuthenticating\nConfiguring\nRecommended settings for writing to object stores\nParquet I/O Settings\nORC I/O Settings\nSpark Streaming and Object Storage\nCommitting work into cloud storage safely and fast.\nHadoop S3A committers\nAmazon EMR: the EMRFS S3-optimized committer\nAzure and Google cloud storage: MapReduce Intermediate Manifest Committer.\nIBM Cloud Object Storage: Stocator\nCloud Committers and\nINSERT OVERWRITE TABLE\nFurther Reading\nIntroduction\nAll major cloud providers offer persistent data storage in\nobject stores\n.\nThese are not classic “POSIX” file systems.\nIn order to store hundreds of petabytes of data without any single points of failure,\nobject stores replace the classic file system directory tree\nwith a simpler model of\nobject-name => data\n. To e", "question": "What do object stores replace the classic file system directory tree with?", "answers": {"text": ["object-name => data"], "answer_start": [774]}}
{"context": "e filesystems, underneath\nthey are still object stores,\nand the difference is significant\nThey cannot be used as a direct replacement for a cluster filesystem such as HDFS\nexcept where this is explicitly stated\n.\nKey differences are:\nThe means by which directories are emulated may make working with them slow.\nRename operations may be very slow and, on failure, leave the store in an unknown state.\nSeeking within a file may require new HTTP calls, hurting performance.\nHow does this affect Spark?\nReading and writing data can be significantly slower than working with a normal filesystem.\nSome directory structures may be very inefficient to scan during query split calculation.\nThe rename-based algorithm by which Spark normally commits work when saving an RDD, DataFrame or Dataset\n is potentiall", "question": "What are some key differences between object stores and cluster filesystems like HDFS?", "answers": {"text": ["Rename operations may be very slow and, on failure, leave the store in an unknown state."], "answer_start": [311]}}
{"context": "clients\nwill be actively reading them.\nOther object stores are\ninconsistent\nThis includes\nOpenStack Swift\n.\nSuch stores are not always safe to use as a destination of work -consult\neach store’s specific documentation.\nInstallation\nWith the relevant libraries on the classpath and Spark configured with valid credentials,\nobjects can be read or written by using their URLs as the path to data.\nFor example\nsparkContext.textFile(\"s3a://landsat-pds/scene_list.gz\")\nwill create\nan RDD of the file\nscene_list.gz\nstored in S3, using the s3a connector.\nTo add the relevant libraries to an application’s classpath, include the\nhadoop-cloud\nmodule and its dependencies.\nIn Maven, add the following to the\npom.xml\nfile, assuming\nspark.version\nis set to the chosen version of Spark:\n<dependencyManagement>\n...\n<", "question": "How can objects be read or written in Spark?", "answers": {"text": ["objects can be read or written by using their URLs as the path to data."], "answer_start": [321]}}
{"context": "ithm\nvery, very slow. The recommended solution to this is switch to an S3 “Zero Rename”\ncommitter (see below).\nFor reference, here are the performance and safety characteristics of\ndifferent stores and connectors when renaming directories:\nStore\nConnector\nDirectory Rename Safety\nRename Performance\nAmazon S3\ns3a\nUnsafe\nO(data)\nAzure Storage\nwasb\nSafe\nO(files)\nAzure Datalake Gen 2\nabfs\nSafe\nO(1)\nGoogle Cloud Storage\ngs\nMixed\nO(files)\nAs storing temporary files can run up charges; delete\ndirectories called\n\"_temporary\"\non a regular basis.\nFor AWS S3, set a limit on how long multipart uploads can remain outstanding.\nThis avoids incurring bills from incompleted uploads.\nFor Google cloud, directory rename is file-by-file. Consider using the v2 committer\nand only write code which generates idempo", "question": "What is the rename performance for Amazon S3 using the s3a connector?", "answers": {"text": ["O(data)"], "answer_start": [320]}}
{"context": "om incompleted uploads.\nFor Google cloud, directory rename is file-by-file. Consider using the v2 committer\nand only write code which generates idempotent output -including filenames,\nas it is\nno more unsafe\nthan the v1 committer, and faster.\nParquet I/O Settings\nFor optimal performance when working with Parquet data use the following settings:\nspark.hadoop.parquet.enable.summary-metadata false\nspark.sql.parquet.mergeSchema false\nspark.sql.parquet.filterPushdown true\nspark.sql.hive.metastorePartitionPruning true\nThese minimise the amount of data read during queries.\nORC I/O Settings\nFor best performance when working with ORC data, use these settings:\nspark.sql.orc.filterPushdown true\nspark.sql.orc.splits.include.file.footer true\nspark.sql.orc.cache.stripe.details.size 10000\nspark.sql.hive.", "question": "What settings are recommended for optimal performance when working with Parquet data?", "answers": {"text": ["spark.hadoop.parquet.enable.summary-metadata false\nspark.sql.parquet.mergeSchema false\nspark.sql.parquet.filterPushdown true\nspark.sql.hive.metastorePartitionPruning true"], "answer_start": [347]}}
{"context": "ettings:\nspark.sql.orc.filterPushdown true\nspark.sql.orc.splits.include.file.footer true\nspark.sql.orc.cache.stripe.details.size 10000\nspark.sql.hive.metastorePartitionPruning true\nAgain, these minimise the amount of data read during queries.\nSpark Streaming and Object Storage\nSpark Streaming can monitor files added to object stores, by\ncreating a\nFileInputDStream\nto monitor a path in the store through a call to\nStreamingContext.textFileStream()\n.\nThe time to scan for new files is proportional to the number of files\nunder the path, not the number of\nnew\nfiles, so it can become a slow operation.\nThe size of the window needs to be set to handle this.\nFiles only appear in an object store once they are completely written; there\nis no need for a workflow of write-then-rename to ensure that file", "question": "What does Spark Streaming use to monitor files added to object stores?", "answers": {"text": ["FileInputDStream"], "answer_start": [350]}}
{"context": " setting the\nspark.sql.streaming.checkpointFileManagerClass\nconfiguration to\norg.apache.spark.internal.io.cloud.AbortableStreamBasedCheckpointFileManager\n)\nwhich eliminates the slow rename. In this case users must be extra careful to avoid the reuse of\nthe checkpoint location among multiple queries running parallelly as that could lead to corruption\nof the checkpointing data.\nCommitting work into cloud storage safely and fast.\nAs covered earlier, commit-by-rename is dangerous on any object store which\nexhibits eventual consistency (example: S3), and often slower than classic\nfilesystem renames.\nSome object store connectors provide custom committers to commit tasks and\njobs without using rename.\nHadoop S3A committers\nIn versions of Spark built with Hadoop 3.1 or later,\nthe hadoop-aws JAR co", "question": "What class can be set to eliminate the slow rename operation?", "answers": {"text": ["org.apache.spark.internal.io.cloud.AbortableStreamBasedCheckpointFileManager"], "answer_start": [77]}}
{"context": " Spark was built with Hadoop\n3.1 or later, and switch the committers through the following options.\nspark.hadoop.fs.s3a.committer.name directory\nspark.sql.sources.commitProtocolClass org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\nspark.sql.parquet.output.committer.class org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\nIt has been tested with the most common formats supported by Spark.\nmydataframe\n.\nwrite\n.\nformat\n(\n\"\nparquet\n\"\n).\nsave\n(\n\"\ns3a://bucket/destination\n\"\n)\nMore details on these committers can be found in\nthe latest Hadoop documentation\nwith S3A committer detail covered in\nCommitting work to S3 with the S3A Committers\n.\nNote: depending upon the committer used, in-progress statistics may be\nunder-reported with Hadoop versions before 3.3.1.\nAmazon EMR: ", "question": "Which class is used for the commit protocol in Spark SQL sources?", "answers": {"text": ["org.apache.spark.internal.io.cloud.PathOutputCommitProtocol"], "answer_start": [183]}}
{"context": "S3A Committers\n.\nNote: depending upon the committer used, in-progress statistics may be\nunder-reported with Hadoop versions before 3.3.1.\nAmazon EMR: the EMRFS S3-optimized committer\nAmazon EMR has its own S3-aware committers for parquet data.\nFor instructions on use, see\nthe EMRFS S3-optimized committer\nFor implementation and performance details, see\n[“Improve Apache Spark write performance on Apache Parquet formats with the EMRFS S3-optimized committer”](https://aws.amazon.com/blogs/big-data/improve-apache-spark-write-performance-on-apache-parquet-formats-with-the-emrfs-s3-optimized-committer/\nAzure and Google cloud storage: MapReduce Intermediate Manifest Committer.\nVersions of the hadoop-mapreduce-core JAR shipped after September 2022 (3.3.5 and later)\ncontain a committer optimized for", "question": "Which committer is optimized for parquet data in Amazon EMR?", "answers": {"text": ["the EMRFS S3-optimized committer"], "answer_start": [150]}}
{"context": "nal\nrate limiting to avoid throttling IO.\nThis delivers performance and scalability on the object stores.\nIt is not critical for job correctness to use this with Azure storage; the\nclassic FileOutputCommitter is safe there -however this new committer scales\nbetter for large jobs with deep and wide directory trees.\nBecause Google GCS does not support atomic directory renaming,\nthe manifest committer should be used where available.\nThis committer does support  “dynamic partition overwrite” (see below).\nFor details on availability and use of this committer, consult\nthe hadoop documentation for the Hadoop release used.\nIt is not available on Hadoop 3.3.4 or earlier.\nIBM Cloud Object Storage: Stocator\nIBM provide the Stocator output committer for IBM Cloud Object Storage and OpenStack Swift.\nSo", "question": "For what storage systems does IBM provide the Stocator output committer?", "answers": {"text": ["IBM Cloud Object Storage and OpenStack Swift."], "answer_start": [752]}}
{"context": "ters, after their instantiation, Spark\nwill probe for their declaration of compatibility, and\npermit the operation if state that they are compatible.\nIf the committer is not compatible, the operation will fail with\nthe error message\nPathOutputCommitter does not support dynamicPartitionOverwrite\nUnless there is a compatible committer for the target filesystem,\nthe sole solution is to use a cloud-friendly format for data\nstorage.\nFurther Reading\nHere is the documentation on the standard connectors both from Apache and the cloud providers.\nAzure Blob Storage\n.\nAzure Blob Filesystem (ABFS) and Azure Datalake Gen 2\n.\nAzure Data Lake Gen 1\n.\nAmazon S3 Strong Consistency\nHadoop-AWS module (Hadoop 3.x)\n.\nAmazon EMR File System (EMRFS)\n. From Amazon.\nUsing the EMRFS S3-optimized Committer\nGoogle Cl", "question": "What error message will occur if the committer is not compatible?", "answers": {"text": ["PathOutputCommitter does not support dynamicPartitionOverwrite"], "answer_start": [233]}}
{"context": " S3 Strong Consistency\nHadoop-AWS module (Hadoop 3.x)\n.\nAmazon EMR File System (EMRFS)\n. From Amazon.\nUsing the EMRFS S3-optimized Committer\nGoogle Cloud Storage Connector for Spark and Hadoop\n. From Google.\nThe Azure Blob Filesystem driver (ABFS)\nIBM Cloud Object Storage connector for Apache Spark:\nStocator\n,\nIBM Object Storage\n. From IBM.\nUsing JindoFS SDK to access Alibaba Cloud OSS\n.\nThe Cloud Committer problem and hive-compatible solutions\nCommitting work to S3 with the S3A Committers\nImprove Apache Spark write performance on Apache Parquet formats with the EMRFS S3-optimized committer\nThe Manifest Committer for Azure and Google Cloud Storage\nA Zero-rename committer\n.\nStocator: A High Performance Object Store Connector for Spark", "question": "What is Stocator?", "answers": {"text": ["Stocator: A High Performance Object Store Connector for Spark"], "answer_start": [682]}}
{"context": "orage\nA Zero-rename committer\n.\nStocator: A High Performance Object Store Connector for Spark", "question": "What is Stocator?", "answers": {"text": ["A High Performance Object Store Connector for Spark"], "answer_start": [42]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMigration Guide\nThis page documents sections of the migration guide for each component in order\nfor users to migrate effectively.\nSpark Core\nSQL, Datasets, and DataFrame\nStructured Streaming\nMLlib (Machine Learning)\nPySpark (Python on Spark)\nSparkR (R", "question": "What components are covered in the migration guide?", "answers": {"text": ["Spark Core\nSQL, Datasets, and DataFrame\nStructured Streaming\nMLlib (Machine Learning)\nPySpark (Python on Spark)\nSparkR (R"], "answer_start": [679]}}
{"context": "sers to migrate effectively.\nSpark Core\nSQL, Datasets, and DataFrame\nStructured Streaming\nMLlib (Machine Learning)\nPySpark (Python on Spark)\nSparkR (R on Spark)", "question": "Quais são alguns dos componentes principais do Spark?", "answers": {"text": ["Spark Core"], "answer_start": [29]}}
{"context": "he Spark Connect API\nbuilds on Spark’s DataFrame API using unresolved logical plans as a\nlanguage-agnostic protocol between the client and the Spark driver.\nThe Spark Connect client translates DataFrame operations into unresolved\nlogical query plans which are encoded using protocol buffers. These are sent\nto the server using the gRPC framework.\nThe Spark Connect endpoint embedded on the Spark Server receives and\ntranslates unresolved logical plans into Spark’s logical plan operators.\nThis is similar to parsing a SQL query, where attributes and relations are\nparsed and an initial parse plan is built. From there, the standard Spark\nexecution process kicks in, ensuring that Spark Connect leverages all of\nSpark’s optimizations and enhancements. Results are streamed back to the\nclient through g", "question": "What does the Spark Connect API build on?", "answers": {"text": ["Spark’s DataFrame API using unresolved logical plans"], "answer_start": [31]}}
{"context": "rom a client application using the Spark Connect client\nlibrary.\nDownload and start Spark server with Spark Connect\nFirst, download Spark from the\nDownload Apache Spark\npage. Choose the\nlatest release in  the release drop down at the top of the page. Then choose your package type, typically\n“Pre-built for Apache Hadoop 3.3 and later”, and click the link to download.\nNow extract the Spark package you just downloaded on your computer, for example:\ntar\n-xvf\nspark-4.0.0-bin-hadoop3.tgz\nIn a terminal window, go to the\nspark\nfolder in the location where you extracted\nSpark before and run the\nstart-connect-server.sh\nscript to start Spark server with\nSpark Connect, like in this example:\n./sbin/start-connect-server.sh\nMake sure to use the same version  of the package as the Spark version you\ndownlo", "question": "How do you start Spark server with Spark Connect?", "answers": {"text": ["./sbin/start-connect-server.sh"], "answer_start": [688]}}
{"context": "olumns\n)\n>>>\ndf\n.\nshow\n()\n+---+-----+\n|\nid\n|\nname\n|\n+---+-----+\n|\n1\n|\nSarah\n|\n|\n2\n|\nMaria\n|\n+---+-----+\nFor the Scala shell, we use an Ammonite-based REPL. Otherwise, very similar with PySpark shell.\n./bin/spark-shell\n--remote\n\"sc://localhost\"\nA greeting message will appear when the REPL successfully initializes:\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\n\\ \\/\n_\n\\/\n_\n`\n/ __/\n'_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 4.0.0-SNAPSHOT\n      /_/\n\nType in expressions to have them evaluated.\nSpark session available as '\nspark\n'.\nBy default, the REPL will attempt to connect to a local Spark Server.\nRun the following Scala code in the shell to see Spark Connect in action:\n@\nspark\n.\nrange\n(\n10\n).\ncount\nres0\n:\nLong\n=\n10L\nConfigure client-server connection\nBy default, the", "question": "What version of Spark is being used in the example?", "answers": {"text": ["version 4.0.0-SNAPSHOT"], "answer_start": [442]}}
{"context": "\n(\nlogData\n.\nvalue\n.\ncontains\n(\n'\nb\n'\n)).\ncount\n()\nprint\n(\n\"\nLines with a: %i, lines with b: %i\n\"\n%\n(\nnumAs\n,\nnumBs\n))\nspark\n.\nstop\n()\nThis program just counts the number of lines containing ‘a’ and the number containing ‘b’ in a text file.\nNote that you’ll need to replace YOUR_SPARK_HOME with the location where Spark is installed.\nWe can run this application with the regular Python interpreter as follows:\n# Use the Python interpreter to run your application\n$\npython\nSimpleApp\n.\npy\n...\nLines\nwith\na\n:\n72\n,\nlines\nwith\nb\n:\n39\nTo use Spark Connect as part of a Scala application/project, we first need to include the right dependencies.\nUsing the\nsbt\nbuild system as an example, we add the following dependencies to the\nbuild.sbt\nfile:\nlibraryDependencies += \"org.apache.spark\" %% \"spark-connect-cl", "question": "What does the program count in a text file?", "answers": {"text": ["This program just counts the number of lines containing ‘a’ and the number containing ‘b’ in a text file."], "answer_start": [135]}}
{"context": " authentication\nlogic in Spark directly.\nWhat is supported\nPySpark\n: Since Spark 3.4, Spark Connect supports most PySpark APIs, including\nDataFrame\n,\nFunctions\n, and\nColumn\n. However,\nsome APIs such as\nSparkContext\nand\nRDD\nare not supported.\nYou can check which APIs are currently\nsupported in the\nAPI reference\ndocumentation.\nSupported APIs are labeled “Supports Spark Connect” so you can check whether the\nAPIs you are using are available before migrating existing code to Spark Connect.\nScala\n: Since Spark 3.5, Spark Connect supports most Scala APIs, including\nDataset\n,\nfunctions\n,\nColumn\n,\nCatalog\nand\nKeyValueGroupedDataset\n.\nUser-Defined Functions (UDFs) are supported, by default for the shell and in standalone applications with\nadditional set-up requirements.\nMajority of the Streaming API", "question": "Which APIs are labeled to indicate support for Spark Connect?", "answers": {"text": ["Supported APIs are labeled “Supports Spark Connect”"], "answer_start": [327]}}
{"context": ", and all stages of the job.\nThe information that is displayed in this section is\nUser: Current Spark user\nStarted At: The startup time of Spark application\nTotal uptime: Time since Spark application started\nScheduling mode: See\njob scheduling\nNumber of jobs per status: Active, Completed, Failed\nEvent timeline: Displays in chronological order the events related to the executors (added, removed) and the jobs\nDetails of jobs grouped by status: Displays detailed information of the jobs including Job ID, description (with a link to detailed job page), submitted time, duration, stages summary and tasks progress bar\nWhen you click on a specific job, you can see the detailed information of this job.\nJobs detail\nThis page displays the details of a specific job identified by its job ID.\nJob Status:", "question": "What information is displayed regarding jobs grouped by status?", "answers": {"text": ["Displays detailed information of the jobs including Job ID, description (with a link to detailed job page), submitted time, duration, stages summary and tasks progress bar"], "answer_start": [446]}}
{"context": ", you can see the detailed information of this job.\nJobs detail\nThis page displays the details of a specific job identified by its job ID.\nJob Status: (running, succeeded, failed)\nNumber of stages per status (active, pending, completed, skipped, failed)\nAssociated SQL Query: Link to the sql tab for this job\nEvent timeline: Displays in chronological order the events related to the executors (added, removed) and the stages of the job\nDAG visualization: Visual representation of the directed acyclic graph of this job where vertices represent the RDDs or DataFrames and the edges represent an operation to be applied on RDD.\nAn example of DAG visualization for\nsc.parallelize(1 to 100).toDF.count()\nList of stages (grouped by state active, pending, completed, skipped, and failed)\nStage ID\nDescripti", "question": "What does the DAG visualization represent?", "answers": {"text": ["Visual representation of the directed acyclic graph of this job where vertices represent the RDDs or DataFrames and the edges represent an operation to be applied on RDD."], "answer_start": [455]}}
{"context": " Spark application.\nAt the beginning of the page is the summary with the count of all stages by status (active, pending, completed, skipped, and failed)\nIn\nFair scheduling mode\nthere is a table that displays\npools properties\nAfter that are the details of stages per status (active, pending, completed, skipped, failed). In active stages, it’s possible to kill the stage with the kill link. Only in failed stages, failure reason is shown. Task detail can be accessed by clicking on the description.\nStage detail\nThe stage detail page begins with information like total time across all tasks,\nLocality level summary\n,\nShuffle Read Size / Records\nand Associated Job IDs.\nThere is also a visual representation of the directed acyclic graph (DAG) of this stage, where vertices represent the RDDs or DataFr", "question": "What information is shown only in failed stages?", "answers": {"text": ["Only in failed stages, failure reason is shown."], "answer_start": [390]}}
{"context": "sociated Job IDs.\nThere is also a visual representation of the directed acyclic graph (DAG) of this stage, where vertices represent the RDDs or DataFrames and the edges represent an operation to be applied.\nNodes are grouped by operation scope in the DAG visualization and labelled with the operation scope name (BatchScan, WholeStageCodegen, Exchange, etc).\nNotably, Whole Stage Code Generation operations are also annotated with the code generation id. For stages belonging to Spark DataFrame or SQL execution, this allows to cross-reference Stage execution details to the relevant details in the Web-UI SQL Tab page where SQL plan graphs and execution plans are reported.\nSummary metrics for all task are represented in a table and in a timeline.\nTasks deserialization time\nDuration of tasks\n.\nGC ", "question": "What do nodes in the DAG visualization represent?", "answers": {"text": ["vertices represent the RDDs or DataFrames and the edges represent an operation to be applied."], "answer_start": [113]}}
{"context": "tion plans are reported.\nSummary metrics for all task are represented in a table and in a timeline.\nTasks deserialization time\nDuration of tasks\n.\nGC time\nis the total JVM garbage collection time.\nResult serialization time\nis the time spent serializing the task result on an executor before sending it back to the driver.\nGetting result time\nis the time that the driver spends fetching task results from workers.\nScheduler delay\nis the time the task waits to be scheduled for execution.\nPeak execution memory\nis the maximum memory used by the internal data structures created during shuffles, aggregations and joins.\nShuffle Read Size / Records\n. Total shuffle bytes read, includes both data read locally and data read from remote executors.\nShuffle Read Fetch Wait Time\nis the time that tasks spent ", "question": "O que é o tempo de leitura de embaralhamento?", "answers": {"text": ["Total shuffle bytes read, includes both data read locally and data read from remote executors."], "answer_start": [647]}}
{"context": "al shuffle bytes read, includes both data read locally and data read from remote executors.\nShuffle Read Fetch Wait Time\nis the time that tasks spent blocked waiting for shuffle data to be read from remote machines.\nShuffle Remote Reads\nis the total shuffle bytes read from remote executors.\nShuffle Write Time\nis the time that tasks spent writing shuffle data.\nShuffle spill (memory)\nis the size of the deserialized form of the shuffled data in memory.\nShuffle spill (disk)\nis the size of the serialized form of the data on disk.\nAggregated metrics by executor show the same information aggregated by executor.\nAccumulators\nare a type of shared variables. It provides a mutable variable that can be updated inside of a variety of transformations. It is possible to create accumulators with and witho", "question": "What does 'Shuffle Remote Reads' represent?", "answers": {"text": ["is the total shuffle bytes read from remote executors."], "answer_start": [237]}}
{"context": "ables. It provides a mutable variable that can be updated inside of a variety of transformations. It is possible to create accumulators with and without name, but only named accumulators are displayed.\nTasks details basically includes the same information as in the summary section but detailed by task. It also includes links to review the logs and the task attempt number if it fails for any reason. If there are named accumulators, here it is possible to see the accumulator value at the end of each task.\nStorage Tab\nThe Storage tab displays the persisted RDDs and DataFrames, if any, in the application. The summary\npage shows the storage levels, sizes and partitions of all RDDs, and the details page shows the\nsizes and using executors for all partitions in an RDD or DataFrame.\nscala\n>\nimport", "question": "What does the Storage tab display in an application?", "answers": {"text": ["The Storage tab displays the persisted RDDs and DataFrames, if any, in the application."], "answer_start": [521]}}
{"context": " block ‘WholeStageCodegen (1)’ compiles multiple operators (‘LocalTableScan’ and ‘HashAggregate’) together into a single Java\nfunction to improve performance, and metrics like number of rows and spill size are listed in the block.\nThe annotation ‘(1)’ in the block name is the code generation id.\nThe second block ‘Exchange’ shows the metrics on the shuffle exchange, including\nnumber of written shuffle records, total data size, etc.\nClicking the ‘Details’ link on the bottom displays the logical plans and the physical plan, which\nillustrate how Spark parses, analyzes, optimizes and performs the query.\nSteps in the physical plan subject to whole stage code generation optimization, are prefixed by a star followed by\nthe code generation id, for example: ‘*(1) LocalTableScan’\nSQL metrics\nThe metr", "question": "What does the annotation '(1)' in a block name represent?", "answers": {"text": ["The annotation ‘(1)’ in the block name is the code generation id."], "answer_start": [231]}}
{"context": "stage code generation optimization, are prefixed by a star followed by\nthe code generation id, for example: ‘*(1) LocalTableScan’\nSQL metrics\nThe metrics of SQL operators are shown in the block of physical operators. The SQL metrics can be useful\nwhen we want to dive into the execution details of each operator. For example, “number of output rows”\ncan answer how many rows are output after a Filter operator, “shuffle bytes written total” in an Exchange\noperator shows the number of bytes written by a shuffle.\nHere is the list of SQL metrics:\nSQL metrics\nMeaning\nOperators\nnumber of output rows\nthe number of output rows of the operator\nAggregate operators, Join operators, Sample, Range, Scan operators, Filter, etc.\ndata size\nthe size of broadcast/shuffled/collected data of the operator\nBroadca", "question": "O que a métrica \"number of output rows\" pode responder?", "answers": {"text": ["can answer how many rows are output after a Filter operator"], "answer_start": [350]}}
{"context": "operators, Join operators, Sample, Range, Scan operators, Filter, etc.\ndata size\nthe size of broadcast/shuffled/collected data of the operator\nBroadcastExchange, ShuffleExchange, Subquery\ntime to collect\nthe time spent on collecting data\nBroadcastExchange, Subquery\nscan time\nthe time spent on scanning data\nColumnarBatchScan, FileSourceScan\nmetadata time\nthe time spent on getting metadata like number of partitions, number of files\nFileSourceScan\nshuffle bytes written\nthe number of bytes written\nCollectLimit, TakeOrderedAndProject, ShuffleExchange\nshuffle records written\nthe number of records written\nCollectLimit, TakeOrderedAndProject, ShuffleExchange\nshuffle write time\nthe time spent on shuffle writing\nCollectLimit, TakeOrderedAndProject, ShuffleExchange\nremote blocks read\nthe number of bl", "question": "What operators are associated with 'shuffle write time'?", "answers": {"text": ["CollectLimit, TakeOrderedAndProject, ShuffleExchange"], "answer_start": [499]}}
{"context": "Exchange\nshuffle write time\nthe time spent on shuffle writing\nCollectLimit, TakeOrderedAndProject, ShuffleExchange\nremote blocks read\nthe number of blocks read remotely\nCollectLimit, TakeOrderedAndProject, ShuffleExchange\nremote bytes read\nthe number of bytes read remotely\nCollectLimit, TakeOrderedAndProject, ShuffleExchange\nremote bytes read to disk\nthe number of bytes read from remote to local disk\nCollectLimit, TakeOrderedAndProject, ShuffleExchange\nlocal blocks read\nthe number of blocks read locally\nCollectLimit, TakeOrderedAndProject, ShuffleExchange\nlocal bytes read\nthe number of bytes read locally\nCollectLimit, TakeOrderedAndProject, ShuffleExchange\nfetch wait time\nthe time spent on fetching data (local and remote)\nCollectLimit, TakeOrderedAndProject, ShuffleExchange\nrecords read\nth", "question": "What does 'remote bytes read to disk' measure?", "answers": {"text": ["the number of bytes read from remote to local disk"], "answer_start": [353]}}
{"context": "huffleExchange\nfetch wait time\nthe time spent on fetching data (local and remote)\nCollectLimit, TakeOrderedAndProject, ShuffleExchange\nrecords read\nthe number of read records\nCollectLimit, TakeOrderedAndProject, ShuffleExchange\nsort time\nthe time spent on sorting\nSort\npeak memory\nthe peak memory usage in the operator\nSort, HashAggregate\nspill size\nnumber of bytes spilled to disk from memory in the operator\nSort, HashAggregate\ntime in aggregation build\nthe time spent on aggregation\nHashAggregate, ObjectHashAggregate\navg hash probe bucket list iters\nthe average bucket list iterations per lookup during aggregation\nHashAggregate\ndata size of build side\nthe size of built hash map\nShuffledHashJoin\ntime to build hash map\nthe time spent on building hash map\nShuffledHashJoin\ntask commit time\nthe ti", "question": "What does 'sort time' refer to?", "answers": {"text": ["the time spent on sorting"], "answer_start": [238]}}
{"context": "d side\nthe size of built hash map\nShuffledHashJoin\ntime to build hash map\nthe time spent on building hash map\nShuffledHashJoin\ntask commit time\nthe time spent on committing the output of a task after the writes succeed\nany write operation on a file-based table\njob commit time\nthe time spent on committing the output of a job after the writes succeed\nany write operation on a file-based table\ndata sent to Python workers\nthe number of bytes of serialized data sent to the Python workers\nPython UDFs, Pandas UDFs, Pandas Functions API and Python Data Source\ndata returned from Python workers\nthe number of bytes of serialized data received back from the Python workers\nPython UDFs, Pandas UDFS, Pandas Functions API and Python Data Source\nStructured Streaming Tab\nWhen running Structured Streaming job", "question": "What is the time spent on building hash map?", "answers": {"text": ["the time spent on building hash map"], "answer_start": [74]}}
{"context": " arriving.\nProcess Rate.\nThe aggregate (across all sources) rate at which Spark is processing data.\nInput Rows.\nThe aggregate (across all sources) number of records processed in a trigger.\nBatch Duration.\nThe process duration of each batch.\nOperation Duration.\nThe amount of time taken to perform various operations in milliseconds.\nThe tracked operations are listed as follows.\naddBatch: Time taken to read the micro-batch’s input data from the sources, process it, and write the batch’s output to the sink. This should take the bulk of the micro-batch’s time.\ngetBatch: Time taken to prepare the logical query to read the input of the current micro-batch from the sources.\nlatestOffset & getOffset: Time taken to query the maximum available offset for this source.\nqueryPlanning: Time taken to gene", "question": "What does 'addBatch' measure?", "answers": {"text": ["Time taken to read the micro-batch’s input data from the sources, process it, and write the batch’s output to the sink."], "answer_start": [389]}}
{"context": "-batch from the sources.\nlatestOffset & getOffset: Time taken to query the maximum available offset for this source.\nqueryPlanning: Time taken to generates the execution plan.\nwalCommit: Time taken to write the offsets to the metadata log.\nGlobal Watermark Gap.\nThe gap between batch timestamp and global watermark for the batch.\nAggregated Number Of Total State Rows.\nThe aggregated number of total state rows.\nAggregated Number Of Updated State Rows.\nThe aggregated number of updated state rows.\nAggregated State Memory Used In Bytes.\nThe aggregated state memory used in bytes.\nAggregated Number Of State Rows Dropped By Watermark.\nThe aggregated number of state rows dropped by watermark.\nAs an early-release version, the statistics page is still under development and will be improved in\nfuture r", "question": "O que a métrica 'queryPlanning' representa?", "answers": {"text": ["Time taken to generates the execution plan."], "answer_start": [132]}}
{"context": "d logical plan, optimized logical plan and physical plan or errors in the SQL statement.", "question": "What types of plans or errors can be present in SQL processing?", "answers": {"text": ["d logical plan, optimized logical plan and physical plan or errors in the SQL statement."], "answer_start": [0]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nJob Scheduling\nOverview\nScheduling Across Applications\nDynamic Resource Allocation\nConfiguration and Setup\nCaveats\nResource Allocation Policy\nRequest Policy\nRemove Policy\nGraceful Decommission of Executors\nScheduling Within an Application\nFair Schedul", "question": "Quais são alguns dos guias de programação listados no texto?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)"], "answer_start": [46]}}
{"context": "Setup\nCaveats\nResource Allocation Policy\nRequest Policy\nRemove Policy\nGraceful Decommission of Executors\nScheduling Within an Application\nFair Scheduler Pools\nDefault Behavior of Pools\nConfiguring Pool Properties\nScheduling using JDBC Connections\nConcurrent Jobs in PySpark\nOverview\nSpark has several facilities for scheduling resources between computations. First, recall that, as described\nin the\ncluster mode overview\n, each Spark application (instance of SparkContext)\nruns an independent set of executor processes. The cluster managers that Spark runs on provide\nfacilities for\nscheduling across applications\n. Second,\nwithin\neach Spark application, multiple “jobs” (Spark actions) may be running concurrently\nif they were submitted by different threads. This is common if your application is se", "question": "What does Spark provide facilities for?", "answers": {"text": ["Spark has several facilities for scheduling resources between computations."], "answer_start": [283]}}
{"context": "s that don’t set this setting through\nspark.deploy.defaultCores\n.\nFinally, in addition to controlling cores, each application’s\nspark.executor.memory\nsetting controls\nits memory use.\nYARN:\nThe\n--num-executors\noption to the Spark YARN client controls how many executors it will allocate\non the cluster (\nspark.executor.instances\nas configuration property), while\n--executor-memory\n(\nspark.executor.memory\nconfiguration property) and\n--executor-cores\n(\nspark.executor.cores\nconfiguration\nproperty) control the resources per executor. For more information, see the\nYARN Spark Properties\n.\nK8s:\nThe same as the situation with Yarn, please refer to the description of Yarn above. Furthermore,\nSpark on K8s offers higher priority versions of\nspark.kubernetes.executor.limit.cores\nand\nspark.kubernetes.execu", "question": "Which option to the Spark YARN client controls how many executors it will allocate on the cluster?", "answers": {"text": ["--num-executors"], "answer_start": [193]}}
{"context": "hen other\nexecutors attempt to fetch them. In the event of stragglers, which are tasks that run for much\nlonger than their peers, dynamic allocation may remove an executor before the shuffle completes,\nin which case the shuffle files written by that executor must be recomputed unnecessarily.\nThe solution for preserving shuffle files is to use an external shuffle service, also introduced\nin Spark 1.2. This service refers to a long-running process that runs on each node of your cluster\nindependently of your Spark applications and their executors. If the service is enabled, Spark\nexecutors will fetch shuffle files from the service instead of from each other. This means any\nshuffle state written by an executor may continue to be served beyond the executor’s lifetime.\nIn addition to writing shu", "question": "What happens when dynamic allocation removes an executor before the shuffle completes?", "answers": {"text": ["in which case the shuffle files written by that executor must be recomputed unnecessarily."], "answer_start": [202]}}
{"context": "m each other. This means any\nshuffle state written by an executor may continue to be served beyond the executor’s lifetime.\nIn addition to writing shuffle files, executors also cache data either on disk or in memory.\nWhen an executor is removed, however, all cached data will no longer be accessible.  To mitigate this,\nby default executors containing cached data are never removed.  You can configure this behavior with\nspark.dynamicAllocation.cachedExecutorIdleTimeout\n. When set\nspark.shuffle.service.fetch.rdd.enabled\nto\ntrue\n, Spark can use ExternalShuffleService for fetching disk persisted RDD blocks. In case of \ndynamic allocation if this feature is enabled executors having only disk persisted blocks are considered\nidle after\nspark.dynamicAllocation.executorIdleTimeout\nand will be release", "question": "What happens to cached data when an executor is removed?", "answers": {"text": ["When an executor is removed, however, all cached data will no longer be accessible."], "answer_start": [217]}}
{"context": "Spark’s scheduler is fully thread-safe\nand supports this use case to enable applications that serve multiple requests (e.g. queries for\nmultiple users).\nBy default, Spark’s scheduler runs jobs in FIFO fashion. Each job is divided into “stages” (e.g. map and\nreduce phases), and the first job gets priority on all available resources while its stages have tasks to\nlaunch, then the second job gets priority, etc. If the jobs at the head of the queue don’t need to use\nthe whole cluster, later jobs can start to run right away, but if the jobs at the head of the queue are\nlarge, then later jobs may be delayed significantly.\nStarting in Spark 0.8, it is also possible to configure fair sharing between jobs. Under fair sharing,\nSpark assigns tasks between jobs in a “round robin” fashion, so that all ", "question": "Como o Spark scheduler lida com a execução de jobs por padrão?", "answers": {"text": ["By default, Spark’s scheduler runs jobs in FIFO fashion."], "answer_start": [153]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nUsing Spark's \"Hadoop Free\" Build\nSpark uses Hadoop client libraries for HDFS and YARN. Starting in version Spark 1.4, the project packages “Hadoop free” builds that lets you more easily connect a single Spark binary to any Hadoop version. To use thes", "question": "What does Spark package starting in version 1.4?", "answers": {"text": ["Starting in version Spark 1.4, the project packages “Hadoop free” builds that lets you more easily connect a single Spark binary to any Hadoop version."], "answer_start": [637]}}
{"context": "ersion Spark 1.4, the project packages “Hadoop free” builds that lets you more easily connect a single Spark binary to any Hadoop version. To use these builds, you need to modify\nSPARK_DIST_CLASSPATH\nto include Hadoop’s package jars. The most convenient place to do this is by adding an entry in\nconf/spark-env.sh\n.\nThis page describes how to connect Spark to Hadoop for different types of distributions.\nApache Hadoop\nFor Apache distributions, you can use Hadoop’s ‘classpath’ command. For instance:\n### in conf/spark-env.sh ###\n# If 'hadoop' binary is on your PATH\nexport\nSPARK_DIST_CLASSPATH\n=\n$(\nhadoop classpath\n)\n# With explicit path to 'hadoop' binary\nexport\nSPARK_DIST_CLASSPATH\n=\n$(\n/path/to/hadoop/bin/hadoop classpath\n)\n# Passing a Hadoop configuration directory\nexport\nSPARK_DIST_CLASSPAT", "question": "How can you connect a single Spark binary to any Hadoop version?", "answers": {"text": ["the project packages “Hadoop free” builds that lets you more easily connect a single Spark binary to any Hadoop version."], "answer_start": [18]}}
{"context": "ME\n/bin:\n$HADOOP_HOME\n/bin:\n$PATH\n\"\n...\n#Copy your target hadoop binaries to the executor hadoop home\nCOPY /opt/hadoop3\n$HADOOP_HOME\n...\n#Copy and use the Spark provided entrypoint.sh. It sets your SPARK_DIST_CLASSPATH using the hadoop binary in $HADOOP_HOME and starts the executor. If you choose to customize the value of SPARK_DIST_CLASSPATH here, the value will be retained in entrypoint.sh\nENTRYPOINT\n[\n\"/opt/entrypoint.sh\"\n]\n...", "question": "Where are the target Hadoop binaries copied to?", "answers": {"text": ["$HADOOP_HOME"], "answer_start": [9]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nBuilding Spark\nBuilding Apache Spark\nApache Maven\nSetting up Maven’s Memory Usage\nbuild/mvn\nBuilding a Runnable Distribution\nSpecifying the Hadoop Version and Enabling YARN\nBuilding With Hive and JDBC Support\nPackaging without Hadoop Dependencies for ", "question": "Quais são algumas das opções de implantação do Spark listadas no texto?", "answers": {"text": ["Spark Standalone\nYARN\nKubernetes"], "answer_start": [353]}}
{"context": "a Runnable Distribution\nSpecifying the Hadoop Version and Enabling YARN\nBuilding With Hive and JDBC Support\nPackaging without Hadoop Dependencies for YARN\nBuilding with Kubernetes support\nBuilding submodules individually\nBuilding with JVM Profile support\nContinuous Compilation\nBuilding with SBT\nSetting up SBT’s Memory Usage\nSpeeding up Compilation\nEncrypted Filesystems\nIntelliJ IDEA or Eclipse\nRunning Tests\nTesting with SBT\nRunning Individual Tests\nPySpark pip installable\nPySpark Tests with Maven or SBT\nRunning R Tests (deprecated)\nRunning Docker-based Integration Test Suites\nBuilding and testing on an IPv6-only environment\nBuilding with a user-defined\nprotoc\nBuilding Apache Spark\nApache Maven\nThe Maven-based build is the build of reference for Apache Spark.\nBuilding Spark using Maven requ", "question": "What is the build of reference for Apache Spark?", "answers": {"text": ["The Maven-based build is the build of reference for Apache Spark."], "answer_start": [703]}}
{"context": "er-defined\nprotoc\nBuilding Apache Spark\nApache Maven\nThe Maven-based build is the build of reference for Apache Spark.\nBuilding Spark using Maven requires Maven 3.9.9 and Java 17/21.\nSpark requires Scala 2.13; support for Scala 2.12 was removed in Spark 4.0.0.\nSetting up Maven’s Memory Usage\nYou’ll need to configure Maven to use more memory than usual by setting\nMAVEN_OPTS\n:\nexport\nMAVEN_OPTS\n=\n\"-Xss64m -Xmx2g -XX:ReservedCodeCacheSize=1g\"\n(The\nReservedCodeCacheSize\nsetting is optional but recommended.)\nIf you don’t add these parameters to\nMAVEN_OPTS\n, you may see errors and warnings like the following:\n[INFO] Compiling 203 Scala sources and 9 Java sources to /Users/me/Development/spark/core/target/scala-2.13/classes...\n[ERROR] Java heap space -> [Help 1]\nYou can fix these problems by sett", "question": "Quais versões de Java são necessárias para construir o Spark usando Maven?", "answers": {"text": ["Building Spark using Maven requires Maven 3.9.9 and Java 17/21."], "answer_start": [119]}}
{"context": "ally download and setup all necessary build requirements (\nMaven\n,\nScala\n) locally within the\nbuild/\ndirectory itself. It honors any\nmvn\nbinary if present already, however, will pull down its own copy of Scala regardless to ensure proper version requirements are met.\nbuild/mvn\nexecution acts as a pass through to the\nmvn\ncall allowing easy transition from previous build methods. As an example, one can build a version of Spark as follows:\n./build/mvn -DskipTests clean package\nOther build examples can be found below.\nBuilding a Runnable Distribution\nTo create a Spark distribution like those distributed by the\nSpark Downloads\npage, and that is laid out so as\nto be runnable, use\n./dev/make-distribution.sh\nin the project root directory. It can be configured\nwith Maven profile settings and so on ", "question": "How can a Spark version be built?", "answers": {"text": ["./build/mvn -DskipTests clean package"], "answer_start": [441]}}
{"context": "id out so as\nto be runnable, use\n./dev/make-distribution.sh\nin the project root directory. It can be configured\nwith Maven profile settings and so on like the direct Maven build. Example:\n./dev/make-distribution.sh --name custom-spark --pip --r --tgz -Psparkr -Phive -Phive-thriftserver -Pyarn -Pkubernetes\nThis will build Spark distribution along with Python pip and R packages. For more information on usage, run\n./dev/make-distribution.sh --help\nSpecifying the Hadoop Version and Enabling YARN\nYou can enable the\nyarn\nprofile and specify the exact version of Hadoop to compile against through the\nhadoop.version\nproperty.\nExample:\n./build/mvn -Pyarn -Dhadoop.version=3.4.1 -DskipTests clean package\nBuilding With Hive and JDBC Support\nTo enable Hive integration for Spark SQL along with its JDBC s", "question": "How can you build a Spark distribution with Python pip and R packages?", "answers": {"text": ["./dev/make-distribution.sh --name custom-spark --pip --r --tgz -Psparkr -Phive -Phive-thriftserver -Pyarn -Pkubernetes"], "answer_start": [188]}}
{"context": "rn -Dhadoop.version=3.4.1 -DskipTests clean package\nBuilding With Hive and JDBC Support\nTo enable Hive integration for Spark SQL along with its JDBC server and CLI,\nadd the\n-Phive\nand\n-Phive-thriftserver\nprofiles to your existing build options.\nBy default Spark will build with Hive 2.3.10.\n# With Hive 2.3.10 support\n./build/mvn -Pyarn -Phive -Phive-thriftserver -DskipTests clean package\nPackaging without Hadoop Dependencies for YARN\nThe assembly directory produced by\nmvn package\nwill, by default, include all of Spark’s\ndependencies, including Hadoop and some of its ecosystem projects. On YARN deployments, this\ncauses multiple versions of these to appear on executor classpaths: the version packaged in\nthe Spark assembly and the version on each node, included with\nyarn.application.classpath\n", "question": "How can Hive integration be enabled for Spark SQL?", "answers": {"text": ["add the\n-Phive\nand\n-Phive-thriftserver\nprofiles to your existing build options."], "answer_start": [165]}}
{"context": "e to appear on executor classpaths: the version packaged in\nthe Spark assembly and the version on each node, included with\nyarn.application.classpath\n.\nThe\nhadoop-provided\nprofile builds the assembly without including Hadoop-ecosystem projects,\nlike ZooKeeper and Hadoop itself.\nBuilding with Kubernetes support\n./build/mvn -Pkubernetes -DskipTests clean package\nBuilding submodules individually\nIt’s possible to build Spark submodules using the\nmvn -pl\noption.\nFor instance, you can build the Spark Streaming module using:\n./build/mvn -pl :spark-streaming_2.13 clean install\nwhere\nspark-streaming_2.13\nis the\nartifactId\nas defined in\nstreaming/pom.xml\nfile.\nBuilding with JVM Profile support\n./build/mvn -Pjvm-profiler -DskipTests clean package\nNote:\nThe\njvm-profiler\nprofile builds the assembly wit", "question": "How can you build the Spark Streaming module?", "answers": {"text": ["./build/mvn -pl :spark-streaming_2.13 clean install"], "answer_start": [524]}}
{"context": "ml\nfile.\nBuilding with JVM Profile support\n./build/mvn -Pjvm-profiler -DskipTests clean package\nNote:\nThe\njvm-profiler\nprofile builds the assembly without including the dependency\nap-loader\n,\nyou can download it manually from maven central repo and use it together with\nspark-profiler_2.13\n.\nContinuous Compilation\nWe use the scala-maven-plugin which supports incremental and continuous compilation. E.g.\n./build/mvn scala:cc\nshould run continuous compilation (i.e. wait for changes). However, this has not been tested\nextensively. A couple of gotchas to note:\nit only scans the paths\nsrc/main\nand\nsrc/test\n(see\ndocs\n), so it will only work\nfrom within certain submodules that have that structure.\nyou’ll typically need to run\nmvn install\nfrom the project root for compilation within\nspecific submodu", "question": "How can continuous compilation be run using the scala-maven-plugin?", "answers": {"text": ["./build/mvn scala:cc"], "answer_start": [405]}}
{"context": "ion about how to run individual tests, refer to the\nUseful Developer Tools page\n.\nPySpark pip installable\nIf you are building Spark for use in a Python environment and you wish to pip install it, you will first need to build the Spark JARs as described above. Then you can construct an sdist package suitable for setup.py and pip installable package.\ncd python; python packaging/classic/setup.py sdist\nNote:\nDue to packaging requirements you can not directly pip install from the Python directory, rather you must first build the sdist package as described above.\nAlternatively, you can also run\nmake-distribution.sh\nwith the\n--pip\noption.\nPySpark Tests with Maven or SBT\nIf you are building PySpark and wish to run the PySpark tests you will need to build Spark with Hive support.\n./build/mvn -Dskip", "question": "How can you create a pip installable package for PySpark after building the Spark JARs?", "answers": {"text": ["cd python; python packaging/classic/setup.py sdist"], "answer_start": [351]}}
{"context": "sts with Maven or SBT\nIf you are building PySpark and wish to run the PySpark tests you will need to build Spark with Hive support.\n./build/mvn -DskipTests clean package -Phive\n./python/run-tests\nIf you are building PySpark with SBT and wish to run the PySpark tests, you will need to build Spark with Hive support and also build the test components:\n./build/sbt -Phive clean package\n./build/sbt test:compile\n./python/run-tests\nThe run-tests script also can be limited to a specific Python version or a specific module\n./python/run-tests --python-executables=python --modules=pyspark-sql\nRunning R Tests (deprecated)\nTo run the SparkR tests you will need to install the\nknitr\n,\nrmarkdown\n,\ntestthat\n,\ne1071\nand\nsurvival\npackages first:\nRscript -e \"install.packages(c('knitr', 'rmarkdown', 'devtools',", "question": "How do you run PySpark tests when building with SBT?", "answers": {"text": ["./build/sbt -Phive clean package\n./build/sbt test:compile\n./python/run-tests"], "answer_start": [351]}}
{"context": "need to install the\nknitr\n,\nrmarkdown\n,\ntestthat\n,\ne1071\nand\nsurvival\npackages first:\nRscript -e \"install.packages(c('knitr', 'rmarkdown', 'devtools', 'testthat', 'e1071', 'survival'), repos='https://cloud.r-project.org/')\"\nYou can run just the SparkR tests using the command:\n./R/run-tests.sh\nRunning Docker-based Integration Test Suites\nIn order to run Docker integration tests, you have to install the\ndocker\nengine on your box.\nThe instructions for installation can be found at\nthe Docker site\n.\nOnce installed, the\ndocker\nservice needs to be started, if not already running.\nOn Linux, this can be done by\nsudo service docker start\n.\n./build/mvn install -DskipTests\n./build/mvn test -Pdocker-integration-tests -pl :spark-docker-integration-tests_2.13\nor\n./build/sbt docker-integration-tests/test\n", "question": "How can you start the docker service on Linux?", "answers": {"text": ["sudo service docker start"], "answer_start": [610]}}
{"context": "install -DskipTests\n./build/mvn test -Pdocker-integration-tests -pl :spark-docker-integration-tests_2.13\nor\n./build/sbt docker-integration-tests/test\nBuilding and testing on an IPv6-only environment\nUse Apache Spark GitBox URL because GitHub doesn’t support IPv6 yet.\nhttps://gitbox.apache.org/repos/asf/spark.git\nTo build and run tests on IPv6-only environment, the following configurations are required.\nexport\nSPARK_LOCAL_HOSTNAME\n=\n\"your-IPv6-address\"\n# e.g. '[2600:1700:232e:3de0:...]'\nexport\nDEFAULT_ARTIFACT_REPOSITORY\n=\nhttps://ipv6.repo1.maven.org/maven2/\nexport\nMAVEN_OPTS\n=\n\"-Djava.net.preferIPv6Addresses=true\"\nexport\nSBT_OPTS\n=\n\"-Djava.net.preferIPv6Addresses=true\"\nexport\nSERIAL_SBT_TESTS\n=\n1\nBuilding with a user-defined\nprotoc\nWhen the user cannot use the official\nprotoc\nbinary files", "question": "What configurations are required to build and run tests on an IPv6-only environment?", "answers": {"text": ["export\nSPARK_LOCAL_HOSTNAME\n=\n\"your-IPv6-address\"\n# e.g. '[2600:1700:232e:3de0:...]'\nexport\nDEFAULT_ARTIFACT_REPOSITORY\n=\nhttps://ipv6.repo1.maven.org/maven2/\nexport\nMAVEN_OPTS\n=\n\"-Djava.net.preferIPv6Addresses=true\"\nexport\nSBT_OPTS\n=\n\"-Djava.net.preferIPv6Addresses=true\"\nexport\nSERIAL_SBT_TESTS\n=\n1"], "answer_start": [406]}}
{"context": "et.preferIPv6Addresses=true\"\nexport\nSERIAL_SBT_TESTS\n=\n1\nBuilding with a user-defined\nprotoc\nWhen the user cannot use the official\nprotoc\nbinary files to build the\ncore\nmodule in the compilation environment, for example, compiling\ncore\nmodule on CentOS 6 or CentOS 7 which the default\nglibc\nversion is less than 2.14, we can try to compile and test by specifying the user-defined\nprotoc\nbinary files as follows:\nexport\nSPARK_PROTOC_EXEC_PATH\n=\n/path-to-protoc-exe\n./build/mvn\n-Puser-defined-protoc\n-DskipDefaultProtoc\nclean package\nor\nexport\nSPARK_PROTOC_EXEC_PATH\n=\n/path-to-protoc-exe\n./build/sbt\n-Puser-defined-protoc\nclean package\nThe user-defined\nprotoc\nbinary files can be produced in the user’s compilation environment by source code compilation, for compilation steps, please refer to\nprotobu", "question": "How can you build the core module when the official protoc binary files cannot be used?", "answers": {"text": ["export\nSPARK_PROTOC_EXEC_PATH\n=\n/path-to-protoc-exe\n./build/mvn\n-Puser-defined-protoc\n-DskipDefaultProtoc\nclean package"], "answer_start": [412]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)"], "answer_start": [46]}}
{"context": "ures\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nData sources\nIn this section, we introduce how to use data source in ML to load data.\nBesides some general data sources such as Parquet, CSV, JSON and JDBC, we also provide some specific data sources for ML.\nTable of Contents\nImage data source\nLIBSVM data source\nImage data source\nThis image data source is used to load image files from a directory, it can load compressed image (jpeg, png, etc", "question": "What types of general data sources are available in ML for loading data?", "answers": {"text": ["Parquet, CSV, JSON and JDBC"], "answer_start": [534]}}
{"context": "LIBSVM data source\nImage data source\nThis image data source is used to load image files from a directory, it can load compressed image (jpeg, png, etc.) into raw image representation via\nImageIO\nin Java library.\nThe loaded DataFrame has one\nStructType\ncolumn: “image”, containing image data stored as image schema.\nThe schema of the\nimage\ncolumn is:\norigin:\nStringType\n(represents the file path of the image)\nheight:\nIntegerType\n(height of the image)\nwidth:\nIntegerType\n(width of the image)\nnChannels:\nIntegerType\n(number of image channels)\nmode:\nIntegerType\n(OpenCV-compatible type)\ndata:\nBinaryType\n(Image bytes in OpenCV-compatible order: row-wise BGR in most cases)\nIn PySpark we provide Spark SQL data source API for loading image data as a DataFrame.\n>>>\ndf\n=\nspark\n.\nread\n.\nformat\n(\n\"\nimage\n\"\n", "question": "What data type represents the file path of the image in the image column schema?", "answers": {"text": ["StringType"], "answer_start": [358]}}
{"context": " BGR in most cases)\nIn PySpark we provide Spark SQL data source API for loading image data as a DataFrame.\n>>>\ndf\n=\nspark\n.\nread\n.\nformat\n(\n\"\nimage\n\"\n).\noption\n(\n\"\ndropInvalid\n\"\n,\nTrue\n).\nload\n(\n\"\ndata/mllib/images/origin/kittens\n\"\n)\n>>>\ndf\n.\nselect\n(\n\"\nimage.origin\n\"\n,\n\"\nimage.width\n\"\n,\n\"\nimage.height\n\"\n).\nshow\n(\ntruncate\n=\nFalse\n)\n+-----------------------------------------------------------------------+-----+------+\n|\norigin\n|\nwidth\n|\nheight\n|\n+-----------------------------------------------------------------------+-----+------+\n|\nfile\n:\n///\nspark\n/\ndata\n/\nmllib\n/\nimages\n/\norigin\n/\nkittens\n/\n54893.j\npg\n|\n300\n|\n311\n|\n|\nfile\n:\n///\nspark\n/\ndata\n/\nmllib\n/\nimages\n/\norigin\n/\nkittens\n/\nDP802813\n.\njpg\n|\n199\n|\n313\n|\n|\nfile\n:\n///\nspark\n/\ndata\n/\nmllib\n/\nimages\n/\norigin\n/\nkittens\n/\n29.5\n.\na_b_EGDP02", "question": "What is the format used for loading image data as a DataFrame in PySpark?", "answers": {"text": ["image"], "answer_start": [80]}}
{"context": "eight|\n+-----------------------------------------------------------------------+-----+------+\n|file:///spark/data/mllib/images/origin/kittens/54893.jpg               |300  |311   |\n|file:///spark/data/mllib/images/origin/kittens/DP802813.jpg            |199  |313   |\n|file:///spark/data/mllib/images/origin/kittens/29.5.a_b_EGDP022204.jpg |300  |200   |\n|file:///spark/data/mllib/images/origin/kittens/DP153539.jpg            |300  |296   |\n+-----------------------------------------------------------------------+-----+------+\n*/\nIn SparkR we provide Spark SQL data source API for loading image data as a DataFrame.\n>\ndf\n=\nread.df\n(\n\"data/mllib/images/origin/kittens\"\n,\n\"image\"\n)\n>\nhead\n(\nselect\n(\ndf\n,\ndf\n$\nimage.origin\n,\ndf\n$\nimage.width\n,\ndf\n$\nimage.height\n))\n1\nfile\n:///\nspark\n/\ndata\n/\nmllib\n/\n", "question": "What is the width of the image file file:///spark/data/mllib/images/origin/kittens/54893.jpg?", "answers": {"text": ["300"], "answer_start": [167]}}
{"context": "ges/origin/kittens\"\n,\n\"image\"\n)\n>\nhead\n(\nselect\n(\ndf\n,\ndf\n$\nimage.origin\n,\ndf\n$\nimage.width\n,\ndf\n$\nimage.height\n))\n1\nfile\n:///\nspark\n/\ndata\n/\nmllib\n/\nimages\n/\norigin\n/\nkittens\n/\n54893\n.jpg\n2\nfile\n:///\nspark\n/\ndata\n/\nmllib\n/\nimages\n/\norigin\n/\nkittens\n/\nDP802813.jpg\n3\nfile\n:///\nspark\n/\ndata\n/\nmllib\n/\nimages\n/\norigin\n/\nkittens\n/\n29.5\n.a_b_EGDP022204.jpg\n4\nfile\n:///\nspark\n/\ndata\n/\nmllib\n/\nimages\n/\norigin\n/\nkittens\n/\nDP153539.jpg\nwidth\nheight\n1\n300\n311\n2\n199\n313\n3\n300\n200\n4\n300\n296\nLIBSVM data source\nThis\nLIBSVM\ndata source is used to load ‘libsvm’ type files from a directory.\nThe loaded DataFrame has two columns: label containing labels stored as doubles and features containing feature vectors stored as Vectors.\nThe schemas of the columns are:\nlabel:\nDoubleType\n(represents the instance label)\n", "question": "What two columns does the DataFrame have when loading ‘libsvm’ type files from a directory?", "answers": {"text": ["label containing labels stored as doubles and features containing feature vectors stored as Vectors."], "answer_start": [617]}}
{"context": " doubles and features containing feature vectors stored as Vectors.\nThe schemas of the columns are:\nlabel:\nDoubleType\n(represents the instance label)\nfeatures:\nVectorUDT\n(represents the feature vector)\nIn PySpark we provide Spark SQL data source API for loading\nLIBSVM\ndata as a DataFrame.\n>>>\ndf\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\noption\n(\n\"\nnumFeatures\n\"\n,\n\"\n780\n\"\n).\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\n>>>\ndf\n.\nshow\n(\n10\n)\n+-----+--------------------+\n|\nlabel\n|\nfeatures\n|\n+-----+--------------------+\n|\n0.0\n|\n(\n780\n,[\n127\n,\n128\n,\n129.\n..\n|\n|\n1.0\n|\n(\n780\n,[\n158\n,\n159\n,\n160.\n..\n|\n|\n1.0\n|\n(\n780\n,[\n124\n,\n125\n,\n126.\n..\n|\n|\n1.0\n|\n(\n780\n,[\n152\n,\n153\n,\n154.\n..\n|\n|\n1.0\n|\n(\n780\n,[\n151\n,\n152\n,\n153.\n..\n|\n|\n0.0\n|\n(\n780\n,[\n129\n,\n130\n,\n131.\n..\n|\n|\n1.0\n|\n(\n780\n,[\n158\n,\n159\n,\n160.\n..\n|\n|\n1.0", "question": "What data type represents the feature vector in the columns?", "answers": {"text": ["VectorUDT"], "answer_start": [160]}}
{"context": "80\n,[\n152\n,\n153\n,\n154.\n..\n|\n|\n1.0\n|\n(\n780\n,[\n151\n,\n152\n,\n153.\n..\n|\n|\n0.0\n|\n(\n780\n,[\n129\n,\n130\n,\n131.\n..\n|\n|\n1.0\n|\n(\n780\n,[\n158\n,\n159\n,\n160.\n..\n|\n|\n1.0\n|\n(\n780\n,[\n99\n,\n100\n,\n101\n,...\n|\n|\n0.0\n|\n(\n780\n,[\n154\n,\n155\n,\n156.\n..\n|\n|\n0.0\n|\n(\n780\n,[\n127\n,\n128\n,\n129.\n..\n|\n+-----+--------------------+\nonly\nshowing\ntop\n10\nrows\nLibSVMDataSource\nimplements a Spark SQL data source API for loading\nLIBSVM\ndata as a DataFrame.\nscala\n>\nval\ndf\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\noption\n(\n\"numFeatures\"\n,\n\"780\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\ndf\n:\norg.apache.spark.sql.DataFrame\n=\n[\nlabel:\ndouble\n,\nfeatures:\nvector\n]\nscala\n>\ndf\n.\nshow\n(\n10\n)\n+-----+--------------------+\n|\nlabel\n|\nfeatures\n|\n+-----+--------------------+\n|\n0.0\n|(\n780\n,[\n127\n,\n128\n,\n129\n...|\n|\n1\n.\n0\n|\n(\n780\n,\n[\n158\n,\n159\n,\n160\n...|", "question": "What data source API does LibSVMDataSource implement?", "answers": {"text": ["a Spark SQL data source API"], "answer_start": [344]}}
{"context": "+--------------------+\n|\nlabel\n|\nfeatures\n|\n+-----+--------------------+\n|\n0.0\n|(\n780\n,[\n127\n,\n128\n,\n129\n...|\n|\n1\n.\n0\n|\n(\n780\n,\n[\n158\n,\n159\n,\n160\n...|\n|\n1\n.\n0\n|\n(\n780\n,\n[\n124\n,\n125\n,\n126\n...|\n|\n1\n.\n0\n|\n(\n780\n,\n[\n152\n,\n153\n,\n154\n...|\n|\n1\n.\n0\n|\n(\n780\n,\n[\n151\n,\n152\n,\n153\n...|\n|\n0\n.\n0\n|\n(\n780\n,\n[\n129\n,\n130\n,\n131\n...|\n|\n1\n.\n0\n|\n(\n780\n,\n[\n158\n,\n159\n,\n160\n...|\n|\n1\n.\n0\n|\n(\n780\n,\n[\n99\n,\n100\n,\n101\n,\n...|\n|\n0\n.\n0\n|\n(\n780\n,\n[\n154\n,\n155\n,\n156\n...|\n|\n0\n.\n0\n|\n(\n780\n,\n[\n127\n,\n128\n,\n129\n...|\n+-----+--------------------+\nonly\nshowing\ntop\n10\nrows\nLibSVMDataSource\nimplements Spark SQL data source API for loading\nLIBSVM\ndata as a DataFrame.\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\noption\n(\n\"numFeatures\"\n,\n\"780\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n);\ndf\n.\nshow\n(\n10\n);\n/*\nWill output:\n+", "question": "What format does LibSVMDataSource implement to load data as a DataFrame?", "answers": {"text": ["Spark SQL data source API"], "answer_start": [562]}}
{"context": "ata as a DataFrame.\n>\ndf\n=\nread.df\n(\n\"data/mllib/sample_libsvm_data.txt\"\n,\n\"libsvm\"\n)\n>\nhead\n(\nselect\n(\ndf\n,\ndf\n$\nlabel\n,\ndf\n$\nfeatures\n),\n10\n)\nlabel\nfeatures\n1\n0\n<\nenvironment\n:\n0x7fe6d35366e8\n>\n2\n1\n<\nenvironment\n:\n0x7fe6d353bf78\n>\n3\n1\n<\nenvironment\n:\n0x7fe6d3541840\n>\n4\n1\n<\nenvironment\n:\n0x7fe6d3545108\n>\n5\n1\n<\nenvironment\n:\n0x7fe6d354c8e0\n>\n6\n0\n<\nenvironment\n:\n0x7fe6d35501a8\n>\n7\n1\n<\nenvironment\n:\n0x7fe6d3555a70\n>\n8\n1\n<\nenvironment\n:\n0x7fe6d3559338\n>\n9\n0\n<\nenvironment\n:\n0x7fe6d355cc00\n>\n10\n0\n<\nenvironment\n:\n0x7fe6d35643d8\n>", "question": "What file is read using read.df?", "answers": {"text": ["\"data/mllib/sample_libsvm_data.txt\""], "answer_start": [37]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available for Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)"], "answer_start": [46]}}
{"context": "ures\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nML Pipelines\n\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\wv}{\\mathbf{w}}\n\\newcommand{\\av}{\\mathbf{\\alpha}}\n\\newcommand{\\bv}{\\mathbf{b}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\id}{\\mathbf{I}}\n\\newcommand{\\ind}{\\mathbf{1}}\n\\newcommand{\\0}{\\mathbf{0}}\n\\newcommand{\\unit}{\\mathbf{e}}\n\\newcommand{\\one}{\\mathbf{1}", "question": "Quais são alguns dos tópicos avançados abordados no texto?", "answers": {"text": ["Advanced topics"], "answer_start": [121]}}
{"context": "\ns in the original\nPipeline\nhave become\nTransformer\ns.\nWhen the\nPipelineModel\n’s\ntransform()\nmethod is called on a test dataset, the data are passed\nthrough the fitted pipeline in order.\nEach stage’s\ntransform()\nmethod updates the dataset and passes it to the next stage.\nPipeline\ns and\nPipelineModel\ns help to ensure that training and test data go through identical feature processing steps.\nDetails\nDAG\nPipeline\ns\n: A\nPipeline\n’s stages are specified as an ordered array.  The examples given here are all for linear\nPipeline\ns, i.e.,\nPipeline\ns in which each stage uses data produced by the previous stage.  It is possible to create non-linear\nPipeline\ns as long as the data flow graph forms a Directed Acyclic Graph (DAG).  This graph is currently specified implicitly based on the input and outpu", "question": "What type of graph is formed by the data flow in non-linear Pipelines?", "answers": {"text": ["Directed Acyclic Graph (DAG)."], "answer_start": [696]}}
{"context": "ithms with the\nmaxIter\nparameter in a\nPipeline\n.\nML persistence: Saving and Loading Pipelines\nOften times it is worth it to save a model or a pipeline to disk for later use. In Spark 1.6, a model import/export functionality was added to the Pipeline API.\nAs of Spark 2.3, the DataFrame-based API in\nspark.ml\nand\npyspark.ml\nhas complete coverage.\nML persistence works across Scala, Java and Python.  However, R currently uses a modified format,\nso models saved in R can only be loaded back in R; this should be fixed in the future and is\ntracked in\nSPARK-15572\n.\nBackwards compatibility for ML persistence\nIn general, MLlib maintains backwards compatibility for ML persistence.  I.e., if you save an ML\nmodel or Pipeline in one version of Spark, then you should be able to load it back and use it in a", "question": "What was added to the Pipeline API in Spark 1.6?", "answers": {"text": ["a model import/export functionality was added to the Pipeline API."], "answer_start": [188]}}
{"context": "bility for ML persistence.  I.e., if you save an ML\nmodel or Pipeline in one version of Spark, then you should be able to load it back and use it in a\nfuture version of Spark.  However, there are rare exceptions, described below.\nModel persistence: Is a model or Pipeline saved using Apache Spark ML persistence in Spark\nversion X loadable by Spark version Y?\nMajor versions: No guarantees, but best-effort.\nMinor and patch versions: Yes; these are backwards compatible.\nNote about the format: There are no guarantees for a stable persistence format, but model loading itself is designed to be backwards compatible.\nModel behavior: Does a model or Pipeline in Spark version X behave identically in Spark version Y?\nMajor versions: No guarantees, but best-effort.\nMinor and patch versions: Identical b", "question": "What guarantees are there for loading a model saved using Apache Spark ML persistence in Spark version X into Spark version Y with respect to major versions?", "answers": {"text": ["No guarantees, but best-effort."], "answer_start": [376]}}
{"context": "peline in Spark version X behave identically in Spark version Y?\nMajor versions: No guarantees, but best-effort.\nMinor and patch versions: Identical behavior, except for bug fixes.\nFor both model persistence and model behavior, any breaking changes across a minor version or patch\nversion are reported in the Spark version release notes. If a breakage is not reported in release\nnotes, then it should be treated as a bug to be fixed.\nCode examples\nThis section gives code examples illustrating the functionality discussed above.\nFor more info, please refer to the API documentation\n(\nPython\n,\nScala\n,\nand\nJava\n).\nExample: Estimator, Transformer, and Param\nThis example covers the concepts of\nEstimator\n,\nTransformer\n, and\nParam\n.\nRefer to the\nEstimator\nPython docs\n,\nthe\nTransformer\nPython docs\nand\nt", "question": "What guarantees are there regarding behavior across minor and patch versions in Spark?", "answers": {"text": ["Minor and patch versions: Identical behavior, except for bug fixes."], "answer_start": [113]}}
{"context": "Param\nThis example covers the concepts of\nEstimator\n,\nTransformer\n, and\nParam\n.\nRefer to the\nEstimator\nPython docs\n,\nthe\nTransformer\nPython docs\nand\nthe\nParams\nPython docs\nfor more details on the API.\nfrom\npyspark.ml.linalg\nimport\nVectors\nfrom\npyspark.ml.classification\nimport\nLogisticRegression\n# Prepare training data from a list of (label, features) tuples.\ntraining\n=\nspark\n.\ncreateDataFrame\n([\n(\n1.0\n,\nVectors\n.\ndense\n([\n0.0\n,\n1.1\n,\n0.1\n])),\n(\n0.0\n,\nVectors\n.\ndense\n([\n2.0\n,\n1.0\n,\n-\n1.0\n])),\n(\n0.0\n,\nVectors\n.\ndense\n([\n2.0\n,\n1.3\n,\n1.0\n])),\n(\n1.0\n,\nVectors\n.\ndense\n([\n0.0\n,\n1.2\n,\n-\n0.5\n]))],\n[\n\"\nlabel\n\"\n,\n\"\nfeatures\n\"\n])\n# Create a LogisticRegression instance. This instance is an Estimator.\nlr\n=\nLogisticRegression\n(\nmaxIter\n=\n10\n,\nregParam\n=\n0.01\n)\n# Print out the parameters, documentation, a", "question": "What is created using LogisticRegression?", "answers": {"text": ["This instance is an Estimator."], "answer_start": [666]}}
{"context": "rint\n(\nmodel1\n.\nextractParamMap\n())\n# We may alternatively specify parameters using a Python dictionary as a paramMap\nparamMap\n=\n{\nlr\n.\nmaxIter\n:\n20\n}\nparamMap\n[\nlr\n.\nmaxIter\n]\n=\n30\n# Specify 1 Param, overwriting the original maxIter.\n# Specify multiple Params.\nparamMap\n.\nupdate\n({\nlr\n.\nregParam\n:\n0.1\n,\nlr\n.\nthreshold\n:\n0.55\n})\n# type: ignore\n# You can combine paramMaps, which are python dictionaries.\n# Change output column name\nparamMap2\n=\n{\nlr\n.\nprobabilityCol\n:\n\"\nmyProbability\n\"\n}\nparamMapCombined\n=\nparamMap\n.\ncopy\n()\nparamMapCombined\n.\nupdate\n(\nparamMap2\n)\n# type: ignore\n# Now learn a new model using the paramMapCombined parameters.\n# paramMapCombined overrides all parameters set earlier via lr.set* methods.\nmodel2\n=\nlr\n.\nfit\n(\ntraining\n,\nparamMapCombined\n)\nprint\n(\n\"\nModel 2 was fit us", "question": "How can parameters be specified using a Python dictionary?", "answers": {"text": ["We may alternatively specify parameters using a Python dictionary as a paramMap"], "answer_start": [38]}}
{"context": "amMapCombined overrides all parameters set earlier via lr.set* methods.\nmodel2\n=\nlr\n.\nfit\n(\ntraining\n,\nparamMapCombined\n)\nprint\n(\n\"\nModel 2 was fit using parameters:\n\"\n)\nprint\n(\nmodel2\n.\nextractParamMap\n())\n# Prepare test data\ntest\n=\nspark\n.\ncreateDataFrame\n([\n(\n1.0\n,\nVectors\n.\ndense\n([\n-\n1.0\n,\n1.5\n,\n1.3\n])),\n(\n0.0\n,\nVectors\n.\ndense\n([\n3.0\n,\n2.0\n,\n-\n0.1\n])),\n(\n1.0\n,\nVectors\n.\ndense\n([\n0.0\n,\n2.2\n,\n-\n1.5\n]))],\n[\n\"\nlabel\n\"\n,\n\"\nfeatures\n\"\n])\n# Make predictions on test data using the Transformer.transform() method.\n# LogisticRegression.transform will only use the 'features' column.\n# Note that model2.transform() outputs a \"myProbability\" column instead of the usual\n# 'probability' column since we renamed the lr.probabilityCol parameter previously.\nprediction\n=\nmodel2\n.\ntransform\n(\ntest\n)\nresult", "question": "What column does LogisticRegression.transform use?", "answers": {"text": ["'features' column."], "answer_start": [565]}}
{"context": "stead of the usual\n# 'probability' column since we renamed the lr.probabilityCol parameter previously.\nprediction\n=\nmodel2\n.\ntransform\n(\ntest\n)\nresult\n=\nprediction\n.\nselect\n(\n\"\nfeatures\n\"\n,\n\"\nlabel\n\"\n,\n\"\nmyProbability\n\"\n,\n\"\nprediction\n\"\n)\n\\\n.\ncollect\n()\nfor\nrow\nin\nresult\n:\nprint\n(\n\"\nfeatures=%s, label=%s -> prob=%s, prediction=%s\n\"\n%\n(\nrow\n.\nfeatures\n,\nrow\n.\nlabel\n,\nrow\n.\nmyProbability\n,\nrow\n.\nprediction\n))\nFind full example code at \"examples/src/main/python/ml/estimator_transformer_param_example.py\" in the Spark repo.\nRefer to the\nEstimator\nScala docs\n,\nthe\nTransformer\nScala docs\nand\nthe\nParams\nScala docs\nfor details on the API.\nimport\norg.apache.spark.ml.classification.LogisticRegression\nimport\norg.apache.spark.ml.linalg.\n{\nVector\n,\nVectors\n}\nimport\norg.apache.spark.ml.param.ParamMap\nimp", "question": "Where can I find a full example code for this process?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/estimator_transformer_param_example.py\" in the Spark repo."], "answer_start": [411]}}
{"context": "pache.spark.ml.classification.LogisticRegression\nimport\norg.apache.spark.ml.linalg.\n{\nVector\n,\nVectors\n}\nimport\norg.apache.spark.ml.param.ParamMap\nimport\norg.apache.spark.sql.Row\n// Prepare training data from a list of (label, features) tuples.\nval\ntraining\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n1.0\n,\nVectors\n.\ndense\n(\n0.0\n,\n1.1\n,\n0.1\n)),\n(\n0.0\n,\nVectors\n.\ndense\n(\n2.0\n,\n1.0\n,\n-\n1.0\n)),\n(\n0.0\n,\nVectors\n.\ndense\n(\n2.0\n,\n1.3\n,\n1.0\n)),\n(\n1.0\n,\nVectors\n.\ndense\n(\n0.0\n,\n1.2\n,\n-\n0.5\n))\n)).\ntoDF\n(\n\"label\"\n,\n\"features\"\n)\n// Create a LogisticRegression instance. This instance is an Estimator.\nval\nlr\n=\nnew\nLogisticRegression\n()\n// Print out the parameters, documentation, and any default values.\nprintln\n(\ns\n\"LogisticRegression parameters:\\n ${lr.explainParams()}\\n\"\n)\n// We may set parameters using setter m", "question": "What is created using `spark.createDataFrame` in the provided code?", "answers": {"text": ["training"], "answer_start": [190]}}
{"context": "ocumentation, and any default values.\nprintln\n(\ns\n\"LogisticRegression parameters:\\n ${lr.explainParams()}\\n\"\n)\n// We may set parameters using setter methods.\nlr\n.\nsetMaxIter\n(\n10\n)\n.\nsetRegParam\n(\n0.01\n)\n// Learn a LogisticRegression model. This uses the parameters stored in lr.\nval\nmodel1\n=\nlr\n.\nfit\n(\ntraining\n)\n// Since model1 is a Model (i.e., a Transformer produced by an Estimator),\n// we can view the parameters it used during fit().\n// This prints the parameter (name: value) pairs, where names are unique IDs for this\n// LogisticRegression instance.\nprintln\n(\ns\n\"Model 1 was fit using parameters: ${model1.parent.extractParamMap()}\"\n)\n// We may alternatively specify parameters using a ParamMap,\n// which supports several methods for specifying parameters.\nval\nparamMap\n=\nParamMap\n(\nlr\n.\nma", "question": "What is done with the parameters stored in 'lr'?", "answers": {"text": ["Learn a LogisticRegression model. This uses the parameters stored in lr."], "answer_start": [207]}}
{"context": " may alternatively specify parameters using a ParamMap,\n// which supports several methods for specifying parameters.\nval\nparamMap\n=\nParamMap\n(\nlr\n.\nmaxIter\n->\n20\n)\n.\nput\n(\nlr\n.\nmaxIter\n,\n30\n)\n// Specify 1 Param. This overwrites the original maxIter.\n.\nput\n(\nlr\n.\nregParam\n->\n0.1\n,\nlr\n.\nthreshold\n->\n0.55\n)\n// Specify multiple Params.\n// One can also combine ParamMaps.\nval\nparamMap2\n=\nParamMap\n(\nlr\n.\nprobabilityCol\n->\n\"myProbability\"\n)\n// Change output column name.\nval\nparamMapCombined\n=\nparamMap\n++\nparamMap2\n// Now learn a new model using the paramMapCombined parameters.\n// paramMapCombined overrides all parameters set earlier via lr.set* methods.\nval\nmodel2\n=\nlr\n.\nfit\n(\ntraining\n,\nparamMapCombined\n)\nprintln\n(\ns\n\"Model 2 was fit using parameters: ${model2.parent.extractParamMap()}\"\n)\n// Prep", "question": "What happens when combining ParamMaps?", "answers": {"text": ["One can also combine ParamMaps."], "answer_start": [337]}}
{"context": "ds.\nval\nmodel2\n=\nlr\n.\nfit\n(\ntraining\n,\nparamMapCombined\n)\nprintln\n(\ns\n\"Model 2 was fit using parameters: ${model2.parent.extractParamMap()}\"\n)\n// Prepare test data.\nval\ntest\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n1.0\n,\nVectors\n.\ndense\n(-\n1.0\n,\n1.5\n,\n1.3\n)),\n(\n0.0\n,\nVectors\n.\ndense\n(\n3.0\n,\n2.0\n,\n-\n0.1\n)),\n(\n1.0\n,\nVectors\n.\ndense\n(\n0.0\n,\n2.2\n,\n-\n1.5\n))\n)).\ntoDF\n(\n\"label\"\n,\n\"features\"\n)\n// Make predictions on test data using the Transformer.transform() method.\n// LogisticRegression.transform will only use the 'features' column.\n// Note that model2.transform() outputs a 'myProbability' column instead of the usual\n// 'probability' column since we renamed the lr.probabilityCol parameter previously.\nmodel2\n.\ntransform\n(\ntest\n)\n.\nselect\n(\n\"features\"\n,\n\"label\"\n,\n\"myProbability\"\n,\n\"prediction\"\n)\n.\ncoll", "question": "What column does LogisticRegression.transform use for predictions?", "answers": {"text": ["'features' column"], "answer_start": [509]}}
{"context": "amed the lr.probabilityCol parameter previously.\nmodel2\n.\ntransform\n(\ntest\n)\n.\nselect\n(\n\"features\"\n,\n\"label\"\n,\n\"myProbability\"\n,\n\"prediction\"\n)\n.\ncollect\n()\n.\nforeach\n{\ncase\nRow\n(\nfeatures\n:\nVector\n,\nlabel\n:\nDouble\n,\nprob\n:\nVector\n,\nprediction\n:\nDouble\n)\n=>\nprintln\n(\ns\n\"($features, $label) -> prob=$prob, prediction=$prediction\"\n)\n}\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\" in the Spark repo.\nRefer to the\nEstimator\nJava docs\n,\nthe\nTransformer\nJava docs\nand\nthe\nParams\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.classification.LogisticRegression\n;\nimport\norg.apache.spark.ml.classification.LogisticRegressionModel\n;\nimport\norg.apache.spark.ml.linalg.Ve", "question": "Where can I find a full example code for EstimatorTransformerParamExample?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\" in the Spark repo."], "answer_start": [334]}}
{"context": ".spark.ml.classification.LogisticRegression\n;\nimport\norg.apache.spark.ml.classification.LogisticRegressionModel\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.ml.param.ParamMap\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\n// Prepare training data.\nList\n<\nRow\n>\ndataTraining\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n1.0\n,\nVectors\n.\ndense\n(\n0.0\n,\n1.1\n,\n0.1\n)),\nRowFactory\n.\ncreate\n(\n0.0\n,\nVectors\n.\ndense\n(\n2.0\n,\n1.0\n,\n-\n1.0\n)),\nRowFactory\n.\ncreate\n(\n0.0\n,\nVectors\n.\ndense\n(\n2", "question": "Which class is imported from org.apache.spark.ml?", "answers": {"text": ["org.apache.spark.ml.classification.LogisticRegressionModel"], "answer_start": [53]}}
{"context": "\n.\nout\n.\nprintln\n(\n\"Model 1 was fit using parameters: \"\n+\nmodel1\n.\nparent\n().\nextractParamMap\n());\n// We may alternatively specify parameters using a ParamMap.\nParamMap\nparamMap\n=\nnew\nParamMap\n()\n.\nput\n(\nlr\n.\nmaxIter\n().\nw\n(\n20\n))\n// Specify 1 Param.\n.\nput\n(\nlr\n.\nmaxIter\n(),\n30\n)\n// This overwrites the original maxIter.\n.\nput\n(\nlr\n.\nregParam\n().\nw\n(\n0.1\n),\nlr\n.\nthreshold\n().\nw\n(\n0.55\n));\n// Specify multiple Params.\n// One can also combine ParamMaps.\nParamMap\nparamMap2\n=\nnew\nParamMap\n()\n.\nput\n(\nlr\n.\nprobabilityCol\n().\nw\n(\n\"myProbability\"\n));\n// Change output column name\nParamMap\nparamMapCombined\n=\nparamMap\n.\n$plus$plus\n(\nparamMap2\n);\n// Now learn a new model using the paramMapCombined parameters.\n// paramMapCombined overrides all parameters set earlier via lr.set* methods.\nLogisticRegressio", "question": "What happens when combining ParamMaps using the $plus$plus operator?", "answers": {"text": ["One can also combine ParamMaps."], "answer_start": [422]}}
{"context": "arn a new model using the paramMapCombined parameters.\n// paramMapCombined overrides all parameters set earlier via lr.set* methods.\nLogisticRegressionModel\nmodel2\n=\nlr\n.\nfit\n(\ntraining\n,\nparamMapCombined\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Model 2 was fit using parameters: \"\n+\nmodel2\n.\nparent\n().\nextractParamMap\n());\n// Prepare test documents.\nList\n<\nRow\n>\ndataTest\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n1.0\n,\nVectors\n.\ndense\n(-\n1.0\n,\n1.5\n,\n1.3\n)),\nRowFactory\n.\ncreate\n(\n0.0\n,\nVectors\n.\ndense\n(\n3.0\n,\n2.0\n,\n-\n0.1\n)),\nRowFactory\n.\ncreate\n(\n1.0\n,\nVectors\n.\ndense\n(\n0.0\n,\n2.2\n,\n-\n1.5\n))\n);\nDataset\n<\nRow\n>\ntest\n=\nspark\n.\ncreateDataFrame\n(\ndataTest\n,\nschema\n);\n// Make predictions on test documents using the Transformer.transform() method.\n// LogisticRegression.transform will only use the 'features' co", "question": "What method is used to make predictions on test documents?", "answers": {"text": ["LogisticRegression.transform will only use the 'features' co"], "answer_start": [740]}}
{"context": "ema\n);\n// Make predictions on test documents using the Transformer.transform() method.\n// LogisticRegression.transform will only use the 'features' column.\n// Note that model2.transform() outputs a 'myProbability' column instead of the usual\n// 'probability' column since we renamed the lr.probabilityCol parameter previously.\nDataset\n<\nRow\n>\nresults\n=\nmodel2\n.\ntransform\n(\ntest\n);\nDataset\n<\nRow\n>\nrows\n=\nresults\n.\nselect\n(\n\"features\"\n,\n\"label\"\n,\n\"myProbability\"\n,\n\"prediction\"\n);\nfor\n(\nRow\nr:\nrows\n.\ncollectAsList\n())\n{\nSystem\n.\nout\n.\nprintln\n(\n\"(\"\n+\nr\n.\nget\n(\n0\n)\n+\n\", \"\n+\nr\n.\nget\n(\n1\n)\n+\n\") -> prob=\"\n+\nr\n.\nget\n(\n2\n)\n+\n\", prediction=\"\n+\nr\n.\nget\n(\n3\n));\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\" in the Spark repo.\nE", "question": "What column does LogisticRegression.transform use?", "answers": {"text": ["LogisticRegression.transform will only use the 'features' column."], "answer_start": [90]}}
{"context": "3\n));\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\" in the Spark repo.\nExample: Pipeline\nThis example follows the simple text document\nPipeline\nillustrated in the figures above.\nRefer to the\nPipeline\nPython docs\nfor more details on the API.\nfrom\npyspark.ml\nimport\nPipeline\nfrom\npyspark.ml.classification\nimport\nLogisticRegression\nfrom\npyspark.ml.feature\nimport\nHashingTF\n,\nTokenizer\n# Prepare training documents from a list of (id, text, label) tuples.\ntraining\n=\nspark\n.\ncreateDataFrame\n([\n(\n0\n,\n\"\na b c d e spark\n\"\n,\n1.0\n),\n(\n1\n,\n\"\nb d\n\"\n,\n0.0\n),\n(\n2\n,\n\"\nspark f g h\n\"\n,\n1.0\n),\n(\n3\n,\n\"\nhadoop mapreduce\n\"\n,\n0.0\n)\n],\n[\n\"\nid\n\"\n,\n\"\ntext\n\"\n,\n\"\nlabel\n\"\n])\n# Configure an ML pipeline, which consists of three stages: tokenizer", "question": "Where can I find a full example code for JavaEstimatorTransformerParamExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\" in the Spark repo."], "answer_start": [8]}}
{"context": "\n1.0\n),\n(\n3\n,\n\"\nhadoop mapreduce\n\"\n,\n0.0\n)\n],\n[\n\"\nid\n\"\n,\n\"\ntext\n\"\n,\n\"\nlabel\n\"\n])\n# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\ntokenizer\n=\nTokenizer\n(\ninputCol\n=\n\"\ntext\n\"\n,\noutputCol\n=\n\"\nwords\n\"\n)\nhashingTF\n=\nHashingTF\n(\ninputCol\n=\ntokenizer\n.\ngetOutputCol\n(),\noutputCol\n=\n\"\nfeatures\n\"\n)\nlr\n=\nLogisticRegression\n(\nmaxIter\n=\n10\n,\nregParam\n=\n0.001\n)\npipeline\n=\nPipeline\n(\nstages\n=\n[\ntokenizer\n,\nhashingTF\n,\nlr\n])\n# Fit the pipeline to training documents.\nmodel\n=\npipeline\n.\nfit\n(\ntraining\n)\n# Prepare test documents, which are unlabeled (id, text) tuples.\ntest\n=\nspark\n.\ncreateDataFrame\n([\n(\n4\n,\n\"\nspark i j k\n\"\n),\n(\n5\n,\n\"\nl m n\n\"\n),\n(\n6\n,\n\"\nspark hadoop spark\n\"\n),\n(\n7\n,\n\"\napache hadoop\n\"\n)\n],\n[\n\"\nid\n\"\n,\n\"\ntext\n\"\n])\n# Make predictions on test documents and", "question": "What stages does the ML pipeline consist of?", "answers": {"text": ["tokenizer, hashingTF, and lr."], "answer_start": [141]}}
{"context": "k\n\"\n),\n(\n5\n,\n\"\nl m n\n\"\n),\n(\n6\n,\n\"\nspark hadoop spark\n\"\n),\n(\n7\n,\n\"\napache hadoop\n\"\n)\n],\n[\n\"\nid\n\"\n,\n\"\ntext\n\"\n])\n# Make predictions on test documents and print columns of interest.\nprediction\n=\nmodel\n.\ntransform\n(\ntest\n)\nselected\n=\nprediction\n.\nselect\n(\n\"\nid\n\"\n,\n\"\ntext\n\"\n,\n\"\nprobability\n\"\n,\n\"\nprediction\n\"\n)\nfor\nrow\nin\nselected\n.\ncollect\n():\nrid\n,\ntext\n,\nprob\n,\nprediction\n=\nrow\nprint\n(\n\"\n(%d, %s) --> prob=%s, prediction=%f\n\"\n%\n(\nrid\n,\ntext\n,\nstr\n(\nprob\n),\nprediction\n# type: ignore\n)\n)\nFind full example code at \"examples/src/main/python/ml/pipeline_example.py\" in the Spark repo.\nRefer to the\nPipeline\nScala docs\nfor details on the API.\nimport\norg.apache.spark.ml.\n{\nPipeline\n,\nPipelineModel\n}\nimport\norg.apache.spark.ml.classification.LogisticRegression\nimport\norg.apache.spark.ml.feature.\n{\nHashin", "question": "Where can one find a full example code for the pipeline?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/pipeline_example.py\" in the Spark repo."], "answer_start": [486]}}
{"context": "pache.spark.ml.\n{\nPipeline\n,\nPipelineModel\n}\nimport\norg.apache.spark.ml.classification.LogisticRegression\nimport\norg.apache.spark.ml.feature.\n{\nHashingTF\n,\nTokenizer\n}\nimport\norg.apache.spark.ml.linalg.Vector\nimport\norg.apache.spark.sql.Row\n// Prepare training documents from a list of (id, text, label) tuples.\nval\ntraining\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0L\n,\n\"a b c d e spark\"\n,\n1.0\n),\n(\n1L\n,\n\"b d\"\n,\n0.0\n),\n(\n2L\n,\n\"spark f g h\"\n,\n1.0\n),\n(\n3L\n,\n\"hadoop mapreduce\"\n,\n0.0\n)\n)).\ntoDF\n(\n\"id\"\n,\n\"text\"\n,\n\"label\"\n)\n// Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\nval\ntokenizer\n=\nnew\nTokenizer\n()\n.\nsetInputCol\n(\n\"text\"\n)\n.\nsetOutputCol\n(\n\"words\"\n)\nval\nhashingTF\n=\nnew\nHashingTF\n()\n.\nsetNumFeatures\n(\n1000\n)\n.\nsetInputCol\n(\ntokenizer\n.\ngetOutputCol\n)\n.\nsetO", "question": "What are the three stages that the ML pipeline consists of?", "answers": {"text": ["tokenizer, hashingTF, and lr"], "answer_start": [577]}}
{"context": "nfit-lr-model\"\n)\n// And load it back in during production\nval\nsameModel\n=\nPipelineModel\n.\nload\n(\n\"/tmp/spark-logistic-regression-model\"\n)\n// Prepare test documents, which are unlabeled (id, text) tuples.\nval\ntest\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n4L\n,\n\"spark i j k\"\n),\n(\n5L\n,\n\"l m n\"\n),\n(\n6L\n,\n\"spark hadoop spark\"\n),\n(\n7L\n,\n\"apache hadoop\"\n)\n)).\ntoDF\n(\n\"id\"\n,\n\"text\"\n)\n// Make predictions on test documents.\nmodel\n.\ntransform\n(\ntest\n)\n.\nselect\n(\n\"id\"\n,\n\"text\"\n,\n\"probability\"\n,\n\"prediction\"\n)\n.\ncollect\n()\n.\nforeach\n{\ncase\nRow\n(\nid\n:\nLong\n,\ntext\n:\nString\n,\nprob\n:\nVector\n,\nprediction\n:\nDouble\n)\n=>\nprintln\n(\ns\n\"($id, $text) --> prob=$prob, prediction=$prediction\"\n)\n}\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\" in the Spark repo.\nRefer t", "question": "Where can I find the full example code for the PipelineExample?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\" in the Spark repo."], "answer_start": [671]}}
{"context": "ion=$prediction\"\n)\n}\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\" in the Spark repo.\nRefer to the\nPipeline\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\norg.apache.spark.ml.Pipeline\n;\nimport\norg.apache.spark.ml.PipelineModel\n;\nimport\norg.apache.spark.ml.PipelineStage\n;\nimport\norg.apache.spark.ml.classification.LogisticRegression\n;\nimport\norg.apache.spark.ml.feature.HashingTF\n;\nimport\norg.apache.spark.ml.feature.Tokenizer\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\n// Prepare training documents, which are labeled.\nDataset\n<\nRow\n>\ntraining\n=\nspark\n.\ncreateDataFrame\n(\nArrays\n.\nasList\n(\nnew\nJavaLabeledDocument\n(\n0L\n,\n\"a b c d e spark\"\n,\n1.0\n),\nnew\nJavaLabeledDocument\n(\n1L\n,\n\"b d\"\n,\n0.0\n", "question": "Where can I find a full example code for the Pipeline?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\" in the Spark repo."], "answer_start": [21]}}
{"context": "\ntransform\n(\ntest\n);\nfor\n(\nRow\nr\n:\npredictions\n.\nselect\n(\n\"id\"\n,\n\"text\"\n,\n\"probability\"\n,\n\"prediction\"\n).\ncollectAsList\n())\n{\nSystem\n.\nout\n.\nprintln\n(\n\"(\"\n+\nr\n.\nget\n(\n0\n)\n+\n\", \"\n+\nr\n.\nget\n(\n1\n)\n+\n\") --> prob=\"\n+\nr\n.\nget\n(\n2\n)\n+\n\", prediction=\"\n+\nr\n.\nget\n(\n3\n));\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\" in the Spark repo.\nModel selection (hyperparameter tuning)\nA big benefit of using ML Pipelines is hyperparameter optimization.  See the\nML Tuning Guide\nfor more information on automatic model selection.", "question": "Where can I find a full example code for the JavaPipelineExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\" in the Spark repo."], "answer_start": [264]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available for Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)"], "answer_start": [46]}}
{"context": "ures\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nExtracting, transforming and selecting features\nThis section covers algorithms for working with features, roughly divided into these groups:\nExtraction: Extracting features from “raw” data\nTransformation: Scaling, converting, or modifying features\nSelection: Selecting a subset from a larger set of features\nLocality Sensitive Hashing (LSH): This class of algorithms combines aspects of feature", "question": "Quais são os três grupos principais de algoritmos para trabalhar com recursos?", "answers": {"text": ["Extraction: Extracting features from “raw” data\nTransformation: Scaling, converting, or modifying features\nSelection: Selecting a subset from a larger set of features"], "answer_start": [547]}}
{"context": "res\nSelection: Selecting a subset from a larger set of features\nLocality Sensitive Hashing (LSH): This class of algorithms combines aspects of feature transformation with other algorithms.\nTable of Contents\nFeature Extractors\nTF-IDF\nWord2Vec\nCountVectorizer\nFeatureHasher\nFeature Transformers\nTokenizer\nStopWordsRemover\n$n$-gram\nBinarizer\nPCA\nPolynomialExpansion\nDiscrete Cosine Transform (DCT)\nStringIndexer\nIndexToString\nOneHotEncoder\nTargetEncoder\nVectorIndexer\nInteraction\nNormalizer\nStandardScaler\nRobustScaler\nMinMaxScaler\nMaxAbsScaler\nBucketizer\nElementwiseProduct\nSQLTransformer\nVectorAssembler\nVectorSizeHint\nQuantileDiscretizer\nImputer\nFeature Selectors\nVectorSlicer\nRFormula\nChiSqSelector\nUnivariateFeatureSelector\nVarianceThresholdSelector\nLocality Sensitive Hashing\nLSH Operations\nFeatur", "question": "What is Locality Sensitive Hashing (LSH)?", "answers": {"text": ["This class of algorithms combines aspects of feature transformation with other algorithms."], "answer_start": [98]}}
{"context": "ure Selectors\nVectorSlicer\nRFormula\nChiSqSelector\nUnivariateFeatureSelector\nVarianceThresholdSelector\nLocality Sensitive Hashing\nLSH Operations\nFeature Transformation\nApproximate Similarity Join\nApproximate Nearest Neighbor Search\nLSH Algorithms\nBucketed Random Projection for Euclidean Distance\nMinHash for Jaccard Distance\nFeature Extractors\nTF-IDF\nTerm frequency-inverse document frequency (TF-IDF)\nis a feature vectorization method widely used in text mining to reflect the importance of a term \nto a document in the corpus. Denote a term by\n$t$\n, a document by\n$d$\n, and the corpus by\n$D$\n.\nTerm frequency\n$TF(t, d)$\nis the number of times that term\n$t$\nappears in document\n$d$\n, while \ndocument frequency\n$DF(t, D)$\nis the number of documents that contains term\n$t$\n. If we only use \nterm frequ", "question": "What does TF-IDF stand for?", "answers": {"text": ["Term frequency-inverse document frequency (TF-IDF)"], "answer_start": [351]}}
{"context": "sing, a “set of terms” might be a bag of words.\nHashingTF\nutilizes the\nhashing trick\n.\nA raw feature is mapped into an index (term) by applying a hash function. The hash function\nused here is\nMurmurHash 3\n. Then term frequencies\nare calculated based on the mapped indices. This approach avoids the need to compute a global \nterm-to-index map, which can be expensive for a large corpus, but it suffers from potential hash \ncollisions, where different raw features may become the same term after hashing. To reduce the \nchance of collision, we can increase the target feature dimension, i.e. the number of buckets \nof the hash table. Since a simple modulo on the hashed value is used to determine the vector index,\nit is advisable to use a power of two as the feature dimension, otherwise the features ", "question": "Which hash function is used in HashingTF?", "answers": {"text": ["MurmurHash 3"], "answer_start": [192]}}
{"context": "ulo on the hashed value is used to determine the vector index,\nit is advisable to use a power of two as the feature dimension, otherwise the features will not\nbe mapped evenly to the vector indices. The default feature dimension is\n$2^{18} = 262,144$\n.\nAn optional binary toggle parameter controls term frequency counts. When set to true all nonzero\nfrequency counts are set to 1. This is especially useful for discrete probabilistic models that\nmodel binary, rather than integer, counts.\nCountVectorizer\nconverts text documents to vectors of term counts. Refer to\nCountVectorizer\nfor more details.\nIDF\n:\nIDF\nis an\nEstimator\nwhich is fit on a dataset and produces an\nIDFModel\n.  The\nIDFModel\ntakes feature vectors (generally created from\nHashingTF\nor\nCountVectorizer\n) and \nscales each feature. Intui", "question": "What is the default feature dimension?", "answers": {"text": ["$2^{18} = 262,144$"], "answer_start": [232]}}
{"context": " and produces an\nIDFModel\n.  The\nIDFModel\ntakes feature vectors (generally created from\nHashingTF\nor\nCountVectorizer\n) and \nscales each feature. Intuitively, it down-weights features which appear frequently in a corpus.\nNote:\nspark.ml\ndoesn’t provide tools for text segmentation.\nWe refer users to the\nStanford NLP Group\nand\nscalanlp/chalk\n.\nExamples\nIn the following code segment, we start with a set of sentences.  We split each sentence into words \nusing\nTokenizer\n.  For each sentence (bag of words), we use\nHashingTF\nto hash the sentence into \na feature vector.  We use\nIDF\nto rescale the feature vectors; this generally improves performance \nwhen using text as features.  Our feature vectors could then be passed to a learning algorithm.\nRefer to the\nHashingTF Python docs\nand\nthe\nIDF Python do", "question": "What do IDFModels do to feature vectors?", "answers": {"text": ["scales each feature. Intuitively, it down-weights features which appear frequently in a corpus."], "answer_start": [124]}}
{"context": "s\n\"\n,\nnumFeatures\n=\n20\n)\nfeaturizedData\n=\nhashingTF\n.\ntransform\n(\nwordsData\n)\n# alternatively, CountVectorizer can also be used to get term frequency vectors\nidf\n=\nIDF\n(\ninputCol\n=\n\"\nrawFeatures\n\"\n,\noutputCol\n=\n\"\nfeatures\n\"\n)\nidfModel\n=\nidf\n.\nfit\n(\nfeaturizedData\n)\nrescaledData\n=\nidfModel\n.\ntransform\n(\nfeaturizedData\n)\nrescaledData\n.\nselect\n(\n\"\nlabel\n\"\n,\n\"\nfeatures\n\"\n).\nshow\n()\nFind full example code at \"examples/src/main/python/ml/tf_idf_example.py\" in the Spark repo.\nRefer to the\nHashingTF Scala docs\nand\nthe\nIDF Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.\n{\nHashingTF\n,\nIDF\n,\nTokenizer\n}\nval\nsentenceData\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0.0\n,\n\"Hi I heard about Spark\"\n),\n(\n0.0\n,\n\"I wish Java could use case classes\"\n),\n(\n1.0\n,\n\"Logistic regression models a", "question": "What can be used alternatively to hashingTF to get term frequency vectors?", "answers": {"text": ["CountVectorizer can also be used to get term frequency vectors"], "answer_start": [95]}}
{"context": "rk\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0.0\n,\n\"Hi I heard about Spark\"\n),\n(\n0.0\n,\n\"I wish Java could use case classes\"\n),\n(\n1.0\n,\n\"Logistic regression models are neat\"\n)\n)).\ntoDF\n(\n\"label\"\n,\n\"sentence\"\n)\nval\ntokenizer\n=\nnew\nTokenizer\n().\nsetInputCol\n(\n\"sentence\"\n).\nsetOutputCol\n(\n\"words\"\n)\nval\nwordsData\n=\ntokenizer\n.\ntransform\n(\nsentenceData\n)\nval\nhashingTF\n=\nnew\nHashingTF\n()\n.\nsetInputCol\n(\n\"words\"\n).\nsetOutputCol\n(\n\"rawFeatures\"\n).\nsetNumFeatures\n(\n20\n)\nval\nfeaturizedData\n=\nhashingTF\n.\ntransform\n(\nwordsData\n)\n// alternatively, CountVectorizer can also be used to get term frequency vectors\nval\nidf\n=\nnew\nIDF\n().\nsetInputCol\n(\n\"rawFeatures\"\n).\nsetOutputCol\n(\n\"features\"\n)\nval\nidfModel\n=\nidf\n.\nfit\n(\nfeaturizedData\n)\nval\nrescaledData\n=\nidfModel\n.\ntransform\n(\nfeaturizedData\n)\nrescaledData\n.\nselect\n(\n\"la", "question": "What is set as the input column for the HashingTF?", "answers": {"text": ["\"words\""], "answer_start": [272]}}
{"context": "tCol\n(\n\"features\"\n)\nval\nidfModel\n=\nidf\n.\nfit\n(\nfeaturizedData\n)\nval\nrescaledData\n=\nidfModel\n.\ntransform\n(\nfeaturizedData\n)\nrescaledData\n.\nselect\n(\n\"label\"\n,\n\"features\"\n).\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\" in the Spark repo.\nRefer to the\nHashingTF Java docs\nand the\nIDF Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.HashingTF\n;\nimport\norg.apache.spark.ml.feature.IDF\n;\nimport\norg.apache.spark.ml.feature.IDFModel\n;\nimport\norg.apache.spark.ml.feature.Tokenizer\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.SparkSession\n;\nimport\norg.apache.spark.sql.typ", "question": "Where can I find a full example code for TfIdf?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\" in the Spark repo."], "answer_start": [179]}}
{"context": "lse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"sentence\"\n,\nDataTypes\n.\nStringType\n,\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\nsentenceData\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nTokenizer\ntokenizer\n=\nnew\nTokenizer\n().\nsetInputCol\n(\n\"sentence\"\n).\nsetOutputCol\n(\n\"words\"\n);\nDataset\n<\nRow\n>\nwordsData\n=\ntokenizer\n.\ntransform\n(\nsentenceData\n);\nint\nnumFeatures\n=\n20\n;\nHashingTF\nhashingTF\n=\nnew\nHashingTF\n()\n.\nsetInputCol\n(\n\"words\"\n)\n.\nsetOutputCol\n(\n\"rawFeatures\"\n)\n.\nsetNumFeatures\n(\nnumFeatures\n);\nDataset\n<\nRow\n>\nfeaturizedData\n=\nhashingTF\n.\ntransform\n(\nwordsData\n);\n// alternatively, CountVectorizer can also be used to get term frequency vectors\nIDF\nidf\n=\nnew\nIDF\n().\nsetInputCol\n(\n\"rawFeatures\"\n).\nsetOutputCol\n(\n\"features\"\n);\nIDFModel\nidfModel\n=\nidf\n.\nfit\n(\nfeaturizedData\n);\nDataset\n<", "question": "What is set as the input column for the HashingTF transformer?", "answers": {"text": ["\"words\""], "answer_start": [274]}}
{"context": "s in the document; this vector\ncan then be used as features for prediction, document similarity calculations, etc.\nPlease refer to the\nMLlib user guide on Word2Vec\nfor more\ndetails.\nExamples\nIn the following code segment, we start with a set of documents, each of which is represented as a sequence of words. For each document, we transform it into a feature vector. This feature vector could then be passed to a learning algorithm.\nRefer to the\nWord2Vec Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nWord2Vec\n# Input data: Each row is a bag of words from a sentence or document.\ndocumentDF\n=\nspark\n.\ncreateDataFrame\n([\n(\n\"\nHi I heard about Spark\n\"\n.\nsplit\n(\n\"\n\"\n),\n),\n(\n\"\nI wish Java could use case classes\n\"\n.\nsplit\n(\n\"\n\"\n),\n),\n(\n\"\nLogistic regression models are neat\n\"\n.\n", "question": "What is each row in the input data representing?", "answers": {"text": ["Each row is a bag of words from a sentence or document."], "answer_start": [550]}}
{"context": "\nval\nmodel\n=\nword2Vec\n.\nfit\n(\ndocumentDF\n)\nval\nresult\n=\nmodel\n.\ntransform\n(\ndocumentDF\n)\nresult\n.\ncollect\n().\nforeach\n{\ncase\nRow\n(\ntext\n:\nSeq\n[\n_\n],\nfeatures\n:\nVector\n)\n=>\nprintln\n(\ns\n\"Text: [${text.mkString(\"\n,\n\")}] => \\nVector: $features\\n\"\n)\n}\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\" in the Spark repo.\nRefer to the\nWord2Vec Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.Word2Vec\n;\nimport\norg.apache.spark.ml.feature.Word2VecModel\n;\nimport\norg.apache.spark.ml.linalg.Vector\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.SparkSession\n;\nimport\norg.apache.spark.", "question": "Where can I find a full example code for Word2Vec?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\" in the Spark repo."], "answer_start": [247]}}
{"context": "(\nDataTypes\n.\nStringType\n,\ntrue\n),\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndocumentDF\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\n// Learn a mapping from words to Vectors.\nWord2Vec\nword2Vec\n=\nnew\nWord2Vec\n()\n.\nsetInputCol\n(\n\"text\"\n)\n.\nsetOutputCol\n(\n\"result\"\n)\n.\nsetVectorSize\n(\n3\n)\n.\nsetMinCount\n(\n0\n);\nWord2VecModel\nmodel\n=\nword2Vec\n.\nfit\n(\ndocumentDF\n);\nDataset\n<\nRow\n>\nresult\n=\nmodel\n.\ntransform\n(\ndocumentDF\n);\nfor\n(\nRow\nrow\n:\nresult\n.\ncollectAsList\n())\n{\nList\n<\nString\n>\ntext\n=\nrow\n.\ngetList\n(\n0\n);\nVector\nvector\n=\n(\nVector\n)\nrow\n.\nget\n(\n1\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Text: \"\n+\ntext\n+\n\" => \\nVector: \"\n+\nvector\n+\n\"\\n\"\n);\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\" in the Spark repo.\nCountVectorizer\nCountVectorizer\nand\n", "question": "What is set as the input column for the Word2Vec model?", "answers": {"text": ["\"text\""], "answer_start": [234]}}
{"context": "cy across the corpus. An optional parameter\nminDF\nalso affects the fitting process\n by specifying the minimum number (or fraction if < 1.0) of documents a term must appear in to be\n included in the vocabulary. Another optional binary toggle parameter controls the output vector.\n If set to true all nonzero counts are set to 1. This is especially useful for discrete probabilistic\n models that model binary, rather than integer, counts.\nExamples\nAssume that we have the following DataFrame with columns\nid\nand\ntexts\n:\nid | texts\n----|----------\n 0  | Array(\"a\", \"b\", \"c\")\n 1  | Array(\"a\", \"b\", \"b\", \"c\", \"a\")\neach row in\ntexts\nis a document of type Array[String].\nInvoking fit of\nCountVectorizer\nproduces a\nCountVectorizerModel\nwith vocabulary (a, b, c).\nThen the output column “vector” after transfo", "question": "What does setting a parameter to true do to nonzero counts in the output vector?", "answers": {"text": ["If set to true all nonzero counts are set to 1."], "answer_start": [280]}}
{"context": "rray[String].\nInvoking fit of\nCountVectorizer\nproduces a\nCountVectorizerModel\nwith vocabulary (a, b, c).\nThen the output column “vector” after transformation contains:\nid | texts                           | vector\n----|---------------------------------|---------------\n 0  | Array(\"a\", \"b\", \"c\")            | (3,[0,1,2],[1.0,1.0,1.0])\n 1  | Array(\"a\", \"b\", \"b\", \"c\", \"a\")  | (3,[0,1,2],[2.0,2.0,1.0])\nEach vector represents the token counts of the document over the vocabulary.\nRefer to the\nCountVectorizer Python docs\nand the\nCountVectorizerModel Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nCountVectorizer\n# Input data: Each row is a bag of words with a ID.\ndf\n=\nspark\n.\ncreateDataFrame\n([\n(\n0\n,\n\"\na b c\n\"\n.\nsplit\n(\n\"\n\"\n)),\n(\n1\n,\n\"\na b b c a\n\"\n.\nsplit\n(\n\"\n\"\n))\n],\n[\n\"\nid", "question": "What does each vector represent?", "answers": {"text": ["Each vector represents the token counts of the document over the vocabulary."], "answer_start": [401]}}
{"context": " full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\" in the Spark repo.\nRefer to the\nCountVectorizer Java docs\nand the\nCountVectorizerModel Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.CountVectorizer\n;\nimport\norg.apache.spark.ml.feature.CountVectorizerModel\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.SparkSession\n;\nimport\norg.apache.spark.sql.types.*\n;\n// Input data: Each row is a bag of words from a sentence or document.\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\nArrays\n.\nasList\n(\n\"a\"\n,\n\"b\"\n,\n\"c\"\n)),\nRowFactory\n.\ncreate\n(\nArrays\n.\nasList\n(\n\"a\"\n", "question": "Where can I find a full example code for CountVectorizer?", "answers": {"text": ["full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\" in the Spark repo."], "answer_start": [1]}}
{"context": "torizerModel with a-priori vocabulary\nCountVectorizerModel\ncvm\n=\nnew\nCountVectorizerModel\n(\nnew\nString\n[]{\n\"a\"\n,\n\"b\"\n,\n\"c\"\n})\n.\nsetInputCol\n(\n\"text\"\n)\n.\nsetOutputCol\n(\n\"feature\"\n);\ncvModel\n.\ntransform\n(\ndf\n).\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\" in the Spark repo.\nFeatureHasher\nFeature hashing projects a set of categorical or numerical features into a feature vector of\nspecified dimension (typically substantially smaller than that of the original feature\nspace). This is done using the\nhashing trick\nto map features to indices in the feature vector.\nThe\nFeatureHasher\ntransformer operates on multiple columns. Each column may contain either\nnumeric or categorical features. Behavior and handling of column", "question": "Where can I find a full example code for CountVectorizer?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\" in the Spark repo."], "answer_start": [225]}}
{"context": " value of\n1.0\n. Thus, categorical features\nare “one-hot” encoded (similarly to using\nOneHotEncoder\nwith\ndropLast=false\n).\nBoolean columns: Boolean values are treated in the same way as string columns. That is,\nboolean features are represented as “column_name=true” or “column_name=false”, with an indicator\nvalue of\n1.0\n.\nNull (missing) values are ignored (implicitly zero in the resulting feature vector).\nThe hash function used here is also the\nMurmurHash 3\nused in\nHashingTF\n. Since a simple modulo on the hashed value is used to\ndetermine the vector index, it is advisable to use a power of two as the numFeatures parameter;\notherwise the features will not be mapped evenly to the vector indices.\nExamples\nAssume that we have a DataFrame with 4 input columns\nreal\n,\nbool\n,\nstringNum\n, and\nstring\n", "question": "What value is used to represent boolean features as an indicator?", "answers": {"text": ["1.0"], "answer_start": [10]}}
{"context": "s will not be mapped evenly to the vector indices.\nExamples\nAssume that we have a DataFrame with 4 input columns\nreal\n,\nbool\n,\nstringNum\n, and\nstring\n.\nThese different data types as input will illustrate the behavior of the transform to produce a\ncolumn of feature vectors.\nreal| bool|stringNum|string\n----|-----|---------|------\n 2.2| true|        1|   foo\n 3.3|false|        2|   bar\n 4.4|false|        3|   baz\n 5.5|false|        4|   foo\nThen the output of\nFeatureHasher.transform\non this DataFrame is:\nreal|bool |stringNum|string|features\n----|-----|---------|------|-------------------------------------------------------\n2.2 |true |1        |foo   |(262144,[51871, 63643,174475,253195],[1.0,1.0,2.2,1.0])\n3.3 |false|2        |bar   |(262144,[6031,  80619,140467,174475],[1.0,1.0,1.0,3.3])\n4.4 ", "question": "What is the output of FeatureHasher.transform on the given DataFrame?", "answers": {"text": ["real|bool |stringNum|string|features\n----|-----|---------|------|-------------------------------------------------------\n2.2 |true |1        |foo   |(262144,[51871, 63643,174475,253195],[1.0,1.0,2.2,1.0])\n3.3 |false|2        |bar   |(262144,[6031,  80619,140467,174475],[1.0,1.0,1.0,3.3])\n4.4"], "answer_start": [507]}}
{"context": "alse\n,\n\"4\"\n,\n\"foo\"\n)\n)).\ntoDF\n(\n\"real\"\n,\n\"bool\"\n,\n\"stringNum\"\n,\n\"string\"\n)\nval\nhasher\n=\nnew\nFeatureHasher\n()\n.\nsetInputCols\n(\n\"real\"\n,\n\"bool\"\n,\n\"stringNum\"\n,\n\"string\"\n)\n.\nsetOutputCol\n(\n\"features\"\n)\nval\nfeaturized\n=\nhasher\n.\ntransform\n(\ndataset\n)\nfeaturized\n.\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/FeatureHasherExample.scala\" in the Spark repo.\nRefer to the\nFeatureHasher Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.FeatureHasher\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache", "question": "Where can I find a full example code for FeatureHasher?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/FeatureHasherExample.scala\" in the Spark repo."], "answer_start": [275]}}
{"context": "n=\"\\\\w+\", gaps(False)\ncountTokens\n=\nudf\n(\nlambda\nwords\n:\nlen\n(\nwords\n),\nIntegerType\n())\ntokenized\n=\ntokenizer\n.\ntransform\n(\nsentenceDataFrame\n)\ntokenized\n.\nselect\n(\n\"\nsentence\n\"\n,\n\"\nwords\n\"\n)\n\\\n.\nwithColumn\n(\n\"\ntokens\n\"\n,\ncountTokens\n(\ncol\n(\n\"\nwords\n\"\n))).\nshow\n(\ntruncate\n=\nFalse\n)\nregexTokenized\n=\nregexTokenizer\n.\ntransform\n(\nsentenceDataFrame\n)\nregexTokenized\n.\nselect\n(\n\"\nsentence\n\"\n,\n\"\nwords\n\"\n)\n\\\n.\nwithColumn\n(\n\"\ntokens\n\"\n,\ncountTokens\n(\ncol\n(\n\"\nwords\n\"\n))).\nshow\n(\ntruncate\n=\nFalse\n)\nFind full example code at \"examples/src/main/python/ml/tokenizer_example.py\" in the Spark repo.\nRefer to the\nTokenizer Scala docs\nand the\nRegexTokenizer Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.\n{\nRegexTokenizer\n,\nTokenizer\n}\nimport\norg.apache.spark.sql.SparkSession\nimport", "question": "Where can I find a full example code for the tokenizer?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/tokenizer_example.py\" in the Spark repo."], "answer_start": [493]}}
{"context": "a docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.\n{\nRegexTokenizer\n,\nTokenizer\n}\nimport\norg.apache.spark.sql.SparkSession\nimport\norg.apache.spark.sql.functions._\nval\nsentenceDataFrame\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0\n,\n\"Hi I heard about Spark\"\n),\n(\n1\n,\n\"I wish Java could use case classes\"\n),\n(\n2\n,\n\"Logistic,regression,models,are,neat\"\n)\n)).\ntoDF\n(\n\"id\"\n,\n\"sentence\"\n)\nval\ntokenizer\n=\nnew\nTokenizer\n().\nsetInputCol\n(\n\"sentence\"\n).\nsetOutputCol\n(\n\"words\"\n)\nval\nregexTokenizer\n=\nnew\nRegexTokenizer\n()\n.\nsetInputCol\n(\n\"sentence\"\n)\n.\nsetOutputCol\n(\n\"words\"\n)\n.\nsetPattern\n(\n\"\\\\W\"\n)\n// alternatively .setPattern(\"\\\\w+\").setGaps(false)\nval\ncountTokens\n=\nudf\n{\n(\nwords\n:\nSeq\n[\nString\n])\n=>\nwords\n.\nlength\n}\nval\ntokenized\n=\ntokenizer\n.\ntransform\n(\nsentenceDataFrame\n)\ntokenized\n.\n", "question": "What is set as the pattern for the regexTokenizer?", "answers": {"text": ["\\W"], "answer_start": [600]}}
{"context": "(false)\nval\ncountTokens\n=\nudf\n{\n(\nwords\n:\nSeq\n[\nString\n])\n=>\nwords\n.\nlength\n}\nval\ntokenized\n=\ntokenizer\n.\ntransform\n(\nsentenceDataFrame\n)\ntokenized\n.\nselect\n(\n\"sentence\"\n,\n\"words\"\n)\n.\nwithColumn\n(\n\"tokens\"\n,\ncountTokens\n(\ncol\n(\n\"words\"\n))).\nshow\n(\nfalse\n)\nval\nregexTokenized\n=\nregexTokenizer\n.\ntransform\n(\nsentenceDataFrame\n)\nregexTokenized\n.\nselect\n(\n\"sentence\"\n,\n\"words\"\n)\n.\nwithColumn\n(\n\"tokens\"\n,\ncountTokens\n(\ncol\n(\n\"words\"\n))).\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\" in the Spark repo.\nRefer to the\nTokenizer Java docs\nand the\nRegexTokenizer Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\nscala.collection.mutable.Seq\n;\nimport\norg.apache.spark.ml.feature.RegexToken", "question": "Where can I find a full example code for the Tokenizer?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\" in the Spark repo."], "answer_start": [449]}}
{"context": "ails on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\nscala.collection.mutable.Seq\n;\nimport\norg.apache.spark.ml.feature.RegexTokenizer\n;\nimport\norg.apache.spark.ml.feature.Tokenizer\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\n// col(\"...\") is preferable to df.col(\"...\")\nimport\nstatic\norg\n.\napache\n.\nspark\n.\nsql\n.\nfunctions\n.\ncall_udf\n;\nimport\nstatic\norg\n.\napache\n.\nspark\n.\nsql\n.\nfunctions\n.\ncol\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n0\n,\n\"Hi I heard about Spark\"\n),\nRowFactory\n.\ncreate\n(\n1\n,\n\"I wish", "question": "Which Java utility class is imported for creating lists?", "answers": {"text": ["java.util.Arrays"], "answer_start": [24]}}
{"context": "\nsql\n.\nfunctions\n.\ncol\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n0\n,\n\"Hi I heard about Spark\"\n),\nRowFactory\n.\ncreate\n(\n1\n,\n\"I wish Java could use case classes\"\n),\nRowFactory\n.\ncreate\n(\n2\n,\n\"Logistic,regression,models,are,neat\"\n)\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"id\"\n,\nDataTypes\n.\nIntegerType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"sentence\"\n,\nDataTypes\n.\nStringType\n,\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\nsentenceDataFrame\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nTokenizer\ntokenizer\n=\nnew\nTokenizer\n().\nsetInputCol\n(\n\"sentence\"\n).\nsetOutputCol\n(\n\"words\"\n);\nRegexTokenizer\nregexTokenizer\n=\nnew\nRegexTokenizer\n()\n.\nsetInputCol\n(\n\"sentence\"\n)\n.\nsetOutputCol\n(\n\"words\"\n)\n.\nsetPattern\n(\n\"\\\\W\"\n);\n// alternative", "question": "What is set as the input column for the tokenizer?", "answers": {"text": ["\"sentence\""], "answer_start": [408]}}
{"context": ";\nRegexTokenizer\nregexTokenizer\n=\nnew\nRegexTokenizer\n()\n.\nsetInputCol\n(\n\"sentence\"\n)\n.\nsetOutputCol\n(\n\"words\"\n)\n.\nsetPattern\n(\n\"\\\\W\"\n);\n// alternatively .setPattern(\"\\\\w+\").setGaps(false);\nspark\n.\nudf\n().\nregister\n(\n\"countTokens\"\n,\n(\nSeq\n<?>\nwords\n)\n->\nwords\n.\nsize\n(),\nDataTypes\n.\nIntegerType\n);\nDataset\n<\nRow\n>\ntokenized\n=\ntokenizer\n.\ntransform\n(\nsentenceDataFrame\n);\ntokenized\n.\nselect\n(\n\"sentence\"\n,\n\"words\"\n)\n.\nwithColumn\n(\n\"tokens\"\n,\ncall_udf\n(\n\"countTokens\"\n,\ncol\n(\n\"words\"\n)))\n.\nshow\n(\nfalse\n);\nDataset\n<\nRow\n>\nregexTokenized\n=\nregexTokenizer\n.\ntransform\n(\nsentenceDataFrame\n);\nregexTokenized\n.\nselect\n(\n\"sentence\"\n,\n\"words\"\n)\n.\nwithColumn\n(\n\"tokens\"\n,\ncall_udf\n(\n\"countTokens\"\n,\ncol\n(\n\"words\"\n)))\n.\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/", "question": "What is the pattern set for the RegexTokenizer?", "answers": {"text": ["\\W"], "answer_start": [129]}}
{"context": "\"tokens\"\n,\ncall_udf\n(\n\"countTokens\"\n,\ncol\n(\n\"words\"\n)))\n.\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\" in the Spark repo.\nStopWordsRemover\nStop words\nare words which\nshould be excluded from the input, typically because the words appear\nfrequently and don’t carry as much meaning.\nStopWordsRemover\ntakes as input a sequence of strings (e.g. the output\nof a\nTokenizer\n) and drops all the stop\nwords from the input sequences. The list of stopwords is specified by\nthe\nstopWords\nparameter. Default stop words for some languages are accessible \nby calling\nStopWordsRemover.loadDefaultStopWords(language)\n, for which available \noptions are “danish”, “dutch”, “english”, “finnish”, “french”, “german”, “hungarian”, \n“italian”, “no", "question": "Where can you find a full example code for tokenization?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\" in the Spark repo."], "answer_start": [74]}}
{"context": "  | filtered\n----|-----------------------------|--------------------\n 0  | [I, saw, the, red, balloon]  |  [saw, red, balloon]\n 1  | [Mary, had, a, little, lamb]|[Mary, little, lamb]\nIn\nfiltered\n, the stop words “I”, “the”, “had”, and “a” have been\nfiltered out.\nRefer to the\nStopWordsRemover Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nStopWordsRemover\nsentenceData\n=\nspark\n.\ncreateDataFrame\n([\n(\n0\n,\n[\n\"\nI\n\"\n,\n\"\nsaw\n\"\n,\n\"\nthe\n\"\n,\n\"\nred\n\"\n,\n\"\nballoon\n\"\n]),\n(\n1\n,\n[\n\"\nMary\n\"\n,\n\"\nhad\n\"\n,\n\"\na\n\"\n,\n\"\nlittle\n\"\n,\n\"\nlamb\n\"\n])\n],\n[\n\"\nid\n\"\n,\n\"\nraw\n\"\n])\nremover\n=\nStopWordsRemover\n(\ninputCol\n=\n\"\nraw\n\"\n,\noutputCol\n=\n\"\nfiltered\n\"\n)\nremover\n.\ntransform\n(\nsentenceData\n).\nshow\n(\ntruncate\n=\nFalse\n)\nFind full example code at \"examples/src/main/python/ml/stopwords_remover_example.py\" i", "question": "What stop words were filtered out in the example?", "answers": {"text": ["“I”, “the”, “had”, and “a”"], "answer_start": [212]}}
{"context": "remover\n.\ntransform\n(\nsentenceData\n).\nshow\n(\ntruncate\n=\nFalse\n)\nFind full example code at \"examples/src/main/python/ml/stopwords_remover_example.py\" in the Spark repo.\nRefer to the\nStopWordsRemover Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.StopWordsRemover\nval\nremover\n=\nnew\nStopWordsRemover\n()\n.\nsetInputCol\n(\n\"raw\"\n)\n.\nsetOutputCol\n(\n\"filtered\"\n)\nval\ndataSet\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0\n,\nSeq\n(\n\"I\"\n,\n\"saw\"\n,\n\"the\"\n,\n\"red\"\n,\n\"balloon\"\n)),\n(\n1\n,\nSeq\n(\n\"Mary\"\n,\n\"had\"\n,\n\"a\"\n,\n\"little\"\n,\n\"lamb\"\n))\n)).\ntoDF\n(\n\"id\"\n,\n\"raw\"\n)\nremover\n.\ntransform\n(\ndataSet\n).\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\" in the Spark repo.\nRefer to the\nStopWordsRemover Java docs\nfor more details", "question": "Where can I find the full example code for StopWordsRemover in Python?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/stopwords_remover_example.py\" in the Spark repo."], "answer_start": [64]}}
{"context": "src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\" in the Spark repo.\nRefer to the\nStopWordsRemover Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.StopWordsRemover\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nStopWordsRemover\nremover\n=\nnew\nStopWordsRemover\n()\n.\nsetInputCol\n(\n\"raw\"\n)\n.\nsetOutputCol\n(\n\"filtered\"\n);\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\nArrays\n.\nasList\n(\n\"I\"\n,\n\"saw\"\n,\n\"the\"\n,\n\"red\"\n,\n\"balloon\"\n)),\nRowFac", "question": "Which class is used to remove stop words from text data?", "answers": {"text": ["StopWordsRemover"], "answer_start": [44]}}
{"context": "utCol\n(\n\"filtered\"\n);\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\nArrays\n.\nasList\n(\n\"I\"\n,\n\"saw\"\n,\n\"the\"\n,\n\"red\"\n,\n\"balloon\"\n)),\nRowFactory\n.\ncreate\n(\nArrays\n.\nasList\n(\n\"Mary\"\n,\n\"had\"\n,\n\"a\"\n,\n\"little\"\n,\n\"lamb\"\n))\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"raw\"\n,\nDataTypes\n.\ncreateArrayType\n(\nDataTypes\n.\nStringType\n),\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndataset\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nremover\n.\ntransform\n(\ndataset\n).\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\" in the Spark repo.\n$n$-gram\nAn\nn-gram\nis a sequence of $n$ tokens (typically words) for some integer $n$. The\nNGram\nclass can be used to transform input features into", "question": "Where can I find the full example code for JavaStopWordsRemoverExample?", "answers": {"text": ["\"examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\" in the Spark repo."], "answer_start": [547]}}
{"context": "me\n=\nspark\n.\ncreateDataFrame\n([\n(\n0\n,\n[\n\"\nHi\n\"\n,\n\"\nI\n\"\n,\n\"\nheard\n\"\n,\n\"\nabout\n\"\n,\n\"\nSpark\n\"\n]),\n(\n1\n,\n[\n\"\nI\n\"\n,\n\"\nwish\n\"\n,\n\"\nJava\n\"\n,\n\"\ncould\n\"\n,\n\"\nuse\n\"\n,\n\"\ncase\n\"\n,\n\"\nclasses\n\"\n]),\n(\n2\n,\n[\n\"\nLogistic\n\"\n,\n\"\nregression\n\"\n,\n\"\nmodels\n\"\n,\n\"\nare\n\"\n,\n\"\nneat\n\"\n])\n],\n[\n\"\nid\n\"\n,\n\"\nwords\n\"\n])\nngram\n=\nNGram\n(\nn\n=\n2\n,\ninputCol\n=\n\"\nwords\n\"\n,\noutputCol\n=\n\"\nngrams\n\"\n)\nngramDataFrame\n=\nngram\n.\ntransform\n(\nwordDataFrame\n)\nngramDataFrame\n.\nselect\n(\n\"\nngrams\n\"\n).\nshow\n(\ntruncate\n=\nFalse\n)\nFind full example code at \"examples/src/main/python/ml/n_gram_example.py\" in the Spark repo.\nRefer to the\nNGram Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.NGram\nval\nwordDataFrame\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0\n,\nArray\n(\n\"Hi\"\n,\n\"I\"\n,\n\"heard\"\n,\n\"about\"\n,\n\"Spark\"\n)),\n(\n1\n,\nArray\n(\n\"I\"\n,\n\"", "question": "Where can I find the full example code for this n-gram example?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/n_gram_example.py\" in the Spark repo."], "answer_start": [475]}}
{"context": ".ml.feature.NGram\nval\nwordDataFrame\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0\n,\nArray\n(\n\"Hi\"\n,\n\"I\"\n,\n\"heard\"\n,\n\"about\"\n,\n\"Spark\"\n)),\n(\n1\n,\nArray\n(\n\"I\"\n,\n\"wish\"\n,\n\"Java\"\n,\n\"could\"\n,\n\"use\"\n,\n\"case\"\n,\n\"classes\"\n)),\n(\n2\n,\nArray\n(\n\"Logistic\"\n,\n\"regression\"\n,\n\"models\"\n,\n\"are\"\n,\n\"neat\"\n))\n)).\ntoDF\n(\n\"id\"\n,\n\"words\"\n)\nval\nngram\n=\nnew\nNGram\n().\nsetN\n(\n2\n).\nsetInputCol\n(\n\"words\"\n).\nsetOutputCol\n(\n\"ngrams\"\n)\nval\nngramDataFrame\n=\nngram\n.\ntransform\n(\nwordDataFrame\n)\nngramDataFrame\n.\nselect\n(\n\"ngrams\"\n).\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\" in the Spark repo.\nRefer to the\nNGram Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.NGram\n;\nimport\norg.apache.spark.", "question": "What is set as the input column for the NGram transformer?", "answers": {"text": ["\"words\""], "answer_start": [297]}}
{"context": "rrays\n.\nasList\n(\n\"Logistic\"\n,\n\"regression\"\n,\n\"models\"\n,\n\"are\"\n,\n\"neat\"\n))\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"id\"\n,\nDataTypes\n.\nIntegerType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"words\"\n,\nDataTypes\n.\ncreateArrayType\n(\nDataTypes\n.\nStringType\n),\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\nwordDataFrame\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nNGram\nngramTransformer\n=\nnew\nNGram\n().\nsetN\n(\n2\n).\nsetInputCol\n(\n\"words\"\n).\nsetOutputCol\n(\n\"ngrams\"\n);\nDataset\n<\nRow\n>\nngramDataFrame\n=\nngramTransformer\n.\ntransform\n(\nwordDataFrame\n);\nngramDataFrame\n.\nselect\n(\n\"ngrams\"\n).\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\" in the Spark repo.\nBinarizer\nBinarization is the proc", "question": "What is set as the input column for the NGram transformer?", "answers": {"text": ["\"words\""], "answer_start": [233]}}
{"context": ".\ncreateDataFrame\n([\n(\n0\n,\n0.1\n),\n(\n1\n,\n0.8\n),\n(\n2\n,\n0.2\n)\n],\n[\n\"\nid\n\"\n,\n\"\nfeature\n\"\n])\nbinarizer\n=\nBinarizer\n(\nthreshold\n=\n0.5\n,\ninputCol\n=\n\"\nfeature\n\"\n,\noutputCol\n=\n\"\nbinarized_feature\n\"\n)\nbinarizedDataFrame\n=\nbinarizer\n.\ntransform\n(\ncontinuousDataFrame\n)\nprint\n(\n\"\nBinarizer output with Threshold = %f\n\"\n%\nbinarizer\n.\ngetThreshold\n())\nbinarizedDataFrame\n.\nshow\n()\nFind full example code at \"examples/src/main/python/ml/binarizer_example.py\" in the Spark repo.\nRefer to the\nBinarizer Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.Binarizer\nval\ndata\n=\nimmutable\n.\nArraySeq\n.\nunsafeWrapArray\n(\nArray\n((\n0\n,\n0.1\n),\n(\n1\n,\n0.8\n),\n(\n2\n,\n0.2\n)))\nval\ndataFrame\n=\nspark\n.\ncreateDataFrame\n(\ndata\n).\ntoDF\n(\n\"id\"\n,\n\"feature\"\n)\nval\nbinarizer\n:\nBinarizer\n=\nnew\nBinarizer\n()\n.\nsetInpu", "question": "Where can I find the full example code for the Binarizer?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/binarizer_example.py\" in the Spark repo."], "answer_start": [367]}}
{"context": "]{\nnew\nStructField\n(\n\"features\"\n,\nnew\nVectorUDT\n(),\nfalse\n,\nMetadata\n.\nempty\n()),\n});\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nPCAModel\npca\n=\nnew\nPCA\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"pcaFeatures\"\n)\n.\nsetK\n(\n3\n)\n.\nfit\n(\ndf\n);\nDataset\n<\nRow\n>\nresult\n=\npca\n.\ntransform\n(\ndf\n).\nselect\n(\n\"pcaFeatures\"\n);\nresult\n.\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\" in the Spark repo.\nPolynomialExpansion\nPolynomial expansion\nis the process of expanding your features into a polynomial space, which is formulated by an n-degree combination of original dimensions. A\nPolynomialExpansion\nclass provides this functionality.  The example below shows how to expand your features into a 3-degree polynomial ", "question": "What is the degree of the polynomial expansion shown in the example?", "answers": {"text": ["3-degree polynomial"], "answer_start": [780]}}
{"context": "\n.\nshow\n(\ntruncate\n=\nFalse\n)\nFind full example code at \"examples/src/main/python/ml/polynomial_expansion_example.py\" in the Spark repo.\nRefer to the\nPolynomialExpansion Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.PolynomialExpansion\nimport\norg.apache.spark.ml.linalg.Vectors\nval\ndata\n=\nArray\n(\nVectors\n.\ndense\n(\n2.0\n,\n1.0\n),\nVectors\n.\ndense\n(\n0.0\n,\n0.0\n),\nVectors\n.\ndense\n(\n3.0\n,\n-\n1.0\n)\n)\nval\ndf\n=\nspark\n.\ncreateDataFrame\n(\nimmutable\n.\nArraySeq\n.\nunsafeWrapArray\n(\ndata\n.\nmap\n(\nTuple1\n.\napply\n)))\n.\ntoDF\n(\n\"features\"\n)\nval\npolyExpansion\n=\nnew\nPolynomialExpansion\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"polyFeatures\"\n)\n.\nsetDegree\n(\n3\n)\nval\npolyDF\n=\npolyExpansion\n.\ntransform\n(\ndf\n)\npolyDF\n.\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/", "question": "Where can I find the full example code for PolynomialExpansion?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/polynomial_expansion_example.py\" in the Spark repo."], "answer_start": [29]}}
{"context": "eatures\"\n)\n.\nsetDegree\n(\n3\n)\nval\npolyDF\n=\npolyExpansion\n.\ntransform\n(\ndf\n)\npolyDF\n.\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\" in the Spark repo.\nRefer to the\nPolynomialExpansion Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.PolynomialExpansion\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nPolynomialExpansion\npolyExpansion\n=\nnew\nPolyno", "question": "Where can I find a full example code for PolynomialExpansion?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\" in the Spark repo."], "answer_start": [99]}}
{"context": "ema\n);\nDataset\n<\nRow\n>\npolyDF\n=\npolyExpansion\n.\ntransform\n(\ndf\n);\npolyDF\n.\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\" in the Spark repo.\nDiscrete Cosine Transform (DCT)\nThe\nDiscrete Cosine\nTransform\ntransforms a length $N$ real-valued sequence in the time domain into\nanother length $N$ real-valued sequence in the frequency domain. A\nDCT\nclass\nprovides this functionality, implementing the\nDCT-II\nand scaling the result by $1/\\sqrt{2}$ such that the representing matrix\nfor the transform is unitary. No shift is applied to the transformed\nsequence (e.g. the $0$th element of the transformed sequence is the\n$0$th DCT coefficient and\nnot\nthe $N/2$th).\nExamples\nRefer to the\nDCT Python docs\nfor more details on t", "question": "Where can I find a full example code for Polynomial Expansion?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\" in the Spark repo."], "answer_start": [91]}}
{"context": "$th element of the transformed sequence is the\n$0$th DCT coefficient and\nnot\nthe $N/2$th).\nExamples\nRefer to the\nDCT Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nDCT\nfrom\npyspark.ml.linalg\nimport\nVectors\ndf\n=\nspark\n.\ncreateDataFrame\n([\n(\nVectors\n.\ndense\n([\n0.0\n,\n1.0\n,\n-\n2.0\n,\n3.0\n]),),\n(\nVectors\n.\ndense\n([\n-\n1.0\n,\n2.0\n,\n4.0\n,\n-\n7.0\n]),),\n(\nVectors\n.\ndense\n([\n14.0\n,\n-\n2.0\n,\n-\n5.0\n,\n1.0\n]),)],\n[\n\"\nfeatures\n\"\n])\ndct\n=\nDCT\n(\ninverse\n=\nFalse\n,\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nfeaturesDCT\n\"\n)\ndctDf\n=\ndct\n.\ntransform\n(\ndf\n)\ndctDf\n.\nselect\n(\n\"\nfeaturesDCT\n\"\n).\nshow\n(\ntruncate\n=\nFalse\n)\nFind full example code at \"examples/src/main/python/ml/dct_example.py\" in the Spark repo.\nRefer to the\nDCT Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature", "question": "Where can I find the full example code for the DCT example?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/dct_example.py\" in the Spark repo."], "answer_start": [619]}}
{"context": "ples/src/main/python/ml/dct_example.py\" in the Spark repo.\nRefer to the\nDCT Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.DCT\nimport\norg.apache.spark.ml.linalg.Vectors\nval\ndata\n=\nSeq\n(\nVectors\n.\ndense\n(\n0.0\n,\n1.0\n,\n-\n2.0\n,\n3.0\n),\nVectors\n.\ndense\n(-\n1.0\n,\n2.0\n,\n4.0\n,\n-\n7.0\n),\nVectors\n.\ndense\n(\n14.0\n,\n-\n2.0\n,\n-\n5.0\n,\n1.0\n))\nval\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n.\nmap\n(\nTuple1\n.\napply\n)).\ntoDF\n(\n\"features\"\n)\nval\ndct\n=\nnew\nDCT\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"featuresDCT\"\n)\n.\nsetInverse\n(\nfalse\n)\nval\ndctDf\n=\ndct\n.\ntransform\n(\ndf\n)\ndctDf\n.\nselect\n(\n\"featuresDCT\"\n).\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\" in the Spark repo.\nRefer to the\nDCT Java docs\nfor more details on the AP", "question": "Where can I find the full example code for DCT?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\" in the Spark repo."], "answer_start": [630]}}
{"context": " row containing the unseen label entirely\nput unseen labels in a special additional bucket, at index numLabels\nExamples\nLet’s go back to our previous example but this time reuse our previously defined\nStringIndexer\non the following dataset:\nid | category\n----|----------\n 0  | a\n 1  | b\n 2  | c\n 3  | d\n 4  | e\nIf you’ve not set how\nStringIndexer\nhandles unseen labels or set it to\n“error”, an exception will be thrown.\nHowever, if you had called\nsetHandleInvalid(\"skip\")\n, the following dataset\nwill be generated:\nid | category | categoryIndex\n----|----------|---------------\n 0  | a        | 0.0\n 1  | b        | 2.0\n 2  | c        | 1.0\nNotice that the rows containing “d” or “e” do not appear.\nIf you call\nsetHandleInvalid(\"keep\")\n, the following dataset\nwill be generated:\nid | category | catego", "question": "What happens if StringIndexer encounters unseen labels and is set to \"skip\"?", "answers": {"text": ["Notice that the rows containing “d” or “e” do not appear."], "answer_start": [640]}}
{"context": "t the rows containing “d” or “e” do not appear.\nIf you call\nsetHandleInvalid(\"keep\")\n, the following dataset\nwill be generated:\nid | category | categoryIndex\n----|----------|---------------\n 0  | a        | 0.0\n 1  | b        | 2.0\n 2  | c        | 1.0\n 3  | d        | 3.0\n 4  | e        | 3.0\nNotice that the rows containing “d” or “e” are mapped to index “3.0”\nRefer to the\nStringIndexer Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nStringIndexer\ndf\n=\nspark\n.\ncreateDataFrame\n(\n[(\n0\n,\n\"\na\n\"\n),\n(\n1\n,\n\"\nb\n\"\n),\n(\n2\n,\n\"\nc\n\"\n),\n(\n3\n,\n\"\na\n\"\n),\n(\n4\n,\n\"\na\n\"\n),\n(\n5\n,\n\"\nc\n\"\n)],\n[\n\"\nid\n\"\n,\n\"\ncategory\n\"\n])\nindexer\n=\nStringIndexer\n(\ninputCol\n=\n\"\ncategory\n\"\n,\noutputCol\n=\n\"\ncategoryIndex\n\"\n)\nindexed\n=\nindexer\n.\nfit\n(\ndf\n).\ntransform\n(\ndf\n)\nindexed\n.\nshow\n()\nFind full example code", "question": "What happens when you call setHandleInvalid(\"keep\")?", "answers": {"text": ["the following dataset\nwill be generated:\nid | category | categoryIndex\n----|----------|---------------\n 0  | a        | 0.0\n 1  | b        | 2.0\n 2  | c        | 1.0\n 3  | d        | 3.0\n 4  | e        | 3.0"], "answer_start": [87]}}
{"context": "\n(\ninputCol\n=\n\"\ncategory\n\"\n,\noutputCol\n=\n\"\ncategoryIndex\n\"\n)\nindexed\n=\nindexer\n.\nfit\n(\ndf\n).\ntransform\n(\ndf\n)\nindexed\n.\nshow\n()\nFind full example code at \"examples/src/main/python/ml/string_indexer_example.py\" in the Spark repo.\nRefer to the\nStringIndexer Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.StringIndexer\nval\ndf\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n((\n0\n,\n\"a\"\n),\n(\n1\n,\n\"b\"\n),\n(\n2\n,\n\"c\"\n),\n(\n3\n,\n\"a\"\n),\n(\n4\n,\n\"a\"\n),\n(\n5\n,\n\"c\"\n))\n).\ntoDF\n(\n\"id\"\n,\n\"category\"\n)\nval\nindexer\n=\nnew\nStringIndexer\n()\n.\nsetInputCol\n(\n\"category\"\n)\n.\nsetOutputCol\n(\n\"categoryIndex\"\n)\nval\nindexed\n=\nindexer\n.\nfit\n(\ndf\n).\ntransform\n(\ndf\n)\nindexed\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\" in the Spark repo.\nRefer to t", "question": "Where can I find the full example code for StringIndexer in Scala?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\" in the Spark repo."], "answer_start": [663]}}
{"context": "ed\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\" in the Spark repo.\nRefer to the\nStringIndexer Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.StringIndexer\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nimport\nstatic\norg\n.\napache\n.\nspark\n.\nsql\n.\ntypes\n.\nDataTypes\n.*;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n0\n,\n\"a\"\n),\nRowFactory\n.\ncreate\n(\n1\n,\n\"b\"\n),\nRowFactory\n.\ncreate\n(\n2\n,\n\"c\"\n),\nRowFactory\n.\ncreate\n(\n3\n,\n\"a\"\n),\nRowFactory\n.\ncreate\n(\n4\n,\n\"a\"\n),\nRowFactory\n.\ncrea", "question": "Where can I find a full example code for StringIndexer?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\" in the Spark repo."], "answer_start": [13]}}
{"context": "\nRowFactory\n.\ncreate\n(\n1\n,\n\"b\"\n),\nRowFactory\n.\ncreate\n(\n2\n,\n\"c\"\n),\nRowFactory\n.\ncreate\n(\n3\n,\n\"a\"\n),\nRowFactory\n.\ncreate\n(\n4\n,\n\"a\"\n),\nRowFactory\n.\ncreate\n(\n5\n,\n\"c\"\n)\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\ncreateStructField\n(\n\"id\"\n,\nIntegerType\n,\nfalse\n),\ncreateStructField\n(\n\"category\"\n,\nStringType\n,\nfalse\n)\n});\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nStringIndexer\nindexer\n=\nnew\nStringIndexer\n()\n.\nsetInputCol\n(\n\"category\"\n)\n.\nsetOutputCol\n(\n\"categoryIndex\"\n);\nDataset\n<\nRow\n>\nindexed\n=\nindexer\n.\nfit\n(\ndf\n).\ntransform\n(\ndf\n);\nindexed\n.\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\" in the Spark repo.\nIndexToString\nSymmetrically to\nStringIndexer\n,\nIndexToString\nmaps a column of l", "question": "What does the StringIndexer input column need to be?", "answers": {"text": ["\"category\""], "answer_start": [295]}}
{"context": "ck to original string column\n'\n%s\n'\nusing\n\"\n\"\nlabels in metadata\n\"\n%\n(\nconverter\n.\ngetInputCol\n(),\nconverter\n.\ngetOutputCol\n()))\nconverted\n.\nselect\n(\n\"\nid\n\"\n,\n\"\ncategoryIndex\n\"\n,\n\"\noriginalCategory\n\"\n).\nshow\n()\nFind full example code at \"examples/src/main/python/ml/index_to_string_example.py\" in the Spark repo.\nRefer to the\nIndexToString Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.attribute.Attribute\nimport\norg.apache.spark.ml.feature.\n{\nIndexToString\n,\nStringIndexer\n}\nval\ndf\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0\n,\n\"a\"\n),\n(\n1\n,\n\"b\"\n),\n(\n2\n,\n\"c\"\n),\n(\n3\n,\n\"a\"\n),\n(\n4\n,\n\"a\"\n),\n(\n5\n,\n\"c\"\n)\n)).\ntoDF\n(\n\"id\"\n,\n\"category\"\n)\nval\nindexer\n=\nnew\nStringIndexer\n()\n.\nsetInputCol\n(\n\"category\"\n)\n.\nsetOutputCol\n(\n\"categoryIndex\"\n)\n.\nfit\n(\ndf\n)\nval\nindexed\n=\nindexer\n.\ntransform\n(\ndf\n)\np", "question": "Where can I find a full example code for IndexToString?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/index_to_string_example.py\" in the Spark repo."], "answer_start": [211]}}
{"context": "ntln\n(\ns\n\"Transformed indexed column '${converter.getInputCol}' back to original string \"\n+\ns\n\"column '${converter.getOutputCol}' using labels in metadata\"\n)\nconverted\n.\nselect\n(\n\"id\"\n,\n\"categoryIndex\"\n,\n\"originalCategory\"\n).\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\" in the Spark repo.\nRefer to the\nIndexToString Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.attribute.Attribute\n;\nimport\norg.apache.spark.ml.feature.IndexToString\n;\nimport\norg.apache.spark.ml.feature.StringIndexer\n;\nimport\norg.apache.spark.ml.feature.StringIndexerModel\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTyp", "question": "Where can I find a full example code for IndexToString?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\" in the Spark repo."], "answer_start": [234]}}
{"context": "l/JavaIndexToStringExample.java\" in the Spark repo.\nOneHotEncoder\nOne-hot encoding\nmaps a categorical feature, represented as a label index, to a binary vector with at most a single one-value indicating the presence of a specific feature value from among the set of all feature values. This encoding allows algorithms which expect continuous features, such as Logistic Regression, to use categorical features. For string type input data, it is common to encode categorical features using\nStringIndexer\nfirst.\nOneHotEncoder\ncan transform multiple columns, returning an one-hot-encoded output vector column for each input column. It is common to merge these vectors into a single feature vector using\nVectorAssembler\n.\nOneHotEncoder\nsupports the\nhandleInvalid\nparameter to choose how to handle invalid ", "question": "What type of data is commonly used to encode categorical features before using OneHotEncoder with string type input?", "answers": {"text": ["StringIndexer"], "answer_start": [488]}}
{"context": "these vectors into a single feature vector using\nVectorAssembler\n.\nOneHotEncoder\nsupports the\nhandleInvalid\nparameter to choose how to handle invalid input during transforming data. Available options include ‘keep’ (any invalid inputs are assigned to an extra categorical index) and ‘error’ (throw an error).\nExamples\nRefer to the\nOneHotEncoder Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nOneHotEncoder\ndf\n=\nspark\n.\ncreateDataFrame\n([\n(\n0.0\n,\n1.0\n),\n(\n1.0\n,\n0.0\n),\n(\n2.0\n,\n1.0\n),\n(\n0.0\n,\n2.0\n),\n(\n0.0\n,\n1.0\n),\n(\n2.0\n,\n0.0\n)\n],\n[\n\"\ncategoryIndex1\n\"\n,\n\"\ncategoryIndex2\n\"\n])\nencoder\n=\nOneHotEncoder\n(\ninputCols\n=\n[\n\"\ncategoryIndex1\n\"\n,\n\"\ncategoryIndex2\n\"\n],\noutputCols\n=\n[\n\"\ncategoryVec1\n\"\n,\n\"\ncategoryVec2\n\"\n])\nmodel\n=\nencoder\n.\nfit\n(\ndf\n)\nencoded\n=\nmodel\n.\ntransform\n(\ndf\n)", "question": "What are the available options for the `handleInvalid` parameter in `OneHotEncoder`?", "answers": {"text": ["Available options include ‘keep’ (any invalid inputs are assigned to an extra categorical index) and ‘error’ (throw an error)."], "answer_start": [182]}}
{"context": "Index1\n\"\n,\n\"\ncategoryIndex2\n\"\n],\noutputCols\n=\n[\n\"\ncategoryVec1\n\"\n,\n\"\ncategoryVec2\n\"\n])\nmodel\n=\nencoder\n.\nfit\n(\ndf\n)\nencoded\n=\nmodel\n.\ntransform\n(\ndf\n)\nencoded\n.\nshow\n()\nFind full example code at \"examples/src/main/python/ml/onehot_encoder_example.py\" in the Spark repo.\nRefer to the\nOneHotEncoder Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.OneHotEncoder\nval\ndf\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0.0\n,\n1.0\n),\n(\n1.0\n,\n0.0\n),\n(\n2.0\n,\n1.0\n),\n(\n0.0\n,\n2.0\n),\n(\n0.0\n,\n1.0\n),\n(\n2.0\n,\n0.0\n)\n)).\ntoDF\n(\n\"categoryIndex1\"\n,\n\"categoryIndex2\"\n)\nval\nencoder\n=\nnew\nOneHotEncoder\n()\n.\nsetInputCols\n(\nArray\n(\n\"categoryIndex1\"\n,\n\"categoryIndex2\"\n))\n.\nsetOutputCols\n(\nArray\n(\n\"categoryVec1\"\n,\n\"categoryVec2\"\n))\nval\nmodel\n=\nencoder\n.\nfit\n(\ndf\n)\nval\nencoded\n=\nmodel\n.\ntransform\n(\ndf\n)\nenc", "question": "Where can I find a full example code for the OneHotEncoder?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/onehot_encoder_example.py\" in the Spark repo."], "answer_start": [169]}}
{"context": "oryIndex2\"\n))\n.\nsetOutputCols\n(\nArray\n(\n\"categoryVec1\"\n,\n\"categoryVec2\"\n))\nval\nmodel\n=\nencoder\n.\nfit\n(\ndf\n)\nval\nencoded\n=\nmodel\n.\ntransform\n(\ndf\n)\nencoded\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala\" in the Spark repo.\nRefer to the\nOneHotEncoder Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.OneHotEncoder\n;\nimport\norg.apache.spark.ml.feature.OneHotEncoderModel\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types", "question": "Where can I find a full example code for OneHotEncoder?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala\" in the Spark repo."], "answer_start": [165]}}
{"context": "())\n});\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nOneHotEncoder\nencoder\n=\nnew\nOneHotEncoder\n()\n.\nsetInputCols\n(\nnew\nString\n[]\n{\n\"categoryIndex1\"\n,\n\"categoryIndex2\"\n})\n.\nsetOutputCols\n(\nnew\nString\n[]\n{\n\"categoryVec1\"\n,\n\"categoryVec2\"\n});\nOneHotEncoderModel\nmodel\n=\nencoder\n.\nfit\n(\ndf\n);\nDataset\n<\nRow\n>\nencoded\n=\nmodel\n.\ntransform\n(\ndf\n);\nencoded\n.\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java\" in the Spark repo.\nTargetEncoder\nTarget Encoding\nis a data-preprocessing technique that transforms high-cardinality categorical features into quasi-continuous scalar attributes suited for use in regression-type models. This paradigm maps individual values of an independent feature to a scalar, representing som", "question": "Where can I find a full example code for the JavaOneHotEncoder?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java\" in the Spark repo."], "answer_start": [379]}}
{"context": "olumn use cases, or\ninputCols\nand\noutputCols\nfor multi-column use cases (both arrays required to have the same size). These columns are expected to contain categorical indices (positive integers), being missing values (null) treated as a separate category. Data type must be any subclass of ‘NumericType’. For string type input data, it is common to encode categorical features using\nStringIndexer\nfirst.\nUser can specify the target column name by setting\nlabel\n. This column is expected to contain the ground-truth labels from which encodings will be derived. Observations with missing label (null) are not considered when calculating estimates. Data type must be any subclass of ‘NumericType’.\nTargetEncoder\nsupports the\nhandleInvalid\nparameter to choose how to handle invalid input, meaning catego", "question": "What data type must the input columns and label column have?", "answers": {"text": ["Data type must be any subclass of ‘NumericType’."], "answer_start": [257]}}
{"context": "a type must be any subclass of ‘NumericType’.\nTargetEncoder\nsupports the\nhandleInvalid\nparameter to choose how to handle invalid input, meaning categories not seen at training, when encoding new data. Available options include ‘keep’ (any invalid inputs are assigned to an extra categorical index) and ‘error’ (throw an exception).\nTargetEncoder\nsupports the\ntargetType\nparameter to choose the label type when fitting data, affecting how estimates are calculated. Available options include ‘binary’  and ‘continuous’.\nWhen set to ‘binary’, the target attribute $Y$ is expected to be binary, $Y\\in{ 0,1 }$. The transformation maps individual values $X_{i}$ to the conditional probability of $Y$ given that $X=X_{i}\\;$: $\\;\\; S_{i}=P(Y\\mid X=X_{i})$. This approach is also known as bin-counting.\nWhen s", "question": "What options are available for the `handleInvalid` parameter in TargetEncoder?", "answers": {"text": ["Available options include ‘keep’ (any invalid inputs are assigned to an extra categorical index) and ‘error’ (throw an exception)."], "answer_start": [201]}}
{"context": "ng encodings $S_{i}$ according only to in-class statistics makes this estimates very unreliable, and rarely seen categories will very likely cause overfitting in learning.\nSmoothing prevents this behaviour by weighting in-class estimates with overall estimates according to the relative size of the particular class on the whole dataset.\n$\\;\\;\\; S_{i}=\\lambda(n_{i})\\, P(Y\\mid X=X_{i})+(1-\\lambda(n_{i}))\\, P(Y)$ for the binary case\n$\\;\\;\\; S_{i}=\\lambda(n_{i})\\, E[Y\\mid X=X_{i}]+(1-\\lambda(n_{i}))\\, E[Y]$ for the continuous case\nbeing $\\lambda(n_{i})$ a monotonically increasing function on $n_{i}$, bounded between 0 and 1.\nUsually $\\lambda(n_{i})$ is implemented as the parametric function $\\lambda(n_{i})=\\frac{n_{i}}{n_{i}+m}$, where $m$ is the smoothing factor, represented by\nsmoothing\nparam", "question": "Como o suavização previne o comportamento de estimativas não confiáveis e overfitting?", "answers": {"text": ["Smoothing prevents this behaviour by weighting in-class estimates with overall estimates according to the relative size of the particular class on the whole dataset."], "answer_start": [172]}}
{"context": "l column\nand\nencoded\nas the output column, we are able to fit a model\non the data to learn encodings and transform the data according\nto these mappings:\nfeature | target | encoded\n         | (bin)  |\n --------|--------|--------\n 1       | 0      | 0.333\n 1       | 1      | 0.333\n 1       | 0      | 0.333\n 2       | 1      | 0.5\n 2       | 0      | 0.5\n 3       | 1      | 1.0\nApplying\nTargetEncoder\nwith ‘continuous’  target type,\nfeature\nas the input column,\ntarget (cont)\nas the label column\nand\nencoded\nas the output column, we are able to fit a model\non the data to learn encodings and transform the data according\nto these mappings:\nfeature | target | encoded\n         | (cont) |\n --------|--------|--------\n 1       | 1.3    | 1.8\n 1       | 2.5    | 1.8\n 1       | 1.6    | 1.8\n 2       | 1.", "question": "What is used as the output column when applying TargetEncoder?", "answers": {"text": ["encoded"], "answer_start": [13]}}
{"context": "ls\n=\n[\n\"\ncategoryIndex1\n\"\n,\n\"\ncategoryIndex2\n\"\n],\noutputCols\n=\n[\n\"\ncategoryIndex1Target\n\"\n,\n\"\ncategoryIndex2Target\n\"\n],\nlabelCol\n=\n\"\nbinaryLabel\n\"\n,\ntargetType\n=\n\"\nbinary\n\"\n)\nmodel\n=\nencoder\n.\nfit\n(\ndf\n)\nencoded\n=\nmodel\n.\ntransform\n(\ndf\n)\nencoded\n.\nshow\n()\n# continuous target\nencoder\n=\nTargetEncoder\n(\ninputCols\n=\n[\n\"\ncategoryIndex1\n\"\n,\n\"\ncategoryIndex2\n\"\n],\noutputCols\n=\n[\n\"\ncategoryIndex1Target\n\"\n,\n\"\ncategoryIndex2Target\n\"\n],\nlabelCol\n=\n\"\ncontinuousLabel\n\"\n,\ntargetType\n=\n\"\ncontinuous\n\"\n)\nmodel\n=\nencoder\n.\nfit\n(\ndf\n)\nencoded\n=\nmodel\n.\ntransform\n(\ndf\n)\nencoded\n.\nshow\n()\nFind full example code at \"examples/src/main/python/ml/target_encoder_example.py\" in the Spark repo.\nRefer to the\nTargetEncoder Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.TargetEncoder\nval\ndf\n=", "question": "Where can I find a full example code for TargetEncoder?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/target_encoder_example.py\" in the Spark repo."], "answer_start": [575]}}
{"context": ")\n.\nsetTargetType\n(\n\"binary\"\n);\nval\nbin_model\n=\nbin_encoder\n.\nfit\n(\ndf\n)\nval\nbin_encoded\n=\nbin_model\n.\ntransform\n(\ndf\n)\nbin_encoded\n.\nshow\n()\n// continuous target\nval\ncont_encoder\n=\nnew\nTargetEncoder\n()\n.\nsetInputCols\n(\nArray\n(\n\"categoryIndex1\"\n,\n\"categoryIndex2\"\n))\n.\nsetOutputCols\n(\nArray\n(\n\"categoryIndex1Target\"\n,\n\"categoryIndex2Target\"\n))\n.\nsetLabelCol\n(\n\"continuousLabel\"\n)\n.\nsetTargetType\n(\n\"continuous\"\n);\nval\ncont_model\n=\ncont_encoder\n.\nfit\n(\ndf\n)\nval\ncont_encoded\n=\ncont_model\n.\ntransform\n(\ndf\n)\ncont_encoded\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/TargetEncoderExample.scala\" in the Spark repo.\nRefer to the\nTargetEncoder Java docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.TargetEncoder\n;\nimport\norg.apache.spark.ml.", "question": "What is set for the target type when using TargetEncoder for a binary target?", "answers": {"text": ["\"binary\""], "answer_start": [20]}}
{"context": "repo.\nRefer to the\nTargetEncoder Java docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.TargetEncoder\n;\nimport\norg.apache.spark.ml.feature.TargetEncoderModel\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n0.0\n,\n1.0\n,\n0\n,\n10.0\n),\nRowFactory\n.\ncreate\n(\n1.0\n,\n0.0\n,\n1\n,\n20.0\n),\nRowFactory\n.\ncreate\n(\n2.0\n,\n1.0\n,\n0\n,\n30.0\n),\nRowFactory\n.\ncreate\n(\n0.0\n,\n2.0\n,\n1\n,\n40.0\n),\nRowFactory\n.\ncreate\n(\n0.0\n,\n1.0\n,\n0\n,\n50.0\n),\nRowF", "question": "Which Java docs should be referred to for more details on the API?", "answers": {"text": ["TargetEncoder Java docs"], "answer_start": [19]}}
{"context": "reateDataFrame\n(\ndata\n,\nschema\n);\n// binary target\nTargetEncoder\nbin_encoder\n=\nnew\nTargetEncoder\n()\n.\nsetInputCols\n(\nnew\nString\n[]\n{\n\"categoryIndex1\"\n,\n\"categoryIndex2\"\n})\n.\nsetOutputCols\n(\nnew\nString\n[]\n{\n\"categoryIndex1Target\"\n,\n\"categoryIndex2Target\"\n})\n.\nsetLabelCol\n(\n\"binaryLabel\"\n)\n.\nsetTargetType\n(\n\"binary\"\n);\nTargetEncoderModel\nbin_model\n=\nbin_encoder\n.\nfit\n(\ndf\n);\nDataset\n<\nRow\n>\nbin_encoded\n=\nbin_model\n.\ntransform\n(\ndf\n);\nbin_encoded\n.\nshow\n();\n// continuous target\nTargetEncoder\ncont_encoder\n=\nnew\nTargetEncoder\n()\n.\nsetInputCols\n(\nnew\nString\n[]\n{\n\"categoryIndex1\"\n,\n\"categoryIndex2\"\n})\n.\nsetOutputCols\n(\nnew\nString\n[]\n{\n\"categoryIndex1Target\"\n,\n\"categoryIndex2Target\"\n})\n.\nsetLabelCol\n(\n\"continuousLabel\"\n)\n.\nsetTargetType\n(\n\"continuous\"\n);\nTargetEncoderModel\ncont_model\n=\ncont_encode", "question": "What target type is set for the continuous target encoder?", "answers": {"text": ["continuous"], "answer_start": [462]}}
{"context": "1Target\"\n,\n\"categoryIndex2Target\"\n})\n.\nsetLabelCol\n(\n\"continuousLabel\"\n)\n.\nsetTargetType\n(\n\"continuous\"\n);\nTargetEncoderModel\ncont_model\n=\ncont_encoder\n.\nfit\n(\ndf\n);\nDataset\n<\nRow\n>\ncont_encoded\n=\ncont_model\n.\ntransform\n(\ndf\n);\ncont_encoded\n.\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaTargetEncoderExample.java\" in the Spark repo.\nVectorIndexer\nVectorIndexer\nhelps index categorical features in datasets of\nVector\ns.\nIt can both automatically decide which features are categorical and convert original values to category indices.  Specifically, it does the following:\nTake an input column of type\nVector\nand a parameter\nmaxCategories\n.\nDecide which features should be categorical based on the number of distinct values, where features with at most\nma", "question": "What does VectorIndexer do?", "answers": {"text": ["It can both automatically decide which features are categorical and convert original values to category indices."], "answer_start": [467]}}
{"context": "tor\nand a parameter\nmaxCategories\n.\nDecide which features should be categorical based on the number of distinct values, where features with at most\nmaxCategories\nare declared categorical.\nCompute 0-based category indices for each categorical feature.\nIndex categorical features and transform original feature values to indices.\nIndexing categorical features allows algorithms such as Decision Trees and Tree Ensembles to treat categorical features appropriately, improving performance.\nExamples\nIn the example below, we read in a dataset of labeled points and then use\nVectorIndexer\nto decide which features should be treated as categorical.  We transform the categorical feature values to their indices.  This transformed data could then be passed to algorithms such as\nDecisionTreeRegressor\nthat ha", "question": "What is the purpose of indexing categorical features?", "answers": {"text": ["Indexing categorical features allows algorithms such as Decision Trees and Tree Ensembles to treat categorical features appropriately, improving performance."], "answer_start": [328]}}
{"context": "sform the categorical feature values to their indices.  This transformed data could then be passed to algorithms such as\nDecisionTreeRegressor\nthat handle categorical features.\nRefer to the\nVectorIndexer Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nVectorIndexer\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\nindexer\n=\nVectorIndexer\n(\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nindexed\n\"\n,\nmaxCategories\n=\n10\n)\nindexerModel\n=\nindexer\n.\nfit\n(\ndata\n)\ncategoricalFeatures\n=\nindexerModel\n.\ncategoryMaps\nprint\n(\n\"\nChose %d categorical features: %s\n\"\n%\n(\nlen\n(\ncategoricalFeatures\n),\n\"\n,\n\"\n.\njoin\n(\nstr\n(\nk\n)\nfor\nk\nin\ncategoricalFeatures\n.\nkeys\n())))\n# Create new column \"indexed\" with categorical values transformed to indices\nindexedD", "question": "What is the purpose of VectorIndexer?", "answers": {"text": ["to their indices.  This transformed data could then be passed to algorithms such as"], "answer_start": [37]}}
{"context": "rModel\n=\nindexer\n.\nfit\n(\ndata\n)\nval\ncategoricalFeatures\n:\nSet\n[\nInt\n]\n=\nindexerModel\n.\ncategoryMaps\n.\nkeys\n.\ntoSet\nprintln\n(\ns\n\"Chose ${categoricalFeatures.size} \"\n+\ns\n\"categorical features: ${categoricalFeatures.mkString(\"\n,\n\")}\"\n)\n// Create new column \"indexed\" with categorical values transformed to indices\nval\nindexedData\n=\nindexerModel\n.\ntransform\n(\ndata\n)\nindexedData\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\" in the Spark repo.\nRefer to the\nVectorIndexer Java docs\nfor more details on the API.\nimport\njava.util.Map\n;\nimport\norg.apache.spark.ml.feature.VectorIndexer\n;\nimport\norg.apache.spark.ml.feature.VectorIndexerModel\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nDataset\n<\nRow\n>\ndata", "question": "Where can I find a full example code for VectorIndexer?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\" in the Spark repo."], "answer_start": [385]}}
{"context": "ions as input columns, then you’ll get a 9-dimensional vector as the output column.\nExamples\nAssume that we have the following DataFrame with the columns “id1”, “vec1”, and “vec2”:\nid1|vec1          |vec2          \n  ---|--------------|--------------\n  1  |[1.0,2.0,3.0] |[8.0,4.0,5.0] \n  2  |[4.0,3.0,8.0] |[7.0,9.0,8.0] \n  3  |[6.0,1.0,9.0] |[2.0,3.0,6.0] \n  4  |[10.0,8.0,6.0]|[9.0,4.0,5.0] \n  5  |[9.0,2.0,7.0] |[10.0,7.0,3.0]\n  6  |[1.0,1.0,4.0] |[2.0,8.0,4.0]\nApplying\nInteraction\nwith those input columns,\nthen\ninteractedCol\nas the output column contains:\nid1|vec1          |vec2          |interactedCol                                         \n  ---|--------------|--------------|------------------------------------------------------\n  1  |[1.0,2.0,3.0] |[8.0,4.0,5.0] |[8.0,4.0,5.0,16.0,8.0", "question": "What does the 'interactedCol' column contain after applying Interaction with the input columns?", "answers": {"text": ["[8.0,4.0,5.0,16.0,8.0"], "answer_start": [779]}}
{"context": " \n  ---|--------------|--------------|------------------------------------------------------\n  1  |[1.0,2.0,3.0] |[8.0,4.0,5.0] |[8.0,4.0,5.0,16.0,8.0,10.0,24.0,12.0,15.0]            \n  2  |[4.0,3.0,8.0] |[7.0,9.0,8.0] |[56.0,72.0,64.0,42.0,54.0,48.0,112.0,144.0,128.0]     \n  3  |[6.0,1.0,9.0] |[2.0,3.0,6.0] |[36.0,54.0,108.0,6.0,9.0,18.0,54.0,81.0,162.0]        \n  4  |[10.0,8.0,6.0]|[9.0,4.0,5.0] |[360.0,160.0,200.0,288.0,128.0,160.0,216.0,96.0,120.0]\n  5  |[9.0,2.0,7.0] |[10.0,7.0,3.0]|[450.0,315.0,135.0,100.0,70.0,30.0,350.0,245.0,105.0] \n  6  |[1.0,1.0,4.0] |[2.0,8.0,4.0] |[12.0,48.0,24.0,12.0,48.0,24.0,48.0,192.0,96.0]\nRefer to the\nInteraction Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nInteraction\n,\nVectorAssembler\ndf\n=\nspark\n.\ncreateDataFrame\n(\n[(\n1\n,\n1\n,", "question": "What is the third row of the third column in the provided data?", "answers": {"text": ["[36.0,54.0,108.0,6.0,9.0,18.0,54.0,81.0,162.0]"], "answer_start": [311]}}
{"context": "tOutputCol\n(\n\"interactedCol\"\n)\nval\ninteracted\n=\ninteraction\n.\ntransform\n(\nassembled2\n)\ninteracted\n.\nshow\n(\ntruncate\n=\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\" in the Spark repo.\nRefer to the\nInteraction Java docs\nfor more details on the API.\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n1\n,\n1\n,\n2\n,\n3\n,\n8\n,\n4\n,\n5\n),\nRowFactory\n.\ncreate\n(\n2\n,\n4\n,\n3\n,\n8\n,\n7\n,\n9\n,\n8\n),\nRowFactory\n.\ncreate\n(\n3\n,\n6\n,\n1\n,\n9\n,\n2\n,\n3\n,\n6\n),\nRowFactory\n.\ncreate\n(\n4\n,\n10\n,\n8\n,\n6\n,\n9\n,\n4\n,\n5\n),\nRowFactory\n.\ncreate\n(\n5\n,\n9\n,\n2\n,\n7\n,\n10\n,\n7\n,\n3\n),\nRowFactory\n.\ncreate\n(\n6\n,\n1\n,\n1\n,\n4\n,\n2\n,\n8\n,\n4\n)\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"id1\"\n,\nDataTypes\n.\nIntegerType\n,\nfalse\n,\nMetadata\n.\nempty\n())", "question": "Where can I find a full example code for the Interaction example?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\" in the Spark repo."], "answer_start": [126]}}
{"context": "w to have unit $L^1$ norm and unit $L^\\infty$ norm.\nRefer to the\nNormalizer Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nNormalizer\nfrom\npyspark.ml.linalg\nimport\nVectors\ndataFrame\n=\nspark\n.\ncreateDataFrame\n([\n(\n0\n,\nVectors\n.\ndense\n([\n1.0\n,\n0.5\n,\n-\n1.0\n]),),\n(\n1\n,\nVectors\n.\ndense\n([\n2.0\n,\n1.0\n,\n1.0\n]),),\n(\n2\n,\nVectors\n.\ndense\n([\n4.0\n,\n10.0\n,\n2.0\n]),)\n],\n[\n\"\nid\n\"\n,\n\"\nfeatures\n\"\n])\n# Normalize each Vector using $L^1$ norm.\nnormalizer\n=\nNormalizer\n(\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nnormFeatures\n\"\n,\np\n=\n1.0\n)\nl1NormData\n=\nnormalizer\n.\ntransform\n(\ndataFrame\n)\nprint\n(\n\"\nNormalized using L^1 norm\n\"\n)\nl1NormData\n.\nshow\n()\n# Normalize each Vector using $L^\\infty$ norm.\nlInfNormData\n=\nnormalizer\n.\ntransform\n(\ndataFrame\n,\n{\nnormalizer\n.\np\n:\nfloat\n(\n\"\ninf\n\"\n)})\nprint\n(", "question": "Como normalizar cada Vetor usando a norma $L^1$?", "answers": {"text": ["Normalize each Vector using $L^1$ norm."], "answer_start": [411]}}
{"context": "d a dense output, so take care when applying to sparse input.\nStandardScaler\nis an\nEstimator\nwhich can be\nfit\non a dataset to produce a\nStandardScalerModel\n; this amounts to computing summary statistics.  The model can then transform a\nVector\ncolumn in a dataset to have unit standard deviation and/or zero mean features.\nNote that if the standard deviation of a feature is zero, it will return default\n0.0\nvalue in the\nVector\nfor that feature.\nExamples\nThe following example demonstrates how to load a dataset in libsvm format and then normalize each feature to have unit standard deviation.\nRefer to the\nStandardScaler Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nStandardScaler\ndataFrame\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt", "question": "What does StandardScaler do to a Vector column in a dataset?", "answers": {"text": ["to have unit standard deviation and/or zero mean features."], "answer_start": [263]}}
{"context": " more details on the API.\nimport\norg.apache.spark.ml.feature.StandardScaler\nval\ndataFrame\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\nval\nscaler\n=\nnew\nStandardScaler\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"scaledFeatures\"\n)\n.\nsetWithStd\n(\ntrue\n)\n.\nsetWithMean\n(\nfalse\n)\n// Compute summary statistics by fitting the StandardScaler.\nval\nscalerModel\n=\nscaler\n.\nfit\n(\ndataFrame\n)\n// Normalize each feature to have unit standard deviation.\nval\nscaledData\n=\nscalerModel\n.\ntransform\n(\ndataFrame\n)\nscaledData\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\" in the Spark repo.\nRefer to the\nStandardScaler Java docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.StandardSc", "question": "Where can I find a full example code for StandardScaler?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\" in the Spark repo."], "answer_start": [560]}}
{"context": " used instead of mean and standard deviation, which make it robust to outliers. It takes parameters:\nlower\n: 0.25 by default. Lower quantile to calculate quantile range, shared by all features.\nupper\n: 0.75 by default. Upper quantile to calculate quantile range, shared by all features.\nwithScaling\n: True by default. Scales the data to quantile range.\nwithCentering\n: False by default. Centers the data with median before scaling. It will build a dense output, so take care when applying to sparse input.\nRobustScaler\nis an\nEstimator\nwhich can be\nfit\non a dataset to produce a\nRobustScalerModel\n; this amounts to computing quantile statistics.  The model can then transform a\nVector\ncolumn in a dataset to have unit quantile range and/or zero median features.\nNote that if the quantile range of a fe", "question": "What is the default value for the 'withCentering' parameter?", "answers": {"text": ["False by default. Centers the data with median before scaling."], "answer_start": [369]}}
{"context": ".\ntransform\n(\ndataFrame\n);\nscaledData\n.\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaRobustScalerExample.java\" in the Spark repo.\nMinMaxScaler\nMinMaxScaler\ntransforms a dataset of\nVector\nrows, rescaling each feature to a specific range (often [0, 1]).  It takes parameters:\nmin\n: 0.0 by default. Lower bound after transformation, shared by all features.\nmax\n: 1.0 by default. Upper bound after transformation, shared by all features.\nMinMaxScaler\ncomputes summary statistics on a data set and produces a\nMinMaxScalerModel\n. The model can then transform each feature individually such that it is in the given range.\nThe rescaled value for a feature E is calculated as,\n\\begin{equation}\n  Rescaled(e_i) = \\frac{e_i - E_{min}}{E_{max} - E_{min}} * (max - m", "question": "What is the default value for the 'min' parameter in MinMaxScaler?", "answers": {"text": ["0.0 by default."], "answer_start": [327]}}
{"context": "iven range.\nThe rescaled value for a feature E is calculated as,\n\\begin{equation}\n  Rescaled(e_i) = \\frac{e_i - E_{min}}{E_{max} - E_{min}} * (max - min) + min\n\\end{equation}\nFor the case\n$E_{max} == E_{min}$\n,\n$Rescaled(e_i) = 0.5 * (max + min)$\nNote that since zero values will probably be transformed to non-zero values, output of the transformer will be\nDenseVector\neven for sparse input.\nExamples\nThe following example demonstrates how to load a dataset in libsvm format and then rescale each feature to [0, 1].\nRefer to the\nMinMaxScaler Python docs\nand the\nMinMaxScalerModel Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nMinMaxScaler\nfrom\npyspark.ml.linalg\nimport\nVectors\ndataFrame\n=\nspark\n.\ncreateDataFrame\n([\n(\n0\n,\nVectors\n.\ndense\n([\n1.0\n,\n0.1\n,\n-\n1.0\n]),),\n(\n1\n,\nVe", "question": "How is the rescaled value for a feature E calculated when E_max equals E_min?", "answers": {"text": ["$Rescaled(e_i) = 0.5 * (max + min)$"], "answer_start": [211]}}
{"context": "select\n(\n\"\nfeatures\n\"\n,\n\"\nscaledFeatures\n\"\n).\nshow\n()\nFind full example code at \"examples/src/main/python/ml/min_max_scaler_example.py\" in the Spark repo.\nRefer to the\nMinMaxScaler Scala docs\nand the\nMinMaxScalerModel Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.MinMaxScaler\nimport\norg.apache.spark.ml.linalg.Vectors\nval\ndataFrame\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0\n,\nVectors\n.\ndense\n(\n1.0\n,\n0.1\n,\n-\n1.0\n)),\n(\n1\n,\nVectors\n.\ndense\n(\n2.0\n,\n1.1\n,\n1.0\n)),\n(\n2\n,\nVectors\n.\ndense\n(\n3.0\n,\n10.1\n,\n3.0\n))\n)).\ntoDF\n(\n\"id\"\n,\n\"features\"\n)\nval\nscaler\n=\nnew\nMinMaxScaler\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"scaledFeatures\"\n)\n// Compute summary statistics and generate MinMaxScalerModel\nval\nscalerModel\n=\nscaler\n.\nfit\n(\ndataFrame\n)\n// rescale each feature to range [m", "question": "Where can I find a full example code for this process?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/min_max_scaler_example.py\" in the Spark repo."], "answer_start": [54]}}
{"context": "tures\"\n)\n// Compute summary statistics and generate MinMaxScalerModel\nval\nscalerModel\n=\nscaler\n.\nfit\n(\ndataFrame\n)\n// rescale each feature to range [min, max].\nval\nscaledData\n=\nscalerModel\n.\ntransform\n(\ndataFrame\n)\nprintln\n(\ns\n\"Features scaled to range: [${scaler.getMin}, ${scaler.getMax}]\"\n)\nscaledData\n.\nselect\n(\n\"features\"\n,\n\"scaledFeatures\"\n).\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\" in the Spark repo.\nRefer to the\nMinMaxScaler Java docs\nand the\nMinMaxScalerModel Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.MinMaxScaler\n;\nimport\norg.apache.spark.ml.feature.MinMaxScalerModel\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache", "question": "Where can I find a full example code for MinMaxScaler?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\" in the Spark repo."], "answer_start": [357]}}
{"context": "e.spark.ml.feature.MinMaxScaler\n;\nimport\norg.apache.spark.ml.feature.MinMaxScalerModel\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n0\n,\nVectors\n.\ndense\n(\n1.0\n,\n0.1\n,\n-\n1.0\n)),\nRowFactory\n.\ncreate\n(\n1\n,\nVectors\n.\ndense\n(\n2.0\n,\n1.1\n,\n1.0\n)),\nRowFactory\n.\ncreate\n(\n2\n,\nVectors\n.\ndense\n(\n3.0\n,\n10.1\n,\n3.0\n))\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"id\"\n,\nDataT", "question": "What is imported from org.apache.spark.ml.linalg?", "answers": {"text": ["Vectors"], "answer_start": [123]}}
{"context": "actory\n.\ncreate\n(\n2\n,\nVectors\n.\ndense\n(\n3.0\n,\n10.1\n,\n3.0\n))\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"id\"\n,\nDataTypes\n.\nIntegerType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"features\"\n,\nnew\nVectorUDT\n(),\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndataFrame\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nMinMaxScaler\nscaler\n=\nnew\nMinMaxScaler\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"scaledFeatures\"\n);\n// Compute summary statistics and generate MinMaxScalerModel\nMinMaxScalerModel\nscalerModel\n=\nscaler\n.\nfit\n(\ndataFrame\n);\n// rescale each feature to range [min, max].\nDataset\n<\nRow\n>\nscaledData\n=\nscalerModel\n.\ntransform\n(\ndataFrame\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Features scaled to range: [\"\n+\nscaler\n.\ngetMin\n()\n+\n\", \"\n+\nscaler\n.\ngetMax\n()\n", "question": "What is done to each feature in the scaledData dataset?", "answers": {"text": ["rescale each feature to range [min, max]."], "answer_start": [586]}}
{"context": "Data\n=\nscalerModel\n.\ntransform\n(\ndataFrame\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Features scaled to range: [\"\n+\nscaler\n.\ngetMin\n()\n+\n\", \"\n+\nscaler\n.\ngetMax\n()\n+\n\"]\"\n);\nscaledData\n.\nselect\n(\n\"features\"\n,\n\"scaledFeatures\"\n).\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\" in the Spark repo.\nMaxAbsScaler\nMaxAbsScaler\ntransforms a dataset of\nVector\nrows, rescaling each feature to range [-1, 1] \nby dividing through the maximum absolute value in each feature. It does not shift/center the \ndata, and thus does not destroy any sparsity.\nMaxAbsScaler\ncomputes summary statistics on a data set and produces a\nMaxAbsScalerModel\n. The \nmodel can then transform each feature individually to range [-1, 1].\nExamples\nThe following example demonstrat", "question": "What range does MaxAbsScaler rescale each feature to?", "answers": {"text": ["[-1, 1]"], "answer_start": [446]}}
{"context": "and produces a\nMaxAbsScalerModel\n. The \nmodel can then transform each feature individually to range [-1, 1].\nExamples\nThe following example demonstrates how to load a dataset in libsvm format and then rescale each feature to [-1, 1].\nRefer to the\nMaxAbsScaler Python docs\nand the\nMaxAbsScalerModel Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nMaxAbsScaler\nfrom\npyspark.ml.linalg\nimport\nVectors\ndataFrame\n=\nspark\n.\ncreateDataFrame\n([\n(\n0\n,\nVectors\n.\ndense\n([\n1.0\n,\n0.1\n,\n-\n8.0\n]),),\n(\n1\n,\nVectors\n.\ndense\n([\n2.0\n,\n1.0\n,\n-\n4.0\n]),),\n(\n2\n,\nVectors\n.\ndense\n([\n4.0\n,\n10.0\n,\n8.0\n]),)\n],\n[\n\"\nid\n\"\n,\n\"\nfeatures\n\"\n])\nscaler\n=\nMaxAbsScaler\n(\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nscaledFeatures\n\"\n)\n# Compute summary statistics and generate MaxAbsScalerModel\nscalerModel\n=\nscaler\n.", "question": "What is the range to which each feature is transformed by the model?", "answers": {"text": ["[-1, 1]"], "answer_start": [100]}}
{"context": "nt a potential out of Bucketizer bounds exception.\nNote also that the splits that you provided have to be in strictly increasing order, i.e.\ns0 < s1 < s2 < ... < sn\n.\nMore details can be found in the API docs for\nBucketizer\n.\nExamples\nThe following example demonstrates how to bucketize a column of\nDouble\ns into another index-wised column.\nRefer to the\nBucketizer Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nBucketizer\nsplits\n=\n[\n-\nfloat\n(\n\"\ninf\n\"\n),\n-\n0.5\n,\n0.0\n,\n0.5\n,\nfloat\n(\n\"\ninf\n\"\n)]\ndata\n=\n[(\n-\n999.9\n,),\n(\n-\n0.5\n,),\n(\n-\n0.3\n,),\n(\n0.0\n,),\n(\n0.2\n,),\n(\n999.9\n,)]\ndataFrame\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\n[\n\"\nfeatures\n\"\n])\nbucketizer\n=\nBucketizer\n(\nsplits\n=\nsplits\n,\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nbucketedFeatures\n\"\n)\n# Transform original data into its ", "question": "What is a requirement for the splits provided to the Bucketizer?", "answers": {"text": ["splits that you provided have to be in strictly increasing order, i.e.\ns0 < s1 < s2 < ... < sn"], "answer_start": [70]}}
{"context": "ures\n\"\n])\nbucketizer\n=\nBucketizer\n(\nsplits\n=\nsplits\n,\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nbucketedFeatures\n\"\n)\n# Transform original data into its bucket index.\nbucketedData\n=\nbucketizer\n.\ntransform\n(\ndataFrame\n)\nprint\n(\n\"\nBucketizer output with %d buckets\n\"\n%\n(\nlen\n(\nbucketizer\n.\ngetSplits\n())\n-\n1\n))\nbucketedData\n.\nshow\n()\nFind full example code at \"examples/src/main/python/ml/bucketizer_example.py\" in the Spark repo.\nRefer to the\nBucketizer Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.Bucketizer\nval\nsplits\n=\nArray\n(\nDouble\n.\nNegativeInfinity\n,\n-\n0.5\n,\n0.0\n,\n0.5\n,\nDouble\n.\nPositiveInfinity\n)\nval\ndata\n=\nArray\n(-\n999.9\n,\n-\n0.5\n,\n-\n0.3\n,\n0.0\n,\n0.2\n,\n999.9\n)\nval\ndataFrame\n=\nspark\n.\ncreateDataFrame\n(\nimmutable\n.\nArraySeq\n.\nunsafeWrapArray\n(\ndata\n.\nmap\n(\nTuple1\n", "question": "Where can I find the full example code for Bucketizer?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/bucketizer_example.py\" in the Spark repo."], "answer_start": [329]}}
{"context": "(-\n999.9\n,\n-\n0.5\n,\n-\n0.3\n,\n0.0\n,\n0.2\n,\n999.9\n)\nval\ndataFrame\n=\nspark\n.\ncreateDataFrame\n(\nimmutable\n.\nArraySeq\n.\nunsafeWrapArray\n(\ndata\n.\nmap\n(\nTuple1\n.\napply\n))).\ntoDF\n(\n\"features\"\n)\nval\nbucketizer\n=\nnew\nBucketizer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"bucketedFeatures\"\n)\n.\nsetSplits\n(\nsplits\n)\n// Transform original data into its bucket index.\nval\nbucketedData\n=\nbucketizer\n.\ntransform\n(\ndataFrame\n)\nprintln\n(\ns\n\"Bucketizer output with ${bucketizer.getSplits.length-1} buckets\"\n)\nbucketedData\n.\nshow\n()\nval\nsplitsArray\n=\nArray\n(\nArray\n(\nDouble\n.\nNegativeInfinity\n,\n-\n0.5\n,\n0.0\n,\n0.5\n,\nDouble\n.\nPositiveInfinity\n),\nArray\n(\nDouble\n.\nNegativeInfinity\n,\n-\n0.3\n,\n0.0\n,\n0.3\n,\nDouble\n.\nPositiveInfinity\n))\nval\ndata2\n=\nArray\n(\n(-\n999.9\n,\n-\n999.9\n),\n(-\n0.5\n,\n-\n0.2\n),\n(-\n0.3\n,\n-\n0.1\n),\n(\n0.0\n,\n0", "question": "What is the input column for the Bucketizer?", "answers": {"text": ["\"features\""], "answer_start": [170]}}
{"context": "iveInfinity\n,\n-\n0.3\n,\n0.0\n,\n0.3\n,\nDouble\n.\nPositiveInfinity\n))\nval\ndata2\n=\nArray\n(\n(-\n999.9\n,\n-\n999.9\n),\n(-\n0.5\n,\n-\n0.2\n),\n(-\n0.3\n,\n-\n0.1\n),\n(\n0.0\n,\n0.0\n),\n(\n0.2\n,\n0.4\n),\n(\n999.9\n,\n999.9\n))\nval\ndataFrame2\n=\nspark\n.\ncreateDataFrame\n(\nimmutable\n.\nArraySeq\n.\nunsafeWrapArray\n(\ndata2\n))\n.\ntoDF\n(\n\"features1\"\n,\n\"features2\"\n)\nval\nbucketizer2\n=\nnew\nBucketizer\n()\n.\nsetInputCols\n(\nArray\n(\n\"features1\"\n,\n\"features2\"\n))\n.\nsetOutputCols\n(\nArray\n(\n\"bucketedFeatures1\"\n,\n\"bucketedFeatures2\"\n))\n.\nsetSplitsArray\n(\nsplitsArray\n)\n// Transform original data into its bucket index.\nval\nbucketedData2\n=\nbucketizer2\n.\ntransform\n(\ndataFrame2\n)\nprintln\n(\ns\n\"Bucketizer output with [\"\n+\ns\n\"${bucketizer2.getSplitsArray(0).length-1}, \"\n+\ns\n\"${bucketizer2.getSplitsArray(1).length-1}] buckets for each input column\"\n)\nbuckete", "question": "What is done with the original data using the bucketizer2 object?", "answers": {"text": ["Transform original data into its bucket index."], "answer_start": [517]}}
{"context": "put with [\"\n+\ns\n\"${bucketizer2.getSplitsArray(0).length-1}, \"\n+\ns\n\"${bucketizer2.getSplitsArray(1).length-1}] buckets for each input column\"\n)\nbucketedData2\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\" in the Spark repo.\nRefer to the\nBucketizer Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.Bucketizer\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\ndouble\n[]\nsplits\n=\n{\nDouble\n.\nNEGATIVE_INFINITY\n,", "question": "Where can I find a full example code for Bucketizer?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\" in the Spark repo."], "answer_start": [167]}}
{"context": "ata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\ndouble\n[]\nsplits\n=\n{\nDouble\n.\nNEGATIVE_INFINITY\n,\n-\n0.5\n,\n0.0\n,\n0.5\n,\nDouble\n.\nPOSITIVE_INFINITY\n};\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(-\n999.9\n),\nRowFactory\n.\ncreate\n(-\n0.5\n),\nRowFactory\n.\ncreate\n(-\n0.3\n),\nRowFactory\n.\ncreate\n(\n0.0\n),\nRowFactory\n.\ncreate\n(\n0.2\n),\nRowFactory\n.\ncreate\n(\n999.9\n)\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"features\"\n,\nDataTypes\n.\nDoubleType\n,\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndataFrame\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nBucketizer\nbucketizer\n=\nnew\nBucketizer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"bucketedFeatures\"\n)\n.\nsetSplits\n(\nsplits\n);\n// Transform original data i", "question": "What is set as the input column for the Bucketizer?", "answers": {"text": ["\"features\""], "answer_start": [497]}}
{"context": "tputCols\n(\nnew\nString\n[]\n{\n\"bucketedFeatures1\"\n,\n\"bucketedFeatures2\"\n})\n.\nsetSplitsArray\n(\nsplitsArray\n);\n// Transform original data into its bucket index.\nDataset\n<\nRow\n>\nbucketedData2\n=\nbucketizer2\n.\ntransform\n(\ndataFrame2\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Bucketizer output with [\"\n+\n(\nbucketizer2\n.\ngetSplitsArray\n()[\n0\n].\nlength\n-\n1\n)\n+\n\", \"\n+\n(\nbucketizer2\n.\ngetSplitsArray\n()[\n1\n].\nlength\n-\n1\n)\n+\n\"] buckets for each input column\"\n);\nbucketedData2\n.\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\" in the Spark repo.\nElementwiseProduct\nElementwiseProduct multiplies each input vector by a provided “weight” vector, using element-wise multiplication. In other words, it scales each column of the dataset by a scalar multiplier.  Th", "question": "Where can I find the full example code for JavaBucketizerExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\" in the Spark repo."], "answer_start": [461]}}
{"context": "tor by a provided “weight” vector, using element-wise multiplication. In other words, it scales each column of the dataset by a scalar multiplier.  This represents the\nHadamard product\nbetween the input vector,\nv\nand transforming vector,\nw\n, to yield a result vector.\n\\[ \\begin{pmatrix}\nv_1 \\\\\n\\vdots \\\\\nv_N\n\\end{pmatrix} \\circ \\begin{pmatrix}\n                    w_1 \\\\\n                    \\vdots \\\\\n                    w_N\n                    \\end{pmatrix}\n= \\begin{pmatrix}\n  v_1 w_1 \\\\\n  \\vdots \\\\\n  v_N w_N\n  \\end{pmatrix}\n\\]\nExamples\nThis example below demonstrates how to transform vectors using a transforming vector value.\nRefer to the\nElementwiseProduct Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nElementwiseProduct\nfrom\npyspark.ml.linalg\nimport\nVectors\n# Creat", "question": "What does the Hadamard product between an input vector v and a transforming vector w yield?", "answers": {"text": ["to yield a result vector."], "answer_start": [242]}}
{"context": "atch transform the vectors to create new column:\ntransformer\n.\ntransform\n(\ndataFrame\n).\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\" in the Spark repo.\nRefer to the\nElementwiseProduct Java docs\nfor more details on the API.\nimport\njava.util.ArrayList\n;\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.ElementwiseProduct\n;\nimport\norg.apache.spark.ml.linalg.Vector\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\n// Create some ve", "question": "Where can I find a full example code for ElementwiseProduct?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\" in the Spark repo."], "answer_start": [96]}}
{"context": "nly support SQL syntax like\n\"SELECT ... FROM __THIS__ ...\"\nwhere\n\"__THIS__\"\nrepresents the underlying table of the input dataset.\nThe select clause specifies the fields, constants, and expressions to display in\nthe output, and can be any select clause that Spark SQL supports. Users can also\nuse Spark SQL built-in function and UDFs to operate on these selected columns.\nFor example,\nSQLTransformer\nsupports statements like:\nSELECT a, a + b AS a_b FROM __THIS__\nSELECT a, SQRT(b) AS b_sqrt FROM __THIS__ where a > 5\nSELECT a, b, SUM(c) AS c_sum FROM __THIS__ GROUP BY a, b\nExamples\nAssume that we have the following DataFrame with columns\nid\n,\nv1\nand\nv2\n:\nid |  v1 |  v2\n----|-----|-----\n 0  | 1.0 | 3.0  \n 2  | 2.0 | 5.0\nThis is the output of the\nSQLTransformer\nwith statement\n\"SELECT *, (v1 + v2) A", "question": "What does the \"__THIS__\" represent in the SQL syntax?", "answers": {"text": ["represents the underlying table of the input dataset."], "answer_start": [76]}}
{"context": "\nv2\n:\nid |  v1 |  v2\n----|-----|-----\n 0  | 1.0 | 3.0  \n 2  | 2.0 | 5.0\nThis is the output of the\nSQLTransformer\nwith statement\n\"SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__\"\n:\nid |  v1 |  v2 |  v3 |  v4\n----|-----|-----|-----|-----\n 0  | 1.0 | 3.0 | 4.0 | 3.0\n 2  | 2.0 | 5.0 | 7.0 |10.0\nRefer to the\nSQLTransformer Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nSQLTransformer\ndf\n=\nspark\n.\ncreateDataFrame\n([\n(\n0\n,\n1.0\n,\n3.0\n),\n(\n2\n,\n2.0\n,\n5.0\n)\n],\n[\n\"\nid\n\"\n,\n\"\nv1\n\"\n,\n\"\nv2\n\"\n])\nsqlTrans\n=\nSQLTransformer\n(\nstatement\n=\n\"\nSELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__\n\"\n)\nsqlTrans\n.\ntransform\n(\ndf\n).\nshow\n()\nFind full example code at \"examples/src/main/python/ml/sql_transformer.py\" in the Spark repo.\nRefer to the\nSQLTransformer Scala docs\nfor more ", "question": "What SQL statement is used within the SQLTransformer?", "answers": {"text": ["SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__"], "answer_start": [129]}}
{"context": "show\n()\nFind full example code at \"examples/src/main/python/ml/sql_transformer.py\" in the Spark repo.\nRefer to the\nSQLTransformer Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.SQLTransformer\nval\ndf\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n((\n0\n,\n1.0\n,\n3.0\n),\n(\n2\n,\n2.0\n,\n5.0\n))).\ntoDF\n(\n\"id\"\n,\n\"v1\"\n,\n\"v2\"\n)\nval\nsqlTrans\n=\nnew\nSQLTransformer\n().\nsetStatement\n(\n\"SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__\"\n)\nsqlTrans\n.\ntransform\n(\ndf\n).\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\" in the Spark repo.\nRefer to the\nSQLTransformer Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.SQLTransformer\n;\nimport\norg.apache.spark.", "question": "Where can I find the full example code for SQLTransformer in Python?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/sql_transformer.py\" in the Spark repo."], "answer_start": [8]}}
{"context": "t to combine\nhour\n,\nmobile\n, and\nuserFeatures\ninto a single feature vector\ncalled\nfeatures\nand use it to predict\nclicked\nor not.\nIf we set\nVectorAssembler\n’s input columns to\nhour\n,\nmobile\n, and\nuserFeatures\nand\noutput column to\nfeatures\n, after transformation we should get the following DataFrame:\nid | hour | mobile | userFeatures     | clicked | features\n----|------|--------|------------------|---------|-----------------------------\n 0  | 18   | 1.0    | [0.0, 10.0, 0.5] | 1.0     | [18.0, 1.0, 0.0, 10.0, 0.5]\nRefer to the\nVectorAssembler Python docs\nfor more details on the API.\nfrom\npyspark.ml.linalg\nimport\nVectors\nfrom\npyspark.ml.feature\nimport\nVectorAssembler\ndataset\n=\nspark\n.\ncreateDataFrame\n(\n[(\n0\n,\n18\n,\n1.0\n,\nVectors\n.\ndense\n([\n0.0\n,\n10.0\n,\n0.5\n]),\n1.0\n)],\n[\n\"\nid\n\"\n,\n\"\nhour\n\"\n,\n\"\nm", "question": "What is the output column name when using VectorAssembler with input columns hour, mobile, and userFeatures?", "answers": {"text": ["features"], "answer_start": [82]}}
{"context": "Assembler Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.VectorAssembler\nimport\norg.apache.spark.ml.linalg.Vectors\nval\ndataset\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n((\n0\n,\n18\n,\n1.0\n,\nVectors\n.\ndense\n(\n0.0\n,\n10.0\n,\n0.5\n),\n1.0\n))\n).\ntoDF\n(\n\"id\"\n,\n\"hour\"\n,\n\"mobile\"\n,\n\"userFeatures\"\n,\n\"clicked\"\n)\nval\nassembler\n=\nnew\nVectorAssembler\n()\n.\nsetInputCols\n(\nArray\n(\n\"hour\"\n,\n\"mobile\"\n,\n\"userFeatures\"\n))\n.\nsetOutputCol\n(\n\"features\"\n)\nval\noutput\n=\nassembler\n.\ntransform\n(\ndataset\n)\nprintln\n(\n\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\"\n)\noutput\n.\nselect\n(\n\"features\"\n,\n\"clicked\"\n).\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\" in the Spark repo.\nRefer to the\nVectorAssem", "question": "Which columns are assembled into the vector column 'features'?", "answers": {"text": ["'hour', 'mobile', 'userFeatures'"], "answer_start": [524]}}
{"context": "d full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\" in the Spark repo.\nRefer to the\nVectorAssembler Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\norg.apache.spark.ml.feature.VectorAssembler\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.*\n;\nimport\nstatic\norg\n.\napache\n.\nspark\n.\nsql\n.\ntypes\n.\nDataTypes\n.*;\nStructType\nschema\n=\ncreateStructType\n(\nnew\nStructField\n[]{\ncreateStructField\n(\n\"id\"\n,\nIntegerType\n,\nfalse\n),\ncreateStructField\n(\n\"hour\"\n,\nIntegerType\n,\nfalse\n),\ncreateStructField\n(\n\"mobile\"\n,\nDoubleType\n,\nfalse\n),\ncreateS", "question": "Where can I find a full example code for VectorAssembler?", "answers": {"text": ["d full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\" in the Spark repo."], "answer_start": [0]}}
{"context": "Field\n(\n\"id\"\n,\nIntegerType\n,\nfalse\n),\ncreateStructField\n(\n\"hour\"\n,\nIntegerType\n,\nfalse\n),\ncreateStructField\n(\n\"mobile\"\n,\nDoubleType\n,\nfalse\n),\ncreateStructField\n(\n\"userFeatures\"\n,\nnew\nVectorUDT\n(),\nfalse\n),\ncreateStructField\n(\n\"clicked\"\n,\nDoubleType\n,\nfalse\n)\n});\nRow\nrow\n=\nRowFactory\n.\ncreate\n(\n0\n,\n18\n,\n1.0\n,\nVectors\n.\ndense\n(\n0.0\n,\n10.0\n,\n0.5\n),\n1.0\n);\nDataset\n<\nRow\n>\ndataset\n=\nspark\n.\ncreateDataFrame\n(\nArrays\n.\nasList\n(\nrow\n),\nschema\n);\nVectorAssembler\nassembler\n=\nnew\nVectorAssembler\n()\n.\nsetInputCols\n(\nnew\nString\n[]{\n\"hour\"\n,\n\"mobile\"\n,\n\"userFeatures\"\n})\n.\nsetOutputCol\n(\n\"features\"\n);\nDataset\n<\nRow\n>\noutput\n=\nassembler\n.\ntransform\n(\ndataset\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column \"\n+\n\"'features'\"\n);\noutput\n.\nselect\n(\n\"features\"\n,\n", "question": "Quais colunas são montadas em uma coluna vetorial chamada 'features'?", "answers": {"text": ["'hour', 'mobile', 'userFeatures'"], "answer_start": [699]}}
{"context": "t\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column \"\n+\n\"'features'\"\n);\noutput\n.\nselect\n(\n\"features\"\n,\n\"clicked\"\n).\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\" in the Spark repo.\nVectorSizeHint\nIt can sometimes be useful to explicitly specify the size of the vectors for a column of\nVectorType\n. For example,\nVectorAssembler\nuses size information from its input columns to\nproduce size information and metadata for its output column. While in some cases this information\ncan be obtained by inspecting the contents of the column, in a streaming dataframe the contents are\nnot available until the stream is started.\nVectorSizeHint\nallows a user to explicitly specify the\n", "question": "Where can I find a full example code for JavaVectorAssembler?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\" in the Spark repo."], "answer_start": [179]}}
{"context": "embler\n=\nnew\nVectorAssembler\n()\n.\nsetInputCols\n(\nArray\n(\n\"hour\"\n,\n\"mobile\"\n,\n\"userFeatures\"\n))\n.\nsetOutputCol\n(\n\"features\"\n)\n// This dataframe can be used by downstream transformers as before\nval\noutput\n=\nassembler\n.\ntransform\n(\ndatasetWithSize\n)\nprintln\n(\n\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\"\n)\noutput\n.\nselect\n(\n\"features\"\n,\n\"clicked\"\n).\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/VectorSizeHintExample.scala\" in the Spark repo.\nRefer to the\nVectorSizeHint Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\norg.apache.spark.ml.feature.VectorAssembler\n;\nimport\norg.apache.spark.ml.feature.VectorSizeHint\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.ml.li", "question": "Which columns are assembled into the vector column 'features'?", "answers": {"text": ["'hour', 'mobile', 'userFeatures'"], "answer_start": [276]}}
{"context": "ot the right size are filtered out\"\n);\ndatasetWithSize\n.\nshow\n(\nfalse\n);\nVectorAssembler\nassembler\n=\nnew\nVectorAssembler\n()\n.\nsetInputCols\n(\nnew\nString\n[]{\n\"hour\"\n,\n\"mobile\"\n,\n\"userFeatures\"\n})\n.\nsetOutputCol\n(\n\"features\"\n);\n// This dataframe can be used by downstream transformers as before\nDataset\n<\nRow\n>\noutput\n=\nassembler\n.\ntransform\n(\ndatasetWithSize\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column \"\n+\n\"'features'\"\n);\noutput\n.\nselect\n(\n\"features\"\n,\n\"clicked\"\n).\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSizeHintExample.java\" in the Spark repo.\nQuantileDiscretizer\nQuantileDiscretizer\ntakes a column with continuous features and outputs a column with binned\ncategorical features. T", "question": "What columns are assembled into a vector column named 'features'?", "answers": {"text": ["'hour', 'mobile', 'userFeatures'"], "answer_start": [404]}}
{"context": "expensive operation). The lower and upper bin bounds\nwill be\n-Infinity\nand\n+Infinity\ncovering all real values.\nExamples\nAssume that we have a DataFrame with the columns\nid\n,\nhour\n:\nid | hour\n----|------\n 0  | 18.0\n----|------\n 1  | 19.0\n----|------\n 2  | 8.0\n----|------\n 3  | 5.0\n----|------\n 4  | 2.2\nhour\nis a continuous feature with\nDouble\ntype. We want to turn the continuous feature into\na categorical one. Given\nnumBuckets = 3\n, we should get the following DataFrame:\nid | hour | result\n----|------|------\n 0  | 18.0 | 2.0\n----|------|------\n 1  | 19.0 | 2.0\n----|------|------\n 2  | 8.0  | 1.0\n----|------|------\n 3  | 5.0  | 1.0\n----|------|------\n 4  | 2.2  | 0.0\nRefer to the\nQuantileDiscretizer Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nQuantileDiscretizer\nd", "question": "What type is the 'hour' feature in the example DataFrame?", "answers": {"text": ["Double"], "answer_start": [337]}}
{"context": "------\n 4  | 2.2  | 0.0\nRefer to the\nQuantileDiscretizer Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nQuantileDiscretizer\ndata\n=\n[(\n0\n,\n18.0\n),\n(\n1\n,\n19.0\n),\n(\n2\n,\n8.0\n),\n(\n3\n,\n5.0\n),\n(\n4\n,\n2.2\n)]\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\n[\n\"\nid\n\"\n,\n\"\nhour\n\"\n])\ndiscretizer\n=\nQuantileDiscretizer\n(\nnumBuckets\n=\n3\n,\ninputCol\n=\n\"\nhour\n\"\n,\noutputCol\n=\n\"\nresult\n\"\n)\nresult\n=\ndiscretizer\n.\nfit\n(\ndf\n).\ntransform\n(\ndf\n)\nresult\n.\nshow\n()\nFind full example code at \"examples/src/main/python/ml/quantile_discretizer_example.py\" in the Spark repo.\nRefer to the\nQuantileDiscretizer Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.QuantileDiscretizer\nval\ndata\n=\nArray\n((\n0\n,\n18.0\n),\n(\n1\n,\n19.0\n),\n(\n2\n,\n8.0\n),\n(\n3\n,\n5.0\n),\n(\n4\n,\n2.2\n))\nval\ndf\n=\nspark\n.\ncreateDat", "question": "What is the value associated with id 4 in the provided data?", "answers": {"text": ["4  | 2.2  | 0.0"], "answer_start": [8]}}
{"context": "e.spark.ml.feature.QuantileDiscretizer\nval\ndata\n=\nArray\n((\n0\n,\n18.0\n),\n(\n1\n,\n19.0\n),\n(\n2\n,\n8.0\n),\n(\n3\n,\n5.0\n),\n(\n4\n,\n2.2\n))\nval\ndf\n=\nspark\n.\ncreateDataFrame\n(\nimmutable\n.\nArraySeq\n.\nunsafeWrapArray\n(\ndata\n)).\ntoDF\n(\n\"id\"\n,\n\"hour\"\n)\nval\ndiscretizer\n=\nnew\nQuantileDiscretizer\n()\n.\nsetInputCol\n(\n\"hour\"\n)\n.\nsetOutputCol\n(\n\"result\"\n)\n.\nsetNumBuckets\n(\n3\n)\nval\nresult\n=\ndiscretizer\n.\nfit\n(\ndf\n).\ntransform\n(\ndf\n)\nresult\n.\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\" in the Spark repo.\nRefer to the\nQuantileDiscretizer Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.QuantileDiscretizer\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.", "question": "Where can I find a full example code for QuantileDiscretizer?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\" in the Spark repo."], "answer_start": [432]}}
{"context": "-----\n     1.0    | Double.NaN\n     2.0    | Double.NaN\n Double.NaN |     3.0   \n     4.0    |     4.0   \n     5.0    |     5.0\nIn this example, Imputer will replace all occurrences of\nDouble.NaN\n(the default for the missing value)\nwith the mean (the default imputation strategy) computed from the other values in the corresponding columns.\nIn this example, the surrogate values for columns\na\nand\nb\nare 3.0 and 4.0 respectively. After\ntransformation, the missing values in the output columns will be replaced by the surrogate value for\nthe relevant column.\na     |      b     | out_a | out_b   \n------------|------------|-------|-------\n     1.0    | Double.NaN |  1.0  |  4.0 \n     2.0    | Double.NaN |  2.0  |  4.0 \n Double.NaN |     3.0    |  3.0  |  3.0 \n     4.0    |     4.0    |  4.0  |  4.0\n", "question": "What will Imputer replace all occurrences of?", "answers": {"text": ["Double.NaN"], "answer_start": [20]}}
{"context": "c/main/python/ml/imputer_example.py\" in the Spark repo.\nRefer to the\nImputer Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.Imputer\nval\ndf\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n1.0\n,\nDouble\n.\nNaN\n),\n(\n2.0\n,\nDouble\n.\nNaN\n),\n(\nDouble\n.\nNaN\n,\n3.0\n),\n(\n4.0\n,\n4.0\n),\n(\n5.0\n,\n5.0\n)\n)).\ntoDF\n(\n\"a\"\n,\n\"b\"\n)\nval\nimputer\n=\nnew\nImputer\n()\n.\nsetInputCols\n(\nArray\n(\n\"a\"\n,\n\"b\"\n))\n.\nsetOutputCols\n(\nArray\n(\n\"out_a\"\n,\n\"out_b\"\n))\nval\nmodel\n=\nimputer\n.\nfit\n(\ndf\n)\nmodel\n.\ntransform\n(\ndf\n).\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\" in the Spark repo.\nRefer to the\nImputer Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.Imputer\n;\nimport\norg.apache.", "question": "Where can I find the full example code for the Imputer in Scala?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\" in the Spark repo."], "answer_start": [506]}}
{"context": "tring name simultaneously. At least one feature must be selected. Duplicate features are not\nallowed, so there can be no overlap between selected indices and names. Note that if names of\nfeatures are selected, an exception will be thrown if empty input attributes are encountered.\nThe output vector will order features with the selected indices first (in the order given),\nfollowed by the selected names (in the order given).\nExamples\nSuppose that we have a DataFrame with the column\nuserFeatures\n:\nuserFeatures\n------------------\n [0.0, 10.0, 0.5]\nuserFeatures\nis a vector column that contains three user features. Assume that the first column\nof\nuserFeatures\nare all zeros, so we want to remove it and select only the last two columns.\nThe\nVectorSlicer\nselects the last two elements with\nsetIndices", "question": "What happens if names of features are selected and empty input attributes are encountered?", "answers": {"text": ["an exception will be thrown if empty input attributes are encountered."], "answer_start": [210]}}
{"context": "erFeatures\nare all zeros, so we want to remove it and select only the last two columns.\nThe\nVectorSlicer\nselects the last two elements with\nsetIndices(1, 2)\nthen produces a new vector\ncolumn named\nfeatures\n:\nuserFeatures     | features\n------------------|-----------------------------\n [0.0, 10.0, 0.5] | [10.0, 0.5]\nSuppose also that we have potential input attributes for the\nuserFeatures\n, i.e.\n[\"f1\", \"f2\", \"f3\"]\n, then we can use\nsetNames(\"f2\", \"f3\")\nto select them.\nuserFeatures     | features\n------------------|-----------------------------\n [0.0, 10.0, 0.5] | [10.0, 0.5]\n [\"f1\", \"f2\", \"f3\"] | [\"f2\", \"f3\"]\nRefer to the\nVectorSlicer Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nVectorSlicer\nfrom\npyspark.ml.linalg\nimport\nVectors\nfrom\npyspark.sql.types\nimport\nRow\nd", "question": "What method is used to select the last two elements of a vector?", "answers": {"text": ["setIndices(1, 2)"], "answer_start": [140]}}
{"context": "sInstanceOf\n[\nArray\n[\nAttribute\n]])\nval\ndataset\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nStructType\n(\nArray\n(\nattrGroup\n.\ntoStructField\n())))\nval\nslicer\n=\nnew\nVectorSlicer\n().\nsetInputCol\n(\n\"userFeatures\"\n).\nsetOutputCol\n(\n\"features\"\n)\nslicer\n.\nsetIndices\n(\nArray\n(\n1\n)).\nsetNames\n(\nArray\n(\n\"f3\"\n))\n// or slicer.setIndices(Array(1, 2)), or slicer.setNames(Array(\"f2\", \"f3\"))\nval\noutput\n=\nslicer\n.\ntransform\n(\ndataset\n)\noutput\n.\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\" in the Spark repo.\nRefer to the\nVectorSlicer Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.attribute.Attribute\n;\nimport\norg.apache.spark.ml.attribute.AttributeGroup\n;\nimport\norg.apache.s", "question": "Where can I find a full example code for VectorSlicer?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\" in the Spark repo."], "answer_start": [437]}}
{"context": "category ('a')     | last alphabetical category ('c')\nIf the label column is of type string, it will be first transformed to double with\nStringIndexer\nusing\nfrequencyDesc\nordering.\nIf the label column does not exist in the DataFrame, the output label column will be created from the specified response variable in the formula.\nNote:\nThe ordering option\nstringOrderType\nis NOT used for the label column. When the label column is indexed, it uses the default descending frequency ordering in\nStringIndexer\n.\nExamples\nAssume that we have a DataFrame with the columns\nid\n,\ncountry\n,\nhour\n, and\nclicked\n:\nid | country | hour | clicked\n---|---------|------|---------\n 7 | \"US\"    | 18   | 1.0\n 8 | \"CA\"    | 12   | 0.0\n 9 | \"NZ\"    | 15   | 0.0\nIf we use\nRFormula\nwith a formula string of\nclicked ~ country", "question": "What happens if the label column does not exist in the DataFrame?", "answers": {"text": ["If the label column does not exist in the DataFrame, the output label column will be created from the specified response variable in the formula."], "answer_start": [181]}}
{"context": "|---------\n 7 | \"US\"    | 18   | 1.0\n 8 | \"CA\"    | 12   | 0.0\n 9 | \"NZ\"    | 15   | 0.0\nIf we use\nRFormula\nwith a formula string of\nclicked ~ country + hour\n, which indicates that we want to\npredict\nclicked\nbased on\ncountry\nand\nhour\n, after transformation we should get the following DataFrame:\nid | country | hour | clicked | features         | label\n---|---------|------|---------|------------------|-------\n 7 | \"US\"    | 18   | 1.0     | [0.0, 0.0, 18.0] | 1.0\n 8 | \"CA\"    | 12   | 0.0     | [0.0, 1.0, 12.0] | 0.0\n 9 | \"NZ\"    | 15   | 0.0     | [1.0, 0.0, 15.0] | 0.0\nRefer to the\nRFormula Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nRFormula\ndataset\n=\nspark\n.\ncreateDataFrame\n(\n[(\n7\n,\n\"\nUS\n\"\n,\n18\n,\n1.0\n),\n(\n8\n,\n\"\nCA\n\"\n,\n12\n,\n0.0\n),\n(\n9\n,\n\"\nNZ\n\"\n,\n15\n,\n0.0\n)],\n[\n", "question": "What is the formula used with RFormula to predict clicked?", "answers": {"text": ["clicked ~ country + hour"], "answer_start": [133]}}
{"context": "k.ml.feature\nimport\nRFormula\ndataset\n=\nspark\n.\ncreateDataFrame\n(\n[(\n7\n,\n\"\nUS\n\"\n,\n18\n,\n1.0\n),\n(\n8\n,\n\"\nCA\n\"\n,\n12\n,\n0.0\n),\n(\n9\n,\n\"\nNZ\n\"\n,\n15\n,\n0.0\n)],\n[\n\"\nid\n\"\n,\n\"\ncountry\n\"\n,\n\"\nhour\n\"\n,\n\"\nclicked\n\"\n])\nformula\n=\nRFormula\n(\nformula\n=\n\"\nclicked ~ country + hour\n\"\n,\nfeaturesCol\n=\n\"\nfeatures\n\"\n,\nlabelCol\n=\n\"\nlabel\n\"\n)\noutput\n=\nformula\n.\nfit\n(\ndataset\n).\ntransform\n(\ndataset\n)\noutput\n.\nselect\n(\n\"\nfeatures\n\"\n,\n\"\nlabel\n\"\n).\nshow\n()\nFind full example code at \"examples/src/main/python/ml/rformula_example.py\" in the Spark repo.\nRefer to the\nRFormula Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.RFormula\nval\ndataset\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n7\n,\n\"US\"\n,\n18\n,\n1.0\n),\n(\n8\n,\n\"CA\"\n,\n12\n,\n0.0\n),\n(\n9\n,\n\"NZ\"\n,\n15\n,\n0.0\n)\n)).\ntoDF\n(\n\"id\"\n,\n\"country\"\n,\n\"hour\"\n,\n\"clicked\"\n)\nval", "question": "What is the formula used in the RFormula transformation?", "answers": {"text": ["clicked ~ country + hour"], "answer_start": [232]}}
{"context": "eateDataFrame\n(\nSeq\n(\n(\n7\n,\n\"US\"\n,\n18\n,\n1.0\n),\n(\n8\n,\n\"CA\"\n,\n12\n,\n0.0\n),\n(\n9\n,\n\"NZ\"\n,\n15\n,\n0.0\n)\n)).\ntoDF\n(\n\"id\"\n,\n\"country\"\n,\n\"hour\"\n,\n\"clicked\"\n)\nval\nformula\n=\nnew\nRFormula\n()\n.\nsetFormula\n(\n\"clicked ~ country + hour\"\n)\n.\nsetFeaturesCol\n(\n\"features\"\n)\n.\nsetLabelCol\n(\n\"label\"\n)\nval\noutput\n=\nformula\n.\nfit\n(\ndataset\n).\ntransform\n(\ndataset\n)\noutput\n.\nselect\n(\n\"features\"\n,\n\"label\"\n).\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\" in the Spark repo.\nRefer to the\nRFormula Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.RFormula\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache", "question": "What is set as the formula for the RFormula object?", "answers": {"text": ["clicked ~ country + hour"], "answer_start": [193]}}
{"context": ".feature.RFormula\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nimport\nstatic\norg\n.\napache\n.\nspark\n.\nsql\n.\ntypes\n.\nDataTypes\n.*;\nStructType\nschema\n=\ncreateStructType\n(\nnew\nStructField\n[]{\ncreateStructField\n(\n\"id\"\n,\nIntegerType\n,\nfalse\n),\ncreateStructField\n(\n\"country\"\n,\nStringType\n,\nfalse\n),\ncreateStructField\n(\n\"hour\"\n,\nIntegerType\n,\nfalse\n),\ncreateStructField\n(\n\"clicked\"\n,\nDoubleType\n,\nfalse\n)\n});\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n7\n,\n\"US\"\n,\n18\n,\n1.0\n),\nRowFactory\n.\ncreate\n(\n8\n,\n\"CA\"\n,\n12\n,\n0.0\n),\nRowFactory\n.\ncreate\n(\n9\n,\n\"NZ\"\n,\n15\n,\n0.0\n)\n);\nDataset\n<\nRow\n>\ndataset\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\ns", "question": "What is used to create a StructField?", "answers": {"text": ["createStructField"], "answer_start": [352]}}
{"context": "false discovery rate is below a threshold.\nfwe\nchooses all features whose p-values are below a threshold. The threshold is scaled by 1/numFeatures, thus controlling the family-wise error rate of selection.\nBy default, the selection method is\nnumTopFeatures\n, with the default number of top features set to 50.\nThe user can choose a selection method using\nsetSelectorType\n.\nExamples\nAssume that we have a DataFrame with the columns\nid\n,\nfeatures\n, and\nclicked\n, which is used as\nour target to be predicted:\nid | features              | clicked\n---|-----------------------|---------\n 7 | [0.0, 0.0, 18.0, 1.0] | 1.0\n 8 | [0.0, 1.0, 12.0, 0.0] | 0.0\n 9 | [1.0, 0.0, 15.0, 0.1] | 0.0\nIf we use\nChiSqSelector\nwith\nnumTopFeatures = 1\n, then according to our label\nclicked\nthe\nlast column in our\nfeatures\nis", "question": "What is the default number of top features when using the numTopFeatures selection method?", "answers": {"text": ["50"], "answer_start": [306]}}
{"context": "he API.\nimport\norg.apache.spark.ml.feature.ChiSqSelector\nimport\norg.apache.spark.ml.linalg.Vectors\nval\ndata\n=\nSeq\n(\n(\n7\n,\nVectors\n.\ndense\n(\n0.0\n,\n0.0\n,\n18.0\n,\n1.0\n),\n1.0\n),\n(\n8\n,\nVectors\n.\ndense\n(\n0.0\n,\n1.0\n,\n12.0\n,\n0.0\n),\n0.0\n),\n(\n9\n,\nVectors\n.\ndense\n(\n1.0\n,\n0.0\n,\n15.0\n,\n0.1\n),\n0.0\n)\n)\nval\ndf\n=\nspark\n.\ncreateDataset\n(\ndata\n).\ntoDF\n(\n\"id\"\n,\n\"features\"\n,\n\"clicked\"\n)\nval\nselector\n=\nnew\nChiSqSelector\n()\n.\nsetNumTopFeatures\n(\n1\n)\n.\nsetFeaturesCol\n(\n\"features\"\n)\n.\nsetLabelCol\n(\n\"clicked\"\n)\n.\nsetOutputCol\n(\n\"selectedFeatures\"\n)\nval\nresult\n=\nselector\n.\nfit\n(\ndf\n).\ntransform\n(\ndf\n)\nprintln\n(\ns\n\"ChiSqSelector output with top ${selector.getNumTopFeatures} features selected\"\n)\nresult\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\" ", "question": "Where can I find the full example code for ChiSqSelector?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\""], "answer_start": [692]}}
{"context": "es} features selected\"\n)\nresult\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\" in the Spark repo.\nRefer to the\nChiSqSelector Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.ChiSqSelector\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n7\n,\nVectors\n.\ndense\n(\n0.0\n,\n0.0\n,\n18.0\n,\n1.0\n),\n1.0\n)", "question": "Where can I find a full example code for ChiSqSelector?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\" in the Spark repo."], "answer_start": [42]}}
{"context": "taFrame\n(\ndata\n,\nschema\n);\nChiSqSelector\nselector\n=\nnew\nChiSqSelector\n()\n.\nsetNumTopFeatures\n(\n1\n)\n.\nsetFeaturesCol\n(\n\"features\"\n)\n.\nsetLabelCol\n(\n\"clicked\"\n)\n.\nsetOutputCol\n(\n\"selectedFeatures\"\n);\nDataset\n<\nRow\n>\nresult\n=\nselector\n.\nfit\n(\ndf\n).\ntransform\n(\ndf\n);\nSystem\n.\nout\n.\nprintln\n(\n\"ChiSqSelector output with top \"\n+\nselector\n.\ngetNumTopFeatures\n()\n+\n\" features selected\"\n);\nresult\n.\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\" in the Spark repo.\nUnivariateFeatureSelector\nUnivariateFeatureSelector\noperates on categorical/continuous labels with categorical/continuous features. \nUser can set\nfeatureType\nand\nlabelType\n, and Spark will pick the score function to use based on the specified\nfeatureType\nand\nlabelType\n.\n", "question": "Where can I find the full example code for JavaChiSqSelectorExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\" in the Spark repo."], "answer_start": [400]}}
{"context": " features. \nUser can set\nfeatureType\nand\nlabelType\n, and Spark will pick the score function to use based on the specified\nfeatureType\nand\nlabelType\n.\nfeatureType |  labelType |score function\n------------|------------|--------------\ncategorical |categorical | chi-squared (chi2)\ncontinuous  |categorical | ANOVATest (f_classif)\ncontinuous  |continuous  | F-value (f_regression)\nIt supports five selection modes:\nnumTopFeatures\n,\npercentile\n,\nfpr\n,\nfdr\n,\nfwe\n:\nnumTopFeatures\nchooses a fixed number of top features.\npercentile\nis similar to\nnumTopFeatures\nbut chooses a fraction of all features instead of a fixed number.\nfpr\nchooses all features whose p-values are below a threshold, thus controlling the false positive rate of selection.\nfdr\nuses the\nBenjamini-Hochberg procedure\nto choose all featur", "question": "What score function is used when both featureType and labelType are categorical?", "answers": {"text": ["chi-squared (chi2)"], "answer_start": [259]}}
{"context": " p-values are below a threshold, thus controlling the false positive rate of selection.\nfdr\nuses the\nBenjamini-Hochberg procedure\nto choose all features whose false discovery rate is below a threshold.\nfwe\nchooses all features whose p-values are below a threshold. The threshold is scaled by 1/numFeatures, thus controlling the family-wise error rate of selection.\nBy default, the selection mode is\nnumTopFeatures\n, with the default selectionThreshold sets to 50.\nExamples\nAssume that we have a DataFrame with the columns\nid\n,\nfeatures\n, and\nlabel\n, which is used as\nour target to be predicted:\nid | features                       | label\n---|--------------------------------|---------\n 1 | [1.7, 4.4, 7.6, 5.8, 9.6, 2.3] | 3.0\n 2 | [8.8, 7.3, 5.7, 7.3, 2.2, 4.1] | 2.0\n 3 | [1.2, 9.5, 2.5, 3.1, 8.7,", "question": "What does fwe choose?", "answers": {"text": ["chooses all features whose p-values are below a threshold."], "answer_start": [206]}}
{"context": "-------------------------|---------\n 1 | [1.7, 4.4, 7.6, 5.8, 9.6, 2.3] | 3.0\n 2 | [8.8, 7.3, 5.7, 7.3, 2.2, 4.1] | 2.0\n 3 | [1.2, 9.5, 2.5, 3.1, 8.7, 2.5] | 3.0\n 4 | [3.7, 9.2, 6.1, 4.1, 7.5, 3.8] | 2.0\n 5 | [8.9, 5.2, 7.8, 8.3, 5.2, 3.0] | 4.0\n 6 | [7.9, 8.5, 9.2, 4.0, 9.4, 2.1] | 4.0\nIf we set\nfeatureType\nto\ncontinuous\nand\nlabelType\nto\ncategorical\nwith\nnumTopFeatures = 1\n, the\nlast column in our\nfeatures\nis chosen as the most useful feature:\nid | features                       | label   | selectedFeatures\n---|--------------------------------|---------|------------------\n 1 | [1.7, 4.4, 7.6, 5.8, 9.6, 2.3] | 3.0     | [2.3]\n 2 | [8.8, 7.3, 5.7, 7.3, 2.2, 4.1] | 2.0     | [4.1]\n 3 | [1.2, 9.5, 2.5, 3.1, 8.7, 2.5] | 3.0     | [2.5]\n 4 | [3.7, 9.2, 6.1, 4.1, 7.5, 3.8] | 2.0     | [3.8]\n 5 |", "question": "What is the selected feature for the data point with ID 1?", "answers": {"text": ["[2.3]"], "answer_start": [628]}}
{"context": "5.7, 7.3, 2.2, 4.1] | 2.0     | [4.1]\n 3 | [1.2, 9.5, 2.5, 3.1, 8.7, 2.5] | 3.0     | [2.5]\n 4 | [3.7, 9.2, 6.1, 4.1, 7.5, 3.8] | 2.0     | [3.8]\n 5 | [8.9, 5.2, 7.8, 8.3, 5.2, 3.0] | 4.0     | [3.0]\n 6 | [7.9, 8.5, 9.2, 4.0, 9.4, 2.1] | 4.0     | [2.1]\nRefer to the\nUnivariateFeatureSelector Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nUnivariateFeatureSelector\nfrom\npyspark.ml.linalg\nimport\nVectors\ndf\n=\nspark\n.\ncreateDataFrame\n([\n(\n1\n,\nVectors\n.\ndense\n([\n1.7\n,\n4.4\n,\n7.6\n,\n5.8\n,\n9.6\n,\n2.3\n]),\n3.0\n,),\n(\n2\n,\nVectors\n.\ndense\n([\n8.8\n,\n7.3\n,\n5.7\n,\n7.3\n,\n2.2\n,\n4.1\n]),\n2.0\n,),\n(\n3\n,\nVectors\n.\ndense\n([\n1.2\n,\n9.5\n,\n2.5\n,\n3.1\n,\n8.7\n,\n2.5\n]),\n3.0\n,),\n(\n4\n,\nVectors\n.\ndense\n([\n3.7\n,\n9.2\n,\n6.1\n,\n4.1\n,\n7.5\n,\n3.8\n]),\n2.0\n,),\n(\n5\n,\nVectors\n.\ndense\n([\n8.9\n,\n5.2\n,\n7.8\n,\n8.3\n,\n5.2\n,\n", "question": "What is the vector associated with the data point where the first element is 8.9?", "answers": {"text": ["[8.9, 5.2, 7.8, 8.3, 5.2, 3.0]"], "answer_start": [151]}}
{"context": "d features selected using f_classif\n\"\n%\nselector\n.\ngetSelectionThreshold\n())\nresult\n.\nshow\n()\nFind full example code at \"examples/src/main/python/ml/univariate_feature_selector_example.py\" in the Spark repo.\nRefer to the\nUnivariateFeatureSelector Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.UnivariateFeatureSelector\nimport\norg.apache.spark.ml.linalg.Vectors\nval\ndata\n=\nSeq\n(\n(\n1\n,\nVectors\n.\ndense\n(\n1.7\n,\n4.4\n,\n7.6\n,\n5.8\n,\n9.6\n,\n2.3\n),\n3.0\n),\n(\n2\n,\nVectors\n.\ndense\n(\n8.8\n,\n7.3\n,\n5.7\n,\n7.3\n,\n2.2\n,\n4.1\n),\n2.0\n),\n(\n3\n,\nVectors\n.\ndense\n(\n1.2\n,\n9.5\n,\n2.5\n,\n3.1\n,\n8.7\n,\n2.5\n),\n3.0\n),\n(\n4\n,\nVectors\n.\ndense\n(\n3.7\n,\n9.2\n,\n6.1\n,\n4.1\n,\n7.5\n,\n3.8\n),\n2.0\n),\n(\n5\n,\nVectors\n.\ndense\n(\n8.9\n,\n5.2\n,\n7.8\n,\n8.3\n,\n5.2\n,\n3.0\n),\n4.0\n),\n(\n6\n,\nVectors\n.\ndense\n(\n7.9\n,\n8.5\n,\n9.2\n,\n4.0\n,\n9.4\n,", "question": "Where can I find a full example code for the univariate feature selector?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/univariate_feature_selector_example.py\" in the Spark repo."], "answer_start": [94]}}
{"context": "ectionThreshold}\"\n+\ns\n\" features selected using f_classif\"\n)\nresult\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/UnivariateFeatureSelectorExample.scala\" in the Spark repo.\nRefer to the\nUnivariateFeatureSelector Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.UnivariateFeatureSelector\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.*\n;\nList\n<\nRow\n>\ndata\n=\nArrays\n.\nasList\n(\nRowFactory\n.\ncreate\n(\n1\n,\nVectors\n.\ndense\n(\n1.7\n,\n4.4\n,\n7.6\n,\n5.8\n,\n9.6\n,\n2.3\n),\n3.0\n),\nRowFactory\n.\ncreate\n(\n2\n,\nVectors\n.\ndense\n(\n8.8\n,\n7.3\n,\n5.7\n,\n7", "question": "Where can I find a full example code for UnivariateFeatureSelector?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/UnivariateFeatureSelectorExample.scala\" in the Spark repo."], "answer_start": [78]}}
{"context": "---------------------------|-------------------\n 1 | [6.0, 7.0, 0.0, 7.0, 6.0, 0.0] | [6.0,0.0,7.0,0.0]\n 2 | [0.0, 9.0, 6.0, 0.0, 5.0, 9.0] | [0.0,6.0,0.0,9.0]\n 3 | [0.0, 9.0, 3.0, 0.0, 5.0, 5.0] | [0.0,3.0,0.0,5.0]\n 4 | [0.0, 9.0, 8.0, 5.0, 6.0, 4.0] | [0.0,8.0,5.0,4.0]\n 5 | [8.0, 9.0, 6.0, 5.0, 4.0, 4.0] | [8.0,6.0,5.0,4.0]\n 6 | [8.0, 9.0, 6.0, 0.0, 0.0, 0.0] | [8.0,6.0,0.0,0.0]\nRefer to the\nVarianceThresholdSelector Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nVarianceThresholdSelector\nfrom\npyspark.ml.linalg\nimport\nVectors\ndf\n=\nspark\n.\ncreateDataFrame\n([\n(\n1\n,\nVectors\n.\ndense\n([\n6.0\n,\n7.0\n,\n0.0\n,\n7.0\n,\n6.0\n,\n0.0\n])),\n(\n2\n,\nVectors\n.\ndense\n([\n0.0\n,\n9.0\n,\n6.0\n,\n0.0\n,\n5.0\n,\n9.0\n])),\n(\n3\n,\nVectors\n.\ndense\n([\n0.0\n,\n9.0\n,\n3.0\n,\n0.0\n,\n5.0\n,\n5.0\n])),\n(\n4\n,\nVectors\n.\nd", "question": "What is the vector associated with the first data point (index 1) in the DataFrame?", "answers": {"text": ["[6.0, 7.0, 0.0, 7.0, 6.0, 0.0]"], "answer_start": [53]}}
{"context": "at \"examples/src/main/python/ml/variance_threshold_selector_example.py\" in the Spark repo.\nRefer to the\nVarianceThresholdSelector Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.VarianceThresholdSelector\nimport\norg.apache.spark.ml.linalg.Vectors\nval\ndata\n=\nSeq\n(\n(\n1\n,\nVectors\n.\ndense\n(\n6.0\n,\n7.0\n,\n0.0\n,\n7.0\n,\n6.0\n,\n0.0\n)),\n(\n2\n,\nVectors\n.\ndense\n(\n0.0\n,\n9.0\n,\n6.0\n,\n0.0\n,\n5.0\n,\n9.0\n)),\n(\n3\n,\nVectors\n.\ndense\n(\n0.0\n,\n9.0\n,\n3.0\n,\n0.0\n,\n5.0\n,\n5.0\n)),\n(\n4\n,\nVectors\n.\ndense\n(\n0.0\n,\n9.0\n,\n8.0\n,\n5.0\n,\n6.0\n,\n4.0\n)),\n(\n5\n,\nVectors\n.\ndense\n(\n8.0\n,\n9.0\n,\n6.0\n,\n5.0\n,\n4.0\n,\n4.0\n)),\n(\n6\n,\nVectors\n.\ndense\n(\n8.0\n,\n9.0\n,\n6.0\n,\n0.0\n,\n0.0\n,\n0.0\n))\n)\nval\ndf\n=\nspark\n.\ncreateDataset\n(\ndata\n).\ntoDF\n(\n\"id\"\n,\n\"features\"\n)\nval\nselector\n=\nnew\nVarianceThresholdSelector\n()\n.\nsetVarianceThreshol", "question": "What is being imported from org.apache.spark.ml.feature?", "answers": {"text": ["VarianceThresholdSelector"], "answer_start": [104]}}
{"context": "0\n,\n0.0\n))\n)\nval\ndf\n=\nspark\n.\ncreateDataset\n(\ndata\n).\ntoDF\n(\n\"id\"\n,\n\"features\"\n)\nval\nselector\n=\nnew\nVarianceThresholdSelector\n()\n.\nsetVarianceThreshold\n(\n8.0\n)\n.\nsetFeaturesCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"selectedFeatures\"\n)\nval\nresult\n=\nselector\n.\nfit\n(\ndf\n).\ntransform\n(\ndf\n)\nprintln\n(\ns\n\"Output: Features with variance lower than\"\n+\ns\n\" ${selector.getVarianceThreshold} are removed.\"\n)\nresult\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/VarianceThresholdSelectorExample.scala\" in the Spark repo.\nRefer to the\nVarianceThresholdSelector Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.VarianceThresholdSelector\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.ap", "question": "What is the variance threshold set for the VarianceThresholdSelector?", "answers": {"text": ["8.0"], "answer_start": [154]}}
{"context": ",\n4.0\n)),\nRowFactory\n.\ncreate\n(\n5\n,\nVectors\n.\ndense\n(\n8.0\n,\n9.0\n,\n6.0\n,\n5.0\n,\n4.0\n,\n4.0\n)),\nRowFactory\n.\ncreate\n(\n6\n,\nVectors\n.\ndense\n(\n8.0\n,\n9.0\n,\n6.0\n,\n0.0\n,\n0.0\n,\n0.0\n))\n);\nStructType\nschema\n=\nnew\nStructType\n(\nnew\nStructField\n[]{\nnew\nStructField\n(\n\"id\"\n,\nDataTypes\n.\nIntegerType\n,\nfalse\n,\nMetadata\n.\nempty\n()),\nnew\nStructField\n(\n\"features\"\n,\nnew\nVectorUDT\n(),\nfalse\n,\nMetadata\n.\nempty\n())\n});\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\ncreateDataFrame\n(\ndata\n,\nschema\n);\nVarianceThresholdSelector\nselector\n=\nnew\nVarianceThresholdSelector\n()\n.\nsetVarianceThreshold\n(\n8.0\n)\n.\nsetFeaturesCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"selectedFeatures\"\n);\nDataset\n<\nRow\n>\nresult\n=\nselector\n.\nfit\n(\ndf\n).\ntransform\n(\ndf\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Output: Features with variance lower than \"\n+\nselector\n.\ngetVarianceThreshol", "question": "What is the variance threshold set for the VarianceThresholdSelector?", "answers": {"text": ["8.0"], "answer_start": [54]}}
{"context": "sult\n=\nselector\n.\nfit\n(\ndf\n).\ntransform\n(\ndf\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Output: Features with variance lower than \"\n+\nselector\n.\ngetVarianceThreshold\n()\n+\n\" are removed.\"\n);\nresult\n.\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaVarianceThresholdSelectorExample.java\" in the Spark repo.\nLocality Sensitive Hashing\nLocality Sensitive Hashing (LSH)\nis an important class of hashing techniques, which is commonly used in clustering, approximate nearest neighbor search and outlier detection with large datasets.\nThe general idea of LSH is to use a family of functions (“LSH families”) to hash data points into buckets, so that the data points which are close to each other are in the same buckets with high probability, while data points that are far away", "question": "Onde posso encontrar o código de exemplo completo para o VarianceThresholdSelector?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaVarianceThresholdSelectorExample.java\" in the Spark repo."], "answer_start": [194]}}
{"context": ". In future releases, we will implement AND-amplification so that users can specify the dimensions of these vectors.\nApproximate Similarity Join\nApproximate similarity join takes two datasets and approximately returns pairs of rows in the datasets whose distance is smaller than a user-defined threshold. Approximate similarity join supports both joining two different datasets and self-joining. Self-joining will produce some duplicate pairs.\nApproximate similarity join accepts both transformed and untransformed datasets as input. If an untransformed dataset is used, it will be transformed automatically. In this case, the hash signature will be created as\noutputCol\n.\nIn the joined dataset, the origin datasets can be queried in\ndatasetA\nand\ndatasetB\n. A distance column will be added to the out", "question": "What happens if an untransformed dataset is used as input for approximate similarity join?", "answers": {"text": ["If an untransformed dataset is used, it will be transformed automatically."], "answer_start": [534]}}
{"context": "created as\noutputCol\n.\nIn the joined dataset, the origin datasets can be queried in\ndatasetA\nand\ndatasetB\n. A distance column will be added to the output dataset to show the true distance between each pair of rows returned.\nApproximate Nearest Neighbor Search\nApproximate nearest neighbor search takes a dataset (of feature vectors) and a key (a single feature vector), and it approximately returns a specified number of rows in the dataset that are closest to the vector.\nApproximate nearest neighbor search accepts both transformed and untransformed datasets as input. If an untransformed dataset is used, it will be transformed automatically. In this case, the hash signature will be created as\noutputCol\n.\nA distance column will be added to the output dataset to show the true distance between ea", "question": "What happens if an untransformed dataset is used as input for approximate nearest neighbor search?", "answers": {"text": ["If an untransformed dataset is used, it will be transformed automatically."], "answer_start": [571]}}
{"context": "d portions the projected results into hash buckets:\n\\[\nh(\\mathbf{x}) = \\Big\\lfloor \\frac{\\mathbf{x} \\cdot \\mathbf{v}}{r} \\Big\\rfloor\n\\]\nwhere\nr\nis a user-defined bucket length. The bucket length can be used to control the average size of hash buckets (and thus the number of buckets). A larger bucket length (i.e., fewer buckets) increases the probability of features being hashed to the same bucket (increasing the numbers of true and false positives).\nBucketed Random Projection accepts arbitrary vectors as input features, and supports both sparse and dense vectors.\nRefer to the\nBucketedRandomProjectionLSH Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nBucketedRandomProjectionLSH\nfrom\npyspark.ml.linalg\nimport\nVectors\nfrom\npyspark.sql.functions\nimport\ncol\ndataA\n=\n[(\n0\n", "question": "How is the number of hash buckets controlled?", "answers": {"text": ["The bucket length can be used to control the average size of hash buckets (and thus the number of buckets)."], "answer_start": [177]}}
{"context": "\n([\n1.0\n,\n0.0\n])\nbrp\n=\nBucketedRandomProjectionLSH\n(\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nhashes\n\"\n,\nbucketLength\n=\n2.0\n,\nnumHashTables\n=\n3\n)\nmodel\n=\nbrp\n.\nfit\n(\ndfA\n)\n# Feature Transformation\nprint\n(\n\"\nThe hashed dataset where hashed values are stored in the column\n'\nhashes\n'\n:\n\"\n)\nmodel\n.\ntransform\n(\ndfA\n).\nshow\n()\n# Compute the locality sensitive hashes for the input rows, then perform approximate\n# similarity join.\n# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n# `model.approxSimilarityJoin(transformedA, transformedB, 1.5)`\nprint\n(\n\"\nApproximately joining dfA and dfB on Euclidean distance smaller than 1.5:\n\"\n)\nmodel\n.\napproxSimilarityJoin\n(\ndfA\n,\ndfB\n,\n1.5\n,\ndistCol\n=\n\"\nEuclideanDistance\n\"\n)\n\\\n.\nselect\n(\ncol\n(\n\"\ndatasetA.id\n\"\n).\nalias\n(\n\"\nidA\n\"", "question": "What is the Euclidean distance threshold used in the approximate similarity join?", "answers": {"text": ["1.5"], "answer_start": [570]}}
{"context": "than 1.5:\n\"\n)\nmodel\n.\napproxSimilarityJoin\n(\ndfA\n,\ndfB\n,\n1.5\n,\ndistCol\n=\n\"\nEuclideanDistance\n\"\n)\n\\\n.\nselect\n(\ncol\n(\n\"\ndatasetA.id\n\"\n).\nalias\n(\n\"\nidA\n\"\n),\ncol\n(\n\"\ndatasetB.id\n\"\n).\nalias\n(\n\"\nidB\n\"\n),\ncol\n(\n\"\nEuclideanDistance\n\"\n)).\nshow\n()\n# Compute the locality sensitive hashes for the input rows, then perform approximate nearest\n# neighbor search.\n# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n# `model.approxNearestNeighbors(transformedA, key, 2)`\nprint\n(\n\"\nApproximately searching dfA for 2 nearest neighbors of the key:\n\"\n)\nmodel\n.\napproxNearestNeighbors\n(\ndfA\n,\nkey\n,\n2\n).\nshow\n()\nFind full example code at \"examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\" in the Spark repo.\nRefer to the\nBucketedRandomProjectionLSH Scala docs\nfor ", "question": "Where can I find full example code for this functionality?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\" in the Spark repo."], "answer_start": [627]}}
{"context": "\ndense\n(\n1.0\n,\n0.0\n)),\n(\n5\n,\nVectors\n.\ndense\n(-\n1.0\n,\n0.0\n)),\n(\n6\n,\nVectors\n.\ndense\n(\n0.0\n,\n1.0\n)),\n(\n7\n,\nVectors\n.\ndense\n(\n0.0\n,\n-\n1.0\n))\n)).\ntoDF\n(\n\"id\"\n,\n\"features\"\n)\nval\nkey\n=\nVectors\n.\ndense\n(\n1.0\n,\n0.0\n)\nval\nbrp\n=\nnew\nBucketedRandomProjectionLSH\n()\n.\nsetBucketLength\n(\n2.0\n)\n.\nsetNumHashTables\n(\n3\n)\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"hashes\"\n)\nval\nmodel\n=\nbrp\n.\nfit\n(\ndfA\n)\n// Feature Transformation\nprintln\n(\n\"The hashed dataset where hashed values are stored in the column 'hashes':\"\n)\nmodel\n.\ntransform\n(\ndfA\n).\nshow\n()\n// Compute the locality sensitive hashes for the input rows, then perform approximate\n// similarity join.\n// We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n// `model.approxSimilarityJoin(transformedA, transformedB, 1.5)`\n", "question": "What is set as the input column for the BucketedRandomProjectionLSH?", "answers": {"text": ["\"features\""], "answer_start": [157]}}
{"context": " We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n// `model.approxSimilarityJoin(transformedA, transformedB, 1.5)`\nprintln\n(\n\"Approximately joining dfA and dfB on Euclidean distance smaller than 1.5:\"\n)\nmodel\n.\napproxSimilarityJoin\n(\ndfA\n,\ndfB\n,\n1.5\n,\n\"EuclideanDistance\"\n)\n.\nselect\n(\ncol\n(\n\"datasetA.id\"\n).\nalias\n(\n\"idA\"\n),\ncol\n(\n\"datasetB.id\"\n).\nalias\n(\n\"idB\"\n),\ncol\n(\n\"EuclideanDistance\"\n)).\nshow\n()\n// Compute the locality sensitive hashes for the input rows, then perform approximate nearest\n// neighbor search.\n// We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n// `model.approxNearestNeighbors(transformedA, key, 2)`\nprintln\n(\n\"Approximately searching dfA for 2 nearest neighbors of the key:\"\n)\nmodel\n.\napproxNearestNeighb", "question": "What is an example of how to avoid computing hashes?", "answers": {"text": ["We could avoid computing hashes by passing in the already-transformed dataset, e.g."], "answer_start": [1]}}
{"context": "pproxNearestNeighbors(transformedA, key, 2)`\nprintln\n(\n\"Approximately searching dfA for 2 nearest neighbors of the key:\"\n)\nmodel\n.\napproxNearestNeighbors\n(\ndfA\n,\nkey\n,\n2\n).\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\" in the Spark repo.\nRefer to the\nBucketedRandomProjectionLSH Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.BucketedRandomProjectionLSH\n;\nimport\norg.apache.spark.ml.feature.BucketedRandomProjectionLSHModel\n;\nimport\norg.apache.spark.ml.linalg.Vector\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\no", "question": "Where can I find a full example code for BucketedRandomProjectionLSH?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\" in the Spark repo."], "answer_start": [181]}}
{"context": "transformed dataset, e.g.\n// `model.approxSimilarityJoin(transformedA, transformedB, 1.5)`\nSystem\n.\nout\n.\nprintln\n(\n\"Approximately joining dfA and dfB on distance smaller than 1.5:\"\n);\nmodel\n.\napproxSimilarityJoin\n(\ndfA\n,\ndfB\n,\n1.5\n,\n\"EuclideanDistance\"\n)\n.\nselect\n(\ncol\n(\n\"datasetA.id\"\n).\nalias\n(\n\"idA\"\n),\ncol\n(\n\"datasetB.id\"\n).\nalias\n(\n\"idB\"\n),\ncol\n(\n\"EuclideanDistance\"\n)).\nshow\n();\n// Compute the locality sensitive hashes for the input rows, then perform approximate nearest\n// neighbor search.\n// We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n// `model.approxNearestNeighbors(transformedA, key, 2)`\nSystem\n.\nout\n.\nprintln\n(\n\"Approximately searching dfA for 2 nearest neighbors of the key:\"\n);\nmodel\n.\napproxNearestNeighbors\n(\ndfA\n,\nkey\n,\n2\n).\nshow\n();\nFind", "question": "What is an example of how to perform an approximate similarity join?", "answers": {"text": ["`model.approxSimilarityJoin(transformedA, transformedB, 1.5)`"], "answer_start": [29]}}
{"context": "\nto each element in the set and take the minimum of all hashed values:\n\\[\nh(\\mathbf{A}) = \\min_{a \\in \\mathbf{A}}(g(a))\n\\]\nThe input sets for MinHash are represented as binary vectors, where the vector indices represent the elements themselves and the non-zero values in the vector represent the presence of that element in the set. While both dense and sparse vectors are supported, typically sparse vectors are recommended for efficiency. For example,\nVectors.sparse(10, Array[(2, 1.0), (3, 1.0), (5, 1.0)])\nmeans there are 10 elements in the space. This set contains elem 2, elem 3 and elem 5. All non-zero values are treated as binary “1” values.\nNote:\nEmpty sets cannot be transformed by MinHash, which means any input vector must have at least 1 non-zero entry.\nRefer to the\nMinHashLSH Python d", "question": "Como os conjuntos de entrada para MinHash são representados?", "answers": {"text": ["The input sets for MinHash are represented as binary vectors, where the vector indices represent the elements themselves and the non-zero values in the vector represent the presence of that element in the set."], "answer_start": [123]}}
{"context": "\nNote:\nEmpty sets cannot be transformed by MinHash, which means any input vector must have at least 1 non-zero entry.\nRefer to the\nMinHashLSH Python docs\nfor more details on the API.\nfrom\npyspark.ml.feature\nimport\nMinHashLSH\nfrom\npyspark.ml.linalg\nimport\nVectors\nfrom\npyspark.sql.functions\nimport\ncol\ndataA\n=\n[(\n0\n,\nVectors\n.\nsparse\n(\n6\n,\n[\n0\n,\n1\n,\n2\n],\n[\n1.0\n,\n1.0\n,\n1.0\n]),),\n(\n1\n,\nVectors\n.\nsparse\n(\n6\n,\n[\n2\n,\n3\n,\n4\n],\n[\n1.0\n,\n1.0\n,\n1.0\n]),),\n(\n2\n,\nVectors\n.\nsparse\n(\n6\n,\n[\n0\n,\n2\n,\n4\n],\n[\n1.0\n,\n1.0\n,\n1.0\n]),)]\ndfA\n=\nspark\n.\ncreateDataFrame\n(\ndataA\n,\n[\n\"\nid\n\"\n,\n\"\nfeatures\n\"\n])\ndataB\n=\n[(\n3\n,\nVectors\n.\nsparse\n(\n6\n,\n[\n1\n,\n3\n,\n5\n],\n[\n1.0\n,\n1.0\n,\n1.0\n]),),\n(\n4\n,\nVectors\n.\nsparse\n(\n6\n,\n[\n2\n,\n3\n,\n5\n],\n[\n1.0\n,\n1.0\n,\n1.0\n]),),\n(\n5\n,\nVectors\n.\nsparse\n(\n6\n,\n[\n1\n,\n2\n,\n4\n],\n[\n1.0\n,\n1.0\n,\n1.0\n]),)]\ndfB\n=\n", "question": "What is a requirement for input vectors when using MinHash?", "answers": {"text": ["Empty sets cannot be transformed by MinHash, which means any input vector must have at least 1 non-zero entry."], "answer_start": [7]}}
{"context": "d computing hashes by passing in the already-transformed dataset, e.g.\n# `model.approxSimilarityJoin(transformedA, transformedB, 0.6)`\nprint\n(\n\"\nApproximately joining dfA and dfB on distance smaller than 0.6:\n\"\n)\nmodel\n.\napproxSimilarityJoin\n(\ndfA\n,\ndfB\n,\n0.6\n,\ndistCol\n=\n\"\nJaccardDistance\n\"\n)\n\\\n.\nselect\n(\ncol\n(\n\"\ndatasetA.id\n\"\n).\nalias\n(\n\"\nidA\n\"\n),\ncol\n(\n\"\ndatasetB.id\n\"\n).\nalias\n(\n\"\nidB\n\"\n),\ncol\n(\n\"\nJaccardDistance\n\"\n)).\nshow\n()\n# Compute the locality sensitive hashes for the input rows, then perform approximate nearest\n# neighbor search.\n# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n# `model.approxNearestNeighbors(transformedA, key, 2)`\n# It may return less than 2 rows when not enough approximate near-neighbor candidates are\n# found.\nprint\n(\n\"\nAppro", "question": "What is an example of how to perform an approximate similarity join using the model?", "answers": {"text": ["`model.approxSimilarityJoin(transformedA, transformedB, 0.6)`"], "answer_start": [73]}}
{"context": "estNeighbors(transformedA, key, 2)`\n# It may return less than 2 rows when not enough approximate near-neighbor candidates are\n# found.\nprint\n(\n\"\nApproximately searching dfA for 2 nearest neighbors of the key:\n\"\n)\nmodel\n.\napproxNearestNeighbors\n(\ndfA\n,\nkey\n,\n2\n).\nshow\n()\nFind full example code at \"examples/src/main/python/ml/min_hash_lsh_example.py\" in the Spark repo.\nRefer to the\nMinHashLSH Scala docs\nfor more details on the API.\nimport\norg.apache.spark.ml.feature.MinHashLSH\nimport\norg.apache.spark.ml.linalg.Vectors\nimport\norg.apache.spark.sql.SparkSession\nimport\norg.apache.spark.sql.functions.col\nval\ndfA\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n0\n,\nVectors\n.\nsparse\n(\n6\n,\nSeq\n((\n0\n,\n1.0\n),\n(\n1\n,\n1.0\n),\n(\n2\n,\n1.0\n)))),\n(\n1\n,\nVectors\n.\nsparse\n(\n6\n,\nSeq\n((\n2\n,\n1.0\n),\n(\n3\n,\n1.0\n),\n(\n4\n,\n1.0\n)))),\n(", "question": "Where can I find a full example code for MinHashLSH?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/min_hash_lsh_example.py\" in the Spark repo."], "answer_start": [271]}}
{"context": "etNumHashTables\n(\n5\n)\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"hashes\"\n)\nval\nmodel\n=\nmh\n.\nfit\n(\ndfA\n)\n// Feature Transformation\nprintln\n(\n\"The hashed dataset where hashed values are stored in the column 'hashes':\"\n)\nmodel\n.\ntransform\n(\ndfA\n).\nshow\n()\n// Compute the locality sensitive hashes for the input rows, then perform approximate\n// similarity join.\n// We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n// `model.approxSimilarityJoin(transformedA, transformedB, 0.6)`\nprintln\n(\n\"Approximately joining dfA and dfB on Jaccard distance smaller than 0.6:\"\n)\nmodel\n.\napproxSimilarityJoin\n(\ndfA\n,\ndfB\n,\n0.6\n,\n\"JaccardDistance\"\n)\n.\nselect\n(\ncol\n(\n\"datasetA.id\"\n).\nalias\n(\n\"idA\"\n),\ncol\n(\n\"datasetB.id\"\n).\nalias\n(\n\"idB\"\n),\ncol\n(\n\"JaccardDistance\"\n)).\nshow\n()\n// ", "question": "What is the Jaccard distance threshold used in the approximate similarity join?", "answers": {"text": ["0.6"], "answer_start": [509]}}
{"context": "\"JaccardDistance\"\n)\n.\nselect\n(\ncol\n(\n\"datasetA.id\"\n).\nalias\n(\n\"idA\"\n),\ncol\n(\n\"datasetB.id\"\n).\nalias\n(\n\"idB\"\n),\ncol\n(\n\"JaccardDistance\"\n)).\nshow\n()\n// Compute the locality sensitive hashes for the input rows, then perform approximate nearest\n// neighbor search.\n// We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n// `model.approxNearestNeighbors(transformedA, key, 2)`\n// It may return less than 2 rows when not enough approximate near-neighbor candidates are\n// found.\nprintln\n(\n\"Approximately searching dfA for 2 nearest neighbors of the key:\"\n)\nmodel\n.\napproxNearestNeighbors\n(\ndfA\n,\nkey\n,\n2\n).\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\" in the Spark repo.\nRefer to the\nMinHashLSH Java docs\nf", "question": "Where can I find full example code for MinHashLSH?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\" in the Spark repo."], "answer_start": [641]}}
{"context": " example code at \"examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\" in the Spark repo.\nRefer to the\nMinHashLSH Java docs\nfor more details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.feature.MinHashLSH\n;\nimport\norg.apache.spark.ml.feature.MinHashLSHModel\n;\nimport\norg.apache.spark.ml.linalg.Vector\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.RowFactory\n;\nimport\norg.apache.spark.sql.types.DataTypes\n;\nimport\norg.apache.spark.sql.types.Metadata\n;\nimport\norg.apache.spark.sql.types.StructField\n;\nimport\norg.apache.spark.sql.types.StructType\n;\nimport\nstatic\norg\n.\napache\n.\nspark\n.\nsq", "question": "Where can I find example code for MinHashLSH?", "answers": {"text": ["examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala"], "answer_start": [18]}}
{"context": "\n[]\nvalues\n=\n{\n1.0\n,\n1.0\n};\nVector\nkey\n=\nVectors\n.\nsparse\n(\n6\n,\nindices\n,\nvalues\n);\nMinHashLSH\nmh\n=\nnew\nMinHashLSH\n()\n.\nsetNumHashTables\n(\n5\n)\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"hashes\"\n);\nMinHashLSHModel\nmodel\n=\nmh\n.\nfit\n(\ndfA\n);\n// Feature Transformation\nSystem\n.\nout\n.\nprintln\n(\n\"The hashed dataset where hashed values are stored in the column 'hashes':\"\n);\nmodel\n.\ntransform\n(\ndfA\n).\nshow\n();\n// Compute the locality sensitive hashes for the input rows, then perform approximate\n// similarity join.\n// We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n// `model.approxSimilarityJoin(transformedA, transformedB, 0.6)`\nSystem\n.\nout\n.\nprintln\n(\n\"Approximately joining dfA and dfB on Jaccard distance smaller than 0.6:\"\n);\nmodel\n.\napproxSimilarityJoin\n(\n", "question": "What is the Jaccard distance threshold used for approximate similarity join?", "answers": {"text": ["0.6"], "answer_start": [661]}}
{"context": "andidates are\n// found.\nSystem\n.\nout\n.\nprintln\n(\n\"Approximately searching dfA for 2 nearest neighbors of the key:\"\n);\nmodel\n.\napproxNearestNeighbors\n(\ndfA\n,\nkey\n,\n2\n).\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\" in the Spark repo.", "question": "Where can I find a full example code for the demonstrated functionality?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\" in the Spark repo."], "answer_start": [177]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars"], "answer_start": [46]}}
{"context": "ures\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nClassification and regression\n\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\wv}{\\mathbf{w}}\n\\newcommand{\\av}{\\mathbf{\\alpha}}\n\\newcommand{\\bv}{\\mathbf{b}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\id}{\\mathbf{I}}\n\\newcommand{\\ind}{\\mathbf{1}}\n\\newcommand{\\0}{\\mathbf{0}}\n\\newcommand{\\unit}{\\mathbf{e}}\n\\newcommand", "question": "Quais tópicos são repetidos várias vezes na lista fornecida?", "answers": {"text": ["Classification and regression"], "answer_start": [192]}}
{"context": "mand{\\N}{\\mathbb{N}}\n\\newcommand{\\id}{\\mathbf{I}}\n\\newcommand{\\ind}{\\mathbf{1}}\n\\newcommand{\\0}{\\mathbf{0}}\n\\newcommand{\\unit}{\\mathbf{e}}\n\\newcommand{\\one}{\\mathbf{1}}\n\\newcommand{\\zero}{\\mathbf{0}}\n\\]\nThis page covers algorithms for Classification and Regression.  It also includes sections\ndiscussing specific classes of algorithms, such as linear methods, trees, and ensembles.\nTable of Contents\nClassification\nLogistic regression\nBinomial logistic regression\nMultinomial logistic regression\nDecision tree classifier\nRandom forest classifier\nGradient-boosted tree classifier\nMultilayer perceptron classifier\nLinear Support Vector Machine\nOne-vs-Rest classifier (a.k.a. One-vs-All)\nNaive Bayes\nFactorization machines classifier\nRegression\nLinear regression\nGeneralized linear regression\nAvailable ", "question": "What types of algorithms does this page cover?", "answers": {"text": ["Classification and Regression."], "answer_start": [235]}}
{"context": "est classifier (a.k.a. One-vs-All)\nNaive Bayes\nFactorization machines classifier\nRegression\nLinear regression\nGeneralized linear regression\nAvailable families\nDecision tree regression\nRandom forest regression\nGradient-boosted tree regression\nSurvival regression\nIsotonic regression\nFactorization machines regressor\nLinear methods\nFactorization Machines\nDecision trees\nInputs and Outputs\nInput Columns\nOutput Columns\nTree Ensembles\nRandom Forests\nInputs and Outputs\nInput Columns\nOutput Columns (Predictions)\nGradient-Boosted Trees (GBTs)\nInputs and Outputs\nInput Columns\nOutput Columns (Predictions)\nClassification\nLogistic regression\nLogistic regression is a popular method to predict a categorical response. It is a special case of\nGeneralized Linear models\nthat predicts the probability of the out", "question": "What type of response does Logistic regression predict?", "answers": {"text": ["predicts the probability of the out"], "answer_start": [765]}}
{"context": "intercepts.\nWhen fitting LogisticRegressionModel without intercept on dataset with constant nonzero column, Spark MLlib outputs zero coefficients for constant nonzero columns. This behavior is the same as R glmnet but different from LIBSVM.\nBinomial logistic regression\nFor more background and more details about the implementation of binomial logistic regression, refer to the documentation of\nlogistic regression in\nspark.mllib\n.\nExamples\nThe following example shows how to train binomial and multinomial logistic regression\nmodels for binary classification with elastic net regularization.\nelasticNetParam\ncorresponds to\n$\\alpha$ and\nregParam\ncorresponds to $\\lambda$.\nMore details on parameters can be found in the\nPython API documentation\n.\nfrom\npyspark.ml.classification\nimport\nLogisticRegressi", "question": "What does Spark MLlib output as coefficients for constant nonzero columns when fitting LogisticRegressionModel without an intercept?", "answers": {"text": ["Spark MLlib outputs zero coefficients for constant nonzero columns."], "answer_start": [108]}}
{"context": "LogisticRegression\n(\nmaxIter\n=\n10\n,\nregParam\n=\n0.3\n,\nelasticNetParam\n=\n0.8\n,\nfamily\n=\n\"\nmultinomial\n\"\n)\n# Fit the model\nmlrModel\n=\nmlr\n.\nfit\n(\ntraining\n)\n# Print the coefficients and intercepts for logistic regression with multinomial family\nprint\n(\n\"\nMultinomial coefficients:\n\"\n+\nstr\n(\nmlrModel\n.\ncoefficientMatrix\n))\nprint\n(\n\"\nMultinomial intercepts:\n\"\n+\nstr\n(\nmlrModel\n.\ninterceptVector\n))\nFind full example code at \"examples/src/main/python/ml/logistic_regression_with_elastic_net.py\" in the Spark repo.\nMore details on parameters can be found in the\nScala API documentation\n.\nimport\norg.apache.spark.ml.classification.LogisticRegression\n// Load training data\nval\ntraining\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\nval\nlr\n=\nnew\nLogisticRegression\n()\n.\nse", "question": "Where can I find a full example code for logistic regression with elastic net?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/logistic_regression_with_elastic_net.py\" in the Spark repo."], "answer_start": [394]}}
{"context": "(\ntraining\n)\n// Print the coefficients and intercepts for logistic regression with multinomial family\nprintln\n(\ns\n\"Multinomial coefficients: ${mlrModel.coefficientMatrix}\"\n)\nprintln\n(\ns\n\"Multinomial intercepts: ${mlrModel.interceptVector}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\" in the Spark repo.\nMore details on parameters can be found in the\nJava API documentation\n.\nimport\norg.apache.spark.ml.classification.LogisticRegression\n;\nimport\norg.apache.spark.ml.classification.LogisticRegressionModel\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Load training data\nDataset\n<\nRow\n>\ntraining\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n)\n.\nload\n(", "question": "Where can I find a full example code for Logistic Regression with ElasticNet?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\" in the Spark repo."], "answer_start": [242]}}
{"context": "\nmlr\n=\nnew\nLogisticRegression\n()\n.\nsetMaxIter\n(\n10\n)\n.\nsetRegParam\n(\n0.3\n)\n.\nsetElasticNetParam\n(\n0.8\n)\n.\nsetFamily\n(\n\"multinomial\"\n);\n// Fit the model\nLogisticRegressionModel\nmlrModel\n=\nmlr\n.\nfit\n(\ntraining\n);\n// Print the coefficients and intercepts for logistic regression with multinomial family\nSystem\n.\nout\n.\nprintln\n(\n\"Multinomial coefficients: \"\n+\nlrModel\n.\ncoefficientMatrix\n()\n+\n\"\\nMultinomial intercepts: \"\n+\nmlrModel\n.\ninterceptVector\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\" in the Spark repo.\nMore details on parameters can be found in the\nR API documentation\n.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_libsvm_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit an b", "question": "Where can one find a full example code for JavaLogisticRegressionWithElasticNetExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\" in the Spark repo."], "answer_start": [452]}}
{"context": "PI documentation\n.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_libsvm_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit an binomial logistic regression model with spark.logit\nmodel\n<-\nspark.logit\n(\ntraining\n,\nlabel\n~\nfeatures\n,\nmaxIter\n=\n10\n,\nregParam\n=\n0.3\n,\nelasticNetParam\n=\n0.8\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\nFind full example code at \"examples/src/main/r/ml/logit.R\" in the Spark repo.\nThe\nspark.ml\nimplementation of logistic regression also supports\nextracting a summary of the model over the training set. Note that the\npredictions and metrics which are stored as\nDataFrame\nin\nLogisticRegressionSummary\nare annotated\n@transient\nand hence\nonly available on the driver.\nLogisticRegressionTr", "question": "Where can I find the full example code for logistic regression?", "answers": {"text": ["Find full example code at \"examples/src/main/r/ml/logit.R\" in the Spark repo."], "answer_start": [418]}}
{"context": "\n===\nmaxFMeasure\n)\n.\nselect\n(\n\"threshold\"\n).\nhead\n().\ngetDouble\n(\n0\n)\nlrModel\n.\nsetThreshold\n(\nbestThreshold\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\" in the Spark repo.\nLogisticRegressionTrainingSummary\nprovides a summary for a\nLogisticRegressionModel\n.\nIn the case of binary classification, certain additional metrics are\navailable, e.g. ROC curve. The binary summary can be accessed via the\nbinarySummary\nmethod. See\nBinaryLogisticRegressionTrainingSummary\n.\nContinuing the earlier example:\nimport\norg.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary\n;\nimport\norg.apache.spark.ml.classification.LogisticRegression\n;\nimport\norg.apache.spark.ml.classification.LogisticRegressionModel\n;\nimport\norg.", "question": "Where can I find a full example code for Logistic Regression?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\" in the Spark repo."], "answer_start": [111]}}
{"context": ".\nout\n.\nprintln\n(\nlossPerIteration\n);\n}\n// Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\nDataset\n<\nRow\n>\nroc\n=\ntrainingSummary\n.\nroc\n();\nroc\n.\nshow\n();\nroc\n.\nselect\n(\n\"FPR\"\n).\nshow\n();\nSystem\n.\nout\n.\nprintln\n(\ntrainingSummary\n.\nareaUnderROC\n());\n// Get the threshold corresponding to the maximum F-Measure and rerun LogisticRegression with\n// this selected threshold.\nDataset\n<\nRow\n>\nfMeasure\n=\ntrainingSummary\n.\nfMeasureByThreshold\n();\ndouble\nmaxFMeasure\n=\nfMeasure\n.\nselect\n(\nfunctions\n.\nmax\n(\n\"F-Measure\"\n)).\nhead\n().\ngetDouble\n(\n0\n);\ndouble\nbestThreshold\n=\nfMeasure\n.\nwhere\n(\nfMeasure\n.\ncol\n(\n\"F-Measure\"\n).\nequalTo\n(\nmaxFMeasure\n))\n.\nselect\n(\n\"threshold\"\n).\nhead\n().\ngetDouble\n(\n0\n);\nlrModel\n.\nsetThreshold\n(\nbestThreshold\n);\nFind full example code at \"examples/s", "question": "What is obtained as a dataframe and areaUnderROC?", "answers": {"text": ["the receiver-operating characteristic"], "answer_start": [50]}}
{"context": "o\n(\nmaxFMeasure\n))\n.\nselect\n(\n\"threshold\"\n).\nhead\n().\ngetDouble\n(\n0\n);\nlrModel\n.\nsetThreshold\n(\nbestThreshold\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\" in the Spark repo.\nMultinomial logistic regression\nMulticlass classification is supported via multinomial logistic (softmax) regression. In multinomial logistic regression,\nthe algorithm produces $K$ sets of coefficients, or a matrix of dimension $K \\times J$ where $K$ is the number of outcome\nclasses and $J$ is the number of features. If the algorithm is fit with an intercept term then a length $K$ vector of\nintercepts is available.\nMultinomial coefficients are available as\ncoefficientMatrix\nand intercepts are available as\ninterceptVector\n.\ncoefficients\nand\n", "question": "Where can I find a full example code for JavaLogisticRegressionSummary?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\" in the Spark repo."], "answer_start": [113]}}
{"context": "tive log-likelihood, using a multinomial response model, with elastic-net penalty to control for overfitting.\n\\[\n\\min_{\\beta, \\beta_0} -\\left[\\sum_{i=1}^L w_i \\cdot \\log P(Y = y_i|\\mathbf{x}_i)\\right] + \\lambda \\left[\\frac{1}{2}\\left(1 - \\alpha\\right)||\\boldsymbol{\\beta}||_2^2 + \\alpha ||\\boldsymbol{\\beta}||_1\\right]\n\\]\nFor a detailed derivation please see\nhere\n.\nExamples\nThe following example shows how to train a multiclass logistic regression\nmodel with elastic net regularization, as well as extract the multiclass\ntraining summary for evaluating the model.\nfrom\npyspark.ml.classification\nimport\nLogisticRegression\n# Load training data\ntraining\n=\nspark\n\\\n.\nread\n\\\n.\nformat\n(\n\"\nlibsvm\n\"\n)\n\\\n.\nload\n(\n\"\ndata/mllib/sample_multiclass_classification_data.txt\n\"\n)\nlr\n=\nLogisticRegression\n(\nmaxIter\n=", "question": "What type of regularization is used in the example to train a multiclass logistic regression model?", "answers": {"text": ["elastic net regularization"], "answer_start": [460]}}
{"context": "eptVector}\"\n)\nval\ntrainingSummary\n=\nlrModel\n.\nsummary\n// Obtain the objective per iteration\nval\nobjectiveHistory\n=\ntrainingSummary\n.\nobjectiveHistory\nprintln\n(\n\"objectiveHistory:\"\n)\nobjectiveHistory\n.\nforeach\n(\nprintln\n)\n// for multiclass, we can inspect metrics on a per-label basis\nprintln\n(\n\"False positive rate by label:\"\n)\ntrainingSummary\n.\nfalsePositiveRateByLabel\n.\nzipWithIndex\n.\nforeach\n{\ncase\n(\nrate\n,\nlabel\n)\n=>\nprintln\n(\ns\n\"label $label: $rate\"\n)\n}\nprintln\n(\n\"True positive rate by label:\"\n)\ntrainingSummary\n.\ntruePositiveRateByLabel\n.\nzipWithIndex\n.\nforeach\n{\ncase\n(\nrate\n,\nlabel\n)\n=>\nprintln\n(\ns\n\"label $label: $rate\"\n)\n}\nprintln\n(\n\"Precision by label:\"\n)\ntrainingSummary\n.\nprecisionByLabel\n.\nzipWithIndex\n.\nforeach\n{\ncase\n(\nprec\n,\nlabel\n)\n=>\nprintln\n(\ns\n\"label $label: $prec\"\n)\n}\nprint", "question": "How can we inspect metrics on a per-label basis for multiclass problems?", "answers": {"text": ["for multiclass, we can inspect metrics on a per-label basis"], "answer_start": [224]}}
{"context": "ositiveRate\n);\nSystem\n.\nout\n.\nprintln\n(\n\"F-measure: \"\n+\nfMeasure\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Precision: \"\n+\nprecision\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Recall: \"\n+\nrecall\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\" in the Spark repo.\nMore details on parameters can be found in the\nR API documentation\n.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_multiclass_classification_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit a multinomial logistic regression model with spark.logit\nmodel\n<-\nspark.logit\n(\ntraining\n,\nlabel\n~\nfeatures\n,\nmaxIter\n=\n10\n,\nregParam\n=\n0.3\n,\nelasticNetParam\n=\n0.8\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead", "question": "Where can I find a full example code for JavaMulticlassLogisticRegressionWithElasticNetExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\" in the Spark repo."], "answer_start": [170]}}
{"context": "\n,\nmaxIter\n=\n10\n,\nregParam\n=\n0.3\n,\nelasticNetParam\n=\n0.8\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\nFind full example code at \"examples/src/main/r/ml/logit.R\" in the Spark repo.\nDecision tree classifier\nDecision trees are a popular family of classification and regression methods.\nMore information about the\nspark.ml\nimplementation can be found further in the\nsection on decision trees\n.\nExamples\nThe following examples load a dataset in LibSVM format, split it into training and test sets, train on the first dataset, and then evaluate on the held-out test set.\nWe use two feature transformers to prepare the data; these help index categories for the label and categorical features, adding metadata to the\nDataFrame\nwhich the Decisi", "question": "Where can I find a full example code for the logistic regression?", "answers": {"text": ["Find full example code at \"examples/src/main/r/ml/logit.R\" in the Spark repo."], "answer_start": [167]}}
{"context": "metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer\n=\nStringIndexer\n(\ninputCol\n=\n\"\nlabel\n\"\n,\noutputCol\n=\n\"\nindexedLabel\n\"\n).\nfit\n(\ndata\n)\n# Automatically identify categorical features, and index them.\n# We specify maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer\n=\n\\\nVectorIndexer\n(\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nindexedFeatures\n\"\n,\nmaxCategories\n=\n4\n).\nfit\n(\ndata\n)\n# Split the data into training and test sets (30% held out for testing)\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n([\n0.7\n,\n0.3\n])\n# Train a DecisionTree model.\ndt\n=\nDecisionTreeClassifier\n(\nlabelCol\n=\n\"\nindexedLabel\n\"\n,\nfeaturesCol\n=\n\"\nindexedFeatures\n\"\n)\n# Chain indexers and tree in a Pipeline\npipeline\n=\nPipeline\n(\nstages\n=", "question": "What is the purpose of `maxCategories` in the `VectorIndexer`?", "answers": {"text": ["We specify maxCategories so features with > 4 distinct values are treated as continuous."], "answer_start": [249]}}
{"context": "cy\n\"\n)\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprint\n(\n\"\nTest Error = %g\n\"\n%\n(\n1.0\n-\naccuracy\n))\ntreeModel\n=\nmodel\n.\nstages\n[\n2\n]\n# summary only\nprint\n(\ntreeModel\n)\nFind full example code at \"examples/src/main/python/ml/decision_tree_classification_example.py\" in the Spark repo.\nMore details on parameters can be found in the\nScala API documentation\n.\nimport\norg.apache.spark.ml.Pipeline\nimport\norg.apache.spark.ml.classification.DecisionTreeClassificationModel\nimport\norg.apache.spark.ml.classification.DecisionTreeClassifier\nimport\norg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport\norg.apache.spark.ml.feature.\n{\nIndexToString\n,\nStringIndexer\n,\nVectorIndexer\n}\n// Load the data stored in LIBSVM format as a DataFrame.\nval\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nlo", "question": "Where can one find a full example code for this decision tree classification?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/decision_tree_classification_example.py\" in the Spark repo."], "answer_start": [173]}}
{"context": "xToString\n,\nStringIndexer\n,\nVectorIndexer\n}\n// Load the data stored in LIBSVM format as a DataFrame.\nval\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Index labels, adding metadata to the label column.\n// Fit on whole dataset to include all labels in index.\nval\nlabelIndexer\n=\nnew\nStringIndexer\n()\n.\nsetInputCol\n(\n\"label\"\n)\n.\nsetOutputCol\n(\n\"indexedLabel\"\n)\n.\nfit\n(\ndata\n)\n// Automatically identify categorical features, and index them.\nval\nfeatureIndexer\n=\nnew\nVectorIndexer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxCategories\n(\n4\n)\n// features with > 4 distinct values are treated as continuous.\n.\nfit\n(\ndata\n)\n// Split the data into training and test sets (30% held out for testing).\nval\nArray\n(\ntrainingData\n,\ntestD", "question": "What is the purpose of setting `setMaxCategories` to 4 in the `featureIndexer`?", "answers": {"text": ["features with > 4 distinct values are treated as continuous."], "answer_start": [618]}}
{"context": "s are treated as continuous.\n.\nfit\n(\ndata\n)\n// Split the data into training and test sets (30% held out for testing).\nval\nArray\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.7\n,\n0.3\n))\n// Train a DecisionTree model.\nval\ndt\n=\nnew\nDecisionTreeClassifier\n()\n.\nsetLabelCol\n(\n\"indexedLabel\"\n)\n.\nsetFeaturesCol\n(\n\"indexedFeatures\"\n)\n// Convert indexed labels back to original labels.\nval\nlabelConverter\n=\nnew\nIndexToString\n()\n.\nsetInputCol\n(\n\"prediction\"\n)\n.\nsetOutputCol\n(\n\"predictedLabel\"\n)\n.\nsetLabels\n(\nlabelIndexer\n.\nlabelsArray\n(\n0\n))\n// Chain indexers and tree in a Pipeline.\nval\npipeline\n=\nnew\nPipeline\n()\n.\nsetStages\n(\nArray\n(\nlabelIndexer\n,\nfeatureIndexer\n,\ndt\n,\nlabelConverter\n))\n// Train model. This also runs the indexers.\nval\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n)\n// Make pre", "question": "What percentage of the data is held out for testing?", "answers": {"text": ["30% held out for testing"], "answer_start": [91]}}
{"context": "l\n=\nmodel\n.\nstages\n(\n2\n).\nasInstanceOf\n[\nDecisionTreeClassificationModel\n]\nprintln\n(\ns\n\"Learned classification tree model:\\n ${treeModel.toDebugString}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\" in the Spark repo.\nMore details on parameters can be found in the\nJava API documentation\n.\nimport\norg.apache.spark.ml.Pipeline\n;\nimport\norg.apache.spark.ml.PipelineModel\n;\nimport\norg.apache.spark.ml.PipelineStage\n;\nimport\norg.apache.spark.ml.classification.DecisionTreeClassifier\n;\nimport\norg.apache.spark.ml.classification.DecisionTreeClassificationModel\n;\nimport\norg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n;\nimport\norg.apache.spark.ml.feature.*\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apac", "question": "Where can I find a full example code for DecisionTreeClassification?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\" in the Spark repo."], "answer_start": [155]}}
{"context": "e.spark.ml.evaluation.MulticlassClassificationEvaluator\n;\nimport\norg.apache.spark.ml.feature.*\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Load the data stored in LIBSVM format as a DataFrame.\nDataset\n<\nRow\n>\ndata\n=\nspark\n.\nread\n()\n.\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n);\n// Index labels, adding metadata to the label column.\n// Fit on whole dataset to include all labels in index.\nStringIndexerModel\nlabelIndexer\n=\nnew\nStringIndexer\n()\n.\nsetInputCol\n(\n\"label\"\n)\n.\nsetOutputCol\n(\n\"indexedLabel\"\n)\n.\nfit\n(\ndata\n);\n// Automatically identify categorical features, and index them.\nVectorIndexerModel\nfeatureIndexer\n=\nnew\nVectorIndexer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"indexedFeatur", "question": "Qual formato de dados é usado para carregar os dados no DataFrame?", "answers": {"text": ["libsvm"], "answer_start": [320]}}
{"context": "edLabel\"\n,\n\"label\"\n,\n\"features\"\n).\nshow\n(\n5\n);\n// Select (prediction, true label) and compute test error.\nMulticlassClassificationEvaluator\nevaluator\n=\nnew\nMulticlassClassificationEvaluator\n()\n.\nsetLabelCol\n(\n\"indexedLabel\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"accuracy\"\n);\ndouble\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Test Error = \"\n+\n(\n1.0\n-\naccuracy\n));\nDecisionTreeClassificationModel\ntreeModel\n=\n(\nDecisionTreeClassificationModel\n)\n(\nmodel\n.\nstages\n()[\n2\n]);\nSystem\n.\nout\n.\nprintln\n(\n\"Learned classification tree model:\\n\"\n+\ntreeModel\n.\ntoDebugString\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details", "question": "Where can I find the full example code for JavaDecisionTreeClassificationExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\" in the Spark repo."], "answer_start": [618]}}
{"context": "les/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_libsvm_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit a DecisionTree classification model with spark.decisionTree\nmodel\n<-\nspark.decisionTree\n(\ntraining\n,\nlabel\n~\nfeatures\n,\n\"classification\"\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\nFind full example code at \"examples/src/main/r/ml/decisionTree.R\" in the Spark repo.\nRandom forest classifier\nRandom forests are a popular family of classification and regression methods.\nMore information about the\nspark.ml\nimplementation can be found further in the\nsection", "question": "Where can I find the full example code for the DecisionTree?", "answers": {"text": ["Find full example code at \"examples/src/main/r/ml/decisionTree.R\" in the Spark repo."], "answer_start": [526]}}
{"context": "cation\nimport\nRandomForestClassifier\nfrom\npyspark.ml.feature\nimport\nIndexToString\n,\nStringIndexer\n,\nVectorIndexer\nfrom\npyspark.ml.evaluation\nimport\nMulticlassClassificationEvaluator\n# Load and parse the data file, converting it to a DataFrame.\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer\n=\nStringIndexer\n(\ninputCol\n=\n\"\nlabel\n\"\n,\noutputCol\n=\n\"\nindexedLabel\n\"\n).\nfit\n(\ndata\n)\n# Automatically identify categorical features, and index them.\n# Set maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer\n=\n\\\nVectorIndexer\n(\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nindexedFeatures\n\"\n,\nmaxCategories\n=", "question": "What is the purpose of `StringIndexer` in this code snippet?", "answers": {"text": ["Index labels, adding metadata to the label column."], "answer_start": [338]}}
{"context": "0\n-\naccuracy\n))\nrfModel\n=\nmodel\n.\nstages\n[\n2\n]\nprint\n(\nrfModel\n)\n# summary only\nFind full example code at \"examples/src/main/python/ml/random_forest_classifier_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.Pipeline\nimport\norg.apache.spark.ml.classification.\n{\nRandomForestClassificationModel\n,\nRandomForestClassifier\n}\nimport\norg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport\norg.apache.spark.ml.feature.\n{\nIndexToString\n,\nStringIndexer\n,\nVectorIndexer\n}\n// Load and parse the data file, converting it to a DataFrame.\nval\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Index labels, adding metadata to the label column.\n// Fit on whole dataset to include all labels in index.\n", "question": "Where can I find a full example code for the random forest classifier?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/random_forest_classifier_example.py\" in the Spark repo."], "answer_start": [80]}}
{"context": "(\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Index labels, adding metadata to the label column.\n// Fit on whole dataset to include all labels in index.\nval\nlabelIndexer\n=\nnew\nStringIndexer\n()\n.\nsetInputCol\n(\n\"label\"\n)\n.\nsetOutputCol\n(\n\"indexedLabel\"\n)\n.\nfit\n(\ndata\n)\n// Automatically identify categorical features, and index them.\n// Set maxCategories so features with > 4 distinct values are treated as continuous.\nval\nfeatureIndexer\n=\nnew\nVectorIndexer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxCategories\n(\n4\n)\n.\nfit\n(\ndata\n)\n// Split the data into training and test sets (30% held out for testing).\nval\nArray\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.7\n,\n0.3\n))\n// Train a RandomForest model.\nval\nrf\n=\nnew\nRandomForestClassifier\n()\n.\nsetLabelCo", "question": "What is the purpose of setting maxCategories to 4 in the VectorIndexer?", "answers": {"text": ["Set maxCategories so features with > 4 distinct values are treated as continuous."], "answer_start": [332]}}
{"context": "forest model:\\n ${rfModel.toDebugString}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.ml.Pipeline\n;\nimport\norg.apache.spark.ml.PipelineModel\n;\nimport\norg.apache.spark.ml.PipelineStage\n;\nimport\norg.apache.spark.ml.classification.RandomForestClassificationModel\n;\nimport\norg.apache.spark.ml.classification.RandomForestClassifier\n;\nimport\norg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n;\nimport\norg.apache.spark.ml.feature.*\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Load and parse the data file, converting it to a DataFrame.\nDataset\n<\nRow\n>\nd", "question": "Where can I find a full example code for RandomForestClassifier?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\" in the Spark repo."], "answer_start": [44]}}
{"context": "org.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Load and parse the data file, converting it to a DataFrame.\nDataset\n<\nRow\n>\ndata\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n);\n// Index labels, adding metadata to the label column.\n// Fit on whole dataset to include all labels in index.\nStringIndexerModel\nlabelIndexer\n=\nnew\nStringIndexer\n()\n.\nsetInputCol\n(\n\"label\"\n)\n.\nsetOutputCol\n(\n\"indexedLabel\"\n)\n.\nfit\n(\ndata\n);\n// Automatically identify categorical features, and index them.\n// Set maxCategories so features with > 4 distinct values are treated as continuous.\nVectorIndexerModel\nfeatureIndexer\n=\nnew\nVectorIndexer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxCategories\n(\n4\n)\n.\nfit\n(\ndata\n);\n// Spl", "question": "What format is used to load the data file?", "answers": {"text": ["\"libsvm\""], "answer_start": [182]}}
{"context": "eatureIndexer\n=\nnew\nVectorIndexer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxCategories\n(\n4\n)\n.\nfit\n(\ndata\n);\n// Split the data into training and test sets (30% held out for testing)\nDataset\n<\nRow\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]\n{\n0.7\n,\n0.3\n});\nDataset\n<\nRow\n>\ntrainingData\n=\nsplits\n[\n0\n];\nDataset\n<\nRow\n>\ntestData\n=\nsplits\n[\n1\n];\n// Train a RandomForest model.\nRandomForestClassifier\nrf\n=\nnew\nRandomForestClassifier\n()\n.\nsetLabelCol\n(\n\"indexedLabel\"\n)\n.\nsetFeaturesCol\n(\n\"indexedFeatures\"\n);\n// Convert indexed labels back to original labels.\nIndexToString\nlabelConverter\n=\nnew\nIndexToString\n()\n.\nsetInputCol\n(\n\"prediction\"\n)\n.\nsetOutputCol\n(\n\"predictedLabel\"\n)\n.\nsetLabels\n(\nlabelIndexer\n.\nlabelsArray\n()[\n0\n]);\n// Chain indexers and forest in a Pi", "question": "What is set as the input column for the VectorIndexer?", "answers": {"text": ["\"features\""], "answer_start": [53]}}
{"context": "ClassificationEvaluator\nevaluator\n=\nnew\nMulticlassClassificationEvaluator\n()\n.\nsetLabelCol\n(\n\"indexedLabel\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"accuracy\"\n);\ndouble\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Test Error = \"\n+\n(\n1.0\n-\naccuracy\n));\nRandomForestClassificationModel\nrfModel\n=\n(\nRandomForestClassificationModel\n)(\nmodel\n.\nstages\n()[\n2\n]);\nSystem\n.\nout\n.\nprintln\n(\n\"Learned classification forest model:\\n\"\n+\nrfModel\n.\ntoDebugString\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_libsvm_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n", "question": "Where can I find a full example code for this process?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\" in the Spark repo."], "answer_start": [499]}}
{"context": "R API docs\nfor more details.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_libsvm_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit a random forest classification model with spark.randomForest\nmodel\n<-\nspark.randomForest\n(\ntraining\n,\nlabel\n~\nfeatures\n,\n\"classification\"\n,\nnumTrees\n=\n10\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\nFind full example code at \"examples/src/main/r/ml/randomForest.R\" in the Spark repo.\nGradient-boosted tree classifier\nGradient-boosted trees (GBTs) are a popular classification and regression method using ensembles of decision trees.\nMore information about the\nspark.ml\nimplementation can be found further in the\nsection on GBTs\n.\nExamples\nThe following examples load a dataset in", "question": "Where can I find the full example code for the random forest?", "answers": {"text": ["Find full example code at \"examples/src/main/r/ml/randomForest.R\" in the Spark repo."], "answer_start": [420]}}
{"context": "er\n,\nVectorIndexer\nfrom\npyspark.ml.evaluation\nimport\nMulticlassClassificationEvaluator\n# Load and parse the data file, converting it to a DataFrame.\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer\n=\nStringIndexer\n(\ninputCol\n=\n\"\nlabel\n\"\n,\noutputCol\n=\n\"\nindexedLabel\n\"\n).\nfit\n(\ndata\n)\n# Automatically identify categorical features, and index them.\n# Set maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer\n=\n\\\nVectorIndexer\n(\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nindexedFeatures\n\"\n,\nmaxCategories\n=\n4\n).\nfit\n(\ndata\n)\n# Split the data into training and test sets (30% held out for testing)\n(\ntr", "question": "What is the purpose of `maxCategories` in `VectorIndexer`?", "answers": {"text": ["Set maxCategories so features with > 4 distinct values are treated as continuous."], "answer_start": [513]}}
{"context": "\n(\n\"\nprediction\n\"\n,\n\"\nindexedLabel\n\"\n,\n\"\nfeatures\n\"\n).\nshow\n(\n5\n)\n# Select (prediction, true label) and compute test error\nevaluator\n=\nMulticlassClassificationEvaluator\n(\nlabelCol\n=\n\"\nindexedLabel\n\"\n,\npredictionCol\n=\n\"\nprediction\n\"\n,\nmetricName\n=\n\"\naccuracy\n\"\n)\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprint\n(\n\"\nTest Error = %g\n\"\n%\n(\n1.0\n-\naccuracy\n))\ngbtModel\n=\nmodel\n.\nstages\n[\n2\n]\nprint\n(\ngbtModel\n)\n# summary only\nFind full example code at \"examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.Pipeline\nimport\norg.apache.spark.ml.classification.\n{\nGBTClassificationModel\n,\nGBTClassifier\n}\nimport\norg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport\norg.apac", "question": "What is used to compute test error?", "answers": {"text": ["MulticlassClassificationEvaluator"], "answer_start": [135]}}
{"context": ".ml.classification.\n{\nGBTClassificationModel\n,\nGBTClassifier\n}\nimport\norg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport\norg.apache.spark.ml.feature.\n{\nIndexToString\n,\nStringIndexer\n,\nVectorIndexer\n}\n// Load and parse the data file, converting it to a DataFrame.\nval\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Index labels, adding metadata to the label column.\n// Fit on whole dataset to include all labels in index.\nval\nlabelIndexer\n=\nnew\nStringIndexer\n()\n.\nsetInputCol\n(\n\"label\"\n)\n.\nsetOutputCol\n(\n\"indexedLabel\"\n)\n.\nfit\n(\ndata\n)\n// Automatically identify categorical features, and index them.\n// Set maxCategories so features with > 4 distinct values are treated as continuous.\nval\nfeatureIndexer\n=\nnew\nVectorIndexer\n()\n.\nsetInp", "question": "What is the purpose of `StringIndexer` in the provided code?", "answers": {"text": ["Index labels, adding metadata to the label column."], "answer_start": [379]}}
{"context": "nd index them.\n// Set maxCategories so features with > 4 distinct values are treated as continuous.\nval\nfeatureIndexer\n=\nnew\nVectorIndexer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxCategories\n(\n4\n)\n.\nfit\n(\ndata\n)\n// Split the data into training and test sets (30% held out for testing).\nval\nArray\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.7\n,\n0.3\n))\n// Train a GBT model.\nval\ngbt\n=\nnew\nGBTClassifier\n()\n.\nsetLabelCol\n(\n\"indexedLabel\"\n)\n.\nsetFeaturesCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxIter\n(\n10\n)\n.\nsetFeatureSubsetStrategy\n(\n\"auto\"\n)\n// Convert indexed labels back to original labels.\nval\nlabelConverter\n=\nnew\nIndexToString\n()\n.\nsetInputCol\n(\n\"prediction\"\n)\n.\nsetOutputCol\n(\n\"predictedLabel\"\n)\n.\nsetLabels\n(\nlabelIndexer\n.\nlabelsArray\n(\n0\n))\n// Ch", "question": "What is the purpose of setting maxCategories in the VectorIndexer?", "answers": {"text": ["Set maxCategories so features with > 4 distinct values are treated as continuous."], "answer_start": [18]}}
{"context": "verter\n=\nnew\nIndexToString\n()\n.\nsetInputCol\n(\n\"prediction\"\n)\n.\nsetOutputCol\n(\n\"predictedLabel\"\n)\n.\nsetLabels\n(\nlabelIndexer\n.\nlabelsArray\n(\n0\n))\n// Chain indexers and GBT in a Pipeline.\nval\npipeline\n=\nnew\nPipeline\n()\n.\nsetStages\n(\nArray\n(\nlabelIndexer\n,\nfeatureIndexer\n,\ngbt\n,\nlabelConverter\n))\n// Train model. This also runs the indexers.\nval\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n)\n// Make predictions.\nval\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n)\n// Select example rows to display.\npredictions\n.\nselect\n(\n\"predictedLabel\"\n,\n\"label\"\n,\n\"features\"\n).\nshow\n(\n5\n)\n// Select (prediction, true label) and compute test error.\nval\nevaluator\n=\nnew\nMulticlassClassificationEvaluator\n()\n.\nsetLabelCol\n(\n\"indexedLabel\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"accuracy\"\n)\nval\naccuracy\n=\neva", "question": "What is set as the prediction column in the MulticlassClassificationEvaluator?", "answers": {"text": ["\"prediction\""], "answer_start": [46]}}
{"context": "classClassificationEvaluator\n()\n.\nsetLabelCol\n(\n\"indexedLabel\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"accuracy\"\n)\nval\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprintln\n(\ns\n\"Test Error = ${1.0 - accuracy}\"\n)\nval\ngbtModel\n=\nmodel\n.\nstages\n(\n2\n).\nasInstanceOf\n[\nGBTClassificationModel\n]\nprintln\n(\ns\n\"Learned classification GBT model:\\n ${gbtModel.toDebugString}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.ml.Pipeline\n;\nimport\norg.apache.spark.ml.PipelineModel\n;\nimport\norg.apache.spark.ml.PipelineStage\n;\nimport\norg.apache.spark.ml.classification.GBTClassificationModel\n;\nimport\norg.apache.spark.ml.classifi", "question": "Where can I find a full example code for GradientBoostedTreeClassifier?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\" in the Spark repo."], "answer_start": [386]}}
{"context": "el\n;\nimport\norg.apache.spark.ml.PipelineStage\n;\nimport\norg.apache.spark.ml.classification.GBTClassificationModel\n;\nimport\norg.apache.spark.ml.classification.GBTClassifier\n;\nimport\norg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n;\nimport\norg.apache.spark.ml.feature.*\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Load and parse the data file, converting it to a DataFrame.\nDataset\n<\nRow\n>\ndata\n=\nspark\n.\nread\n()\n.\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n);\n// Index labels, adding metadata to the label column.\n// Fit on whole dataset to include all labels in index.\nStringIndexerModel\nlabelIndexer\n=\nnew\nStringIndexer\n()\n.\nsetInputCol\n(\n\"label\"\n)\n.\nsetOutputCol\n(\n\"indexedLabel\"\n)\n.\n", "question": "Which format is used to load the data file?", "answers": {"text": ["libsvm"], "answer_start": [515]}}
{"context": " to include all labels in index.\nStringIndexerModel\nlabelIndexer\n=\nnew\nStringIndexer\n()\n.\nsetInputCol\n(\n\"label\"\n)\n.\nsetOutputCol\n(\n\"indexedLabel\"\n)\n.\nfit\n(\ndata\n);\n// Automatically identify categorical features, and index them.\n// Set maxCategories so features with > 4 distinct values are treated as continuous.\nVectorIndexerModel\nfeatureIndexer\n=\nnew\nVectorIndexer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxCategories\n(\n4\n)\n.\nfit\n(\ndata\n);\n// Split the data into training and test sets (30% held out for testing)\nDataset\n<\nRow\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]\n{\n0.7\n,\n0.3\n});\nDataset\n<\nRow\n>\ntrainingData\n=\nsplits\n[\n0\n];\nDataset\n<\nRow\n>\ntestData\n=\nsplits\n[\n1\n];\n// Train a GBT model.\nGBTClassifier\ngbt\n=\nnew\nGBTClassifier\n()\n.\nsetLabelCol\n(\n\"indexe", "question": "What is the purpose of setting `setMaxCategories` to 4 in the `VectorIndexerModel`?", "answers": {"text": ["Set maxCategories so features with > 4 distinct values are treated as continuous."], "answer_start": [231]}}
{"context": "tModel\n=\n(\nGBTClassificationModel\n)(\nmodel\n.\nstages\n()[\n2\n]);\nSystem\n.\nout\n.\nprintln\n(\n\"Learned classification GBT model:\\n\"\n+\ngbtModel\n.\ntoDebugString\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_libsvm_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit a GBT classification model with spark.gbt\nmodel\n<-\nspark.gbt\n(\ntraining\n,\nlabel\n~\nfeatures\n,\n\"classification\"\n,\nmaxIter\n=\n10\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\nFind full example code at \"examples/src/main/r/ml/gbt.R\" in the Spark repo.\nMultilayer percept", "question": "Where can I find the full example code for JavaGradientBoostedTreeClassifier?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\" in the Spark repo."], "answer_start": [157]}}
{"context": "s as follows:\n\\[\n\\mathrm{y}(\\x) = \\mathrm{f_K}(...\\mathrm{f_2}(\\wv_2^T\\mathrm{f_1}(\\wv_1^T \\x+b_1)+b_2)...+b_K)\n\\]\nNodes in intermediate layers use sigmoid (logistic) function:\n\\[\n\\mathrm{f}(z_i) = \\frac{1}{1 + e^{-z_i}}\n\\]\nNodes in the output layer use softmax function:\n\\[\n\\mathrm{f}(z_i) = \\frac{e^{z_i}}{\\sum_{k=1}^N e^{z_k}}\n\\]\nThe number of nodes\n$N$\nin the output layer corresponds to the number of classes.\nMLPC employs backpropagation for learning the model. We use the logistic loss function for optimization and L-BFGS as an optimization routine.\nExamples\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml.classification\nimport\nMultilayerPerceptronClassifier\nfrom\npyspark.ml.evaluation\nimport\nMulticlassClassificationEvaluator\n# Load training data\ndata\n=\nspark\n.\nread\n.\nformat", "question": "What function do nodes in the output layer use?", "answers": {"text": ["Nodes in the output layer use softmax function:"], "answer_start": [224]}}
{"context": "l.evaluation.MulticlassClassificationEvaluator\n// Load the data stored in LIBSVM format as a DataFrame.\nval\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_multiclass_classification_data.txt\"\n)\n// Split the data into train and test\nval\nsplits\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.6\n,\n0.4\n),\nseed\n=\n1234L\n)\nval\ntrain\n=\nsplits\n(\n0\n)\nval\ntest\n=\nsplits\n(\n1\n)\n// specify layers for the neural network:\n// input layer of size 4 (features), two intermediate of size 5 and 4\n// and output of size 3 (classes)\nval\nlayers\n=\nArray\n[\nInt\n](\n4\n,\n5\n,\n4\n,\n3\n)\n// create the trainer and set its parameters\nval\ntrainer\n=\nnew\nMultilayerPerceptronClassifier\n()\n.\nsetLayers\n(\nlayers\n)\n.\nsetBlockSize\n(\n128\n)\n.\nsetSeed\n(\n1234L\n)\n.\nsetMaxIter\n(\n100\n)\n// train the model\nval\nmodel\n=\ntrainer\n.\nfit\n(\ntrain\n", "question": "What is the size of the input layer for the neural network?", "answers": {"text": ["4"], "answer_start": [304]}}
{"context": "te-dimensional space, which can be used for classification,\nregression, or other tasks. Intuitively, a good separation is achieved by the hyperplane that has\nthe largest distance to the nearest training-data points of any class (so-called functional margin),\nsince in general the larger the margin the lower the generalization error of the classifier. LinearSVC\nin Spark ML supports binary classification with linear SVM. Internally, it optimizes the\nHinge Loss\nusing OWLQN optimizer.\nExamples\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml.classification\nimport\nLinearSVC\n# Load training data\ntraining\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\nlsvc\n=\nLinearSVC\n(\nmaxIter\n=\n10\n,\nregParam\n=\n0.1\n)\n# Fit the model\nlsvcModel\n=\nlsvc\n.\nfit\n(\ntr", "question": "Which optimizer does LinearSVC in Spark ML use to optimize the Hinge Loss?", "answers": {"text": ["using OWLQN optimizer."], "answer_start": [462]}}
{"context": "svm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\nlsvc\n=\nLinearSVC\n(\nmaxIter\n=\n10\n,\nregParam\n=\n0.1\n)\n# Fit the model\nlsvcModel\n=\nlsvc\n.\nfit\n(\ntraining\n)\n# Print the coefficients and intercept for linear SVC\nprint\n(\n\"\nCoefficients:\n\"\n+\nstr\n(\nlsvcModel\n.\ncoefficients\n))\nprint\n(\n\"\nIntercept:\n\"\n+\nstr\n(\nlsvcModel\n.\nintercept\n))\nFind full example code at \"examples/src/main/python/ml/linearsvc.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.classification.LinearSVC\n// Load training data\nval\ntraining\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\nval\nlsvc\n=\nnew\nLinearSVC\n()\n.\nsetMaxIter\n(\n10\n)\n.\nsetRegParam\n(\n0.1\n)\n// Fit the model\nval\nlsvcModel\n=\nlsvc\n.\nfit\n(\ntraining\n)\n// Print the coefficients and intercep", "question": "Where can I find the full example code for LinearSVC?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/linearsvc.py\" in the Spark repo."], "answer_start": [331]}}
{"context": "LinearSVC\n()\n.\nsetMaxIter\n(\n10\n)\n.\nsetRegParam\n(\n0.1\n)\n// Fit the model\nval\nlsvcModel\n=\nlsvc\n.\nfit\n(\ntraining\n)\n// Print the coefficients and intercept for linear svc\nprintln\n(\ns\n\"Coefficients: ${lsvcModel.coefficients} Intercept: ${lsvcModel.intercept}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.ml.classification.LinearSVC\n;\nimport\norg.apache.spark.ml.classification.LinearSVCModel\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Load training data\nDataset\n<\nRow\n>\ntraining\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n);\nLinearSVC\nlsv", "question": "Where can I find a full example code for LinearSVC?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\" in the Spark repo."], "answer_start": [257]}}
{"context": ".data.frame\n(\nTitanic\n)\ntraining\n<-\ncreateDataFrame\n(\nt\n)\n# fit Linear SVM model\nmodel\n<-\nspark.svmLinear\n(\ntraining\n,\nSurvived\n~\n.\n,\nregParam\n=\n0.01\n,\nmaxIter\n=\n10\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\nprediction\n<-\npredict\n(\nmodel\n,\ntraining\n)\nshowDF\n(\nprediction\n)\nFind full example code at \"examples/src/main/r/ml/svmLinear.R\" in the Spark repo.\nOne-vs-Rest classifier (a.k.a. One-vs-All)\nOneVsRest\nis an example of a machine learning reduction for performing multiclass classification given a base classifier that can perform binary classification efficiently.  It is also known as “One-vs-All.”\nOneVsRest\nis implemented as an\nEstimator\n. For the base classifier, it takes instances of\nClassifier\nand creates a binary classification problem for each of the k classes. The classifier f", "question": "What is OneVsRest also known as?", "answers": {"text": ["It is also known as “One-vs-All.”"], "answer_start": [578]}}
{"context": "or\n. For the base classifier, it takes instances of\nClassifier\nand creates a binary classification problem for each of the k classes. The classifier for class i is trained to predict whether the label is i or not, distinguishing class i from all other classes.\nPredictions are done by evaluating each binary classifier and the index of the most confident classifier is output as label.\nExamples\nThe example below demonstrates how to load the\nIris dataset\n, parse it as a DataFrame and perform multiclass classification using\nOneVsRest\n. The test error is calculated to measure the algorithm accuracy.\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml.classification\nimport\nLogisticRegression\n,\nOneVsRest\nfrom\npyspark.ml.evaluation\nimport\nMulticlassClassificationEvaluator\n# load data fil", "question": "How are predictions made using the OneVsRest classifier?", "answers": {"text": ["Predictions are done by evaluating each binary classifier and the index of the most confident classifier is output as label."], "answer_start": [261]}}
{"context": "etricName\n(\n\"accuracy\"\n)\n// compute the classification error on test data.\nval\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprintln\n(\ns\n\"Test Error = ${1 - accuracy}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.ml.classification.LogisticRegression\n;\nimport\norg.apache.spark.ml.classification.OneVsRest\n;\nimport\norg.apache.spark.ml.classification.OneVsRestModel\n;\nimport\norg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\n// load data file.\nDataset\n<\nRow\n>\ninputData\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_multiclass_classification_", "question": "Where can I find a full example code for OneVsRestExample?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\" in the Spark repo."], "answer_start": [172]}}
{"context": "k.sql.Row\n;\n// load data file.\nDataset\n<\nRow\n>\ninputData\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_multiclass_classification_data.txt\"\n);\n// generate the train/test split.\nDataset\n<\nRow\n>[]\ntmp\n=\ninputData\n.\nrandomSplit\n(\nnew\ndouble\n[]{\n0.8\n,\n0.2\n});\nDataset\n<\nRow\n>\ntrain\n=\ntmp\n[\n0\n];\nDataset\n<\nRow\n>\ntest\n=\ntmp\n[\n1\n];\n// configure the base classifier.\nLogisticRegression\nclassifier\n=\nnew\nLogisticRegression\n()\n.\nsetMaxIter\n(\n10\n)\n.\nsetTol\n(\n1\nE\n-\n6\n)\n.\nsetFitIntercept\n(\ntrue\n);\n// instantiate the One Vs Rest Classifier.\nOneVsRest\novr\n=\nnew\nOneVsRest\n().\nsetClassifier\n(\nclassifier\n);\n// train the multiclass model.\nOneVsRestModel\novrModel\n=\novr\n.\nfit\n(\ntrain\n);\n// score the model on test data.\nDataset\n<\nRow\n>\npredictions\n=\novrModel\n.\ntransform\n(\ntest\n)\n.\nselect\n(\n\"pred", "question": "What is the format used to load the input data?", "answers": {"text": ["\"libsvm\""], "answer_start": [85]}}
{"context": "ly of simple\nprobabilistic, multiclass classifiers based on applying Bayes’ theorem with strong (naive) independence\nassumptions between every pair of features.\nNaive Bayes can be trained very efficiently. With a single pass over the training data,\nit computes the conditional probability distribution of each feature given each label.\nFor prediction, it applies Bayes’ theorem to compute the conditional probability distribution\nof each label given an observation.\nMLlib supports\nMultinomial naive Bayes\n,\nComplement naive Bayes\n,\nBernoulli naive Bayes\nand\nGaussian naive Bayes\n.\nInput data\n:\nThese Multinomial, Complement and Bernoulli models are typically used for\ndocument classification\n.\nWithin that context, each observation is a document and each feature represents a term.\nA feature’s value ", "question": "What type of data are Multinomial, Complement and Bernoulli models typically used for?", "answers": {"text": ["document classification"], "answer_start": [668]}}
{"context": "ypically used for\ndocument classification\n.\nWithin that context, each observation is a document and each feature represents a term.\nA feature’s value is the frequency of the term (in Multinomial or Complement Naive Bayes) or\na zero or one indicating whether the term was found in the document (in Bernoulli Naive Bayes).\nFeature values for Multinomial and Bernoulli models must be\nnon-negative\n. The model type is selected with an optional parameter\n“multinomial”, “complement”, “bernoulli” or “gaussian”, with “multinomial” as the default.\nFor document classification, the input feature vectors should usually be sparse vectors.\nSince the training data is only used once, it is not necessary to cache it.\nAdditive smoothing\ncan be used by\nsetting the parameter $\\lambda$ (default to $1.0$).\nExamples", "question": "What are the possible model types that can be selected for document classification?", "answers": {"text": ["“multinomial”, “complement”, “bernoulli” or “gaussian”, with “multinomial” as the default."], "answer_start": [450]}}
{"context": "ata is only used once, it is not necessary to cache it.\nAdditive smoothing\ncan be used by\nsetting the parameter $\\lambda$ (default to $1.0$).\nExamples\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml.classification\nimport\nNaiveBayes\nfrom\npyspark.ml.evaluation\nimport\nMulticlassClassificationEvaluator\n# Load training data\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n)\n\\\n.\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\n# Split the data into train and test\nsplits\n=\ndata\n.\nrandomSplit\n([\n0.6\n,\n0.4\n],\n1234\n)\ntrain\n=\nsplits\n[\n0\n]\ntest\n=\nsplits\n[\n1\n]\n# create the trainer and set its parameters\nnb\n=\nNaiveBayes\n(\nsmoothing\n=\n1.0\n,\nmodelType\n=\n\"\nmultinomial\n\"\n)\n# train the model\nmodel\n=\nnb\n.\nfit\n(\ntrain\n)\n# select example rows to display.\npredictions\n=\nmodel\n.\ntransform\n(\ntest\n)\nprediction", "question": "What is the default value for the smoothing parameter λ?", "answers": {"text": ["default to $1.0$)"], "answer_start": [123]}}
{"context": "icationEvaluator\n()\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"accuracy\"\n)\nval\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprintln\n(\ns\n\"Test set accuracy = $accuracy\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.ml.classification.NaiveBayes\n;\nimport\norg.apache.spark.ml.classification.NaiveBayesModel\n;\nimport\norg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Load training data\nDataset\n<\nRow\n>\ndataFrame\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_li", "question": "Where can I find a full example code for NaiveBayes?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\" in the Spark repo."], "answer_start": [211]}}
{"context": "g.apache.spark.sql.SparkSession\n;\n// Load training data\nDataset\n<\nRow\n>\ndataFrame\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n);\n// Split the data into train and test\nDataset\n<\nRow\n>[]\nsplits\n=\ndataFrame\n.\nrandomSplit\n(\nnew\ndouble\n[]{\n0.6\n,\n0.4\n},\n1234L\n);\nDataset\n<\nRow\n>\ntrain\n=\nsplits\n[\n0\n];\nDataset\n<\nRow\n>\ntest\n=\nsplits\n[\n1\n];\n// create the trainer and set its parameters\nNaiveBayes\nnb\n=\nnew\nNaiveBayes\n();\n// train the model\nNaiveBayesModel\nmodel\n=\nnb\n.\nfit\n(\ntrain\n);\n// Select example rows to display.\nDataset\n<\nRow\n>\npredictions\n=\nmodel\n.\ntransform\n(\ntest\n);\npredictions\n.\nshow\n();\n// compute accuracy on the test set\nMulticlassClassificationEvaluator\nevaluator\n=\nnew\nMulticlassClassificationEvaluator\n()\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetPredictionCol\n(\n", "question": "What is used to compute accuracy on the test set?", "answers": {"text": ["MulticlassClassificationEvaluator"], "answer_start": [666]}}
{"context": "\n<-\ntitanicDF\nnbTestDF\n<-\ntitanicDF\nnbModel\n<-\nspark.naiveBayes\n(\nnbDF\n,\nSurvived\n~\nClass\n+\nSex\n+\nAge\n)\n# Model summary\nsummary\n(\nnbModel\n)\n# Prediction\nnbPredictions\n<-\npredict\n(\nnbModel\n,\nnbTestDF\n)\nhead\n(\nnbPredictions\n)\nFind full example code at \"examples/src/main/r/ml/naiveBayes.R\" in the Spark repo.\nFactorization machines classifier\nFor more background and more details about the implementation of factorization machines,\nrefer to the\nFactorization Machines section\n.\nExamples\nThe following examples load a dataset in LibSVM format, split it into training and test sets,\ntrain on the first dataset, and then evaluate on the held-out test set.\nWe scale features to be between 0 and 1 to prevent the exploding gradient problem.\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml\nimp", "question": "Where can I find the full example code for naiveBayes?", "answers": {"text": ["Find full example code at \"examples/src/main/r/ml/naiveBayes.R\" in the Spark repo."], "answer_start": [224]}}
{"context": "\nWe scale features to be between 0 and 1 to prevent the exploding gradient problem.\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml\nimport\nPipeline\nfrom\npyspark.ml.classification\nimport\nFMClassifier\nfrom\npyspark.ml.feature\nimport\nMinMaxScaler\n,\nStringIndexer\nfrom\npyspark.ml.evaluation\nimport\nMulticlassClassificationEvaluator\n# Load and parse the data file, converting it to a DataFrame.\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer\n=\nStringIndexer\n(\ninputCol\n=\n\"\nlabel\n\"\n,\noutputCol\n=\n\"\nindexedLabel\n\"\n).\nfit\n(\ndata\n)\n# Scale features.\nfeatureScaler\n=\nMinMaxScaler\n(\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nscaledFe", "question": "What is the purpose of scaling features?", "answers": {"text": ["We scale features to be between 0 and 1 to prevent the exploding gradient problem."], "answer_start": [1]}}
{"context": "elect example rows to display.\npredictions\n.\nselect\n(\n\"\nprediction\n\"\n,\n\"\nindexedLabel\n\"\n,\n\"\nfeatures\n\"\n).\nshow\n(\n5\n)\n# Select (prediction, true label) and compute test accuracy\nevaluator\n=\nMulticlassClassificationEvaluator\n(\nlabelCol\n=\n\"\nindexedLabel\n\"\n,\npredictionCol\n=\n\"\nprediction\n\"\n,\nmetricName\n=\n\"\naccuracy\n\"\n)\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprint\n(\n\"\nTest set accuracy = %g\n\"\n%\naccuracy\n)\nfmModel\n=\nmodel\n.\nstages\n[\n2\n]\nprint\n(\n\"\nFactors:\n\"\n+\nstr\n(\nfmModel\n.\nfactors\n))\n# type: ignore\nprint\n(\n\"\nLinear:\n\"\n+\nstr\n(\nfmModel\n.\nlinear\n))\n# type: ignore\nprint\n(\n\"\nIntercept:\n\"\n+\nstr\n(\nfmModel\n.\nintercept\n))\n# type: ignore\nFind full example code at \"examples/src/main/python/ml/fm_classifier_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apa", "question": "What is used to compute test accuracy?", "answers": {"text": ["MulticlassClassificationEvaluator"], "answer_start": [189]}}
{"context": "example code at \"examples/src/main/python/ml/fm_classifier_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.Pipeline\nimport\norg.apache.spark.ml.classification.\n{\nFMClassificationModel\n,\nFMClassifier\n}\nimport\norg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport\norg.apache.spark.ml.feature.\n{\nIndexToString\n,\nMinMaxScaler\n,\nStringIndexer\n}\n// Load and parse the data file, converting it to a DataFrame.\nval\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Index labels, adding metadata to the label column.\n// Fit on whole dataset to include all labels in index.\nval\nlabelIndexer\n=\nnew\nStringIndexer\n()\n.\nsetInputCol\n(\n\"label\"\n)\n.\nsetOutputCol\n(\n\"indexedLabel\"\n)\n.\nfit\n(\ndata\n)\n// Scal", "question": "Which format is used to load the data file?", "answers": {"text": ["libsvm"], "answer_start": [512]}}
{"context": "dexedLabel\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"accuracy\"\n)\nval\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprintln\n(\ns\n\"Test set accuracy = $accuracy\"\n)\nval\nfmModel\n=\nmodel\n.\nstages\n(\n2\n).\nasInstanceOf\n[\nFMClassificationModel\n]\nprintln\n(\ns\n\"Factors: ${fmModel.factors} Linear: ${fmModel.linear} \"\n+\ns\n\"Intercept: ${fmModel.intercept}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/FMClassifierExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.ml.Pipeline\n;\nimport\norg.apache.spark.ml.PipelineModel\n;\nimport\norg.apache.spark.ml.PipelineStage\n;\nimport\norg.apache.spark.ml.classification.FMClassificationModel\n;\nimport\norg.apache.spark.ml.classification.FMClassifier\n;\nimport\norg.apache.s", "question": "Where can I find a full example code for FMClassifier?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/FMClassifierExample.scala\" in the Spark repo."], "answer_start": [363]}}
{"context": "Stage\n;\nimport\norg.apache.spark.ml.classification.FMClassificationModel\n;\nimport\norg.apache.spark.ml.classification.FMClassifier\n;\nimport\norg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n;\nimport\norg.apache.spark.ml.feature.*\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Load and parse the data file, converting it to a DataFrame.\nDataset\n<\nRow\n>\ndata\n=\nspark\n.\nread\n()\n.\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n);\n// Index labels, adding metadata to the label column.\n// Fit on whole dataset to include all labels in index.\nStringIndexerModel\nlabelIndexer\n=\nnew\nStringIndexer\n()\n.\nsetInputCol\n(\n\"label\"\n)\n.\nsetOutputCol\n(\n\"indexedLabel\"\n)\n.\nfit\n(\ndata\n);\n// Scale features.\nMinMaxSca", "question": "Which format is used to load the data file?", "answers": {"text": ["libsvm"], "answer_start": [473]}}
{"context": "epSize\n(\n0.001\n);\n// Convert indexed labels back to original labels.\nIndexToString\nlabelConverter\n=\nnew\nIndexToString\n()\n.\nsetInputCol\n(\n\"prediction\"\n)\n.\nsetOutputCol\n(\n\"predictedLabel\"\n)\n.\nsetLabels\n(\nlabelIndexer\n.\nlabelsArray\n()[\n0\n]);\n// Create a Pipeline.\nPipeline\npipeline\n=\nnew\nPipeline\n()\n.\nsetStages\n(\nnew\nPipelineStage\n[]\n{\nlabelIndexer\n,\nfeatureScaler\n,\nfm\n,\nlabelConverter\n});\n// Train model.\nPipelineModel\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n);\n// Make predictions.\nDataset\n<\nRow\n>\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n);\n// Select example rows to display.\npredictions\n.\nselect\n(\n\"predictedLabel\"\n,\n\"label\"\n,\n\"features\"\n).\nshow\n(\n5\n);\n// Select (prediction, true label) and compute test accuracy.\nMulticlassClassificationEvaluator\nevaluator\n=\nnew\nMulticlassClassificationEvaluato", "question": "What is set as the input column for the IndexToString converter?", "answers": {"text": ["\"prediction\""], "answer_start": [137]}}
{"context": "\n5\n);\n// Select (prediction, true label) and compute test accuracy.\nMulticlassClassificationEvaluator\nevaluator\n=\nnew\nMulticlassClassificationEvaluator\n()\n.\nsetLabelCol\n(\n\"indexedLabel\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"accuracy\"\n);\ndouble\naccuracy\n=\nevaluator\n.\nevaluate\n(\npredictions\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Test Accuracy = \"\n+\naccuracy\n);\nFMClassificationModel\nfmModel\n=\n(\nFMClassificationModel\n)(\nmodel\n.\nstages\n()[\n2\n]);\nSystem\n.\nout\n.\nprintln\n(\n\"Factors: \"\n+\nfmModel\n.\nfactors\n());\nSystem\n.\nout\n.\nprintln\n(\n\"Linear: \"\n+\nfmModel\n.\nlinear\n());\nSystem\n.\nout\n.\nprintln\n(\n\"Intercept: \"\n+\nfmModel\n.\nintercept\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaFMClassifierExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor mo", "question": "Where can I find the full example code for this JavaFMClassifier?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaFMClassifierExample.java\" in the Spark repo."], "answer_start": [642]}}
{"context": "l example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaFMClassifierExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\nNote: At the moment SparkR doesn’t support feature scaling.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_libsvm_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit a FM classification model\nmodel\n<-\nspark.fmClassifier\n(\ntraining\n,\nlabel\n~\nfeatures\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\nFind full example code at \"examples/src/main/r/ml/fmClassifier.R\" in the Spark repo.\nRegression\nLinear regression\nThe interface for working with linear regression models and model\nsummaries is similar to the logistic regression case.\nWhen fitting LinearRegr", "question": "Where can I find the full example code for FM classification in Spark?", "answers": {"text": ["Find full example code at \"examples/src/main/r/ml/fmClassifier.R\" in the Spark repo."], "answer_start": [543]}}
{"context": "ession\nThe interface for working with linear regression models and model\nsummaries is similar to the logistic regression case.\nWhen fitting LinearRegressionModel without intercept on dataset with constant nonzero column by “l-bfgs” solver, Spark MLlib outputs zero coefficients for constant nonzero columns. This behavior is the same as R glmnet but different from LIBSVM.\nExamples\nThe following\nexample demonstrates training an elastic net regularized linear\nregression model and extracting model summary statistics.\nMore details on parameters can be found in the\nPython API documentation\n.\nfrom\npyspark.ml.regression\nimport\nLinearRegression\n# Load training data\ntraining\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n)\n\\\n.\nload\n(\n\"\ndata/mllib/sample_linear_regression_data.txt\n\"\n)\nlr\n=\nLinearRegression\n(\nma", "question": "What is the behavior of Spark MLlib when fitting LinearRegressionModel without intercept on a dataset with a constant nonzero column using the “l-bfgs” solver?", "answers": {"text": ["When fitting LinearRegressionModel without intercept on dataset with constant nonzero column by “l-bfgs” solver, Spark MLlib outputs zero coefficients for constant nonzero columns."], "answer_start": [127]}}
{"context": "rainingSummary\n.\nobjectiveHistory\n))\ntrainingSummary\n.\nresiduals\n.\nshow\n()\nprint\n(\n\"\nRMSE: %f\n\"\n%\ntrainingSummary\n.\nrootMeanSquaredError\n)\nprint\n(\n\"\nr2: %f\n\"\n%\ntrainingSummary\n.\nr2\n)\nFind full example code at \"examples/src/main/python/ml/linear_regression_with_elastic_net.py\" in the Spark repo.\nMore details on parameters can be found in the\nScala API documentation\n.\nimport\norg.apache.spark.ml.regression.LinearRegression\n// Load training data\nval\ntraining\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_linear_regression_data.txt\"\n)\nval\nlr\n=\nnew\nLinearRegression\n()\n.\nsetMaxIter\n(\n10\n)\n.\nsetRegParam\n(\n0.3\n)\n.\nsetElasticNetParam\n(\n0.8\n)\n// Fit the model\nval\nlrModel\n=\nlr\n.\nfit\n(\ntraining\n)\n// Print the coefficients and intercept for linear regression\nprintln\n(\ns\n\"Coefficients: ", "question": "Where can I find a full example code for linear regression with elastic net?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/linear_regression_with_elastic_net.py\" in the Spark repo."], "answer_start": [183]}}
{"context": "es/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\" in the Spark repo.\nMore details on parameters can be found in the\nJava API documentation\n.\nimport\norg.apache.spark.ml.regression.LinearRegression\n;\nimport\norg.apache.spark.ml.regression.LinearRegressionModel\n;\nimport\norg.apache.spark.ml.regression.LinearRegressionTrainingSummary\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Load training data.\nDataset\n<\nRow\n>\ntraining\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_linear_regression_data.txt\"\n);\nLinearRegression\nlr\n=\nnew\nLinearRegression\n()\n.\nsetMaxIter\n(\n10\n)\n.\nsetRegParam\n(\n0.3\n)\n.\nsetElasticNetParam\n(\n0.8\n);\n// ", "question": "Where can more details on parameters be found?", "answers": {"text": ["More details on parameters can be found in the\nJava API documentation\n."], "answer_start": [111]}}
{"context": "\n\"objectiveHistory: \"\n+\nVectors\n.\ndense\n(\ntrainingSummary\n.\nobjectiveHistory\n()));\ntrainingSummary\n.\nresiduals\n().\nshow\n();\nSystem\n.\nout\n.\nprintln\n(\n\"RMSE: \"\n+\ntrainingSummary\n.\nrootMeanSquaredError\n());\nSystem\n.\nout\n.\nprintln\n(\n\"r2: \"\n+\ntrainingSummary\n.\nr2\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\" in the Spark repo.\nMore details on parameters can be found in the\nR API documentation\n.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_linear_regression_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit a linear regression model\nmodel\n<-\nspark.lm\n(\ntraining\n,\nlabel\n~\nfeatures\n,\nregParam\n=\n0.3\n,\nelasticNetParam\n=\n0.8\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredic", "question": "Where can I find a full example code for JavaLinearRegressionWithElasticNet?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\" in the Spark repo."], "answer_start": [264]}}
{"context": "-\nspark.lm\n(\ntraining\n,\nlabel\n~\nfeatures\n,\nregParam\n=\n0.3\n,\nelasticNetParam\n=\n0.8\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\n# Summarize\nsummary\n(\nmodel\n)\nFind full example code at \"examples/src/main/r/ml/lm_with_elastic_net.R\" in the Spark repo.\nGeneralized linear regression\nContrasted with linear regression where the output is assumed to follow a Gaussian\ndistribution,\ngeneralized linear models\n(GLMs) are specifications of linear models where the response variable $Y_i$ follows some\ndistribution from the\nexponential family of distributions\n.\nSpark’s\nGeneralizedLinearRegression\ninterface\nallows for flexible specification of GLMs which can be used for various types of\nprediction problems including linear regression, Poisson regression, logistic regression, ", "question": "What type of prediction problems can Spark’s GeneralizedLinearRegression interface be used for?", "answers": {"text": ["linear regression, Poisson regression, logistic regression"], "answer_start": [740]}}
{"context": "s.\nSee here\nfor a more comprehensive review of GLMs and their applications.\nAvailable families\nFamily\nResponse Type\nSupported Links\nGaussian\nContinuous\nIdentity*, Log, Inverse\nBinomial\nBinary\nLogit*, Probit, CLogLog\nPoisson\nCount\nLog*, Identity, Sqrt\nGamma\nContinuous\nInverse*, Identity, Log\nTweedie\nZero-inflated continuous\nPower link function\n* Canonical Link\nExamples\nThe following example demonstrates training a GLM with a Gaussian response and identity link\nfunction and extracting model summary statistics.\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml.regression\nimport\nGeneralizedLinearRegression\n# Load training data\ndataset\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n)\n\\\n.\nload\n(\n\"\ndata/mllib/sample_linear_regression_data.txt\n\"\n)\nglr\n=\nGeneralizedLinearRegression\n(\nfamily\n=\n\"\n", "question": "What response type is supported by the Gaussian family in GLMs?", "answers": {"text": ["Continuous"], "answer_start": [141]}}
{"context": "alues:\n\"\n+\nstr\n(\nsummary\n.\ntValues\n))\nprint\n(\n\"\nP Values:\n\"\n+\nstr\n(\nsummary\n.\npValues\n))\nprint\n(\n\"\nDispersion:\n\"\n+\nstr\n(\nsummary\n.\ndispersion\n))\nprint\n(\n\"\nNull Deviance:\n\"\n+\nstr\n(\nsummary\n.\nnullDeviance\n))\nprint\n(\n\"\nResidual Degree Of Freedom Null:\n\"\n+\nstr\n(\nsummary\n.\nresidualDegreeOfFreedomNull\n))\nprint\n(\n\"\nDeviance:\n\"\n+\nstr\n(\nsummary\n.\ndeviance\n))\nprint\n(\n\"\nResidual Degree Of Freedom:\n\"\n+\nstr\n(\nsummary\n.\nresidualDegreeOfFreedom\n))\nprint\n(\n\"\nAIC:\n\"\n+\nstr\n(\nsummary\n.\naic\n))\nprint\n(\n\"\nDeviance Residuals:\n\"\n)\nsummary\n.\nresiduals\n().\nshow\n()\nFind full example code at \"examples/src/main/python/ml/generalized_linear_regression_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.regression.GeneralizedLinearRegression\n// Load training data\nval\nd", "question": "Where can I find a full example code for this?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/generalized_linear_regression_example.py\" in the Spark repo."], "answer_start": [545]}}
{"context": "tln\n(\n\"Deviance Residuals: \"\n)\nsummary\n.\nresiduals\n().\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\njava.util.Arrays\n;\nimport\norg.apache.spark.ml.regression.GeneralizedLinearRegression\n;\nimport\norg.apache.spark.ml.regression.GeneralizedLinearRegressionModel\n;\nimport\norg.apache.spark.ml.regression.GeneralizedLinearRegressionTrainingSummary\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\n// Load training data\nDataset\n<\nRow\n>\ndataset\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_linear_regression_data.txt\"\n);\nGeneralizedLinearRegression\nglr\n=\nnew\nGeneralizedLinearRegression\n()\n.\nsetFamily\n(\n\"g", "question": "Where can I find a full example code for GeneralizedLinearRegression?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\" in the Spark repo."], "answer_start": [63]}}
{"context": "ual Degree Of Freedom: \"\n+\nsummary\n.\nresidualDegreeOfFreedom\n());\nSystem\n.\nout\n.\nprintln\n(\n\"AIC: \"\n+\nsummary\n.\naic\n());\nSystem\n.\nout\n.\nprintln\n(\n\"Deviance Residuals: \"\n);\nsummary\n.\nresiduals\n().\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\ntraining\n<-\nread.df\n(\n\"data/mllib/sample_multiclass_classification_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\n# Fit a generalized linear model of family \"gaussian\" with spark.glm\ndf_list\n<-\nrandomSplit\n(\ntraining\n,\nc\n(\n7\n,\n3\n),\n2\n)\ngaussianDF\n<-\ndf_list\n[[\n1\n]]\ngaussianTestDF\n<-\ndf_list\n[[\n2\n]]\ngaussianGLM\n<-\nspark.glm\n(\ngaussianDF\n,\nlabel\n~\nfeatures\n,\nfamily\n=\n\"gaussian\"\n)\n# Model summary\nsummary\n(\ngaussianGLM\n)\n# Pre", "question": "Where can I find a full example code for JavaGeneralizedLinearRegression?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\" in the Spark repo."], "answer_start": [204]}}
{"context": "eDF\n,\nlabel\n~\nfeatures\n,\nfamily\n=\n\"tweedie\"\n,\nvar.power\n=\n1.2\n,\nlink.power\n=\n0\n)\n# Model summary\nsummary\n(\ntweedieGLM\n)\nFind full example code at \"examples/src/main/r/ml/glm.R\" in the Spark repo.\nDecision tree regression\nDecision trees are a popular family of classification and regression methods.\nMore information about the\nspark.ml\nimplementation can be found further in the\nsection on decision trees\n.\nExamples\nThe following examples load a dataset in LibSVM format, split it into training and test sets, train on the first dataset, and then evaluate on the held-out test set.\nWe use a feature transformer to index categorical features, adding metadata to the\nDataFrame\nwhich the Decision Tree algorithm can recognize.\nMore details on parameters can be found in the\nPython API documentation\n.\nfro", "question": "Where can full example code for tweedieGLM be found?", "answers": {"text": ["Find full example code at \"examples/src/main/r/ml/glm.R\" in the Spark repo."], "answer_start": [120]}}
{"context": "tadata to the\nDataFrame\nwhich the Decision Tree algorithm can recognize.\nMore details on parameters can be found in the\nPython API documentation\n.\nfrom\npyspark.ml\nimport\nPipeline\nfrom\npyspark.ml.regression\nimport\nDecisionTreeRegressor\nfrom\npyspark.ml.feature\nimport\nVectorIndexer\nfrom\npyspark.ml.evaluation\nimport\nRegressionEvaluator\n# Load the data stored in LIBSVM format as a DataFrame.\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\n# Automatically identify categorical features, and index them.\n# We specify maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer\n=\n\\\nVectorIndexer\n(\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nindexedFeatures\n\"\n,\nmaxCategories\n=\n4\n).\nfit\n(\ndata\n)\n# Split the data into training and ", "question": "What is the purpose of `maxCategories` in the `VectorIndexer`?", "answers": {"text": ["We specify maxCategories so features with > 4 distinct values are treated as continuous."], "answer_start": [547]}}
{"context": ".ml.evaluation.RegressionEvaluator\nimport\norg.apache.spark.ml.feature.VectorIndexer\nimport\norg.apache.spark.ml.regression.DecisionTreeRegressionModel\nimport\norg.apache.spark.ml.regression.DecisionTreeRegressor\n// Load the data stored in LIBSVM format as a DataFrame.\nval\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Automatically identify categorical features, and index them.\n// Here, we treat features with > 4 distinct values as continuous.\nval\nfeatureIndexer\n=\nnew\nVectorIndexer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxCategories\n(\n4\n)\n.\nfit\n(\ndata\n)\n// Split the data into training and test sets (30% held out for testing).\nval\nArray\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.7\n,\n0.3\n))\n// Tra", "question": "What format is the data loaded in?", "answers": {"text": ["libsvm"], "answer_start": [303]}}
{"context": " data into training and test sets (30% held out for testing).\nval\nArray\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.7\n,\n0.3\n))\n// Train a DecisionTree model.\nval\ndt\n=\nnew\nDecisionTreeRegressor\n()\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetFeaturesCol\n(\n\"indexedFeatures\"\n)\n// Chain indexer and tree in a Pipeline.\nval\npipeline\n=\nnew\nPipeline\n()\n.\nsetStages\n(\nArray\n(\nfeatureIndexer\n,\ndt\n))\n// Train model. This also runs the indexer.\nval\nmodel\n=\npipeline\n.\nfit\n(\ntrainingData\n)\n// Make predictions.\nval\npredictions\n=\nmodel\n.\ntransform\n(\ntestData\n)\n// Select example rows to display.\npredictions\n.\nselect\n(\n\"prediction\"\n,\n\"label\"\n,\n\"features\"\n).\nshow\n(\n5\n)\n// Select (prediction, true label) and compute test error.\nval\nevaluator\n=\nnew\nRegressionEvaluator\n()\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetPredic", "question": "What is done with the data after it is split into training and test sets?", "answers": {"text": ["data into training and test sets (30% held out for testing)."], "answer_start": [1]}}
{"context": "show\n(\n5\n)\n// Select (prediction, true label) and compute test error.\nval\nevaluator\n=\nnew\nRegressionEvaluator\n()\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"rmse\"\n)\nval\nrmse\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprintln\n(\ns\n\"Root Mean Squared Error (RMSE) on test data = $rmse\"\n)\nval\ntreeModel\n=\nmodel\n.\nstages\n(\n1\n).\nasInstanceOf\n[\nDecisionTreeRegressionModel\n]\nprintln\n(\ns\n\"Learned regression tree model:\\n ${treeModel.toDebugString}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\" in the Spark repo.\nMore details on parameters can be found in the\nJava API documentation\n.\nimport\norg.apache.spark.ml.Pipeline\n;\nimport\norg.apache.spark.ml.PipelineModel\n;\nimport\norg.apache.spark.ml.Pipelin", "question": "What metric is used to evaluate the regression model?", "answers": {"text": ["rmse"], "answer_start": [194]}}
{"context": "nd in the\nJava API documentation\n.\nimport\norg.apache.spark.ml.Pipeline\n;\nimport\norg.apache.spark.ml.PipelineModel\n;\nimport\norg.apache.spark.ml.PipelineStage\n;\nimport\norg.apache.spark.ml.evaluation.RegressionEvaluator\n;\nimport\norg.apache.spark.ml.feature.VectorIndexer\n;\nimport\norg.apache.spark.ml.feature.VectorIndexerModel\n;\nimport\norg.apache.spark.ml.regression.DecisionTreeRegressionModel\n;\nimport\norg.apache.spark.ml.regression.DecisionTreeRegressor\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Load the data stored in LIBSVM format as a DataFrame.\nDataset\n<\nRow\n>\ndata\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n);\n// Automatically identify categorical features, and index them.\n", "question": "Which format is used to load the data as a DataFrame?", "answers": {"text": ["libsvm"], "answer_start": [678]}}
{"context": "abelCol\n(\n\"label\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"rmse\"\n);\ndouble\nrmse\n=\nevaluator\n.\nevaluate\n(\npredictions\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Root Mean Squared Error (RMSE) on test data = \"\n+\nrmse\n);\nDecisionTreeRegressionModel\ntreeModel\n=\n(\nDecisionTreeRegressionModel\n)\n(\nmodel\n.\nstages\n()[\n1\n]);\nSystem\n.\nout\n.\nprintln\n(\n\"Learned regression tree model:\\n\"\n+\ntreeModel\n.\ntoDebugString\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_linear_regression_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit a DecisionTree regression model with spark.decisionTree\nmodel\n<-\nspark", "question": "Where can I find the full example code for this Decision Tree Regression?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\" in the Spark repo."], "answer_start": [412]}}
{"context": "near_regression_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit a DecisionTree regression model with spark.decisionTree\nmodel\n<-\nspark.decisionTree\n(\ntraining\n,\nlabel\n~\nfeatures\n,\n\"regression\"\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\nFind full example code at \"examples/src/main/r/ml/decisionTree.R\" in the Spark repo.\nRandom forest regression\nRandom forests are a popular family of classification and regression methods.\nMore information about the\nspark.ml\nimplementation can be found further in the\nsection on random forests\n.\nExamples\nThe following examples load a dataset in LibSVM format, split it into training and test sets, train on the first dataset, and then evaluate on the held-out test set.\nWe use a fe", "question": "What format is the dataset loaded in the examples?", "answers": {"text": ["LibSVM format"], "answer_start": [664]}}
{"context": " a dataset in LibSVM format, split it into training and test sets, train on the first dataset, and then evaluate on the held-out test set.\nWe use a feature transformer to index categorical features, adding metadata to the\nDataFrame\nwhich the tree-based algorithms can recognize.\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml\nimport\nPipeline\nfrom\npyspark.ml.regression\nimport\nRandomForestRegressor\nfrom\npyspark.ml.feature\nimport\nVectorIndexer\nfrom\npyspark.ml.evaluation\nimport\nRegressionEvaluator\n# Load and parse the data file, converting it to a DataFrame.\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\n# Automatically identify categorical features, and index them.\n# Set maxCategories so features with > 4 distinct values are treated a", "question": "What format is the dataset expected to be in?", "answers": {"text": ["a dataset in LibSVM format"], "answer_start": [1]}}
{"context": "t_regressor_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.Pipeline\nimport\norg.apache.spark.ml.evaluation.RegressionEvaluator\nimport\norg.apache.spark.ml.feature.VectorIndexer\nimport\norg.apache.spark.ml.regression.\n{\nRandomForestRegressionModel\n,\nRandomForestRegressor\n}\n// Load and parse the data file, converting it to a DataFrame.\nval\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Automatically identify categorical features, and index them.\n// Set maxCategories so features with > 4 distinct values are treated as continuous.\nval\nfeatureIndexer\n=\nnew\nVectorIndexer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxCategories\n(\n4\n)\n.\nfit\n(\ndata\n)\n// Split the data int", "question": "What is the purpose of setting `setMaxCategories` to 4 in the `VectorIndexer`?", "answers": {"text": ["Set maxCategories so features with > 4 distinct values are treated as continuous."], "answer_start": [549]}}
{"context": "les/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.ml.Pipeline\n;\nimport\norg.apache.spark.ml.PipelineModel\n;\nimport\norg.apache.spark.ml.PipelineStage\n;\nimport\norg.apache.spark.ml.evaluation.RegressionEvaluator\n;\nimport\norg.apache.spark.ml.feature.VectorIndexer\n;\nimport\norg.apache.spark.ml.feature.VectorIndexerModel\n;\nimport\norg.apache.spark.ml.regression.RandomForestRegressionModel\n;\nimport\norg.apache.spark.ml.regression.RandomForestRegressor\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Load and parse the data file, converting it to a DataFrame.\nDataset\n<\nRow\n>\ndata\n=\nspark\n.\nread\n().\nformat\n(\n\"lib", "question": "Which API documentation should be referred to for more details?", "answers": {"text": ["Java API docs"], "answer_start": [116]}}
{"context": "g.apache.spark.sql.SparkSession\n;\n// Load and parse the data file, converting it to a DataFrame.\nDataset\n<\nRow\n>\ndata\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n);\n// Automatically identify categorical features, and index them.\n// Set maxCategories so features with > 4 distinct values are treated as continuous.\nVectorIndexerModel\nfeatureIndexer\n=\nnew\nVectorIndexer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxCategories\n(\n4\n)\n.\nfit\n(\ndata\n);\n// Split the data into training and test sets (30% held out for testing)\nDataset\n<\nRow\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]\n{\n0.7\n,\n0.3\n});\nDataset\n<\nRow\n>\ntrainingData\n=\nsplits\n[\n0\n];\nDataset\n<\nRow\n>\ntestData\n=\nsplits\n[\n1\n];\n// Train a RandomForest model.\nRandomForestReg", "question": "What percentage of the data is held out for testing?", "answers": {"text": ["30% held out for testing"], "answer_start": [564]}}
{"context": "\n\"label\"\n,\n\"features\"\n).\nshow\n(\n5\n);\n// Select (prediction, true label) and compute test error\nRegressionEvaluator\nevaluator\n=\nnew\nRegressionEvaluator\n()\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"rmse\"\n);\ndouble\nrmse\n=\nevaluator\n.\nevaluate\n(\npredictions\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Root Mean Squared Error (RMSE) on test data = \"\n+\nrmse\n);\nRandomForestRegressionModel\nrfModel\n=\n(\nRandomForestRegressionModel\n)(\nmodel\n.\nstages\n()[\n1\n]);\nSystem\n.\nout\n.\nprintln\n(\n\"Learned regression forest model:\\n\"\n+\nrfModel\n.\ntoDebugString\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sam", "question": "What metric is used to evaluate the regression model?", "answers": {"text": ["rmse"], "answer_start": [235]}}
{"context": "vaRandomForestRegressorExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_linear_regression_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit a random forest regression model with spark.randomForest\nmodel\n<-\nspark.randomForest\n(\ntraining\n,\nlabel\n~\nfeatures\n,\n\"regression\"\n,\nnumTrees\n=\n10\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\nFind full example code at \"examples/src/main/r/ml/randomForest.R\" in the Spark repo.\nGradient-boosted tree regression\nGradient-boosted trees (GBTs) are a popular regression method using ensembles of decision trees.\nMore information about the\nspark.ml\nimplementation can be found further in the\nsection on GBT", "question": "Where can I find the full example code for random forest regression?", "answers": {"text": ["Find full example code at \"examples/src/main/r/ml/randomForest.R\" in the Spark repo."], "answer_start": [492]}}
{"context": "lar regression method using ensembles of decision trees.\nMore information about the\nspark.ml\nimplementation can be found further in the\nsection on GBTs\n.\nExamples\nNote: For this example dataset,\nGBTRegressor\nactually only needs 1 iteration, but that will not\nbe true in general.\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml\nimport\nPipeline\nfrom\npyspark.ml.regression\nimport\nGBTRegressor\nfrom\npyspark.ml.feature\nimport\nVectorIndexer\nfrom\npyspark.ml.evaluation\nimport\nRegressionEvaluator\n# Load and parse the data file, converting it to a DataFrame.\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\n# Automatically identify categorical features, and index them.\n# Set maxCategories so features with > 4 distinct values are treated as continu", "question": "Where can more information about the spark.ml implementation be found?", "answers": {"text": ["GBTs"], "answer_start": [147]}}
{"context": "egressor_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.Pipeline\nimport\norg.apache.spark.ml.evaluation.RegressionEvaluator\nimport\norg.apache.spark.ml.feature.VectorIndexer\nimport\norg.apache.spark.ml.regression.\n{\nGBTRegressionModel\n,\nGBTRegressor\n}\n// Load and parse the data file, converting it to a DataFrame.\nval\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Automatically identify categorical features, and index them.\n// Set maxCategories so features with > 4 distinct values are treated as continuous.\nval\nfeatureIndexer\n=\nnew\nVectorIndexer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxCategories\n(\n4\n)\n.\nfit\n(\ndata\n)\n// Split the data into training and test s", "question": "What is the purpose of setting `setMaxCategories` to 4 in the `VectorIndexer`?", "answers": {"text": ["Set maxCategories so features with > 4 distinct values are treated as continuous."], "answer_start": [528]}}
{"context": "\n.\ntransform\n(\ntestData\n)\n// Select example rows to display.\npredictions\n.\nselect\n(\n\"prediction\"\n,\n\"label\"\n,\n\"features\"\n).\nshow\n(\n5\n)\n// Select (prediction, true label) and compute test error.\nval\nevaluator\n=\nnew\nRegressionEvaluator\n()\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"rmse\"\n)\nval\nrmse\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprintln\n(\ns\n\"Root Mean Squared Error (RMSE) on test data = $rmse\"\n)\nval\ngbtModel\n=\nmodel\n.\nstages\n(\n1\n).\nasInstanceOf\n[\nGBTRegressionModel\n]\nprintln\n(\ns\n\"Learned regression GBT model:\\n ${gbtModel.toDebugString}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.m", "question": "What metric is used to evaluate the regression model?", "answers": {"text": ["rmse"], "answer_start": [317]}}
{"context": "spark/examples/ml/GradientBoostedTreeRegressorExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.ml.Pipeline\n;\nimport\norg.apache.spark.ml.PipelineModel\n;\nimport\norg.apache.spark.ml.PipelineStage\n;\nimport\norg.apache.spark.ml.evaluation.RegressionEvaluator\n;\nimport\norg.apache.spark.ml.feature.VectorIndexer\n;\nimport\norg.apache.spark.ml.feature.VectorIndexerModel\n;\nimport\norg.apache.spark.ml.regression.GBTRegressionModel\n;\nimport\norg.apache.spark.ml.regression.GBTRegressor\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Load and parse the data file, converting it to a DataFrame.\nDataset\n<\nRow\n>\ndata\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_", "question": "Which format is used to load the data file?", "answers": {"text": ["libsvm"], "answer_start": [756]}}
{"context": " and parse the data file, converting it to a DataFrame.\nDataset\n<\nRow\n>\ndata\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n);\n// Automatically identify categorical features, and index them.\n// Set maxCategories so features with > 4 distinct values are treated as continuous.\nVectorIndexerModel\nfeatureIndexer\n=\nnew\nVectorIndexer\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"indexedFeatures\"\n)\n.\nsetMaxCategories\n(\n4\n)\n.\nfit\n(\ndata\n);\n// Split the data into training and test sets (30% held out for testing).\nDataset\n<\nRow\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]\n{\n0.7\n,\n0.3\n});\nDataset\n<\nRow\n>\ntrainingData\n=\nsplits\n[\n0\n];\nDataset\n<\nRow\n>\ntestData\n=\nsplits\n[\n1\n];\n// Train a GBT model.\nGBTRegressor\ngbt\n=\nnew\nGBTRegressor\n()\n.\nsetLabelCol\n(\n\"label\"\n)", "question": "What percentage of the data is held out for testing?", "answers": {"text": ["30% held out for testing"], "answer_start": [523]}}
{"context": "rtial \\beta}=\\sum_{1=1}^{n}[\\delta_{i}-e^{\\epsilon_{i}}]\\frac{x_{i}}{\\sigma}\n\\]\n\\[\n\\frac{\\partial (-\\iota)}{\\partial (\\log\\sigma)}=\\sum_{i=1}^{n}[\\delta_{i}+(\\delta_{i}-e^{\\epsilon_{i}})\\epsilon_{i}]\n\\]\nThe AFT model can be formulated as a convex optimization problem,\ni.e. the task of finding a minimizer of a convex function $-\\iota(\\beta,\\sigma)$\nthat depends on the coefficients vector $\\beta$ and the log of scale parameter $\\log\\sigma$.\nThe optimization algorithm underlying the implementation is L-BFGS.\nThe implementation matches the result from R’s survival function\nsurvreg\nWhen fitting AFTSurvivalRegressionModel without intercept on dataset with constant nonzero column, Spark MLlib outputs zero coefficients for constant nonzero columns. This behavior is different from R survival::survr", "question": "Which optimization algorithm underlies the implementation of the AFT model?", "answers": {"text": ["The optimization algorithm underlying the implementation is L-BFGS."], "answer_start": [443]}}
{"context": "et with constant nonzero column, Spark MLlib outputs zero coefficients for constant nonzero columns. This behavior is different from R survival::survreg.\nExamples\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml.regression\nimport\nAFTSurvivalRegression\nfrom\npyspark.ml.linalg\nimport\nVectors\ntraining\n=\nspark\n.\ncreateDataFrame\n([\n(\n1.218\n,\n1.0\n,\nVectors\n.\ndense\n(\n1.560\n,\n-\n0.605\n)),\n(\n2.949\n,\n0.0\n,\nVectors\n.\ndense\n(\n0.346\n,\n2.158\n)),\n(\n3.627\n,\n0.0\n,\nVectors\n.\ndense\n(\n1.380\n,\n0.231\n)),\n(\n0.273\n,\n1.0\n,\nVectors\n.\ndense\n(\n0.520\n,\n1.151\n)),\n(\n4.199\n,\n0.0\n,\nVectors\n.\ndense\n(\n0.795\n,\n-\n0.226\n))],\n[\n\"\nlabel\n\"\n,\n\"\ncensor\n\"\n,\n\"\nfeatures\n\"\n])\nquantileProbabilities\n=\n[\n0.3\n,\n0.6\n]\naft\n=\nAFTSurvivalRegression\n(\nquantileProbabilities\n=\nquantileProbabilities\n,\nquantilesCol\n=\n\"\nquantiles\n\"\n)\nmod", "question": "What does Spark MLlib output for constant nonzero columns?", "answers": {"text": ["Spark MLlib outputs zero coefficients for constant nonzero columns."], "answer_start": [33]}}
{"context": "quantileProbabilities\n=\n[\n0.3\n,\n0.6\n]\naft\n=\nAFTSurvivalRegression\n(\nquantileProbabilities\n=\nquantileProbabilities\n,\nquantilesCol\n=\n\"\nquantiles\n\"\n)\nmodel\n=\naft\n.\nfit\n(\ntraining\n)\n# Print the coefficients, intercept and scale parameter for AFT survival regression\nprint\n(\n\"\nCoefficients:\n\"\n+\nstr\n(\nmodel\n.\ncoefficients\n))\nprint\n(\n\"\nIntercept:\n\"\n+\nstr\n(\nmodel\n.\nintercept\n))\nprint\n(\n\"\nScale:\n\"\n+\nstr\n(\nmodel\n.\nscale\n))\nmodel\n.\ntransform\n(\ntraining\n).\nshow\n(\ntruncate\n=\nFalse\n)\nFind full example code at \"examples/src/main/python/ml/aft_survival_regression.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.linalg.Vectors\nimport\norg.apache.spark.ml.regression.AFTSurvivalRegression\nval\ntraining\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n1.218\n,\n1.0\n,\nVectors\n.\nden", "question": "Where can I find the full example code for AFT survival regression?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/aft_survival_regression.py\" in the Spark repo."], "answer_start": [474]}}
{"context": " coefficients, intercept and scale parameter for AFT survival regression\nprintln\n(\ns\n\"Coefficients: ${model.coefficients}\"\n)\nprintln\n(\ns\n\"Intercept: ${model.intercept}\"\n)\nprintln\n(\ns\n\"Scale: ${model.scale}\"\n)\nmodel\n.\ntransform\n(\ntraining\n).\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.regression.AFTSurvivalRegression\n;\nimport\norg.apache.spark.ml.regression.AFTSurvivalRegressionModel\n;\nimport\norg.apache.spark.ml.linalg.VectorUDT\n;\nimport\norg.apache.spark.ml.linalg.Vectors\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark", "question": "Where can I find a full example code for AFTSurvivalRegression?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\" in the Spark repo."], "answer_start": [256]}}
{"context": "tQuantileProbabilities\n(\nquantileProbabilities\n)\n.\nsetQuantilesCol\n(\n\"quantiles\"\n);\nAFTSurvivalRegressionModel\nmodel\n=\naft\n.\nfit\n(\ntraining\n);\n// Print the coefficients, intercept and scale parameter for AFT survival regression\nSystem\n.\nout\n.\nprintln\n(\n\"Coefficients: \"\n+\nmodel\n.\ncoefficients\n());\nSystem\n.\nout\n.\nprintln\n(\n\"Intercept: \"\n+\nmodel\n.\nintercept\n());\nSystem\n.\nout\n.\nprintln\n(\n\"Scale: \"\n+\nmodel\n.\nscale\n());\nmodel\n.\ntransform\n(\ntraining\n).\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# Use the ovarian dataset available in R survival package\nlibrary\n(\nsurvival\n)\n# Fit an accelerated failure time (AFT) survival regression model with spark", "question": "Where can I find a full example code for JavaAFTSurvivalRegressionExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\" in the Spark repo."], "answer_start": [466]}}
{"context": " the ovarian dataset available in R survival package\nlibrary\n(\nsurvival\n)\n# Fit an accelerated failure time (AFT) survival regression model with spark.survreg\novarianDF\n<-\nsuppressWarnings\n(\ncreateDataFrame\n(\novarian\n))\naftDF\n<-\novarianDF\naftTestDF\n<-\novarianDF\naftModel\n<-\nspark.survreg\n(\naftDF\n,\nSurv\n(\nfutime\n,\nfustat\n)\n~\necog_ps\n+\nrx\n)\n# Model summary\nsummary\n(\naftModel\n)\n# Prediction\naftPredictions\n<-\npredict\n(\naftModel\n,\naftTestDF\n)\nhead\n(\naftPredictions\n)\nFind full example code at \"examples/src/main/r/ml/survreg.R\" in the Spark repo.\nIsotonic regression\nIsotonic regression\nbelongs to the family of regression algorithms. Formally isotonic regression is a problem where\ngiven a finite set of real numbers\n$Y = {y_1, y_2, ..., y_n}$\nrepresenting observed responses\nand\n$X = {x_1, x_2, ..., ", "question": "What is the name of the dataset used in the example?", "answers": {"text": ["the ovarian dataset"], "answer_start": [1]}}
{"context": " regression is a problem where\ngiven a finite set of real numbers\n$Y = {y_1, y_2, ..., y_n}$\nrepresenting observed responses\nand\n$X = {x_1, x_2, ..., x_n}$\nthe unknown response values to be fitted\nfinding a function that minimizes\n\\begin{equation}\n  f(x) = \\sum_{i=1}^n w_i (y_i - x_i)^2\n\\end{equation}\nwith respect to complete order subject to\n$x_1\\le x_2\\le ...\\le x_n$\nwhere\n$w_i$\nare positive weights.\nThe resulting function is called isotonic regression and it is unique.\nIt can be viewed as least squares problem under order restriction.\nEssentially isotonic regression is a\nmonotonic function\nbest fitting the original data points.\nWe implement a\npool adjacent violators algorithm\nwhich uses an approach to\nparallelizing isotonic regression\n.\nThe training input is a DataFrame which contains t", "question": "What is the resulting function called when minimizing f(x) with respect to complete order subject to x₁ ≤ x₂ ≤ ... ≤ xₙ?", "answers": {"text": ["The resulting function is called isotonic regression and it is unique."], "answer_start": [406]}}
{"context": "t a\npool adjacent violators algorithm\nwhich uses an approach to\nparallelizing isotonic regression\n.\nThe training input is a DataFrame which contains three columns\nlabel, features and weight. Additionally, IsotonicRegression algorithm has one\noptional parameter called $isotonic$ defaulting to true.\nThis argument specifies if the isotonic regression is\nisotonic (monotonically increasing) or antitonic (monotonically decreasing).\nTraining returns an IsotonicRegressionModel that can be used to predict\nlabels for both known and unknown features. The result of isotonic regression\nis treated as piecewise linear function. The rules for prediction therefore are:\nIf the prediction input exactly matches a training feature\nthen associated prediction is returned. In case there are multiple predictions w", "question": "What are the three columns required in the training input DataFrame?", "answers": {"text": ["label, features and weight"], "answer_start": [163]}}
{"context": "efore are:\nIf the prediction input exactly matches a training feature\nthen associated prediction is returned. In case there are multiple predictions with the same\nfeature then one of them is returned. Which one is undefined\n(same as java.util.Arrays.binarySearch).\nIf the prediction input is lower or higher than all training features\nthen prediction with lowest or highest feature is returned respectively.\nIn case there are multiple predictions with the same feature\nthen the lowest or highest is returned respectively.\nIf the prediction input falls between two training features then prediction is treated\nas piecewise linear function and interpolated value is calculated from the\npredictions of the two closest features. In case there are multiple values\nwith the same feature then the same rules", "question": "O que acontece se a entrada de previsão corresponder exatamente a uma característica de treinamento?", "answers": {"text": ["then associated prediction is returned."], "answer_start": [70]}}
{"context": "th the boundaries: %s\n\\n\n\"\n%\nstr\n(\nmodel\n.\npredictions\n))\n# Makes predictions.\nmodel\n.\ntransform\n(\ndataset\n).\nshow\n()\nFind full example code at \"examples/src/main/python/ml/isotonic_regression_example.py\" in the Spark repo.\nRefer to the\nIsotonicRegression\nScala docs\nfor details on the API.\nimport\norg.apache.spark.ml.regression.IsotonicRegression\n// Loads data.\nval\ndataset\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_isotonic_regression_libsvm_data.txt\"\n)\n// Trains an isotonic regression model.\nval\nir\n=\nnew\nIsotonicRegression\n()\nval\nmodel\n=\nir\n.\nfit\n(\ndataset\n)\nprintln\n(\ns\n\"Boundaries in increasing order: ${model.boundaries}\\n\"\n)\nprintln\n(\ns\n\"Predictions associated with the boundaries: ${model.predictions}\\n\"\n)\n// Makes predictions.\nmodel\n.\ntransform\n(\ndataset\n).\nshow\n()", "question": "Where can I find a full example code for isotonic regression?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/isotonic_regression_example.py\" in the Spark repo."], "answer_start": [118]}}
{"context": "}\\n\"\n)\nprintln\n(\ns\n\"Predictions associated with the boundaries: ${model.predictions}\\n\"\n)\n// Makes predictions.\nmodel\n.\ntransform\n(\ndataset\n).\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\" in the Spark repo.\nRefer to the\nIsotonicRegression\nJava docs\nfor details on the API.\nimport\norg.apache.spark.ml.regression.IsotonicRegression\n;\nimport\norg.apache.spark.ml.regression.IsotonicRegressionModel\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\n// Loads data.\nDataset\n<\nRow\n>\ndataset\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_isotonic_regression_libsvm_data.txt\"\n);\n// Trains an isotonic regression model.\nIsotonicRegression\nir\n=\nnew\nIsotonicRegression\n();\nIsotonicRegressionM", "question": "Where can I find a full example code for Isotonic Regression?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\" in the Spark repo."], "answer_start": [151]}}
{"context": "isotonic_regression_libsvm_data.txt\"\n);\n// Trains an isotonic regression model.\nIsotonicRegression\nir\n=\nnew\nIsotonicRegression\n();\nIsotonicRegressionModel\nmodel\n=\nir\n.\nfit\n(\ndataset\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Boundaries in increasing order: \"\n+\nmodel\n.\nboundaries\n()\n+\n\"\\n\"\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Predictions associated with the boundaries: \"\n+\nmodel\n.\npredictions\n()\n+\n\"\\n\"\n);\n// Makes predictions.\nmodel\n.\ntransform\n(\ndataset\n).\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\" in the Spark repo.\nRefer to the\nIsotonicRegression\nR API docs\nfor more details on the API.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_isotonic_regression_libsvm_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit", "question": "Where can I find a full example code for JavaIsotonicRegression?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\" in the Spark repo."], "answer_start": [448]}}
{"context": "PI.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_isotonic_regression_libsvm_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit an isotonic regression model with spark.isoreg\nmodel\n<-\nspark.isoreg\n(\ntraining\n,\nlabel\n~\nfeatures\n,\nisotonic\n=\nFALSE\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\nFind full example code at \"examples/src/main/r/ml/isoreg.R\" in the Spark repo.\nFactorization machines regressor\nFor more background and more details about the implementation of factorization machines,\nrefer to the\nFactorization Machines section\n.\nExamples\nThe following examples load a dataset in LibSVM format, split it into training and test sets,\ntrain on the first dataset, and then evaluate on the held-out test set.", "question": "Where can I find the full example code for isotonic regression?", "answers": {"text": ["Find full example code at \"examples/src/main/r/ml/isoreg.R\" in the Spark repo."], "answer_start": [379]}}
{"context": "xamples load a dataset in LibSVM format, split it into training and test sets,\ntrain on the first dataset, and then evaluate on the held-out test set.\nWe scale features to be between 0 and 1 to prevent the exploding gradient problem.\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml\nimport\nPipeline\nfrom\npyspark.ml.regression\nimport\nFMRegressor\nfrom\npyspark.ml.feature\nimport\nMinMaxScaler\nfrom\npyspark.ml.evaluation\nimport\nRegressionEvaluator\n# Load and parse the data file, converting it to a DataFrame.\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\n# Scale features.\nfeatureScaler\n=\nMinMaxScaler\n(\ninputCol\n=\n\"\nfeatures\n\"\n,\noutputCol\n=\n\"\nscaledFeatures\n\"\n).\nfit\n(\ndata\n)\n# Split the data into training and test sets (30% held out for test", "question": "What is done to prevent the exploding gradient problem?", "answers": {"text": ["We scale features to be between 0 and 1 to prevent the exploding gradient problem."], "answer_start": [151]}}
{"context": "ark.ml.Pipeline\nimport\norg.apache.spark.ml.evaluation.RegressionEvaluator\nimport\norg.apache.spark.ml.feature.MinMaxScaler\nimport\norg.apache.spark.ml.regression.\n{\nFMRegressionModel\n,\nFMRegressor\n}\n// Load and parse the data file, converting it to a DataFrame.\nval\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Scale features.\nval\nfeatureScaler\n=\nnew\nMinMaxScaler\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"scaledFeatures\"\n)\n.\nfit\n(\ndata\n)\n// Split the data into training and test sets (30% held out for testing).\nval\nArray\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.7\n,\n0.3\n))\n// Train a FM model.\nval\nfm\n=\nnew\nFMRegressor\n()\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetFeaturesCol\n(\n\"scaledFeatures\"\n)\n.\nsetStepSize\n(\n0.001\n)\n// Create a Pipe", "question": "Which format is used to load the data file?", "answers": {"text": ["libsvm"], "answer_start": [296]}}
{"context": "\n\"rmse\"\n)\nval\nrmse\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprintln\n(\ns\n\"Root Mean Squared Error (RMSE) on test data = $rmse\"\n)\nval\nfmModel\n=\nmodel\n.\nstages\n(\n1\n).\nasInstanceOf\n[\nFMRegressionModel\n]\nprintln\n(\ns\n\"Factors: ${fmModel.factors} Linear: ${fmModel.linear} \"\n+\ns\n\"Intercept: ${fmModel.intercept}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/FMRegressorExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.ml.Pipeline\n;\nimport\norg.apache.spark.ml.PipelineModel\n;\nimport\norg.apache.spark.ml.PipelineStage\n;\nimport\norg.apache.spark.ml.evaluation.RegressionEvaluator\n;\nimport\norg.apache.spark.ml.feature.MinMaxScaler\n;\nimport\norg.apache.spark.ml.feature.MinMaxScalerModel\n;\nimport\norg.apache.spark.ml.regressi", "question": "Where can I find the full example code for the FMRegressor?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/FMRegressorExample.scala\" in the Spark repo."], "answer_start": [306]}}
{"context": "aluator\n;\nimport\norg.apache.spark.ml.feature.MinMaxScaler\n;\nimport\norg.apache.spark.ml.feature.MinMaxScalerModel\n;\nimport\norg.apache.spark.ml.regression.FMRegressionModel\n;\nimport\norg.apache.spark.ml.regression.FMRegressor\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Load and parse the data file, converting it to a DataFrame.\nDataset\n<\nRow\n>\ndata\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_libsvm_data.txt\"\n);\n// Scale features.\nMinMaxScalerModel\nfeatureScaler\n=\nnew\nMinMaxScaler\n()\n.\nsetInputCol\n(\n\"features\"\n)\n.\nsetOutputCol\n(\n\"scaledFeatures\"\n)\n.\nfit\n(\ndata\n);\n// Split the data into training and test sets (30% held out for testing).\nDataset\n<\nRow\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]\n", "question": "Which format is used to load the data file?", "answers": {"text": ["libsvm"], "answer_start": [453]}}
{"context": "form\n(\ntestData\n);\n// Select example rows to display.\npredictions\n.\nselect\n(\n\"prediction\"\n,\n\"label\"\n,\n\"features\"\n).\nshow\n(\n5\n);\n// Select (prediction, true label) and compute test error.\nRegressionEvaluator\nevaluator\n=\nnew\nRegressionEvaluator\n()\n.\nsetLabelCol\n(\n\"label\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n)\n.\nsetMetricName\n(\n\"rmse\"\n);\ndouble\nrmse\n=\nevaluator\n.\nevaluate\n(\npredictions\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Root Mean Squared Error (RMSE) on test data = \"\n+\nrmse\n);\nFMRegressionModel\nfmModel\n=\n(\nFMRegressionModel\n)(\nmodel\n.\nstages\n()[\n1\n]);\nSystem\n.\nout\n.\nprintln\n(\n\"Factors: \"\n+\nfmModel\n.\nfactors\n());\nSystem\n.\nout\n.\nprintln\n(\n\"Linear: \"\n+\nfmModel\n.\nlinear\n());\nSystem\n.\nout\n.\nprintln\n(\n\"Intercept: \"\n+\nfmModel\n.\nintercept\n());\nFind full example code at \"examples/src/main/java/org/apache/spa", "question": "What metric is used to evaluate the regression model?", "answers": {"text": ["rmse"], "answer_start": [327]}}
{"context": "Model\n.\nlinear\n());\nSystem\n.\nout\n.\nprintln\n(\n\"Intercept: \"\n+\nfmModel\n.\nintercept\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaFMRegressorExample.java\" in the Spark repo.\nRefer to the\nR API documentation\nfor more details.\nNote: At the moment SparkR doesn’t support feature scaling.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_linear_regression_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining_test\n<-\nrandomSplit\n(\ndf\n,\nc\n(\n0.7\n,\n0.3\n))\ntraining\n<-\ntraining_test\n[[\n1\n]]\ntest\n<-\ntraining_test\n[[\n2\n]]\n# Fit a FM regression model\nmodel\n<-\nspark.fmRegressor\n(\ntraining\n,\nlabel\n~\nfeatures\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\nFind full example code at \"examples/src/main/r/ml/fmRegres", "question": "Where can I find the full example code for JavaFMRegressor?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaFMRegressorExample.java\" in the Spark repo."], "answer_start": [86]}}
{"context": "elastic net regularization.\nFactorization Machines\nFactorization Machines\nare able to estimate interactions\nbetween features even in problems with huge sparsity (like advertising and recommendation system).\nThe\nspark.ml\nimplementation supports factorization machines for binary classification and for regression.\nFactorization machines formula is:\n\\[\\hat{y} = w_0 + \\sum\\limits^n_{i-1} w_i x_i +\n  \\sum\\limits^n_{i=1} \\sum\\limits^n_{j=i+1} \\langle v_i, v_j \\rangle x_i x_j\\]\nThe first two terms denote intercept and linear term (same as in linear regression),\nand the last term denotes pairwise interactions term. \\(v_i\\) describes the i-th variable\nwith k factors.\nFM can be used for regression and optimization criterion is mean square error. FM also can be used for\nbinary classification through s", "question": "What does the first two terms in the factorization machines formula denote?", "answers": {"text": ["The first two terms denote intercept and linear term (same as in linear regression)"], "answer_start": [475]}}
{"context": "s decision trees for binary and multiclass classification and for regression,\nusing both continuous and categorical features. The implementation partitions data by rows,\nallowing distributed training with millions or even billions of instances.\nUsers can find more information about the decision tree algorithm in the\nMLlib Decision Tree guide\n.\nThe main differences between this API and the\noriginal MLlib Decision Tree API\nare:\nsupport for ML Pipelines\nseparation of Decision Trees for classification vs. regression\nuse of DataFrame metadata to distinguish continuous and categorical features\nThe Pipelines API for Decision Trees offers a bit more functionality than the original API.\nIn particular, for classification, users can get the predicted probability of each class (a.k.a. class conditiona", "question": "What are some of the main differences between the current Decision Tree API and the original MLlib Decision Tree API?", "answers": {"text": ["support for ML Pipelines\nseparation of Decision Trees for classification vs. regression\nuse of DataFrame metadata to distinguish continuous and categorical features"], "answer_start": [430]}}
{"context": "functionality than the original API.\nIn particular, for classification, users can get the predicted probability of each class (a.k.a. class conditional probabilities);\nfor regression, users can get the biased sample variance of prediction.\nEnsembles of trees (Random Forests and Gradient-Boosted Trees) are described below in the\nTree ensembles section\n.\nInputs and Outputs\nWe list the input and output (prediction) column types here.\nAll output columns are optional; to exclude an output column, set its corresponding Param to an empty string.\nInput Columns\nParam name\nType(s)\nDefault\nDescription\nlabelCol\nDouble\n\"label\"\nLabel to predict\nfeaturesCol\nVector\n\"features\"\nFeature vector\nOutput Columns\nParam name\nType(s)\nDefault\nDescription\nNotes\npredictionCol\nDouble\n\"prediction\"\nPredicted label\nrawPre", "question": "What is the default label to predict?", "answers": {"text": ["\"label\""], "answer_start": [614]}}
{"context": "oosted Trees (GBTs)\n.\nBoth use\nspark.ml\ndecision trees\nas their base models.\nUsers can find more information about ensemble algorithms in the\nMLlib Ensemble guide\n.\nIn this section, we demonstrate the DataFrame API for ensembles.\nThe main differences between this API and the\noriginal MLlib ensembles API\nare:\nsupport for DataFrames and ML Pipelines\nseparation of classification vs. regression\nuse of DataFrame metadata to distinguish continuous and categorical features\nmore functionality for random forests: estimates of feature importance, as well as the predicted probability of each class (a.k.a. class conditional probabilities) for classification.\nRandom Forests\nRandom forests\nare ensembles of\ndecision trees\n.\nRandom forests combine many decision trees in order to reduce the risk of overfit", "question": "What is a key difference between the DataFrame API for ensembles and the original MLlib ensembles API?", "answers": {"text": ["support for DataFrames and ML Pipelines"], "answer_start": [310]}}
{"context": "ault\nDescription\nlabelCol\nDouble\n\"label\"\nLabel to predict\nfeaturesCol\nVector\n\"features\"\nFeature vector\nOutput Columns (Predictions)\nParam name\nType(s)\nDefault\nDescription\nNotes\npredictionCol\nDouble\n\"prediction\"\nPredicted label\nrawPredictionCol\nVector\n\"rawPrediction\"\nVector of length # classes, with the counts of training instance labels at the tree node which makes the prediction\nClassification only\nprobabilityCol\nVector\n\"probability\"\nVector of length # classes equal to rawPrediction normalized to a multinomial distribution\nClassification only\nGradient-Boosted Trees (GBTs)\nGradient-Boosted Trees (GBTs)\nare ensembles of\ndecision trees\n.\nGBTs iteratively train decision trees in order to minimize a loss function.\nThe\nspark.ml\nimplementation supports GBTs for binary classification and for regr", "question": "What is the purpose of the 'rawPredictionCol'?", "answers": {"text": ["Vector of length # classes, with the counts of training instance labels at the tree node which makes the prediction"], "answer_start": [267]}}
{"context": "teratively train decision trees in order to minimize a loss function.\nThe\nspark.ml\nimplementation supports GBTs for binary classification and for regression,\nusing both continuous and categorical features.\nFor more information on the algorithm itself, please see the\nspark.mllib\ndocumentation on GBTs\n.\nInputs and Outputs\nWe list the input and output (prediction) column types here.\nAll output columns are optional; to exclude an output column, set its corresponding Param to an empty string.\nInput Columns\nParam name\nType(s)\nDefault\nDescription\nlabelCol\nDouble\n\"label\"\nLabel to predict\nfeaturesCol\nVector\n\"features\"\nFeature vector\nNote that\nGBTClassifier\ncurrently only supports binary labels.\nOutput Columns (Predictions)\nParam name\nType(s)\nDefault\nDescription\nNotes\npredictionCol\nDouble\n\"predictio", "question": "What is the default label column name for GBTs?", "answers": {"text": ["\"label\""], "answer_start": [562]}}
{"context": "ifier\ncurrently only supports binary labels.\nOutput Columns (Predictions)\nParam name\nType(s)\nDefault\nDescription\nNotes\npredictionCol\nDouble\n\"prediction\"\nPredicted label\nIn the future,\nGBTClassifier\nwill also output columns for\nrawPrediction\nand\nprobability\n, just as\nRandomForestClassifier\ndoes.", "question": "What type of labels does the classifier currently support?", "answers": {"text": ["binary labels"], "answer_start": [30]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)"], "answer_start": [46]}}
{"context": "ures\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nCollaborative Filtering\nCollaborative filtering\nExplicit vs. implicit feedback\nScaling of the regularization parameter\nCold-start strategy\nCollaborative filtering\nCollaborative filtering\nis commonly used for recommender systems.  These techniques aim to fill in the\nmissing entries of a user-item association matrix.\nspark.ml\ncurrently supports\nmodel-based collaborative filtering, in which use", "question": "Para que é comumente usado o filtro colaborativo?", "answers": {"text": ["is commonly used for recommender systems."], "answer_start": [593]}}
{"context": "ions to run (defaults to 10).\nregParam\nspecifies the regularization parameter in ALS (defaults to 1.0).\nimplicitPrefs\nspecifies whether to use the\nexplicit feedback\nALS variant or one adapted for\nimplicit feedback\ndata (defaults to\nfalse\nwhich means using\nexplicit feedback\n).\nalpha\nis a parameter applicable to the implicit feedback variant of ALS that governs the\nbaseline\nconfidence in preference observations (defaults to 1.0).\nnonnegative\nspecifies whether or not to use nonnegative constraints for least squares (defaults to\nfalse\n).\nNote:\nThe DataFrame-based API for ALS currently only supports integers for user and item ids.\nOther numeric types are supported for the user and item id columns, \nbut the ids must be within the integer value range.\nExplicit vs. implicit feedback\nThe standard a", "question": "What does the 'implicitPrefs' parameter specify?", "answers": {"text": ["specifies whether to use the\nexplicit feedback\nALS variant or one adapted for\nimplicit feedback\ndata (defaults to\nfalse\nwhich means using\nexplicit feedback\n)."], "answer_start": [118]}}
{"context": "updating user factors,\nor the number of ratings the product received in updating product factors.\nThis approach is named “ALS-WR” and discussed in the paper\n“\nLarge-Scale Parallel Collaborative Filtering for the Netflix Prize\n”.\nIt makes\nregParam\nless dependent on the scale of the dataset, so we can apply the\nbest parameter learned from a sampled subset to the full dataset and expect similar performance.\nCold-start strategy\nWhen making predictions using an\nALSModel\n, it is common to encounter users and/or items in the \ntest dataset that were not present during training the model. This typically occurs in two \nscenarios:\nIn production, for new users or items that have no rating history and on which the model has not \nbeen trained (this is the “cold start problem”).\nDuring cross-validation, ", "question": "What is the name of the approach discussed in the paper “Large-Scale Parallel Collaborative Filtering for the Netflix Prize”?", "answers": {"text": ["This approach is named “ALS-WR”"], "answer_start": [98]}}
{"context": " behavior mentioned \nabove) and “drop”. Further strategies may be supported in future.\nExamples\nIn the following example, we load ratings data from the\nMovieLens dataset\n, each row\nconsisting of a user, a movie, a rating and a timestamp.\nWe then train an ALS model which assumes, by default, that the ratings are\nexplicit (\nimplicitPrefs\nis\nFalse\n).\nWe evaluate the recommendation model by measuring the root-mean-square error of\nrating prediction.\nRefer to the\nALS\nPython docs\nfor more details on the API.\nfrom\npyspark.ml.evaluation\nimport\nRegressionEvaluator\nfrom\npyspark.ml.recommendation\nimport\nALS\nfrom\npyspark.sql\nimport\nRow\nlines\n=\nspark\n.\nread\n.\ntext\n(\n\"\ndata/mllib/als/sample_movielens_ratings.txt\n\"\n).\nrdd\nparts\n=\nlines\n.\nmap\n(\nlambda\nrow\n:\nrow\n.\nvalue\n.\nsplit\n(\n\"\n::\n\"\n))\nratingsRDD\n=\npart", "question": "What dataset is used as an example to load ratings data?", "answers": {"text": ["MovieLens dataset"], "answer_start": [152]}}
{"context": " of users\nusers\n=\nratings\n.\nselect\n(\nals\n.\ngetUserCol\n()).\ndistinct\n().\nlimit\n(\n3\n)\nuserSubsetRecs\n=\nmodel\n.\nrecommendForUserSubset\n(\nusers\n,\n10\n)\n# Generate top 10 user recommendations for a specified set of movies\nmovies\n=\nratings\n.\nselect\n(\nals\n.\ngetItemCol\n()).\ndistinct\n().\nlimit\n(\n3\n)\nmovieSubSetRecs\n=\nmodel\n.\nrecommendForItemSubset\n(\nmovies\n,\n10\n)\nFind full example code at \"examples/src/main/python/ml/als_example.py\" in the Spark repo.\nIf the rating matrix is derived from another source of information (i.e. it is\ninferred from other signals), you can set\nimplicitPrefs\nto\nTrue\nto get\nbetter results:\nals\n=\nALS\n(\nmaxIter\n=\n5\n,\nregParam\n=\n0.01\n,\nimplicitPrefs\n=\nTrue\n,\nuserCol\n=\n\"\nuserId\n\"\n,\nitemCol\n=\n\"\nmovieId\n\"\n,\nratingCol\n=\n\"\nrating\n\"\n)\nIn the following example, we load ratings data fr", "question": "What can you set `implicitPrefs` to if the rating matrix is derived from another source of information?", "answers": {"text": ["True"], "answer_start": [584]}}
{"context": "recommendations for each movie\nval\nmovieRecs\n=\nmodel\n.\nrecommendForAllItems\n(\n10\n)\n// Generate top 10 movie recommendations for a specified set of users\nval\nusers\n=\nratings\n.\nselect\n(\nals\n.\ngetUserCol\n).\ndistinct\n().\nlimit\n(\n3\n)\nval\nuserSubsetRecs\n=\nmodel\n.\nrecommendForUserSubset\n(\nusers\n,\n10\n)\n// Generate top 10 user recommendations for a specified set of movies\nval\nmovies\n=\nratings\n.\nselect\n(\nals\n.\ngetItemCol\n).\ndistinct\n().\nlimit\n(\n3\n)\nval\nmovieSubSetRecs\n=\nmodel\n.\nrecommendForItemSubset\n(\nmovies\n,\n10\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\" in the Spark repo.\nIf the rating matrix is derived from another source of information (i.e. it is\ninferred from other signals), you can set\nimplicitPrefs\nto\ntrue\nto get\nbetter results:\nval\na", "question": "Where can I find the full example code for the ALSExample?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\" in the Spark repo."], "answer_start": [512]}}
{"context": "ng\n);\n// Evaluate the model by computing the RMSE on the test data\n// Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\nmodel\n.\nsetColdStartStrategy\n(\n\"drop\"\n);\nDataset\n<\nRow\n>\npredictions\n=\nmodel\n.\ntransform\n(\ntest\n);\nRegressionEvaluator\nevaluator\n=\nnew\nRegressionEvaluator\n()\n.\nsetMetricName\n(\n\"rmse\"\n)\n.\nsetLabelCol\n(\n\"rating\"\n)\n.\nsetPredictionCol\n(\n\"prediction\"\n);\ndouble\nrmse\n=\nevaluator\n.\nevaluate\n(\npredictions\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Root-mean-square error = \"\n+\nrmse\n);\n// Generate top 10 movie recommendations for each user\nDataset\n<\nRow\n>\nuserRecs\n=\nmodel\n.\nrecommendForAllUsers\n(\n10\n);\n// Generate top 10 user recommendations for each movie\nDataset\n<\nRow\n>\nmovieRecs\n=\nmodel\n.\nrecommendForAllItems\n(\n10\n);\n// Generate top 10 movie recommendation", "question": "What strategy is set for cold start to avoid NaN evaluation metrics?", "answers": {"text": ["drop"], "answer_start": [106]}}
{"context": "te top 10 user recommendations for each movie\nDataset\n<\nRow\n>\nmovieRecs\n=\nmodel\n.\nrecommendForAllItems\n(\n10\n);\n// Generate top 10 movie recommendations for a specified set of users\nDataset\n<\nRow\n>\nusers\n=\nratings\n.\nselect\n(\nals\n.\ngetUserCol\n()).\ndistinct\n().\nlimit\n(\n3\n);\nDataset\n<\nRow\n>\nuserSubsetRecs\n=\nmodel\n.\nrecommendForUserSubset\n(\nusers\n,\n10\n);\n// Generate top 10 user recommendations for a specified set of movies\nDataset\n<\nRow\n>\nmovies\n=\nratings\n.\nselect\n(\nals\n.\ngetItemCol\n()).\ndistinct\n().\nlimit\n(\n3\n);\nDataset\n<\nRow\n>\nmovieSubSetRecs\n=\nmodel\n.\nrecommendForItemSubset\n(\nmovies\n,\n10\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\" in the Spark repo.\nIf the rating matrix is derived from another source of information (i.e. it is\ninferr", "question": "Where can I find a full example code for the JavaALSExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\" in the Spark repo."], "answer_start": [596]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars"], "answer_start": [46]}}
{"context": "ures\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nClustering\nThis page describes clustering algorithms in MLlib.\nThe\nguide for clustering in the RDD-based API\nalso has relevant information\nabout these algorithms.\nTable of Contents\nK-means\nInput Columns\nOutput Columns\nLatent Dirichlet allocation (LDA)\nBisecting k-means\nGaussian Mixture Model (GMM)\nInput Columns\nOutput Columns\nPower Iteration Clustering (PIC)\nK-means\nk-means\nis one of the\nmos", "question": "What does this page describe?", "answers": {"text": ["This page describes clustering algorithms in MLlib."], "answer_start": [417]}}
{"context": "n (LDA)\nBisecting k-means\nGaussian Mixture Model (GMM)\nInput Columns\nOutput Columns\nPower Iteration Clustering (PIC)\nK-means\nk-means\nis one of the\nmost commonly used clustering algorithms that clusters the data points into a\npredefined number of clusters. The MLlib implementation includes a parallelized\nvariant of the\nk-means++\nmethod\ncalled\nkmeans||\n.\nKMeans\nis implemented as an\nEstimator\nand generates a\nKMeansModel\nas the base model.\nInput Columns\nParam name\nType(s)\nDefault\nDescription\nfeaturesCol\nVector\n\"features\"\nFeature vector\nOutput Columns\nParam name\nType(s)\nDefault\nDescription\npredictionCol\nInt\n\"prediction\"\nPredicted cluster center\nExamples\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml.clustering\nimport\nKMeans\nfrom\npyspark.ml.evaluation\nimport\nClusteringEvaluator\n#", "question": "What is the default value for the 'predictionCol' parameter?", "answers": {"text": ["\"prediction\""], "answer_start": [610]}}
{"context": "amples\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml.clustering\nimport\nKMeans\nfrom\npyspark.ml.evaluation\nimport\nClusteringEvaluator\n# Loads data.\ndataset\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_kmeans_data.txt\n\"\n)\n# Trains a k-means model.\nkmeans\n=\nKMeans\n().\nsetK\n(\n2\n).\nsetSeed\n(\n1\n)\nmodel\n=\nkmeans\n.\nfit\n(\ndataset\n)\n# Make predictions\npredictions\n=\nmodel\n.\ntransform\n(\ndataset\n)\n# Evaluate clustering by computing Silhouette score\nevaluator\n=\nClusteringEvaluator\n()\nsilhouette\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprint\n(\n\"\nSilhouette with squared euclidean distance =\n\"\n+\nstr\n(\nsilhouette\n))\n# Shows the result.\ncenters\n=\nmodel\n.\nclusterCenters\n()\nprint\n(\n\"\nCluster Centers:\n\"\n)\nfor\ncenter\nin\ncenters\n:\nprint\n(\ncenter\n)\nFind full example code at \"exa", "question": "What is used to evaluate clustering?", "answers": {"text": ["ClusteringEvaluator"], "answer_start": [129]}}
{"context": "the result.\ncenters\n=\nmodel\n.\nclusterCenters\n()\nprint\n(\n\"\nCluster Centers:\n\"\n)\nfor\ncenter\nin\ncenters\n:\nprint\n(\ncenter\n)\nFind full example code at \"examples/src/main/python/ml/kmeans_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.clustering.KMeans\nimport\norg.apache.spark.ml.evaluation.ClusteringEvaluator\n// Loads data.\nval\ndataset\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_kmeans_data.txt\"\n)\n// Trains a k-means model.\nval\nkmeans\n=\nnew\nKMeans\n().\nsetK\n(\n2\n).\nsetSeed\n(\n1L\n)\nval\nmodel\n=\nkmeans\n.\nfit\n(\ndataset\n)\n// Make predictions\nval\npredictions\n=\nmodel\n.\ntransform\n(\ndataset\n)\n// Evaluate clustering by computing Silhouette score\nval\nevaluator\n=\nnew\nClusteringEvaluator\n()\nval\nsilhouette\n=\nevaluator\n.\nevaluate\n(\npredi", "question": "Where can I find a full example code for k-means?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/kmeans_example.py\" in the Spark repo."], "answer_start": [120]}}
{"context": "ataset\n)\n// Evaluate clustering by computing Silhouette score\nval\nevaluator\n=\nnew\nClusteringEvaluator\n()\nval\nsilhouette\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprintln\n(\ns\n\"Silhouette with squared euclidean distance = $silhouette\"\n)\n// Shows the result.\nprintln\n(\n\"Cluster Centers: \"\n)\nmodel\n.\nclusterCenters\n.\nforeach\n(\nprintln\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.ml.clustering.KMeansModel\n;\nimport\norg.apache.spark.ml.clustering.KMeans\n;\nimport\norg.apache.spark.ml.evaluation.ClusteringEvaluator\n;\nimport\norg.apache.spark.ml.linalg.Vector\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\n// Loads data.\nDataset\n<\nRow\n>\n", "question": "How is clustering evaluated in this code snippet?", "answers": {"text": ["Evaluate clustering by computing Silhouette score"], "answer_start": [12]}}
{"context": "\n(\n\"Silhouette with squared euclidean distance = \"\n+\nsilhouette\n);\n// Shows the result.\nVector\n[]\ncenters\n=\nmodel\n.\nclusterCenters\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Cluster Centers: \"\n);\nfor\n(\nVector\ncenter:\ncenters\n)\n{\nSystem\n.\nout\n.\nprintln\n(\ncenter\n);\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# Fit a k-means model with spark.kmeans\nt\n<-\nas.data.frame\n(\nTitanic\n)\ntraining\n<-\ncreateDataFrame\n(\nt\n)\ndf_list\n<-\nrandomSplit\n(\ntraining\n,\nc\n(\n7\n,\n3\n),\n2\n)\nkmeansDF\n<-\ndf_list\n[[\n1\n]]\nkmeansTestDF\n<-\ndf_list\n[[\n2\n]]\nkmeansModel\n<-\nspark.kmeans\n(\nkmeansDF\n,\n~\nClass\n+\nSex\n+\nAge\n+\nFreq\n,\nk\n=\n3\n)\n# Model summary\nsummary\n(\nkmeansModel\n)\n# Get fitted result from the k-means model\nhead", "question": "Where can I find a full example code for JavaKMeansExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\" in the Spark repo."], "answer_start": [253]}}
{"context": "spark.kmeans\n(\nkmeansDF\n,\n~\nClass\n+\nSex\n+\nAge\n+\nFreq\n,\nk\n=\n3\n)\n# Model summary\nsummary\n(\nkmeansModel\n)\n# Get fitted result from the k-means model\nhead\n(\nfitted\n(\nkmeansModel\n))\n# Prediction\nkmeansPredictions\n<-\npredict\n(\nkmeansModel\n,\nkmeansTestDF\n)\nhead\n(\nkmeansPredictions\n)\nFind full example code at \"examples/src/main/r/ml/kmeans.R\" in the Spark repo.\nLatent Dirichlet allocation (LDA)\nLDA\nis implemented as an\nEstimator\nthat supports both\nEMLDAOptimizer\nand\nOnlineLDAOptimizer\n,\nand generates a\nLDAModel\nas the base model. Expert users may cast a\nLDAModel\ngenerated by\nEMLDAOptimizer\nto a\nDistributedLDAModel\nif needed.\nExamples\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml.clustering\nimport\nLDA\n# Loads data.\ndataset\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib", "question": "Where can I find full example code for k-means?", "answers": {"text": ["Find full example code at \"examples/src/main/r/ml/kmeans.R\" in the Spark repo."], "answer_start": [277]}}
{"context": "hows the result\ntransformed\n=\nmodel\n.\ntransform\n(\ndataset\n)\ntransformed\n.\nshow\n(\ntruncate\n=\nFalse\n)\nFind full example code at \"examples/src/main/python/ml/lda_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.clustering.LDA\n// Loads data.\nval\ndataset\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_lda_libsvm_data.txt\"\n)\n// Trains a LDA model.\nval\nlda\n=\nnew\nLDA\n().\nsetK\n(\n10\n).\nsetMaxIter\n(\n10\n)\nval\nmodel\n=\nlda\n.\nfit\n(\ndataset\n)\nval\nll\n=\nmodel\n.\nlogLikelihood\n(\ndataset\n)\nval\nlp\n=\nmodel\n.\nlogPerplexity\n(\ndataset\n)\nprintln\n(\ns\n\"The lower bound on the log likelihood of the entire corpus: $ll\"\n)\nprintln\n(\ns\n\"The upper bound on perplexity: $lp\"\n)\n// Describe topics.\nval\ntopics\n=\nmodel\n.\ndescribeTopics\n(\n3\n)\nprintln\n(\n\"The top", "question": "Where can I find a full example code for LDA?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/lda_example.py\" in the Spark repo."], "answer_start": [100]}}
{"context": "ire corpus: $ll\"\n)\nprintln\n(\ns\n\"The upper bound on perplexity: $lp\"\n)\n// Describe topics.\nval\ntopics\n=\nmodel\n.\ndescribeTopics\n(\n3\n)\nprintln\n(\n\"The topics described by their top-weighted terms:\"\n)\ntopics\n.\nshow\n(\nfalse\n)\n// Shows the result.\nval\ntransformed\n=\nmodel\n.\ntransform\n(\ndataset\n)\ntransformed\n.\nshow\n(\nfalse\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.ml.clustering.LDA\n;\nimport\norg.apache.spark.ml.clustering.LDAModel\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.SparkSession\n;\n// Loads data.\nDataset\n<\nRow\n>\ndataset\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_lda_li", "question": "Where can I find a full example code for LDA?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\" in the Spark repo."], "answer_start": [318]}}
{"context": "ics described by their top-weighted terms:\"\n);\ntopics\n.\nshow\n(\nfalse\n);\n// Shows the result.\nDataset\n<\nRow\n>\ntransformed\n=\nmodel\n.\ntransform\n(\ndataset\n);\ntransformed\n.\nshow\n(\nfalse\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_lda_libsvm_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit a latent dirichlet allocation model with spark.lda\nmodel\n<-\nspark.lda\n(\ntraining\n,\nk\n=\n10\n,\nmaxIter\n=\n10\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Posterior probabilities\nposterior\n<-\nspark.posterior\n(\nmodel\n,\ntest\n)\nhead\n(\nposterior\n)\n# The log perplexity of the LDA model\nlogPerplexity\n<-\nspark.perplexity\n(\nmodel\n,\ntest\n)\npri", "question": "Where can I find a full example code for JavaLDAExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\" in the Spark repo."], "answer_start": [184]}}
{"context": "or\n<-\nspark.posterior\n(\nmodel\n,\ntest\n)\nhead\n(\nposterior\n)\n# The log perplexity of the LDA model\nlogPerplexity\n<-\nspark.perplexity\n(\nmodel\n,\ntest\n)\nprint\n(\npaste0\n(\n\"The upper bound bound on perplexity: \"\n,\nlogPerplexity\n))\nFind full example code at \"examples/src/main/r/ml/lda.R\" in the Spark repo.\nBisecting k-means\nBisecting k-means is a kind of\nhierarchical clustering\nusing a\ndivisive (or “top-down”) approach: all observations start in one cluster, and splits are performed recursively as one\nmoves down the hierarchy.\nBisecting K-means can often be much faster than regular K-means, but it will generally produce a different clustering.\nBisectingKMeans\nis implemented as an\nEstimator\nand generates a\nBisectingKMeansModel\nas the base model.\nExamples\nRefer to the\nPython API docs\nfor more details", "question": "Onde posso encontrar um exemplo de código completo para LDA?", "answers": {"text": ["Find full example code at \"examples/src/main/r/ml/lda.R\" in the Spark repo."], "answer_start": [223]}}
{"context": "ngKMeans\nis implemented as an\nEstimator\nand generates a\nBisectingKMeansModel\nas the base model.\nExamples\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml.clustering\nimport\nBisectingKMeans\nfrom\npyspark.ml.evaluation\nimport\nClusteringEvaluator\n# Loads data.\ndataset\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_kmeans_data.txt\n\"\n)\n# Trains a bisecting k-means model.\nbkm\n=\nBisectingKMeans\n().\nsetK\n(\n2\n).\nsetSeed\n(\n1\n)\nmodel\n=\nbkm\n.\nfit\n(\ndataset\n)\n# Make predictions\npredictions\n=\nmodel\n.\ntransform\n(\ndataset\n)\n# Evaluate clustering by computing Silhouette score\nevaluator\n=\nClusteringEvaluator\n()\nsilhouette\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprint\n(\n\"\nSilhouette with squared euclidean distance =\n\"\n+\nstr\n(\nsilhouette\n))\n# Shows the result.\nprint\n(\n\"\nCluster ", "question": "What is generated as the base model by ngKMeans?", "answers": {"text": ["BisectingKMeansModel"], "answer_start": [56]}}
{"context": "uator\n.\nevaluate\n(\npredictions\n)\nprint\n(\n\"\nSilhouette with squared euclidean distance =\n\"\n+\nstr\n(\nsilhouette\n))\n# Shows the result.\nprint\n(\n\"\nCluster Centers:\n\"\n)\ncenters\n=\nmodel\n.\nclusterCenters\n()\nfor\ncenter\nin\ncenters\n:\nprint\n(\ncenter\n)\nFind full example code at \"examples/src/main/python/ml/bisecting_k_means_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.clustering.BisectingKMeans\nimport\norg.apache.spark.ml.evaluation.ClusteringEvaluator\n// Loads data.\nval\ndataset\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_kmeans_data.txt\"\n)\n// Trains a bisecting k-means model.\nval\nbkm\n=\nnew\nBisectingKMeans\n().\nsetK\n(\n2\n).\nsetSeed\n(\n1\n)\nval\nmodel\n=\nbkm\n.\nfit\n(\ndataset\n)\n// Make predictions\nval\npredictions\n=\nmodel\n.\ntransform\n(", "question": "Where can I find a full example code for bisecting k-means?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/bisecting_k_means_example.py\" in the Spark repo."], "answer_start": [240]}}
{"context": "al\nbkm\n=\nnew\nBisectingKMeans\n().\nsetK\n(\n2\n).\nsetSeed\n(\n1\n)\nval\nmodel\n=\nbkm\n.\nfit\n(\ndataset\n)\n// Make predictions\nval\npredictions\n=\nmodel\n.\ntransform\n(\ndataset\n)\n// Evaluate clustering by computing Silhouette score\nval\nevaluator\n=\nnew\nClusteringEvaluator\n()\nval\nsilhouette\n=\nevaluator\n.\nevaluate\n(\npredictions\n)\nprintln\n(\ns\n\"Silhouette with squared euclidean distance = $silhouette\"\n)\n// Shows the result.\nprintln\n(\n\"Cluster Centers: \"\n)\nval\ncenters\n=\nmodel\n.\nclusterCenters\ncenters\n.\nforeach\n(\nprintln\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.ml.clustering.BisectingKMeans\n;\nimport\norg.apache.spark.ml.clustering.BisectingKMeansModel\n;\nimpor", "question": "Where can I find the full example code for BisectingKMeans?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\" in the Spark repo."], "answer_start": [504]}}
{"context": " API docs\nfor more details.\nimport\norg.apache.spark.ml.clustering.BisectingKMeans\n;\nimport\norg.apache.spark.ml.clustering.BisectingKMeansModel\n;\nimport\norg.apache.spark.ml.evaluation.ClusteringEvaluator\n;\nimport\norg.apache.spark.ml.linalg.Vector\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\n// Loads data.\nDataset\n<\nRow\n>\ndataset\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_kmeans_data.txt\"\n);\n// Trains a bisecting k-means model.\nBisectingKMeans\nbkm\n=\nnew\nBisectingKMeans\n().\nsetK\n(\n2\n).\nsetSeed\n(\n1\n);\nBisectingKMeansModel\nmodel\n=\nbkm\n.\nfit\n(\ndataset\n);\n// Make predictions\nDataset\n<\nRow\n>\npredictions\n=\nmodel\n.\ntransform\n(\ndataset\n);\n// Evaluate clustering by computing Silhouette score\nClusteringEvaluator\nevaluator\n=\nnew\nClusteringEvaluator\n();", "question": "Which class is used to evaluate clustering by computing the Silhouette score?", "answers": {"text": ["ClusteringEvaluator"], "answer_start": [183]}}
{"context": "ions\n=\nmodel\n.\ntransform\n(\ndataset\n);\n// Evaluate clustering by computing Silhouette score\nClusteringEvaluator\nevaluator\n=\nnew\nClusteringEvaluator\n();\ndouble\nsilhouette\n=\nevaluator\n.\nevaluate\n(\npredictions\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Silhouette with squared euclidean distance = \"\n+\nsilhouette\n);\n// Shows the result.\nSystem\n.\nout\n.\nprintln\n(\n\"Cluster Centers: \"\n);\nVector\n[]\ncenters\n=\nmodel\n.\nclusterCenters\n();\nfor\n(\nVector\ncenter\n:\ncenters\n)\n{\nSystem\n.\nout\n.\nprintln\n(\ncenter\n);\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\nt\n<-\nas.data.frame\n(\nTitanic\n)\ntraining\n<-\ncreateDataFrame\n(\nt\n)\n# Fit bisecting k-means model with four centers\nmodel\n<-\nspark.bisectingKmean", "question": "Where can I find the full example code for JavaBisectingKMeansExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\" in the Spark repo."], "answer_start": [485]}}
{"context": "ts are drawn from one of\nk\nGaussian sub-distributions,\neach with its own probability. The\nspark.ml\nimplementation uses the\nexpectation-maximization\nalgorithm to induce the maximum-likelihood model given a set of samples.\nGaussianMixture\nis implemented as an\nEstimator\nand generates a\nGaussianMixtureModel\nas the base\nmodel.\nInput Columns\nParam name\nType(s)\nDefault\nDescription\nfeaturesCol\nVector\n\"features\"\nFeature vector\nOutput Columns\nParam name\nType(s)\nDefault\nDescription\npredictionCol\nInt\n\"prediction\"\nPredicted cluster center\nprobabilityCol\nVector\n\"probability\"\nProbability of each cluster\nExamples\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml.clustering\nimport\nGaussianMixture\n# loads data\ndataset\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n).\nload\n(\n\"\ndata/mllib/sample_kmeans_dat", "question": "What is the default column name for the predicted cluster center?", "answers": {"text": ["\"prediction\""], "answer_start": [494]}}
{"context": "xt\"\n)\n// Trains Gaussian Mixture Model\nval\ngmm\n=\nnew\nGaussianMixture\n()\n.\nsetK\n(\n2\n)\nval\nmodel\n=\ngmm\n.\nfit\n(\ndataset\n)\n// output parameters of mixture model model\nfor\n(\ni\n<-\n0\nuntil\nmodel\n.\ngetK\n)\n{\nprintln\n(\ns\n\"Gaussian $i:\\nweight=${model.weights(i)}\\n\"\n+\ns\n\"mu=${model.gaussians(i).mean}\\nsigma=\\n${model.gaussians(i).cov}\\n\"\n)\n}\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\norg.apache.spark.ml.clustering.GaussianMixture\n;\nimport\norg.apache.spark.ml.clustering.GaussianMixtureModel\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\n// Loads data\nDataset\n<\nRow\n>\ndataset\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_", "question": "Where can I find a full example code for Gaussian Mixture Model?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\" in the Spark repo."], "answer_start": [333]}}
{"context": "l.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\n// Loads data\nDataset\n<\nRow\n>\ndataset\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_kmeans_data.txt\"\n);\n// Trains a GaussianMixture model\nGaussianMixture\ngmm\n=\nnew\nGaussianMixture\n()\n.\nsetK\n(\n2\n);\nGaussianMixtureModel\nmodel\n=\ngmm\n.\nfit\n(\ndataset\n);\n// Output the parameters of the mixture model\nfor\n(\nint\ni\n=\n0\n;\ni\n<\nmodel\n.\ngetK\n();\ni\n++)\n{\nSystem\n.\nout\n.\nprintf\n(\n\"Gaussian %d:\\nweight=%f\\nmu=%s\\nsigma=\\n%s\\n\\n\"\n,\ni\n,\nmodel\n.\nweights\n()[\ni\n],\nmodel\n.\ngaussians\n()[\ni\n].\nmean\n(),\nmodel\n.\ngaussians\n()[\ni\n].\ncov\n());\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# Load training data\ndf\n<-\nread.df\n(\n\"dat", "question": "Where can I find the full example code for JavaGaussianMixtureExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\" in the Spark repo."], "answer_start": [586]}}
{"context": "rk/examples/ml/JavaGaussianMixtureExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# Load training data\ndf\n<-\nread.df\n(\n\"data/mllib/sample_kmeans_data.txt\"\n,\nsource\n=\n\"libsvm\"\n)\ntraining\n<-\ndf\ntest\n<-\ndf\n# Fit a gaussian mixture clustering model with spark.gaussianMixture\nmodel\n<-\nspark.gaussianMixture\n(\ntraining\n,\n~\nfeatures\n,\nk\n=\n2\n)\n# Model summary\nsummary\n(\nmodel\n)\n# Prediction\npredictions\n<-\npredict\n(\nmodel\n,\ntest\n)\nhead\n(\npredictions\n)\nFind full example code at \"examples/src/main/r/ml/gaussianMixture.R\" in the Spark repo.\nPower Iteration Clustering (PIC)\nPower Iteration Clustering (PIC) is  a scalable graph clustering algorithm\ndeveloped by\nLin and Cohen\n.\nFrom the abstract: PIC finds a very low-dimensional embedding of a dataset\nusing truncated power iterat", "question": "Where can I find full example code for gaussian mixture clustering?", "answers": {"text": ["Find full example code at \"examples/src/main/r/ml/gaussianMixture.R\" in the Spark repo."], "answer_start": [472]}}
{"context": "he/spark/examples/ml/JavaPowerIterationClusteringExample.java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\ndf\n<-\ncreateDataFrame\n(\nlist\n(\nlist\n(\n0L\n,\n1L\n,\n1.0\n),\nlist\n(\n0L\n,\n2L\n,\n1.0\n),\nlist\n(\n1L\n,\n2L\n,\n1.0\n),\nlist\n(\n3L\n,\n4L\n,\n1.0\n),\nlist\n(\n4L\n,\n0L\n,\n0.1\n)),\nschema\n=\nc\n(\n\"src\"\n,\n\"dst\"\n,\n\"weight\"\n))\n# assign clusters\nclusters\n<-\nspark.assignClusters\n(\ndf\n,\nk\n=\n2L\n,\nmaxIter\n=\n20L\n,\ninitMode\n=\n\"degree\"\n,\nweightCol\n=\n\"weight\"\n)\nshowDF\n(\narrange\n(\nclusters\n,\nclusters\n$\nid\n))\nFind full example code at \"examples/src/main/r/ml/powerIterationClustering.R\" in the Spark repo.", "question": "Where can the full example code for power iteration clustering be found?", "answers": {"text": ["Find full example code at \"examples/src/main/r/ml/powerIterationClustering.R\" in the Spark repo."], "answer_start": [492]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)"], "answer_start": [46]}}
{"context": " Wikipedia’s\nassociation rule learning\nfor more information.\nTable of Contents\nFP-Growth\nPrefixSpan\nFP-Growth\nThe FP-growth algorithm is described in the paper\nHan et al., Mining frequent patterns without candidate generation\n,\nwhere “FP” stands for frequent pattern.\nGiven a dataset of transactions, the first step of FP-growth is to calculate item frequencies and identify frequent items.\nDifferent from\nApriori-like\nalgorithms designed for the same purpose,\nthe second step of FP-growth uses a suffix tree (FP-tree) structure to encode transactions without generating candidate sets\nexplicitly, which are usually expensive to generate.\nAfter the second step, the frequent itemsets can be extracted from the FP-tree.\nIn\nspark.mllib\n, we implemented a parallel version of FP-growth called PFP,\nas de", "question": "What does \"FP\" stand for in the FP-growth algorithm?", "answers": {"text": ["“FP” stands for frequent pattern."], "answer_start": [234]}}
{"context": "econd step, the frequent itemsets can be extracted from the FP-tree.\nIn\nspark.mllib\n, we implemented a parallel version of FP-growth called PFP,\nas described in\nLi et al., PFP: Parallel FP-growth for query recommendation\n.\nPFP distributes the work of growing FP-trees based on the suffixes of transactions,\nand hence is more scalable than a single-machine implementation.\nWe refer users to the papers for more details.\nFP-growth operates on\nitemsets\n. An itemset is an unordered collection of unique items. Spark does not have a\nset\ntype, so itemsets are represented as arrays.\nspark.ml\n’s FP-growth implementation takes the following (hyper-)parameters:\nminSupport\n: the minimum support for an itemset to be identified as frequent.\nFor example, if an item appears 3 out of 5 transactions, it has a s", "question": "How are itemsets represented in Spark since it lacks a set type?", "answers": {"text": ["itemsets are represented as arrays."], "answer_start": [542]}}
{"context": "umber of partitions used to distribute the work. By default the param is not set, and\nnumber of partitions of the input dataset is used.\nThe\nFPGrowthModel\nprovides:\nfreqItemsets\n: frequent itemsets in the format of a DataFrame with the following columns:\nitems: array\n: A given itemset.\nfreq: long\n: A count of how many times this itemset was seen, given the configured model parameters.\nassociationRules\n: association rules generated with confidence above\nminConfidence\n, in the format of a DataFrame with the following columns:\nantecedent: array\n: The itemset that is the hypothesis of the association rule.\nconsequent: array\n: An itemset that always contains a single element representing the conclusion of the association rule.\nconfidence: double\n: Refer to\nminConfidence\nabove for a definition o", "question": "What columns are present in the DataFrame representing frequent itemsets provided by the FPGrowthModel?", "answers": {"text": ["items: array\n: A given itemset.\nfreq: long\n: A count of how many times this itemset was seen, given the configured model parameters."], "answer_start": [255]}}
{"context": "ys contains a single element representing the conclusion of the association rule.\nconfidence: double\n: Refer to\nminConfidence\nabove for a definition of\nconfidence\n.\nlift: double\n: A measure of how well the antecedent predicts the consequent, calculated as\nsupport(antecedent U consequent) / (support(antecedent) x support(consequent))\nsupport: double\n: Refer to\nminSupport\nabove for a definition of\nsupport\n.\ntransform\n: For each transaction in\nitemsCol\n, the\ntransform\nmethod will compare its items against the antecedents\nof each association rule. If the record contains all the antecedents of a specific association rule, the rule\nwill be considered as applicable and its consequents will be added to the prediction result. The transform\nmethod will summarize the consequents from all the applicab", "question": "Como o lift é calculado?", "answers": {"text": ["support(antecedent U consequent) / (support(antecedent) x support(consequent))"], "answer_start": [256]}}
{"context": "l\n.\nfreqItemsets\n.\nshow\n()\n# Display generated association rules.\nmodel\n.\nassociationRules\n.\nshow\n()\n# transform examines the input items against all the association rules and summarize the\n# consequents as prediction\nmodel\n.\ntransform\n(\ndf\n).\nshow\n()\nFind full example code at \"examples/src/main/python/ml/fpgrowth_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.fpm.FPGrowth\nval\ndataset\n=\nspark\n.\ncreateDataset\n(\nSeq\n(\n\"1 2 5\"\n,\n\"1 2 3 5\"\n,\n\"1 2\"\n)\n).\nmap\n(\nt\n=>\nt\n.\nsplit\n(\n\" \"\n)).\ntoDF\n(\n\"items\"\n)\nval\nfpgrowth\n=\nnew\nFPGrowth\n().\nsetItemsCol\n(\n\"items\"\n).\nsetMinSupport\n(\n0.5\n).\nsetMinConfidence\n(\n0.6\n)\nval\nmodel\n=\nfpgrowth\n.\nfit\n(\ndataset\n)\n// Display frequent itemsets.\nmodel\n.\nfreqItemsets\n.\nshow\n()\n// Display generated association rule", "question": "What does the code `model.freqItemsets.show()` do?", "answers": {"text": ["Display generated association rules."], "answer_start": [29]}}
{"context": "\nselectExpr\n(\ncreateDataFrame\n(\ndata.frame\n(\nrawItems\n=\nc\n(\n\"1,2,5\"\n,\n\"1,2,3,5\"\n,\n\"1,2\"\n))),\n\"split(rawItems, ',') AS items\"\n)\nfpm\n<-\nspark.fpGrowth\n(\ndf\n,\nitemsCol\n=\n\"items\"\n,\nminSupport\n=\n0.5\n,\nminConfidence\n=\n0.6\n)\n# Extracting frequent itemsets\nspark.freqItemsets\n(\nfpm\n)\n# Extracting association rules\nspark.associationRules\n(\nfpm\n)\n# Predict uses association rules to and combines possible consequents\npredict\n(\nfpm\n,\ndf\n)\nFind full example code at \"examples/src/main/r/ml/fpm.R\" in the Spark repo.\nPrefixSpan\nPrefixSpan is a sequential pattern mining algorithm described in\nPei et al., Mining Sequential Patterns by Pattern-Growth: The\nPrefixSpan Approach\n. We refer\nthe reader to the referenced paper for formalizing the sequential\npattern mining problem.\nspark.ml\n’s PrefixSpan implementatio", "question": "Where can I find the full example code for fpm?", "answers": {"text": ["Find full example code at \"examples/src/main/r/ml/fpm.R\" in the Spark repo."], "answer_start": [429]}}
{"context": "utors.\nsequenceCol\n: the name of the sequence column in dataset (default “sequence”), rows with\nnulls in this column are ignored.\nExamples\nRefer to the\nPython API docs\nfor more details.\nfrom\npyspark.ml.fpm\nimport\nPrefixSpan\ndf\n=\nsc\n.\nparallelize\n([\nRow\n(\nsequence\n=\n[[\n1\n,\n2\n],\n[\n3\n]]),\nRow\n(\nsequence\n=\n[[\n1\n],\n[\n3\n,\n2\n],\n[\n1\n,\n2\n]]),\nRow\n(\nsequence\n=\n[[\n1\n,\n2\n],\n[\n5\n]]),\nRow\n(\nsequence\n=\n[[\n6\n]])]).\ntoDF\n()\nprefixSpan\n=\nPrefixSpan\n(\nminSupport\n=\n0.5\n,\nmaxPatternLength\n=\n5\n,\nmaxLocalProjDBSize\n=\n32000000\n)\n# Find frequent sequential patterns.\nprefixSpan\n.\nfindFrequentSequentialPatterns\n(\ndf\n).\nshow\n()\nFind full example code at \"examples/src/main/python/ml/prefixspan_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.fpm.PrefixSpan\nval\nsma", "question": "What is the default name of the sequence column in the dataset?", "answers": {"text": ["“sequence”"], "answer_start": [73]}}
{"context": "in/python/ml/prefixspan_example.py\" in the Spark repo.\nRefer to the\nScala API docs\nfor more details.\nimport\norg.apache.spark.ml.fpm.PrefixSpan\nval\nsmallTestData\n=\nSeq\n(\nSeq\n(\nSeq\n(\n1\n,\n2\n),\nSeq\n(\n3\n)),\nSeq\n(\nSeq\n(\n1\n),\nSeq\n(\n3\n,\n2\n),\nSeq\n(\n1\n,\n2\n)),\nSeq\n(\nSeq\n(\n1\n,\n2\n),\nSeq\n(\n5\n)),\nSeq\n(\nSeq\n(\n6\n)))\nval\ndf\n=\nsmallTestData\n.\ntoDF\n(\n\"sequence\"\n)\nval\nresult\n=\nnew\nPrefixSpan\n()\n.\nsetMinSupport\n(\n0.5\n)\n.\nsetMaxPatternLength\n(\n5\n)\n.\nsetMaxLocalProjDBSize\n(\n32000000\n)\n.\nfindFrequentSequentialPatterns\n(\ndf\n)\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/PrefixSpanExample.scala\" in the Spark repo.\nRefer to the\nJava API docs\nfor more details.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.ml.fpm.PrefixSpan\n;\nimport\norg.apache.spa", "question": "Where can I find the full example code for PrefixSpan in Scala?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/PrefixSpanExample.scala\" in the Spark repo."], "answer_start": [516]}}
{"context": ".java\" in the Spark repo.\nRefer to the\nR API docs\nfor more details.\n# Load training data\ndf\n<-\ncreateDataFrame\n(\nlist\n(\nlist\n(\nlist\n(\nlist\n(\n1L\n,\n2L\n),\nlist\n(\n3L\n))),\nlist\n(\nlist\n(\nlist\n(\n1L\n),\nlist\n(\n3L\n,\n2L\n),\nlist\n(\n1L\n,\n2L\n))),\nlist\n(\nlist\n(\nlist\n(\n1L\n,\n2L\n),\nlist\n(\n5L\n))),\nlist\n(\nlist\n(\nlist\n(\n6L\n)))),\nschema\n=\nc\n(\n\"sequence\"\n))\n# Finding frequent sequential patterns\nfrequency\n<-\nspark.findFrequentSequentialPatterns\n(\ndf\n,\nminSupport\n=\n0.5\n,\nmaxPatternLength\n=\n5L\n,\nmaxLocalProjDBSize\n=\n32000000L\n)\nshowDF\n(\nfrequency\n)\nFind full example code at \"examples/src/main/r/ml/prefixSpan.R\" in the Spark repo.", "question": "Where can I find the full example code for prefixSpan?", "answers": {"text": ["Find full example code at \"examples/src/main/r/ml/prefixSpan.R\" in the Spark repo."], "answer_start": [529]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)"], "answer_start": [46]}}
{"context": "ures\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nML Tuning: model selection and hyperparameter tuning\n\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\wv}{\\mathbf{w}}\n\\newcommand{\\av}{\\mathbf{\\alpha}}\n\\newcommand{\\bv}{\\mathbf{b}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\id}{\\mathbf{I}}\n\\newcommand{\\ind}{\\mathbf{1}}\n\\newcommand{\\0}{\\mathbf{0}}\n\\newcommand{\\unit}{", "question": "Quais tópicos avançados são mencionados no texto?", "answers": {"text": ["Advanced topics"], "answer_start": [121]}}
{"context": "bv}{\\mathbf{b}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\id}{\\mathbf{I}}\n\\newcommand{\\ind}{\\mathbf{1}}\n\\newcommand{\\0}{\\mathbf{0}}\n\\newcommand{\\unit}{\\mathbf{e}}\n\\newcommand{\\one}{\\mathbf{1}}\n\\newcommand{\\zero}{\\mathbf{0}}\n\\]\nThis section describes how to use MLlib’s tooling for tuning ML algorithms and Pipelines.\nBuilt-in Cross-Validation and other tooling allow users to optimize hyperparameters in algorithms and Pipelines.\nTable of contents\nModel selection (a.k.a. hyperparameter tuning)\nCross-Validation\nTrain-Validation Split\nModel selection (a.k.a. hyperparameter tuning)\nAn important task in ML is\nmodel selection\n, or using data to find the best model or parameters for a given task.  This is also called\ntuning\n.\nTuning may be done for individual\nEstimator\ns such as\nLogisticRegression\n, ", "question": "What is model selection also known as?", "answers": {"text": ["tuning"], "answer_start": [280]}}
{"context": "n carefully to maximize parallelism without exceeding cluster resources, and larger values may not always lead to improved performance.  Generally speaking, a value up to 10 should be sufficient for most clusters.\nCross-Validation\nCrossValidator\nbegins by splitting the dataset into a set of\nfolds\nwhich are used as separate training and test datasets. E.g., with\n$k=3$\nfolds,\nCrossValidator\nwill generate 3 (training, test) dataset pairs, each of which uses 2/3 of the data for training and 1/3 for testing.  To evaluate a particular\nParamMap\n,\nCrossValidator\ncomputes the average evaluation metric for the 3\nModel\ns produced by fitting the\nEstimator\non the 3 different (training, test) dataset pairs.\nAfter identifying the best\nParamMap\n,\nCrossValidator\nfinally re-fits the\nEstimator\nusing the best", "question": "How many (training, test) dataset pairs will CrossValidator generate with k=3 folds?", "answers": {"text": ["3 (training, test) dataset pairs, each of which uses 2/3 of the data for training and 1/3 for testing."], "answer_start": [406]}}
{"context": "t can be common to try many more parameters and use more folds (\n$k=3$\nand\n$k=10$\nare common).\nIn other words, using\nCrossValidator\ncan be very expensive.\nHowever, it is also a well-established method for choosing parameters which is more statistically sound than heuristic hand-tuning.\nRefer to the\nCrossValidator\nPython docs\nfor more details on the API.\nfrom\npyspark.ml\nimport\nPipeline\nfrom\npyspark.ml.classification\nimport\nLogisticRegression\nfrom\npyspark.ml.evaluation\nimport\nBinaryClassificationEvaluator\nfrom\npyspark.ml.feature\nimport\nHashingTF\n,\nTokenizer\nfrom\npyspark.ml.tuning\nimport\nCrossValidator\n,\nParamGridBuilder\n# Prepare training documents, which are labeled.\ntraining\n=\nspark\n.\ncreateDataFrame\n([\n(\n0\n,\n\"\na b c d e spark\n\"\n,\n1.0\n),\n(\n1\n,\n\"\nb d\n\"\n,\n0.0\n),\n(\n2\n,\n\"\nspark f g h\n\"\n,\n1.0\n)", "question": "What is a well-established method for choosing parameters that is more statistically sound than heuristic hand-tuning?", "answers": {"text": ["it is also a well-established method for choosing parameters which is more statistically sound than heuristic hand-tuning."], "answer_start": [164]}}
{"context": "ents, which are labeled.\ntraining\n=\nspark\n.\ncreateDataFrame\n([\n(\n0\n,\n\"\na b c d e spark\n\"\n,\n1.0\n),\n(\n1\n,\n\"\nb d\n\"\n,\n0.0\n),\n(\n2\n,\n\"\nspark f g h\n\"\n,\n1.0\n),\n(\n3\n,\n\"\nhadoop mapreduce\n\"\n,\n0.0\n),\n(\n4\n,\n\"\nb spark who\n\"\n,\n1.0\n),\n(\n5\n,\n\"\ng d a y\n\"\n,\n0.0\n),\n(\n6\n,\n\"\nspark fly\n\"\n,\n1.0\n),\n(\n7\n,\n\"\nwas mapreduce\n\"\n,\n0.0\n),\n(\n8\n,\n\"\ne spark program\n\"\n,\n1.0\n),\n(\n9\n,\n\"\na e c l\n\"\n,\n0.0\n),\n(\n10\n,\n\"\nspark compile\n\"\n,\n1.0\n),\n(\n11\n,\n\"\nhadoop software\n\"\n,\n0.0\n)\n],\n[\n\"\nid\n\"\n,\n\"\ntext\n\"\n,\n\"\nlabel\n\"\n])\n# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\ntokenizer\n=\nTokenizer\n(\ninputCol\n=\n\"\ntext\n\"\n,\noutputCol\n=\n\"\nwords\n\"\n)\nhashingTF\n=\nHashingTF\n(\ninputCol\n=\ntokenizer\n.\ngetOutputCol\n(),\noutputCol\n=\n\"\nfeatures\n\"\n)\nlr\n=\nLogisticRegression\n(\nmaxIter\n=\n10\n)\npipeline\n=\nPipeline\n(\nstages\n=\n[", "question": "What are the three stages configured in the ML pipeline?", "answers": {"text": ["tokenizer, hashingTF, and lr"], "answer_start": [536]}}
{"context": "# Make predictions on test documents. cvModel uses the best model found (lrModel).\nprediction\n=\ncvModel\n.\ntransform\n(\ntest\n)\nselected\n=\nprediction\n.\nselect\n(\n\"\nid\n\"\n,\n\"\ntext\n\"\n,\n\"\nprobability\n\"\n,\n\"\nprediction\n\"\n)\nfor\nrow\nin\nselected\n.\ncollect\n():\nprint\n(\nrow\n)\nFind full example code at \"examples/src/main/python/ml/cross_validator.py\" in the Spark repo.\nRefer to the\nCrossValidator\nScala docs\nfor details on the API.\nimport\norg.apache.spark.ml.Pipeline\nimport\norg.apache.spark.ml.classification.LogisticRegression\nimport\norg.apache.spark.ml.evaluation.BinaryClassificationEvaluator\nimport\norg.apache.spark.ml.feature.\n{\nHashingTF\n,\nTokenizer\n}\nimport\norg.apache.spark.ml.linalg.Vector\nimport\norg.apache.spark.ml.tuning.\n{\nCrossValidator\n,\nParamGridBuilder\n}\nimport\norg.apache.spark.sql.Row\n// Prepar", "question": "Where can I find a full example code for this process?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/cross_validator.py\" in the Spark repo."], "answer_start": [261]}}
{"context": "l\"\n)\n// Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\nval\ntokenizer\n=\nnew\nTokenizer\n()\n.\nsetInputCol\n(\n\"text\"\n)\n.\nsetOutputCol\n(\n\"words\"\n)\nval\nhashingTF\n=\nnew\nHashingTF\n()\n.\nsetInputCol\n(\ntokenizer\n.\ngetOutputCol\n)\n.\nsetOutputCol\n(\n\"features\"\n)\nval\nlr\n=\nnew\nLogisticRegression\n()\n.\nsetMaxIter\n(\n10\n)\nval\npipeline\n=\nnew\nPipeline\n()\n.\nsetStages\n(\nArray\n(\ntokenizer\n,\nhashingTF\n,\nlr\n))\n// We use a ParamGridBuilder to construct a grid of parameters to search over.\n// With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n// this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\nval\nparamGrid\n=\nnew\nParamGridBuilder\n()\n.\naddGrid\n(\nhashingTF\n.\nnumFeatures\n,\nArray\n(\n10\n,\n100\n,\n1000\n))\n.\naddGrid\n(\nlr\n.\nregParam\n,\nArr", "question": "What components are used to configure the ML pipeline?", "answers": {"text": ["tokenizer, hashingTF, and lr"], "answer_start": [66]}}
{"context": " choose from.\nval\nparamGrid\n=\nnew\nParamGridBuilder\n()\n.\naddGrid\n(\nhashingTF\n.\nnumFeatures\n,\nArray\n(\n10\n,\n100\n,\n1000\n))\n.\naddGrid\n(\nlr\n.\nregParam\n,\nArray\n(\n0.1\n,\n0.01\n))\n.\nbuild\n()\n// We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n// This will allow us to jointly choose parameters for all Pipeline stages.\n// A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n// Note that the evaluator here is a BinaryClassificationEvaluator and its default metric\n// is areaUnderROC.\nval\ncv\n=\nnew\nCrossValidator\n()\n.\nsetEstimator\n(\npipeline\n)\n.\nsetEvaluator\n(\nnew\nBinaryClassificationEvaluator\n)\n.\nsetEstimatorParamMaps\n(\nparamGrid\n)\n.\nsetNumFolds\n(\n2\n)\n// Use 3+ in practice\n.\nsetParallelism\n(\n2\n)\n// Evaluate up to 2 parameter settings i", "question": "What is the default metric of the BinaryClassificationEvaluator used in the CrossValidator?", "answers": {"text": ["is areaUnderROC."], "answer_start": [528]}}
{"context": "ator\n)\n.\nsetEstimatorParamMaps\n(\nparamGrid\n)\n.\nsetNumFolds\n(\n2\n)\n// Use 3+ in practice\n.\nsetParallelism\n(\n2\n)\n// Evaluate up to 2 parameter settings in parallel\n// Run cross-validation, and choose the best set of parameters.\nval\ncvModel\n=\ncv\n.\nfit\n(\ntraining\n)\n// Prepare test documents, which are unlabeled (id, text) tuples.\nval\ntest\n=\nspark\n.\ncreateDataFrame\n(\nSeq\n(\n(\n4L\n,\n\"spark i j k\"\n),\n(\n5L\n,\n\"l m n\"\n),\n(\n6L\n,\n\"mapreduce spark\"\n),\n(\n7L\n,\n\"apache hadoop\"\n)\n)).\ntoDF\n(\n\"id\"\n,\n\"text\"\n)\n// Make predictions on test documents. cvModel uses the best model found (lrModel).\ncvModel\n.\ntransform\n(\ntest\n)\n.\nselect\n(\n\"id\"\n,\n\"text\"\n,\n\"probability\"\n,\n\"prediction\"\n)\n.\ncollect\n()\n.\nforeach\n{\ncase\nRow\n(\nid\n:\nLong\n,\ntext\n:\nString\n,\nprob\n:\nVector\n,\nprediction\n:\nDouble\n)\n=>\nprintln\n(\ns\n\"($id, $text) --> pr", "question": "What is the number of folds set to for cross-validation?", "answers": {"text": ["2"], "answer_start": [61]}}
{"context": "rediction\"\n)\n.\ncollect\n()\n.\nforeach\n{\ncase\nRow\n(\nid\n:\nLong\n,\ntext\n:\nString\n,\nprob\n:\nVector\n,\nprediction\n:\nDouble\n)\n=>\nprintln\n(\ns\n\"($id, $text) --> prob=$prob, prediction=$prediction\"\n)\n}\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\" in the Spark repo.\nRefer to the\nCrossValidator\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\norg.apache.spark.ml.Pipeline\n;\nimport\norg.apache.spark.ml.PipelineStage\n;\nimport\norg.apache.spark.ml.classification.LogisticRegression\n;\nimport\norg.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n;\nimport\norg.apache.spark.ml.feature.HashingTF\n;\nimport\norg.apache.spark.ml.feature.Tokenizer\n;\nimport\norg.apache.spark.ml.param.ParamMap\n;\nimport\norg.apache.spark.ml", "question": "Where can I find a full example code for ModelSelectionViaCrossValidationExample?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\" in the Spark repo."], "answer_start": [188]}}
{"context": "che.spark.ml.feature.HashingTF\n;\nimport\norg.apache.spark.ml.feature.Tokenizer\n;\nimport\norg.apache.spark.ml.param.ParamMap\n;\nimport\norg.apache.spark.ml.tuning.CrossValidator\n;\nimport\norg.apache.spark.ml.tuning.CrossValidatorModel\n;\nimport\norg.apache.spark.ml.tuning.ParamGridBuilder\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\n// Prepare training documents, which are labeled.\nDataset\n<\nRow\n>\ntraining\n=\nspark\n.\ncreateDataFrame\n(\nArrays\n.\nasList\n(\nnew\nJavaLabeledDocument\n(\n0L\n,\n\"a b c d e spark\"\n,\n1.0\n),\nnew\nJavaLabeledDocument\n(\n1L\n,\n\"b d\"\n,\n0.0\n),\nnew\nJavaLabeledDocument\n(\n2L\n,\n\"spark f g h\"\n,\n1.0\n),\nnew\nJavaLabeledDocument\n(\n3L\n,\n\"hadoop mapreduce\"\n,\n0.0\n),\nnew\nJavaLabeledDocument\n(\n4L\n,\n\"b spark who\"\n,\n1.0\n),\nnew\nJavaLabeledDocument\n(\n5L\n,\n\"g d a y\"\n,\n0.0\n),\nne", "question": "What is being prepared in the provided code snippet?", "answers": {"text": ["Prepare training documents, which are labeled."], "answer_start": [359]}}
{"context": "Document\n(\n3L\n,\n\"hadoop mapreduce\"\n,\n0.0\n),\nnew\nJavaLabeledDocument\n(\n4L\n,\n\"b spark who\"\n,\n1.0\n),\nnew\nJavaLabeledDocument\n(\n5L\n,\n\"g d a y\"\n,\n0.0\n),\nnew\nJavaLabeledDocument\n(\n6L\n,\n\"spark fly\"\n,\n1.0\n),\nnew\nJavaLabeledDocument\n(\n7L\n,\n\"was mapreduce\"\n,\n0.0\n),\nnew\nJavaLabeledDocument\n(\n8L\n,\n\"e spark program\"\n,\n1.0\n),\nnew\nJavaLabeledDocument\n(\n9L\n,\n\"a e c l\"\n,\n0.0\n),\nnew\nJavaLabeledDocument\n(\n10L\n,\n\"spark compile\"\n,\n1.0\n),\nnew\nJavaLabeledDocument\n(\n11L\n,\n\"hadoop software\"\n,\n0.0\n)\n),\nJavaLabeledDocument\n.\nclass\n);\n// Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\nTokenizer\ntokenizer\n=\nnew\nTokenizer\n()\n.\nsetInputCol\n(\n\"text\"\n)\n.\nsetOutputCol\n(\n\"words\"\n);\nHashingTF\nhashingTF\n=\nnew\nHashingTF\n()\n.\nsetNumFeatures\n(\n1000\n)\n.\nsetInputCol\n(\ntokenizer\n.\ngetOutputCo", "question": "What is set as the number of features in the HashingTF stage?", "answers": {"text": ["1000"], "answer_start": [754]}}
{"context": "w\nParamGridBuilder\n()\n.\naddGrid\n(\nhashingTF\n.\nnumFeatures\n(),\nnew\nint\n[]\n{\n10\n,\n100\n,\n1000\n})\n.\naddGrid\n(\nlr\n.\nregParam\n(),\nnew\ndouble\n[]\n{\n0.1\n,\n0.01\n})\n.\nbuild\n();\n// We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n// This will allow us to jointly choose parameters for all Pipeline stages.\n// A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n// Note that the evaluator here is a BinaryClassificationEvaluator and its default metric\n// is areaUnderROC.\nCrossValidator\ncv\n=\nnew\nCrossValidator\n()\n.\nsetEstimator\n(\npipeline\n)\n.\nsetEvaluator\n(\nnew\nBinaryClassificationEvaluator\n())\n.\nsetEstimatorParamMaps\n(\nparamGrid\n)\n.\nsetNumFolds\n(\n2\n)\n// Use 3+ in practice\n.\nsetParallelism\n(\n2\n);\n// Evaluate up to 2 parameter settings i", "question": "What is the default metric of the BinaryClassificationEvaluator used in the CrossValidator?", "answers": {"text": ["is areaUnderROC."], "answer_start": [514]}}
{"context": "r\n())\n.\nsetEstimatorParamMaps\n(\nparamGrid\n)\n.\nsetNumFolds\n(\n2\n)\n// Use 3+ in practice\n.\nsetParallelism\n(\n2\n);\n// Evaluate up to 2 parameter settings in parallel\n// Run cross-validation, and choose the best set of parameters.\nCrossValidatorModel\ncvModel\n=\ncv\n.\nfit\n(\ntraining\n);\n// Prepare test documents, which are unlabeled.\nDataset\n<\nRow\n>\ntest\n=\nspark\n.\ncreateDataFrame\n(\nArrays\n.\nasList\n(\nnew\nJavaDocument\n(\n4L\n,\n\"spark i j k\"\n),\nnew\nJavaDocument\n(\n5L\n,\n\"l m n\"\n),\nnew\nJavaDocument\n(\n6L\n,\n\"mapreduce spark\"\n),\nnew\nJavaDocument\n(\n7L\n,\n\"apache hadoop\"\n)\n),\nJavaDocument\n.\nclass\n);\n// Make predictions on test documents. cvModel uses the best model found (lrModel).\nDataset\n<\nRow\n>\npredictions\n=\ncvModel\n.\ntransform\n(\ntest\n);\nfor\n(\nRow\nr\n:\npredictions\n.\nselect\n(\n\"id\"\n,\n\"text\"\n,\n\"probability\"\n,\n\"pre", "question": "What is used to make predictions on test documents?", "answers": {"text": ["cvModel uses the best model found (lrModel)."], "answer_start": [622]}}
{"context": "found (lrModel).\nDataset\n<\nRow\n>\npredictions\n=\ncvModel\n.\ntransform\n(\ntest\n);\nfor\n(\nRow\nr\n:\npredictions\n.\nselect\n(\n\"id\"\n,\n\"text\"\n,\n\"probability\"\n,\n\"prediction\"\n).\ncollectAsList\n())\n{\nSystem\n.\nout\n.\nprintln\n(\n\"(\"\n+\nr\n.\nget\n(\n0\n)\n+\n\", \"\n+\nr\n.\nget\n(\n1\n)\n+\n\") --> prob=\"\n+\nr\n.\nget\n(\n2\n)\n+\n\", prediction=\"\n+\nr\n.\nget\n(\n3\n));\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\" in the Spark repo.\nTrain-Validation Split\nIn addition to\nCrossValidator\nSpark also offers\nTrainValidationSplit\nfor hyper-parameter tuning.\nTrainValidationSplit\nonly evaluates each combination of parameters once, as opposed to k times in\n the case of\nCrossValidator\n. It is, therefore, less expensive,\n but will not produce as reliable results when the", "question": "Where can I find a full example code for model selection via cross validation?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\" in the Spark repo."], "answer_start": [320]}}
{"context": "t.\nExamples: model selection via train validation split\nRefer to the\nTrainValidationSplit\nPython docs\nfor more details on the API.\nfrom\npyspark.ml.evaluation\nimport\nRegressionEvaluator\nfrom\npyspark.ml.regression\nimport\nLinearRegression\nfrom\npyspark.ml.tuning\nimport\nParamGridBuilder\n,\nTrainValidationSplit\n# Prepare training and test data.\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"\nlibsvm\n\"\n)\n\\\n.\nload\n(\n\"\ndata/mllib/sample_linear_regression_data.txt\n\"\n)\ntrain\n,\ntest\n=\ndata\n.\nrandomSplit\n([\n0.9\n,\n0.1\n],\nseed\n=\n12345\n)\nlr\n=\nLinearRegression\n(\nmaxIter\n=\n10\n)\n# We use a ParamGridBuilder to construct a grid of parameters to search over.\n# TrainValidationSplit will try all combinations of values and determine best model using\n# the evaluator.\nparamGrid\n=\nParamGridBuilder\n()\n\\\n.\naddGrid\n(\nlr\n.\nregParam\n,\n[\n0", "question": "What is used to construct a grid of parameters to search over?", "answers": {"text": ["We use a ParamGridBuilder to construct a grid of parameters to search over."], "answer_start": [552]}}
{"context": " will try all combinations of values and determine best model using\n# the evaluator.\nparamGrid\n=\nParamGridBuilder\n()\n\\\n.\naddGrid\n(\nlr\n.\nregParam\n,\n[\n0.1\n,\n0.01\n])\n\\\n.\naddGrid\n(\nlr\n.\nfitIntercept\n,\n[\nFalse\n,\nTrue\n])\n\\\n.\naddGrid\n(\nlr\n.\nelasticNetParam\n,\n[\n0.0\n,\n0.5\n,\n1.0\n])\n\\\n.\nbuild\n()\n# In this case the estimator is simply the linear regression.\n# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\ntvs\n=\nTrainValidationSplit\n(\nestimator\n=\nlr\n,\nestimatorParamMaps\n=\nparamGrid\n,\nevaluator\n=\nRegressionEvaluator\n(),\n# 80% of the data will be used for training, 20% for validation.\ntrainRatio\n=\n0.8\n)\n# Run TrainValidationSplit, and choose the best set of parameters.\nmodel\n=\ntvs\n.\nfit\n(\ntrain\n)\n# Make predictions on test data. model is the model with combi", "question": "What percentage of the data will be used for training in the TrainValidationSplit?", "answers": {"text": ["80% of the data will be used for training, 20% for validation."], "answer_start": [561]}}
{"context": "rainValidationSplit, and choose the best set of parameters.\nmodel\n=\ntvs\n.\nfit\n(\ntrain\n)\n# Make predictions on test data. model is the model with combination of parameters\n# that performed best.\nmodel\n.\ntransform\n(\ntest\n)\n\\\n.\nselect\n(\n\"\nfeatures\n\"\n,\n\"\nlabel\n\"\n,\n\"\nprediction\n\"\n)\n\\\n.\nshow\n()\nFind full example code at \"examples/src/main/python/ml/train_validation_split.py\" in the Spark repo.\nRefer to the\nTrainValidationSplit\nScala docs\nfor details on the API.\nimport\norg.apache.spark.ml.evaluation.RegressionEvaluator\nimport\norg.apache.spark.ml.regression.LinearRegression\nimport\norg.apache.spark.ml.tuning.\n{\nParamGridBuilder\n,\nTrainValidationSplit\n}\n// Prepare training and test data.\nval\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_linear_regression_data.txt\"\n)\nval\nArray\n", "question": "Where can I find a full example code for train validation split?", "answers": {"text": ["Find full example code at \"examples/src/main/python/ml/train_validation_split.py\" in the Spark repo."], "answer_start": [290]}}
{"context": "}\n// Prepare training and test data.\nval\ndata\n=\nspark\n.\nread\n.\nformat\n(\n\"libsvm\"\n).\nload\n(\n\"data/mllib/sample_linear_regression_data.txt\"\n)\nval\nArray\n(\ntraining\n,\ntest\n)\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.9\n,\n0.1\n),\nseed\n=\n12345\n)\nval\nlr\n=\nnew\nLinearRegression\n()\n.\nsetMaxIter\n(\n10\n)\n// We use a ParamGridBuilder to construct a grid of parameters to search over.\n// TrainValidationSplit will try all combinations of values and determine best model using\n// the evaluator.\nval\nparamGrid\n=\nnew\nParamGridBuilder\n()\n.\naddGrid\n(\nlr\n.\nregParam\n,\nArray\n(\n0.1\n,\n0.01\n))\n.\naddGrid\n(\nlr\n.\nfitIntercept\n)\n.\naddGrid\n(\nlr\n.\nelasticNetParam\n,\nArray\n(\n0.0\n,\n0.5\n,\n1.0\n))\n.\nbuild\n()\n// In this case the estimator is simply the linear regression.\n// A TrainValidationSplit requires an Estimator, a set of Estimator Para", "question": "What is used to construct a grid of parameters to search over?", "answers": {"text": ["We use a ParamGridBuilder to construct a grid of parameters to search over."], "answer_start": [285]}}
{"context": "\n))\n.\nbuild\n()\n// In this case the estimator is simply the linear regression.\n// A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\nval\ntrainValidationSplit\n=\nnew\nTrainValidationSplit\n()\n.\nsetEstimator\n(\nlr\n)\n.\nsetEvaluator\n(\nnew\nRegressionEvaluator\n)\n.\nsetEstimatorParamMaps\n(\nparamGrid\n)\n// 80% of the data will be used for training and the remaining 20% for validation.\n.\nsetTrainRatio\n(\n0.8\n)\n// Evaluate up to 2 parameter settings in parallel\n.\nsetParallelism\n(\n2\n)\n// Run train validation split, and choose the best set of parameters.\nval\nmodel\n=\ntrainValidationSplit\n.\nfit\n(\ntraining\n)\n// Make predictions on test data. model is the model with combination of parameters\n// that performed best.\nmodel\n.\ntransform\n(\ntest\n)\n.\nselect\n(\n\"features\"\n,\n\"labe", "question": "What is required by a TrainValidationSplit?", "answers": {"text": ["A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator."], "answer_start": [81]}}
{"context": "tions on test data. model is the model with combination of parameters\n// that performed best.\nmodel\n.\ntransform\n(\ntest\n)\n.\nselect\n(\n\"features\"\n,\n\"label\"\n,\n\"prediction\"\n)\n.\nshow\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\" in the Spark repo.\nRefer to the\nTrainValidationSplit\nJava docs\nfor details on the API.\nimport\norg.apache.spark.ml.evaluation.RegressionEvaluator\n;\nimport\norg.apache.spark.ml.param.ParamMap\n;\nimport\norg.apache.spark.ml.regression.LinearRegression\n;\nimport\norg.apache.spark.ml.tuning.ParamGridBuilder\n;\nimport\norg.apache.spark.ml.tuning.TrainValidationSplit\n;\nimport\norg.apache.spark.ml.tuning.TrainValidationSplitModel\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Ro", "question": "Where can I find a full example code for model selection?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\" in the Spark repo."], "answer_start": [180]}}
{"context": "inValidationSplit\n;\nimport\norg.apache.spark.ml.tuning.TrainValidationSplitModel\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nDataset\n<\nRow\n>\ndata\n=\nspark\n.\nread\n().\nformat\n(\n\"libsvm\"\n)\n.\nload\n(\n\"data/mllib/sample_linear_regression_data.txt\"\n);\n// Prepare training and test data.\nDataset\n<\nRow\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]\n{\n0.9\n,\n0.1\n},\n12345\n);\nDataset\n<\nRow\n>\ntraining\n=\nsplits\n[\n0\n];\nDataset\n<\nRow\n>\ntest\n=\nsplits\n[\n1\n];\nLinearRegression\nlr\n=\nnew\nLinearRegression\n();\n// We use a ParamGridBuilder to construct a grid of parameters to search over.\n// TrainValidationSplit will try all combinations of values and determine best model using\n// the evaluator.\nParamMap\n[]\nparamGrid\n=\nnew\nParamGridBuilder\n()\n.\naddGrid\n(\nlr\n.\nregParam\n(),\nnew\ndouble\n[]\n{", "question": "What is used to construct a grid of parameters to search over?", "answers": {"text": ["ParamGridBuilder"], "answer_start": [531]}}
{"context": " values and determine best model using\n// the evaluator.\nParamMap\n[]\nparamGrid\n=\nnew\nParamGridBuilder\n()\n.\naddGrid\n(\nlr\n.\nregParam\n(),\nnew\ndouble\n[]\n{\n0.1\n,\n0.01\n})\n.\naddGrid\n(\nlr\n.\nfitIntercept\n())\n.\naddGrid\n(\nlr\n.\nelasticNetParam\n(),\nnew\ndouble\n[]\n{\n0.0\n,\n0.5\n,\n1.0\n})\n.\nbuild\n();\n// In this case the estimator is simply the linear regression.\n// A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\nTrainValidationSplit\ntrainValidationSplit\n=\nnew\nTrainValidationSplit\n()\n.\nsetEstimator\n(\nlr\n)\n.\nsetEvaluator\n(\nnew\nRegressionEvaluator\n())\n.\nsetEstimatorParamMaps\n(\nparamGrid\n)\n.\nsetTrainRatio\n(\n0.8\n)\n// 80% for training and the remaining 20% for validation\n.\nsetParallelism\n(\n2\n);\n// Evaluate up to 2 parameter settings in parallel\n// Run train validation ", "question": "What ratio is used for training and validation in the TrainValidationSplit?", "answers": {"text": ["80% for training and the remaining 20% for validation"], "answer_start": [647]}}
{"context": " for training and the remaining 20% for validation\n.\nsetParallelism\n(\n2\n);\n// Evaluate up to 2 parameter settings in parallel\n// Run train validation split, and choose the best set of parameters.\nTrainValidationSplitModel\nmodel\n=\ntrainValidationSplit\n.\nfit\n(\ntraining\n);\n// Make predictions on test data. model is the model with combination of parameters\n// that performed best.\nmodel\n.\ntransform\n(\ntest\n)\n.\nselect\n(\n\"features\"\n,\n\"label\"\n,\n\"prediction\"\n)\n.\nshow\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\" in the Spark repo.", "question": "What is the purpose of the `setParallelism(2)` function?", "answers": {"text": ["// Evaluate up to 2 parameter settings in parallel"], "answer_start": [75]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars"], "answer_start": [46]}}
{"context": "ures\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nAdvanced topics\nOptimization of linear methods (developer)\nLimited-memory BFGS (L-BFGS)\nNormal equation solver for weighted least squares\nIteratively reweighted least squares (IRLS)\n\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}} \n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\wv}{\\mathbf{w}}\n\\newcommand{\\av}{\\mathbf{\\alpha}}\n\\newcommand{\\bv}{\\mathbf{b}}\n\\new", "question": "What are some of the advanced topics covered in the text?", "answers": {"text": ["Advanced topics"], "answer_start": [121]}}
{"context": ",\nAFTSurvivalRegression\nand\nMultilayerPerceptronClassifier\n.\nMLlib L-BFGS solver calls the corresponding implementation in\nbreeze\n.\nNormal equation solver for weighted least squares\nMLlib implements normal equation solver for\nweighted least squares\nby\nWeightedLeastSquares\n.\nGiven $n$ weighted observations $(w_i, a_i, b_i)$:\n$w_i$ the weight of i-th observation\n$a_i$ the features vector of i-th observation\n$b_i$ the label of i-th observation\nThe number of features for each observation is $m$. We use the following weighted least squares formulation:\n\\[   \n\\min_{\\mathbf{x}}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(\\mathbf{a}_i^T \\mathbf{x} -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{\\lambda}{\\delta}\\left[\\frac{1}{2}(1 - \\alpha)\\sum_{j=1}^m(\\sigma_j x_j)^2 + \\alpha\\sum_{j=1}^m |\\sigma_j x_j|\\right]\n\\]\nwhere $", "question": "What does $w_i$ represent in the weighted least squares formulation?", "answers": {"text": ["$w_i$ the weight of i-th observation"], "answer_start": [326]}}
{"context": "m_{k=1}^n w_k} + \\frac{\\lambda}{\\delta}\\left[\\frac{1}{2}(1 - \\alpha)\\sum_{j=1}^m(\\sigma_j x_j)^2 + \\alpha\\sum_{j=1}^m |\\sigma_j x_j|\\right]\n\\]\nwhere $\\lambda$ is the regularization parameter, $\\alpha$ is the elastic-net mixing parameter, $\\delta$ is the population standard deviation of the label\nand $\\sigma_j$ is the population standard deviation of the j-th feature column.\nThis objective function requires only one pass over the data to collect the statistics necessary to solve it. For an\n$n \\times m$ data matrix, these statistics require only $O(m^2)$ storage and so can be stored on a single machine when $m$ (the number of features) is\nrelatively small. We can then solve the normal equations on a single machine using local methods like direct Cholesky factorization or iterative optimizati", "question": "What is the storage requirement for the statistics needed to solve the objective function for an n x m data matrix?", "answers": {"text": ["these statistics require only $O(m^2)$ storage"], "answer_start": [520]}}
{"context": "ively small. We can then solve the normal equations on a single machine using local methods like direct Cholesky factorization or iterative optimization programs.\nSpark MLlib currently supports two types of solvers for the normal equations: Cholesky factorization and Quasi-Newton methods (L-BFGS/OWL-QN). Cholesky factorization\ndepends on a positive definite covariance matrix (i.e. columns of the data matrix must be linearly independent) and will fail if this condition is violated. Quasi-Newton methods\nare still capable of providing a reasonable solution even when the covariance matrix is not positive definite, so the normal equation solver can also fall back to \nQuasi-Newton methods in this case. This fallback is currently always enabled for the\nLinearRegression\nand\nGeneralizedLinearRegres", "question": "What are the two types of solvers supported by Spark MLlib for the normal equations?", "answers": {"text": ["Cholesky factorization and Quasi-Newton methods (L-BFGS/OWL-QN)."], "answer_start": [241]}}
{"context": "n also fall back to \nQuasi-Newton methods in this case. This fallback is currently always enabled for the\nLinearRegression\nand\nGeneralizedLinearRegression\nestimators.\nWeightedLeastSquares\nsupports L1, L2, and elastic-net regularization and provides options to enable or disable regularization and standardization. In the case where no \nL1 regularization is applied (i.e. $\\alpha = 0$), there exists an analytical solution and either Cholesky or Quasi-Newton solver may be used. When $\\alpha > 0$ no analytical \nsolution exists and we instead use the Quasi-Newton solver to find the coefficients iteratively.\nIn order to make the normal equation approach efficient,\nWeightedLeastSquares\nrequires that the number of features is no more than 4096. For larger problems, use L-BFGS instead.\nIteratively re", "question": "What solver is used when no L1 regularization is applied in WeightedLeastSquares?", "answers": {"text": ["either Cholesky or Quasi-Newton solver may be used."], "answer_start": [426]}}
{"context": "signed to stop by itself if isStopped() returns false\n}\n/** Create a socket connection and receive data until receiver is stopped */\nprivate\ndef\nreceive\n()\n{\nvar\nsocket\n:\nSocket\n=\nnull\nvar\nuserInput\n:\nString\n=\nnull\ntry\n{\n// Connect to host:port\nsocket\n=\nnew\nSocket\n(\nhost\n,\nport\n)\n// Until stopped or connection broken continue reading\nval\nreader\n=\nnew\nBufferedReader\n(\nnew\nInputStreamReader\n(\nsocket\n.\ngetInputStream\n(),\nStandardCharsets\n.\nUTF_8\n))\nuserInput\n=\nreader\n.\nreadLine\n()\nwhile\n(!\nisStopped\n&&\nuserInput\n!=\nnull\n)\n{\nstore\n(\nuserInput\n)\nuserInput\n=\nreader\n.\nreadLine\n()\n}\nreader\n.\nclose\n()\nsocket\n.\nclose\n()\n// Restart in an attempt to connect again when server is active again\nrestart\n(\n\"Trying to connect again\"\n)\n}\ncatch\n{\ncase\ne\n:\njava.net.ConnectException\n=>\n// restart if could not co", "question": "What happens when the receiver is stopped or the connection is broken?", "answers": {"text": ["Until stopped or connection broken continue reading"], "answer_start": [284]}}
{"context": "nection\nnew\nThread\n(\nthis\n::\nreceive\n).\nstart\n();\n}\n@Override\npublic\nvoid\nonStop\n()\n{\n// There is nothing much to do as the thread calling receive()\n// is designed to stop by itself if isStopped() returns false\n}\n/** Create a socket connection and receive data until receiver is stopped */\nprivate\nvoid\nreceive\n()\n{\nSocket\nsocket\n=\nnull\n;\nString\nuserInput\n=\nnull\n;\ntry\n{\n// connect to the server\nsocket\n=\nnew\nSocket\n(\nhost\n,\nport\n);\nBufferedReader\nreader\n=\nnew\nBufferedReader\n(\nnew\nInputStreamReader\n(\nsocket\n.\ngetInputStream\n(),\nStandardCharsets\n.\nUTF_8\n));\n// Until stopped or connection broken continue reading\nwhile\n(!\nisStopped\n()\n&&\n(\nuserInput\n=\nreader\n.\nreadLine\n())\n!=\nnull\n)\n{\nSystem\n.\nout\n.\nprintln\n(\n\"Received data '\"\n+\nuserInput\n+\n\"'\"\n);\nstore\n(\nuserInput\n);\n}\nreader\n.\nclose\n();\nsocket\n", "question": "What happens when the receiver is stopped or the connection is broken?", "answers": {"text": ["Until stopped or connection broken continue reading"], "answer_start": [562]}}
{"context": "\n=\nreader\n.\nreadLine\n())\n!=\nnull\n)\n{\nSystem\n.\nout\n.\nprintln\n(\n\"Received data '\"\n+\nuserInput\n+\n\"'\"\n);\nstore\n(\nuserInput\n);\n}\nreader\n.\nclose\n();\nsocket\n.\nclose\n();\n// Restart in an attempt to connect again when server is active again\nrestart\n(\n\"Trying to connect again\"\n);\n}\ncatch\n(\nConnectException\nce\n)\n{\n// restart if could not connect to server\nrestart\n(\n\"Could not connect\"\n,\nce\n);\n}\ncatch\n(\nThrowable\nt\n)\n{\n// restart if there is any other error\nrestart\n(\n\"Error receiving data\"\n,\nt\n);\n}\n}\n}\nUsing the custom receiver in a Spark Streaming application\nThe custom receiver can be used in a Spark Streaming application by using\nstreamingContext.receiverStream(<instance of custom receiver>)\n. This will create\nan input DStream using data received by the instance of custom receiver, as shown below:\n", "question": "How can a custom receiver be used in a Spark Streaming application?", "answers": {"text": ["streamingContext.receiverStream(<instance of custom receiver>)"], "answer_start": [629]}}
{"context": "g the receiving rates if the rate limits have been specified.\nBecause of these two, unreliable receivers are simpler to implement than reliable receivers.\nThe following table summarizes the characteristics of both types of receivers\nReceiver Type\nCharacteristics\nUnreliable Receivers\nSimple to implement.\nSystem takes care of block generation and rate control.\n    No fault-tolerance guarantees, can lose data on receiver failure.\nReliable Receivers\nStrong fault-tolerance guarantees, can ensure zero data loss.\nBlock generation and rate control to be handled by the receiver implementation.\nImplementation complexity depends on the acknowledgement mechanisms of the source.", "question": "What is a characteristic of unreliable receivers?", "answers": {"text": ["Simple to implement."], "answer_start": [284]}}
{"context": "echanisms of the source.", "question": "What are being discussed regarding the source?", "answers": {"text": ["echanisms of the source."], "answer_start": [0]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark Streaming + Kafka Integration Guide (Kafka broker version 0.10.0 or higher)\nThe Spark Streaming integration for Kafka 0.10 provides simple parallelism, 1:1 correspondence between Kafka \npartitions and Spark partitions, and access to offsets and ", "question": "What Kafka broker version is mentioned in the Spark Streaming + Kafka Integration Guide?", "answers": {"text": ["Kafka broker version 0.10.0 or higher"], "answer_start": [592]}}
{"context": " integration for Kafka 0.10 provides simple parallelism, 1:1 correspondence between Kafka \npartitions and Spark partitions, and access to offsets and metadata. However, because the newer integration uses \nthe\nnew Kafka consumer API\ninstead of the simple API, \nthere are notable differences in usage.\nLinking\nFor Scala/Java applications using SBT/Maven project definitions, link your streaming application with the following artifact (see\nLinking section\nin the main programming guide for further information).\ngroupId = org.apache.spark\nartifactId = spark-streaming-kafka-0-10_2.13\nversion = 4.0.0\nDo not\nmanually add dependencies on\norg.apache.kafka\nartifacts (e.g.\nkafka-clients\n).  The\nspark-streaming-kafka-0-10\nartifact has the appropriate transitive dependencies already, and different versions", "question": "What artifact should Scala/Java applications using SBT/Maven link with?", "answers": {"text": ["artifactId = spark-streaming-kafka-0-10_2.13"], "answer_start": [537]}}
{"context": "\nartifacts (e.g.\nkafka-clients\n).  The\nspark-streaming-kafka-0-10\nartifact has the appropriate transitive dependencies already, and different versions may be incompatible in hard to diagnose ways.\nCreating a Direct Stream\nNote that the namespace for the import includes the version, org.apache.spark.streaming.kafka010\nimport\norg.apache.kafka.clients.consumer.ConsumerRecord\nimport\norg.apache.kafka.common.serialization.StringDeserializer\nimport\norg.apache.spark.streaming.kafka010._\nimport\norg.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\nimport\norg.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\nval\nkafkaParams\n=\nMap\n[\nString\n,\nObject\n](\n\"bootstrap.servers\"\n->\n\"localhost:9092,anotherhost:9092\"\n,\n\"key.deserializer\"\n->\nclassOf\n[\nStringDeserializer\n],\n\"value.de", "question": "What namespace should be used for importing Kafka 0.10 related classes?", "answers": {"text": ["org.apache.spark.streaming.kafka010"], "answer_start": [283]}}
{"context": "Deserializer\n.\nclass\n);\nkafkaParams\n.\nput\n(\n\"value.deserializer\"\n,\nStringDeserializer\n.\nclass\n);\nkafkaParams\n.\nput\n(\n\"group.id\"\n,\n\"use_a_separate_group_id_for_each_stream\"\n);\nkafkaParams\n.\nput\n(\n\"auto.offset.reset\"\n,\n\"latest\"\n);\nkafkaParams\n.\nput\n(\n\"enable.auto.commit\"\n,\nfalse\n);\nCollection\n<\nString\n>\ntopics\n=\nArrays\n.\nasList\n(\n\"topicA\"\n,\n\"topicB\"\n);\nJavaInputDStream\n<\nConsumerRecord\n<\nString\n,\nString\n>>\nstream\n=\nKafkaUtils\n.\ncreateDirectStream\n(\nstreamingContext\n,\nLocationStrategies\n.\nPreferConsistent\n(),\nConsumerStrategies\n.<\nString\n,\nString\n>\nSubscribe\n(\ntopics\n,\nkafkaParams\n)\n);\nstream\n.\nmapToPair\n(\nrecord\n->\nnew\nTuple2\n<>(\nrecord\n.\nkey\n(),\nrecord\n.\nvalue\n()));\nFor possible kafkaParams, see\nKafka consumer config docs\n.\nIf your Spark batch duration is larger than the default Kafka heart", "question": "What is set as the value for \"auto.offset.reset\" in the kafkaParams?", "answers": {"text": ["\"latest\""], "answer_start": [217]}}
{"context": "),\nrecord\n.\nvalue\n()));\nFor possible kafkaParams, see\nKafka consumer config docs\n.\nIf your Spark batch duration is larger than the default Kafka heartbeat session timeout (30 seconds), increase heartbeat.interval.ms and session.timeout.ms appropriately.  For batches larger than 5 minutes, this will require changing group.max.session.timeout.ms on the broker.\nNote that the example sets enable.auto.commit to false, for discussion see\nStoring Offsets\nbelow.\nLocationStrategies\nThe new Kafka consumer API will pre-fetch messages into buffers.  Therefore it is important for performance reasons that the Spark integration keep cached consumers on executors (rather than recreating them for each batch), and prefer to schedule partitions on the host locations that have the appropriate consumers.\nIn mo", "question": "What should be increased if your Spark batch duration is larger than the default Kafka heartbeat session timeout (30 seconds)?", "answers": {"text": ["increase heartbeat.interval.ms and session.timeout.ms appropriately."], "answer_start": [185]}}
{"context": "e a consistent location).\nThe cache for consumers has a default maximum size of 64.  If you expect to be handling more than (64 * number of executors) Kafka partitions, you can change this setting via\nspark.streaming.kafka.consumer.cache.maxCapacity\n.\nIf you would like to disable the caching for Kafka consumers, you can set\nspark.streaming.kafka.consumer.cache.enabled\nto\nfalse\n.\nThe cache is keyed by topicpartition and group.id, so use a\nseparate\ngroup.id\nfor each call to\ncreateDirectStream\n.\nConsumerStrategies\nThe new Kafka consumer API has a number of different ways to specify topics, some of which require considerable post-object-instantiation setup.\nConsumerStrategies\nprovides an abstraction that allows Spark to obtain properly configured consumers even after restart from checkpoint.\nC", "question": "How can you disable the caching for Kafka consumers?", "answers": {"text": ["you can set\nspark.streaming.kafka.consumer.cache.enabled\nto\nfalse"], "answer_start": [314]}}
{"context": "tion setup.\nConsumerStrategies\nprovides an abstraction that allows Spark to obtain properly configured consumers even after restart from checkpoint.\nConsumerStrategies.Subscribe\n, as shown above, allows you to subscribe to a fixed collection of topics.\nSubscribePattern\nallows you to use a regex to specify topics of interest. Note that unlike the 0.8 integration, using\nSubscribe\nor\nSubscribePattern\nshould respond to adding partitions during a running stream. Finally,\nAssign\nallows you to specify a fixed collection of partitions.  All three strategies have overloaded constructors that allow you to specify the starting offset for a particular partition.\nIf you have specific consumer setup needs that are not met by the options above,\nConsumerStrategy\nis a public class that you can extend.\nCrea", "question": "What does ConsumerStrategies allow Spark to do?", "answers": {"text": ["provides an abstraction that allows Spark to obtain properly configured consumers even after restart from checkpoint."], "answer_start": [31]}}
{"context": "ntilOffset\n());\n});\n});\nNote that the typecast to\nHasOffsetRanges\nwill only succeed if it is done in the first method called on the result of\ncreateDirectStream\n, not later down a chain of methods. Be aware that the one-to-one mapping between RDD partition and Kafka partition does not remain after any methods that shuffle or repartition, e.g. reduceByKey() or window().\nStoring Offsets\nKafka delivery semantics in the case of failure depend on how and when offsets are stored.  Spark output operations are\nat-least-once\n.  So if you want the equivalent of exactly-once semantics, you must either store offsets after an idempotent output, or store offsets in an atomic transaction alongside output. With this integration, you have 3 options, in order of increasing reliability (and code complexity),", "question": "What is required to achieve exactly-once semantics with Spark output operations?", "answers": {"text": ["you must either store offsets after an idempotent output, or store offsets in an atomic transaction alongside output."], "answer_start": [582]}}
{"context": "ffsets in an atomic transaction alongside output. With this integration, you have 3 options, in order of increasing reliability (and code complexity), for how to store offsets.\nCheckpoints\nIf you enable Spark\ncheckpointing\n, offsets will be stored in the checkpoint.  This is easy to enable, but there are drawbacks. Your output operation must be idempotent, since you will get repeated outputs; transactions are not an option.  Furthermore, you cannot recover from a checkpoint if your application code has changed.  For planned upgrades, you can mitigate this by running the new code at the same time as the old code (since outputs need to be idempotent anyway, they should not clash).  But for unplanned failures that require code changes, you will lose data unless you have another way to identif", "question": "What is a drawback of using Spark checkpointing for storing offsets?", "answers": {"text": ["Your output operation must be idempotent, since you will get repeated outputs; transactions are not an option."], "answer_start": [317]}}
{"context": "otent anyway, they should not clash).  But for unplanned failures that require code changes, you will lose data unless you have another way to identify known good starting offsets.\nKafka itself\nKafka has an offset commit API that stores offsets in a special Kafka topic.  By default, the new consumer will periodically auto-commit offsets. This is almost certainly not what you want, because messages successfully polled by the consumer may not yet have resulted in a Spark output operation, resulting in undefined semantics. This is why the stream example above sets “enable.auto.commit” to false.  However, you can commit offsets to Kafka after you know your output has been stored, using the\ncommitAsync\nAPI. The benefit as compared to checkpoints is that Kafka is a durable store regardless of ch", "question": "What does Kafka have that stores offsets?", "answers": {"text": ["Kafka has an offset commit API that stores offsets in a special Kafka topic."], "answer_start": [194]}}
{"context": " know your output has been stored, using the\ncommitAsync\nAPI. The benefit as compared to checkpoints is that Kafka is a durable store regardless of changes to your application code.  However, Kafka is not transactional, so your outputs must still be idempotent.\nstream\n.\nforeachRDD\n{\nrdd\n=>\nval\noffsetRanges\n=\nrdd\n.\nasInstanceOf\n[\nHasOffsetRanges\n].\noffsetRanges\n// some time later, after outputs have completed\nstream\n.\nasInstanceOf\n[\nCanCommitOffsets\n].\ncommitAsync\n(\noffsetRanges\n)\n}\nAs with HasOffsetRanges, the cast to CanCommitOffsets will only succeed if called on the result of createDirectStream, not after transformations.  The commitAsync call is threadsafe, but must occur after outputs if you want meaningful semantics.\nstream\n.\nforeachRDD\n(\nrdd\n->\n{\nOffsetRange\n[]\noffsetRanges\n=\n((\nHas", "question": "What is a benefit of using the commitAsync API compared to checkpoints?", "answers": {"text": ["The benefit as compared to checkpoints is that Kafka is a durable store regardless of changes to your application code."], "answer_start": [62]}}
{"context": "call is threadsafe, but must occur after outputs if you want meaningful semantics.\nstream\n.\nforeachRDD\n(\nrdd\n->\n{\nOffsetRange\n[]\noffsetRanges\n=\n((\nHasOffsetRanges\n)\nrdd\n.\nrdd\n()).\noffsetRanges\n();\n// some time later, after outputs have completed\n((\nCanCommitOffsets\n)\nstream\n.\ninputDStream\n()).\ncommitAsync\n(\noffsetRanges\n);\n});\nYour own data store\nFor data stores that support transactions, saving offsets in the same transaction as the results can keep the two in sync, even in failure situations.  If you’re careful about detecting repeated or skipped offset ranges, rolling back the transaction prevents duplicated or lost messages from affecting results.  This gives the equivalent of exactly-once semantics.  It is also possible to use this tactic even for outputs that result from aggregations", "question": "What can saving offsets in the same transaction as the results do for data stores that support transactions?", "answers": {"text": ["saving offsets in the same transaction as the results can keep the two in sync, even in failure situations."], "answer_start": [392]}}
{"context": "ectory/kafka.client.truststore.jks\"\n,\n\"ssl.truststore.password\"\n->\n\"test1234\"\n,\n\"ssl.keystore.location\"\n->\n\"/some-directory/kafka.client.keystore.jks\"\n,\n\"ssl.keystore.password\"\n->\n\"test1234\"\n,\n\"ssl.key.password\"\n->\n\"test1234\"\n)\nMap\n<\nString\n,\nObject\n>\nkafkaParams\n=\nnew\nHashMap\n<\nString\n,\nObject\n>();\n// the usual params, make sure to change the port in bootstrap.servers if 9092 is not TLS\nkafkaParams\n.\nput\n(\n\"security.protocol\"\n,\n\"SSL\"\n);\nkafkaParams\n.\nput\n(\n\"ssl.truststore.location\"\n,\n\"/some-directory/kafka.client.truststore.jks\"\n);\nkafkaParams\n.\nput\n(\n\"ssl.truststore.password\"\n,\n\"test1234\"\n);\nkafkaParams\n.\nput\n(\n\"ssl.keystore.location\"\n,\n\"/some-directory/kafka.client.keystore.jks\"\n);\nkafkaParams\n.\nput\n(\n\"ssl.keystore.password\"\n,\n\"test1234\"\n);\nkafkaParams\n.\nput\n(\n\"ssl.key.password\"\n,\n\"test", "question": "What is the value of the 'ssl.key.password' parameter?", "answers": {"text": ["test"], "answer_start": [68]}}
{"context": "ain programming guide).\nSecurity\nSee\nStructured Streaming Security\n.\nAdditional Caveats\nKafka native sink is not available so delegation token used only on consumer side.", "question": "What limitation exists regarding the Kafka native sink?", "answers": {"text": ["Kafka native sink is not available so delegation token used only on consumer side."], "answer_start": [88]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark Streaming + Kinesis Integration\nAmazon Kinesis\nis a fully managed service for real-time processing of streaming data at massive scale.\nThe Kinesis receiver creates an input DStream using the Kinesis Client Library (KCL) provided by Amazon under ", "question": "What does the Kinesis receiver create?", "answers": {"text": ["The Kinesis receiver creates an input DStream"], "answer_start": [690]}}
{"context": "ing of streaming data at massive scale.\nThe Kinesis receiver creates an input DStream using the Kinesis Client Library (KCL) provided by Amazon under the Amazon Software License (ASL).\nThe KCL builds on top of the Apache 2.0 licensed AWS Java SDK and provides load-balancing, fault-tolerance, checkpointing through the concepts of Workers, Checkpoints, and Shard Leases.\nHere we explain how to configure Spark Streaming to receive data from Kinesis.\nConfiguring Kinesis\nA Kinesis stream can be set up at one of the valid Kinesis endpoints with 1 or more shards per the following\nguide\n.\nConfiguring Spark Streaming Application\nLinking:\nFor Scala/Java applications using SBT/Maven project definitions, link your streaming application against the following artifact (see\nLinking section\nin the main pro", "question": "What library does the Kinesis receiver use to create an input DStream?", "answers": {"text": ["Kinesis Client Library (KCL)"], "answer_start": [96]}}
{"context": " applications using SBT/Maven project definitions, link your streaming application against the following artifact (see\nLinking section\nin the main programming guide for further information).\ngroupId = org.apache.spark\n artifactId = spark-streaming-kinesis-asl_2.13\n version = 4.0.0\nFor Python applications, you will have to add this above library and its dependencies when deploying your application. See the\nDeploying\nsubsection below.\nNote that by linking to this library, you will include\nASL\n-licensed code in your application.\nProgramming:\nIn the streaming application code, import\nKinesisInputDStream\nand create the input DStream of byte array as follows:\nfrom\npyspark.streaming.kinesis\nimport\nKinesisUtils\n,\nInitialPositionInStream\nkinesisStream\n=\nKinesisUtils\n.\ncreateStream\n(\nstreamingContex", "question": "What artifact should streaming applications using SBT/Maven project definitions be linked against?", "answers": {"text": ["groupId = org.apache.spark\n artifactId = spark-streaming-kinesis-asl_2.13\n version = 4.0.0"], "answer_start": [191]}}
{"context": "me\n])\n.\ncheckpointInterval\n([\ncheckpoint\ninterval\n])\n.\nmetricsLevel\n([\nmetricsLevel\n.\nDETAILED\n])\n.\nstorageLevel\n(\nStorageLevel\n.\nMEMORY_AND_DISK_2\n)\n.\nbuild\n();\nSee the\nAPI docs\nand the\nexample\n. Refer to the\nRunning the Example\nsubsection for instructions to run the example.\nYou may also provide the following settings. This is currently only supported in Scala and Java.\nA “message handler function” that takes a Kinesis\nRecord\nand returns a generic object\nT\n, in case you would like to use other data included in a\nRecord\nsuch as partition key.\nimport\ncollection.JavaConverters._\nimport\norg.apache.spark.storage.StorageLevel\nimport\norg.apache.spark.streaming.kinesis.KinesisInputDStream\nimport\norg.apache.spark.streaming.\n{\nSeconds\n,\nStreamingContext\n}\nimport\norg.apache.spark.streaming.kinesis.", "question": "What storage level is used in the provided code snippet?", "answers": {"text": ["MEMORY_AND_DISK_2"], "answer_start": [130]}}
{"context": ")\n.\nstorageLevel\n(\nStorageLevel\n.\nMEMORY_AND_DISK_2\n)\n.\nmetricsLevel\n(\nMetricsLevel\n.\nDETAILED\n)\n.\nmetricsEnabledDimensions\n(\nKinesisClientLibConfiguration\n.\nDEFAULT_METRICS_ENABLED_DIMENSIONS\n.\nasScala\n.\ntoSet\n)\n.\nbuildWithMessageHandler\n([\nmessage\nhandler\n])\nimport\norg.apache.spark.storage.StorageLevel\n;\nimport\norg.apache.spark.streaming.kinesis.KinesisInputDStream\n;\nimport\norg.apache.spark.streaming.Seconds\n;\nimport\norg.apache.spark.streaming.StreamingContext\n;\nimport\norg.apache.spark.streaming.kinesis.KinesisInitialPositions\n;\nimport\ncom.amazonaws.services.kinesis.clientlibrary.lib.worker.KinesisClientLibConfiguration\n;\nimport\ncom.amazonaws.services.kinesis.metrics.interfaces.MetricsLevel\n;\nimport\nscala.collection.JavaConverters\n;\nKinesisInputDStream\n<\nbyte\n[]>\nkinesisStream\n=\nKinesisI", "question": "What is the storage level used in the configuration?", "answers": {"text": ["MEMORY_AND_DISK_2"], "answer_start": [34]}}
{"context": "_ENABLED_DIMENSIONS\n)\n.\nasScala\n().\ntoSet\n()\n)\n.\nbuildWithMessageHandler\n([\nmessage\nhandler\n]);\nstreamingContext\n: StreamingContext containing an application name used by Kinesis to tie this Kinesis application to the Kinesis stream\n[Kinesis app name]\n: The application name that will be used to checkpoint the Kinesis\n  sequence numbers in DynamoDB table.\nThe application name must be unique for a given account and region.\nIf the table exists but has incorrect checkpoint information (for a different stream, or\n  old expired sequenced numbers), then there may be temporary errors.\n[Kinesis stream name]\n: The Kinesis stream that this streaming application will pull data from.\n[endpoint URL]\n: Valid Kinesis endpoints URL can be found\nhere\n.\n[region name]\n: Valid Kinesis region names can be found", "question": "What is the purpose of the application name used by Kinesis?", "answers": {"text": ["containing an application name used by Kinesis to tie this Kinesis application to the Kinesis stream"], "answer_start": [132]}}
{"context": "s ordered per partition and occurs at-least once per message.\nMultiple applications can read from the same Kinesis stream.  Kinesis will maintain the application-specific shard and checkpoint info in DynamoDB.\nA single Kinesis stream shard is processed by one input DStream at a time.\nA single Kinesis input DStream can read from multiple shards of a Kinesis stream by creating multiple KinesisRecordProcessor threads.\nMultiple input DStreams running in separate processes/instances can read from a Kinesis stream.\nYou never need more Kinesis input DStreams than the number of Kinesis stream shards as each input DStream will create at least one KinesisRecordProcessor thread that handles a single shard.\nHorizontal scaling is achieved by adding/removing  Kinesis input DStreams (within a single proc", "question": "How does horizontal scaling occur with Kinesis input DStreams?", "answers": {"text": ["Horizontal scaling is achieved by adding/removing  Kinesis input DStreams (within a single proc"], "answer_start": [705]}}
{"context": "sisRecordProcessor thread that handles a single shard.\nHorizontal scaling is achieved by adding/removing  Kinesis input DStreams (within a single process or across multiple processes/instances) - up to the total number of Kinesis stream shards per the previous point.\nThe Kinesis input DStream will balance the load between all DStreams - even across processes/instances.\nThe Kinesis input DStream will balance the load during re-shard events (merging and splitting) due to changes in load.\nAs a best practice, it’s recommended that you avoid re-shard jitter by over-provisioning when possible.\nEach Kinesis input DStream maintains its own checkpoint info.  See the Kinesis Checkpointing section for more details.\nThere is no correlation between the number of Kinesis stream shards and the number of ", "question": "Como a escalabilidade horizontal é alcançada?", "answers": {"text": ["Horizontal scaling is achieved by adding/removing  Kinesis input DStreams (within a single process or across multiple processes/instances) - up to the total number of Kinesis stream shards per the previous point."], "answer_start": [55]}}
{"context": " info.  See the Kinesis Checkpointing section for more details.\nThere is no correlation between the number of Kinesis stream shards and the number of RDD partitions/shards created across the Spark cluster during input DStream processing.  These are 2 independent partitioning schemes.\nRunning the Example\nTo run the example,\nDownload a Spark binary from the\ndownload site\n.\nSet up Kinesis stream (see earlier section) within AWS. Note the name of the Kinesis stream and the endpoint URL corresponding to the region where the stream was created.\nSet up the environment variables\nAWS_ACCESS_KEY_ID\nand\nAWS_SECRET_ACCESS_KEY\nwith your AWS credentials.\nIn the Spark root directory, run the example as\n./bin/spark-submit\n--jars\n'connector/kinesis-asl-assembly/target/spark-streaming-kinesis-asl-assembly_*", "question": "What is required to run the example?", "answers": {"text": ["Set up the environment variables\nAWS_ACCESS_KEY_ID\nand\nAWS_SECRET_ACCESS_KEY\nwith your AWS credentials."], "answer_start": [545]}}
{"context": "n the Spark root directory, run the example as\n./bin/spark-submit\n--jars\n'connector/kinesis-asl-assembly/target/spark-streaming-kinesis-asl-assembly_*.jar'\n\\\nconnector/kinesis-asl/src/main/python/examples/streaming/kinesis_wordcount_asl.py\n\\\n[\nKinesis app name]\n[\nKinesis stream name]\n[\nendpoint URL]\n[\nregion name]\n./bin/run-example\n--packages\norg.apache.spark:spark-streaming-kinesis-asl_2.13:4.0.0 streaming.KinesisWordCountASL\n[\nKinesis app name]\n[\nKinesis stream name]\n[\nendpoint URL]\n./bin/run-example\n--packages\norg.apache.spark:spark-streaming-kinesis-asl_2.13:4.0.0 streaming.JavaKinesisWordCountASL\n[\nKinesis app name]\n[\nKinesis stream name]\n[\nendpoint URL]\nThis will wait for data to be received from the Kinesis stream.\nTo generate random string data to put onto the Kinesis stream, in an", "question": "How can the example be run in the Spark root directory?", "answers": {"text": ["./bin/spark-submit\n--jars\n'connector/kinesis-asl-assembly/target/spark-streaming-kinesis-asl-assembly_*.jar'"], "answer_start": [47]}}
{"context": "]\n[\nendpoint URL]\nThis will wait for data to be received from the Kinesis stream.\nTo generate random string data to put onto the Kinesis stream, in another terminal, run the associated Kinesis data producer.\n./bin/run-example streaming.KinesisWordProducerASL\n[\nKinesis stream name]\n[\nendpoint URL] 1000 10\nThis will push 1000 lines per second of 10 random numbers per line to the Kinesis stream.  This data should then be received and processed by the running example.\nRecord De-aggregation\nWhen data is generated using the\nKinesis Producer Library (KPL)\n, messages may be aggregated for cost savings. Spark Streaming will automatically\nde-aggregate records during consumption.\nKinesis Checkpointing\nEach Kinesis input DStream periodically stores the current position of the stream in the backing Dyn", "question": "What happens when data is generated using the Kinesis Producer Library (KPL)?", "answers": {"text": ["messages may be aggregated for cost savings."], "answer_start": [557]}}
{"context": " When reading from Amazon Kinesis, users may hit\nProvisionedThroughputExceededException\n’s, when consuming faster than 5 transactions/second or, exceeding the maximum read rate of 2 MiB/second. This configuration can be tweaked to increase the sleep between fetches when a fetch fails to reduce these exceptions. Default is “100ms”.\nspark.streaming.kinesis.retry.maxAttempts\n: Max number of retries for Kinesis fetches. This config can also be used to tackle the Kinesis\nProvisionedThroughputExceededException\n’s in scenarios mentioned above. It can be increased to have more number of retries for Kinesis reads. Default is 3.", "question": "What is the default number of retries for Kinesis fetches?", "answers": {"text": ["Default is 3."], "answer_start": [613]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)"], "answer_start": [46]}}
{"context": "ures\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nLinear Methods - RDD-based API\nMathematical formulation\nLoss functions\nRegularizers\nOptimization\nClassification\nLinear Support Vector Machines (SVMs)\nLogistic regression\nRegression\nLinear least squares, Lasso, and ridge regression\nStreaming linear regression\nImplementation (developer)\n\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\math", "question": "Quais métodos lineares são abordados na seção 'Linear Methods - RDD-based API'?", "answers": {"text": ["Classification and regression"], "answer_start": [192]}}
{"context": "asier to solve than L1-regularized due to smoothness.\nHowever, L1 regularization can help promote sparsity in weights leading to smaller and more interpretable models, the latter of which can be useful for feature selection.\nElastic net\nis a combination of L1 and L2 regularization. It is not recommended to train models without any regularization,\nespecially when the number of training examples is small.\nOptimization\nUnder the hood, linear methods use convex optimization methods to optimize the objective functions.\nspark.mllib\nuses two methods, SGD and L-BFGS, described in the\noptimization section\n.\nCurrently, most algorithm APIs support Stochastic Gradient Descent (SGD), and a few support L-BFGS.\nRefer to\nthis optimization section\nfor guidelines on choosing between optimization methods.\nCl", "question": "What two methods does spark.mllib use for optimization?", "answers": {"text": ["SGD and L-BFGS"], "answer_start": [550]}}
{"context": "astic Gradient Descent (SGD), and a few support L-BFGS.\nRefer to\nthis optimization section\nfor guidelines on choosing between optimization methods.\nClassification\nClassification\naims to divide items into\ncategories.\nThe most common classification type is\nbinary classification\n, where there are two\ncategories, usually named positive and negative.\nIf there are more than two categories, it is called\nmulticlass classification\n.\nspark.mllib\nsupports two linear methods for classification: linear Support Vector Machines (SVMs)\nand logistic regression.\nLinear SVMs supports only binary classification, while logistic regression supports both binary and\nmulticlass classification problems.\nFor both methods,\nspark.mllib\nsupports L1 and L2 regularized variants.\nThe training data set is represented by an", "question": "What type of classification has more than two categories?", "answers": {"text": ["multiclass classification"], "answer_start": [400]}}
{"context": "\nmulticlass classification problems.\nFor both methods,\nspark.mllib\nsupports L1 and L2 regularized variants.\nThe training data set is represented by an RDD of\nLabeledPoint\nin MLlib,\nwhere labels are class indices starting from zero: $0, 1, 2, \\ldots$.\nLinear Support Vector Machines (SVMs)\nThe\nlinear SVM\nis a standard method for large-scale classification tasks. It is a linear method as described above in equation\n$\\eqref{eq:regPrimal}$\n, with the loss function in the formulation given by the hinge loss:\n\\[\nL(\\wv;\\x,y) := \\max \\{0, 1-y \\wv^T \\x \\}.\n\\]\nBy default, linear SVMs are trained with an L2 regularization.\nWe also support alternative L1 regularization. In this case,\nthe problem becomes a\nlinear program\n.\nThe linear SVMs algorithm outputs an SVM model. Given a new data point,\ndenoted b", "question": "What type of regularization is used by default when training linear SVMs?", "answers": {"text": ["By default, linear SVMs are trained with an L2 regularization."], "answer_start": [556]}}
{"context": "nErr\n))\n# Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"\ntarget/tmp/pythonSVMWithSGDModel\n\"\n)\nsameModel\n=\nSVMModel\n.\nload\n(\nsc\n,\n\"\ntarget/tmp/pythonSVMWithSGDModel\n\"\n)\nFind full example code at \"examples/src/main/python/mllib/svm_with_sgd_example.py\" in the Spark repo.\nThe following code snippet illustrates how to load a sample dataset, execute a\ntraining algorithm on this training data using a static method in the algorithm\nobject, and make predictions with the resulting model to compute the training\nerror.\nRefer to the\nSVMWithSGD\nScala docs\nand\nSVMModel\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.classification.\n{\nSVMModel\n,\nSVMWithSGD\n}\nimport\norg.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nimport\norg.apache.spark.mllib.util.MLUtils\n// Load training data in", "question": "Where can I find a full example code for SVM with SGD?", "answers": {"text": ["Find full example code at \"examples/src/main/python/mllib/svm_with_sgd_example.py\" in the Spark repo."], "answer_start": [163]}}
{"context": "t.\nval\nscoreAndLabels\n=\ntest\n.\nmap\n{\npoint\n=>\nval\nscore\n=\nmodel\n.\npredict\n(\npoint\n.\nfeatures\n)\n(\nscore\n,\npoint\n.\nlabel\n)\n}\n// Get evaluation metrics.\nval\nmetrics\n=\nnew\nBinaryClassificationMetrics\n(\nscoreAndLabels\n)\nval\nauROC\n=\nmetrics\n.\nareaUnderROC\n()\nprintln\n(\ns\n\"Area under ROC = $auROC\"\n)\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/scalaSVMWithSGDModel\"\n)\nval\nsameModel\n=\nSVMModel\n.\nload\n(\nsc\n,\n\"target/tmp/scalaSVMWithSGDModel\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\" in the Spark repo.\nThe\nSVMWithSGD.train()\nmethod by default performs L2 regularization with the\nregularization parameter set to 1.0. If we want to configure this algorithm, we\ncan customize\nSVMWithSGD\nfurther by creating a new object directly an", "question": "Where can I find the full example code for SVMWithSGD?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\" in the Spark repo."], "answer_start": [447]}}
{"context": "ssification.SVMWithSGD\n;\nimport\norg.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n;\nimport\norg.apache.spark.mllib.regression.LabeledPoint\n;\nimport\norg.apache.spark.mllib.util.MLUtils\n;\nString\npath\n=\n\"data/mllib/sample_libsvm_data.txt\"\n;\nJavaRDD\n<\nLabeledPoint\n>\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\npath\n).\ntoJavaRDD\n();\n// Split initial RDD into two... [60% training data, 40% testing data].\nJavaRDD\n<\nLabeledPoint\n>\ntraining\n=\ndata\n.\nsample\n(\nfalse\n,\n0.6\n,\n11L\n);\ntraining\n.\ncache\n();\nJavaRDD\n<\nLabeledPoint\n>\ntest\n=\ndata\n.\nsubtract\n(\ntraining\n);\n// Run training algorithm to build the model.\nint\nnumIterations\n=\n100\n;\nSVMModel\nmodel\n=\nSVMWithSGD\n.\ntrain\n(\ntraining\n.\nrdd\n(),\nnumIterations\n);\n// Clear the default threshold.\nmodel\n.\nclearThreshold\n();\n// Compute raw scores on the tes", "question": "What is the path to the data file used for training and testing?", "answers": {"text": ["\"data/mllib/sample_libsvm_data.txt\""], "answer_start": [211]}}
{"context": "=\nSVMWithSGD\n.\ntrain\n(\ntraining\n.\nrdd\n(),\nnumIterations\n);\n// Clear the default threshold.\nmodel\n.\nclearThreshold\n();\n// Compute raw scores on the test set.\nJavaRDD\n<\nTuple2\n<\nObject\n,\nObject\n>>\nscoreAndLabels\n=\ntest\n.\nmap\n(\np\n->\nnew\nTuple2\n<>(\nmodel\n.\npredict\n(\np\n.\nfeatures\n()),\np\n.\nlabel\n()));\n// Get evaluation metrics.\nBinaryClassificationMetrics\nmetrics\n=\nnew\nBinaryClassificationMetrics\n(\nJavaRDD\n.\ntoRDD\n(\nscoreAndLabels\n));\ndouble\nauROC\n=\nmetrics\n.\nareaUnderROC\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Area under ROC = \"\n+\nauROC\n);\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/javaSVMWithSGDModel\"\n);\nSVMModel\nsameModel\n=\nSVMModel\n.\nload\n(\nsc\n,\n\"target/tmp/javaSVMWithSGDModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\" ", "question": "Where can I find the full example code for JavaSVMWithSGDExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\""], "answer_start": [690]}}
{"context": "200 iterations.\nimport\norg.apache.spark.mllib.optimization.L1Updater\n;\nSVMWithSGD\nsvmAlg\n=\nnew\nSVMWithSGD\n();\nsvmAlg\n.\noptimizer\n()\n.\nsetNumIterations\n(\n200\n)\n.\nsetRegParam\n(\n0.1\n)\n.\nsetUpdater\n(\nnew\nL1Updater\n());\nSVMModel\nmodelL1\n=\nsvmAlg\n.\nrun\n(\ntraining\n.\nrdd\n());\nIn order to run the above application, follow the instructions\nprovided in the\nSelf-Contained\nApplications\nsection of the Spark\nquick-start guide. Be sure to also include\nspark-mllib\nto your build file as\na dependency.\nLogistic regression\nLogistic regression\nis widely used to predict a\nbinary response. It is a linear method as described above in equation\n$\\eqref{eq:regPrimal}$\n,\nwith the loss function in the formulation given by the logistic loss:\n\\[\nL(\\wv;\\x,y) :=  \\log(1+\\exp( -y \\wv^T \\x)).\n\\]\nFor binary classification pro", "question": "What is set as the number of iterations for the SVM algorithm?", "answers": {"text": ["200 iterations."], "answer_start": [0]}}
{"context": "Binary logistic regression can be generalized into\nmultinomial logistic regression\nto\ntrain and predict multiclass classification problems.\nFor example, for $K$ possible outcomes, one of the outcomes can be chosen as a “pivot”, and the\nother $K - 1$ outcomes can be separately regressed against the pivot outcome.\nIn\nspark.mllib\n, the first class $0$ is chosen as the “pivot” class.\nSee Section 4.4 of\nThe Elements of Statistical Learning\nfor\nreferences.\nHere is a\ndetailed mathematical derivation\n.\nFor multiclass classification problems, the algorithm will output a multinomial logistic regression\nmodel, which contains $K - 1$ binary logistic regression models regressed against the first class.\nGiven a new data points, $K - 1$ models will be run, and the class with largest probability will be\nc", "question": "How many binary logistic regression models does a multinomial logistic regression model contain for multiclass classification problems?", "answers": {"text": ["which contains $K - 1$ binary logistic regression models regressed against the first class."], "answer_start": [607]}}
{"context": "ession models regressed against the first class.\nGiven a new data points, $K - 1$ models will be run, and the class with largest probability will be\nchosen as the predicted class.\nWe implemented two algorithms to solve logistic regression: mini-batch gradient descent and L-BFGS.\nWe recommend L-BFGS over mini-batch gradient descent for faster convergence.\nExamples\nThe following example shows how to load a sample dataset, build Logistic Regression model,\nand make predictions with the resulting model to compute the training error.\nNote that the Python API does not yet support multiclass classification and model save/load but\nwill in the future.\nRefer to the\nLogisticRegressionWithLBFGS\nPython docs\nand\nLogisticRegressionModel\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.classific", "question": "Which algorithm is recommended for faster convergence in logistic regression?", "answers": {"text": ["We recommend L-BFGS over mini-batch gradient descent for faster convergence."], "answer_start": [280]}}
{"context": "eds\n=\nparsedData\n.\nmap\n(\nlambda\np\n:\n(\np\n.\nlabel\n,\nmodel\n.\npredict\n(\np\n.\nfeatures\n)))\ntrainErr\n=\nlabelsAndPreds\n.\nfilter\n(\nlambda\nlp\n:\nlp\n[\n0\n]\n!=\nlp\n[\n1\n]).\ncount\n()\n/\nfloat\n(\nparsedData\n.\ncount\n())\nprint\n(\n\"\nTraining Error =\n\"\n+\nstr\n(\ntrainErr\n))\n# Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"\ntarget/tmp/pythonLogisticRegressionWithLBFGSModel\n\"\n)\nsameModel\n=\nLogisticRegressionModel\n.\nload\n(\nsc\n,\n\"\ntarget/tmp/pythonLogisticRegressionWithLBFGSModel\n\"\n)\nFind full example code at \"examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\" in the Spark repo.\nThe following code illustrates how to load a sample multiclass dataset, split it into train and\ntest, and use\nLogisticRegressionWithLBFGS\nto fit a logistic regression model.\nThen the model is evaluated against the test dataset a", "question": "Where can I find the full example code for logistic regression?", "answers": {"text": ["Find full example code at \"examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\" in the Spark repo."], "answer_start": [452]}}
{"context": "m_data.txt\"\n)\n// Split data into training (60%) and test (40%).\nval\nsplits\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.6\n,\n0.4\n),\nseed\n=\n11L\n)\nval\ntraining\n=\nsplits\n(\n0\n).\ncache\n()\nval\ntest\n=\nsplits\n(\n1\n)\n// Run training algorithm to build the model\nval\nmodel\n=\nnew\nLogisticRegressionWithLBFGS\n()\n.\nsetNumClasses\n(\n10\n)\n.\nrun\n(\ntraining\n)\n// Compute raw scores on the test set.\nval\npredictionAndLabels\n=\ntest\n.\nmap\n{\ncase\nLabeledPoint\n(\nlabel\n,\nfeatures\n)\n=>\nval\nprediction\n=\nmodel\n.\npredict\n(\nfeatures\n)\n(\nprediction\n,\nlabel\n)\n}\n// Get evaluation metrics.\nval\nmetrics\n=\nnew\nMulticlassMetrics\n(\npredictionAndLabels\n)\nval\naccuracy\n=\nmetrics\n.\naccuracy\nprintln\n(\ns\n\"Accuracy = $accuracy\"\n)\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/scalaLogisticRegressionWithLBFGSModel\"\n)\nval\nsameModel\n=\nLogisticReg", "question": "What percentage of the data is used for training?", "answers": {"text": ["0.6"], "answer_start": [106]}}
{"context": "s\n\"Accuracy = $accuracy\"\n)\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/scalaLogisticRegressionWithLBFGSModel\"\n)\nval\nsameModel\n=\nLogisticRegressionModel\n.\nload\n(\nsc\n,\n\"target/tmp/scalaLogisticRegressionWithLBFGSModel\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\" in the Spark repo.\nThe following code illustrates how to load a sample multiclass dataset, split it into train and\ntest, and use\nLogisticRegressionWithLBFGS\nto fit a logistic regression model.\nThen the model is evaluated against the test dataset and saved to disk.\nRefer to the\nLogisticRegressionWithLBFGS\nJava docs\nand\nLogisticRegressionModel\nJava docs\nfor details on the API.\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.api.java.JavaPairRDD\n", "question": "Where can I find a full example code for Logistic Regression?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\" in the Spark repo."], "answer_start": [230]}}
{"context": "initial RDD into two... [60% training data, 40% testing data].\nJavaRDD\n<\nLabeledPoint\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]\n{\n0.6\n,\n0.4\n},\n11L\n);\nJavaRDD\n<\nLabeledPoint\n>\ntraining\n=\nsplits\n[\n0\n].\ncache\n();\nJavaRDD\n<\nLabeledPoint\n>\ntest\n=\nsplits\n[\n1\n];\n// Run training algorithm to build the model.\nLogisticRegressionModel\nmodel\n=\nnew\nLogisticRegressionWithLBFGS\n()\n.\nsetNumClasses\n(\n10\n)\n.\nrun\n(\ntraining\n.\nrdd\n());\n// Compute raw scores on the test set.\nJavaPairRDD\n<\nObject\n,\nObject\n>\npredictionAndLabels\n=\ntest\n.\nmapToPair\n(\np\n->\nnew\nTuple2\n<>(\nmodel\n.\npredict\n(\np\n.\nfeatures\n()),\np\n.\nlabel\n()));\n// Get evaluation metrics.\nMulticlassMetrics\nmetrics\n=\nnew\nMulticlassMetrics\n(\npredictionAndLabels\n.\nrdd\n());\ndouble\naccuracy\n=\nmetrics\n.\naccuracy\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Accuracy = \"\n", "question": "How is the initial RDD split into training and testing data?", "answers": {"text": ["[60% training data, 40% testing data]"], "answer_start": [24]}}
{"context": "rics\nmetrics\n=\nnew\nMulticlassMetrics\n(\npredictionAndLabels\n.\nrdd\n());\ndouble\naccuracy\n=\nmetrics\n.\naccuracy\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Accuracy = \"\n+\naccuracy\n);\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/javaLogisticRegressionWithLBFGSModel\"\n);\nLogisticRegressionModel\nsameModel\n=\nLogisticRegressionModel\n.\nload\n(\nsc\n,\n\"target/tmp/javaLogisticRegressionWithLBFGSModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\" in the Spark repo.\nRegression\nLinear least squares, Lasso, and ridge regression\nLinear least squares is the most common formulation for regression problems.\nIt is a linear method as described above in equation\n$\\eqref{eq:regPrimal}$\n, with the loss\nfunction in the formulation given by ", "question": "Where can I find the full example code for JavaLogisticRegressionWithLBFGSExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\" in the Spark repo."], "answer_start": [387]}}
{"context": "in a streaming fashion, it is useful to fit regression models online,\nupdating the parameters of the model as new data arrives.\nspark.mllib\ncurrently supports\nstreaming linear regression using ordinary least squares. The fitting is similar\nto that performed offline, except fitting occurs on each batch of data, so that\nthe model continually updates to reflect the data from the stream.\nExamples\nThe following example demonstrates how to load training and testing data from two different\ninput streams of text files, parse the streams as labeled points, fit a linear regression model\nonline to the first stream, and make predictions on the second stream.\nFirst, we import the necessary classes for parsing our input data and creating the model.\nThen we make input streams for training and testing dat", "question": "What type of linear regression does spark.mllib currently support for streaming?", "answers": {"text": ["streaming linear regression using ordinary least squares"], "answer_start": [159]}}
{"context": "matted as\n(y,[x1,x2,x3])\nwhere\ny\nis the label\nand\nx1,x2,x3\nare the features. Anytime a text file is placed in\nsys.argv[1]\nthe model will update. Anytime a text file is placed in\nsys.argv[2]\nyou will see predictions.\nAs you feed more data to the training directory, the predictions\nwill get better!\nHere a complete example:\nimport\nsys\nfrom\npyspark.mllib.linalg\nimport\nVectors\nfrom\npyspark.mllib.regression\nimport\nLabeledPoint\nfrom\npyspark.mllib.regression\nimport\nStreamingLinearRegressionWithSGD\ndef\nparse\n(\nlp\n):\nlabel\n=\nfloat\n(\nlp\n[\nlp\n.\nfind\n(\n'\n(\n'\n)\n+\n1\n:\nlp\n.\nfind\n(\n'\n,\n'\n)])\nvec\n=\nVectors\n.\ndense\n(\nlp\n[\nlp\n.\nfind\n(\n'\n[\n'\n)\n+\n1\n:\nlp\n.\nfind\n(\n'\n]\n'\n)].\nsplit\n(\n'\n,\n'\n))\nreturn\nLabeledPoint\n(\nlabel\n,\nvec\n)\ntrainingData\n=\nssc\n.\ntextFileStream\n(\nsys\n.\nargv\n[\n1\n]).\nmap\n(\nparse\n).\ncache\n()\ntestDat", "question": "Where does the model update when a text file is placed?", "answers": {"text": ["sys.argv[1]"], "answer_start": [110]}}
{"context": "\n]\n'\n)].\nsplit\n(\n'\n,\n'\n))\nreturn\nLabeledPoint\n(\nlabel\n,\nvec\n)\ntrainingData\n=\nssc\n.\ntextFileStream\n(\nsys\n.\nargv\n[\n1\n]).\nmap\n(\nparse\n).\ncache\n()\ntestData\n=\nssc\n.\ntextFileStream\n(\nsys\n.\nargv\n[\n2\n]).\nmap\n(\nparse\n)\nnumFeatures\n=\n3\nmodel\n=\nStreamingLinearRegressionWithSGD\n()\nmodel\n.\nsetInitialWeights\n([\n0.0\n,\n0.0\n,\n0.0\n])\nmodel\n.\ntrainOn\n(\ntrainingData\n)\nprint\n(\nmodel\n.\npredictOnValues\n(\ntestData\n.\nmap\n(\nlambda\nlp\n:\n(\nlp\n.\nlabel\n,\nlp\n.\nfeatures\n))))\nssc\n.\nstart\n()\nssc\n.\nawaitTermination\n()\nFind full example code at \"examples/src/main/python/mllib/streaming_linear_regression_example.py\" in the Spark repo.\nFirst, we import the necessary classes for parsing our input data and creating the model.\nThen we make input streams for training and testing data. We assume a StreamingContext\nssc\nhas already b", "question": "Where can I find a full example code for this streaming linear regression?", "answers": {"text": ["Find full example code at \"examples/src/main/python/mllib/streaming_linear_regression_example.py\" in the Spark repo."], "answer_start": [489]}}
{"context": "arsing our input data and creating the model.\nThen we make input streams for training and testing data. We assume a StreamingContext\nssc\nhas already been created, see\nSpark Streaming Programming Guide\nfor more info. For this example, we use labeled points in training and testing streams,\nbut in practice you will likely want to use unlabeled vectors for test data.\nWe create our model by initializing the weights to zero and register the streams for training and\ntesting then start the job. Printing predictions alongside true labels lets us easily see the\nresult.\nFinally, we can save text files with data to the training or testing folders.\nEach line should be a data point formatted as\n(y,[x1,x2,x3])\nwhere\ny\nis the label\nand\nx1,x2,x3\nare the features. Anytime a text file is placed in\nargs(0)\nth", "question": "How should each line of data in the training or testing folders be formatted?", "answers": {"text": ["(y,[x1,x2,x3])"], "answer_start": [690]}}
{"context": "ine should be a data point formatted as\n(y,[x1,x2,x3])\nwhere\ny\nis the label\nand\nx1,x2,x3\nare the features. Anytime a text file is placed in\nargs(0)\nthe model will update. Anytime a text file is placed in\nargs(1)\nyou will see predictions.\nAs you feed more data to the training directory, the predictions\nwill get better!\nHere is a complete example:\nimport\norg.apache.spark.mllib.linalg.Vectors\nimport\norg.apache.spark.mllib.regression.LabeledPoint\nimport\norg.apache.spark.mllib.regression.StreamingLinearRegressionWithSGD\nval\ntrainingData\n=\nssc\n.\ntextFileStream\n(\nargs\n(\n0\n)).\nmap\n(\nLabeledPoint\n.\nparse\n).\ncache\n()\nval\ntestData\n=\nssc\n.\ntextFileStream\n(\nargs\n(\n1\n)).\nmap\n(\nLabeledPoint\n.\nparse\n)\nval\nnumFeatures\n=\n3\nval\nmodel\n=\nnew\nStreamingLinearRegressionWithSGD\n()\n.\nsetInitialWeights\n(\nVectors\n.\nz", "question": "How should a data point be formatted?", "answers": {"text": ["(y,[x1,x2,x3])"], "answer_start": [40]}}
{"context": "\n(\nargs\n(\n1\n)).\nmap\n(\nLabeledPoint\n.\nparse\n)\nval\nnumFeatures\n=\n3\nval\nmodel\n=\nnew\nStreamingLinearRegressionWithSGD\n()\n.\nsetInitialWeights\n(\nVectors\n.\nzeros\n(\nnumFeatures\n))\nmodel\n.\ntrainOn\n(\ntrainingData\n)\nmodel\n.\npredictOnValues\n(\ntestData\n.\nmap\n(\nlp\n=>\n(\nlp\n.\nlabel\n,\nlp\n.\nfeatures\n))).\nprint\n()\nssc\n.\nstart\n()\nssc\n.\nawaitTermination\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\" in the Spark repo.\nImplementation (developer)\nBehind the scene,\nspark.mllib\nimplements a simple distributed version of stochastic gradient descent\n(SGD), building on the underlying gradient descent primitive (as described in the\noptimization\nsection).  All provided algorithms take as input a\nregularization parameter (\nregParam\n) along wi", "question": "Where can I find the full example code for StreamingLinearRegressionExample?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\" in the Spark repo."], "answer_start": [338]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMigration Guide: Spark Core\nUpgrading from Core 3.5 to 4.0\nUpgrading from Core 3.5.3 to 3.5.4\nUpgrading from Core 3.4 to 3.5\nUpgrading from Core 3.3 to 3.4\nUpgrading from Core 3.2 to 3.3\nUpgrading from Core 3.1 to 3.2\nUpgrading from Core 3.0 to 3.1\nUp", "question": "What versions are covered in the Migration Guide?", "answers": {"text": ["Upgrading from Core 3.5 to 4.0"], "answer_start": [577]}}
{"context": " clean up worker and stopped application directories periodically. To restore the behavior before Spark 4.0, you can set\nspark.worker.cleanup.enabled\nto\nfalse\n.\nSince Spark 4.0,\nspark.shuffle.service.db.backend\nis set to\nROCKSDB\nby default which means Spark will use RocksDB store for shuffle service. To restore the behavior before Spark 4.0, you can set\nspark.shuffle.service.db.backend\nto\nLEVELDB\n.\nIn Spark 4.0, support for Apache Mesos as a resource manager was removed.\nSince Spark 4.0, Spark will allocate executor pods with a batch size of\n10\n. To restore the legacy behavior, you can set\nspark.kubernetes.allocation.batch.size\nto\n5\n.\nSince Spark 4.0, Spark uses\nReadWriteOncePod\ninstead of\nReadWriteOnce\naccess mode in persistence volume claims. To restore the legacy behavior, you can set\ns", "question": "What is the default value of spark.shuffle.service.db.backend since Spark 4.0?", "answers": {"text": ["ROCKSDB"], "answer_start": [221]}}
{"context": "park 4.0, Spark uses\nReadWriteOncePod\ninstead of\nReadWriteOnce\naccess mode in persistence volume claims. To restore the legacy behavior, you can set\nspark.kubernetes.legacy.useReadWriteOnceAccessMode\nto\ntrue\n.\nSince Spark 4.0, Spark reports its executor pod status by checking all containers of that pod. To restore the legacy behavior, you can set\nspark.kubernetes.executor.checkAllContainers\nto\nfalse\n.\nSince Spark 4.0, Spark uses\n~/.ivy2.5.2\nas Ivy user directory by default to isolate the existing systems from Apache Ivy’s incompatibility. To restore the legacy behavior, you can set\nspark.jars.ivy\nto\n~/.ivy2\n.\nSince Spark 4.0, Spark uses the external shuffle service for deleting shuffle blocks for deallocated executors when the shuffle is no longer needed. To restore the legacy behavior, yo", "question": "Como restaurar o comportamento legado do Spark 4.0 em relação ao modo de acesso ReadWriteOnce em reivindicações de volume persistente?", "answers": {"text": ["To restore the legacy behavior, you can set\nspark.kubernetes.legacy.useReadWriteOnceAccessMode\nto\ntrue\n."], "answer_start": [105]}}
{"context": "xternal shuffle service for deleting shuffle blocks for deallocated executors when the shuffle is no longer needed. To restore the legacy behavior, you can set\nspark.shuffle.service.removeShuffle\nto\nfalse\n.\nSince Spark 4.0, the MDC (Mapped Diagnostic Context) key for Spark task names in Spark logs has been changed from\nmdc.taskName\nto\ntask_name\n. To use the key\nmdc.taskName\n, you can set\nspark.log.legacyTaskNameMdc.enabled\nto\ntrue\n.\nSince Spark 4.0, Spark performs speculative executions less aggressively with\nspark.speculation.multiplier=3\nand\nspark.speculation.quantile=0.9\n. To restore the legacy behavior, you can set\nspark.speculation.multiplier=1.5\nand\nspark.speculation.quantile=0.75\n.\nSince Spark 4.0,\nspark.shuffle.unsafe.file.output.buffer\nis deprecated though still works. Use\nspark.s", "question": "How can you restore the legacy behavior of speculative executions in Spark?", "answers": {"text": ["To restore the legacy behavior, you can set\nspark.speculation.multiplier=1.5\nand\nspark.speculation.quantile=0.75"], "answer_start": [583]}}
{"context": "plier=1.5\nand\nspark.speculation.quantile=0.75\n.\nSince Spark 4.0,\nspark.shuffle.unsafe.file.output.buffer\nis deprecated though still works. Use\nspark.shuffle.localDisk.file.output.buffer\ninstead.\nSince Spark 4.0, when reading files hits\norg.apache.hadoop.security.AccessControlException\nand\norg.apache.hadoop.hdfs.BlockMissingException\n, the exception will be thrown and fail the task, even if\nspark.files.ignoreCorruptFiles\nis set to\ntrue\n.\nUpgrading from Core 3.5.3 to 3.5.4\nSince Spark 3.5.4, when reading files hits\norg.apache.hadoop.security.AccessControlException\nand\norg.apache.hadoop.hdfs.BlockMissingException\n, the exception will be thrown and fail the task, even if\nspark.files.ignoreCorruptFiles\nis set to\ntrue\n.\nUpgrading from Core 3.4 to 3.5\nSince Spark 3.5,\nspark.yarn.executor.failures", "question": "What should be used instead of spark.shuffle.unsafe.file.output.buffer since Spark 4.0?", "answers": {"text": ["spark.shuffle.localDisk.file.output.buffer"], "answer_start": [143]}}
{"context": "e.hybridStore.enabled\nis true. To restore the behavior before Spark 3.4, you can set\nspark.history.store.hybridStore.diskBackend\nto\nLEVELDB\n.\nUpgrading from Core 3.2 to 3.3\nSince Spark 3.3, Spark migrates its log4j dependency from 1.x to 2.x because log4j 1.x has reached end of life and is no longer supported by the community. Vulnerabilities reported after August 2015 against log4j 1.x were not checked and will not be fixed. Users should rewrite original log4j properties files using log4j2 syntax (XML, JSON, YAML, or properties format). Spark rewrites the\nconf/log4j.properties.template\nwhich is included in Spark distribution, to\nconf/log4j2.properties.template\nwith log4j2 properties format.\nUpgrading from Core 3.1 to 3.2\nSince Spark 3.2,\nspark.scheduler.allocation.file\nsupports read remot", "question": "What happened to the log4j dependency since Spark 3.3?", "answers": {"text": ["Since Spark 3.3, Spark migrates its log4j dependency from 1.x to 2.x because log4j 1.x has reached end of life and is no longer supported by the community."], "answer_start": [173]}}
{"context": "properties.template\nwith log4j2 properties format.\nUpgrading from Core 3.1 to 3.2\nSince Spark 3.2,\nspark.scheduler.allocation.file\nsupports read remote file using hadoop filesystem which means if the path has no scheme Spark will respect hadoop configuration to read it. To restore the behavior before Spark 3.2, you can specify the local scheme for\nspark.scheduler.allocation.file\ne.g.\nfile:///path/to/file\n.\nSince Spark 3.2,\nspark.hadoopRDD.ignoreEmptySplits\nis set to\ntrue\nby default which means Spark will not create empty partitions for empty input splits. To restore the behavior before Spark 3.2, you can set\nspark.hadoopRDD.ignoreEmptySplits\nto\nfalse\n.\nSince Spark 3.2,\nspark.eventLog.compression.codec\nis set to\nzstd\nby default which means Spark will not fallback to use\nspark.io.compression", "question": "What is the default value of spark.hadoopRDD.ignoreEmptySplits since Spark 3.2?", "answers": {"text": ["true"], "answer_start": [471]}}
{"context": "to\nfalse\n.\nSince Spark 3.2,\nspark.eventLog.compression.codec\nis set to\nzstd\nby default which means Spark will not fallback to use\nspark.io.compression.codec\nanymore.\nSince Spark 3.2,\nspark.storage.replication.proactive\nis enabled by default which means Spark tries to replenish in case of the loss of cached RDD block replicas due to executor failures. To restore the behavior before Spark 3.2, you can set\nspark.storage.replication.proactive\nto\nfalse\n.\nIn Spark 3.2,\nspark.launcher.childConectionTimeout\nis deprecated (typo) though still works. Use\nspark.launcher.childConnectionTimeout\ninstead.\nIn Spark 3.2, support for Apache Mesos as a resource manager is deprecated and will be removed in a future version.\nIn Spark 3.2, Spark will delete K8s driver service resource when the application termin", "question": "What is the default value of spark.eventLog.compression.codec since Spark 3.2?", "answers": {"text": ["zstd"], "answer_start": [71]}}
{"context": "manager is deprecated and will be removed in a future version.\nIn Spark 3.2, Spark will delete K8s driver service resource when the application terminates by itself. To restore the behavior before Spark 3.2, you can set\nspark.kubernetes.driver.service.deleteOnTermination\nto\nfalse\n.\nUpgrading from Core 3.0 to 3.1\nIn Spark 3.0 and below,\nSparkContext\ncan be created in executors. Since Spark 3.1, an exception will be thrown when creating\nSparkContext\nin executors. You can allow it by setting the configuration\nspark.executor.allowSparkContext\nwhen creating\nSparkContext\nin executors.\nIn Spark 3.0 and below, Spark propagated the Hadoop classpath from\nyarn.application.classpath\nand\nmapreduce.application.classpath\ninto the Spark application submitted to YARN when Spark distribution is with the bui", "question": "What configuration option can be set to allow creating SparkContext in executors in Spark 3.1?", "answers": {"text": ["spark.executor.allowSparkContext"], "answer_start": [512]}}
{"context": "lugin.SparkPlugin\n, which adds new functionality. Plugins using the old\ninterface must be modified to extend the new interfaces. Check the\nMonitoring\nguide for more details.\nDeprecated method\nTaskContext.isRunningLocally\nhas been removed. Local execution was removed and it always has returned\nfalse\n.\nDeprecated method\nshuffleBytesWritten\n,\nshuffleWriteTime\nand\nshuffleRecordsWritten\nin\nShuffleWriteMetrics\nhave been removed. Instead, use\nbytesWritten\n,\nwriteTime\nand\nrecordsWritten\nrespectively.\nDeprecated method\nAccumulableInfo.apply\nhave been removed because creating\nAccumulableInfo\nis disallowed.\nDeprecated accumulator v1 APIs have been removed and please use v2 APIs instead.\nEvent log file will be written as UTF-8 encoding, and Spark History Server will replay event log files as UTF-8 enc", "question": "What encoding will the event log file be written as?", "answers": {"text": ["Event log file will be written as UTF-8 encoding"], "answer_start": [685]}}
{"context": "rs with messages like\nIllegalArgumentException: Unexpected message type: <number>\n.\nSPARK_WORKER_INSTANCES\nis deprecated in Standalone mode. It’s recommended to launch multiple executors in one worker and launch one worker per node instead of launching multiple workers per node and launching one executor per worker.", "question": "What exception is mentioned in the text regarding unexpected message types?", "answers": {"text": ["IllegalArgumentException: Unexpected message type: <number>"], "answer_start": [22]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nStructured Streaming Programming Guide\nOverview\nGetting Started\nAPIs on DataFrames and Datasets\nPerformance Tips\nAdditional Information\nMigration Guide: Structured Streaming\nNote that this migration guide describes the items specific to Structured Str", "question": "What topics are covered in the Spark documentation?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars"], "answer_start": [46]}}
{"context": "or more details.)\nSince Spark 4.0, new configuration\nspark.sql.streaming.ratioExtraSpaceAllowedInCheckpoint\n(default:\n0.3\n) controls the amount of additional space allowed in the checkpoint directory to store stale version files for batch deletion inside maintenance task. This is to amortize the cost of listing in cloud store. Setting this to\n0\ndefaults to the old behavior. (See\nSPARK-48931\nfor more details.)\nSince Spark 4.0, when relative path is used to output data in\nDataStreamWriter\nthe resolution to absolute path is done in the Spark Driver and is not deferred to Spark Executor. This is to make Structured Streaming behavior similar to DataFrame API (\nDataFrameWriter\n). (See\nSPARK-50854\nfor more details.)\nSince Spark 4.0, the deprecated config\nspark.databricks.sql.optimizer.pruneFilter", "question": "What does the configuration spark.sql.streaming.ratioExtraSpaceAllowedInCheckpoint control?", "answers": {"text": ["controls the amount of additional space allowed in the checkpoint directory to store stale version files for batch deletion inside maintenance task."], "answer_start": [124]}}
{"context": " group based scheduling, which affect the required ACL. For further details please see\nStructured Streaming Kafka Integration\n.\nUpgrading from Structured Streaming 3.2 to 3.3\nSince Spark 3.3, all stateful operators require hash partitioning with exact grouping keys. In previous versions, all stateful operators except stream-stream join require loose partitioning criteria which opens the possibility on correctness issue. (See\nSPARK-38204\nfor more details.) To ensure backward compatibility, we retain the old behavior with the checkpoint built from older versions.\nUpgrading from Structured Streaming 3.0 to 3.1\nIn Spark 3.0 and before, for the queries that have stateful operation which can emit rows older than the current watermark plus allowed late record delay, which are “late rows” in downs", "question": "What change was introduced in Spark 3.3 regarding stateful operators?", "answers": {"text": ["Since Spark 3.3, all stateful operators require hash partitioning with exact grouping keys."], "answer_start": [175]}}
{"context": "t caused issues tricky to debug with NPE. To restore the previous behavior, set\nspark.sql.streaming.fileSource.schema.forceNullable\nto\nfalse\n.\nSpark 3.0 fixes the correctness issue on Stream-stream outer join, which changes the schema of state. (See\nSPARK-26154\nfor more details). If you start your query from checkpoint constructed from Spark 2.x which uses stream-stream outer join, Spark 3.0 fails the query. To recalculate outputs, discard the checkpoint and replay previous inputs.\nIn Spark 3.0, the deprecated class\norg.apache.spark.sql.streaming.ProcessingTime\nhas been removed. Use\norg.apache.spark.sql.streaming.Trigger.ProcessingTime\ninstead. Likewise,\norg.apache.spark.sql.execution.streaming.continuous.ContinuousTrigger\nhas been removed in favor of\nTrigger.Continuous\n, and\norg.apache.sp", "question": "What should you do to restore the previous behavior related to schema nullability?", "answers": {"text": ["set\nspark.sql.streaming.fileSource.schema.forceNullable\nto\nfalse"], "answer_start": [76]}}
{"context": "d. Likewise,\norg.apache.spark.sql.execution.streaming.continuous.ContinuousTrigger\nhas been removed in favor of\nTrigger.Continuous\n, and\norg.apache.spark.sql.execution.streaming.OneTimeTrigger\nhas been hidden in favor of\nTrigger.Once\n.", "question": "What has been removed in favor of Trigger.Continuous?", "answers": {"text": ["org.apache.spark.sql.execution.streaming.continuous.ContinuousTrigger"], "answer_start": [13]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMigration Guide: PySpark (Python on Spark)\nThe migration guide is now archived on\nthis page\n.", "question": "What topics are covered in the Spark documentation?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars"], "answer_start": [46]}}
{"context": "braries\nUntil Spark 3.4, extensions to Spark (e.g.,\nSpark ML\nor\nSpark-NLP\n) were built and deployed like Spark\nClient Applications. With Spark 3.4 and Spark Connect,  explicit extension points are offered to\nextend Spark via Spark Server Libraries. These extension points provide functionality that can be\nexposed to a client, which differs from existing extension points in Spark such as\nSparkSession extensions\nor\nSpark Plugins\n.\nGetting Started: Extending Spark with Spark Server Libraries\nSpark Connect is available and supports PySpark and Scala\napplications. We will walk through how to run an Apache Spark server with Spark\nConnect and connect to it from a client application using the Spark Connect client\nlibrary.\nA Spark Server Library consists of the following components, illustrated in F", "question": "What is used to extend Spark in Spark 3.4 and with Spark Connect?", "answers": {"text": ["Spark Server Libraries"], "answer_start": [225]}}
{"context": "Relation\n{\noneof\nrel_type\n{\nRead\nread\n=\n1\n;\n// ...\ngoogle.protobuf.Any\nextension\n=\n998\n;\n}\n}\nmessage\nExpression\n{\noneof\nexpr_type\n{\nLiteral\nliteral\n=\n1\n;\n// ...\ngoogle.protobuf.Any\nextension\n=\n999\n;\n}\n}\nmessage\nCommand\n{\noneof\ncommand_type\n{\nWriteCommand\nwrite_command\n=\n1\n;\n// ...\ngoogle.protobuf.Any\nextension\n=\n999\n;\n}\n}\nTheir extension fields allow serializing arbitrary protobuf messages as part of the Spark Connect\nprotocol. These messages represent the parameters or state of the extension implementation.\nTo build a custom expression type, the developer first defines the custom protobuf definition\nof the expression.\nmessage\nExamplePluginExpression\n{\nExpression\nchild\n=\n1\n;\nstring\ncustom_field\n=\n2\n;\n}\n(2) Spark Connect Plugin implementation with (3) custom application logic\nAs a next step", "question": "What do extension fields allow in the Spark Connect protocol?", "answers": {"text": ["These messages represent the parameters or state of the extension implementation."], "answer_start": [432]}}
{"context": "ark will\nload the values at startup and make them available for processing.\n(4) Spark Server Library Client Package\nOnce the server component is deployed, any client can use it with the right protobuf messages.\nIn the example above, the following message payload sent to the Spark Connect endpoint would be\nenough to trigger the extension mechanism.\n{\n\"project\"\n:\n{\n\"input\"\n:\n{\n\"sql\"\n:\n{\n\"query\"\n:\n\"select * from samples.nyctaxi.trips\"\n}\n},\n\"expressions\"\n:\n[\n{\n\"extension\"\n:\n{\n\"typeUrl\"\n:\n\"type.googleapis.com/spark.connect.ExamplePluginExpression\"\n,\n\"value\"\n:\n\"\n\\n\\0\n06\n\\0\n22\n\\0\n04\n\\n\\0\n02id\n\\0\n22\n\\0\n06testval\"\n}\n}\n]\n}\n}\nTo make the example available in Python, the application developer provides a Python library that\nwraps the new expression and embeds it into PySpark. The easiest way to provide", "question": "What query is sent to the Spark Connect endpoint in the example?", "answers": {"text": ["select * from samples.nyctaxi.trips"], "answer_start": [399]}}
{"context": "lan\n(\nself\n,\nsession\n)\n->\nproto\n.\nExpression\n:\nfun\n=\nproto\n.\nExpression\n()\nplugin\n=\nExamplePluginExpression\n()\nplugin\n.\nchild\n.\nliteral\n.\nlong\n=\n10\nplugin\n.\ncustom_field\n=\n\"\nexample\n\"\nfun\n.\nextension\n.\nPack\n(\nplugin\n)\nreturn\nfun\n# Defining the function to be used from the consumers.\ndef\nexample_expression\n(\ncol\n:\nColumn\n)\n->\nColumn\n:\nreturn\nColumn\n(\nExampleExpression\n())\n# Using the expression in the Spark Connect client code.\ndf\n=\nspark\n.\nread\n.\ntable\n(\n\"\nsamples.nyctaxi.trips\n\"\n)\ndf\n.\nselect\n(\nexample_expression\n(\ndf\n[\n\"\nfare_amount\n\"\n])).\ncollect\n()", "question": "What is the value assigned to `plugin.child.literal.long`?", "answers": {"text": ["10"], "answer_start": [145]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars"], "answer_start": [46]}}
{"context": "ures\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nAdvanced topics\nOptimization of linear methods (developer)\nLimited-memory BFGS (L-BFGS)\nNormal equation solver for weighted least squares\nIteratively reweighted least squares (IRLS)\n\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}} \n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\wv}{\\mathbf{w}}\n\\newcommand{\\av}{\\mathbf{\\alpha}}\n\\newcommand{\\bv}{\\mathbf{b}}\n\\new", "question": "Quais são alguns dos tópicos avançados mencionados no texto?", "answers": {"text": ["Advanced topics"], "answer_start": [121]}}
{"context": "m_{k=1}^n w_k} + \\frac{\\lambda}{\\delta}\\left[\\frac{1}{2}(1 - \\alpha)\\sum_{j=1}^m(\\sigma_j x_j)^2 + \\alpha\\sum_{j=1}^m |\\sigma_j x_j|\\right]\n\\]\nwhere $\\lambda$ is the regularization parameter, $\\alpha$ is the elastic-net mixing parameter, $\\delta$ is the population standard deviation of the label\nand $\\sigma_j$ is the population standard deviation of the j-th feature column.\nThis objective function requires only one pass over the data to collect the statistics necessary to solve it. For an\n$n \\times m$ data matrix, these statistics require only $O(m^2)$ storage and so can be stored on a single machine when $m$ (the number of features) is\nrelatively small. We can then solve the normal equations on a single machine using local methods like direct Cholesky factorization or iterative optimizati", "question": "What is the storage requirement for the statistics needed to solve the objective function for an n x m data matrix?", "answers": {"text": ["these statistics require only $O(m^2)$ storage"], "answer_start": [520]}}
{"context": "ively small. We can then solve the normal equations on a single machine using local methods like direct Cholesky factorization or iterative optimization programs.\nSpark MLlib currently supports two types of solvers for the normal equations: Cholesky factorization and Quasi-Newton methods (L-BFGS/OWL-QN). Cholesky factorization\ndepends on a positive definite covariance matrix (i.e. columns of the data matrix must be linearly independent) and will fail if this condition is violated. Quasi-Newton methods\nare still capable of providing a reasonable solution even when the covariance matrix is not positive definite, so the normal equation solver can also fall back to \nQuasi-Newton methods in this case. This fallback is currently always enabled for the\nLinearRegression\nand\nGeneralizedLinearRegres", "question": "What are the two types of solvers supported by Spark MLlib for the normal equations?", "answers": {"text": ["Cholesky factorization and Quasi-Newton methods (L-BFGS/OWL-QN)."], "answer_start": [241]}}
{"context": "n also fall back to \nQuasi-Newton methods in this case. This fallback is currently always enabled for the\nLinearRegression\nand\nGeneralizedLinearRegression\nestimators.\nWeightedLeastSquares\nsupports L1, L2, and elastic-net regularization and provides options to enable or disable regularization and standardization. In the case where no \nL1 regularization is applied (i.e. $\\alpha = 0$), there exists an analytical solution and either Cholesky or Quasi-Newton solver may be used. When $\\alpha > 0$ no analytical \nsolution exists and we instead use the Quasi-Newton solver to find the coefficients iteratively.\nIn order to make the normal equation approach efficient,\nWeightedLeastSquares\nrequires that the number of features is no more than 4096. For larger problems, use L-BFGS instead.\nIteratively re", "question": "What solver is used when no L1 regularization is applied in WeightedLeastSquares?", "answers": {"text": ["either Cholesky or Quasi-Newton solver may be used."], "answer_start": [426]}}
{"context": "ach efficient,\nWeightedLeastSquares\nrequires that the number of features is no more than 4096. For larger problems, use L-BFGS instead.\nIteratively reweighted least squares (IRLS)\nMLlib implements\niteratively reweighted least squares (IRLS)\nby\nIterativelyReweightedLeastSquares\n.\nIt can be used to find the maximum likelihood estimates of a generalized linear model (GLM), find M-estimator in robust regression and other optimization problems.\nRefer to\nIteratively Reweighted Least Squares for Maximum Likelihood Estimation, and some Robust and Resistant Alternatives\nfor more information.\nIt solves certain optimization problems iteratively through the following procedure:\nlinearize the objective at current solution and update corresponding weight.\nsolve a weighted least squares (WLS) problem by ", "question": "What is the limit on the number of features for WeightedLeastSquares?", "answers": {"text": ["requires that the number of features is no more than 4096."], "answer_start": [36]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available for Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)"], "answer_start": [46]}}
{"context": "ures\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nDecision Trees - RDD-based API\nBasic algorithm\nNode impurity and information gain\nSplit candidates\nStopping rule\nUsage tips\nProblem specification parameters\nStopping criteria\nTunable parameters\nCaching and checkpointing\nScaling\nExamples\nClassification\nRegression\nDecision trees\nand their ensembles are popular methods for the machine learning tasks of\nclassification and regression. Decision tr", "question": "What are popular methods for the machine learning tasks of classification and regression?", "answers": {"text": ["Decision Trees - RDD-based API"], "answer_start": [406]}}
{"context": "y(D_{right})$\nSplit candidates\nContinuous features\nFor small datasets in single-machine implementations, the split candidates for each continuous\nfeature are typically the unique values for the feature. Some implementations sort the feature\nvalues and then use the ordered unique values as split candidates for faster tree calculations.\nSorting feature values is expensive for large distributed datasets.\nThis implementation computes an approximate set of split candidates by performing a quantile\ncalculation over a sampled fraction of the data.\nThe ordered splits create “bins” and the maximum number of such\nbins can be specified using the\nmaxBins\nparameter.\nNote that the number of bins cannot be greater than the number of instances\n$N$\n(a rare scenario\nsince the default\nmaxBins\nvalue is 32). T", "question": "How are split candidates determined for continuous features in small datasets with single-machine implementations?", "answers": {"text": ["the unique values for the feature."], "answer_start": [168]}}
{"context": "\nparameter.\nNote that the number of bins cannot be greater than the number of instances\n$N$\n(a rare scenario\nsince the default\nmaxBins\nvalue is 32). The tree algorithm automatically reduces the number of\nbins if the condition is not satisfied.\nCategorical features\nFor a categorical feature with\n$M$\npossible values (categories), one could come up with\n$2^{M-1}-1$\nsplit candidates. For binary (0/1) classification and regression,\nwe can reduce the number of split candidates to\n$M-1$\nby ordering the\ncategorical feature values by the average label. (See Section 9.2.4 in\nElements of Statistical Machine Learning\nfor\ndetails.) For example, for a binary classification problem with one categorical feature with three\ncategories A, B and C whose corresponding proportions of label 1 are 0.2, 0.6 and 0.", "question": "How many split candidates can be reduced for binary classification and regression by ordering categorical feature values?", "answers": {"text": ["M-1"], "answer_start": [357]}}
{"context": "ry classification problem with one categorical feature with three\ncategories A, B and C whose corresponding proportions of label 1 are 0.2, 0.6 and 0.4, the categorical\nfeatures are ordered as A, C, B. The two split candidates are A | C, B\nand A , C | B where | denotes the split.\nIn multiclass classification, all\n$2^{M-1}-1$\npossible splits are used whenever possible.\nWhen\n$2^{M-1}-1$\nis greater than the\nmaxBins\nparameter, we use a (heuristic) method\nsimilar to the method used for binary classification and regression.\nThe\n$M$\ncategorical feature values are ordered by impurity,\nand the resulting\n$M-1$\nsplit candidates are considered.\nStopping rule\nThe recursive tree construction is stopped at a node when one of the following conditions is met:\nThe node depth is equal to the\nmaxDepth\ntrainin", "question": "How many possible splits are used in multiclass classification whenever possible?", "answers": {"text": ["$2^{M-1}-1$"], "answer_start": [315]}}
{"context": "rule\nThe recursive tree construction is stopped at a node when one of the following conditions is met:\nThe node depth is equal to the\nmaxDepth\ntraining parameter.\nNo split candidate leads to an information gain greater than\nminInfoGain\n.\nNo split candidate produces child nodes which each have at least\nminInstancesPerNode\ntraining instances.\nUsage tips\nWe include a few guidelines for using decision trees by discussing the various parameters.\nThe parameters are listed below roughly in order of descending importance.  New users should mainly consider the “Problem specification parameters” section and the\nmaxDepth\nparameter.\nProblem specification parameters\nThese parameters describe the problem you want to solve and your dataset.\nThey should be specified and do not require tuning.\nalgo\n: Type ", "question": "Quais parâmetros devem ser especificados e não requerem ajuste?", "answers": {"text": ["They should be specified and do not require tuning."], "answer_start": [736]}}
{"context": " parameters\nThese parameters describe the problem you want to solve and your dataset.\nThey should be specified and do not require tuning.\nalgo\n: Type of decision tree, either\nClassification\nor\nRegression\n.\nnumClasses\n: Number of classes (for\nClassification\nonly).\ncategoricalFeaturesInfo\n: Specifies which features are categorical and how many categorical values each of those features can take.  This is given as a map from feature indices to feature arity (number of categories).  Any features not in this map are treated as continuous.\nFor example,\nMap(0 -> 2, 4 -> 10)\nspecifies that feature\n0\nis binary (taking values\n0\nor\n1\n) and that feature\n4\nhas 10 categories (values\n{0, 1, ..., 9}\n).  Note that feature indices are 0-based: features\n0\nand\n4\nare the 1st and 5th elements of an instance’s fe", "question": "What does the 'categoricalFeaturesInfo' parameter specify?", "answers": {"text": ["Specifies which features are categorical and how many categorical values each of those features can take."], "answer_start": [290]}}
{"context": "\nhas 10 categories (values\n{0, 1, ..., 9}\n).  Note that feature indices are 0-based: features\n0\nand\n4\nare the 1st and 5th elements of an instance’s feature vector.\nNote that you do not have to specify\ncategoricalFeaturesInfo\n.  The algorithm will still run and may get reasonable results.  However, performance should be better if categorical features are properly designated.\nStopping criteria\nThese parameters determine when the tree stops building (adding new nodes).\nWhen tuning these parameters, be careful to validate on held-out test data to avoid overfitting.\nmaxDepth\n: Maximum depth of a tree.  Deeper trees are more expressive (potentially allowing higher accuracy), but they are also more costly to train and are more likely to overfit.\nminInstancesPerNode\n: For a node to be split furthe", "question": "What does 'maxDepth' control?", "answers": {"text": ["Maximum depth of a tree.  Deeper trees are more expressive (potentially allowing higher accuracy), but they are also more costly to train and are more likely to overfit."], "answer_start": [579]}}
{"context": "g returns as\nmaxMemoryInMB\ngrows since the amount of communication on each iteration can be proportional to\nmaxMemoryInMB\n.\nImplementation details\n: For faster processing, the decision tree algorithm collects statistics about groups of nodes to split (rather than 1 node at a time).  The number of nodes which can be handled in one group is determined by the memory requirements (which vary per features).  The\nmaxMemoryInMB\nparameter specifies the memory limit in terms of megabytes which each worker can use for these statistics.\nsubsamplingRate\n: Fraction of the training data used for learning the decision tree.  This parameter is most relevant for training ensembles of trees (using\nRandomForest\nand\nGradientBoostedTrees\n), where it can be useful to subsample the original data.  For training a", "question": "What does the maxMemoryInMB parameter specify?", "answers": {"text": ["the memory limit in terms of megabytes which each worker can use for these statistics."], "answer_start": [445]}}
{"context": "also useful for\nRandomForest\nwhen\nnumTrees\nis set to be large.\nuseNodeIdCache\n: If this is set to true, the algorithm will avoid passing the current model (tree or trees) to executors on each iteration.\nThis can be useful with deep trees (speeding up computation on workers) and for large Random Forests (reducing communication on each iteration).\nImplementation details\n: By default, the algorithm communicates the current model to executors so that executors can match training instances with tree nodes.  When this setting is turned on, then the algorithm will instead cache this information.\nNode ID caching generates a sequence of RDDs (1 per iteration).  This long lineage can cause performance problems, but checkpointing intermediate RDDs can alleviate those problems.\nNote that checkpointing", "question": "What does setting 'useNodeIdCache' to true avoid?", "answers": {"text": ["avoid passing the current model (tree or trees) to executors on each iteration."], "answer_start": [123]}}
{"context": "eration).  This long lineage can cause performance problems, but checkpointing intermediate RDDs can alleviate those problems.\nNote that checkpointing is only applicable when\nuseNodeIdCache\nis set to true.\ncheckpointDir\n: Directory for checkpointing node ID cache RDDs.\ncheckpointInterval\n: Frequency for checkpointing node ID cache RDDs.  Setting this too low will cause extra overhead from writing to HDFS; setting this too high can cause problems if executors fail and the RDD needs to be recomputed.\nScaling\nComputation scales approximately linearly in the number of training instances,\nin the number of features, and in the\nmaxBins\nparameter.\nCommunication scales approximately linearly in the number of features and in\nmaxBins\n.\nThe implemented algorithm reads both sparse and dense data. Howev", "question": "What happens if the checkpointInterval is set too high?", "answers": {"text": ["setting this too high can cause problems if executors fail and the RDD needs to be recomputed."], "answer_start": [409]}}
{"context": "\n,\nDecisionTreeModel\nfrom\npyspark.mllib.util\nimport\nMLUtils\n# Load and parse the data file into an RDD of LabeledPoint.\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n'\ndata/mllib/sample_libsvm_data.txt\n'\n)\n# Split the data into training and test sets (30% held out for testing)\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n([\n0.7\n,\n0.3\n])\n# Train a DecisionTree model.\n#  Empty categoricalFeaturesInfo indicates all features are continuous.\nmodel\n=\nDecisionTree\n.\ntrainClassifier\n(\ntrainingData\n,\nnumClasses\n=\n2\n,\ncategoricalFeaturesInfo\n=\n{},\nimpurity\n=\n'\ngini\n'\n,\nmaxDepth\n=\n5\n,\nmaxBins\n=\n32\n)\n# Evaluate model on test instances and compute test error\npredictions\n=\nmodel\n.\npredict\n(\ntestData\n.\nmap\n(\nlambda\nx\n:\nx\n.\nfeatures\n))\nlabelsAndPredictions\n=\ntestData\n.\nmap\n(\nlambda\nlp\n:\nlp\n.\nlabel\n).\nzip\n(\np", "question": "What does an empty categoricalFeaturesInfo indicate when training a DecisionTree model?", "answers": {"text": ["Empty categoricalFeaturesInfo indicates all features are continuous."], "answer_start": [369]}}
{"context": "\npredictions\n=\nmodel\n.\npredict\n(\ntestData\n.\nmap\n(\nlambda\nx\n:\nx\n.\nfeatures\n))\nlabelsAndPredictions\n=\ntestData\n.\nmap\n(\nlambda\nlp\n:\nlp\n.\nlabel\n).\nzip\n(\npredictions\n)\ntestErr\n=\nlabelsAndPredictions\n.\nfilter\n(\nlambda\nlp\n:\nlp\n[\n0\n]\n!=\nlp\n[\n1\n]).\ncount\n()\n/\nfloat\n(\ntestData\n.\ncount\n())\nprint\n(\n'\nTest Error =\n'\n+\nstr\n(\ntestErr\n))\nprint\n(\n'\nLearned classification tree model:\n'\n)\nprint\n(\nmodel\n.\ntoDebugString\n())\n# Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"\ntarget/tmp/myDecisionTreeClassificationModel\n\"\n)\nsameModel\n=\nDecisionTreeModel\n.\nload\n(\nsc\n,\n\"\ntarget/tmp/myDecisionTreeClassificationModel\n\"\n)\nFind full example code at \"examples/src/main/python/mllib/decision_tree_classification_example.py\" in the Spark repo.\nRefer to the\nDecisionTree\nScala docs\nand\nDecisionTreeModel\nScala docs\nfor details on th", "question": "Where can I find the full example code for this decision tree classification?", "answers": {"text": ["Find full example code at \"examples/src/main/python/mllib/decision_tree_classification_example.py\" in the Spark repo."], "answer_start": [595]}}
{"context": "tErr\"\n)\nprintln\n(\ns\n\"Learned classification tree model:\\n ${model.toDebugString}\"\n)\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/myDecisionTreeClassificationModel\"\n)\nval\nsameModel\n=\nDecisionTreeModel\n.\nload\n(\nsc\n,\n\"target/tmp/myDecisionTreeClassificationModel\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\" in the Spark repo.\nRefer to the\nDecisionTree\nJava docs\nand\nDecisionTreeModel\nJava docs\nfor details on the API.\nimport\njava.util.HashMap\n;\nimport\njava.util.Map\n;\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.SparkConf\n;\nimport\norg.apache.spark.api.java.JavaPairRDD\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.api.java.JavaSparkContext\n;\nimport\norg.apache.spark.mllib.regression.L", "question": "Where can I find a full example code for DecisionTreeClassification?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\" in the Spark repo."], "answer_start": [273]}}
{"context": "l\"\n);\nDecisionTreeModel\nsameModel\n=\nDecisionTreeModel\n.\nload\n(\njsc\n.\nsc\n(),\n\"target/tmp/myDecisionTreeClassificationModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\" in the Spark repo.\nRegression\nThe example below demonstrates how to load a\nLIBSVM data file\n,\nparse it as an RDD of\nLabeledPoint\nand then\nperform regression using a decision tree with variance as an impurity measure and a maximum tree\ndepth of 5. The Mean Squared Error (MSE) is computed at the end to evaluate\ngoodness of fit\n.\nRefer to the\nDecisionTree\nPython docs\nand\nDecisionTreeModel\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.tree\nimport\nDecisionTree\n,\nDecisionTreeModel\nfrom\npyspark.mllib.util\nimport\nMLUtils\n# Load and pars", "question": "Where can I find a full example code for DecisionTreeClassification?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\" in the Spark repo."], "answer_start": [126]}}
{"context": "2\n)\n# Evaluate model on test instances and compute test error\npredictions\n=\nmodel\n.\npredict\n(\ntestData\n.\nmap\n(\nlambda\nx\n:\nx\n.\nfeatures\n))\nlabelsAndPredictions\n=\ntestData\n.\nmap\n(\nlambda\nlp\n:\nlp\n.\nlabel\n).\nzip\n(\npredictions\n)\ntestMSE\n=\nlabelsAndPredictions\n.\nmap\n(\nlambda\nlp\n:\n(\nlp\n[\n0\n]\n-\nlp\n[\n1\n])\n*\n(\nlp\n[\n0\n]\n-\nlp\n[\n1\n])).\nsum\n()\n/\n\\\nfloat\n(\ntestData\n.\ncount\n())\nprint\n(\n'\nTest Mean Squared Error =\n'\n+\nstr\n(\ntestMSE\n))\nprint\n(\n'\nLearned regression tree model:\n'\n)\nprint\n(\nmodel\n.\ntoDebugString\n())\n# Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"\ntarget/tmp/myDecisionTreeRegressionModel\n\"\n)\nsameModel\n=\nDecisionTreeModel\n.\nload\n(\nsc\n,\n\"\ntarget/tmp/myDecisionTreeRegressionModel\n\"\n)\nFind full example code at \"examples/src/main/python/mllib/decision_tree_regression_example.py\" in the Spark repo.\nRefer", "question": "Where can I find the full example code for the decision tree regression?", "answers": {"text": ["Find full example code at \"examples/src/main/python/mllib/decision_tree_regression_example.py\" in the Spark repo."], "answer_start": [681]}}
{"context": "ecisionTreeRegressionModel\n\"\n)\nFind full example code at \"examples/src/main/python/mllib/decision_tree_regression_example.py\" in the Spark repo.\nRefer to the\nDecisionTree\nScala docs\nand\nDecisionTreeModel\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.tree.DecisionTree\nimport\norg.apache.spark.mllib.tree.model.DecisionTreeModel\nimport\norg.apache.spark.mllib.util.MLUtils\n// Load and parse the data file.\nval\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Split the data into training and test sets (30% held out for testing)\nval\nsplits\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.7\n,\n0.3\n))\nval\n(\ntrainingData\n,\ntestData\n)\n=\n(\nsplits\n(\n0\n),\nsplits\n(\n1\n))\n// Train a DecisionTree model.\n//  Empty categoricalFeaturesInfo indicates all features are continuous.\nv", "question": "Where can I find a full example code for DecisionTreeRegressionModel?", "answers": {"text": ["Find full example code at \"examples/src/main/python/mllib/decision_tree_regression_example.py\" in the Spark repo."], "answer_start": [31]}}
{"context": ",\ntestData\n)\n=\n(\nsplits\n(\n0\n),\nsplits\n(\n1\n))\n// Train a DecisionTree model.\n//  Empty categoricalFeaturesInfo indicates all features are continuous.\nval\ncategoricalFeaturesInfo\n=\nMap\n[\nInt\n,\nInt\n]()\nval\nimpurity\n=\n\"variance\"\nval\nmaxDepth\n=\n5\nval\nmaxBins\n=\n32\nval\nmodel\n=\nDecisionTree\n.\ntrainRegressor\n(\ntrainingData\n,\ncategoricalFeaturesInfo\n,\nimpurity\n,\nmaxDepth\n,\nmaxBins\n)\n// Evaluate model on test instances and compute test error\nval\nlabelsAndPredictions\n=\ntestData\n.\nmap\n{\npoint\n=>\nval\nprediction\n=\nmodel\n.\npredict\n(\npoint\n.\nfeatures\n)\n(\npoint\n.\nlabel\n,\nprediction\n)\n}\nval\ntestMSE\n=\nlabelsAndPredictions\n.\nmap\n{\ncase\n(\nv\n,\np\n)\n=>\nmath\n.\npow\n(\nv\n-\np\n,\n2\n)\n}.\nmean\n()\nprintln\n(\ns\n\"Test Mean Squared Error = $testMSE\"\n)\nprintln\n(\ns\n\"Learned regression tree model:\\n ${model.toDebugString}\"\n)\n// Sa", "question": "What is the value assigned to the 'impurity' parameter when training the DecisionTree model?", "answers": {"text": ["\"variance\""], "answer_start": [214]}}
{"context": "\n-\np\n,\n2\n)\n}.\nmean\n()\nprintln\n(\ns\n\"Test Mean Squared Error = $testMSE\"\n)\nprintln\n(\ns\n\"Learned regression tree model:\\n ${model.toDebugString}\"\n)\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/myDecisionTreeRegressionModel\"\n)\nval\nsameModel\n=\nDecisionTreeModel\n.\nload\n(\nsc\n,\n\"target/tmp/myDecisionTreeRegressionModel\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\" in the Spark repo.\nRefer to the\nDecisionTree\nJava docs\nand\nDecisionTreeModel\nJava docs\nfor details on the API.\nimport\njava.util.HashMap\n;\nimport\njava.util.Map\n;\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.SparkConf\n;\nimport\norg.apache.spark.api.java.JavaPairRDD\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.api.java.JavaSparkCon", "question": "Where can I find the full example code for DecisionTreeRegression?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\" in the Spark repo."], "answer_start": [326]}}
{"context": "\nloadLibSVMFile\n(\njsc\n.\nsc\n(),\ndatapath\n).\ntoJavaRDD\n();\n// Split the data into training and test sets (30% held out for testing)\nJavaRDD\n<\nLabeledPoint\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]{\n0.7\n,\n0.3\n});\nJavaRDD\n<\nLabeledPoint\n>\ntrainingData\n=\nsplits\n[\n0\n];\nJavaRDD\n<\nLabeledPoint\n>\ntestData\n=\nsplits\n[\n1\n];\n// Set parameters.\n// Empty categoricalFeaturesInfo indicates all features are continuous.\nMap\n<\nInteger\n,\nInteger\n>\ncategoricalFeaturesInfo\n=\nnew\nHashMap\n<>();\nString\nimpurity\n=\n\"variance\"\n;\nint\nmaxDepth\n=\n5\n;\nint\nmaxBins\n=\n32\n;\n// Train a DecisionTree model.\nDecisionTreeModel\nmodel\n=\nDecisionTree\n.\ntrainRegressor\n(\ntrainingData\n,\ncategoricalFeaturesInfo\n,\nimpurity\n,\nmaxDepth\n,\nmaxBins\n);\n// Evaluate model on test instances and compute test error\nJavaPairRDD\n<\nDouble\n,\nDoubl", "question": "What percentage of the data is held out for testing?", "answers": {"text": ["30% held out for testing"], "answer_start": [104]}}
{"context": "\"\n);\nDecisionTreeModel\nsameModel\n=\nDecisionTreeModel\n.\nload\n(\njsc\n.\nsc\n(),\n\"target/tmp/myDecisionTreeRegressionModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\" in the Spark repo.", "question": "Where can I find a full example code for the DecisionTreeModel?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\" in the Spark repo."], "answer_start": [121]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars"], "answer_start": [46]}}
{"context": "lel.\nOn the other hand, it is often reasonable to use smaller (shallower) trees with GBTs than with Random Forests, and training smaller trees takes less time.\nRandom Forests can be less prone to overfitting.  Training more trees in a Random Forest reduces the likelihood of overfitting, but training more trees with GBTs increases the likelihood of overfitting.  (In statistical language, Random Forests reduce variance by using more trees, whereas GBTs reduce bias by using more trees.)\nRandom Forests can be easier to tune since performance improves monotonically with the number of trees (whereas performance can start to decrease for GBTs if the number of trees grows too large).\nIn short, both algorithms can be effective, and the choice should be based on the particular dataset.\nRandom Forest", "question": "What happens to the likelihood of overfitting when training more trees with GBTs?", "answers": {"text": ["training more trees with GBTs increases the likelihood of overfitting."], "answer_start": [292]}}
{"context": " number of trees grows too large).\nIn short, both algorithms can be effective, and the choice should be based on the particular dataset.\nRandom Forests\nRandom forests\nare ensembles of\ndecision trees\n.\nRandom forests are one of the most successful machine learning models for classification and\nregression.  They combine many decision trees in order to reduce the risk of overfitting.\nLike decision trees, random forests handle categorical features,\nextend to the multiclass classification setting, do not require\nfeature scaling, and are able to capture non-linearities and feature interactions.\nspark.mllib\nsupports random forests for binary and multiclass classification and for regression,\nusing both continuous and categorical features.\nspark.mllib\nimplements random forests using the existing\nde", "question": "What type of features do random forests handle?", "answers": {"text": ["Like decision trees, random forests handle categorical features,"], "answer_start": [384]}}
{"context": "ludes:\nSubsampling the original dataset on each iteration to get a different training set (a.k.a. bootstrapping).\nConsidering different random subsets of features to split on at each tree node.\nApart from these randomizations, decision tree training is done in the same way as for individual decision trees.\nPrediction\nTo make a prediction on a new instance, a random forest must aggregate the predictions from its set of decision trees.  This aggregation is done differently for classification and regression.\nClassification\n: Majority vote. Each tree’s prediction is counted as a vote for one class.  The label is predicted to be the class which receives the most votes.\nRegression\n: Averaging. Each tree predicts a real value.  The label is predicted to be the average of the tree predictions.\nUsa", "question": "How is prediction done in a random forest for classification tasks?", "answers": {"text": ["Majority vote. Each tree’s prediction is counted as a vote for one class.  The label is predicted to be the class which receives the most votes."], "answer_start": [528]}}
{"context": "ceives the most votes.\nRegression\n: Averaging. Each tree predicts a real value.  The label is predicted to be the average of the tree predictions.\nUsage tips\nWe include a few guidelines for using random forests by discussing the various parameters.\nWe omit some decision tree parameters since those are covered in the\ndecision tree guide\n.\nThe first two parameters we mention are the most important, and tuning them can often improve performance:\nnumTrees\n: Number of trees in the forest.\nIncreasing the number of trees will decrease the variance in predictions, improving the model’s test-time accuracy.\nTraining time increases roughly linearly in the number of trees.\nmaxDepth\n: Maximum depth of each tree in the forest.\nIncreasing the depth makes the model more expressive and powerful.  However, ", "question": "What happens when the number of trees in a random forest is increased?", "answers": {"text": ["Increasing the number of trees will decrease the variance in predictions, improving the model’s test-time accuracy."], "answer_start": [489]}}
{"context": "he number of trees.\nmaxDepth\n: Maximum depth of each tree in the forest.\nIncreasing the depth makes the model more expressive and powerful.  However, deep trees take longer to train and are also more prone to overfitting.\nIn general, it is acceptable to train deeper trees when using random forests than when using a single decision tree.  One tree is more likely to overfit than a random forest (because of the variance reduction from averaging multiple trees in the forest).\nThe next two parameters generally do not require tuning.  However, they can be tuned to speed up training.\nsubsamplingRate\n: This parameter specifies the size of the dataset used for training each tree in the forest, as a fraction of the size of the original dataset.  The default (1.0) is recommended, but decreasing this ", "question": "What happens when the depth of trees is increased?", "answers": {"text": ["Increasing the depth makes the model more expressive and powerful."], "answer_start": [73]}}
{"context": " used for training each tree in the forest, as a fraction of the size of the original dataset.  The default (1.0) is recommended, but decreasing this fraction can speed up training.\nfeatureSubsetStrategy\n: Number of features to use as candidates for splitting at each tree node.  The number is specified as a fraction or function of the total number of features.  Decreasing this number will speed up training, but can sometimes impact performance if too low.\nExamples\nClassification\nThe example below demonstrates how to load a\nLIBSVM data file\n,\nparse it as an RDD of\nLabeledPoint\nand then\nperform classification using a Random Forest.\nThe test error is calculated to measure the algorithm accuracy.\nRefer to the\nRandomForest\nPython docs\nand\nRandomForest\nPython docs\nfor more details on the API.\nfr", "question": "What is the default value for the fraction of the original dataset size used for training each tree?", "answers": {"text": ["The default (1.0) is recommended"], "answer_start": [96]}}
{"context": " =\n'\n+\nstr\n(\ntestErr\n))\nprint\n(\n'\nLearned classification forest model:\n'\n)\nprint\n(\nmodel\n.\ntoDebugString\n())\n# Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"\ntarget/tmp/myRandomForestClassificationModel\n\"\n)\nsameModel\n=\nRandomForestModel\n.\nload\n(\nsc\n,\n\"\ntarget/tmp/myRandomForestClassificationModel\n\"\n)\nFind full example code at \"examples/src/main/python/mllib/random_forest_classification_example.py\" in the Spark repo.\nRefer to the\nRandomForest\nScala docs\nand\nRandomForestModel\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.tree.RandomForest\nimport\norg.apache.spark.mllib.tree.model.RandomForestModel\nimport\norg.apache.spark.mllib.util.MLUtils\n// Load and parse the data file.\nval\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Split the data into trai", "question": "Where can I find the full example code for this random forest classification?", "answers": {"text": ["Find full example code at \"examples/src/main/python/mllib/random_forest_classification_example.py\" in the Spark repo."], "answer_start": [297]}}
{"context": ".MLUtils\n// Load and parse the data file.\nval\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Split the data into training and test sets (30% held out for testing)\nval\nsplits\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.7\n,\n0.3\n))\nval\n(\ntrainingData\n,\ntestData\n)\n=\n(\nsplits\n(\n0\n),\nsplits\n(\n1\n))\n// Train a RandomForest model.\n// Empty categoricalFeaturesInfo indicates all features are continuous.\nval\nnumClasses\n=\n2\nval\ncategoricalFeaturesInfo\n=\nMap\n[\nInt\n,\nInt\n]()\nval\nnumTrees\n=\n3\n// Use more in practice.\nval\nfeatureSubsetStrategy\n=\n\"auto\"\n// Let the algorithm choose.\nval\nimpurity\n=\n\"gini\"\nval\nmaxDepth\n=\n4\nval\nmaxBins\n=\n32\nval\nmodel\n=\nRandomForest\n.\ntrainClassifier\n(\ntrainingData\n,\nnumClasses\n,\ncategoricalFeaturesInfo\n,\nnumTrees\n,\nfeatureSubsetStrategy\n,\nimpurity\n,\nmaxDep", "question": "What percentage of the data is held out for testing?", "answers": {"text": ["30% held out for testing"], "answer_start": [170]}}
{"context": "ssificationModel\"\n)\nval\nsameModel\n=\nRandomForestModel\n.\nload\n(\nsc\n,\n\"target/tmp/myRandomForestClassificationModel\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\" in the Spark repo.\nRefer to the\nRandomForest\nJava docs\nand\nRandomForestModel\nJava docs\nfor details on the API.\nimport\njava.util.HashMap\n;\nimport\njava.util.Map\n;\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.SparkConf\n;\nimport\norg.apache.spark.api.java.JavaPairRDD\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.api.java.JavaSparkContext\n;\nimport\norg.apache.spark.mllib.regression.LabeledPoint\n;\nimport\norg.apache.spark.mllib.tree.RandomForest\n;\nimport\norg.apache.spark.mllib.tree.model.RandomForestModel\n;\nimport\norg.apache.spark.mllib.u", "question": "Where can I find a full example code for RandomForestClassification?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\" in the Spark repo."], "answer_start": [117]}}
{"context": "ategy\n,\nimpurity\n,\nmaxDepth\n,\nmaxBins\n,\nseed\n);\n// Evaluate model on test instances and compute test error\nJavaPairRDD\n<\nDouble\n,\nDouble\n>\npredictionAndLabel\n=\ntestData\n.\nmapToPair\n(\np\n->\nnew\nTuple2\n<>(\nmodel\n.\npredict\n(\np\n.\nfeatures\n()),\np\n.\nlabel\n()));\ndouble\ntestErr\n=\npredictionAndLabel\n.\nfilter\n(\npl\n->\n!\npl\n.\n_1\n().\nequals\n(\npl\n.\n_2\n())).\ncount\n()\n/\n(\ndouble\n)\ntestData\n.\ncount\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Test Error: \"\n+\ntestErr\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Learned classification forest model:\\n\"\n+\nmodel\n.\ntoDebugString\n());\n// Save and load model\nmodel\n.\nsave\n(\njsc\n.\nsc\n(),\n\"target/tmp/myRandomForestClassificationModel\"\n);\nRandomForestModel\nsameModel\n=\nRandomForestModel\n.\nload\n(\njsc\n.\nsc\n(),\n\"target/tmp/myRandomForestClassificationModel\"\n);\nFind full example code at \"examples/src/mai", "question": "Where is the learned classification forest model saved?", "answers": {"text": ["\"target/tmp/myRandomForestClassificationModel\""], "answer_start": [587]}}
{"context": "odel\nsameModel\n=\nRandomForestModel\n.\nload\n(\njsc\n.\nsc\n(),\n\"target/tmp/myRandomForestClassificationModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\" in the Spark repo.\nRegression\nThe example below demonstrates how to load a\nLIBSVM data file\n,\nparse it as an RDD of\nLabeledPoint\nand then\nperform regression using a Random Forest.\nThe Mean Squared Error (MSE) is computed at the end to evaluate\ngoodness of fit\n.\nRefer to the\nRandomForest\nPython docs\nand\nRandomForest\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.tree\nimport\nRandomForest\n,\nRandomForestModel\nfrom\npyspark.mllib.util\nimport\nMLUtils\n# Load and parse the data file into an RDD of LabeledPoint.\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n'\ndata/", "question": "Where can I find a full example code for JavaRandomForestClassificationExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\" in the Spark repo."], "answer_start": [107]}}
{"context": "odel\nfrom\npyspark.mllib.util\nimport\nMLUtils\n# Load and parse the data file into an RDD of LabeledPoint.\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n'\ndata/mllib/sample_libsvm_data.txt\n'\n)\n# Split the data into training and test sets (30% held out for testing)\n(\ntrainingData\n,\ntestData\n)\n=\ndata\n.\nrandomSplit\n([\n0.7\n,\n0.3\n])\n# Train a RandomForest model.\n#  Empty categoricalFeaturesInfo indicates all features are continuous.\n#  Note: Use larger numTrees in practice.\n#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\nmodel\n=\nRandomForest\n.\ntrainRegressor\n(\ntrainingData\n,\ncategoricalFeaturesInfo\n=\n{},\nnumTrees\n=\n3\n,\nfeatureSubsetStrategy\n=\n\"\nauto\n\"\n,\nimpurity\n=\n'\nvariance\n'\n,\nmaxDepth\n=\n4\n,\nmaxBins\n=\n32\n)\n# Evaluate model on test instances and compute test error\npredictions\n=\nmodel\n", "question": "What does an empty categoricalFeaturesInfo indicate?", "answers": {"text": ["Empty categoricalFeaturesInfo indicates all features are continuous."], "answer_start": [353]}}
{"context": "Model\n\"\n)\nsameModel\n=\nRandomForestModel\n.\nload\n(\nsc\n,\n\"\ntarget/tmp/myRandomForestRegressionModel\n\"\n)\nFind full example code at \"examples/src/main/python/mllib/random_forest_regression_example.py\" in the Spark repo.\nRefer to the\nRandomForest\nScala docs\nand\nRandomForestModel\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.tree.RandomForest\nimport\norg.apache.spark.mllib.tree.model.RandomForestModel\nimport\norg.apache.spark.mllib.util.MLUtils\n// Load and parse the data file.\nval\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Split the data into training and test sets (30% held out for testing)\nval\nsplits\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.7\n,\n0.3\n))\nval\n(\ntrainingData\n,\ntestData\n)\n=\n(\nsplits\n(\n0\n),\nsplits\n(\n1\n))\n// Train a RandomForest model.\n// E", "question": "Where can I find a full example code for random forest regression?", "answers": {"text": ["Find full example code at \"examples/src/main/python/mllib/random_forest_regression_example.py\" in the Spark repo."], "answer_start": [101]}}
{"context": "te test error\nval\nlabelsAndPredictions\n=\ntestData\n.\nmap\n{\npoint\n=>\nval\nprediction\n=\nmodel\n.\npredict\n(\npoint\n.\nfeatures\n)\n(\npoint\n.\nlabel\n,\nprediction\n)\n}\nval\ntestMSE\n=\nlabelsAndPredictions\n.\nmap\n{\ncase\n(\nv\n,\np\n)\n=>\nmath\n.\npow\n((\nv\n-\np\n),\n2\n)}.\nmean\n()\nprintln\n(\ns\n\"Test Mean Squared Error = $testMSE\"\n)\nprintln\n(\ns\n\"Learned regression forest model:\\n ${model.toDebugString}\"\n)\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/myRandomForestRegressionModel\"\n)\nval\nsameModel\n=\nRandomForestModel\n.\nload\n(\nsc\n,\n\"target/tmp/myRandomForestRegressionModel\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\" in the Spark repo.\nRefer to the\nRandomForest\nJava docs\nand\nRandomForestModel\nJava docs\nfor details on the API.\nimport\njava", "question": "Where can I find the full example code for this Random Forest Regression?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\" in the Spark repo."], "answer_start": [558]}}
{"context": "e\n(\n\"JavaRandomForestRegressionExample\"\n);\nJavaSparkContext\njsc\n=\nnew\nJavaSparkContext\n(\nsparkConf\n);\n// Load and parse the data file.\nString\ndatapath\n=\n\"data/mllib/sample_libsvm_data.txt\"\n;\nJavaRDD\n<\nLabeledPoint\n>\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\njsc\n.\nsc\n(),\ndatapath\n).\ntoJavaRDD\n();\n// Split the data into training and test sets (30% held out for testing)\nJavaRDD\n<\nLabeledPoint\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]{\n0.7\n,\n0.3\n});\nJavaRDD\n<\nLabeledPoint\n>\ntrainingData\n=\nsplits\n[\n0\n];\nJavaRDD\n<\nLabeledPoint\n>\ntestData\n=\nsplits\n[\n1\n];\n// Set parameters.\n// Empty categoricalFeaturesInfo indicates all features are continuous.\nMap\n<\nInteger\n,\nInteger\n>\ncategoricalFeaturesInfo\n=\nnew\nHashMap\n<>();\nint\nnumTrees\n=\n3\n;\n// Use more in practice.\nString\nfeatureSubsetStrategy\n=\n\"auto\"\n;\n// ", "question": "What percentage of the data is held out for testing?", "answers": {"text": ["30% held out for testing"], "answer_start": [336]}}
{"context": "ures\n()),\np\n.\nlabel\n()));\ndouble\ntestMSE\n=\npredictionAndLabel\n.\nmapToDouble\n(\npl\n->\n{\ndouble\ndiff\n=\npl\n.\n_1\n()\n-\npl\n.\n_2\n();\nreturn\ndiff\n*\ndiff\n;\n}).\nmean\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Test Mean Squared Error: \"\n+\ntestMSE\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Learned regression forest model:\\n\"\n+\nmodel\n.\ntoDebugString\n());\n// Save and load model\nmodel\n.\nsave\n(\njsc\n.\nsc\n(),\n\"target/tmp/myRandomForestRegressionModel\"\n);\nRandomForestModel\nsameModel\n=\nRandomForestModel\n.\nload\n(\njsc\n.\nsc\n(),\n\"target/tmp/myRandomForestRegressionModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\" in the Spark repo.\nGradient-Boosted Trees (GBTs)\nGradient-Boosted Trees (GBTs)\nare ensembles of\ndecision trees\n.\nGBTs iteratively train decision tree", "question": "Where can I find the full example code for JavaRandomForestRegressionExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\" in the Spark repo."], "answer_start": [529]}}
{"context": " in the Spark repo.\nGradient-Boosted Trees (GBTs)\nGradient-Boosted Trees (GBTs)\nare ensembles of\ndecision trees\n.\nGBTs iteratively train decision trees in order to minimize a loss function.\nLike decision trees, GBTs handle categorical features,\nextend to the multiclass classification setting, do not require\nfeature scaling, and are able to capture non-linearities and feature interactions.\nspark.mllib\nsupports GBTs for binary classification and for regression,\nusing both continuous and categorical features.\nspark.mllib\nimplements GBTs using the existing\ndecision tree\nimplementation.  Please see the decision tree guide for more information on trees.\nNote\n: GBTs do not yet support multiclass classification.  For multiclass problems, please use\ndecision trees\nor\nRandom Forests\n.\nBasic algorith", "question": "What do Gradient-Boosted Trees (GBTs) iteratively train?", "answers": {"text": ["GBTs iteratively train decision trees in order to minimize a loss function."], "answer_start": [114]}}
{"context": "rees.\nNote\n: GBTs do not yet support multiclass classification.  For multiclass problems, please use\ndecision trees\nor\nRandom Forests\n.\nBasic algorithm\nGradient boosting iteratively trains a sequence of decision trees.\nOn each iteration, the algorithm uses the current ensemble to predict the label of each training instance and then compares the prediction with the true label.  The dataset is re-labeled to put more emphasis on training instances with poor predictions.  Thus, in the next iteration, the decision tree will help correct for previous mistakes.\nThe specific mechanism for re-labeling instances is defined by a loss function (discussed below).  With each iteration, GBTs further reduce this loss function on the training data.\nLosses\nThe table below lists the losses currently supporte", "question": "What does Gradient boosting iteratively train?", "answers": {"text": ["a sequence of decision trees."], "answer_start": [189]}}
{"context": " below).  With each iteration, GBTs further reduce this loss function on the training data.\nLosses\nThe table below lists the losses currently supported by GBTs in\nspark.mllib\n.\nNote that each loss is applicable to one of classification or regression, not both.\nNotation: $N$ = number of instances. $y_i$ = label of instance $i$.  $x_i$ = features of instance $i$.  $F(x_i)$ = model’s predicted label for instance $i$.\nLoss\nTask\nFormula\nDescription\nLog Loss\nClassification\n$2 \\sum_{i=1}^{N} \\log(1+\\exp(-2 y_i F(x_i)))$\nTwice binomial negative log likelihood.\nSquared Error\nRegression\n$\\sum_{i=1}^{N} (y_i - F(x_i))^2$\nAlso called L2 loss.  Default loss for regression tasks.\nAbsolute Error\nRegression\n$\\sum_{i=1}^{N} |y_i - F(x_i)|$\nAlso called L1 loss.  Can be more robust to outliers than Squared E", "question": "What is the default loss for regression tasks in spark.mllib?", "answers": {"text": ["Also called L2 loss.  Default loss for regression tasks."], "answer_start": [618]}}
{"context": "ss for regression tasks.\nAbsolute Error\nRegression\n$\\sum_{i=1}^{N} |y_i - F(x_i)|$\nAlso called L1 loss.  Can be more robust to outliers than Squared Error.\nUsage tips\nWe include a few guidelines for using GBTs by discussing the various parameters.\nWe omit some decision tree parameters since those are covered in the\ndecision tree guide\n.\nloss\n: See the section above for information on losses and their applicability to tasks (classification vs. regression).  Different losses can give significantly different results, depending on the dataset.\nnumIterations\n: This sets the number of trees in the ensemble.  Each iteration produces one tree.  Increasing this number makes the model more expressive, improving training data accuracy.  However, test-time accuracy may suffer if this is too large.\nlea", "question": "What is another name for Absolute Error?", "answers": {"text": ["Also called L1 loss."], "answer_start": [83]}}
{"context": "of RDD’s as arguments, the\nfirst one being the training dataset and the second being the validation dataset.\nThe training is stopped when the improvement in the validation error is not more than a certain tolerance\n(supplied by the\nvalidationTol\nargument in\nBoostingStrategy\n). In practice, the validation error\ndecreases initially and later increases. There might be cases in which the validation error does not change monotonically,\nand the user is advised to set a large enough negative tolerance and examine the validation curve using\nevaluateEachIteration\n(which gives the error or loss per iteration) to tune the number of iterations.\nExamples\nClassification\nThe example below demonstrates how to load a\nLIBSVM data file\n,\nparse it as an RDD of\nLabeledPoint\nand then\nperform classification usin", "question": "When is the training stopped in BoostingStrategy?", "answers": {"text": ["The training is stopped when the improvement in the validation error is not more than a certain tolerance"], "answer_start": [109]}}
{"context": "\n]\n!=\nlp\n[\n1\n]).\ncount\n()\n/\nfloat\n(\ntestData\n.\ncount\n())\nprint\n(\n'\nTest Error =\n'\n+\nstr\n(\ntestErr\n))\nprint\n(\n'\nLearned classification GBT model:\n'\n)\nprint\n(\nmodel\n.\ntoDebugString\n())\n# Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"\ntarget/tmp/myGradientBoostingClassificationModel\n\"\n)\nsameModel\n=\nGradientBoostedTreesModel\n.\nload\n(\nsc\n,\n\"\ntarget/tmp/myGradientBoostingClassificationModel\n\"\n)\nFind full example code at \"examples/src/main/python/mllib/gradient_boosting_classification_example.py\" in the Spark repo.\nRefer to the\nGradientBoostedTrees\nScala docs\nand\nGradientBoostedTreesModel\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.tree.GradientBoostedTrees\nimport\norg.apache.spark.mllib.tree.configuration.BoostingStrategy\nimport\norg.apache.spark.mllib.tree.model.GradientBoostedTre", "question": "Where can I find the full example code for gradient boosting classification?", "answers": {"text": ["Find full example code at \"examples/src/main/python/mllib/gradient_boosting_classification_example.py\" in the Spark repo."], "answer_start": [387]}}
{"context": "ree.GradientBoostedTrees\nimport\norg.apache.spark.mllib.tree.configuration.BoostingStrategy\nimport\norg.apache.spark.mllib.tree.model.GradientBoostedTreesModel\nimport\norg.apache.spark.mllib.util.MLUtils\n// Load and parse the data file.\nval\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Split the data into training and test sets (30% held out for testing)\nval\nsplits\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.7\n,\n0.3\n))\nval\n(\ntrainingData\n,\ntestData\n)\n=\n(\nsplits\n(\n0\n),\nsplits\n(\n1\n))\n// Train a GradientBoostedTrees model.\n// The defaultParams for Classification use LogLoss by default.\nval\nboostingStrategy\n=\nBoostingStrategy\n.\ndefaultParams\n(\n\"Classification\"\n)\nboostingStrategy\n.\nnumIterations\n=\n3\n// Note: Use more iterations in practice.\nboostingStrategy\n.\ntreeStrategy\n.\n", "question": "What is used as the default loss function for Classification when training a GradientBoostedTrees model?", "answers": {"text": ["The defaultParams for Classification use LogLoss by default."], "answer_start": [549]}}
{"context": "elAndPreds\n.\nfilter\n(\nr\n=>\nr\n.\n_1\n!=\nr\n.\n_2\n).\ncount\n().\ntoDouble\n/\ntestData\n.\ncount\n()\nprintln\n(\ns\n\"Test Error = $testErr\"\n)\nprintln\n(\ns\n\"Learned classification GBT model:\\n ${model.toDebugString}\"\n)\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/myGradientBoostingClassificationModel\"\n)\nval\nsameModel\n=\nGradientBoostedTreesModel\n.\nload\n(\nsc\n,\n\"target/tmp/myGradientBoostingClassificationModel\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\" in the Spark repo.\nRefer to the\nGradientBoostedTrees\nJava docs\nand\nGradientBoostedTreesModel\nJava docs\nfor details on the API.\nimport\njava.util.HashMap\n;\nimport\njava.util.Map\n;\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.SparkConf\n;\nimport\norg.apache.spark.api.jav", "question": "Where can I find a full example code for Gradient Boosting Classification?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\" in the Spark repo."], "answer_start": [406]}}
{"context": "n the API.\nimport\njava.util.HashMap\n;\nimport\njava.util.Map\n;\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.SparkConf\n;\nimport\norg.apache.spark.api.java.JavaPairRDD\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.api.java.JavaSparkContext\n;\nimport\norg.apache.spark.mllib.regression.LabeledPoint\n;\nimport\norg.apache.spark.mllib.tree.GradientBoostedTrees\n;\nimport\norg.apache.spark.mllib.tree.configuration.BoostingStrategy\n;\nimport\norg.apache.spark.mllib.tree.model.GradientBoostedTreesModel\n;\nimport\norg.apache.spark.mllib.util.MLUtils\n;\nSparkConf\nsparkConf\n=\nnew\nSparkConf\n()\n.\nsetAppName\n(\n\"JavaGradientBoostedTreesClassificationExample\"\n);\nJavaSparkContext\njsc\n=\nnew\nJavaSparkContext\n(\nsparkConf\n);\n// Load and parse the data file.\nString\ndatapath\n=\n\"data/mllib/sample_libsvm_da", "question": "What is the name of the Spark application being set?", "answers": {"text": ["JavaGradientBoostedTreesClassificationExample"], "answer_start": [612]}}
{"context": "Example\"\n);\nJavaSparkContext\njsc\n=\nnew\nJavaSparkContext\n(\nsparkConf\n);\n// Load and parse the data file.\nString\ndatapath\n=\n\"data/mllib/sample_libsvm_data.txt\"\n;\nJavaRDD\n<\nLabeledPoint\n>\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\njsc\n.\nsc\n(),\ndatapath\n).\ntoJavaRDD\n();\n// Split the data into training and test sets (30% held out for testing)\nJavaRDD\n<\nLabeledPoint\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]{\n0.7\n,\n0.3\n});\nJavaRDD\n<\nLabeledPoint\n>\ntrainingData\n=\nsplits\n[\n0\n];\nJavaRDD\n<\nLabeledPoint\n>\ntestData\n=\nsplits\n[\n1\n];\n// Train a GradientBoostedTrees model.\n// The defaultParams for Classification use LogLoss by default.\nBoostingStrategy\nboostingStrategy\n=\nBoostingStrategy\n.\ndefaultParams\n(\n\"Classification\"\n);\nboostingStrategy\n.\nsetNumIterations\n(\n3\n);\n// Note: Use more iterations in practice.\n", "question": "What percentage of the data is held out for testing?", "answers": {"text": ["30% held out for testing"], "answer_start": [305]}}
{"context": "BoostingClassificationModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\" in the Spark repo.\nRegression\nThe example below demonstrates how to load a\nLIBSVM data file\n,\nparse it as an RDD of\nLabeledPoint\nand then\nperform regression using Gradient-Boosted Trees with Squared Error as the loss.\nThe Mean Squared Error (MSE) is computed at the end to evaluate\ngoodness of fit\n.\nRefer to the\nGradientBoostedTrees\nPython docs\nand\nGradientBoostedTreesModel\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.tree\nimport\nGradientBoostedTrees\n,\nGradientBoostedTreesModel\nfrom\npyspark.mllib.util\nimport\nMLUtils\n# Load and parse the data file.\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"\ndata/mllib/sample_libsvm_data", "question": "Where can I find a full example code for Java Gradient Boosting Classification?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\" in the Spark repo."], "answer_start": [32]}}
{"context": "odel\n.\npredict\n(\ntestData\n.\nmap\n(\nlambda\nx\n:\nx\n.\nfeatures\n))\nlabelsAndPredictions\n=\ntestData\n.\nmap\n(\nlambda\nlp\n:\nlp\n.\nlabel\n).\nzip\n(\npredictions\n)\ntestMSE\n=\nlabelsAndPredictions\n.\nmap\n(\nlambda\nlp\n:\n(\nlp\n[\n0\n]\n-\nlp\n[\n1\n])\n*\n(\nlp\n[\n0\n]\n-\nlp\n[\n1\n])).\nsum\n()\n/\n\\\nfloat\n(\ntestData\n.\ncount\n())\nprint\n(\n'\nTest Mean Squared Error =\n'\n+\nstr\n(\ntestMSE\n))\nprint\n(\n'\nLearned regression GBT model:\n'\n)\nprint\n(\nmodel\n.\ntoDebugString\n())\n# Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"\ntarget/tmp/myGradientBoostingRegressionModel\n\"\n)\nsameModel\n=\nGradientBoostedTreesModel\n.\nload\n(\nsc\n,\n\"\ntarget/tmp/myGradientBoostingRegressionModel\n\"\n)\nFind full example code at \"examples/src/main/python/mllib/gradient_boosting_regression_example.py\" in the Spark repo.\nRefer to the\nGradientBoostedTrees\nScala docs\nand\nGradientBooste", "question": "Where can I find the full example code for gradient boosting regression?", "answers": {"text": ["Find full example code at \"examples/src/main/python/mllib/gradient_boosting_regression_example.py\" in the Spark repo."], "answer_start": [619]}}
{"context": "Array\n(\n0.7\n,\n0.3\n))\nval\n(\ntrainingData\n,\ntestData\n)\n=\n(\nsplits\n(\n0\n),\nsplits\n(\n1\n))\n// Train a GradientBoostedTrees model.\n// The defaultParams for Regression use SquaredError by default.\nval\nboostingStrategy\n=\nBoostingStrategy\n.\ndefaultParams\n(\n\"Regression\"\n)\nboostingStrategy\n.\nnumIterations\n=\n3\n// Note: Use more iterations in practice.\nboostingStrategy\n.\ntreeStrategy\n.\nmaxDepth\n=\n5\n// Empty categoricalFeaturesInfo indicates all features are continuous.\nboostingStrategy\n.\ntreeStrategy\n.\ncategoricalFeaturesInfo\n=\nMap\n[\nInt\n,\nInt\n]()\nval\nmodel\n=\nGradientBoostedTrees\n.\ntrain\n(\ntrainingData\n,\nboostingStrategy\n)\n// Evaluate model on test instances and compute test error\nval\nlabelsAndPredictions\n=\ntestData\n.\nmap\n{\npoint\n=>\nval\nprediction\n=\nmodel\n.\npredict\n(\npoint\n.\nfeatures\n)\n(\npoint\n.\nlabel\n,", "question": "What is set as the default error metric for Regression in GradientBoostedTrees?", "answers": {"text": ["The defaultParams for Regression use SquaredError by default."], "answer_start": [127]}}
{"context": "es and compute test error\nval\nlabelsAndPredictions\n=\ntestData\n.\nmap\n{\npoint\n=>\nval\nprediction\n=\nmodel\n.\npredict\n(\npoint\n.\nfeatures\n)\n(\npoint\n.\nlabel\n,\nprediction\n)\n}\nval\ntestMSE\n=\nlabelsAndPredictions\n.\nmap\n{\ncase\n(\nv\n,\np\n)\n=>\nmath\n.\npow\n((\nv\n-\np\n),\n2\n)}.\nmean\n()\nprintln\n(\ns\n\"Test Mean Squared Error = $testMSE\"\n)\nprintln\n(\ns\n\"Learned regression GBT model:\\n ${model.toDebugString}\"\n)\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/myGradientBoostingRegressionModel\"\n)\nval\nsameModel\n=\nGradientBoostedTreesModel\n.\nload\n(\nsc\n,\n\"target/tmp/myGradientBoostingRegressionModel\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\" in the Spark repo.\nRefer to the\nGradientBoostedTrees\nJava docs\nand\nGradientBoostedTreesModel\n", "question": "Where can I find the full example code for Gradient Boosting Regression?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\" in the Spark repo."], "answer_start": [583]}}
{"context": ".model.GradientBoostedTreesModel\n;\nimport\norg.apache.spark.mllib.util.MLUtils\n;\nSparkConf\nsparkConf\n=\nnew\nSparkConf\n()\n.\nsetAppName\n(\n\"JavaGradientBoostedTreesRegressionExample\"\n);\nJavaSparkContext\njsc\n=\nnew\nJavaSparkContext\n(\nsparkConf\n);\n// Load and parse the data file.\nString\ndatapath\n=\n\"data/mllib/sample_libsvm_data.txt\"\n;\nJavaRDD\n<\nLabeledPoint\n>\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\njsc\n.\nsc\n(),\ndatapath\n).\ntoJavaRDD\n();\n// Split the data into training and test sets (30% held out for testing)\nJavaRDD\n<\nLabeledPoint\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]{\n0.7\n,\n0.3\n});\nJavaRDD\n<\nLabeledPoint\n>\ntrainingData\n=\nsplits\n[\n0\n];\nJavaRDD\n<\nLabeledPoint\n>\ntestData\n=\nsplits\n[\n1\n];\n// Train a GradientBoostedTrees model.\n// The defaultParams for Regression use SquaredError by default.\nBoosti", "question": "What data file is loaded and parsed in the provided code snippet?", "answers": {"text": ["data/mllib/sample_libsvm_data.txt"], "answer_start": [292]}}
{"context": "LabeledPoint\n>\ntestData\n=\nsplits\n[\n1\n];\n// Train a GradientBoostedTrees model.\n// The defaultParams for Regression use SquaredError by default.\nBoostingStrategy\nboostingStrategy\n=\nBoostingStrategy\n.\ndefaultParams\n(\n\"Regression\"\n);\nboostingStrategy\n.\nsetNumIterations\n(\n3\n);\n// Note: Use more iterations in practice.\nboostingStrategy\n.\ngetTreeStrategy\n().\nsetMaxDepth\n(\n5\n);\n// Empty categoricalFeaturesInfo indicates all features are continuous.\nMap\n<\nInteger\n,\nInteger\n>\ncategoricalFeaturesInfo\n=\nnew\nHashMap\n<>();\nboostingStrategy\n.\ntreeStrategy\n().\nsetCategoricalFeaturesInfo\n(\ncategoricalFeaturesInfo\n);\nGradientBoostedTreesModel\nmodel\n=\nGradientBoostedTrees\n.\ntrain\n(\ntrainingData\n,\nboostingStrategy\n);\n// Evaluate model on test instances and compute test error\nJavaPairRDD\n<\nDouble\n,\nDouble\n>\np", "question": "What is used by default for Regression in the defaultParams?", "answers": {"text": ["SquaredError"], "answer_start": [119]}}
{"context": ");\nGradientBoostedTreesModel\nsameModel\n=\nGradientBoostedTreesModel\n.\nload\n(\njsc\n.\nsc\n(),\n\"target/tmp/myGradientBoostingRegressionModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\" in the Spark repo.", "question": "Where can I find a full example code for JavaGradientBoostingRegression?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\" in the Spark repo."], "answer_start": [139]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nStructured Streaming + Kafka Integration Guide (Kafka broker version 0.10.0 or higher)\nThis page has moved\nhere\n.", "question": "What Kafka broker version is required for Structured Streaming + Kafka Integration Guide?", "answers": {"text": ["Kafka broker version 0.10.0 or higher"], "answer_start": [597]}}
{"context": "moved\nhere\n.", "question": "What action was performed?", "answers": {"text": ["moved"], "answer_start": [0]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nStructured Streaming Programming Guide\nOverview\nGetting Started\nAPIs on DataFrames and Datasets\nPerformance Tips\nAdditional Information\nStructured Streaming + Kafka Integration Guide (Kafka broker version 0.10.0 or higher)\nStructured Streaming integra", "question": "What versions of Kafka are supported with Structured Streaming?", "answers": {"text": ["Structured Streaming + Kafka Integration Guide (Kafka broker version 0.10.0 or higher)"], "answer_start": [685]}}
{"context": "rmance Tips\nAdditional Information\nStructured Streaming + Kafka Integration Guide (Kafka broker version 0.10.0 or higher)\nStructured Streaming integration for Kafka 0.10 to read data from and write data to Kafka.\nLinking\nFor Scala/Java applications using SBT/Maven project definitions, link your application with the following artifact:\ngroupId = org.apache.spark\nartifactId = spark-sql-kafka-0-10_2.13\nversion = 4.0.0\nPlease note that to use the headers functionality, your Kafka client version should be version 0.11.0.0 or up.\nFor Python applications, you need to add this above library and its dependencies when deploying your\napplication. See the\nDeploying\nsubsection below.\nFor experimenting on\nspark-shell\n, you need to add this above library and its dependencies too when invoking\nspark-shell", "question": "Which artifact should be linked for Scala/Java applications using SBT/Maven?", "answers": {"text": ["groupId = org.apache.spark\nartifactId = spark-sql-kafka-0-10_2.13\nversion = 4.0.0"], "answer_start": [337]}}
{"context": "tstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\noption\n(\n\"\nsubscribe\n\"\n,\n\"\ntopic1\n\"\n)\n\\\n.\noption\n(\n\"\nincludeHeaders\n\"\n,\n\"\ntrue\n\"\n)\n\\\n.\nload\n()\ndf\n.\nselectExpr\n(\n\"\nCAST(key AS STRING)\n\"\n,\n\"\nCAST(value AS STRING)\n\"\n,\n\"\nheaders\n\"\n)\n# Subscribe to multiple topics\ndf\n=\nspark\n\\\n.\nreadStream\n\\\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\noption\n(\n\"\nsubscribe\n\"\n,\n\"\ntopic1,topic2\n\"\n)\n\\\n.\nload\n()\ndf\n.\nselectExpr\n(\n\"\nCAST(key AS STRING)\n\"\n,\n\"\nCAST(value AS STRING)\n\"\n)\n# Subscribe to a pattern\ndf\n=\nspark\n\\\n.\nreadStream\n\\\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\noption\n(\n\"\nsubscribePattern\n\"\n,\n\"\ntopic.*\n\"\n)\n\\\n.\nload\n()\ndf\n.\nselectExpr\n(\n\"\nCAST(key AS STRING)\n\"\n,\n\"\nCAST(value AS STRING)", "question": "What bootstrap servers are used when reading from Kafka?", "answers": {"text": ["host1:port1,host2:port2"], "answer_start": [21]}}
{"context": "ort1,host2:port2\n\"\n)\n\\\n.\noption\n(\n\"\nsubscribePattern\n\"\n,\n\"\ntopic.*\n\"\n)\n\\\n.\nload\n()\ndf\n.\nselectExpr\n(\n\"\nCAST(key AS STRING)\n\"\n,\n\"\nCAST(value AS STRING)\n\"\n)\n// Subscribe to 1 topic\nval\ndf\n=\nspark\n.\nreadStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribe\"\n,\n\"topic1\"\n)\n.\nload\n()\ndf\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n)\n.\nas\n[(\nString\n,\nString\n)]\n// Subscribe to 1 topic, with headers\nval\ndf\n=\nspark\n.\nreadStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribe\"\n,\n\"topic1\"\n)\n.\noption\n(\n\"includeHeaders\"\n,\n\"true\"\n)\n.\nload\n()\ndf\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n,\n\"headers\"\n)\n.\nas\n[(\nString\n,\nString\n,\nArray\n[(\nString\n,\nA", "question": "What bootstrap servers are used in the Kafka stream read?", "answers": {"text": ["host1:port1,host2:port2"], "answer_start": [268]}}
{"context": ".\noption\n(\n\"\nsubscribe\n\"\n,\n\"\ntopic1,topic2\n\"\n)\n\\\n.\noption\n(\n\"\nstartingOffsets\n\"\n,\n\"\"\"\n{\n\"\ntopic1\n\"\n:{\n\"\n0\n\"\n:23,\n\"\n1\n\"\n:-2},\n\"\ntopic2\n\"\n:{\n\"\n0\n\"\n:-2}}\n\"\"\"\n)\n\\\n.\noption\n(\n\"\nendingOffsets\n\"\n,\n\"\"\"\n{\n\"\ntopic1\n\"\n:{\n\"\n0\n\"\n:50,\n\"\n1\n\"\n:-1},\n\"\ntopic2\n\"\n:{\n\"\n0\n\"\n:-1}}\n\"\"\"\n)\n\\\n.\nload\n()\ndf\n.\nselectExpr\n(\n\"\nCAST(key AS STRING)\n\"\n,\n\"\nCAST(value AS STRING)\n\"\n)\n# Subscribe to a pattern, at the earliest and latest offsets\ndf\n=\nspark\n\\\n.\nread\n\\\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\noption\n(\n\"\nsubscribePattern\n\"\n,\n\"\ntopic.*\n\"\n)\n\\\n.\noption\n(\n\"\nstartingOffsets\n\"\n,\n\"\nearliest\n\"\n)\n\\\n.\noption\n(\n\"\nendingOffsets\n\"\n,\n\"\nlatest\n\"\n)\n\\\n.\nload\n()\ndf\n.\nselectExpr\n(\n\"\nCAST(key AS STRING)\n\"\n,\n\"\nCAST(value AS STRING)\n\"\n)\n// Subscribe to 1 topic defaults to the ea", "question": "What is the value set for 'kafka.bootstrap.servers'?", "answers": {"text": ["host1:port1,host2:port2"], "answer_start": [500]}}
{"context": "2}}\"\n)\n.\noption\n(\n\"endingOffsets\"\n,\n\"{\\\"topic1\\\":{\\\"0\\\":50,\\\"1\\\":-1},\\\"topic2\\\":{\\\"0\\\":-1}}\"\n)\n.\nload\n();\ndf\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n);\n// Subscribe to a pattern, at the earliest and latest offsets\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\nread\n()\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribePattern\"\n,\n\"topic.*\"\n)\n.\noption\n(\n\"startingOffsets\"\n,\n\"earliest\"\n)\n.\noption\n(\n\"endingOffsets\"\n,\n\"latest\"\n)\n.\nload\n();\ndf\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n);\nEach row in the source has the following schema:\nColumn\nType\nkey\nbinary\nvalue\nbinary\ntopic\nstring\npartition\nint\noffset\nlong\ntimestamp\ntimestamp\ntimestampType\nint\nheaders (optional)\narray\nThe following options must be set for the K", "question": "What data types are present in each row of the source?", "answers": {"text": ["Column\nType\nkey\nbinary\nvalue\nbinary\ntopic\nstring\npartition\nint\noffset\nlong\ntimestamp\ntimestamp\ntimestampType\nint\nheaders (optional)\narray"], "answer_start": [619]}}
{"context": " started, a json string specifying a starting timestamp for\n  each TopicPartition. Please refer the details on timestamp offset options below. If Kafka doesn't return the matched offset,\n  the behavior will follow to the value of the option\nstartingOffsetsByTimestampStrategy\nNote1:\nstartingOffsetsByTimestamp\ntakes precedence over\nstartingOffsets\n.\nNote2: For streaming queries, this only applies when a new query is started, and that resuming will\n  always pick up from where the query left off. Newly discovered partitions during a query will start at\n  earliest.\nstartingOffsets\n\"earliest\", \"latest\" (streaming only), or json string\n  \"\"\" {\"topicA\":{\"0\":23,\"1\":-1},\"topicB\":{\"0\":-2}} \"\"\"\n\"latest\" for streaming, \"earliest\" for batch\nstreaming and batch\nThe start point when a query is started, ei", "question": "What are the valid options for startingOffsets?", "answers": {"text": ["\"earliest\", \"latest\" (streaming only), or json string"], "answer_start": [583]}}
{"context": "A\":{\"0\":23,\"1\":-1},\"topicB\":{\"0\":-2}} \"\"\"\n\"latest\" for streaming, \"earliest\" for batch\nstreaming and batch\nThe start point when a query is started, either \"earliest\" which is from the earliest offsets,\n  \"latest\" which is just from the latest offsets, or a json string specifying a starting offset for\n  each TopicPartition.  In the json, -2 as an offset can be used to refer to earliest, -1 to latest.\n  Note: For batch queries, latest (either implicitly or by using -1 in json) is not allowed.\n  For streaming queries, this only applies when a new query is started, and that resuming will\n  always pick up from where the query left off. Newly discovered partitions during a query will start at\n  earliest.\nendingTimestamp\ntimestamp string e.g. \"1000\"\nnone (next preference is\nendingOffsetsByTimesta", "question": "What offset value in a JSON string refers to the earliest offset?", "answers": {"text": ["-2 as an offset can be used to refer to earliest"], "answer_start": [339]}}
{"context": "vered partitions during a query will start at\n  earliest.\nendingTimestamp\ntimestamp string e.g. \"1000\"\nnone (next preference is\nendingOffsetsByTimestamp\n)\nbatch query\nThe end point when a batch query is ended, a json string specifying an ending timestamp for\n  all partitions in topics being subscribed. Please refer the details on timestamp offset options below.\n  If Kafka doesn't return the matched offset, the offset will be set to latest.\nNote:\nendingTimestamp\ntakes precedence over\nendingOffsetsByTimestamp\nand\nendingOffsets\n.\nendingOffsetsByTimestamp\njson string\n  \"\"\" {\"topicA\":{\"0\": 1000, \"1\": 1000}, \"topicB\": {\"0\": 2000, \"1\": 2000}} \"\"\"\nnone (next preference is\nendingOffsets\n)\nbatch query\nThe end point when a batch query is ended, a json string specifying an ending timestamp for each To", "question": "What happens if Kafka doesn't return the matched offset during a batch query?", "answers": {"text": ["If Kafka doesn't return the matched offset, the offset will be set to latest."], "answer_start": [366]}}
{"context": "ne (next preference is\nendingOffsets\n)\nbatch query\nThe end point when a batch query is ended, a json string specifying an ending timestamp for each TopicPartition.\n  Please refer the details on timestamp offset options below. If Kafka doesn't return the matched offset,\n  the offset will be set to latest.\nNote:\nendingOffsetsByTimestamp\ntakes precedence over\nendingOffsets\n.\nendingOffsets\nlatest or json string\n  {\"topicA\":{\"0\":23,\"1\":-1},\"topicB\":{\"0\":-1}}\nlatest\nbatch query\nThe end point when a batch query is ended, either \"latest\" which is just referred to the\n  latest, or a json string specifying an ending offset for each TopicPartition.  In the json, -1\n  as an offset can be used to refer to latest, and -2 (earliest) as an offset is not allowed.\nfailOnDataLoss\ntrue or false\ntrue\nstreaming", "question": "What value can be used as an offset in a JSON string to refer to latest?", "answers": {"text": ["-1"], "answer_start": [435]}}
{"context": "the json, -1\n  as an offset can be used to refer to latest, and -2 (earliest) as an offset is not allowed.\nfailOnDataLoss\ntrue or false\ntrue\nstreaming and batch\nWhether to fail the query when it's possible that data is lost (e.g., topics are deleted, or\n  offsets are out of range). This may be a false alarm. You can disable it when it doesn't work\n  as you expected.\nkafkaConsumer.pollTimeoutMs\nlong\n120000\nstreaming and batch\nThe timeout in milliseconds to poll data from Kafka in executors. When not defined it falls\n  back to\nspark.network.timeout\n.\nfetchOffset.numRetries\nint\n3\nstreaming and batch\nNumber of times to retry before giving up fetching Kafka offsets.\nfetchOffset.retryIntervalMs\nlong\n10\nstreaming and batch\nmilliseconds to wait before retrying to fetch Kafka offsets\nmaxOffsetsPerT", "question": "What is the timeout in milliseconds to poll data from Kafka in executors?", "answers": {"text": ["120000"], "answer_start": [402]}}
{"context": "hing Kafka offsets.\nfetchOffset.retryIntervalMs\nlong\n10\nstreaming and batch\nmilliseconds to wait before retrying to fetch Kafka offsets\nmaxOffsetsPerTrigger\nlong\nnone\nstreaming query\nRate limit on maximum number of offsets processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume.\nminOffsetsPerTrigger\nlong\nnone\nstreaming query\nMinimum number of offsets to be processed per trigger interval. The specified total number of\n  offsets will be proportionally split across topicPartitions of different volume. Note, if the\n  maxTriggerDelay is exceeded, a trigger will be fired even if the number of available offsets\n  doesn't reach minOffsetsPerTrigger.\nmaxTriggerDelay\ntime with units\n15m\nstreaming query\nMaximum amou", "question": "What is the purpose of 'maxOffsetsPerTrigger'?", "answers": {"text": ["Rate limit on maximum number of offsets processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume."], "answer_start": [183]}}
{"context": "ller pieces. Please note that this configuration is like a\nhint\n: the\n  number of Spark tasks will be\napproximately\nminPartitions\n. It can be less or more depending on\n  rounding errors or Kafka partitions that didn't receive any new data.\nmaxRecordsPerPartition\nlong\nnone\nstreaming and batch\nLimit maximum number of records present in a partition.\n  By default, Spark has a 1-1 mapping of topicPartitions to Spark partitions consuming from Kafka.\n  If you set this option, Spark will divvy up Kafka partitions to smaller pieces so that each partition\n  has upto\nmaxRecordsPerPartition\nrecords. When both\nminPartitions\nand\nmaxRecordsPerPartition\nare set, number of partitions will be\napproximately\nmax of\n(recordsPerPartition / maxRecordsPerPartition)\nand\nminPartitions\n. In such case spark\n  will di", "question": "How does Spark handle the mapping of topic partitions to Spark partitions when consuming from Kafka?", "answers": {"text": ["By default, Spark has a 1-1 mapping of topicPartitions to Spark partitions consuming from Kafka."], "answer_start": [351]}}
{"context": "nterfere with each other causing each query to read only part of the\n  data. This may also occur when queries are started/restarted in quick succession. To minimize such\n  issues, set the Kafka consumer session timeout (by setting option \"kafka.session.timeout.ms\") to\n  be very small. When this is set, option \"groupIdPrefix\" will be ignored.\nincludeHeaders\nboolean\nfalse\nstreaming and batch\nWhether to include the Kafka headers in the row.\nstartingOffsetsByTimestampStrategy\n\"error\" or \"latest\"\n\"error\"\nstreaming and batch\nThe strategy will be used when the specified starting offset by timestamp (either global or per partition) doesn't match with the offset Kafka returned. Here's the strategy name and corresponding descriptions:\n\"error\": fail the query and end users have to deal with workaroun", "question": "What will happen when the specified starting offset by timestamp doesn't match with the offset Kafka returned?", "answers": {"text": ["The strategy will be used when the specified starting offset by timestamp (either global or per partition) doesn't match with the offset Kafka returned."], "answer_start": [525]}}
{"context": " the offset Kafka returned. Here's the strategy name and corresponding descriptions:\n\"error\": fail the query and end users have to deal with workarounds requiring manual steps.\n\"latest\": assigns the latest offset for these partitions, so that Spark can read newer records from these partitions in further micro-batches.\nDetails on timestamp offset options\nThe returned offset for each partition is the earliest offset whose timestamp is greater than or equal to the given timestamp in the corresponding partition.\nThe behavior varies across options if Kafka doesn’t return the matched offset - check the description of each option.\nSpark simply passes the timestamp information to\nKafkaConsumer.offsetsForTimes\n, and doesn’t interpret or reason about the value.\nFor more details on\nKafkaConsumer.offs", "question": "What does the \"latest\" strategy do with offsets in Kafka?", "answers": {"text": ["\"latest\": assigns the latest offset for these partitions, so that Spark can read newer records from these partitions in further micro-batches."], "answer_start": [177]}}
{"context": "ching\n(default:\nfalse\n)\nwhich allows Spark to use new offset fetching mechanism using\nAdminClient\n. (Set this to\ntrue\nto use old offset fetching with\nKafkaConsumer\n.)\nWhen the new mechanism used the following applies.\nFirst of all the new approach supports Kafka brokers\n0.11.0.0+\n.\nIn Spark 3.0 and below, secure Kafka processing needed the following ACLs from driver perspective:\nTopic resource describe operation\nTopic resource read operation\nGroup resource read operation\nSince Spark 3.1, offsets can be obtained with\nAdminClient\ninstead of\nKafkaConsumer\nand for that the following ACLs needed from driver perspective:\nTopic resource describe operation\nSince\nAdminClient\nin driver is not connecting to consumer group,\ngroup.id\nbased authorization will not work anymore (executors never done group", "question": "What is required for secure Kafka processing in Spark 3.0 and below from the driver's perspective?", "answers": {"text": ["Topic resource describe operation\nTopic resource read operation\nGroup resource read operation"], "answer_start": [382]}}
{"context": "ration\nSince\nAdminClient\nin driver is not connecting to consumer group,\ngroup.id\nbased authorization will not work anymore (executors never done group based authorization).\nWorth to mention executor side is behaving the exact same way like before (group prefix and override works).\nConsumer Caching\nIt’s time-consuming to initialize Kafka consumers, especially in streaming scenarios where processing time is a key factor.\nBecause of this, Spark pools Kafka consumers on executors, by leveraging Apache Commons Pool.\nThe caching key is built up from the following information:\nTopic name\nTopic partition\nGroup ID\nThe following properties are available to configure the consumer pool:\nProperty Name\nDefault\nMeaning\nSince Version\nspark.kafka.consumer.cache.capacity\n64\nThe maximum number of consumers c", "question": "What information is used to build the caching key for Kafka consumers?", "answers": {"text": ["Topic name\nTopic partition\nGroup ID"], "answer_start": [577]}}
{"context": "e to configure the consumer pool:\nProperty Name\nDefault\nMeaning\nSince Version\nspark.kafka.consumer.cache.capacity\n64\nThe maximum number of consumers cached. Please note that it's a soft limit.\n3.0.0\nspark.kafka.consumer.cache.timeout\n5m (5 minutes)\nThe minimum amount of time a consumer may sit idle in the pool before it is eligible for eviction by the evictor.\n3.0.0\nspark.kafka.consumer.cache.evictorThreadRunInterval\n1m (1 minute)\nThe interval of time between runs of the idle evictor thread for consumer pool. When non-positive, no idle evictor thread will be run.\n3.0.0\nspark.kafka.consumer.cache.jmx.enable\nfalse\nEnable or disable JMX for pools created with this configuration instance. Statistics of the pool are available via JMX instance.\n  The prefix of JMX name is set to \"kafka010-cached", "question": "What is the default value for spark.kafka.consumer.cache.capacity?", "answers": {"text": ["64"], "answer_start": [114]}}
{"context": "s created with this configuration instance. Statistics of the pool are available via JMX instance.\n  The prefix of JMX name is set to \"kafka010-cached-simple-kafka-consumer-pool\".\n3.0.0\nThe size of the pool is limited by\nspark.kafka.consumer.cache.capacity\n,\nbut it works as “soft-limit” to not block Spark tasks.\nIdle eviction thread periodically removes consumers which are not used longer than given timeout.\nIf this threshold is reached when borrowing, it tries to remove the least-used entry that is currently not in use.\nIf it cannot be removed, then the pool will keep growing. In the worst case, the pool will grow to\nthe max number of concurrent tasks that can run in the executor (that is, number of task slots).\nIf a task fails for any reason, the new task is executed with a newly created", "question": "What is the prefix of the JMX name for the consumer pool?", "answers": {"text": ["kafka010-cached-simple-kafka-consumer-pool"], "answer_start": [135]}}
{"context": "e key with Kafka consumers pool.\nNote that it doesn’t leverage Apache Commons Pool due to the difference of characteristics.\nThe following properties are available to configure the fetched data pool:\nProperty Name\nDefault\nMeaning\nSince Version\nspark.kafka.consumer.fetchedData.cache.timeout\n5m (5 minutes)\nThe minimum amount of time a fetched data may sit idle in the pool before it is eligible for eviction by the evictor.\n3.0.0\nspark.kafka.consumer.fetchedData.cache.evictorThreadRunInterval\n1m (1 minute)\nThe interval of time between runs of the idle evictor thread for fetched data pool. When non-positive, no idle evictor thread will be run.\n3.0.0\nWriting Data to Kafka\nHere, we describe the support for writing Streaming Queries and Batch Queries to Apache Kafka. Take note that\nApache Kafka on", "question": "What is the default value for spark.kafka.consumer.fetchedData.cache.timeout?", "answers": {"text": ["5m (5 minutes)"], "answer_start": [291]}}
{"context": ".0\nWriting Data to Kafka\nHere, we describe the support for writing Streaming Queries and Batch Queries to Apache Kafka. Take note that\nApache Kafka only supports at least once write semantics. Consequently, when writing—either Streaming Queries\nor Batch Queries—to Kafka, some records may be duplicated; this can happen, for example, if Kafka needs\nto retry a message that was not acknowledged by a Broker, even though that Broker received and wrote the message record.\nStructured Streaming cannot prevent such duplicates from occurring due to these Kafka write semantics. However,\nif writing the query is successful, then you can assume that the query output was written at least once. A possible\nsolution to remove duplicates when reading the written data could be to introduce a primary (unique) k", "question": "What write semantics does Apache Kafka support?", "answers": {"text": ["Apache Kafka only supports at least once write semantics."], "answer_start": [135]}}
{"context": "omatically added (see Kafka semantics on\nhow\nnull\nvalued key values are handled). If a topic column exists then its value\nis used as the topic when writing the given row to Kafka, unless the “topic” configuration\noption is set i.e., the “topic” configuration option overrides the topic column.\nIf a “partition” column is not specified (or its value is\nnull\n)\nthen the partition is calculated by the Kafka producer.\nA Kafka partitioner can be specified in Spark by setting the\nkafka.partitioner.class\noption. If not present, Kafka default partitioner\nwill be used.\nThe following options must be set for the Kafka sink\nfor both batch and streaming queries.\nOption\nvalue\nmeaning\nkafka.bootstrap.servers\nA comma-separated list of host:port\nThe Kafka \"bootstrap.servers\" configuration.\nThe following confi", "question": "What happens if a \"partition\" column is not specified or its value is null?", "answers": {"text": ["then the partition is calculated by the Kafka producer."], "answer_start": [359]}}
{"context": "ies.\nOption\nvalue\nmeaning\nkafka.bootstrap.servers\nA comma-separated list of host:port\nThe Kafka \"bootstrap.servers\" configuration.\nThe following configurations are optional:\nOption\nvalue\ndefault\nquery type\nmeaning\ntopic\nstring\nnone\nstreaming and batch\nSets the topic that all rows will be written to in Kafka. This option overrides any\n  topic column that may exist in the data.\nincludeHeaders\nboolean\nfalse\nstreaming and batch\nWhether to include the Kafka headers in the row.\nCreating a Kafka Sink for Streaming Queries\n# Write key-value data from a DataFrame to a specific Kafka topic specified in an option\nds\n=\ndf\n\\\n.\nselectExpr\n(\n\"\nCAST(key AS STRING)\n\"\n,\n\"\nCAST(value AS STRING)\n\"\n)\n\\\n.\nwriteStream\n\\\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"", "question": "What does the 'kafka.bootstrap.servers' option configure?", "answers": {"text": ["The Kafka \"bootstrap.servers\" configuration."], "answer_start": [86]}}
{"context": "TRING)\n\"\n,\n\"\nCAST(value AS STRING)\n\"\n)\n\\\n.\nwriteStream\n\\\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\noption\n(\n\"\ntopic\n\"\n,\n\"\ntopic1\n\"\n)\n\\\n.\nstart\n()\n# Write key-value data from a DataFrame to Kafka using a topic specified in the data\nds\n=\ndf\n\\\n.\nselectExpr\n(\n\"\ntopic\n\"\n,\n\"\nCAST(key AS STRING)\n\"\n,\n\"\nCAST(value AS STRING)\n\"\n)\n\\\n.\nwriteStream\n\\\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\nstart\n()\n// Write key-value data from a DataFrame to a specific Kafka topic specified in an option\nval\nds\n=\ndf\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n)\n.\nwriteStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"topic\"\n,\n\"to", "question": "What is the format used for writing the stream to Kafka?", "answers": {"text": ["\"kafka\""], "answer_start": [699]}}
{"context": "AST(value AS STRING)\"\n)\n.\nwriteStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"topic\"\n,\n\"topic1\"\n)\n.\nstart\n()\n// Write key-value data from a DataFrame to Kafka using a topic specified in the data\nval\nds\n=\ndf\n.\nselectExpr\n(\n\"topic\"\n,\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n)\n.\nwriteStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\nstart\n()\n// Write key-value data from a DataFrame to a specific Kafka topic specified in an option\nStreamingQuery\nds\n=\ndf\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n)\n.\nwriteStream\n()\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"topic\"\n,\n\"topic1\"\n)\n.\nstart\n();\n// Write key-value", "question": "What Kafka option is used to specify the Kafka servers?", "answers": {"text": ["\"kafka.bootstrap.servers\""], "answer_start": [70]}}
{"context": ".\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"topic\"\n,\n\"topic1\"\n)\n.\nstart\n();\n// Write key-value data from a DataFrame to Kafka using a topic specified in the data\nStreamingQuery\nds\n=\ndf\n.\nselectExpr\n(\n\"topic\"\n,\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n)\n.\nwriteStream\n()\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\nstart\n();\nWriting the output of Batch Queries to Kafka\n# Write key-value data from a DataFrame to a specific Kafka topic specified in an option\ndf\n.\nselectExpr\n(\n\"\nCAST(key AS STRING)\n\"\n,\n\"\nCAST(value AS STRING)\n\"\n)\n\\\n.\nwrite\n\\\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\noption\n(\n\"\ntopic\n\"\n,\n\"\ntopic1\n\"\n)\n\\\n.\nsave\n()\n# W", "question": "What are the bootstrap servers specified for Kafka?", "answers": {"text": ["host1:port1,host2:port2"], "answer_start": [61]}}
{"context": "\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\noption\n(\n\"\ntopic\n\"\n,\n\"\ntopic1\n\"\n)\n\\\n.\nsave\n()\n# Write key-value data from a DataFrame to Kafka using a topic specified in the data\ndf\n.\nselectExpr\n(\n\"\ntopic\n\"\n,\n\"\nCAST(key AS STRING)\n\"\n,\n\"\nCAST(value AS STRING)\n\"\n)\n\\\n.\nwrite\n\\\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\nsave\n()\n// Write key-value data from a DataFrame to a specific Kafka topic specified in an option\ndf\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n)\n.\nwrite\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"topic\"\n,\n\"topic1\"\n)\n.\nsave\n()\n// Write key-value data from a DataFrame to Kafka using a topic spec", "question": "What format is used to write data to Kafka?", "answers": {"text": ["\"kafka\""], "answer_start": [613]}}
{"context": ";\n// Write key-value data from a DataFrame to Kafka using a topic specified in the data\ndf\n.\nselectExpr\n(\n\"topic\"\n,\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n)\n.\nwrite\n()\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\nsave\n();\nProducer Caching\nGiven Kafka producer instance is designed to be thread-safe, Spark initializes a Kafka producer instance and co-use across tasks for same caching key.\nThe caching key is built up from the following information:\nKafka producer configuration\nThis includes configuration for authorization, which Spark will automatically include when delegation token is being used. Even we take authorization into account, you can expect same Kafka producer instance will be used among same Kafka producer configuration.\nIt wi", "question": "What information is used to build up the caching key?", "answers": {"text": ["Kafka producer configuration"], "answer_start": [504]}}
{"context": "used. Even we take authorization into account, you can expect same Kafka producer instance will be used among same Kafka producer configuration.\nIt will use different Kafka producer when delegation token is renewed; Kafka producer instance for old delegation token will be evicted according to the cache policy.\nThe following properties are available to configure the producer pool:\nProperty Name\nDefault\nMeaning\nSince Version\nspark.kafka.producer.cache.timeout\n10m (10 minutes)\nThe minimum amount of time a producer may sit idle in the pool before it is eligible for eviction by the evictor.\n2.2.1\nspark.kafka.producer.cache.evictorThreadRunInterval\n1m (1 minute)\nThe interval of time between runs of the idle evictor thread for producer pool. When non-positive, no idle evictor thread will be run.\n", "question": "What is the default value for spark.kafka.producer.cache.timeout?", "answers": {"text": ["10m (10 minutes)"], "answer_start": [462]}}
{"context": "\n1m (1 minute)\nThe interval of time between runs of the idle evictor thread for producer pool. When non-positive, no idle evictor thread will be run.\n3.0.0\nIdle eviction thread periodically removes producers which are not used longer than given timeout. Note that the producer is shared and used concurrently, so the last used timestamp is determined by the moment the producer instance is returned and reference count is 0.\nKafka Specific Configurations\nKafka’s own configurations can be set via\nDataStreamReader.option\nwith\nkafka.\nprefix, e.g,\nstream.option(\"kafka.bootstrap.servers\", \"host:port\")\n. For possible kafka parameters, see\nKafka consumer config docs\nfor\nparameters related to reading data, and\nKafka producer config docs\nfor parameters related to writing data.\nNote that the following K", "question": "What happens when the interval of time between runs of the idle evictor thread is non-positive?", "answers": {"text": ["When non-positive, no idle evictor thread will be run."], "answer_start": [95]}}
{"context": "me operations to explicitly deserialize the values.\nkey.serializer\n: Keys are always serialized with ByteArraySerializer or StringSerializer. Use\nDataFrame operations to explicitly serialize the keys into either strings or byte arrays.\nvalue.serializer\n: values are always serialized with ByteArraySerializer or StringSerializer. Use\nDataFrame operations to explicitly serialize the values into either strings or byte arrays.\nenable.auto.commit\n: Kafka source doesn’t commit any offset.\ninterceptor.classes\n: Kafka source always read keys and values as byte arrays. It’s not safe to\n use ConsumerInterceptor as it may break the query.\nDeploying\nAs with any Spark applications,\nspark-submit\nis used to launch your application.\nspark-sql-kafka-0-10_2.13\nand its dependencies can be directly added to\nsp", "question": "What serializer types are always used for keys in Kafka?", "answers": {"text": ["Keys are always serialized with ByteArraySerializer or StringSerializer."], "answer_start": [69]}}
{"context": "led\nto\nfalse\n(default:\ntrue\n).\nSpark can be configured to use the following authentication protocols to obtain token (it must match with\nKafka broker configuration):\nSASL SSL (default)\nSSL\nSASL PLAINTEXT (for testing)\nAfter obtaining delegation token successfully, Spark distributes it across nodes and renews it accordingly.\nDelegation token uses\nSCRAM\nlogin module for authentication and because of that the appropriate\nspark.kafka.clusters.${cluster}.sasl.token.mechanism\n(default:\nSCRAM-SHA-512\n) has to be configured. Also, this parameter\nmust match with Kafka broker configuration.\nWhen delegation token is available on an executor Spark considers the following log in options, in order of preference:\nJAAS login configuration\n, please see example below.\nDelegation token\n, please see\nspark.kaf", "question": "What is the default SASL token mechanism for delegation token authentication in Spark?", "answers": {"text": ["SCRAM-SHA-512"], "answer_start": [485]}}
{"context": "nnection\n      to the Kafka cluster. For further details please see Kafka documentation. Only used to obtain delegation token.\n3.0.0\nspark.kafka.clusters.${cluster}.target.bootstrap.servers.regex\n.*\nRegular expression to match against the\nbootstrap.servers\nconfig for sources and sinks in the application.\n      If a server address matches this regex, the delegation token obtained from the respective bootstrap servers will be used when connecting.\n      If multiple clusters match the address, an exception will be thrown and the query won't be started.\n      Kafka's secure and unsecure listeners are bound to different ports. When both used the secure listener port has to be part of the regular expression.\n3.0.0\nspark.kafka.clusters.${cluster}.security.protocol\nSASL_SSL\nProtocol used to commun", "question": "What happens if multiple clusters match the bootstrap server address?", "answers": {"text": ["If multiple clusters match the address, an exception will be thrown and the query won't be started."], "answer_start": [456]}}
{"context": " optional and only needed if\nspark.kafka.clusters.${cluster}.ssl.truststore.location\nis configured.\n      For further details please see Kafka documentation. Only used to obtain delegation token.\n3.0.0\nspark.kafka.clusters.${cluster}.ssl.keystore.type\nNone\nThe file format of the key store file. This is optional for client.\n      For further details please see Kafka documentation. Only used to obtain delegation token.\n3.2.0\nspark.kafka.clusters.${cluster}.ssl.keystore.location\nNone\nThe location of the key store file. This is optional for client and can be used for two-way authentication for client.\n      For further details please see Kafka documentation. Only used to obtain delegation token.\n3.0.0\nspark.kafka.clusters.${cluster}.ssl.keystore.password\nNone\nThe store password for the key sto", "question": "What is the purpose of spark.kafka.clusters.${cluster}.ssl.keystore.location?", "answers": {"text": ["The location of the key store file. This is optional for client and can be used for two-way authentication for client."], "answer_start": [486]}}
{"context": "cumentation. Only used to obtain delegation token.\n3.0.0\nspark.kafka.clusters.${cluster}.ssl.keystore.password\nNone\nThe store password for the key store file. This is optional and only needed if\nspark.kafka.clusters.${cluster}.ssl.keystore.location\nis configured.\n      For further details please see Kafka documentation. Only used to obtain delegation token.\n3.0.0\nspark.kafka.clusters.${cluster}.ssl.key.password\nNone\nThe password of the private key in the key store file. This is optional for client.\n      For further details please see Kafka documentation. Only used to obtain delegation token.\n3.0.0\nspark.kafka.clusters.${cluster}.sasl.token.mechanism\nSCRAM-SHA-512\nSASL mechanism used for client connections with delegation token. Because SCRAM login module used for authentication a compatib", "question": "What is the purpose of spark.kafka.clusters.${cluster}.sasl.token.mechanism?", "answers": {"text": ["SASL mechanism used for client connections with delegation token."], "answer_start": [673]}}
{"context": "echanism\nSCRAM-SHA-512\nSASL mechanism used for client connections with delegation token. Because SCRAM login module used for authentication a compatible mechanism has to be set here.\n      For further details please see Kafka documentation (\nsasl.mechanism\n). Only used to authenticate against Kafka broker with delegation token.\n3.0.0\nKafka Specific Configurations\nKafka’s own configurations can be set with\nkafka.\nprefix, e.g,\n--conf spark.kafka.clusters.${cluster}.kafka.retries=1\n.\nFor possible Kafka parameters, see\nKafka adminclient config docs\n.\nCaveats\nObtaining delegation token for proxy user is not yet supported (\nKAFKA-6945\n).\nJAAS login configuration\nJAAS login configuration must placed on all nodes where Spark tries to access Kafka cluster.\nThis provides the possibility to apply any", "question": "What is SCRAM-SHA-512 used for?", "answers": {"text": ["SASL mechanism used for client connections with delegation token."], "answer_start": [23]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)"], "answer_start": [46]}}
{"context": "ures\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nMachine Learning Library (MLlib) Guide\nMLlib is Spark’s machine learning (ML) library.\nIts goal is to make practical machine learning scalable and easy.\nAt a high level, it provides tools such as:\nML Algorithms: common learning algorithms such as classification, regression, clustering, and collaborative filtering\nFeaturization: feature extraction, transformation, dimensionality reduction, an", "question": "What is the main goal of MLlib?", "answers": {"text": ["Its goal is to make practical machine learning scalable and easy."], "answer_start": [493]}}
{"context": "as classification, regression, clustering, and collaborative filtering\nFeaturization: feature extraction, transformation, dimensionality reduction, and selection\nPipelines: tools for constructing, evaluating, and tuning ML Pipelines\nPersistence: saving and load algorithms, models, and Pipelines\nUtilities: linear algebra, statistics, data handling, etc.\nAnnouncement: DataFrame-based API is primary API\nThe MLlib RDD-based API is now in maintenance mode.\nAs of Spark 2.0, the\nRDD\n-based APIs in the\nspark.mllib\npackage have entered maintenance mode.\nThe primary Machine Learning API for Spark is now the\nDataFrame\n-based API in the\nspark.ml\npackage.\nWhat are the implications?\nMLlib will still support the RDD-based API in\nspark.mllib\nwith bug fixes.\nMLlib will not add new features to the RDD-based", "question": "What is the current status of the MLlib RDD-based API?", "answers": {"text": ["The MLlib RDD-based API is now in maintenance mode."], "answer_start": [404]}}
{"context": "\nWhat are the implications?\nMLlib will still support the RDD-based API in\nspark.mllib\nwith bug fixes.\nMLlib will not add new features to the RDD-based API.\nIn the Spark 2.x releases, MLlib will add features to the DataFrames-based API to reach feature parity with the RDD-based API.\nWhy is MLlib switching to the DataFrame-based API?\nDataFrames provide a more user-friendly API than RDDs.  The many benefits of DataFrames include Spark Datasources, SQL/DataFrame queries, Tungsten and Catalyst optimizations, and uniform APIs across languages.\nThe DataFrame-based API for MLlib provides a uniform API across ML algorithms and across multiple languages.\nDataFrames facilitate practical ML Pipelines, particularly feature transformations.  See the\nPipelines guide\nfor details.\nWhat is “Spark ML”?\n“Spar", "question": "Why is MLlib switching to the DataFrame-based API?", "answers": {"text": ["DataFrames provide a more user-friendly API than RDDs."], "answer_start": [334]}}
{"context": "s.\nDataFrames facilitate practical ML Pipelines, particularly feature transformations.  See the\nPipelines guide\nfor details.\nWhat is “Spark ML”?\n“Spark ML” is not an official name but occasionally used to refer to the MLlib DataFrame-based API.\nThis is majorly due to the\norg.apache.spark.ml\nScala package name used by the DataFrame-based API, \nand the “Spark ML Pipelines” term we used initially to emphasize the pipeline concept.\nIs MLlib deprecated?\nNo. MLlib includes both the RDD-based API and the DataFrame-based API.\nThe RDD-based API is now in maintenance mode.\nBut neither API is deprecated, nor MLlib as a whole.\nDependencies\nMLlib uses linear algebra packages\nBreeze\nand\ndev.ludovic.netlib\nfor optimised numerical processing\n1\n. Those packages may call native acceleration libraries such a", "question": "What is “Spark ML”?", "answers": {"text": ["“Spark ML” is not an official name but occasionally used to refer to the MLlib DataFrame-based API."], "answer_start": [145]}}
{"context": "in Python, you will need\nNumPy\nversion 1.4 or newer.\nHighlights in 3.0\nThe list below highlights some of the new features and enhancements added to MLlib in the\n3.0\nrelease of Spark:\nMultiple columns support was added to\nBinarizer\n(\nSPARK-23578\n),\nStringIndexer\n(\nSPARK-11215\n),\nStopWordsRemover\n(\nSPARK-29808\n) and PySpark\nQuantileDiscretizer\n(\nSPARK-22796\n).\nTree-Based Feature Transformation was added\n(\nSPARK-13677\n).\nTwo new evaluators\nMultilabelClassificationEvaluator\n(\nSPARK-16692\n) and\nRankingEvaluator\n(\nSPARK-28045\n) were added.\nSample weights support was added in\nDecisionTreeClassifier/Regressor\n(\nSPARK-19591\n),\nRandomForestClassifier/Regressor\n(\nSPARK-9478\n),\nGBTClassifier/Regressor\n(\nSPARK-9612\n),\nMulticlassClassificationEvaluator\n(\nSPARK-24101\n),\nRegressionEvaluator\n(\nSPARK-24102\n", "question": "Which versions of Spark were enhancements added to Binarizer, StringIndexer, StopWordsRemover, and PySpark QuantileDiscretizer?", "answers": {"text": ["3.0"], "answer_start": [67]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nANSI Compliance\nData Types\nDatetime Pattern\nNumber Pattern\nOperators\nFunctions\nIdenti", "question": "What are some of the programming guides available for Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)"], "answer_start": [46]}}
{"context": "Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nANSI Compliance\nData Types\nDatetime Pattern\nNumber Pattern\nOperators\nFunctions\nIdentifiers\nIDENTIFIER clause\nLiterals\nNull Semantics\nSQL Syntax\nError Conditions\nSQL Reference\nSpark SQL is Apache Spark’s module for working with structured data. This guide is a reference for Structured Query Language (SQL) and includes syntax, semantics, keywords, and examples for common SQL usage. It contains information for the following topics:\nANSI Compliance\nData Types\nDatetime Pattern\nNumber Pattern\nOperators\nFunctions\nBuilt-in Functions\nScalar User-Defined Functions (UDFs)\nUser-Defined Aggregate Functions (UDAFs)\nIntegration with Hive UDFs/UDAFs/UDTFs\nFunction Invocation\nIdentifiers\nIDENTIFIER clause\nLiterals\nNull Semantics\nSQL Syntax\nDD", "question": "What does Spark SQL provide a reference for?", "answers": {"text": ["Structured Query Language (SQL)"], "answer_start": [339]}}
{"context": "egate Functions (UDAFs)\nIntegration with Hive UDFs/UDAFs/UDTFs\nFunction Invocation\nIdentifiers\nIDENTIFIER clause\nLiterals\nNull Semantics\nSQL Syntax\nDDL Statements\nDML Statements\nData Retrieval Statements\nAuxiliary Statements\nPipe Syntax", "question": "Quais tipos de funções Hive são integradas?", "answers": {"text": ["UDFs/UDAFs/UDTFs"], "answer_start": [46]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMigration Guide: SparkR (R on Spark)\nUpgrading from SparkR 3.5 to 4.0\nUpgrading from SparkR 3.1 to 3.2\nUpgrading from SparkR 2.4 to 3.0\nUpgrading from SparkR 2.3 to 2.4\nUpgrading from SparkR 2.3 to 2.3.1 and above\nUpgrading from SparkR 2.2 to 2.3\nUpgr", "question": "What are some of the programming guides available for Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars"], "answer_start": [46]}}
{"context": "2\nUpgrading from SparkR 2.4 to 3.0\nUpgrading from SparkR 2.3 to 2.4\nUpgrading from SparkR 2.3 to 2.3.1 and above\nUpgrading from SparkR 2.2 to 2.3\nUpgrading from SparkR 2.1 to 2.2\nUpgrading from SparkR 2.0 to 3.1\nUpgrading from SparkR 1.6 to 2.0\nUpgrading from SparkR 1.5 to 1.6\nNote that this migration guide describes the items specific to SparkR.\nMany items of SQL migration can be applied when migrating SparkR to higher versions.\nPlease refer\nMigration Guide: SQL, Datasets and DataFrame\n.\nUpgrading from SparkR 3.5 to 4.0\nIn Spark 4.0, SparkR is deprecated and will be removed in a future version.\nUpgrading from SparkR 3.1 to 3.2\nPreviously, SparkR automatically downloaded and installed the Spark distribution in user’s cache directory to complete SparkR installation when SparkR runs in a pla", "question": "What happens when upgrading from SparkR 3.5 to 4.0?", "answers": {"text": ["In Spark 4.0, SparkR is deprecated and will be removed in a future version."], "answer_start": [527]}}
{"context": "arkR automatically downloaded and installed the Spark distribution in user’s cache directory to complete SparkR installation when SparkR runs in a plain R shell or Rscript, and the Spark distribution cannot be found. Now, it asks if users want to download and install or not. To restore the previous behavior, set\nSPARKR_ASK_INSTALLATION\nenvironment variable to\nFALSE\n.\nUpgrading from SparkR 2.4 to 3.0\nThe deprecated methods\nparquetFile\n,\nsaveAsParquetFile\n,\njsonFile\n,\njsonRDD\nhave been removed. Use\nread.parquet\n,\nwrite.parquet\n,\nread.json\ninstead.\nUpgrading from SparkR 2.3 to 2.4\nPreviously, we don’t check the validity of the size of the last layer in\nspark.mlp\n. For example, if the training data only has two labels, a\nlayers\nparam like\nc(1, 3)\ndoesn’t cause an error previously, now it does.", "question": "What should be done to restore the previous behavior of SparkR automatically downloading and installing the Spark distribution?", "answers": {"text": ["set\nSPARKR_ASK_INSTALLATION\nenvironment variable to\nFALSE\n."], "answer_start": [310]}}
{"context": "orms Cartesian Product by default, use\ncrossJoin\ninstead.\nUpgrading from SparkR 1.6 to 2.0\nThe method\ntable\nhas been removed and replaced by\ntableToDF\n.\nThe class\nDataFrame\nhas been renamed to\nSparkDataFrame\nto avoid name conflicts.\nSpark’s\nSQLContext\nand\nHiveContext\nhave been deprecated to be replaced by\nSparkSession\n. Instead of\nsparkR.init()\n, call\nsparkR.session()\nin its place to instantiate the SparkSession. Once that is done, that currently active SparkSession will be used for SparkDataFrame operations.\nThe parameter\nsparkExecutorEnv\nis not supported by\nsparkR.session\n. To set environment for the executors, set Spark config properties with the prefix “spark.executorEnv.VAR_NAME”, for example, “spark.executorEnv.PATH”\nThe\nsqlContext\nparameter is no longer required for these functions:", "question": "What should be used instead of sparkR.init() to instantiate the SparkSession?", "answers": {"text": ["sparkR.session()"], "answer_start": [354]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTrou", "question": "Which file formats are mentioned as data sources in the Spark SQL Guide?", "answers": {"text": ["Parquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files"], "answer_start": [650]}}
{"context": "Parquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTroubleshooting\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nData Sources\nSpark SQL supports operating on a variety of data sources through the DataFrame interface.\nA DataFrame can be operated on using relational transformations and can also be used to create a temporary view.\nRegistering a DataFrame as a temporary view allows you to run SQL queries over its data. This section\ndescribes the general methods for loading and saving data using the Spark Data Sources and then\ngoes into specific options that are available for the built-in data sources.\nGeneric ", "question": "What does Spark SQL support operating on?", "answers": {"text": ["a variety of data sources through the DataFrame interface."], "answer_start": [345]}}
{"context": "or loading and saving data using the Spark Data Sources and then\ngoes into specific options that are available for the built-in data sources.\nGeneric Load/Save Functions\nManually Specifying Options\nRun SQL on files directly\nSave Modes\nSaving to Persistent Tables\nBucketing, Sorting and Partitioning\nGeneric File Source Options\nIgnore Corrupt Files\nIgnore Missing Files\nPath Glob Filter\nRecursive File Lookup\nParquet Files\nLoading Data Programmatically\nPartition Discovery\nSchema Merging\nHive metastore Parquet table conversion\nConfiguration\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nSpecifying storage format for Hive tables\nInteracting with Different Versions of Hive Metastore\nJDBC To Other Databases\nAvro Files\nDeploying\nLoad and Save Functions\nto_avro() and from_avro()\nData", "question": "What are some of the generic file source options available?", "answers": {"text": ["Ignore Corrupt Files\nIgnore Missing Files\nPath Glob Filter\nRecursive File Lookup"], "answer_start": [327]}}
{"context": "eracting with Different Versions of Hive Metastore\nJDBC To Other Databases\nAvro Files\nDeploying\nLoad and Save Functions\nto_avro() and from_avro()\nData Source Option\nConfiguration\nCompatibility with Databricks spark-avro\nSupported types for Avro -> Spark SQL conversion\nSupported types for Spark SQL -> Avro conversion\nProtobuf data\nDeploying\nto_protobuf() and from_protobuf()\nSupported types for Protobuf -> Spark SQL conversion\nSupported types for Spark SQL -> Protobuf conversion\nHandling circular references protobuf fields\nWhole Binary Files\nTroubleshooting", "question": "What functions are available for working with Avro files?", "answers": {"text": ["to_avro() and from_avro()"], "answer_start": [120]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nStructured Streaming Programming Guide\nOverview\nGetting Started\nAPIs on DataFrames and Datasets\nPerformance Tips\nAdditional Information\nMiscellaneous Notes\nRelated Resources\nMigration Guide\nStructured Streaming Programming Guide\nMiscellaneous Notes\nSe", "question": "What are some of the programming guides available for Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)"], "answer_start": [46]}}
{"context": " of (reduced) tasks will be kept unless another shuffle happens.\nspark.sql.streaming.stateStore.providerClass\n: To read the previous state of the query properly, the class of state store provider should be unchanged.\nspark.sql.streaming.multipleWatermarkPolicy\n: Modification of this would lead inconsistent watermark value when query contains multiple watermarks, hence the policy should be unchanged.\nRelated Resources\nFurther Reading\nSee and run the\nPython\n/\nScala\n/\nJava\n/\nR\nexamples.\nInstructions\non how to run Spark examples\nRead about integrating with Kafka in the\nStructured Streaming Kafka Integration Guide\nRead more details about using DataFrames/Datasets in the\nSpark SQL Programming Guide\nThird-party Blog Posts\nReal-time Streaming ETL with Structured Streaming in Apache Spark 2.1 (Data", "question": "What should be unchanged to properly read the previous state of the query?", "answers": {"text": ["the class of state store provider should be unchanged."], "answer_start": [162]}}
{"context": "aFrames/Datasets in the\nSpark SQL Programming Guide\nThird-party Blog Posts\nReal-time Streaming ETL with Structured Streaming in Apache Spark 2.1 (Databricks Blog)\nReal-Time End-to-End Integration with Apache Kafka in Apache Spark’s Structured Streaming (Databricks Blog)\nEvent-time Aggregation and Watermarking in Apache Spark’s Structured Streaming (Databricks Blog)\nTalks\nSpark Summit Europe 2017\nEasy, Scalable, Fault-tolerant Stream Processing with Structured Streaming in Apache Spark -\nPart 1 slides/video\n,\nPart 2 slides/video\nDeep Dive into Stateful Stream Processing in Structured Streaming -\nslides/video\nSpark Summit 2016\nA Deep Dive into Structured Streaming -\nslides/video\nMigration Guide\nThe migration guide is now archived\non this page\n.", "question": "Where is the migration guide archived?", "answers": {"text": ["on this page"], "answer_start": [738]}}
{"context": "Structured Streaming -\nslides/video\nMigration Guide\nThe migration guide is now archived\non this page\n.", "question": "Onde o guia de migração está agora arquivado?", "answers": {"text": ["on this page"], "answer_start": [88]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nPySpark Usage Guide for Pandas with Apache Arrow\nThe Arrow usage gui", "question": "What topics are covered under the 'Deploying' section?", "answers": {"text": ["Submitting Applications\nSpark Standalone\nYARN\nKubernetes"], "answer_start": [329]}}
{"context": "Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nPySpark Usage Guide for Pandas with Apache Arrow\nThe Arrow usage guide is now archived on\nthis page\n.", "question": "Where is the Arrow usage guide now located?", "answers": {"text": ["this page"], "answer_start": [172]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)"], "answer_start": [46]}}
{"context": "ta types\nBasic statistics\nsummary statistics\ncorrelations\nstratified sampling\nhypothesis testing\nstreaming significance testing\nrandom data generation\nClassification and regression\nlinear models (SVMs, logistic regression, linear regression)\nnaive Bayes\ndecision trees\nensembles of trees (Random Forests and Gradient-Boosted Trees)\nisotonic regression\nCollaborative filtering\nalternating least squares (ALS)\nClustering\nk-means\nGaussian mixture\npower iteration clustering (PIC)\nlatent Dirichlet allocation (LDA)\nbisecting k-means\nstreaming k-means\nDimensionality reduction\nsingular value decomposition (SVD)\nprincipal component analysis (PCA)\nFeature extraction and transformation\nFrequent pattern mining\nFP-growth\nassociation rules\nPrefixSpan\nEvaluation metrics\nPMML model export\nOptimization (develo", "question": "Quais modelos são incluídos na seção de modelos lineares?", "answers": {"text": ["linear models (SVMs, logistic regression, linear regression)"], "answer_start": [181]}}
{"context": "extraction and transformation\nFrequent pattern mining\nFP-growth\nassociation rules\nPrefixSpan\nEvaluation metrics\nPMML model export\nOptimization (developer)\nstochastic gradient descent\nlimited-memory BFGS (L-BFGS)", "question": "What optimization technique is mentioned in the text?", "answers": {"text": ["stochastic gradient descent"], "answer_start": [155]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)"], "answer_start": [46]}}
{"context": "rs\n,\nnumIterations\n)\n// Export to PMML to a String in PMML format\nprintln\n(\ns\n\"PMML Model:\\n ${clusters.toPMML()}\"\n)\n// Export the model to a local file in PMML format\nclusters\n.\ntoPMML\n(\n\"/tmp/kmeans.xml\"\n)\n// Export the model to a directory on a distributed file system in PMML format\nclusters\n.\ntoPMML\n(\nsc\n,\n\"/tmp/kmeans\"\n)\n// Export the model to the OutputStream in PMML format\nclusters\n.\ntoPMML\n(\nSystem\n.\nout\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\" in the Spark repo.\nFor unsupported models, either you will not find a\n.toPMML\nmethod or an\nIllegalArgumentException\nwill be thrown.", "question": "Where can I find a full example code for PMML model export?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\" in the Spark repo."], "answer_start": [418]}}
{"context": "l be thrown.", "question": "What will be thrown?", "answers": {"text": ["l be thrown."], "answer_start": [0]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nPerformance Tuning\nDistributed SQL Engine\nRunning the Thrift JDBC/ODBC server\nRunning the Spark SQL CLI\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nDistri", "question": "What are some of the programming guides available for Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)"], "answer_start": [46]}}
{"context": "rift JDBC/ODBC server\nRunning the Spark SQL CLI\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nDistributed SQL Engine\nRunning the Thrift JDBC/ODBC server\nRunning the Spark SQL CLI\nSpark SQL can also act as a distributed query engine using its JDBC/ODBC or command-line interface.\nIn this mode, end-users or applications can interact with Spark SQL directly to run SQL queries,\nwithout the need to write any code.\nRunning the Thrift JDBC/ODBC server\nThe Thrift JDBC/ODBC server implemented here corresponds to the\nHiveServer2\nin built-in Hive. You can test the JDBC server with the beeline script that comes with either Spark or compatible Hive.\nTo start the JDBC/ODBC server, run the following in the Spark directory:\n./sbin/start-thriftserver.sh\nThis", "question": "What can Spark SQL act as?", "answers": {"text": ["Spark SQL can also act as a distributed query engine using its JDBC/ODBC or command-line interface."], "answer_start": [229]}}
{"context": "comes with either Spark or compatible Hive.\nTo start the JDBC/ODBC server, run the following in the Spark directory:\n./sbin/start-thriftserver.sh\nThis script accepts all\nbin/spark-submit\ncommand line options, plus a\n--hiveconf\noption to\nspecify Hive properties. You may run\n./sbin/start-thriftserver.sh --help\nfor a complete list of\nall available options. By default, the server listens on localhost:10000. You may override this\nbehaviour via either environment variables, i.e.:\nexport\nHIVE_SERVER2_THRIFT_PORT\n=\n<listening-port>\nexport\nHIVE_SERVER2_THRIFT_BIND_HOST\n=\n<listening-host>\n./sbin/start-thriftserver.sh\n\\\n--master\n<master-uri>\n\\\n...\nor system properties:\n./sbin/start-thriftserver.sh\n\\\n--hiveconf\nhive.server2.thrift.port\n=\n<listening-port>\n\\\n--hiveconf\nhive.server2.thrift.bind.host\n=\n<l", "question": "How can you start the JDBC/ODBC server?", "answers": {"text": ["./sbin/start-thriftserver.sh"], "answer_start": [117]}}
{"context": "stem properties:\n./sbin/start-thriftserver.sh\n\\\n--hiveconf\nhive.server2.thrift.port\n=\n<listening-port>\n\\\n--hiveconf\nhive.server2.thrift.bind.host\n=\n<listening-host>\n\\\n--master\n<master-uri>\n  ...\nNow you can use beeline to test the Thrift JDBC/ODBC server:\n./bin/beeline\nConnect to the JDBC/ODBC server in beeline with:\nbeeline> !connect jdbc:hive2://localhost:10000\nBeeline will ask you for a username and password. In non-secure mode, simply enter the username on\nyour machine and a blank password. For secure mode, please follow the instructions given in the\nbeeline documentation\n.\nConfiguration of Hive is done by placing your\nhive-site.xml\n,\ncore-site.xml\nand\nhdfs-site.xml\nfiles in\nconf/\n.\nYou may also use the beeline script that comes with Hive.\nThrift JDBC server also supports sending thrif", "question": "How can you test the Thrift JDBC/ODBC server?", "answers": {"text": ["./bin/beeline"], "answer_start": [256]}}
{"context": "p_endpoint>\nIf you closed a session and do CTAS, you must set\nfs.%s.impl.disable.cache\nto true in\nhive-site.xml\n.\nSee more details in\n[SPARK-21067]\n.\nRunning the Spark SQL CLI\nTo use the Spark SQL command line interface (CLI) from the shell:\n./bin/spark-sql\nFor details, please refer to\nSpark SQL CLI", "question": "Where can you find more details about the issue mentioned regarding CTAS after closing a session?", "answers": {"text": ["[SPARK-21067]"], "answer_start": [134]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)"], "answer_start": [46]}}
{"context": "ports various methods for\nbinary classification\n,\nmulticlass\nclassification\n, and\nregression analysis\n. The table below outlines\nthe supported algorithms for each type of problem.\nProblem Type\nSupported Methods\nBinary Classification\nlinear SVMs, logistic regression, decision trees, random forests, gradient-boosted trees, naive Bayes\nMulticlass Classification\nlogistic regression, decision trees, random forests, naive Bayes\nRegression\nlinear least squares, Lasso, ridge regression, decision trees, random forests, gradient-boosted trees, isotonic regression\nMore details for these methods can be found here:\nLinear models\nclassification (SVMs, logistic regression)\nlinear regression (least squares, Lasso, ridge)\nDecision trees\nEnsembles of decision trees\nrandom forests\ngradient-boosted trees\nNaiv", "question": "What methods are supported for binary classification?", "answers": {"text": ["linear SVMs, logistic regression, decision trees, random forests, gradient-boosted trees, naive Bayes"], "answer_start": [233]}}
{"context": "stic regression)\nlinear regression (least squares, Lasso, ridge)\nDecision trees\nEnsembles of decision trees\nrandom forests\ngradient-boosted trees\nNaive Bayes\nIsotonic regression", "question": "Which regression techniques are listed in the text?", "answers": {"text": ["linear regression (least squares, Lasso, ridge)"], "answer_start": [17]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars"], "answer_start": [46]}}
{"context": "ures\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nalternating least squares (ALS)\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nCollaborative Filtering - RDD-based API\nCollaborative filtering\nExplicit vs. implicit feedback\nScaling of the regularization parameter\nExamples\nTutorial\nCollaborative filtering\nCollaborative filtering\nis commonly used for recommender systems.  These techniques aim to fill in the\nmissing entries of a user-item association matrix.\nspark.mllib\ncurrently supports\n", "question": "Para que collaborative filtering é comumente usado?", "answers": {"text": ["is commonly used for recommender systems."], "answer_start": [639]}}
{"context": " the\nbest parameter learned from a sampled subset to the full dataset and expect similar performance.\nExamples\nIn the following example we load rating data. Each row consists of a user, a product and a rating.\nWe use the default ALS.train() method which assumes ratings are explicit. We evaluate the\nrecommendation by measuring the Mean Squared Error of rating prediction.\nRefer to the\nALS\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.recommendation\nimport\nALS\n,\nMatrixFactorizationModel\n,\nRating\n# Load and parse the data\ndata\n=\nsc\n.\ntextFile\n(\n\"\ndata/mllib/als/test.data\n\"\n)\nratings\n=\ndata\n.\nmap\n(\nlambda\nl\n:\nl\n.\nsplit\n(\n'\n,\n'\n))\n\\\n.\nmap\n(\nlambda\nl\n:\nRating\n(\nint\n(\nl\n[\n0\n]),\nint\n(\nl\n[\n1\n]),\nfloat\n(\nl\n[\n2\n])))\n# Build the recommendation model using Alternating Least Squares\nrank\n=\n", "question": "What is used to build the recommendation model?", "answers": {"text": ["Alternating Least Squares"], "answer_start": [767]}}
{"context": "\n# Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"\ntarget/tmp/myCollaborativeFilter\n\"\n)\nsameModel\n=\nMatrixFactorizationModel\n.\nload\n(\nsc\n,\n\"\ntarget/tmp/myCollaborativeFilter\n\"\n)\nFind full example code at \"examples/src/main/python/mllib/recommendation_example.py\" in the Spark repo.\nIf the rating matrix is derived from other source of information (i.e. it is inferred from other\nsignals), you can use the trainImplicit method to get better results.\n# Build the recommendation model using Alternating Least Squares based on implicit ratings\nmodel\n=\nALS\n.\ntrainImplicit\n(\nratings\n,\nrank\n,\nnumIterations\n,\nalpha\n=\n0.01\n)\nIn the following example, we load rating data. Each row consists of a user, a product and a rating.\nWe use the default\nALS.train()\nmethod which assumes ratings are explicit. We evaluate th", "question": "Where can I find the full example code for the recommendation example?", "answers": {"text": ["Find full example code at \"examples/src/main/python/mllib/recommendation_example.py\" in the Spark repo."], "answer_start": [172]}}
{"context": "\n))\n=>\nval\nerr\n=\n(\nr1\n-\nr2\n)\nerr\n*\nerr\n}.\nmean\n()\nprintln\n(\ns\n\"Mean Squared Error = $MSE\"\n)\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/myCollaborativeFilter\"\n)\nval\nsameModel\n=\nMatrixFactorizationModel\n.\nload\n(\nsc\n,\n\"target/tmp/myCollaborativeFilter\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\" in the Spark repo.\nIf the rating matrix is derived from another source of information (i.e. it is inferred from\nother signals), you can use the\ntrainImplicit\nmethod to get better results.\nval\nalpha\n=\n0.01\nval\nlambda\n=\n0.01\nval\nmodel\n=\nALS\n.\ntrainImplicit\n(\nratings\n,\nrank\n,\nnumIterations\n,\nlambda\n,\nalpha\n)\nAll of MLlib’s methods use Java-friendly types, so you can import and call them there the same\nway you do in Scala. T", "question": "Where can I find a full example code for this?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\" in the Spark repo."], "answer_start": [264]}}
{"context": "\n(\nratings\n.\nmap\n(\nr\n->\nnew\nTuple2\n<>(\nnew\nTuple2\n<>(\nr\n.\nuser\n(),\nr\n.\nproduct\n()),\nr\n.\nrating\n())))\n.\njoin\n(\npredictions\n).\nvalues\n();\ndouble\nMSE\n=\nratesAndPreds\n.\nmapToDouble\n(\npair\n->\n{\ndouble\nerr\n=\npair\n.\n_1\n()\n-\npair\n.\n_2\n();\nreturn\nerr\n*\nerr\n;\n}).\nmean\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Mean Squared Error = \"\n+\nMSE\n);\n// Save and load model\nmodel\n.\nsave\n(\njsc\n.\nsc\n(),\n\"target/tmp/myCollaborativeFilter\"\n);\nMatrixFactorizationModel\nsameModel\n=\nMatrixFactorizationModel\n.\nload\n(\njsc\n.\nsc\n(),\n\"target/tmp/myCollaborativeFilter\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\" in the Spark repo.\nIn order to run the above application, follow the instructions\nprovided in the\nSelf-Contained Applications\nsection of the Spark\nQuick S", "question": "Where can I find the full example code for the application?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\" in the Spark repo."], "answer_start": [532]}}
{"context": "he Spark repo.\nIn order to run the above application, follow the instructions\nprovided in the\nSelf-Contained Applications\nsection of the Spark\nQuick Start guide. Be sure to also include\nspark-mllib\nto your build file as\na dependency.\nTutorial\nThe\ntraining exercises\nfrom the Spark Summit 2014 include a hands-on tutorial for\npersonalized movie recommendation with\nspark.mllib\n.", "question": "What dependency should be included in your build file to run the application?", "answers": {"text": ["spark-mllib"], "answer_start": [186]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)"], "answer_start": [46]}}
{"context": "earch topic in\ndata mining for years.\nWe refer users to Wikipedia’s\nassociation rule learning\nfor more information.\nspark.mllib\nprovides a parallel implementation of FP-growth,\na popular algorithm to mining frequent itemsets.\nFP-growth\nThe FP-growth algorithm is described in the paper\nHan et al., Mining frequent patterns without candidate generation\n,\nwhere “FP” stands for frequent pattern.\nGiven a dataset of transactions, the first step of FP-growth is to calculate item frequencies and identify frequent items.\nDifferent from\nApriori-like\nalgorithms designed for the same purpose,\nthe second step of FP-growth uses a suffix tree (FP-tree) structure to encode transactions without generating candidate sets\nexplicitly, which are usually expensive to generate.\nAfter the second step, the frequent", "question": "What does \"FP\" stand for in the FP-growth algorithm?", "answers": {"text": ["“FP” stands for frequent pattern."], "answer_start": [360]}}
{"context": "rateAssociationRules\n(\nminConfidence\n).\ncollect\n().\nforeach\n{\nrule\n=>\nprintln\n(\ns\n\"${rule.antecedent.mkString(\"\n[\n\"\n,\n\"\n,\n\"\n,\n\"\n]\n\")}=> \"\n+\ns\n\"${rule.consequent .mkString(\"\n[\n\"\n,\n\"\n,\n\"\n,\n\"\n]\n\")},${rule.confidence}\"\n)\n}\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\" in the Spark repo.\nFPGrowth\nimplements the\nFP-growth algorithm.\nIt takes a\nJavaRDD\nof transactions, where each transaction is an\nIterable\nof items of a generic type.\nCalling\nFPGrowth.run\nwith transactions returns an\nFPGrowthModel\nthat stores the frequent itemsets with their frequencies.  The following\nexample illustrates how to mine frequent itemsets and association rules\n(see\nAssociation\nRules\nfor\ndetails) from\ntransactions\n.\nRefer to the\nFPGrowth\nJava docs\nfor details o", "question": "Where can I find a full example code for FPGrowth?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\" in the Spark repo."], "answer_start": [219]}}
{"context": " to mine frequent itemsets and association rules\n(see\nAssociation\nRules\nfor\ndetails) from\ntransactions\n.\nRefer to the\nFPGrowth\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.api.java.JavaSparkContext\n;\nimport\norg.apache.spark.mllib.fpm.AssociationRules\n;\nimport\norg.apache.spark.mllib.fpm.FPGrowth\n;\nimport\norg.apache.spark.mllib.fpm.FPGrowthModel\n;\nJavaRDD\n<\nString\n>\ndata\n=\nsc\n.\ntextFile\n(\n\"data/mllib/sample_fpgrowth.txt\"\n);\nJavaRDD\n<\nList\n<\nString\n>>\ntransactions\n=\ndata\n.\nmap\n(\nline\n->\nArrays\n.\nasList\n(\nline\n.\nsplit\n(\n\" \"\n)));\nFPGrowth\nfpg\n=\nnew\nFPGrowth\n()\n.\nsetMinSupport\n(\n0.2\n)\n.\nsetNumPartitions\n(\n10\n);\nFPGrowthModel\n<\nString\n>\nmodel\n=\nfpg\n.\nrun\n(\ntransactions\n);\nfor\n(\nFPGrowth\n.\nFre", "question": "Which class is used to mine frequent itemsets and association rules?", "answers": {"text": ["AssociationRules"], "answer_start": [340]}}
{"context": "=\nnew\nFPGrowth\n()\n.\nsetMinSupport\n(\n0.2\n)\n.\nsetNumPartitions\n(\n10\n);\nFPGrowthModel\n<\nString\n>\nmodel\n=\nfpg\n.\nrun\n(\ntransactions\n);\nfor\n(\nFPGrowth\n.\nFreqItemset\n<\nString\n>\nitemset:\nmodel\n.\nfreqItemsets\n().\ntoJavaRDD\n().\ncollect\n())\n{\nSystem\n.\nout\n.\nprintln\n(\n\"[\"\n+\nitemset\n.\njavaItems\n()\n+\n\"], \"\n+\nitemset\n.\nfreq\n());\n}\ndouble\nminConfidence\n=\n0.8\n;\nfor\n(\nAssociationRules\n.\nRule\n<\nString\n>\nrule\n:\nmodel\n.\ngenerateAssociationRules\n(\nminConfidence\n).\ntoJavaRDD\n().\ncollect\n())\n{\nSystem\n.\nout\n.\nprintln\n(\nrule\n.\njavaAntecedent\n()\n+\n\" => \"\n+\nrule\n.\njavaConsequent\n()\n+\n\", \"\n+\nrule\n.\nconfidence\n());\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\" in the Spark repo.\nAssociation Rules\nAssociationRules\nimplements a parallel rule generation algori", "question": "Where can I find a full example code for FPGrowth?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\" in the Spark repo."], "answer_start": [595]}}
{"context": "eqItemsets\n)\nresults\n.\ncollect\n().\nforeach\n{\nrule\n=>\nprintln\n(\ns\n\"[${rule.antecedent.mkString(\"\n,\n\")}=>${rule.consequent.mkString(\"\n,\n\")} ]\"\n+\ns\n\" ${rule.confidence}\"\n)\n}\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\" in the Spark repo.\nAssociationRules\nimplements a parallel rule generation algorithm for constructing rules\nthat have a single item as the consequent.\nRefer to the\nAssociationRules\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.api.java.JavaSparkContext\n;\nimport\norg.apache.spark.mllib.fpm.AssociationRules\n;\nimport\norg.apache.spark.mllib.fpm.FPGrowth\n;\nimport\norg.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n;\nJavaRDD\n<\nFPGrowth\n.\nFreq", "question": "Where can I find a full example code for AssociationRules?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\" in the Spark repo."], "answer_start": [171]}}
{"context": "javaAntecedent\n()\n+\n\" => \"\n+\nrule\n.\njavaConsequent\n()\n+\n\", \"\n+\nrule\n.\nconfidence\n());\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\" in the Spark repo.\nPrefixSpan\nPrefixSpan is a sequential pattern mining algorithm described in\nPei et al., Mining Sequential Patterns by Pattern-Growth: The\nPrefixSpan Approach\n. We refer\nthe reader to the referenced paper for formalizing the sequential\npattern mining problem.\nspark.mllib\n’s PrefixSpan implementation takes the following parameters:\nminSupport\n: the minimum support required to be considered a frequent\nsequential pattern.\nmaxPatternLength\n: the maximum length of a frequent sequential\npattern. Any frequent pattern exceeding this length will not be\nincluded in the results.\nmax", "question": "Where can I find a full example code for JavaAssociationRulesExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\" in the Spark repo."], "answer_start": [88]}}
{"context": ")).mkString(\"\n[\n\"\n,\n\"\n,\n\"\n,\n\"\n]\n\")},\"\n+\ns\n\" ${freqSequence.freq}\"\n)\n}\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\" in the Spark repo.\nPrefixSpan\nimplements the\nPrefixSpan algorithm.\nCalling\nPrefixSpan.run\nreturns a\nPrefixSpanModel\nthat stores the frequent sequences with their frequencies.\nRefer to the\nPrefixSpan\nJava docs\nand\nPrefixSpanModel\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.mllib.fpm.PrefixSpan\n;\nimport\norg.apache.spark.mllib.fpm.PrefixSpanModel\n;\nJavaRDD\n<\nList\n<\nList\n<\nInteger\n>>>\nsequences\n=\nsc\n.\nparallelize\n(\nArrays\n.\nasList\n(\nArrays\n.\nasList\n(\nArrays\n.\nasList\n(\n1\n,\n2\n),\nArrays\n.\nasList\n(\n3\n)),\nArrays\n.\nasList\n(\nArrays\n.\nasList\n(\n1\n),\nArrays\n.\nasList", "question": "Where can I find a full example code for PrefixSpan?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\" in the Spark repo."], "answer_start": [70]}}
{"context": "eq\n());\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\" in the Spark repo.", "question": "Where can I find a full example code for JavaPrefixSpan?", "answers": {"text": ["examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java"], "answer_start": [37]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)"], "answer_start": [46]}}
{"context": "ures\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nstochastic gradient descent\nlimited-memory BFGS (L-BFGS)\nOptimization - RDD-based API\nMathematical description\nGradient descent\nStochastic gradient descent (SGD)\nUpdate schemes for distributed SGD\nLimited-memory BFGS (L-BFGS)\nChoosing an Optimization Method\nImplementation in MLlib\nGradient descent and stochastic gradient descent\nL-BFGS\nDeveloper’s notes\n\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcom", "question": "What optimization methods are discussed in the text?", "answers": {"text": ["stochastic gradient descent\nlimited-memory BFGS (L-BFGS)"], "answer_start": [406]}}
{"context": "zation Method\nImplementation in MLlib\nGradient descent and stochastic gradient descent\nL-BFGS\nDeveloper’s notes\n\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}} \n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\wv}{\\mathbf{w}}\n\\newcommand{\\av}{\\mathbf{\\alpha}}\n\\newcommand{\\bv}{\\mathbf{b}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\id}{\\mathbf{I}} \n\\newcommand{\\ind}{\\mathbf{1}} \n\\newcommand{\\0}{\\mathbf{0}} \n\\newcommand{\\unit}{\\mathbf{e}} \n\\newcommand{\\one}{\\mathbf{1}} \n\\newcommand{\\zero}{\\mathbf{0}}\n\\]\nMathematical description\nGradient descent\nThe simplest method to solve optimization problems of the form\n$\\min_{\\wv \\in\\R^d} \\; f(\\wv)$\nis\ngradient descent\n.\nSuch first-order optimization methods (including gradient descent and stochastic variants\nthereof) are well", "question": "What is the simplest method to solve optimization problems of the form min_{w ∈ R^d} f(w)?", "answers": {"text": ["gradient descent"], "answer_start": [70]}}
{"context": " directions.\nOn the other extreme, if\nminiBatchFraction\nis chosen very small, such that only a single point\nis sampled, i.e.\n$|S|=$ miniBatchFraction $\\cdot n = 1$\n, then the algorithm is equivalent to\nstandard SGD. In that case, the step direction depends from the uniformly random sampling of the\npoint.\nLimited-memory BFGS (L-BFGS)\nL-BFGS\nis an optimization \nalgorithm in the family of quasi-Newton methods to solve the optimization problems of the form\n$\\min_{\\wv \\in\\R^d} \\; f(\\wv)$\n. The L-BFGS method approximates the objective function locally as a \nquadratic without evaluating the second partial derivatives of the objective function to construct the \nHessian matrix. The Hessian matrix is approximated by previous gradient evaluations, so there is no \nvertical scalability issue (the numbe", "question": "What does the algorithm become equivalent to when miniBatchFraction is very small, such that only a single point is sampled?", "answers": {"text": ["standard SGD"], "answer_start": [202]}}
{"context": "struct the \nHessian matrix. The Hessian matrix is approximated by previous gradient evaluations, so there is no \nvertical scalability issue (the number of training features) when computing the Hessian matrix \nexplicitly in Newton’s method. As a result, L-BFGS often achieves more rapid convergence compared with\nother first-order optimization.\nChoosing an Optimization Method\nLinear methods\nuse optimization internally, and some linear methods in\nspark.mllib\nsupport both SGD and L-BFGS.\nDifferent optimization methods can have different convergence guarantees depending on the properties of the objective function, and we cannot cover the literature here.\nIn general, when L-BFGS is available, we recommend using it instead of SGD since L-BFGS tends to converge faster (in fewer iterations).\nImpleme", "question": "When is L-BFGS recommended over SGD?", "answers": {"text": ["we recommend using it instead of SGD since L-BFGS tends to converge faster (in fewer iterations)."], "answer_start": [695]}}
{"context": "\n\"Area under ROC = $auROC\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\" in the Spark repo.\nRefer to the\nLBFGS\nJava docs\nand\nSquaredL2Updater\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.api.java.*\n;\nimport\norg.apache.spark.mllib.classification.LogisticRegressionModel\n;\nimport\norg.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n;\nimport\norg.apache.spark.mllib.linalg.Vector\n;\nimport\norg.apache.spark.mllib.linalg.Vectors\n;\nimport\norg.apache.spark.mllib.optimization.*\n;\nimport\norg.apache.spark.mllib.regression.LabeledPoint\n;\nimport\norg.apache.spark.mllib.util.MLUtils\n;\nimport\norg.apache.spark.SparkConf\n;\nimport\norg.apache.spark.SparkContext\n;\nString\npath\n=\n\"data/ml", "question": "Where can I find a full example code for LBFGS?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\" in the Spark repo."], "answer_start": [29]}}
{"context": "Point\n;\nimport\norg.apache.spark.mllib.util.MLUtils\n;\nimport\norg.apache.spark.SparkConf\n;\nimport\norg.apache.spark.SparkContext\n;\nString\npath\n=\n\"data/mllib/sample_libsvm_data.txt\"\n;\nJavaRDD\n<\nLabeledPoint\n>\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\npath\n).\ntoJavaRDD\n();\nint\nnumFeatures\n=\ndata\n.\ntake\n(\n1\n).\nget\n(\n0\n).\nfeatures\n().\nsize\n();\n// Split initial RDD into two... [60% training data, 40% testing data].\nJavaRDD\n<\nLabeledPoint\n>\ntrainingInit\n=\ndata\n.\nsample\n(\nfalse\n,\n0.6\n,\n11L\n);\nJavaRDD\n<\nLabeledPoint\n>\ntest\n=\ndata\n.\nsubtract\n(\ntrainingInit\n);\n// Append 1 into the training data as intercept.\nJavaPairRDD\n<\nObject\n,\nVector\n>\ntraining\n=\ndata\n.\nmapToPair\n(\np\n->\nnew\nTuple2\n<>(\np\n.\nlabel\n(),\nMLUtils\n.\nappendBias\n(\np\n.\nfeatures\n())));\ntraining\n.\ncache\n();\n// Run training algorithm to build the m", "question": "What file is loaded using MLUtils.loadLibSVMFile?", "answers": {"text": ["data/mllib/sample_libsvm_data.txt"], "answer_start": [143]}}
{"context": "erROC\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Loss of each step in training process\"\n);\nfor\n(\ndouble\nl\n:\nloss\n)\n{\nSystem\n.\nout\n.\nprintln\n(\nl\n);\n}\nSystem\n.\nout\n.\nprintln\n(\n\"Area under ROC = \"\n+\nauROC\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\" in the Spark repo.\nDeveloper’s notes\nSince the Hessian is constructed approximately from previous gradient evaluations, \nthe objective function can not be changed during the optimization process. \nAs a result, Stochastic L-BFGS will not work naively by just using miniBatch; \ntherefore, we don’t provide this until we have better understanding.\nUpdater\nis a class originally designed for gradient decent which computes \nthe actual gradient descent step. However, we’re able to take the gradient and \nlos", "question": "Where can I find a full example code for JavaLBFGSExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\" in the Spark repo."], "answer_start": [192]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)"], "answer_start": [46]}}
{"context": "ures\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nMLlib Linear Algebra Acceleration Guide\nIntroduction\nThis guide provides necessary information to enable accelerated linear algebra processing for Spark MLlib.\nSpark MLlib defines Vector and Matrix as basic data types for machine learning algorithms. On top of them,\nBLAS\nand\nLAPACK\noperations are implemented and supported by\ndev.ludovic.netlib\n(the algorithms may also call\nBreeze\n).\ndev.ludo", "question": "What basic data types does Spark MLlib define for machine learning algorithms?", "answers": {"text": ["Spark MLlib defines Vector and Matrix as basic data types for machine learning algorithms."], "answer_start": [566]}}
{"context": "two popular native linear algebra libraries. You can choose one of them based on your preference. We provide basic instructions as below.\nIntel MKL\nDownload and install Intel MKL. The installation should be done on all nodes of the cluster. We assume the installation location is $MKLROOT (e.g. /opt/intel/mkl).\nCreate soft links to\nlibmkl_rt.so\nwith specific names in system library search paths. For instance, make sure\n/usr/local/lib\nis in system library search paths and run the following commands:\n$ ln -sf $MKLROOT/lib/intel64/libmkl_rt.so /usr/local/lib/libblas.so.3\n$ ln -sf $MKLROOT/lib/intel64/libmkl_rt.so /usr/local/lib/liblapack.so.3\nOpenBLAS\nThe installation should be done on all nodes of the cluster. Generic version of OpenBLAS are available with most distributions. You can install ", "question": "Where should the Intel MKL installation be performed?", "answers": {"text": ["The installation should be done on all nodes of the cluster."], "answer_start": [180]}}
{"context": "nBLAS\nThe installation should be done on all nodes of the cluster. Generic version of OpenBLAS are available with most distributions. You can install it with a distribution package manager like\napt\nor\nyum\n.\nFor Debian / Ubuntu:\nsudo apt-get install libopenblas-base\nsudo update-alternatives --config libblas.so.3\nFor CentOS / RHEL:\nsudo yum install openblas\nCheck if native libraries are enabled for MLlib\nTo verify native libraries are properly loaded, start\nspark-shell\nand run the following code:\nscala> import dev.ludovic.netlib.blas.NativeBLAS\nscala> NativeBLAS.getInstance()\nIf they are correctly loaded, it should print\ndev.ludovic.netlib.blas.NativeBLAS = dev.ludovic.netlib.blas.JNIBLAS@...\n. Otherwise the warnings should be printed:\nWARN InstanceBuilder: Failed to load implementation from", "question": "How can you install OpenBLAS on Debian / Ubuntu?", "answers": {"text": ["sudo apt-get install libopenblas-base\nsudo update-alternatives --config libblas.so.3"], "answer_start": [228]}}
{"context": ".NativeBLAS = dev.ludovic.netlib.blas.JNIBLAS@...\n. Otherwise the warnings should be printed:\nWARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n...\njava.lang.RuntimeException: Unable to load native implementation\n  at dev.ludovic.netlib.blas.InstanceBuilder.nativeBlas(InstanceBuilder.java:59)\n  at dev.ludovic.netlib.blas.NativeBLAS.getInstance(NativeBLAS.java:31)\n  ...\nYou can also point\ndev.ludovic.netlib\nto specific libraries names and paths. For example,\n-Ddev.ludovic.netlib.blas.nativeLib=libmkl_rt.so\nor\n-Ddev.ludovic.netlib.blas.nativeLibPath=$MKLROOT/lib/intel64/libmkl_rt.so\nfor Intel MKL. You have similar parameters for LAPACK and ARPACK:\n-Ddev.ludovic.netlib.lapack.nativeLib=...\n,\n-Ddev.ludovic.netlib.lapack.nativeLibPath=...\n,\n-Ddev.ludovic.n", "question": "Como especificar caminhos e nomes de bibliotecas para dev.ludovic.netlib?", "answers": {"text": ["You can also point\ndev.ludovic.netlib\nto specific libraries names and paths. For example,\n-Ddev.ludovic.netlib.blas.nativeLib=libmkl_rt.so\nor\n-Ddev.ludovic.netlib.blas.nativeLibPath=$MKLROOT/lib/intel64/libmkl_rt.so\nfor Intel MKL."], "answer_start": [411]}}
{"context": " similar parameters for LAPACK and ARPACK:\n-Ddev.ludovic.netlib.lapack.nativeLib=...\n,\n-Ddev.ludovic.netlib.lapack.nativeLibPath=...\n,\n-Ddev.ludovic.netlib.arpack.nativeLib=...\n, and\n-Ddev.ludovic.netlib.arpack.nativeLibPath=...\n.\nIf native libraries are not properly configured in the system, the Java implementation (javaBLAS) will be used as fallback option.\nSpark Configuration\nThe default behavior of multi-threading in either Intel MKL or OpenBLAS may not be optimal with Spark’s execution model\n1\n.\nTherefore configuring these native libraries to use a single thread for operations may actually improve performance (see\nSPARK-21305\n). It is usually optimal to match this to the number of\nspark.task.cpus\n, which is\n1\nby default and typically left at\n1\n.\nYou can use the options in\nconfig/spark", "question": "What happens if native libraries are not properly configured?", "answers": {"text": ["If native libraries are not properly configured in the system, the Java implementation (javaBLAS) will be used as fallback option."], "answer_start": [231]}}
{"context": "ually optimal to match this to the number of\nspark.task.cpus\n, which is\n1\nby default and typically left at\n1\n.\nYou can use the options in\nconfig/spark-env.sh\nto set thread number for Intel MKL or OpenBLAS:\nFor Intel MKL:\nMKL_NUM_THREADS=1\nFor OpenBLAS:\nOPENBLAS_NUM_THREADS=1\nPlease refer to the following resources to understand how to configure the number of threads for these BLAS implementations:\nIntel MKL\nor\nIntel oneMKL\nand\nOpenBLAS\n.\n↩", "question": "What is the default value of spark.task.cpus?", "answers": {"text": ["1"], "answer_start": [72]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)"], "answer_start": [46]}}
{"context": "taFrame-based API, which is detailed in the\nML user guide on \nTF-IDF\n.\nTerm frequency-inverse document frequency (TF-IDF)\nis a feature \nvectorization method widely used in text mining to reflect the importance of a term to a document in the corpus.\nDenote a term by\n$t$\n, a document by\n$d$\n, and the corpus by\n$D$\n.\nTerm frequency\n$TF(t, d)$\nis the number of times that term\n$t$\nappears in document\n$d$\n,\nwhile document frequency\n$DF(t, D)$\nis the number of documents that contains term\n$t$\n.\nIf we only use term frequency to measure the importance, it is very easy to over-emphasize terms that\nappear very often but carry little information about the document, e.g., “a”, “the”, and “of”.\nIf a term appears very often across the corpus, it means it doesn’t carry special information about\na particul", "question": "What does TF-IDF stand for?", "answers": {"text": ["Term frequency-inverse document frequency (TF-IDF)"], "answer_start": [71]}}
{"context": "]\nThere are several variants on the definition of term frequency and document frequency.\nIn\nspark.mllib\n, we separate TF and IDF to make them flexible.\nOur implementation of term frequency utilizes the\nhashing trick\n.\nA raw feature is mapped into an index (term) by applying a hash function.\nThen term frequencies are calculated based on the mapped indices.\nThis approach avoids the need to compute a global term-to-index map,\nwhich can be expensive for a large corpus, but it suffers from potential hash collisions,\nwhere different raw features may become the same term after hashing.\nTo reduce the chance of collision, we can increase the target feature dimension, i.e., \nthe number of buckets of the hash table.\nThe default feature dimension is\n$2^{20} = 1,048,576$\n.\nNote:\nspark.mllib\ndoesn’t pro", "question": "What is the default feature dimension in spark.mllib?", "answers": {"text": ["$2^{20} = 1,048,576$"], "answer_start": [748]}}
{"context": "ature dimension, i.e., \nthe number of buckets of the hash table.\nThe default feature dimension is\n$2^{20} = 1,048,576$\n.\nNote:\nspark.mllib\ndoesn’t provide tools for text segmentation.\nWe refer users to the\nStanford NLP Group\nand\nscalanlp/chalk\n.\nTF and IDF are implemented in\nHashingTF\nand\nIDF\n.\nHashingTF\ntakes an RDD of list as the input.\nEach record could be an iterable of strings or other types.\nRefer to the\nHashingTF\nPython docs\nfor details on the API.\nfrom\npyspark.mllib.feature\nimport\nHashingTF\n,\nIDF\n# Load documents (one per line).\ndocuments\n=\nsc\n.\ntextFile\n(\n\"\ndata/mllib/kmeans_data.txt\n\"\n).\nmap\n(\nlambda\nline\n:\nline\n.\nsplit\n(\n\"\n\"\n))\nhashingTF\n=\nHashingTF\n()\ntf\n=\nhashingTF\n.\ntransform\n(\ndocuments\n)\n# While applying HashingTF only needs a single pass to the data, applying IDF needs two", "question": "What is the default feature dimension?", "answers": {"text": ["$2^{20} = 1,048,576$"], "answer_start": [98]}}
{"context": "hingTF\n=\nHashingTF\n()\ntf\n=\nhashingTF\n.\ntransform\n(\ndocuments\n)\n# While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n# First to compute the IDF vector and second to scale the term frequencies by IDF.\ntf\n.\ncache\n()\nidf\n=\nIDF\n().\nfit\n(\ntf\n)\ntfidf\n=\nidf\n.\ntransform\n(\ntf\n)\n# spark.mllib's IDF implementation provides an option for ignoring terms\n# which occur in less than a minimum number of documents.\n# In such cases, the IDF for these terms is set to 0.\n# This feature can be used by passing the minDocFreq value to the IDF constructor.\nidfIgnore\n=\nIDF\n(\nminDocFreq\n=\n2\n).\nfit\n(\ntf\n)\ntfidfIgnore\n=\nidfIgnore\n.\ntransform\n(\ntf\n)\nFind full example code at \"examples/src/main/python/mllib/tf_idf_example.py\" in the Spark repo.\nTF and IDF are implemented in\nHash", "question": "What is the purpose of the `minDocFreq` parameter in the IDF constructor?", "answers": {"text": ["In such cases, the IDF for these terms is set to 0."], "answer_start": [445]}}
{"context": "req\n=\n2\n).\nfit\n(\ntf\n)\nval\ntfidfIgnore\n:\nRDD\n[\nVector\n]\n=\nidfIgnore\n.\ntransform\n(\ntf\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\" in the Spark repo.\nWord2Vec\nWord2Vec\ncomputes distributed vector representation of words.\nThe main advantage of the distributed\nrepresentations is that similar words are close in the vector space, which makes generalization to \nnovel patterns easier and model estimation more robust. Distributed vector representation is \nshowed to be useful in many natural language processing applications such as named entity \nrecognition, disambiguation, parsing, tagging and machine translation.\nModel\nIn our implementation of Word2Vec, we used skip-gram model. The training objective of skip-gram is\nto learn word vector r", "question": "What is a main advantage of distributed word representations?", "answers": {"text": ["similar words are close in the vector space"], "answer_start": [341]}}
{"context": "d machine translation.\nModel\nIn our implementation of Word2Vec, we used skip-gram model. The training objective of skip-gram is\nto learn word vector representations that are good at predicting its context in the same sentence. \nMathematically, given a sequence of training words\n$w_1, w_2, \\dots, w_T$\n, the objective of the\nskip-gram model is to maximize the average log-likelihood\n\\[\n\\frac{1}{T} \\sum_{t = 1}^{T}\\sum_{j=-k}^{j=k} \\log p(w_{t+j} | w_t)\n\\]\nwhere $k$ is the size of the training window.\nIn the skip-gram model, every word $w$ is associated with two vectors $u_w$ and $v_w$ which are \nvector representations of $w$ as word and context respectively. The probability of correctly \npredicting word $w_i$ given word $w_j$ is determined by the softmax model, which is\n\\[\np(w_i | w_j ) = \\fr", "question": "What is the objective of the skip-gram model?", "answers": {"text": ["to learn word vector representations that are good at predicting its context in the same sentence."], "answer_start": [128]}}
{"context": "\n)\nfor\nword\n,\ncosine_distance\nin\nsynonyms\n:\nprint\n(\n\"\n{}: {}\n\"\n.\nformat\n(\nword\n,\ncosine_distance\n))\nFind full example code at \"examples/src/main/python/mllib/word2vec_example.py\" in the Spark repo.\nRefer to the\nWord2Vec\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.feature.\n{\nWord2Vec\n,\nWord2VecModel\n}\nval\ninput\n=\nsc\n.\ntextFile\n(\n\"data/mllib/sample_lda_data.txt\"\n).\nmap\n(\nline\n=>\nline\n.\nsplit\n(\n\" \"\n).\ntoSeq\n)\nval\nword2vec\n=\nnew\nWord2Vec\n()\nval\nmodel\n=\nword2vec\n.\nfit\n(\ninput\n)\nval\nsynonyms\n=\nmodel\n.\nfindSynonyms\n(\n\"1\"\n,\n5\n)\nfor\n((\nsynonym\n,\ncosineSimilarity\n)\n<-\nsynonyms\n)\n{\nprintln\n(\ns\n\"$synonym $cosineSimilarity\"\n)\n}\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"myModelPath\"\n)\nval\nsameModel\n=\nWord2VecModel\n.\nload\n(\nsc\n,\n\"myModelPath\"\n)\nFind full example code at \"examples/s", "question": "Where can I find the full example code?", "answers": {"text": ["Find full example code at \"examples/src/main/python/mllib/word2vec_example.py\" in the Spark repo."], "answer_start": [100]}}
{"context": "e and load model\nmodel\n.\nsave\n(\nsc\n,\n\"myModelPath\"\n)\nval\nsameModel\n=\nWord2VecModel\n.\nload\n(\nsc\n,\n\"myModelPath\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\" in the Spark repo.\nStandardScaler\nStandardizes features by scaling to unit variance and/or removing the mean using column summary\nstatistics on the samples in the training set. This is a very common pre-processing step.\nFor example, RBF kernel of Support Vector Machines or the L1 and L2 regularized linear models\ntypically work better when all features have unit variance and/or zero mean.\nStandardization can improve the convergence rate during the optimization process, and also prevents\nagainst features with very large variances exerting an overly large influence during model", "question": "What is StandardScaler used for?", "answers": {"text": ["StandardScaler\nStandardizes features by scaling to unit variance and/or removing the mean using column summary\nstatistics on the samples in the training set."], "answer_start": [238]}}
{"context": " example code at \"examples/src/main/python/mllib/standard_scaler_example.py\" in the Spark repo.\nRefer to the\nStandardScaler\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.feature.\n{\nStandardScaler\n,\nStandardScalerModel\n}\nimport\norg.apache.spark.mllib.linalg.Vectors\nimport\norg.apache.spark.mllib.util.MLUtils\nval\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_libsvm_data.txt\"\n)\nval\nscaler1\n=\nnew\nStandardScaler\n().\nfit\n(\ndata\n.\nmap\n(\nx\n=>\nx\n.\nfeatures\n))\nval\nscaler2\n=\nnew\nStandardScaler\n(\nwithMean\n=\ntrue\n,\nwithStd\n=\ntrue\n).\nfit\n(\ndata\n.\nmap\n(\nx\n=>\nx\n.\nfeatures\n))\n// scaler3 is an identical model to scaler2, and will produce identical transformations\nval\nscaler3\n=\nnew\nStandardScalerModel\n(\nscaler2\n.\nstd\n,\nscaler2\n.\nmean\n)\n// data1 will be unit variance.\nval\ndata1\n=", "question": "Where can I find example code for StandardScaler?", "answers": {"text": ["example code at \"examples/src/main/python/mllib/standard_scaler_example.py\" in the Spark repo."], "answer_start": [1]}}
{"context": "produce identical transformations\nval\nscaler3\n=\nnew\nStandardScalerModel\n(\nscaler2\n.\nstd\n,\nscaler2\n.\nmean\n)\n// data1 will be unit variance.\nval\ndata1\n=\ndata\n.\nmap\n(\nx\n=>\n(\nx\n.\nlabel\n,\nscaler1\n.\ntransform\n(\nx\n.\nfeatures\n)))\n// data2 will be unit variance and zero mean.\nval\ndata2\n=\ndata\n.\nmap\n(\nx\n=>\n(\nx\n.\nlabel\n,\nscaler2\n.\ntransform\n(\nVectors\n.\ndense\n(\nx\n.\nfeatures\n.\ntoArray\n))))\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\" in the Spark repo.\nNormalizer\nNormalizer scales individual samples to have unit $L^p$ norm. This is a common operation for text\nclassification or clustering. For example, the dot product of two $L^2$ normalized TF-IDF vectors\nis the cosine similarity of the vectors.\nNormalizer\nhas the following\nparameter in", "question": "What does Normalizer do?", "answers": {"text": ["Normalizer scales individual samples to have unit $L^p$ norm."], "answer_start": [522]}}
{"context": "For example, the dot product of two $L^2$ normalized TF-IDF vectors\nis the cosine similarity of the vectors.\nNormalizer\nhas the following\nparameter in the constructor:\np\nNormalization in $L^p$ space, $p = 2$ by default.\nNormalizer\nimplements\nVectorTransformer\nwhich can apply the normalization on a\nVector\nto produce a transformed\nVector\nor on\nan\nRDD[Vector]\nto produce a transformed\nRDD[Vector]\n.\nNote that if the norm of the input is zero, it will return the input vector.\nExample\nThe example below demonstrates how to load a dataset in libsvm format, and normalizes the features\nwith $L^2$ norm, and $L^\\infty$ norm.\nRefer to the\nNormalizer\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.feature\nimport\nNormalizer\nfrom\npyspark.mllib.util\nimport\nMLUtils\ndata\n=\nMLUtils\n.\nloadLibSVMFile", "question": "What is the default value for the normalization parameter 'p' in the Normalizer constructor?", "answers": {"text": ["$p = 2$ by default."], "answer_start": [200]}}
{"context": " docs\nfor more details on the API.\nfrom\npyspark.mllib.feature\nimport\nNormalizer\nfrom\npyspark.mllib.util\nimport\nMLUtils\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"\ndata/mllib/sample_libsvm_data.txt\n\"\n)\nlabels\n=\ndata\n.\nmap\n(\nlambda\nx\n:\nx\n.\nlabel\n)\nfeatures\n=\ndata\n.\nmap\n(\nlambda\nx\n:\nx\n.\nfeatures\n)\nnormalizer1\n=\nNormalizer\n()\nnormalizer2\n=\nNormalizer\n(\np\n=\nfloat\n(\n\"\ninf\n\"\n))\n# Each sample in data1 will be normalized using $L^2$ norm.\ndata1\n=\nlabels\n.\nzip\n(\nnormalizer1\n.\ntransform\n(\nfeatures\n))\n# Each sample in data2 will be normalized using $L^\\infty$ norm.\ndata2\n=\nlabels\n.\nzip\n(\nnormalizer2\n.\ntransform\n(\nfeatures\n))\nFind full example code at \"examples/src/main/python/mllib/normalizer_example.py\" in the Spark repo.\nRefer to the\nNormalizer\nScala docs\nfor details on the API.\nimport\norg.apache.spark", "question": "Where can I find the full example code for the Normalizer?", "answers": {"text": ["Find full example code at \"examples/src/main/python/mllib/normalizer_example.py\" in the Spark repo."], "answer_start": [618]}}
{"context": "-wise error rate of selection.\nBy default, the selection method is\nnumTopFeatures\n, with the default number of top features set to 50.\nThe user can choose a selection method using\nsetSelectorType\n.\nThe number of features to select can be tuned using a held-out validation set.\nModel Fitting\nThe\nfit\nmethod takes\nan input of\nRDD[LabeledPoint]\nwith categorical features, learns the summary statistics, and then\nreturns a\nChiSqSelectorModel\nwhich can transform an input dataset into the reduced feature space.\nThe\nChiSqSelectorModel\ncan be applied either to a\nVector\nto produce a reduced\nVector\n, or to\nan\nRDD[Vector]\nto produce a reduced\nRDD[Vector]\n.\nNote that the user can also construct a\nChiSqSelectorModel\nby hand by providing an array of selected feature indices (which must be sorted in ascendin", "question": "What does the fit method return?", "answers": {"text": ["ChiSqSelectorModel"], "answer_start": [419]}}
{"context": "Note that the user can also construct a\nChiSqSelectorModel\nby hand by providing an array of selected feature indices (which must be sorted in ascending order).\nExample\nThe following example shows the basic use of ChiSqSelector. The data set used has a feature matrix consisting of greyscale values that vary from 0 to 255 for each feature.\nRefer to the\nChiSqSelector\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.feature.ChiSqSelector\nimport\norg.apache.spark.mllib.linalg.Vectors\nimport\norg.apache.spark.mllib.regression.LabeledPoint\nimport\norg.apache.spark.mllib.util.MLUtils\n// Load some data in libsvm format\nval\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Discretize data in 16 equal bins since ChiSqSelector requires categorical features\n//", "question": "What is required for the features used with ChiSqSelector?", "answers": {"text": ["ChiSqSelector requires categorical features"], "answer_start": [754]}}
{"context": ".\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Discretize data in 16 equal bins since ChiSqSelector requires categorical features\n// Even though features are doubles, the ChiSqSelector treats each unique value as a category\nval\ndiscretizedData\n=\ndata\n.\nmap\n{\nlp\n=>\nLabeledPoint\n(\nlp\n.\nlabel\n,\nVectors\n.\ndense\n(\nlp\n.\nfeatures\n.\ntoArray\n.\nmap\n{\nx\n=>\n(\nx\n/\n16\n).\nfloor\n}))\n}\n// Create ChiSqSelector that will select top 50 of 692 features\nval\nselector\n=\nnew\nChiSqSelector\n(\n50\n)\n// Create ChiSqSelector model (selecting features)\nval\ntransformer\n=\nselector\n.\nfit\n(\ndiscretizedData\n)\n// Filter the top 50 features from each feature vector\nval\nfilteredData\n=\ndiscretizedData\n.\nmap\n{\nlp\n=>\nLabeledPoint\n(\nlp\n.\nlabel\n,\ntransformer\n.\ntransform\n(\nlp\n.\nfeatures\n))\n}\nFind full example code at", "question": "What does the ChiSqSelector require as input features?", "answers": {"text": ["categorical features"], "answer_start": [127]}}
{"context": "or\nval\nfilteredData\n=\ndiscretizedData\n.\nmap\n{\nlp\n=>\nLabeledPoint\n(\nlp\n.\nlabel\n,\ntransformer\n.\ntransform\n(\nlp\n.\nfeatures\n))\n}\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\" in the Spark repo.\nRefer to the\nChiSqSelector\nJava docs\nfor details on the API.\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.mllib.feature.ChiSqSelector\n;\nimport\norg.apache.spark.mllib.feature.ChiSqSelectorModel\n;\nimport\norg.apache.spark.mllib.linalg.Vectors\n;\nimport\norg.apache.spark.mllib.regression.LabeledPoint\n;\nimport\norg.apache.spark.mllib.util.MLUtils\n;\nJavaRDD\n<\nLabeledPoint\n>\npoints\n=\nMLUtils\n.\nloadLibSVMFile\n(\njsc\n.\nsc\n(),\n\"data/mllib/sample_libsvm_data.txt\"\n).\ntoJavaRDD\n().\ncache\n();\n// Discretize data in 16 equal bins since C", "question": "Where can I find a full example code for ChiSqSelector?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\" in the Spark repo."], "answer_start": [125]}}
{"context": "s\n=\nMLUtils\n.\nloadLibSVMFile\n(\njsc\n.\nsc\n(),\n\"data/mllib/sample_libsvm_data.txt\"\n).\ntoJavaRDD\n().\ncache\n();\n// Discretize data in 16 equal bins since ChiSqSelector requires categorical features\n// Although features are doubles, the ChiSqSelector treats each unique value as a category\nJavaRDD\n<\nLabeledPoint\n>\ndiscretizedData\n=\npoints\n.\nmap\n(\nlp\n->\n{\ndouble\n[]\ndiscretizedFeatures\n=\nnew\ndouble\n[\nlp\n.\nfeatures\n().\nsize\n()];\nfor\n(\nint\ni\n=\n0\n;\ni\n<\nlp\n.\nfeatures\n().\nsize\n();\n++\ni\n)\n{\ndiscretizedFeatures\n[\ni\n]\n=\nMath\n.\nfloor\n(\nlp\n.\nfeatures\n().\napply\n(\ni\n)\n/\n16\n);\n}\nreturn\nnew\nLabeledPoint\n(\nlp\n.\nlabel\n(),\nVectors\n.\ndense\n(\ndiscretizedFeatures\n));\n});\n// Create ChiSqSelector that will select top 50 of 692 features\nChiSqSelector\nselector\n=\nnew\nChiSqSelector\n(\n50\n);\n// Create ChiSqSelector model (sel", "question": "What is the purpose of discretizing the data?", "answers": {"text": ["Discretize data in 16 equal bins since ChiSqSelector requires categorical features"], "answer_start": [110]}}
{"context": "c\n.\nparallelize\n(\nSeq\n(\nVectors\n.\ndense\n(\n1.0\n,\n2.0\n,\n3.0\n),\nVectors\n.\ndense\n(\n4.0\n,\n5.0\n,\n6.0\n)))\nval\ntransformingVector\n=\nVectors\n.\ndense\n(\n0.0\n,\n1.0\n,\n2.0\n)\nval\ntransformer\n=\nnew\nElementwiseProduct\n(\ntransformingVector\n)\n// Batch transform and per-row transform give the same results:\nval\ntransformedData\n=\ntransformer\n.\ntransform\n(\ndata\n)\nval\ntransformedData2\n=\ndata\n.\nmap\n(\nx\n=>\ntransformer\n.\ntransform\n(\nx\n))\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\" in the Spark repo.\nRefer to the\nElementwiseProduct\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.mllib.feature.ElementwiseProduct\n;\nimport\norg.apache.spark.mllib.linalg.Vector\n;\nimport\norg", "question": "Where can I find a full example code for ElementwiseProduct?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\" in the Spark repo."], "answer_start": [415]}}
{"context": "form\n(\ndata\n);\nJavaRDD\n<\nVector\n>\ntransformedData2\n=\ndata\n.\nmap\n(\ntransformer:\n:\ntransform\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\" in the Spark repo.\nPCA\nA feature transformer that projects vectors to a low-dimensional space using PCA.\nDetails you can read at\ndimensionality reduction\n.", "question": "Where can I find a full example code for JavaElementwiseProductExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\" in the Spark repo."], "answer_start": [94]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars"], "answer_start": [46]}}
{"context": "presented in dense\nformat as\n[1.0, 0.0, 3.0]\nor in sparse format as\n(3, [0, 2], [1.0, 3.0])\n, where\n3\nis the size\nof the vector.\nMLlib recognizes the following types as dense vectors:\nNumPy’s\narray\nPython’s list, e.g.,\n[1, 2, 3]\nand the following as sparse vectors:\nMLlib’s\nSparseVector\n.\nSciPy’s\ncsc_matrix\nwith a single column\nWe recommend using NumPy arrays over lists for efficiency, and using the factory methods implemented\nin\nVectors\nto create sparse vectors.\nRefer to the\nVectors\nPython docs\nfor more details on the API.\nimport\nnumpy\nas\nnp\nimport\nscipy.sparse\nas\nsps\nfrom\npyspark.mllib.linalg\nimport\nVectors\n# Use a NumPy array as a dense vector.\ndv1\n=\nnp\n.\narray\n([\n1.0\n,\n0.0\n,\n3.0\n])\n# Use a Python list as a dense vector.\ndv2\n=\n[\n1.0\n,\n0.0\n,\n3.0\n]\n# Create a SparseVector.\nsv1\n=\nVectors\n.\n", "question": "What types does MLlib recognize as dense vectors?", "answers": {"text": ["NumPy’s\narray\nPython’s list, e.g.,\n[1, 2, 3]"], "answer_start": [184]}}
{"context": "cs\nfor details on the API.\nimport\norg.apache.spark.mllib.regression.LabeledPoint\n;\nimport\norg.apache.spark.mllib.util.MLUtils\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nJavaRDD\n<\nLabeledPoint\n>\nexamples\n=\nMLUtils\n.\nloadLibSVMFile\n(\njsc\n.\nsc\n(),\n\"data/mllib/sample_libsvm_data.txt\"\n).\ntoJavaRDD\n();\nLocal matrix\nA local matrix has integer-typed row and column indices and double-typed values, stored on a single\nmachine.  MLlib supports dense matrices, whose entry values are stored in a single double array in\ncolumn-major order, and sparse matrices, whose non-zero entry values are stored in the Compressed Sparse\nColumn (CSC) format in column-major order.  For example, the following dense matrix\n\\[ \\begin{pmatrix}\n1.0 & 2.0 \\\\\n3.0 & 4.0 \\\\\n5.0 & 6.0\n\\end{pmatrix}\n\\]\nis stored in a one-dimensio", "question": "What order are the entry values stored in for dense matrices?", "answers": {"text": ["column-major order"], "answer_start": [512]}}
{"context": "or order.  For example, the following dense matrix\n\\[ \\begin{pmatrix}\n1.0 & 2.0 \\\\\n3.0 & 4.0 \\\\\n5.0 & 6.0\n\\end{pmatrix}\n\\]\nis stored in a one-dimensional array\n[1.0, 3.0, 5.0, 2.0, 4.0, 6.0]\nwith the matrix size\n(3, 2)\n.\nThe base class of local matrices is\nMatrix\n, and we provide two\nimplementations:\nDenseMatrix\n,\nand\nSparseMatrix\n.\nWe recommend using the factory methods implemented\nin\nMatrices\nto create local\nmatrices. Remember, local matrices in MLlib are stored in column-major order.\nRefer to the\nMatrix\nPython docs\nand\nMatrices\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.linalg\nimport\nMatrix\n,\nMatrices\n# Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\ndm2\n=\nMatrices\n.\ndense\n(\n3\n,\n2\n,\n[\n1\n,\n3\n,\n5\n,\n2\n,\n4\n,\n6\n])\n# Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), ", "question": "How is a dense matrix like the one provided stored in a one-dimensional array?", "answers": {"text": ["[1.0, 3.0, 5.0, 2.0, 4.0, 6.0]"], "answer_start": [160]}}
{"context": "ix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\ndm2\n=\nMatrices\n.\ndense\n(\n3\n,\n2\n,\n[\n1\n,\n3\n,\n5\n,\n2\n,\n4\n,\n6\n])\n# Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))\nsm\n=\nMatrices\n.\nsparse\n(\n3\n,\n2\n,\n[\n0\n,\n1\n,\n3\n],\n[\n0\n,\n2\n,\n1\n],\n[\n9\n,\n6\n,\n8\n])\nThe base class of local matrices is\nMatrix\n, and we provide two\nimplementations:\nDenseMatrix\n,\nand\nSparseMatrix\n.\nWe recommend using the factory methods implemented\nin\nMatrices\nto create local\nmatrices. Remember, local matrices in MLlib are stored in column-major order.\nRefer to the\nMatrix\nScala docs\nand\nMatrices\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.linalg.\n{\nMatrix\n,\nMatrices\n}\n// Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\nval\ndm\n:\nMatrix\n=\nMatrices\n.\ndense\n(\n3\n,\n2\n,\nArray\n(\n1.0\n,\n3.0\n,\n5.0\n,\n2.0\n,\n4.0\n,\n6.0\n)", "question": "How are local matrices stored in MLlib?", "answers": {"text": ["Remember, local matrices in MLlib are stored in column-major order."], "answer_start": [443]}}
{"context": "// Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\nval\ndm\n:\nMatrix\n=\nMatrices\n.\ndense\n(\n3\n,\n2\n,\nArray\n(\n1.0\n,\n3.0\n,\n5.0\n,\n2.0\n,\n4.0\n,\n6.0\n))\n// Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))\nval\nsm\n:\nMatrix\n=\nMatrices\n.\nsparse\n(\n3\n,\n2\n,\nArray\n(\n0\n,\n1\n,\n3\n),\nArray\n(\n0\n,\n2\n,\n1\n),\nArray\n(\n9\n,\n6\n,\n8\n))\nThe base class of local matrices is\nMatrix\n, and we provide two\nimplementations:\nDenseMatrix\n,\nand\nSparseMatrix\n.\nWe recommend using the factory methods implemented\nin\nMatrices\nto create local\nmatrices. Remember, local matrices in MLlib are stored in column-major order.\nRefer to the\nMatrix\nJava docs\nand\nMatrices\nJava docs\nfor details on the API.\nimport\norg.apache.spark.mllib.linalg.Matrix\n;\nimport\norg.apache.spark.mllib.linalg.Matrices\n;\n// Create a dense matrix ((1.0, 2.", "question": "How are local matrices in MLlib stored?", "answers": {"text": ["Remember, local matrices in MLlib are stored in column-major order."], "answer_start": [527]}}
{"context": " to store large\nand distributed matrices.  Converting a distributed matrix to a different format may require a\nglobal shuffle, which is quite expensive. Four types of distributed matrices have been implemented\nso far.\nThe basic type is called\nRowMatrix\n. A\nRowMatrix\nis a row-oriented distributed\nmatrix without meaningful row indices, e.g., a collection of feature vectors.\nIt is backed by an RDD of its rows, where each row is a local vector.\nWe assume that the number of columns is not huge for a\nRowMatrix\nso that a single\nlocal vector can be reasonably communicated to the driver and can also be stored /\noperated on using a single node. \nAn\nIndexedRowMatrix\nis similar to a\nRowMatrix\nbut with row indices,\nwhich can be used for identifying rows and executing joins.\nA\nCoordinateMatrix\nis a dist", "question": "What is a RowMatrix backed by?", "answers": {"text": ["It is backed by an RDD of its rows, where each row is a local vector."], "answer_start": [375]}}
{"context": "inalg.distributed.RowMatrix\n;\nJavaRDD\n<\nIndexedRow\n>\nrows\n=\n...\n// a JavaRDD of indexed rows\n// Create an IndexedRowMatrix from a JavaRDD<IndexedRow>.\nIndexedRowMatrix\nmat\n=\nnew\nIndexedRowMatrix\n(\nrows\n.\nrdd\n());\n// Get its size.\nlong\nm\n=\nmat\n.\nnumRows\n();\nlong\nn\n=\nmat\n.\nnumCols\n();\n// Drop its row indices.\nRowMatrix\nrowMat\n=\nmat\n.\ntoRowMatrix\n();\nCoordinateMatrix\nA\nCoordinateMatrix\nis a distributed matrix backed by an RDD of its entries.  Each entry is a tuple\nof\n(i: Long, j: Long, value: Double)\n, where\ni\nis the row index,\nj\nis the column index, and\nvalue\nis the entry value.  A\nCoordinateMatrix\nshould be used only when both\ndimensions of the matrix are huge and the matrix is very sparse.\nA\nCoordinateMatrix\ncan be created from an\nRDD\nof\nMatrixEntry\nentries, where\nMatrixEntry\nis a \nwrapper", "question": "What is a CoordinateMatrix backed by?", "answers": {"text": ["an RDD of its entries"], "answer_start": [420]}}
{"context": "e matrix are huge and the matrix is very sparse.\nA\nCoordinateMatrix\ncan be created from an\nRDD\nof\nMatrixEntry\nentries, where\nMatrixEntry\nis a \nwrapper over\n(long, long, float)\n.  A\nCoordinateMatrix\ncan be converted to a\nRowMatrix\nby \ncalling\ntoRowMatrix\n, or to an\nIndexedRowMatrix\nwith sparse rows by calling\ntoIndexedRowMatrix\n.\nRefer to the\nCoordinateMatrix\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.linalg.distributed\nimport\nCoordinateMatrix\n,\nMatrixEntry\n# Create an RDD of coordinate entries.\n#   - This can be done explicitly with the MatrixEntry class:\nentries\n=\nsc\n.\nparallelize\n([\nMatrixEntry\n(\n0\n,\n0\n,\n1.2\n),\nMatrixEntry\n(\n1\n,\n0\n,\n2.1\n),\nMatrixEntry\n(\n2\n,\n1\n,\n3.7\n)])\n#   - or using (long, long, float) tuples:\nentries\n=\nsc\n.\nparallelize\n([(\n0\n,\n0\n,\n1.2\n),\n(\n1\n,\n0\n,\n2.1\n", "question": "What is a MatrixEntry a wrapper over?", "answers": {"text": ["(long, long, float)"], "answer_start": [156]}}
{"context": "(\n1\n,\n0\n,\n2.1\n),\nMatrixEntry\n(\n2\n,\n1\n,\n3.7\n)])\n#   - or using (long, long, float) tuples:\nentries\n=\nsc\n.\nparallelize\n([(\n0\n,\n0\n,\n1.2\n),\n(\n1\n,\n0\n,\n2.1\n),\n(\n2\n,\n1\n,\n3.7\n)])\n# Create a CoordinateMatrix from an RDD of MatrixEntries.\nmat\n=\nCoordinateMatrix\n(\nentries\n)\n# Get its size.\nm\n=\nmat\n.\nnumRows\n()\n# 3\nn\n=\nmat\n.\nnumCols\n()\n# 2\n# Get the entries as an RDD of MatrixEntries.\nentriesRDD\n=\nmat\n.\nentries\n# Convert to a RowMatrix.\nrowMat\n=\nmat\n.\ntoRowMatrix\n()\n# Convert to an IndexedRowMatrix.\nindexedRowMat\n=\nmat\n.\ntoIndexedRowMatrix\n()\n# Convert to a BlockMatrix.\nblockMat\n=\nmat\n.\ntoBlockMatrix\n()\nA\nCoordinateMatrix\ncan be created from an\nRDD[MatrixEntry]\ninstance, where\nMatrixEntry\nis a\nwrapper over\n(Long, Long, Double)\n.  A\nCoordinateMatrix\ncan be converted to an\nIndexedRowMatrix\nwith sparse r", "question": "From what type of instance can a CoordinateMatrix be created?", "answers": {"text": ["RDD[MatrixEntry]"], "answer_start": [641]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)"], "answer_start": [46]}}
{"context": " based on some notion of similarity.  Clustering is\noften used for exploratory analysis and/or as a component of a hierarchical\nsupervised learning\npipeline (in which distinct classifiers or regression\nmodels are trained for each cluster).\nThe\nspark.mllib\npackage supports the following models:\nK-means\nGaussian mixture\nPower iteration clustering (PIC)\nLatent Dirichlet allocation (LDA)\nBisecting k-means\nStreaming k-means\nK-means\nK-means\nis one of the\nmost commonly used clustering algorithms that clusters the data points into a\npredefined number of clusters. The\nspark.mllib\nimplementation includes a parallelized\nvariant of the\nk-means++\nmethod\ncalled\nkmeans||\n.\nThe implementation in\nspark.mllib\nhas the following parameters:\nk\nis the number of desired clusters. Note that it is possible for few", "question": "Which package in Spark supports clustering models?", "answers": {"text": ["spark.mllib"], "answer_start": [244]}}
{"context": "alled\nkmeans||\n.\nThe implementation in\nspark.mllib\nhas the following parameters:\nk\nis the number of desired clusters. Note that it is possible for fewer than k clusters to be returned, for example, if there are fewer than k distinct points to cluster.\nmaxIterations\nis the maximum number of iterations to run.\ninitializationMode\nspecifies either random initialization or\ninitialization via k-means||.\nruns\nThis param has no effect since Spark 2.0.0.\ninitializationSteps\ndetermines the number of steps in the k-means|| algorithm.\nepsilon\ndetermines the distance threshold within which we consider k-means to have converged.\ninitialModel\nis an optional set of cluster centers used for initialization. If this parameter is supplied, only one run is performed.\nExamples\nThe following examples can be test", "question": "What does the 'initializationMode' parameter specify?", "answers": {"text": ["specifies either random initialization or\ninitialization via k-means||."], "answer_start": [329]}}
{"context": "\n:\nerror\n(\npoint\n)).\nreduce\n(\nlambda\nx\n,\ny\n:\nx\n+\ny\n)\nprint\n(\n\"\nWithin Set Sum of Squared Error =\n\"\n+\nstr\n(\nWSSSE\n))\n# Save and load model\nclusters\n.\nsave\n(\nsc\n,\n\"\ntarget/org/apache/spark/PythonKMeansExample/KMeansModel\n\"\n)\nsameModel\n=\nKMeansModel\n.\nload\n(\nsc\n,\n\"\ntarget/org/apache/spark/PythonKMeansExample/KMeansModel\n\"\n)\nFind full example code at \"examples/src/main/python/mllib/k_means_example.py\" in the Spark repo.\nThe following code snippets can be executed in\nspark-shell\n.\nIn the following example after loading and parsing data, we use the\nKMeans\nobject to cluster the data\ninto two clusters. The number of desired clusters is passed to the algorithm. We then compute Within\nSet Sum of Squared Error (WSSSE). You can reduce this error measure by increasing\nk\n. In fact, the\noptimal\nk\nis usua", "question": "Where can I find the full example code for this?", "answers": {"text": ["Find full example code at \"examples/src/main/python/mllib/k_means_example.py\" in the Spark repo."], "answer_start": [323]}}
{"context": "ers\n=\n2\nval\nnumIterations\n=\n20\nval\nclusters\n=\nKMeans\n.\ntrain\n(\nparsedData\n,\nnumClusters\n,\nnumIterations\n)\n// Evaluate clustering by computing Within Set Sum of Squared Errors\nval\nWSSSE\n=\nclusters\n.\ncomputeCost\n(\nparsedData\n)\nprintln\n(\ns\n\"Within Set Sum of Squared Errors = $WSSSE\"\n)\n// Save and load model\nclusters\n.\nsave\n(\nsc\n,\n\"target/org/apache/spark/KMeansExample/KMeansModel\"\n)\nval\nsameModel\n=\nKMeansModel\n.\nload\n(\nsc\n,\n\"target/org/apache/spark/KMeansExample/KMeansModel\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\" in the Spark repo.\nAll of MLlib’s methods use Java-friendly types, so you can import and call them there the same\nway you do in Scala. The only caveat is that the methods take Scala RDD objects, while the\nSpark Java A", "question": "Where can the full example code be found?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\" in the Spark repo."], "answer_start": [479]}}
{"context": "ions\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Cluster centers:\"\n);\nfor\n(\nVector\ncenter:\nclusters\n.\nclusterCenters\n())\n{\nSystem\n.\nout\n.\nprintln\n(\n\" \"\n+\ncenter\n);\n}\ndouble\ncost\n=\nclusters\n.\ncomputeCost\n(\nparsedData\n.\nrdd\n());\nSystem\n.\nout\n.\nprintln\n(\n\"Cost: \"\n+\ncost\n);\n// Evaluate clustering by computing Within Set Sum of Squared Errors\ndouble\nWSSSE\n=\nclusters\n.\ncomputeCost\n(\nparsedData\n.\nrdd\n());\nSystem\n.\nout\n.\nprintln\n(\n\"Within Set Sum of Squared Errors = \"\n+\nWSSSE\n);\n// Save and load model\nclusters\n.\nsave\n(\njsc\n.\nsc\n(),\n\"target/org/apache/spark/JavaKMeansExample/KMeansModel\"\n);\nKMeansModel\nsameModel\n=\nKMeansModel\n.\nload\n(\njsc\n.\nsc\n(),\n\"target/org/apache/spark/JavaKMeansExample/KMeansModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\" in ", "question": "What is computed to evaluate clustering?", "answers": {"text": ["Within Set Sum of Squared Errors"], "answer_start": [292]}}
{"context": "/spark/JavaKMeansExample/KMeansModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\" in the Spark repo.\nGaussian mixture\nA\nGaussian Mixture Model\nrepresents a composite distribution whereby points are drawn from one of\nk\nGaussian sub-distributions,\neach with its own probability.  The\nspark.mllib\nimplementation uses the\nexpectation-maximization\nalgorithm to induce the maximum-likelihood model given a set of samples.  The implementation\nhas the following parameters:\nk\nis the number of desired clusters.\nconvergenceTol\nis the maximum change in log-likelihood at which we consider convergence achieved.\nmaxIterations\nis the maximum number of iterations to perform without reaching convergence.\ninitialModel\nis an optional starting point ", "question": "What algorithm does the spark.mllib implementation use to induce the maximum-likelihood model?", "answers": {"text": ["expectation-maximization"], "answer_start": [383]}}
{"context": "gence achieved.\nmaxIterations\nis the maximum number of iterations to perform without reaching convergence.\ninitialModel\nis an optional starting point from which to start the EM algorithm. If this parameter is omitted, a random starting point will be constructed from the data.\nExamples\nIn the following example after loading and parsing data, we use a\nGaussianMixture\nobject to cluster the data into two clusters. The number of desired clusters is passed\nto the algorithm. We then output the parameters of the mixture model.\nRefer to the\nGaussianMixture\nPython docs\nand\nGaussianMixtureModel\nPython docs\nfor more details on the API.\nfrom\nnumpy\nimport\narray\nfrom\npyspark.mllib.clustering\nimport\nGaussianMixture\n,\nGaussianMixtureModel\n# Load and parse the data\ndata\n=\nsc\n.\ntextFile\n(\n\"\ndata/mllib/gmm_da", "question": "What happens if the initialModel parameter is omitted?", "answers": {"text": ["If this parameter is omitted, a random starting point will be constructed from the data."], "answer_start": [188]}}
{"context": "\nprint\n(\n\"\nweight =\n\"\n,\ngmm\n.\nweights\n[\ni\n],\n\"\nmu =\n\"\n,\ngmm\n.\ngaussians\n[\ni\n].\nmu\n,\n\"\nsigma =\n\"\n,\ngmm\n.\ngaussians\n[\ni\n].\nsigma\n.\ntoArray\n())\nFind full example code at \"examples/src/main/python/mllib/gaussian_mixture_example.py\" in the Spark repo.\nIn the following example after loading and parsing data, we use a\nGaussianMixture\nobject to cluster the data into two clusters. The number of desired clusters is passed\nto the algorithm. We then output the parameters of the mixture model.\nRefer to the\nGaussianMixture\nScala docs\nand\nGaussianMixtureModel\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.clustering.\n{\nGaussianMixture\n,\nGaussianMixtureModel\n}\nimport\norg.apache.spark.mllib.linalg.Vectors\n// Load and parse the data\nval\ndata\n=\nsc\n.\ntextFile\n(\n\"data/mllib/gmm_data.txt\"\n)\nva", "question": "Where can I find a full example code for Gaussian Mixture?", "answers": {"text": ["Find full example code at \"examples/src/main/python/mllib/gaussian_mixture_example.py\" in the Spark repo."], "answer_start": [141]}}
{"context": "kelihood model\nfor\n(\ni\n<-\n0\nuntil\ngmm\n.\nk\n)\n{\nprintln\n(\n\"weight=%f\\nmu=%s\\nsigma=\\n%s\\n\"\nformat\n(\ngmm\n.\nweights\n(\ni\n),\ngmm\n.\ngaussians\n(\ni\n).\nmu\n,\ngmm\n.\ngaussians\n(\ni\n).\nsigma\n))\n}\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\" in the Spark repo.\nAll of MLlib’s methods use Java-friendly types, so you can import and call them there the same\nway you do in Scala. The only caveat is that the methods take Scala RDD objects, while the\nSpark Java API uses a separate\nJavaRDD\nclass. You can convert a Java RDD to a Scala one by\ncalling\n.rdd()\non your\nJavaRDD\nobject. A self-contained application example\nthat is equivalent to the provided example in Scala is given below:\nRefer to the\nGaussianMixture\nJava docs\nand\nGaussianMixtureModel\nJa", "question": "Where can I find a full example code for GaussianMixtureExample?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\" in the Spark repo."], "answer_start": [181]}}
{"context": "ixtureModel\"\n);\n// Output the parameters of the mixture model\nfor\n(\nint\nj\n=\n0\n;\nj\n<\ngmm\n.\nk\n();\nj\n++)\n{\nSystem\n.\nout\n.\nprintf\n(\n\"weight=%f\\nmu=%s\\nsigma=\\n%s\\n\"\n,\ngmm\n.\nweights\n()[\nj\n],\ngmm\n.\ngaussians\n()[\nj\n].\nmu\n(),\ngmm\n.\ngaussians\n()[\nj\n].\nsigma\n());\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\" in the Spark repo.\nPower iteration clustering (PIC)\nPower iteration clustering (PIC) is a scalable and efficient algorithm for clustering vertices of a\ngraph given pairwise similarities as edge properties,\ndescribed in\nLin and Cohen, Power Iteration Clustering\n.\nIt computes a pseudo-eigenvector of the normalized affinity matrix of the graph via\npower iteration\nand uses it to cluster vertices.\nspark.mllib\nincludes an implement", "question": "Where can I find a full example code for JavaGaussianMixtureExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\" in the Spark repo."], "answer_start": [256]}}
{"context": "eudo-eigenvector of the normalized affinity matrix of the graph via\npower iteration\nand uses it to cluster vertices.\nspark.mllib\nincludes an implementation of PIC using GraphX as its backend.\nIt takes an\nRDD\nof\n(srcId, dstId, similarity)\ntuples and outputs a model with the clustering assignments.\nThe similarities must be nonnegative.\nPIC assumes that the similarity measure is symmetric.\nA pair\n(srcId, dstId)\nregardless of the ordering should appear at most once in the input data.\nIf a pair is missing from input, their similarity is treated as zero.\nspark.mllib\n’s PIC implementation takes the following (hyper-)parameters:\nk\n: number of clusters\nmaxIterations\n: maximum number of power iterations\ninitializationMode\n: initialization model. This can be either “random”, which is the default,\nto ", "question": "What does the PIC implementation in spark.mllib assume about the similarity measure?", "answers": {"text": ["PIC assumes that the similarity measure is symmetric."], "answer_start": [336]}}
{"context": "0\n);\nPowerIterationClusteringModel\nmodel\n=\npic\n.\nrun\n(\nsimilarities\n);\nfor\n(\nPowerIterationClustering\n.\nAssignment\na:\nmodel\n.\nassignments\n().\ntoJavaRDD\n().\ncollect\n())\n{\nSystem\n.\nout\n.\nprintln\n(\na\n.\nid\n()\n+\n\" -> \"\n+\na\n.\ncluster\n());\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\" in the Spark repo.\nLatent Dirichlet allocation (LDA)\nLatent Dirichlet allocation (LDA)\nis a topic model which infers topics from a collection of text documents.\nLDA can be thought of as a clustering algorithm as follows:\nTopics correspond to cluster centers, and documents correspond to\nexamples (rows) in a dataset.\nTopics and documents both exist in a feature space, where feature\nvectors are vectors of word counts (bag of words).\nRather ", "question": "Where can I find a full example code for PowerIterationClustering?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\" in the Spark repo."], "answer_start": [235]}}
{"context": "apache/spark/PythonLatentDirichletAllocationExample/LDAModel\n\"\n)\nFind full example code at \"examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\" in the Spark repo.\nRefer to the\nLDA\nScala docs\nand\nDistributedLDAModel\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.clustering.\n{\nDistributedLDAModel\n,\nLDA\n}\nimport\norg.apache.spark.mllib.linalg.Vectors\n// Load and parse the data\nval\ndata\n=\nsc\n.\ntextFile\n(\n\"data/mllib/sample_lda_data.txt\"\n)\nval\nparsedData\n=\ndata\n.\nmap\n(\ns\n=>\nVectors\n.\ndense\n(\ns\n.\ntrim\n.\nsplit\n(\n' '\n).\nmap\n(\n_\n.\ntoDouble\n)))\n// Index documents with unique IDs\nval\ncorpus\n=\nparsedData\n.\nzipWithIndex\n().\nmap\n(\n_\n.\nswap\n).\ncache\n()\n// Cluster the documents into three topics using LDA\nval\nldaModel\n=\nnew\nLDA\n().\nsetK\n(\n3\n).\nrun\n(\ncorpus\n)\n// Output t", "question": "Where can I find the full example code for Latent Dirichlet Allocation?", "answers": {"text": ["Find full example code at \"examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\" in the Spark repo."], "answer_start": [65]}}
{"context": "\nDistributedLDAModel\n.\nload\n(\nsc\n,\n\"target/org/apache/spark/LatentDirichletAllocationExample/LDAModel\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\" in the Spark repo.\nRefer to the\nLDA\nJava docs\nand\nDistributedLDAModel\nJava docs\nfor details on the API.\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.api.java.JavaPairRDD\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.mllib.clustering.DistributedLDAModel\n;\nimport\norg.apache.spark.mllib.clustering.LDA\n;\nimport\norg.apache.spark.mllib.clustering.LDAModel\n;\nimport\norg.apache.spark.mllib.linalg.Matrix\n;\nimport\norg.apache.spark.mllib.linalg.Vector\n;\nimport\norg.apache.spark.mllib.linalg.Vectors\n;\n// Load and parse the data\nString\npath\n=\n\"data/mllib/sa", "question": "Where can I find the full example code for LatentDirichletAllocationExample?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\" in the Spark repo."], "answer_start": [105]}}
{"context": "daModel\n.\nsave\n(\njsc\n.\nsc\n(),\n\"target/org/apache/spark/JavaLatentDirichletAllocationExample/LDAModel\"\n);\nDistributedLDAModel\nsameModel\n=\nDistributedLDAModel\n.\nload\n(\njsc\n.\nsc\n(),\n\"target/org/apache/spark/JavaLatentDirichletAllocationExample/LDAModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\" in the Spark repo.\nBisecting k-means\nBisecting K-means can often be much faster than regular K-means, but it will generally produce a different clustering.\nBisecting k-means is a kind of\nhierarchical clustering\n.\nHierarchical clustering is one of the most commonly used  method of cluster analysis which seeks to build a hierarchy of clusters.\nStrategies for hierarchical clustering generally fall into two types:\nAgglome", "question": "Where can I find the full example code for JavaLatentDirichletAllocationExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\" in the Spark repo."], "answer_start": [254]}}
{"context": "method of cluster analysis which seeks to build a hierarchy of clusters.\nStrategies for hierarchical clustering generally fall into two types:\nAgglomerative: This is a “bottom up” approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.\nDivisive: This is a “top down” approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.\nBisecting k-means algorithm is a kind of divisive algorithms.\nThe implementation in MLlib has the following parameters:\nk\n: the desired number of leaf clusters (default: 4). The actual number could be smaller if there are no divisible leaf clusters.\nmaxIterations\n: the max number of k-means iterations to split clusters (default: 20)\nminDivisibleClust", "question": "What is the default value for the 'k' parameter in the MLlib implementation?", "answers": {"text": ["4"], "answer_start": [618]}}
{"context": "maller if there are no divisible leaf clusters.\nmaxIterations\n: the max number of k-means iterations to split clusters (default: 20)\nminDivisibleClusterSize\n: the minimum number of points (if >= 1.0) or the minimum proportion of points (if < 1.0) of a divisible cluster (default: 1)\nseed\n: a random seed (default: hash value of the class name)\nExamples\nRefer to the\nBisectingKMeans\nPython docs\nand\nBisectingKMeansModel\nPython docs\nfor more details on the API.\nfrom\nnumpy\nimport\narray\nfrom\npyspark.mllib.clustering\nimport\nBisectingKMeans\n# Load and parse the data\ndata\n=\nsc\n.\ntextFile\n(\n\"\ndata/mllib/kmeans_data.txt\n\"\n)\nparsedData\n=\ndata\n.\nmap\n(\nlambda\nline\n:\narray\n([\nfloat\n(\nx\n)\nfor\nx\nin\nline\n.\nsplit\n(\n'\n'\n)]))\n# Build the model (cluster the data)\nmodel\n=\nBisectingKMeans\n.\ntrain\n(\nparsedData\n,\n2\n,", "question": "What is the default value for maxIterations?", "answers": {"text": ["the max number of k-means iterations to split clusters (default: 20)"], "answer_start": [64]}}
{"context": "a\nline\n:\narray\n([\nfloat\n(\nx\n)\nfor\nx\nin\nline\n.\nsplit\n(\n'\n'\n)]))\n# Build the model (cluster the data)\nmodel\n=\nBisectingKMeans\n.\ntrain\n(\nparsedData\n,\n2\n,\nmaxIterations\n=\n5\n)\n# Evaluate clustering\ncost\n=\nmodel\n.\ncomputeCost\n(\nparsedData\n)\nprint\n(\n\"\nBisecting K-means Cost =\n\"\n+\nstr\n(\ncost\n))\nFind full example code at \"examples/src/main/python/mllib/bisecting_k_means_example.py\" in the Spark repo.\nRefer to the\nBisectingKMeans\nScala docs\nand\nBisectingKMeansModel\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.clustering.BisectingKMeans\nimport\norg.apache.spark.mllib.linalg.\n{\nVector\n,\nVectors\n}\n// Loads and parses data\ndef\nparse\n(\nline\n:\nString\n)\n:\nVector\n=\nVectors\n.\ndense\n(\nline\n.\nsplit\n(\n\" \"\n).\nmap\n(\n_\n.\ntoDouble\n))\nval\ndata\n=\nsc\n.\ntextFile\n(\n\"data/mllib/kmeans_data.txt\"\n).\nmap\n", "question": "Where can I find a full example code for Bisecting K-means?", "answers": {"text": ["Find full example code at \"examples/src/main/python/mllib/bisecting_k_means_example.py\" in the Spark repo."], "answer_start": [288]}}
{"context": "ne\n:\nString\n)\n:\nVector\n=\nVectors\n.\ndense\n(\nline\n.\nsplit\n(\n\" \"\n).\nmap\n(\n_\n.\ntoDouble\n))\nval\ndata\n=\nsc\n.\ntextFile\n(\n\"data/mllib/kmeans_data.txt\"\n).\nmap\n(\nparse\n).\ncache\n()\n// Clustering the data into 6 clusters by BisectingKMeans.\nval\nbkm\n=\nnew\nBisectingKMeans\n().\nsetK\n(\n6\n)\nval\nmodel\n=\nbkm\n.\nrun\n(\ndata\n)\n// Show the compute cost and the cluster centers\nprintln\n(\ns\n\"Compute Cost: ${model.computeCost(data)}\"\n)\nmodel\n.\nclusterCenters\n.\nzipWithIndex\n.\nforeach\n{\ncase\n(\ncenter\n,\nidx\n)\n=>\nprintln\n(\ns\n\"Cluster Center ${idx}: ${center}\"\n)\n}\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\" in the Spark repo.\nRefer to the\nBisectingKMeans\nJava docs\nand\nBisectingKMeansModel\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\n", "question": "Where can I find the full example code for BisectingKMeans?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\" in the Spark repo."], "answer_start": [537]}}
{"context": "\n20.3\n,\n20.3\n),\nVectors\n.\ndense\n(\n30.1\n,\n30.1\n),\nVectors\n.\ndense\n(\n30.3\n,\n30.3\n)\n);\nJavaRDD\n<\nVector\n>\ndata\n=\nsc\n.\nparallelize\n(\nlocalData\n,\n2\n);\nBisectingKMeans\nbkm\n=\nnew\nBisectingKMeans\n()\n.\nsetK\n(\n4\n);\nBisectingKMeansModel\nmodel\n=\nbkm\n.\nrun\n(\ndata\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Compute Cost: \"\n+\nmodel\n.\ncomputeCost\n(\ndata\n));\nVector\n[]\nclusterCenters\n=\nmodel\n.\nclusterCenters\n();\nfor\n(\nint\ni\n=\n0\n;\ni\n<\nclusterCenters\n.\nlength\n;\ni\n++)\n{\nVector\nclusterCenter\n=\nclusterCenters\n[\ni\n];\nSystem\n.\nout\n.\nprintln\n(\n\"Cluster Center \"\n+\ni\n+\n\": \"\n+\nclusterCenter\n);\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\" in the Spark repo.\nStreaming k-means\nWhen data arrive in a stream, we may want to estimate clusters dynamically,\nupdating them", "question": "Where can I find the full example code for JavaBisectingKMeansExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\" in the Spark repo."], "answer_start": [559]}}
{"context": "tingKMeansExample.java\" in the Spark repo.\nStreaming k-means\nWhen data arrive in a stream, we may want to estimate clusters dynamically,\nupdating them as new data arrive.\nspark.mllib\nprovides support for streaming k-means clustering,\nwith parameters to control the decay (or “forgetfulness”) of the estimates. The algorithm\nuses a generalization of the mini-batch k-means update rule. For each batch of data, we assign\nall points to their nearest cluster, compute new cluster centers, then update each cluster using:\n\\begin{equation}\n    c_{t+1} = \\frac{c_tn_t\\alpha + x_tm_t}{n_t\\alpha+m_t}\n\\end{equation}\n\\begin{equation}\n    n_{t+1} = n_t + m_t\n\\end{equation}\nWhere\n$c_t$\nis the previous center for the cluster,\n$n_t$\nis the number of points assigned\nto the cluster thus far,\n$x_t$\nis the new clus", "question": "What is used to update each cluster in streaming k-means clustering?", "answers": {"text": ["c_{t+1} = \\frac{c_tn_t\\alpha + x_tm_t}{n_t\\alpha+m_t}"], "answer_start": [538]}}
{"context": "sters and specify the number of clusters to find\nmodel\n=\nStreamingKMeans\n(\nk\n=\n2\n,\ndecayFactor\n=\n1.0\n).\nsetRandomCenters\n(\n3\n,\n1.0\n,\n0\n)\n# Now register the streams for training and testing and start the job,\n# printing the predicted cluster assignments on new data points as they arrive.\nmodel\n.\ntrainOn\n(\ntrainingStream\n)\nresult\n=\nmodel\n.\npredictOnValues\n(\ntestingStream\n.\nmap\n(\nlambda\nlp\n:\n(\nlp\n.\nlabel\n,\nlp\n.\nfeatures\n)))\nresult\n.\npprint\n()\nssc\n.\nstart\n()\nssc\n.\nstop\n(\nstopSparkContext\n=\nTrue\n,\nstopGraceFully\n=\nTrue\n)\nFind full example code at \"examples/src/main/python/mllib/streaming_k_means_example.py\" in the Spark repo.\nRefer to the\nStreamingKMeans\nScala docs\nfor details on the API.\nAnd Refer to\nSpark Streaming Programming Guide\nfor details on StreamingContext.\nimport\norg.apache.spark.mll", "question": "Where can I find a full example code for StreamingKMeans?", "answers": {"text": ["Find full example code at \"examples/src/main/python/mllib/streaming_k_means_example.py\" in the Spark repo."], "answer_start": [522]}}
{"context": "gKMeans\nScala docs\nfor details on the API.\nAnd Refer to\nSpark Streaming Programming Guide\nfor details on StreamingContext.\nimport\norg.apache.spark.mllib.clustering.StreamingKMeans\nimport\norg.apache.spark.mllib.linalg.Vectors\nimport\norg.apache.spark.mllib.regression.LabeledPoint\nimport\norg.apache.spark.streaming.\n{\nSeconds\n,\nStreamingContext\n}\nval\nconf\n=\nnew\nSparkConf\n().\nsetAppName\n(\n\"StreamingKMeansExample\"\n)\nval\nssc\n=\nnew\nStreamingContext\n(\nconf\n,\nSeconds\n(\nargs\n(\n2\n).\ntoLong\n))\nval\ntrainingData\n=\nssc\n.\ntextFileStream\n(\nargs\n(\n0\n)).\nmap\n(\nVectors\n.\nparse\n)\nval\ntestData\n=\nssc\n.\ntextFileStream\n(\nargs\n(\n1\n)).\nmap\n(\nLabeledPoint\n.\nparse\n)\nval\nmodel\n=\nnew\nStreamingKMeans\n()\n.\nsetK\n(\nargs\n(\n3\n).\ntoInt\n)\n.\nsetDecayFactor\n(\n1.0\n)\n.\nsetRandomCenters\n(\nargs\n(\n4\n).\ntoInt\n,\n0.0\n)\nmodel\n.\ntrainOn\n(\nt", "question": "Which class is used to create a StreamingContext?", "answers": {"text": ["org.apache.spark.streaming."], "answer_start": [286]}}
{"context": "odel\n=\nnew\nStreamingKMeans\n()\n.\nsetK\n(\nargs\n(\n3\n).\ntoInt\n)\n.\nsetDecayFactor\n(\n1.0\n)\n.\nsetRandomCenters\n(\nargs\n(\n4\n).\ntoInt\n,\n0.0\n)\nmodel\n.\ntrainOn\n(\ntrainingData\n)\nmodel\n.\npredictOnValues\n(\ntestData\n.\nmap\n(\nlp\n=>\n(\nlp\n.\nlabel\n,\nlp\n.\nfeatures\n))).\nprint\n()\nssc\n.\nstart\n()\nssc\n.\nawaitTermination\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\" in the Spark repo.\nAs you add new text files with data the cluster centers will update. Each training\npoint should be formatted as\n[x1, x2, x3]\n, and each test data point\nshould be formatted as\n(y, [x1, x2, x3])\n, where\ny\nis some useful label or identifier\n(e.g. a true category assignment). Anytime a text file is placed in\n/training/data/dir\nthe model will update. Anytime a text file is ", "question": "How should each training point be formatted?", "answers": {"text": ["[x1, x2, x3]"], "answer_start": [541]}}
{"context": "el or identifier\n(e.g. a true category assignment). Anytime a text file is placed in\n/training/data/dir\nthe model will update. Anytime a text file is placed in\n/testing/data/dir\nyou will see predictions. With new data, the cluster centers will change!", "question": "Where should text files be placed to trigger model updates?", "answers": {"text": ["the model will update."], "answer_start": [104]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nStarting Point: SparkSession\nCreating DataFrames\nUntyped Dataset Operations (DataFrame operations)\nRunning SQL Queries Programmatically\nGlobal Temporary View\nCreating Datasets\nInteroperating with RDDs\nScalar Functions\nA", "question": "What is the starting point for getting started with Spark SQL?", "answers": {"text": ["Starting Point: SparkSession"], "answer_start": [581]}}
{"context": "ions\nStarting Point: SparkSession\nThe entry point into all functionality in Spark is the\nSparkSession\nclass. To create a basic\nSparkSession\n, just use\nSparkSession.builder\n:\nfrom\npyspark.sql\nimport\nSparkSession\nspark\n=\nSparkSession\n\\\n.\nbuilder\n\\\n.\nappName\n(\n\"\nPython Spark SQL basic example\n\"\n)\n\\\n.\nconfig\n(\n\"\nspark.some.config.option\n\"\n,\n\"\nsome-value\n\"\n)\n\\\n.\ngetOrCreate\n()\nFind full example code at \"examples/src/main/python/sql/basic.py\" in the Spark repo.\nThe entry point into all functionality in Spark is the\nSparkSession\nclass. To create a basic\nSparkSession\n, just use\nSparkSession.builder()\n:\nimport\norg.apache.spark.sql.SparkSession\nval\nspark\n=\nSparkSession\n.\nbuilder\n()\n.\nappName\n(\n\"Spark SQL basic example\"\n)\n.\nconfig\n(\n\"spark.some.config.option\"\n,\n\"some-value\"\n)\n.\ngetOrCreate\n()\nFind fu", "question": "How can a basic SparkSession be created in Python?", "answers": {"text": ["SparkSession.builder"], "answer_start": [151]}}
{"context": "rk\n=\nSparkSession\n.\nbuilder\n()\n.\nappName\n(\n\"Spark SQL basic example\"\n)\n.\nconfig\n(\n\"spark.some.config.option\"\n,\n\"some-value\"\n)\n.\ngetOrCreate\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\" in the Spark repo.\nThe entry point into all functionality in Spark is the\nSparkSession\nclass. To create a basic\nSparkSession\n, just use\nSparkSession.builder()\n:\nimport\norg.apache.spark.sql.SparkSession\n;\nSparkSession\nspark\n=\nSparkSession\n.\nbuilder\n()\n.\nappName\n(\n\"Java Spark SQL basic example\"\n)\n.\nconfig\n(\n\"spark.some.config.option\"\n,\n\"some-value\"\n)\n.\ngetOrCreate\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\" in the Spark repo.\nThe entry point into all functionality in Spark is the\nSpar", "question": "Where can I find the full example code for Scala?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\" in the Spark repo."], "answer_start": [143]}}
{"context": "l.Row\n;\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\nread\n().\njson\n(\n\"examples/src/main/resources/people.json\"\n);\n// Displays the content of the DataFrame to stdout\ndf\n.\nshow\n();\n// +----+-------+\n// | age|   name|\n// +----+-------+\n// |null|Michael|\n// |  30|   Andy|\n// |  19| Justin|\n// +----+-------+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\" in the Spark repo.\nWith a\nSparkSession\n, applications can create DataFrames from a local R data.frame,\nfrom a Hive table, or from\nSpark data sources\n.\nAs an example, the following creates a DataFrame based on the content of a JSON file:\ndf\n<-\nread.json\n(\n\"examples/src/main/resources/people.json\"\n)\n# Displays the content of the DataFrame\nhead\n(\ndf\n)\n##   age    name\n## 1  NA Michael\n## 2  30    Andy\n## ", "question": "From what file does the example create a DataFrame?", "answers": {"text": ["\"examples/src/main/resources/people.json\""], "answer_start": [53]}}
{"context": "more. The complete list is available in the\nDataFrame Function Reference\n.\n// This import is needed to use the $-notation\nimport\nspark.implicits._\n// Print the schema in a tree format\ndf\n.\nprintSchema\n()\n// root\n// |-- age: long (nullable = true)\n// |-- name: string (nullable = true)\n// Select only the \"name\" column\ndf\n.\nselect\n(\n\"name\"\n).\nshow\n()\n// +-------+\n// |   name|\n// +-------+\n// |Michael|\n// |   Andy|\n// | Justin|\n// +-------+\n// Select everybody, but increment the age by 1\ndf\n.\nselect\n(\n$\n\"name\"\n,\n$\n\"age\"\n+\n1\n).\nshow\n()\n// +-------+---------+\n// |   name|(age + 1)|\n// +-------+---------+\n// |Michael|     null|\n// |   Andy|       31|\n// | Justin|       20|\n// +-------+---------+\n// Select people older than 21\ndf\n.\nfilter\n(\n$\n\"age\"\n>\n21\n).\nshow\n()\n// +---+----+\n// |age|name|\n// +-", "question": "What does `df.printSchema()` do?", "answers": {"text": ["Print the schema in a tree format"], "answer_start": [150]}}
{"context": "e by 1\ndf\n.\nselect\n(\ncol\n(\n\"name\"\n),\ncol\n(\n\"age\"\n).\nplus\n(\n1\n)).\nshow\n();\n// +-------+---------+\n// |   name|(age + 1)|\n// +-------+---------+\n// |Michael|     null|\n// |   Andy|       31|\n// | Justin|       20|\n// +-------+---------+\n// Select people older than 21\ndf\n.\nfilter\n(\ncol\n(\n\"age\"\n).\ngt\n(\n21\n)).\nshow\n();\n// +---+----+\n// |age|name|\n// +---+----+\n// | 30|Andy|\n// +---+----+\n// Count people by age\ndf\n.\ngroupBy\n(\n\"age\"\n).\ncount\n().\nshow\n();\n// +----+-----+\n// | age|count|\n// +----+-----+\n// |  19|    1|\n// |null|    1|\n// |  30|    1|\n// +----+-----+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\" in the Spark repo.\nFor a complete list of the types of operations that can be performed on a Dataset refer to the\nAPI Documentatio", "question": "Where can I find the full example code?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\" in the Spark repo."], "answer_start": [564]}}
{"context": "Schema\n(\ndf\n)\n## root\n## |-- age: long (nullable = true)\n## |-- name: string (nullable = true)\n# Select only the \"name\" column\nhead\n(\nselect\n(\ndf\n,\n\"name\"\n))\n##      name\n## 1 Michael\n## 2    Andy\n## 3  Justin\n# Select everybody, but increment the age by 1\nhead\n(\nselect\n(\ndf\n,\ndf\n$\nname\n,\ndf\n$\nage\n+\n1\n))\n##      name (age + 1.0)\n## 1 Michael          NA\n## 2    Andy          31\n## 3  Justin          20\n# Select people older than 21\nhead\n(\nwhere\n(\ndf\n,\ndf\n$\nage\n>\n21\n))\n##   age name\n## 1  30 Andy\n# Count people by age\nhead\n(\ncount\n(\ngroupBy\n(\ndf\n,\n\"age\"\n)))\n##   age count\n## 1  19     1\n## 2  NA     1\n## 3  30     1\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nFor a complete list of the types of operations that can be performed on a DataFrame refer t", "question": "What does the code do when it selects people older than 21?", "answers": {"text": ["##   age name\n## 1  30 Andy"], "answer_start": [473]}}
{"context": "------+\n// |null|Michael|\n// |  30|   Andy|\n// |  19| Justin|\n// +----+-------+\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\" in the Spark repo.\nThe\nsql\nfunction on a\nSparkSession\nenables applications to run SQL queries programmatically and returns the result as a\nDataset<Row>\n.\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\n// Register the DataFrame as a SQL temporary view\ndf\n.\ncreateOrReplaceTempView\n(\n\"people\"\n);\nDataset\n<\nRow\n>\nsqlDF\n=\nspark\n.\nsql\n(\n\"SELECT * FROM people\"\n);\nsqlDF\n.\nshow\n();\n// +----+-------+\n// | age|   name|\n// +----+-------+\n// |null|Michael|\n// |  30|   Andy|\n// |  19| Justin|\n// +----+-------+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSparkS", "question": "What is the result of the SQL query \"SELECT * FROM people\"?", "answers": {"text": ["+----+-------+\n// | age|   name|\n// +----+-------+\n// |null|Michael|\n// |  30|   Andy|\n// |  19| Justin|\n// +----+-------+"], "answer_start": [587]}}
{"context": "hael|\n// |  30|   Andy|\n// |  19| Justin|\n// +----+-------+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\" in the Spark repo.\nThe\nsql\nfunction enables applications to run SQL queries programmatically and returns the result as a\nSparkDataFrame\n.\ndf\n<-\nsql\n(\n\"SELECT * FROM table\"\n)\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nGlobal Temporary View\nTemporary views in Spark SQL are session-scoped and will disappear if the session that creates it\nterminates. If you want to have a temporary view that is shared among all sessions and keep alive\nuntil the Spark application terminates, you can create a global temporary view. Global temporary\nview is tied to a system preserved database\nglobal_temp\n, a", "question": "Where can I find a full example code for JavaSparkSQLExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\" in the Spark repo."], "answer_start": [60]}}
{"context": "the Spark application terminates, you can create a global temporary view. Global temporary\nview is tied to a system preserved database\nglobal_temp\n, and we must use the qualified name to\nrefer it, e.g.\nSELECT * FROM global_temp.view1\n.\n# Register the DataFrame as a global temporary view\ndf\n.\ncreateGlobalTempView\n(\n\"\npeople\n\"\n)\n# Global temporary view is tied to a system preserved database `global_temp`\nspark\n.\nsql\n(\n\"\nSELECT * FROM global_temp.people\n\"\n).\nshow\n()\n# +----+-------+\n# | age|   name|\n# +----+-------+\n# |null|Michael|\n# |  30|   Andy|\n# |  19| Justin|\n# +----+-------+\n# Global temporary view is cross-session\nspark\n.\nnewSession\n().\nsql\n(\n\"\nSELECT * FROM global_temp.people\n\"\n).\nshow\n()\n# +----+-------+\n# | age|   name|\n# +----+-------+\n# |null|Michael|\n# |  30|   Andy|\n# |  19| J", "question": "How do you refer to a global temporary view?", "answers": {"text": ["SELECT * FROM global_temp.view1"], "answer_start": [202]}}
{"context": "\nsql\n(\n\"\nSELECT * FROM global_temp.people\n\"\n).\nshow\n()\n# +----+-------+\n# | age|   name|\n# +----+-------+\n# |null|Michael|\n# |  30|   Andy|\n# |  19| Justin|\n# +----+-------+\nFind full example code at \"examples/src/main/python/sql/basic.py\" in the Spark repo.\n// Register the DataFrame as a global temporary view\ndf\n.\ncreateGlobalTempView\n(\n\"people\"\n)\n// Global temporary view is tied to a system preserved database `global_temp`\nspark\n.\nsql\n(\n\"SELECT * FROM global_temp.people\"\n).\nshow\n()\n// +----+-------+\n// | age|   name|\n// +----+-------+\n// |null|Michael|\n// |  30|   Andy|\n// |  19| Justin|\n// +----+-------+\n// Global temporary view is cross-session\nspark\n.\nnewSession\n().\nsql\n(\n\"SELECT * FROM global_temp.people\"\n).\nshow\n()\n// +----+-------+\n// | age|   name|\n// +----+-------+\n// |null|Micha", "question": "What database is the global temporary view tied to?", "answers": {"text": ["Global temporary view is tied to a system preserved database `global_temp`"], "answer_start": [354]}}
{"context": "ession\nspark\n.\nnewSession\n().\nsql\n(\n\"SELECT * FROM global_temp.people\"\n).\nshow\n()\n// +----+-------+\n// | age|   name|\n// +----+-------+\n// |null|Michael|\n// |  30|   Andy|\n// |  19| Justin|\n// +----+-------+\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\" in the Spark repo.\n// Register the DataFrame as a global temporary view\ndf\n.\ncreateGlobalTempView\n(\n\"people\"\n);\n// Global temporary view is tied to a system preserved database `global_temp`\nspark\n.\nsql\n(\n\"SELECT * FROM global_temp.people\"\n).\nshow\n();\n// +----+-------+\n// | age|   name|\n// +----+-------+\n// |null|Michael|\n// |  30|   Andy|\n// |  19| Justin|\n// +----+-------+\n// Global temporary view is cross-session\nspark\n.\nnewSession\n().\nsql\n(\n\"SELECT * FROM global_temp.people\"\n).\nsh", "question": "Where can I find the full example code for SparkSQLExample?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\" in the Spark repo."], "answer_start": [208]}}
{"context": "y|\n// |  19| Justin|\n// +----+-------+\n// Global temporary view is cross-session\nspark\n.\nnewSession\n().\nsql\n(\n\"SELECT * FROM global_temp.people\"\n).\nshow\n();\n// +----+-------+\n// | age|   name|\n// +----+-------+\n// |null|Michael|\n// |  30|   Andy|\n// |  19| Justin|\n// +----+-------+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\" in the Spark repo.\nCREATE\nGLOBAL\nTEMPORARY\nVIEW\ntemp_view\nAS\nSELECT\na\n+\n1\n,\nb\n*\n2\nFROM\ntbl\nSELECT\n*\nFROM\nglobal_temp\n.\ntemp_view\nCreating Datasets\nDatasets are similar to RDDs, however, instead of using Java serialization or Kryo they use\na specialized\nEncoder\nto serialize the objects\nfor processing or transmitting over the network. While both encoders and standard serialization are\nresponsible for turning a", "question": "What is the full example code location?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\" in the Spark repo."], "answer_start": [283]}}
{"context": "ly provided by importing spark.implicits._\nval\nprimitiveDS\n=\nSeq\n(\n1\n,\n2\n,\n3\n).\ntoDS\n()\nprimitiveDS\n.\nmap\n(\n_\n+\n1\n).\ncollect\n()\n// Returns: Array(2, 3, 4)\n// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name\nval\npath\n=\n\"examples/src/main/resources/people.json\"\nval\npeopleDS\n=\nspark\n.\nread\n.\njson\n(\npath\n).\nas\n[\nPerson\n]\npeopleDS\n.\nshow\n()\n// +----+-------+\n// | age|   name|\n// +----+-------+\n// |null|Michael|\n// |  30|   Andy|\n// |  19| Justin|\n// +----+-------+\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\" in the Spark repo.\nimport\njava.util.Arrays\n;\nimport\njava.util.Collections\n;\nimport\njava.io.Serializable\n;\nimport\norg.apache.spark.api.java.function.MapFunction\n;\nimport\norg.apache.spark.sql", "question": "What is returned by primitiveDS.map(_ + 1).collect()?", "answers": {"text": ["Array(2, 3, 4)"], "answer_start": [140]}}
{"context": "ays\n;\nimport\njava.util.Collections\n;\nimport\njava.io.Serializable\n;\nimport\norg.apache.spark.api.java.function.MapFunction\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nimport\norg.apache.spark.sql.Encoder\n;\nimport\norg.apache.spark.sql.Encoders\n;\npublic\nstatic\nclass\nPerson\nimplements\nSerializable\n{\nprivate\nString\nname\n;\nprivate\nlong\nage\n;\npublic\nString\ngetName\n()\n{\nreturn\nname\n;\n}\npublic\nvoid\nsetName\n(\nString\nname\n)\n{\nthis\n.\nname\n=\nname\n;\n}\npublic\nlong\ngetAge\n()\n{\nreturn\nage\n;\n}\npublic\nvoid\nsetAge\n(\nlong\nage\n)\n{\nthis\n.\nage\n=\nage\n;\n}\n}\n// Create an instance of a Bean class\nPerson\nperson\n=\nnew\nPerson\n();\nperson\n.\nsetName\n(\n\"Andy\"\n);\nperson\n.\nsetAge\n(\n32\n);\n// Encoders are created for Java beans\nEncoder\n<\nPerson\n>\npersonEncoder\n=\nEncoders\n.\nbean\n(\nPerson\n.\nclass\n);\nDa", "question": "What is created for Java beans?", "answers": {"text": ["Encoders are created for Java beans"], "answer_start": [691]}}
{"context": "e\n(\n\"Andy\"\n);\nperson\n.\nsetAge\n(\n32\n);\n// Encoders are created for Java beans\nEncoder\n<\nPerson\n>\npersonEncoder\n=\nEncoders\n.\nbean\n(\nPerson\n.\nclass\n);\nDataset\n<\nPerson\n>\njavaBeanDS\n=\nspark\n.\ncreateDataset\n(\nCollections\n.\nsingletonList\n(\nperson\n),\npersonEncoder\n);\njavaBeanDS\n.\nshow\n();\n// +---+----+\n// |age|name|\n// +---+----+\n// | 32|Andy|\n// +---+----+\n// Encoders for most common types are provided in class Encoders\nEncoder\n<\nLong\n>\nlongEncoder\n=\nEncoders\n.\nLONG\n();\nDataset\n<\nLong\n>\nprimitiveDS\n=\nspark\n.\ncreateDataset\n(\nArrays\n.\nasList\n(\n1L\n,\n2L\n,\n3L\n),\nlongEncoder\n);\nDataset\n<\nLong\n>\ntransformedDS\n=\nprimitiveDS\n.\nmap\n(\n(\nMapFunction\n<\nLong\n,\nLong\n>)\nvalue\n->\nvalue\n+\n1L\n,\nlongEncoder\n);\ntransformedDS\n.\ncollect\n();\n// Returns [2, 3, 4]\n// DataFrames can be converted to a Dataset by providing ", "question": "What is returned by `transformedDS.collect()`?", "answers": {"text": ["// Returns [2, 3, 4]"], "answer_start": [722]}}
{"context": "ong\n>)\nvalue\n->\nvalue\n+\n1L\n,\nlongEncoder\n);\ntransformedDS\n.\ncollect\n();\n// Returns [2, 3, 4]\n// DataFrames can be converted to a Dataset by providing a class. Mapping based on name\nString\npath\n=\n\"examples/src/main/resources/people.json\"\n;\nDataset\n<\nPerson\n>\npeopleDS\n=\nspark\n.\nread\n().\njson\n(\npath\n).\nas\n(\npersonEncoder\n);\npeopleDS\n.\nshow\n();\n// +----+-------+\n// | age|   name|\n// +----+-------+\n// |null|Michael|\n// |  30|   Andy|\n// |  19| Justin|\n// +----+-------+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\" in the Spark repo.\nInteroperating with RDDs\nSpark SQL supports two different methods for converting existing RDDs into Datasets. The first\nmethod uses reflection to infer the schema of an RDD that contains specific types of o", "question": "Where can I find the full example code for the JavaSparkSQLExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\" in the Spark repo."], "answer_start": [469]}}
{"context": "t methods for converting existing RDDs into Datasets. The first\nmethod uses reflection to infer the schema of an RDD that contains specific types of objects. This\nreflection-based approach leads to more concise code and works well when you already know the schema\nwhile writing your Spark application.\nThe second method for creating Datasets is through a programmatic interface that allows you to\nconstruct a schema and then apply it to an existing RDD. While this method is more verbose, it allows\nyou to construct Datasets when the columns and their types are not known until runtime.\nInferring the Schema Using Reflection\nSpark SQL can convert an RDD of Row objects to a DataFrame, inferring the datatypes. Rows are constructed by passing a list of\nkey/value pairs as kwargs to the Row class. The ", "question": "What is one method Spark SQL uses to convert an RDD of Row objects to a DataFrame?", "answers": {"text": ["inferring the datatypes."], "answer_start": [685]}}
{"context": "RDD of Row objects to a DataFrame, inferring the datatypes. Rows are constructed by passing a list of\nkey/value pairs as kwargs to the Row class. The keys of this list define the column names of the table,\nand the types are inferred by sampling the whole dataset, similar to the inference that is performed on JSON files.\nfrom\npyspark.sql\nimport\nRow\nsc\n=\nspark\n.\nsparkContext\n# Load a text file and convert each line to a Row.\nlines\n=\nsc\n.\ntextFile\n(\n\"\nexamples/src/main/resources/people.txt\n\"\n)\nparts\n=\nlines\n.\nmap\n(\nlambda\nl\n:\nl\n.\nsplit\n(\n\"\n,\n\"\n))\npeople\n=\nparts\n.\nmap\n(\nlambda\np\n:\nRow\n(\nname\n=\np\n[\n0\n],\nage\n=\nint\n(\np\n[\n1\n])))\n# Infer the schema, and register the DataFrame as a table.\nschemaPeople\n=\nspark\n.\ncreateDataFrame\n(\npeople\n)\nschemaPeople\n.\ncreateOrReplaceTempView\n(\n\"\npeople\n\"\n)\n# SQL ca", "question": "Como as colunas de uma tabela são definidas ao construir Rows?", "answers": {"text": ["The keys of this list define the column names of the table"], "answer_start": [146]}}
{"context": "nd register the DataFrame as a table.\nschemaPeople\n=\nspark\n.\ncreateDataFrame\n(\npeople\n)\nschemaPeople\n.\ncreateOrReplaceTempView\n(\n\"\npeople\n\"\n)\n# SQL can be run over DataFrames that have been registered as a table.\nteenagers\n=\nspark\n.\nsql\n(\n\"\nSELECT name FROM people WHERE age >= 13 AND age <= 19\n\"\n)\n# The results of SQL queries are Dataframe objects.\n# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\nteenNames\n=\nteenagers\n.\nrdd\n.\nmap\n(\nlambda\np\n:\n\"\nName:\n\"\n+\np\n.\nname\n).\ncollect\n()\nfor\nname\nin\nteenNames\n:\nprint\n(\nname\n)\n# Name: Justin\nFind full example code at \"examples/src/main/python/sql/basic.py\" in the Spark repo.\nThe Scala interface for Spark SQL supports automatically converting an RDD containing case classes\nto a DataFrame. The case class\ndefines the schema of the ta", "question": "What does the code do with the results of the SQL query 'SELECT name FROM people WHERE age >= 13 AND age <= 19'?", "answers": {"text": ["The results of SQL queries are Dataframe objects."], "answer_start": [301]}}
{"context": "\n=\nspark\n.\nsparkContext\n.\ntextFile\n(\n\"examples/src/main/resources/people.txt\"\n)\n.\nmap\n(\n_\n.\nsplit\n(\n\",\"\n))\n.\nmap\n(\nattributes\n=>\nPerson\n(\nattributes\n(\n0\n),\nattributes\n(\n1\n).\ntrim\n.\ntoInt\n))\n.\ntoDF\n()\n// Register the DataFrame as a temporary view\npeopleDF\n.\ncreateOrReplaceTempView\n(\n\"people\"\n)\n// SQL statements can be run by using the sql methods provided by Spark\nval\nteenagersDF\n=\nspark\n.\nsql\n(\n\"SELECT name, age FROM people WHERE age BETWEEN 13 AND 19\"\n)\n// The columns of a row in the result can be accessed by field index\nteenagersDF\n.\nmap\n(\nteenager\n=>\n\"Name: \"\n+\nteenager\n(\n0\n)).\nshow\n()\n// +------------+\n// |       value|\n// +------------+\n// |Name: Justin|\n// +------------+\n// or by field name\nteenagersDF\n.\nmap\n(\nteenager\n=>\n\"Name: \"\n+\nteenager\n.\ngetAs\n[\nString\n](\n\"name\"\n)).\nshow\n()\n// ", "question": "How can the columns of a row in the result of `teenagersDF` be accessed?", "answers": {"text": ["The columns of a row in the result can be accessed by field index"], "answer_start": [462]}}
{"context": "\n.\nsetAge\n(\nInteger\n.\nparseInt\n(\nparts\n[\n1\n].\ntrim\n()));\nreturn\nperson\n;\n});\n// Apply a schema to an RDD of JavaBeans to get a DataFrame\nDataset\n<\nRow\n>\npeopleDF\n=\nspark\n.\ncreateDataFrame\n(\npeopleRDD\n,\nPerson\n.\nclass\n);\n// Register the DataFrame as a temporary view\npeopleDF\n.\ncreateOrReplaceTempView\n(\n\"people\"\n);\n// SQL statements can be run by using the sql methods provided by spark\nDataset\n<\nRow\n>\nteenagersDF\n=\nspark\n.\nsql\n(\n\"SELECT name FROM people WHERE age BETWEEN 13 AND 19\"\n);\n// The columns of a row in the result can be accessed by field index\nEncoder\n<\nString\n>\nstringEncoder\n=\nEncoders\n.\nSTRING\n();\nDataset\n<\nString\n>\nteenagerNamesByIndexDF\n=\nteenagersDF\n.\nmap\n(\n(\nMapFunction\n<\nRow\n,\nString\n>)\nrow\n->\n\"Name: \"\n+\nrow\n.\ngetString\n(\n0\n),\nstringEncoder\n);\nteenagerNamesByIndexDF\n.\nshow\n()", "question": "How are the columns of a row in the result accessed?", "answers": {"text": ["The columns of a row in the result can be accessed by field index"], "answer_start": [491]}}
{"context": "k.sql.types\nimport\nStringType\n,\nStructType\n,\nStructField\nsc\n=\nspark\n.\nsparkContext\n# Load a text file and convert each line to a Row.\nlines\n=\nsc\n.\ntextFile\n(\n\"\nexamples/src/main/resources/people.txt\n\"\n)\nparts\n=\nlines\n.\nmap\n(\nlambda\nl\n:\nl\n.\nsplit\n(\n\"\n,\n\"\n))\n# Each line is converted to a tuple.\npeople\n=\nparts\n.\nmap\n(\nlambda\np\n:\n(\np\n[\n0\n],\np\n[\n1\n].\nstrip\n()))\n# The schema is encoded in a string.\nschemaString\n=\n\"\nname age\n\"\nfields\n=\n[\nStructField\n(\nfield_name\n,\nStringType\n(),\nTrue\n)\nfor\nfield_name\nin\nschemaString\n.\nsplit\n()]\nschema\n=\nStructType\n(\nfields\n)\n# Apply the schema to the RDD.\nschemaPeople\n=\nspark\n.\ncreateDataFrame\n(\npeople\n,\nschema\n)\n# Creates a temporary view using the DataFrame\nschemaPeople\n.\ncreateOrReplaceTempView\n(\n\"\npeople\n\"\n)\n# SQL can be run over DataFrames that have been reg", "question": "What is done with the DataFrame `schemaPeople` after it's created?", "answers": {"text": ["Creates a temporary view using the DataFrame"], "answer_start": [650]}}
{"context": "of a row in the result can be accessed by field index or by field name\nDataset\n<\nString\n>\nnamesDS\n=\nresults\n.\nmap\n(\n(\nMapFunction\n<\nRow\n,\nString\n>)\nrow\n->\n\"Name: \"\n+\nrow\n.\ngetString\n(\n0\n),\nEncoders\n.\nSTRING\n());\nnamesDS\n.\nshow\n();\n// +-------------+\n// |        value|\n// +-------------+\n// |Name: Michael|\n// |   Name: Andy|\n// | Name: Justin|\n// +-------------+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\" in the Spark repo.\nScalar Functions\nScalar functions are functions that return a single value per row, as opposed to aggregation functions, which return a value for a group of rows. Spark SQL supports a variety of\nBuilt-in Scalar Functions\n. It also supports\nUser Defined Scalar Functions\n.\nAggregate Functions\nAggregate functions", "question": "How can a field in a result row be accessed?", "answers": {"text": ["of a row in the result can be accessed by field index or by field name"], "answer_start": [0]}}
{"context": ". Spark SQL supports a variety of\nBuilt-in Scalar Functions\n. It also supports\nUser Defined Scalar Functions\n.\nAggregate Functions\nAggregate functions are functions that return a single value on a group of rows. The\nBuilt-in Aggregate Functions\nprovide common aggregations such as\ncount()\n,\ncount_distinct()\n,\navg()\n,\nmax()\n,\nmin()\n, etc.\nUsers are not limited to the predefined aggregate functions and can create their own. For more details\nabout user defined aggregate functions, please refer to the documentation of\nUser Defined Aggregate Functions\n.", "question": "What do aggregate functions return?", "answers": {"text": ["a single value on a group of rows"], "answer_start": [177]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available for Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)"], "answer_start": [46]}}
{"context": "x\nstoring the left singular vectors $U$, is computed via matrix multiplication as\n$U = A (V S^{-1})$, if requested by the user via the computeU parameter. \nThe actual method to use is determined automatically based on the computational cost:\nIf $n$ is small ($n < 100$) or $k$ is large compared with $n$ ($k > n / 2$), we compute the Gramian matrix\nfirst and then compute its top eigenvalues and eigenvectors locally on the driver.\nThis requires a single pass with $O(n^2)$ storage on each executor and on the driver, and\n$O(n^2 k)$ time on the driver.\nOtherwise, we compute $(A^T A) v$ in a distributive way and send it to\nARPACK\nto\ncompute $(A^T A)$’s top eigenvalues and eigenvectors on the driver node. This requires $O(k)$\npasses, $O(n)$ storage on each executor, and $O(n k)$ storage on the dri", "question": "What is the storage requirement on each executor when computing the Gramian matrix?", "answers": {"text": ["$O(n^2)$ storage on each executor"], "answer_start": [465]}}
{"context": "$’s top eigenvalues and eigenvectors on the driver node. This requires $O(k)$\npasses, $O(n)$ storage on each executor, and $O(n k)$ storage on the driver.\nSVD Example\nspark.mllib\nprovides SVD functionality to row-oriented matrices, provided in the\nRowMatrix\nclass.\nRefer to the\nSingularValueDecomposition\nPython docs\nfor details on the API.\nfrom\npyspark.mllib.linalg\nimport\nVectors\nfrom\npyspark.mllib.linalg.distributed\nimport\nRowMatrix\nrows\n=\nsc\n.\nparallelize\n([\nVectors\n.\nsparse\n(\n5\n,\n{\n1\n:\n1.0\n,\n3\n:\n7.0\n}),\nVectors\n.\ndense\n(\n2.0\n,\n0.0\n,\n3.0\n,\n4.0\n,\n5.0\n),\nVectors\n.\ndense\n(\n4.0\n,\n0.0\n,\n0.0\n,\n6.0\n,\n7.0\n)\n])\nmat\n=\nRowMatrix\n(\nrows\n)\n# Compute the top 5 singular values and corresponding singular vectors.\nsvd\n=\nmat\n.\ncomputeSVD\n(\n5\n,\ncomputeU\n=\nTrue\n)\nU\n=\nsvd\n.\nU\n# The U factor is a RowMatrix.\ns\n", "question": "What class in spark.mllib provides SVD functionality to row-oriented matrices?", "answers": {"text": ["RowMatrix"], "answer_start": [248]}}
{"context": "top 5 singular values and corresponding singular vectors.\nsvd\n=\nmat\n.\ncomputeSVD\n(\n5\n,\ncomputeU\n=\nTrue\n)\nU\n=\nsvd\n.\nU\n# The U factor is a RowMatrix.\ns\n=\nsvd\n.\ns\n# The singular values are stored in a local dense vector.\nV\n=\nsvd\n.\nV\n# The V factor is a local dense matrix.\nFind full example code at \"examples/src/main/python/mllib/svd_example.py\" in the Spark repo.\nThe same code applies to\nIndexedRowMatrix\nif\nU\nis defined as an\nIndexedRowMatrix\n.\nRefer to the\nSingularValueDecomposition\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.linalg.Matrix\nimport\norg.apache.spark.mllib.linalg.SingularValueDecomposition\nimport\norg.apache.spark.mllib.linalg.Vector\nimport\norg.apache.spark.mllib.linalg.Vectors\nimport\norg.apache.spark.mllib.linalg.distributed.RowMatrix\nval\ndata\n=\nArray\n(\nVect", "question": "What is stored in svd.s?", "answers": {"text": ["The singular values are stored in a local dense vector."], "answer_start": [162]}}
{"context": "rk.mllib.linalg.Vector\nimport\norg.apache.spark.mllib.linalg.Vectors\nimport\norg.apache.spark.mllib.linalg.distributed.RowMatrix\nval\ndata\n=\nArray\n(\nVectors\n.\nsparse\n(\n5\n,\nSeq\n((\n1\n,\n1.0\n),\n(\n3\n,\n7.0\n))),\nVectors\n.\ndense\n(\n2.0\n,\n0.0\n,\n3.0\n,\n4.0\n,\n5.0\n),\nVectors\n.\ndense\n(\n4.0\n,\n0.0\n,\n0.0\n,\n6.0\n,\n7.0\n))\nval\nrows\n=\nsc\n.\nparallelize\n(\nimmutable\n.\nArraySeq\n.\nunsafeWrapArray\n(\ndata\n))\nval\nmat\n:\nRowMatrix\n=\nnew\nRowMatrix\n(\nrows\n)\n// Compute the top 5 singular values and corresponding singular vectors.\nval\nsvd\n:\nSingularValueDecomposition\n[\nRowMatrix\n,\nMatrix\n]\n=\nmat\n.\ncomputeSVD\n(\n5\n,\ncomputeU\n=\ntrue\n)\nval\nU\n:\nRowMatrix\n=\nsvd\n.\nU\n// The U factor is a RowMatrix.\nval\ns\n:\nVector\n=\nsvd\n.\ns\n// The singular values are stored in a local dense vector.\nval\nV\n:\nMatrix\n=\nsvd\n.\nV\n// The V factor is a local dens", "question": "What is stored in the 's' variable after computing the Singular Value Decomposition?", "answers": {"text": ["The singular values are stored in a local dense vector."], "answer_start": [688]}}
{"context": "owMatrix.\nval\ns\n:\nVector\n=\nsvd\n.\ns\n// The singular values are stored in a local dense vector.\nval\nV\n:\nMatrix\n=\nsvd\n.\nV\n// The V factor is a local dense matrix.\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\" in the Spark repo.\nThe same code applies to\nIndexedRowMatrix\nif\nU\nis defined as an\nIndexedRowMatrix\n.\nRefer to the\nSingularValueDecomposition\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.api.java.JavaSparkContext\n;\nimport\norg.apache.spark.mllib.linalg.Matrix\n;\nimport\norg.apache.spark.mllib.linalg.SingularValueDecomposition\n;\nimport\norg.apache.spark.mllib.linalg.Vector\n;\nimport\norg.apache.spark.mllib.linalg.Vectors\n;\nimport\norg.ap", "question": "Where can I find a full example code for SVD?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\" in the Spark repo."], "answer_start": [160]}}
{"context": "mposition\n<\nRowMatrix\n,\nMatrix\n>\nsvd\n=\nmat\n.\ncomputeSVD\n(\n5\n,\ntrue\n,\n1.0\nE\n-\n9\nd\n);\nRowMatrix\nU\n=\nsvd\n.\nU\n();\n// The U factor is a RowMatrix.\nVector\ns\n=\nsvd\n.\ns\n();\n// The singular values are stored in a local dense vector.\nMatrix\nV\n=\nsvd\n.\nV\n();\n// The V factor is a local dense matrix.\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\" in the Spark repo.\nThe same code applies to\nIndexedRowMatrix\nif\nU\nis defined as an\nIndexedRowMatrix\n.\nPrincipal component analysis (PCA)\nPrincipal component analysis (PCA)\nis a\nstatistical method to find a rotation such that the first coordinate has the largest variance\npossible, and each succeeding coordinate, in turn, has the largest variance possible. The columns of\nthe rotation matrix are called princi", "question": "Onde posso encontrar um exemplo de código completo para SVD?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\" in the Spark repo."], "answer_start": [288]}}
{"context": "st variance\npossible, and each succeeding coordinate, in turn, has the largest variance possible. The columns of\nthe rotation matrix are called principal components. PCA is used widely in dimensionality reduction.\nspark.mllib\nsupports PCA for tall-and-skinny matrices stored in row-oriented format and any Vectors.\nThe following code demonstrates how to compute principal components on a\nRowMatrix\nand use them to project the vectors into a low-dimensional space.\nRefer to the\nRowMatrix\nPython docs\nfor details on the API.\nfrom\npyspark.mllib.linalg\nimport\nVectors\nfrom\npyspark.mllib.linalg.distributed\nimport\nRowMatrix\nrows\n=\nsc\n.\nparallelize\n([\nVectors\n.\nsparse\n(\n5\n,\n{\n1\n:\n1.0\n,\n3\n:\n7.0\n}),\nVectors\n.\ndense\n(\n2.0\n,\n0.0\n,\n3.0\n,\n4.0\n,\n5.0\n),\nVectors\n.\ndense\n(\n4.0\n,\n0.0\n,\n0.0\n,\n6.0\n,\n7.0\n)\n])\nmat\n=\nR", "question": "What does PCA stand for?", "answers": {"text": ["PCA is used widely in dimensionality reduction."], "answer_start": [166]}}
{"context": " into a low-dimensional space.\nRefer to the\nRowMatrix\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.linalg.Matrix\nimport\norg.apache.spark.mllib.linalg.Vectors\nimport\norg.apache.spark.mllib.linalg.distributed.RowMatrix\nval\ndata\n=\nArray\n(\nVectors\n.\nsparse\n(\n5\n,\nSeq\n((\n1\n,\n1.0\n),\n(\n3\n,\n7.0\n))),\nVectors\n.\ndense\n(\n2.0\n,\n0.0\n,\n3.0\n,\n4.0\n,\n5.0\n),\nVectors\n.\ndense\n(\n4.0\n,\n0.0\n,\n0.0\n,\n6.0\n,\n7.0\n))\nval\nrows\n=\nsc\n.\nparallelize\n(\nimmutable\n.\nArraySeq\n.\nunsafeWrapArray\n(\ndata\n))\nval\nmat\n:\nRowMatrix\n=\nnew\nRowMatrix\n(\nrows\n)\n// Compute the top 4 principal components.\n// Principal components are stored in a local dense matrix.\nval\npc\n:\nMatrix\n=\nmat\n.\ncomputePrincipalComponents\n(\n4\n)\n// Project the rows to the linear space spanned by the top 4 principal components.\nval\nprojected\n:\nRowMatr", "question": "What is computed using mat.computePrincipalComponents(4)?", "answers": {"text": ["Compute the top 4 principal components."], "answer_start": [537]}}
{"context": "ix\n=\nmat\n.\ncomputePrincipalComponents\n(\n4\n)\n// Project the rows to the linear space spanned by the top 4 principal components.\nval\nprojected\n:\nRowMatrix\n=\nmat\n.\nmultiply\n(\npc\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\" in the Spark repo.\nThe following code demonstrates how to compute principal components on source vectors\nand use them to project the vectors into a low-dimensional space while keeping associated labels:\nRefer to the\nPCA\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.feature.PCA\nimport\norg.apache.spark.mllib.linalg.Vectors\nimport\norg.apache.spark.mllib.regression.LabeledPoint\nimport\norg.apache.spark.rdd.RDD\nval\ndata\n:\nRDD\n[\nLabeledPoint\n]\n=\nsc\n.\nparallelize\n(\nSeq\n(\nnew\nLabeledPoint\n(\n0\n,\nV", "question": "Where can I find a full example code for PCA on RowMatrix?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\" in the Spark repo."], "answer_start": [177]}}
{"context": "park.mllib.regression.LabeledPoint\nimport\norg.apache.spark.rdd.RDD\nval\ndata\n:\nRDD\n[\nLabeledPoint\n]\n=\nsc\n.\nparallelize\n(\nSeq\n(\nnew\nLabeledPoint\n(\n0\n,\nVectors\n.\ndense\n(\n1\n,\n0\n,\n0\n,\n0\n,\n1\n)),\nnew\nLabeledPoint\n(\n1\n,\nVectors\n.\ndense\n(\n1\n,\n1\n,\n0\n,\n1\n,\n0\n)),\nnew\nLabeledPoint\n(\n1\n,\nVectors\n.\ndense\n(\n1\n,\n1\n,\n0\n,\n0\n,\n0\n)),\nnew\nLabeledPoint\n(\n0\n,\nVectors\n.\ndense\n(\n1\n,\n0\n,\n0\n,\n0\n,\n0\n)),\nnew\nLabeledPoint\n(\n1\n,\nVectors\n.\ndense\n(\n1\n,\n1\n,\n0\n,\n0\n,\n0\n))))\n// Compute the top 5 principal components.\nval\npca\n=\nnew\nPCA\n(\n5\n).\nfit\n(\ndata\n.\nmap\n(\n_\n.\nfeatures\n))\n// Project vectors to the linear space spanned by the top 5 principal\n// components, keeping the label\nval\nprojected\n=\ndata\n.\nmap\n(\np\n=>\np\n.\ncopy\n(\nfeatures\n=\npca\n.\ntransform\n(\np\n.\nfeatures\n)))\nFind full example code at \"examples/src/main/scala/org/apache", "question": "What is the number of principal components computed in the provided code?", "answers": {"text": ["5"], "answer_start": [461]}}
{"context": "l\nprojected\n=\ndata\n.\nmap\n(\np\n=>\np\n.\ncopy\n(\nfeatures\n=\npca\n.\ntransform\n(\np\n.\nfeatures\n)))\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\" in the Spark repo.\nThe following code demonstrates how to compute principal components on a\nRowMatrix\nand use them to project the vectors into a low-dimensional space.\nRefer to the\nRowMatrix\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.api.java.JavaSparkContext\n;\nimport\norg.apache.spark.mllib.linalg.Matrix\n;\nimport\norg.apache.spark.mllib.linalg.Vector\n;\nimport\norg.apache.spark.mllib.linalg.Vectors\n;\nimport\norg.apache.spark.mllib.linalg.distributed.RowMatrix\n;\nList\n<\nVector\n>\ndata\n=\nArr", "question": "Where can I find a full example code for PCA?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\" in the Spark repo."], "answer_start": [89]}}
{"context": "linalg.Vector\n;\nimport\norg.apache.spark.mllib.linalg.Vectors\n;\nimport\norg.apache.spark.mllib.linalg.distributed.RowMatrix\n;\nList\n<\nVector\n>\ndata\n=\nArrays\n.\nasList\n(\nVectors\n.\nsparse\n(\n5\n,\nnew\nint\n[]\n{\n1\n,\n3\n},\nnew\ndouble\n[]\n{\n1.0\n,\n7.0\n}),\nVectors\n.\ndense\n(\n2.0\n,\n0.0\n,\n3.0\n,\n4.0\n,\n5.0\n),\nVectors\n.\ndense\n(\n4.0\n,\n0.0\n,\n0.0\n,\n6.0\n,\n7.0\n)\n);\nJavaRDD\n<\nVector\n>\nrows\n=\njsc\n.\nparallelize\n(\ndata\n);\n// Create a RowMatrix from JavaRDD<Vector>.\nRowMatrix\nmat\n=\nnew\nRowMatrix\n(\nrows\n.\nrdd\n());\n// Compute the top 4 principal components.\n// Principal components are stored in a local dense matrix.\nMatrix\npc\n=\nmat\n.\ncomputePrincipalComponents\n(\n4\n);\n// Project the rows to the linear space spanned by the top 4 principal components.\nRowMatrix\nprojected\n=\nmat\n.\nmultiply\n(\npc\n);\nFind full example code at \"exam", "question": "How many principal components are computed in the example code?", "answers": {"text": ["4"], "answer_start": [276]}}
{"context": "t the rows to the linear space spanned by the top 4 principal components.\nRowMatrix\nprojected\n=\nmat\n.\nmultiply\n(\npc\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\" in the Spark repo.", "question": "Where can I find a full example code for the described process?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\" in the Spark repo."], "answer_start": [119]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTrou", "question": "Which file formats are mentioned as data sources in the Spark SQL Guide?", "answers": {"text": ["Parquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files"], "answer_start": [650]}}
{"context": "Parquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTroubleshooting\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nGeneric Load/Save Functions\nManually Specifying Options\nRun SQL on files directly\nSave Modes\nSaving to Persistent Tables\nBucketing, Sorting and Partitioning\nIn the simplest form, the default data source (\nparquet\nunless otherwise configured by\nspark.sql.sources.default\n) will be used for all operations.\nusers_df\n=\nspark\n.\nread\n.\nload\n(\n\"\nexamples/src/main/resources/users.parquet\n\"\n)\nusers_df\n.\nselect\n(\n\"\nname\n\"\n,\n\"\nfavorite_color\n\"\n).\nwrite\n.\nsave\n(\n\"\nnamesAndFavColors.parquet\n\"\n)\nFind full exam", "question": "What is the default data source used for all operations unless otherwise configured?", "answers": {"text": ["parquet"], "answer_start": [505]}}
{"context": "rc/main/resources/users.parquet\n\"\n)\nusers_df\n.\nselect\n(\n\"\nname\n\"\n,\n\"\nfavorite_color\n\"\n).\nwrite\n.\nsave\n(\n\"\nnamesAndFavColors.parquet\n\"\n)\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\nval\nusersDF\n=\nspark\n.\nread\n.\nload\n(\n\"examples/src/main/resources/users.parquet\"\n)\nusersDF\n.\nselect\n(\n\"name\"\n,\n\"favorite_color\"\n).\nwrite\n.\nsave\n(\n\"namesAndFavColors.parquet\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nDataset\n<\nRow\n>\nusersDF\n=\nspark\n.\nread\n().\nload\n(\n\"examples/src/main/resources/users.parquet\"\n);\nusersDF\n.\nselect\n(\n\"name\"\n,\n\"favorite_color\"\n).\nwrite\n().\nsave\n(\n\"namesAndFavColors.parquet\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sq", "question": "Where can I find the full example code for the Scala implementation?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo."], "answer_start": [401]}}
{"context": "_df\n=\nspark\n.\nread\n.\nload\n(\n\"\nexamples/src/main/resources/people.json\n\"\n,\nformat\n=\n\"\njson\n\"\n)\npeople_df\n.\nselect\n(\n\"\nname\n\"\n,\n\"\nage\n\"\n).\nwrite\n.\nsave\n(\n\"\nnamesAndAges.parquet\n\"\n,\nformat\n=\n\"\nparquet\n\"\n)\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\nval\npeopleDF\n=\nspark\n.\nread\n.\nformat\n(\n\"json\"\n).\nload\n(\n\"examples/src/main/resources/people.json\"\n)\npeopleDF\n.\nselect\n(\n\"name\"\n,\n\"age\"\n).\nwrite\n.\nformat\n(\n\"parquet\"\n).\nsave\n(\n\"namesAndAges.parquet\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nDataset\n<\nRow\n>\npeopleDF\n=\nspark\n.\nread\n().\nformat\n(\n\"json\"\n).\nload\n(\n\"examples/src/main/resources/people.json\"\n);\npeopleDF\n.\nselect\n(\n\"name\"\n,\n\"age\"\n).\nwrite\n().\nformat\n(\n\"parquet", "question": "Where can I find the full example code for the Scala implementation?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo."], "answer_start": [492]}}
{"context": "rk\n.\nread\n().\nformat\n(\n\"json\"\n).\nload\n(\n\"examples/src/main/resources/people.json\"\n);\npeopleDF\n.\nselect\n(\n\"name\"\n,\n\"age\"\n).\nwrite\n().\nformat\n(\n\"parquet\"\n).\nsave\n(\n\"namesAndAges.parquet\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\ndf\n<-\nread.df\n(\n\"examples/src/main/resources/people.json\"\n,\n\"json\"\n)\nnamesAndAges\n<-\nselect\n(\ndf\n,\n\"name\"\n,\n\"age\"\n)\nwrite.df\n(\nnamesAndAges\n,\n\"namesAndAges.parquet\"\n,\n\"parquet\"\n)\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nTo load a CSV file you can use:\npeople_df\n=\nspark\n.\nread\n.\nload\n(\n\"\nexamples/src/main/resources/people.csv\n\"\n,\nformat\n=\n\"\ncsv\n\"\n,\nsep\n=\n\"\n;\n\"\n,\ninferSchema\n=\n\"\ntrue\n\"\n,\nheader\n=\n\"\ntrue\n\"\n)\nFind full example code at \"e", "question": "What format is used to save the 'namesAndAges' dataframe?", "answers": {"text": ["\"parquet\""], "answer_start": [142]}}
{"context": "\nexamples/src/main/resources/people.csv\n\"\n,\nformat\n=\n\"\ncsv\n\"\n,\nsep\n=\n\"\n;\n\"\n,\ninferSchema\n=\n\"\ntrue\n\"\n,\nheader\n=\n\"\ntrue\n\"\n)\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\nval\npeopleDFCsv\n=\nspark\n.\nread\n.\nformat\n(\n\"csv\"\n)\n.\noption\n(\n\"sep\"\n,\n\";\"\n)\n.\noption\n(\n\"inferSchema\"\n,\n\"true\"\n)\n.\noption\n(\n\"header\"\n,\n\"true\"\n)\n.\nload\n(\n\"examples/src/main/resources/people.csv\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nDataset\n<\nRow\n>\npeopleDFCsv\n=\nspark\n.\nread\n().\nformat\n(\n\"csv\"\n)\n.\noption\n(\n\"sep\"\n,\n\";\"\n)\n.\noption\n(\n\"inferSchema\"\n,\n\"true\"\n)\n.\noption\n(\n\"header\"\n,\n\"true\"\n)\n.\nload\n(\n\"examples/src/main/resources/people.csv\"\n);\nFind full example code at \"examples/src/main/java/org/apa", "question": "What is the file path used to load the CSV data?", "answers": {"text": ["examples/src/main/resources/people.csv"], "answer_start": [1]}}
{"context": "\"true\"\n)\n.\noption\n(\n\"header\"\n,\n\"true\"\n)\n.\nload\n(\n\"examples/src/main/resources/people.csv\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\ndf\n<-\nread.df\n(\n\"examples/src/main/resources/people.csv\"\n,\n\"csv\"\n,\nsep\n=\n\";\"\n,\ninferSchema\n=\nTRUE\n,\nheader\n=\nTRUE\n)\nnamesAndAges\n<-\nselect\n(\ndf\n,\n\"name\"\n,\n\"age\"\n)\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nThe extra options are also used during write operation.\nFor example, you can control bloom filters and dictionary encodings for ORC data sources.\nThe following ORC example will create bloom filter and use dictionary encoding only for\nfavorite_color\n.\nFor Parquet, there exists\nparquet.bloom.filter.enabled\nand\nparquet.enable.dic", "question": "Where can you find the full example code for the JavaSQLDataSourceExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo."], "answer_start": [93]}}
{"context": "reate bloom filter and use dictionary encoding only for\nfavorite_color\n.\nFor Parquet, there exists\nparquet.bloom.filter.enabled\nand\nparquet.enable.dictionary\n, too.\nTo find more detailed information about the extra ORC/Parquet options,\nvisit the official Apache\nORC\n/\nParquet\nwebsites.\nORC data source:\nusers_df\n=\nspark\n.\nread\n.\norc\n(\n\"\nexamples/src/main/resources/users.orc\n\"\n)\n(\nusers_df\n.\nwrite\n.\nformat\n(\n\"\norc\n\"\n)\n.\noption\n(\n\"\norc.bloom.filter.columns\n\"\n,\n\"\nfavorite_color\n\"\n)\n.\noption\n(\n\"\norc.dictionary.key.threshold\n\"\n,\n\"\n1.0\n\"\n)\n.\noption\n(\n\"\norc.column.encoding.direct\n\"\n,\n\"\nname\n\"\n)\n.\nsave\n(\n\"\nusers_with_options.orc\n\"\n))\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\nusersDF\n.\nwrite\n.\nformat\n(\n\"orc\"\n)\n.\noption\n(\n\"orc.bloom.filter.columns\"\n,\n\"fa", "question": "Which column is used for bloom filter in the ORC data source example?", "answers": {"text": ["favorite_color"], "answer_start": [56]}}
{"context": "code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\nusersDF\n.\nwrite\n.\nformat\n(\n\"orc\"\n)\n.\noption\n(\n\"orc.bloom.filter.columns\"\n,\n\"favorite_color\"\n)\n.\noption\n(\n\"orc.dictionary.key.threshold\"\n,\n\"1.0\"\n)\n.\noption\n(\n\"orc.column.encoding.direct\"\n,\n\"name\"\n)\n.\nsave\n(\n\"users_with_options.orc\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nusersDF\n.\nwrite\n().\nformat\n(\n\"orc\"\n)\n.\noption\n(\n\"orc.bloom.filter.columns\"\n,\n\"favorite_color\"\n)\n.\noption\n(\n\"orc.dictionary.key.threshold\"\n,\n\"1.0\"\n)\n.\noption\n(\n\"orc.column.encoding.direct\"\n,\n\"name\"\n)\n.\nsave\n(\n\"users_with_options.orc\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\nd", "question": "What format is used when writing the usersDF?", "answers": {"text": ["\"orc\""], "answer_start": [99]}}
{"context": "th_options.orc\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\ndf\n<-\nread.df\n(\n\"examples/src/main/resources/users.orc\"\n,\n\"orc\"\n)\nwrite.orc\n(\ndf\n,\n\"users_with_options.orc\"\n,\norc.bloom.filter.columns\n=\n\"favorite_color\"\n,\norc.dictionary.key.threshold\n=\n1.0\n,\norc.column.encoding.direct\n=\n\"name\"\n)\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nCREATE\nTABLE\nusers_with_options\n(\nname\nSTRING\n,\nfavorite_color\nSTRING\n,\nfavorite_numbers\narray\n<\ninteger\n>\n)\nUSING\nORC\nOPTIONS\n(\norc\n.\nbloom\n.\nfilter\n.\ncolumns\n'favorite_color'\n,\norc\n.\ndictionary\n.\nkey\n.\nthreshold\n'1.0'\n,\norc\n.\ncolumn\n.\nencoding\n.\ndirect\n'name'\n)\nParquet data source:\nusers_df\n=\nspark\n.\nread\n.\nparquet\n(\n\"\nexamples/sr", "question": "Which column is specified for bloom filter in the ORC write options?", "answers": {"text": ["favorite_color"], "answer_start": [287]}}
{"context": "\ndictionary\n.\nkey\n.\nthreshold\n'1.0'\n,\norc\n.\ncolumn\n.\nencoding\n.\ndirect\n'name'\n)\nParquet data source:\nusers_df\n=\nspark\n.\nread\n.\nparquet\n(\n\"\nexamples/src/main/resources/users.parquet\n\"\n)\n(\nusers_df\n.\nwrite\n.\nformat\n(\n\"\nparquet\n\"\n)\n.\noption\n(\n\"\nparquet.bloom.filter.enabled#favorite_color\n\"\n,\n\"\ntrue\n\"\n)\n.\noption\n(\n\"\nparquet.bloom.filter.expected.ndv#favorite_color\n\"\n,\n\"\n1000000\n\"\n)\n.\noption\n(\n\"\nparquet.enable.dictionary\n\"\n,\n\"\ntrue\n\"\n)\n.\noption\n(\n\"\nparquet.page.write-checksum.enabled\n\"\n,\n\"\nfalse\n\"\n)\n.\nsave\n(\n\"\nusers_with_options.parquet\n\"\n))\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\nusersDF\n.\nwrite\n.\nformat\n(\n\"parquet\"\n)\n.\noption\n(\n\"parquet.bloom.filter.enabled#favorite_color\"\n,\n\"true\"\n)\n.\noption\n(\n\"parquet.bloom.filter.expected.ndv#favorite_color", "question": "What format is used when writing the users_df DataFrame?", "answers": {"text": ["parquet"], "answer_start": [127]}}
{"context": "\nformat\n(\n\"parquet\"\n)\n.\noption\n(\n\"parquet.bloom.filter.enabled#favorite_color\"\n,\n\"true\"\n)\n.\noption\n(\n\"parquet.bloom.filter.expected.ndv#favorite_color\"\n,\n\"1000000\"\n)\n.\noption\n(\n\"parquet.enable.dictionary\"\n,\n\"true\"\n)\n.\noption\n(\n\"parquet.page.write-checksum.enabled\"\n,\n\"false\"\n)\n.\nsave\n(\n\"users_with_options.parquet\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nusersDF\n.\nwrite\n().\nformat\n(\n\"parquet\"\n)\n.\noption\n(\n\"parquet.bloom.filter.enabled#favorite_color\"\n,\n\"true\"\n)\n.\noption\n(\n\"parquet.bloom.filter.expected.ndv#favorite_color\"\n,\n\"1000000\"\n)\n.\noption\n(\n\"parquet.enable.dictionary\"\n,\n\"true\"\n)\n.\noption\n(\n\"parquet.page.write-checksum.enabled\"\n,\n\"false\"\n)\n.\nsave\n(\n\"users_with_options.parquet\"\n);\nFind full example ", "question": "What file format is being used to save the data?", "answers": {"text": ["\"parquet\""], "answer_start": [10]}}
{"context": "ce.py\" in the Spark repo.\nval\nsqlDF\n=\nspark\n.\nsql\n(\n\"SELECT * FROM parquet.`examples/src/main/resources/users.parquet`\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nDataset\n<\nRow\n>\nsqlDF\n=\nspark\n.\nsql\n(\n\"SELECT * FROM parquet.`examples/src/main/resources/users.parquet`\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\ndf\n<-\nsql\n(\n\"SELECT * FROM parquet.`examples/src/main/resources/users.parquet`\"\n)\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nSELECT\n*\nFROM\nparquet\n.\n`examples/src/main/resources/users.parquet`\nSave Modes\nSave operations can optionally take a\nSaveMode\n, that specifies ", "question": "Where can I find the full example code for Scala?", "answers": {"text": ["examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala"], "answer_start": [149]}}
{"context": "o persistent tables:\npeople_df\n=\nspark\n.\nread\n.\njson\n(\n\"\nexamples/src/main/resources/people.json\n\"\n)\npeople_df\n.\nwrite\n.\nbucketBy\n(\n42\n,\n\"\nname\n\"\n).\nsortBy\n(\n\"\nage\n\"\n).\nsaveAsTable\n(\n\"\npeople_bucketed\n\"\n)\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\npeopleDF\n.\nwrite\n.\nbucketBy\n(\n42\n,\n\"name\"\n).\nsortBy\n(\n\"age\"\n).\nsaveAsTable\n(\n\"people_bucketed\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\npeopleDF\n.\nwrite\n().\nbucketBy\n(\n42\n,\n\"name\"\n).\nsortBy\n(\n\"age\"\n).\nsaveAsTable\n(\n\"people_bucketed\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\nCREATE\nTABLE\npeople_bucketed\nUSING\njson\nCLUSTERED\n", "question": "Where can I find the full example code for the provided Spark examples?", "answers": {"text": ["Find full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo."], "answer_start": [205]}}
{"context": "mples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\nCREATE\nTABLE\npeople_bucketed\nUSING\njson\nCLUSTERED\nBY\n(\nname\n)\nINTO\n42\nBUCKETS\nAS\nSELECT\n*\nFROM\njson\n.\n`examples/src/main/resources/people.json`\n;\nwhile partitioning can be used with both\nsave\nand\nsaveAsTable\nwhen using the Dataset APIs.\nusers_df\n=\nspark\n.\nread\n.\nload\n(\n\"\nexamples/src/main/resources/users.parquet\n\"\n)\nusers_df\n.\nwrite\n.\npartitionBy\n(\n\"\nfavorite_color\n\"\n).\nformat\n(\n\"\nparquet\n\"\n).\nsave\n(\n\"\nnamesPartByColor.parquet\n\"\n)\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\nusersDF\n.\nwrite\n.\npartitionBy\n(\n\"favorite_color\"\n).\nformat\n(\n\"parquet\"\n).\nsave\n(\n\"namesPartByColor.parquet\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spa", "question": "Where can the full example code for the datasource be found?", "answers": {"text": ["Find full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo."], "answer_start": [535]}}
{"context": "onBy\n(\n\"favorite_color\"\n).\nformat\n(\n\"parquet\"\n).\nsave\n(\n\"namesPartByColor.parquet\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nusersDF\n.\nwrite\n()\n.\npartitionBy\n(\n\"favorite_color\"\n)\n.\nformat\n(\n\"parquet\"\n)\n.\nsave\n(\n\"namesPartByColor.parquet\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\nCREATE\nTABLE\nusers_by_favorite_color\nUSING\nparquet\nPARTITIONED\nBY\n(\nfavorite_color\n)\nAS\nSELECT\n*\nFROM\nparquet\n.\n`examples/src/main/resources/users.parquet`\n;\nIt is possible to use both partitioning and bucketing for a single table:\nusers_df\n=\nspark\n.\nread\n.\nparquet\n(\n\"\nexamples/src/main/resources/users.parquet\n\"\n)\n(\nusers_df\n.\nwrite\n.\npart", "question": "Where can I find the full example code for Scala?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo."], "answer_start": [85]}}
{"context": "ixed number of buckets and can be used when the number of unique values is unbounded.", "question": "What is a characteristic of a count-min sketch regarding the number of unique values it can handle?", "answers": {"text": ["can be used when the number of unique values is unbounded."], "answer_start": [27]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nStructured Streaming Programming Guide\nOverview\nGetting Started\nQuick Example\nProgramming Model\nAPIs on DataFrames and Datasets\nPerformance Tips\nAdditional Information\nStructured Streaming Programming Guide\nQuick Example\nLet’s say you want to maintain", "question": "What topics are covered in the Programming Guides section?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)"], "answer_start": [46]}}
{"context": "on DataFrames and Datasets\nPerformance Tips\nAdditional Information\nStructured Streaming Programming Guide\nQuick Example\nLet’s say you want to maintain a running word count of text data received from a data server listening on a TCP socket. Let’s see how you can express this using Structured Streaming. You can see the full code in\nPython\n/\nScala\n/\nJava\n/\nR\n.\nAnd if you\ndownload Spark\n, you can directly\nrun the example\n. In any case, let’s walk through the example step-by-step and understand how it works. First, we have to import the necessary classes and create a local SparkSession, the starting point of all functionalities related to Spark.\nfrom\npyspark.sql\nimport\nSparkSession\nfrom\npyspark.sql.functions\nimport\nexplode\nfrom\npyspark.sql.functions\nimport\nsplit\nspark\n=\nSparkSession\n\\\n.\nbuilder", "question": "What is the starting point of all functionalities related to Spark?", "answers": {"text": ["a local SparkSession, the starting point of all functionalities related to Spark."], "answer_start": [567]}}
{"context": "ord\n\"\n)\n)\n# Generate running word count\nwordCounts\n=\nwords\n.\ngroupBy\n(\n\"\nword\n\"\n).\ncount\n()\nThis\nlines\nDataFrame represents an unbounded table containing the streaming text data. This table contains one column of strings named “value”, and each line in the streaming text data becomes a row in the table. Note, that this is not currently receiving any data as we are just setting up the transformation, and have not yet started it. Next, we have used two built-in SQL functions - split and explode, to split each line into multiple rows with a word each. In addition, we use the function\nalias\nto name the new column as “word”. Finally, we have defined the\nwordCounts\nDataFrame by grouping by the unique values in the Dataset and counting them. Note that this is a streaming DataFrame which represent", "question": "What is the name of the column in the DataFrame representing the streaming text data?", "answers": {"text": ["“value”"], "answer_start": [227]}}
{"context": "Note, that this is not currently receiving any data as we are just setting up the transformation, and have not yet started it. Next, we have a SQL expression with two SQL functions - split and explode, to split each line into multiple rows with a word each. In addition, we name the new column as “word”. Finally, we have defined the\nwordCounts\nSparkDataFrame by grouping by the unique values in the SparkDataFrame and counting them. Note that this is a streaming SparkDataFrame which represents the running word counts of the stream.\nWe have now set up the query on the streaming data. All that is left is to actually start receiving data and computing the counts. To do this, we set it up to print the complete set of counts (specified by\noutputMode(\"complete\")\n) to the console every time they are", "question": "What is the purpose of using the SQL functions split and explode?", "answers": {"text": ["to split each line into multiple rows with a word each"], "answer_start": [202]}}
{"context": "ing the counts. To do this, we set it up to print the complete set of counts (specified by\noutputMode(\"complete\")\n) to the console every time they are updated. And then start the streaming computation using\nstart()\n.\n# Start running the query that prints the running counts to the console\nquery\n=\nwordCounts\n\\\n.\nwriteStream\n\\\n.\noutputMode\n(\n\"\ncomplete\n\"\n)\n\\\n.\nformat\n(\n\"\nconsole\n\"\n)\n\\\n.\nstart\n()\nquery\n.\nawaitTermination\n()\n// Start running the query that prints the running counts to the console\nval\nquery\n=\nwordCounts\n.\nwriteStream\n.\noutputMode\n(\n\"complete\"\n)\n.\nformat\n(\n\"console\"\n)\n.\nstart\n()\nquery\n.\nawaitTermination\n()\n// Start running the query that prints the running counts to the console\nStreamingQuery\nquery\n=\nwordCounts\n.\nwriteStream\n()\n.\noutputMode\n(\n\"complete\"\n)\n.\nformat\n(\n\"console\"\n)\n.", "question": "What is used to print the complete set of counts to the console?", "answers": {"text": ["outputMode(\"complete\")"], "answer_start": [91]}}
{"context": " that prints the running counts to the console\nStreamingQuery\nquery\n=\nwordCounts\n.\nwriteStream\n()\n.\noutputMode\n(\n\"complete\"\n)\n.\nformat\n(\n\"console\"\n)\n.\nstart\n();\nquery\n.\nawaitTermination\n();\n# Start running the query that prints the running counts to the console\nquery\n<-\nwrite.stream\n(\nwordCounts\n,\n\"console\"\n,\noutputMode\n=\n\"complete\"\n)\nawaitTermination\n(\nquery\n)\nAfter this code is executed, the streaming computation will have started in the background. The\nquery\nobject is a handle to that active streaming query, and we have decided to wait for the termination of the query using\nawaitTermination()\nto prevent the process from exiting while the query is active.\nTo actually execute this example code, you can either compile the code in your own\nSpark application\n, or simply\nrun the example\nonce ", "question": "What does the `awaitTermination()` function do?", "answers": {"text": ["to prevent the process from exiting while the query is active."], "answer_start": [603]}}
{"context": "-----------------\n+------+-----+\n| value|count|\n+------+-----+\n|apache|    1|\n| spark|    1|\n+------+-----+\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+------+-----+\n| value|count|\n+------+-----+\n|apache|    2|\n| spark|    1|\n|hadoop|    1|\n+------+-----+\n...\n# TERMINAL 2: RUNNING StructuredNetworkWordCount\n$\n./bin/run-example org.apache.spark.examples.sql.streaming.StructuredNetworkWordCount localhost 9999\n-------------------------------------------\nBatch: 0\n-------------------------------------------\n+------+-----+\n| value|count|\n+------+-----+\n|apache|    1|\n| spark|    1|\n+------+-----+\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+------+-----+\n| value|count|\n+------+-----+\n|apach", "question": "What is the count of 'apache' in Batch 1?", "answers": {"text": ["apache|    2|"], "answer_start": [251]}}
{"context": "+\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+------+-----+\n| value|count|\n+------+-----+\n|apache|    2|\n| spark|    1|\n|hadoop|    1|\n+------+-----+\n...\n# TERMINAL 2: RUNNING JavaStructuredNetworkWordCount\n$\n./bin/run-example org.apache.spark.examples.sql.streaming.JavaStructuredNetworkWordCount localhost 9999\n-------------------------------------------\nBatch: 0\n-------------------------------------------\n+------+-----+\n| value|count|\n+------+-----+\n|apache|    1|\n| spark|    1|\n+------+-----+\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+------+-----+\n| value|count|\n+------+-----+\n|apache|    2|\n| spark|    1|\n|hadoop|    1|\n+------+-----+\n...\n# TERMINAL 2: RUNNING structured_network", "question": "What command is used to run the JavaStructuredNetworkWordCount example?", "answers": {"text": ["./bin/run-example org.apache.spark.examples.sql.streaming.JavaStructuredNetworkWordCount localhost 9999"], "answer_start": [263]}}
{"context": "Result Table”. Every trigger interval (say, every 1 second), new rows get appended to the Input Table, which eventually updates the Result Table. Whenever the result table gets updated, we would want to write the changed result rows to an external sink.\nThe “Output” is defined as what gets written out to the external storage. The output can be defined in a different mode:\nComplete Mode\n- The entire updated Result Table will be written to the external storage. It is up to the storage connector to decide how to handle writing of the entire table.\nAppend Mode\n- Only the new rows appended in the Result Table since the last trigger will be written to the external storage. This is applicable only on the queries where existing rows in the Result Table are not expected to change.\nUpdate Mode\n- Onl", "question": "What happens in Complete Mode regarding the Result Table?", "answers": {"text": ["The entire updated Result Table will be written to the external storage."], "answer_start": [391]}}
{"context": " to the external storage. This is applicable only on the queries where existing rows in the Result Table are not expected to change.\nUpdate Mode\n- Only the rows that were updated in the Result Table since the last trigger will be written to the external storage (available since Spark 2.1.1). Note that this is different from the Complete Mode in that this mode only outputs the rows that have changed since the last trigger. If the query doesn’t contain aggregations, it will be equivalent to Append mode.\nNote that each mode is applicable on certain types of queries. This is discussed in detail\nlater\n.\nTo illustrate the use of this model, let’s understand the model in context of\nthe\nQuick Example\nabove. The first\nlines\nDataFrame is the input table, and\nthe final\nwordCounts\nDataFrame is the res", "question": "What is the key difference between Update Mode and Complete Mode?", "answers": {"text": ["Note that this is different from the Complete Mode in that this mode only outputs the rows that have changed since the last trigger."], "answer_start": [293]}}
{"context": "relieving the users from reasoning about it. As an example, let’s\nsee how this model handles event-time based processing and late arriving data.\nHandling Event-time and Late Data\nEvent-time is the time embedded in the data itself. For many applications, you may want to operate on this event-time. For example, if you want to get the number of events generated by IoT devices every minute, then you probably want to use the time when the data was generated (that is, event-time in the data), rather than the time Spark receives them. This event-time is very naturally expressed in this model – each event from the devices is a row in the table, and event-time is a column value in the row. This allows window-based aggregations (e.g. number of events every minute) to be just a special type of groupi", "question": "What time should you use to get the number of events generated by IoT devices every minute?", "answers": {"text": ["you probably want to use the time when the data was generated (that is, event-time in the data), rather than the time Spark receives them."], "answer_start": [395]}}
{"context": "vent-time is a column value in the row. This allows window-based aggregations (e.g. number of events every minute) to be just a special type of grouping and aggregation on the event-time column – each time window is a group and each row can belong to multiple windows/groups. Therefore, such event-time-window-based aggregation queries can be defined consistently on both a static dataset (e.g. from collected device events logs) as well as on a data stream, making the life of the user much easier.\nFurthermore, this model naturally handles data that has arrived later than\nexpected based on its event-time. Since Spark is updating the Result Table,\nit has full control over updating old aggregates when there is late data,\nas well as cleaning up old aggregates to limit the size of intermediate\nsta", "question": "What does the event-time model allow for in terms of aggregation queries?", "answers": {"text": ["window-based aggregations (e.g. number of events every minute) to be just a special type of grouping and aggregation on the event-time column – each time window is a group and each row can belong to multiple windows/groups."], "answer_start": [52]}}
{"context": "\nit has full control over updating old aggregates when there is late data,\nas well as cleaning up old aggregates to limit the size of intermediate\nstate data. Since Spark 2.1, we have support for watermarking which\nallows the user to specify the threshold of late data, and allows the engine\nto accordingly clean up old state. These are explained later in more\ndetail in the\nWindow Operations\nsection.\nFault Tolerance Semantics\nDelivering end-to-end exactly-once semantics was one of key goals behind the design of Structured Streaming. To achieve that, we have designed the Structured Streaming sources, the sinks and the execution engine to reliably track the exact progress of the processing so that it can handle any kind of failure by restarting and/or reprocessing. Every streaming source is as", "question": "What is a key goal behind the design of Structured Streaming?", "answers": {"text": ["Delivering end-to-end exactly-once semantics was one of key goals behind the design of Structured Streaming."], "answer_start": [428]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nStructured Streaming Programming Guide\nOverview\nGetting Started\nAPIs on DataFrames and Datasets\nPerformance Tips\nAsynchronous Progress Tracking\nContinuous Processing\nAdditional Information\nStructured Streaming Programming Guide\nTable of contents\nAsync", "question": "What topics are covered under the 'Deploying' section?", "answers": {"text": ["Submitting Applications\nSpark Standalone\nYARN\nKubernetes"], "answer_start": [329]}}
{"context": "rmance Tips\nAsynchronous Progress Tracking\nContinuous Processing\nAdditional Information\nStructured Streaming Programming Guide\nTable of contents\nAsynchronous Progress Tracking\nWhat is it?\nAsynchronous progress tracking allows streaming queries to checkpoint progress asynchronously and in parallel to the actual data processing within a micro-batch, reducing latency associated with maintaining the offset log and commit log.\nHow does it work?\nStructured Streaming relies on persisting and managing offsets as progress indicators for query processing. Offset management operation directly impacts processing latency, because no data processing can occur until these operations are complete. Asynchronous progress tracking enables streaming queries to checkpoint progress without being impacted by the", "question": "What does asynchronous progress tracking allow streaming queries to do?", "answers": {"text": ["Asynchronous progress tracking allows streaming queries to checkpoint progress asynchronously and in parallel to the actual data processing within a micro-batch, reducing latency associated with maintaining the offset log and commit log."], "answer_start": [188]}}
{"context": "cur until these operations are complete. Asynchronous progress tracking enables streaming queries to checkpoint progress without being impacted by these offset management operations.\nHow to use it?\nThe code snippet below provides an example of how to use this feature:\nval\nstream\n=\nspark\n.\nreadStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribe\"\n,\n\"in\"\n)\n.\nload\n()\nval\nquery\n=\nstream\n.\nwriteStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"topic\"\n,\n\"out\"\n)\n.\noption\n(\n\"checkpointLocation\"\n,\n\"/tmp/checkpoint\"\n)\n.\noption\n(\n\"asyncProgressTrackingEnabled\"\n,\n\"true\"\n)\n.\nstart\n()\nThe table below describes the configurations for this feature and default values associated with them.\nOption\nValue\nDefault\nDescription\nasyncProgressTrackingEnabled\ntru", "question": "What is the default value for the option 'asyncProgressTrackingEnabled'?", "answers": {"text": ["tru"], "answer_start": [613]}}
{"context": "scribes the configurations for this feature and default values associated with them.\nOption\nValue\nDefault\nDescription\nasyncProgressTrackingEnabled\ntrue/false\nfalse\nenable or disable asynchronous progress tracking\nasyncProgressTrackingCheckpointIntervalMs\nmillisecond\n1000\nthe interval in which we commit offsets and completion commits\nLimitations\nThe initial version of the feature has the following limitations:\nAsynchronous progress tracking is only supported in stateless queries using Kafka Sink\nExactly once end-to-end processing will not be supported with this asynchronous progress tracking because offset ranges for batch can be changed in case of failure. Though many sinks, such as Kafka sink, do not support writing exactly once anyways.\nSwitching the setting off\nTurning the async progres", "question": "What is the default value for asyncProgressTrackingEnabled?", "answers": {"text": ["false"], "answer_start": [152]}}
{"context": "l or one batch earlier than the latest batch for offset log.\nThis is caused by the fact that when async progress tracking is enabled, the framework will not checkpoint progress for every batch as would be done if async progress tracking is not used. To solve this problem simply re-enable “asyncProgressTrackingEnabled” and set “asyncProgressTrackingCheckpointIntervalMs” to 0 and run the streaming query until at least two micro-batches have been processed. Async progress tracking can be now safely disabled and restarting query should proceed normally.\nContinuous Processing\n[Experimental]\nContinuous processing\nis a new, experimental streaming execution mode introduced in Spark 2.3 that enables low (~1 ms) end-to-end latency with at-least-once fault-tolerance guarantees. Compare this with the ", "question": "What is the cause of the offset log being one batch earlier than the latest batch?", "answers": {"text": ["This is caused by the fact that when async progress tracking is enabled, the framework will not checkpoint progress for every batch as would be done if async progress tracking is not used."], "answer_start": [61]}}
{"context": "ecution mode introduced in Spark 2.3 that enables low (~1 ms) end-to-end latency with at-least-once fault-tolerance guarantees. Compare this with the default\nmicro-batch processing\nengine which can achieve exactly-once guarantees but achieve latencies of ~100ms at best. For some types of queries (discussed below), you can choose which mode to execute them in without modifying the application logic (i.e. without changing the DataFrame/Dataset operations).\nTo run a supported query in continuous processing mode, all you need to do is specify a\ncontinuous trigger\nwith the desired checkpoint interval as a parameter. For example,\nspark\n\\\n.\nreadStream\n\\\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\noption\n(\n\"\nsubscribe\n\"\n,\n\"\ntopic1\n\"\n)\n\\\n.\nloa", "question": "What is a key benefit of the execution mode introduced in Spark 2.3 compared to the default micro-batch processing engine?", "answers": {"text": ["that enables low (~1 ms) end-to-end latency with at-least-once fault-tolerance guarantees"], "answer_start": [37]}}
{"context": "d\n()\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n)\n.\nwriteStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"topic\"\n,\n\"topic1\"\n)\n.\ntrigger\n(\nTrigger\n.\nContinuous\n(\n\"1 second\"\n))\n// only change in query\n.\nstart\n()\nimport\norg.apache.spark.sql.streaming.Trigger\n;\nspark\n.\nreadStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribe\"\n,\n\"topic1\"\n)\n.\nload\n()\n.\nselectExpr\n(\n\"CAST(key AS STRING)\"\n,\n\"CAST(value AS STRING)\"\n)\n.\nwriteStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"topic\"\n,\n\"topic1\"\n)\n.\ntrigger\n(\nTrigger\n.\nContinuous\n(\n\"1 second\"\n))\n// only change in query\n.\nstart\n();\nA checkpoint interval of 1 ", "question": "What are the bootstrap servers configured for Kafka?", "answers": {"text": ["\"host1:port1,host2:port2\""], "answer_start": [144]}}
{"context": "t2\"\n)\n.\noption\n(\n\"topic\"\n,\n\"topic1\"\n)\n.\ntrigger\n(\nTrigger\n.\nContinuous\n(\n\"1 second\"\n))\n// only change in query\n.\nstart\n();\nA checkpoint interval of 1 second means that the continuous processing engine will record the progress of the query every second. The resulting checkpoints are in a format compatible with the micro-batch engine, hence any query can be restarted with any trigger. For example, a supported query started with the micro-batch mode can be restarted in continuous mode, and vice versa. Note that any time you switch to continuous mode, you will get at-least-once fault-tolerance guarantees.\nSupported Queries\nAs of Spark 2.4, only the following type of queries are supported in the continuous processing mode.\nOperations\n: Only map-like Dataset/DataFrame operations are supported in", "question": "What does a checkpoint interval of 1 second signify?", "answers": {"text": ["A checkpoint interval of 1 second means that the continuous processing engine will record the progress of the query every second."], "answer_start": [123]}}
{"context": "Second\n.\nSinks\n:\nKafka sink: All options are supported.\nMemory sink: Good for debugging.\nConsole sink: Good for debugging. All options are supported. Note that the console will print every checkpoint interval that you have specified in the continuous trigger.\nSee\nInput Sources\nand\nOutput Sinks\nsections for more details on them. While the console sink is good for testing, the end-to-end low-latency processing can be best observed with Kafka as the source and sink, as this allows the engine to process the data and make the results available in the output topic within milliseconds of the input data being available in the input topic.\nCaveats\nContinuous processing engine launches multiple long-running tasks that continuously read data from sources, process it and continuously write to sinks. T", "question": "What is a good sink for observing end-to-end low-latency processing?", "answers": {"text": ["Kafka as the source and sink, as this allows the engine to process the data and make the results available in the output topic within milliseconds of the input data being available in the input topic."], "answer_start": [438]}}
{"context": "tinuous processing engine launches multiple long-running tasks that continuously read data from sources, process it and continuously write to sinks. The number of tasks required by the query depends on how many partitions the query can read from the sources in parallel. Therefore, before starting a continuous processing query, you must ensure there are enough cores in the cluster to all the tasks in parallel. For example, if you are reading from a Kafka topic that has 10 partitions, then the cluster must have at least 10 cores for the query to make progress.\nStopping a continuous processing stream may produce spurious task termination warnings. These can be safely ignored.\nThere are currently no automatic retries of failed tasks. Any failure will lead to the query being stopped and it need", "question": "What is a potential consequence of stopping a continuous processing stream?", "answers": {"text": ["Stopping a continuous processing stream may produce spurious task termination warnings."], "answer_start": [565]}}
{"context": "s. These can be safely ignored.\nThere are currently no automatic retries of failed tasks. Any failure will lead to the query being stopped and it needs to be manually restarted from the checkpoint.", "question": "What happens when a task fails?", "answers": {"text": ["Any failure will lead to the query being stopped and it needs to be manually restarted from the checkpoint."], "answer_start": [90]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)"], "answer_start": [46]}}
{"context": "ures\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nMigration Guide: MLlib (Machine Learning)\nUpgrading from MLlib 3.5 to 4.0\nUpgrading from MLlib 2.4 to 3.0\nUpgrading from MLlib 2.2 to 2.3\nUpgrading from MLlib 2.1 to 2.2\nUpgrading from MLlib 2.0 to 2.1\nUpgrading from MLlib 1.6 to 2.0\nUpgrading from MLlib 1.5 to 1.6\nUpgrading from MLlib 1.4 to 1.5\nUpgrading from MLlib 1.3 to 1.4\nUpgrading from MLlib 1.2 to 1.3\nUpgrading from MLlib 1.1 to 1.2\n", "question": "Quais são alguns dos tópicos avançados cobertos no texto?", "answers": {"text": ["Optimization (developer)"], "answer_start": [381]}}
{"context": "from MLlib 1.5 to 1.6\nUpgrading from MLlib 1.4 to 1.5\nUpgrading from MLlib 1.3 to 1.4\nUpgrading from MLlib 1.2 to 1.3\nUpgrading from MLlib 1.1 to 1.2\nUpgrading from MLlib 1.0 to 1.1\nUpgrading from MLlib 0.9 to 1.0\nNote that this migration guide describes the items specific to MLlib.\nMany items of SQL migration can be applied when migrating MLlib to higher versions for DataFrame-based APIs.\nPlease refer\nMigration Guide: SQL, Datasets and DataFrame\n.\nUpgrading from MLlib 3.5 to 4.0\nBreaking changes\nThere are no breaking changes.\nDeprecations and changes of behavior\nDeprecations\nThere are no deprecations.\nChanges of behavior\nSPARK-51132\n:\nThe PMML XML schema version of exported PMML format models by\nPMML model export\nhas been upgraded from\nPMML-4_3\nto\nPMML-4_4\n.\nUpgrading from MLlib 2.4 to 3.", "question": "What was the PMML XML schema version upgraded to when exporting PMML models?", "answers": {"text": ["PMML-4_4"], "answer_start": [759]}}
{"context": "0,\norg.apache.spark.ml.classification.MultilayerPerceptronClassificationModel\nextends\nMultilayerPerceptronParams\nto expose the training params. As a result,\nlayers\nin\nMultilayerPerceptronClassificationModel\nhas been changed from\nArray[Int]\nto\nIntArrayParam\n. Users should use\nMultilayerPerceptronClassificationModel.getLayers\ninstead of\nMultilayerPerceptronClassificationModel.layers\nto retrieve the size of layers.\norg.apache.spark.ml.classification.GBTClassifier.numTrees\nwhich is deprecated in 2.4.5, is removed in 3.0, use\ngetNumTrees\ninstead.\norg.apache.spark.ml.clustering.KMeansModel.computeCost\nwhich is deprecated in 2.4, is removed in 3.0, use\nClusteringEvaluator\ninstead.\nThe member variable\nprecision\nin\norg.apache.spark.mllib.evaluation.MulticlassMetrics\nwhich is deprecated in 2.0, is r", "question": "What should users use to retrieve the size of layers in MultilayerPerceptronClassificationModel?", "answers": {"text": ["MultilayerPerceptronClassificationModel.getLayers"], "answer_start": [276]}}
{"context": "use\nClusteringEvaluator\ninstead.\nThe member variable\nprecision\nin\norg.apache.spark.mllib.evaluation.MulticlassMetrics\nwhich is deprecated in 2.0, is removed in 3.0. Use\naccuracy\ninstead.\nThe member variable\nrecall\nin\norg.apache.spark.mllib.evaluation.MulticlassMetrics\nwhich is deprecated in 2.0, is removed in 3.0. Use\naccuracy\ninstead.\nThe member variable\nfMeasure\nin\norg.apache.spark.mllib.evaluation.MulticlassMetrics\nwhich is deprecated in 2.0, is removed in 3.0. Use\naccuracy\ninstead.\norg.apache.spark.ml.util.GeneralMLWriter.context\nwhich is deprecated in 2.0, is removed in 3.0, use\nsession\ninstead.\norg.apache.spark.ml.util.MLWriter.context\nwhich is deprecated in 2.0, is removed in 3.0, use\nsession\ninstead.\norg.apache.spark.ml.util.MLReader.context\nwhich is deprecated in 2.0, is removed i", "question": "What should be used instead of the deprecated 'precision' variable in org.apache.spark.mllib.evaluation.MulticlassMetrics?", "answers": {"text": ["accuracy"], "answer_start": [169]}}
{"context": "which is deprecated in 2.0, is removed in 3.0, use\nsession\ninstead.\norg.apache.spark.ml.util.MLReader.context\nwhich is deprecated in 2.0, is removed in 3.0, use\nsession\ninstead.\nabstract class UnaryTransformer[IN, OUT, T <: UnaryTransformer[IN, OUT, T]]\nis changed to\nabstract class UnaryTransformer[IN: TypeTag, OUT: TypeTag, T <: UnaryTransformer[IN, OUT, T]]\nin 3.0.\nDeprecations and changes of behavior\nDeprecations\nSPARK-11215\n:\nlabels\nin\nStringIndexerModel\nis deprecated and will be removed in 3.1.0. Use\nlabelsArray\ninstead.\nSPARK-25758\n:\ncomputeCost\nin\nBisectingKMeansModel\nis deprecated and will be removed in future versions. Use\nClusteringEvaluator\ninstead.\nChanges of behavior\nSPARK-11215\n:\n In Spark 2.4 and previous versions, when specifying\nfrequencyDesc\nor\nfrequencyAsc\nas\nstringOrder", "question": "What should be used instead of `labels` in `StringIndexerModel`?", "answers": {"text": ["Use\nlabelsArray\ninstead."], "answer_start": [507]}}
{"context": "t data, previously if rerun happens, users\n would see ArrayIndexOutOfBoundsException caused by mismatch between In/Out user/item blocks.\n From 3.0, a SparkException with more clear message will be thrown, and original\n ArrayIndexOutOfBoundsException is wrapped.\nSPARK-29232\n:\n In prior to 3.0 releases,\nRandomForestRegressionModel\ndoesn’t update the parameter maps\n of the DecisionTreeRegressionModels underneath. This is fixed in 3.0.\nUpgrading from MLlib 2.2 to 2.3\nBreaking changes\nThe class and trait hierarchy for logistic regression model summaries was changed to be cleaner\nand better accommodate the addition of the multi-class summary. This is a breaking change for user\ncode that casts a\nLogisticRegressionTrainingSummary\nto a\nBinaryLogisticRegressionTrainingSummary\n. Users should instead ", "question": "What exception was thrown prior to version 3.0 if a rerun happened, causing issues with user/item blocks?", "answers": {"text": ["ArrayIndexOutOfBoundsException"], "answer_start": [54]}}
{"context": "en’t available or aren’t selected.\nSPARK-17389\n:\nKMeans\nreduces the default number of steps from 5 to 2 for the k-means|| initialization mode.\nUpgrading from MLlib 1.6 to 2.0\nBreaking changes\nThere were several breaking changes in Spark 2.0, which are outlined below.\nLinear algebra classes for DataFrame-based APIs\nSpark’s linear algebra dependencies were moved to a new project,\nmllib-local\n(see\nSPARK-13944\n). \nAs part of this change, the linear algebra classes were copied to a new package,\nspark.ml.linalg\n. \nThe DataFrame-based APIs in\nspark.ml\nnow depend on the\nspark.ml.linalg\nclasses, \nleading to a few breaking changes, predominantly in various model classes \n(see\nSPARK-14810\nfor a full list).\nNote:\nthe RDD-based APIs in\nspark.mllib\ncontinue to depend on the previous package\nspark.mllib.", "question": "Where were Spark’s linear algebra dependencies moved to?", "answers": {"text": ["mllib-local"], "answer_start": [381]}}
{"context": "ious model classes \n(see\nSPARK-14810\nfor a full list).\nNote:\nthe RDD-based APIs in\nspark.mllib\ncontinue to depend on the previous package\nspark.mllib.linalg\n.\nConverting vectors and matrices\nWhile most pipeline components support backward compatibility for loading, \nsome existing\nDataFrames\nand pipelines in Spark versions prior to 2.0, that contain vector or matrix \ncolumns, may need to be migrated to the new\nspark.ml\nvector and matrix types. \nUtilities for converting\nDataFrame\ncolumns from\nspark.mllib.linalg\nto\nspark.ml.linalg\ntypes\n(and vice versa) can be found in\nspark.mllib.util.MLUtils\n.\nThere are also utility methods available for converting single instances of \nvectors and matrices. Use the\nasML\nmethod on a\nmllib.linalg.Vector\n/\nmllib.linalg.Matrix\nfor converting to\nml.linalg\ntypes,", "question": "Where can utilities for converting DataFrame columns from spark.mllib.linalg to spark.ml.linalg types be found?", "answers": {"text": ["spark.mllib.util.MLUtils"], "answer_start": [573]}}
{"context": "er\nand\nspark.ml.util.MLWriter\n, the\ncontext\nmethod has been deprecated in favor of\nsession\n.\nIn\nspark.ml.feature.ChiSqSelectorModel\n, the\nsetLabelCol\nmethod has been deprecated since it was not used by\nChiSqSelectorModel\n.\nChanges of behavior\nChanges of behavior in the\nspark.mllib\nand\nspark.ml\npackages include:\nSPARK-7780\n:\nspark.mllib.classification.LogisticRegressionWithLBFGS\ndirectly calls\nspark.ml.classification.LogisticRegression\nfor binary classification now.\n This will introduce the following behavior changes for\nspark.mllib.classification.LogisticRegressionWithLBFGS\n:\nThe intercept will not be regularized when training binary classification model with L1/L2 Updater.\nIf users set without regularization, training with or without feature scaling will return the same solution by the sa", "question": "Which method has been deprecated in favor of session in spark.ml.util.MLWriter?", "answers": {"text": ["context"], "answer_start": [36]}}
{"context": "pects sentence boundaries. Previously, it did not handle them correctly.\nSPARK-10574\n:\nHashingTF\nuses\nMurmurHash3\nas default hash algorithm in both\nspark.ml\nand\nspark.mllib\n.\nSPARK-14768\n:\n The\nexpectedType\nargument for PySpark\nParam\nwas removed.\nSPARK-14931\n:\n Some default\nParam\nvalues, which were mismatched between pipelines in Scala and Python, have been changed.\nSPARK-13600\n:\nQuantileDiscretizer\nnow uses\nspark.sql.DataFrameStatFunctions.approxQuantile\nto find splits (previously used custom sampling logic).\n The output buckets will differ for same input data and params.\nUpgrading from MLlib 1.5 to 1.6\nThere are no breaking API changes in the\nspark.mllib\nor\nspark.ml\npackages, but there are\ndeprecations and changes of behavior.\nDeprecations:\nSPARK-11358\n:\n In\nspark.mllib.clustering.KMeans", "question": "Which hash algorithm is used as default in both spark.ml and spark.mllib?", "answers": {"text": ["MurmurHash3"], "answer_start": [102]}}
{"context": " resembles the behavior of\nGradientDescent\n’s\nconvergenceTol\n: For large errors, it uses relative error (relative to the\n previous error); for small errors (\n< 0.01\n), it uses absolute error.\nSPARK-11069\n:\nspark.ml.feature.RegexTokenizer\n: Previously, it did not convert strings to lowercase before\n tokenizing. Now, it converts to lowercase by default, with an option not to. This matches the\n behavior of the simpler\nTokenizer\ntransformer.\nUpgrading from MLlib 1.4 to 1.5\nIn the\nspark.mllib\npackage, there are no breaking API changes but several behavior changes:\nSPARK-9005\n:\nRegressionMetrics.explainedVariance\nreturns the average regression sum of squares.\nSPARK-8600\n:\nNaiveBayesModel.labels\nbecome\nsorted.\nSPARK-3382\n:\nGradientDescent\nhas a default\nconvergence tolerance\n1e-3\n, and hence itera", "question": "What is the default convergence tolerance for GradientDescent?", "answers": {"text": ["1e-3"], "answer_start": [778]}}
{"context": "of squares.\nSPARK-8600\n:\nNaiveBayesModel.labels\nbecome\nsorted.\nSPARK-3382\n:\nGradientDescent\nhas a default\nconvergence tolerance\n1e-3\n, and hence iterations might end earlier than 1.4.\nIn the\nspark.ml\npackage, there exists one breaking API change and one behavior change:\nSPARK-9268\n: Java’s varargs support is removed\nfrom\nParams.setDefault\ndue to a\nScala compiler bug\n.\nSPARK-10097\n:\nEvaluator.isLargerBetter\nis\nadded to indicate metric ordering. Metrics like RMSE no longer flip signs as in 1.4.\nUpgrading from MLlib 1.3 to 1.4\nIn the\nspark.mllib\npackage, there were several breaking changes, but all in\nDeveloperApi\nor\nExperimental\nAPIs:\nGradient-Boosted Trees\n(Breaking change)\nThe signature of the\nLoss.gradient\nmethod was changed.  This is only an issues for users who wrote their own losses fo", "question": "What is the default convergence tolerance for GradientDescent?", "answers": {"text": ["1e-3"], "answer_start": [128]}}
{"context": "imization algorithm.\nIn the\nspark.ml\npackage, several major API changes occurred, including:\nParam\nand other APIs for specifying parameters\nuid\nunique IDs for Pipeline components\nReorganization of certain classes\nSince the\nspark.ml\nAPI was an alpha component in Spark 1.3, we do not list all changes here.\nHowever, since 1.4\nspark.ml\nis no longer an alpha component, we will provide details on any API\nchanges for future releases.\nUpgrading from MLlib 1.2 to 1.3\nIn the\nspark.mllib\npackage, there were several breaking changes.  The first change (in\nALS\n) is the only one in a component not marked as Alpha or Experimental.\n(Breaking change)\nIn\nALS\n, the extraneous method\nsolveLeastSquares\nhas been removed.  The\nDeveloperApi\nmethod\nanalyzeBlocks\nwas also removed.\n(Breaking change)\nStandardScalerMo", "question": "Which method was removed from ALS in spark.mllib?", "answers": {"text": ["solveLeastSquares"], "answer_start": [673]}}
{"context": "ble\nmodel\nis no longer public.\n(Breaking change)\nDecisionTree\nremains an Experimental component.  In it and its associated classes, there were several changes:\nIn\nDecisionTree\n, the deprecated class method\ntrain\nhas been removed.  (The object/static\ntrain\nmethods remain.)\nIn\nStrategy\n, the\ncheckpointDir\nparameter has been removed.  Checkpointing is still supported, but the checkpoint directory must be set before calling tree and tree ensemble training.\nPythonMLlibAPI\n(the interface between Scala/Java and Python for MLlib) was a public API but is now private, declared\nprivate[python]\n.  This was never meant for external use.\nIn linear regression (including Lasso and ridge regression), the squared loss is now divided by 2.\nSo in order to produce the same result as in 1.2, the regularization ", "question": "What change was made to the loss function in linear regression?", "answers": {"text": ["In linear regression (including Lasso and ridge regression), the squared loss is now divided by 2."], "answer_start": [632]}}
{"context": "on (including Lasso and ridge regression), the squared loss is now divided by 2.\nSo in order to produce the same result as in 1.2, the regularization parameter needs to be divided by 2 and the step size needs to be multiplied by 2.\nIn the\nspark.ml\npackage, the main API changes are from Spark SQL.  We list the most important changes here:\nThe old\nSchemaRDD\nhas been replaced with\nDataFrame\nwith a somewhat modified API.  All algorithms in\nspark.ml\nwhich used to use SchemaRDD now use DataFrame.\nIn Spark 1.2, we used implicit conversions from\nRDD\ns of\nLabeledPoint\ninto\nSchemaRDD\ns by calling\nimport sqlContext._\nwhere\nsqlContext\nwas an instance of\nSQLContext\n.  These implicits have been moved, so we now call\nimport sqlContext.implicits._\n.\nJava APIs for SQL have also changed accordingly.  Please", "question": "What has replaced the old SchemaRDD in the spark.ml package?", "answers": {"text": ["DataFrame"], "answer_start": [381]}}
{"context": "SQLContext\n.  These implicits have been moved, so we now call\nimport sqlContext.implicits._\n.\nJava APIs for SQL have also changed accordingly.  Please see the examples above and the\nSpark SQL Programming Guide\nfor details.\nOther changes were in\nLogisticRegression\n:\nThe\nscoreCol\noutput column (with default value “score”) was renamed to be\nprobabilityCol\n(with default value “probability”).  The type was originally\nDouble\n(for the probability of class 1.0), but it is now\nVector\n(for the probability of each class, to support multiclass classification in the future).\nIn Spark 1.2,\nLogisticRegressionModel\ndid not include an intercept.  In Spark 1.3, it includes an intercept; however, it will always be 0.0 since it uses the default settings for\nspark.mllib.LogisticRegressionWithLBFGS\n.  The optio", "question": "What was the original type of the scoreCol output column in LogisticRegression before the change?", "answers": {"text": ["Double"], "answer_start": [416]}}
{"context": "e store it in a sparse format instead of dense to\ntake advantage of sparsity in both storage and computation. Details are described below.", "question": "What should be done with the data to leverage sparsity?", "answers": {"text": ["store it in a sparse format instead of dense"], "answer_start": [2]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)"], "answer_start": [46]}}
{"context": "ures\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nSummary statistics\nCorrelations\nStratified sampling\nHypothesis testing\nRandom data generation\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nBasic Statistics - RDD-based API\nSummary statistics\nCorrelations\nStratified sampling\nHypothesis testing\nStreaming Significance Testing\nRandom data generation\nKernel density estimation\n\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\", "question": "Quais são alguns dos tópicos avançados abordados no texto?", "answers": {"text": ["Model selection and tuning"], "answer_start": [94]}}
{"context": "eration\nKernel density estimation\n\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\wv}{\\mathbf{w}}\n\\newcommand{\\av}{\\mathbf{\\alpha}}\n\\newcommand{\\bv}{\\mathbf{b}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\id}{\\mathbf{I}}\n\\newcommand{\\ind}{\\mathbf{1}}\n\\newcommand{\\0}{\\mathbf{0}}\n\\newcommand{\\unit}{\\mathbf{e}}\n\\newcommand{\\one}{\\mathbf{1}}\n\\newcommand{\\zero}{\\mathbf{0}}\n\\]\nSummary statistics\nWe provide column summary statistics for\nRDD[Vector]\nthrough the function\ncolStats\navailable in\nStatistics\n.\ncolStats()\nreturns an instance of\nMultivariateStatisticalSummary\n,\nwhich contains the column-wise max, min, mean, variance, and number of nonzeros, as well as the\ntotal count.\nRefer to the\nMultivariateStatisticalSummary\nP", "question": "What does the colStats() function return?", "answers": {"text": ["MultivariateStatisticalSummary"], "answer_start": [613]}}
{"context": "onzeros\n())\n# number of nonzeros in each column\nFind full example code at \"examples/src/main/python/mllib/summary_statistics_example.py\" in the Spark repo.\ncolStats()\nreturns an instance of\nMultivariateStatisticalSummary\n,\nwhich contains the column-wise max, min, mean, variance, and number of nonzeros, as well as the\ntotal count.\nRefer to the\nMultivariateStatisticalSummary\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.linalg.Vectors\nimport\norg.apache.spark.mllib.stat.\n{\nMultivariateStatisticalSummary\n,\nStatistics\n}\nval\nobservations\n=\nsc\n.\nparallelize\n(\nSeq\n(\nVectors\n.\ndense\n(\n1.0\n,\n10.0\n,\n100.0\n),\nVectors\n.\ndense\n(\n2.0\n,\n20.0\n,\n200.0\n),\nVectors\n.\ndense\n(\n3.0\n,\n30.0\n,\n300.0\n)\n)\n)\n// Compute column summary statistics.\nval\nsummary\n:\nMultivariateStatisticalSummary\n=\nStatisti", "question": "What does colStats() return?", "answers": {"text": ["MultivariateStatisticalSummary"], "answer_start": [190]}}
{"context": "tics.\nMultivariateStatisticalSummary\nsummary\n=\nStatistics\n.\ncolStats\n(\nmat\n.\nrdd\n());\nSystem\n.\nout\n.\nprintln\n(\nsummary\n.\nmean\n());\n// a dense vector containing the mean value for each column\nSystem\n.\nout\n.\nprintln\n(\nsummary\n.\nvariance\n());\n// column-wise variance\nSystem\n.\nout\n.\nprintln\n(\nsummary\n.\nnumNonzeros\n());\n// number of nonzeros in each column\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\" in the Spark repo.\nCorrelations\nCalculating the correlation between two series of data is a common operation in Statistics. In\nspark.mllib\nwe provide the flexibility to calculate pairwise correlations among many series. The supported\ncorrelation methods are currently Pearson’s and Spearman’s correlation.\nStatistics\nprovides meth", "question": "What does `summary.mean()` return?", "answers": {"text": ["a dense vector containing the mean value for each column"], "answer_start": [134]}}
{"context": "5.0\n])\n# Compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method.\n# If a method is not specified, Pearson's method will be used by default.\nprint\n(\n\"\nCorrelation is:\n\"\n+\nstr\n(\nStatistics\n.\ncorr\n(\nseriesX\n,\nseriesY\n,\nmethod\n=\n\"\npearson\n\"\n)))\ndata\n=\nsc\n.\nparallelize\n(\n[\nnp\n.\narray\n([\n1.0\n,\n10.0\n,\n100.0\n]),\nnp\n.\narray\n([\n2.0\n,\n20.0\n,\n200.0\n]),\nnp\n.\narray\n([\n5.0\n,\n33.0\n,\n366.0\n])]\n)\n# an RDD of Vectors\n# calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method.\n# If a method is not specified, Pearson's method will be used by default.\nprint\n(\nStatistics\n.\ncorr\n(\ndata\n,\nmethod\n=\n\"\npearson\n\"\n))\nFind full example code at \"examples/src/main/python/mllib/correlations_example.py\" in the Spark repo.\nStatistics\nprovides methods to\ncalc", "question": "What method is used by default if no method is specified when calculating correlation?", "answers": {"text": ["Pearson's method will be used by default."], "answer_start": [129]}}
{"context": "pearson\n\"\n))\nFind full example code at \"examples/src/main/python/mllib/correlations_example.py\" in the Spark repo.\nStatistics\nprovides methods to\ncalculate correlations between series. Depending on the type of input, two\nRDD[Double]\ns or\nan\nRDD[Vector]\n, the output will be a\nDouble\nor the correlation\nMatrix\nrespectively.\nRefer to the\nStatistics\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.linalg._\nimport\norg.apache.spark.mllib.stat.Statistics\nimport\norg.apache.spark.rdd.RDD\nval\nseriesX\n:\nRDD\n[\nDouble\n]\n=\nsc\n.\nparallelize\n(\nimmutable\n.\nArraySeq\n.\nunsafeWrapArray\n(\nArray\n(\n1.0\n,\n2.0\n,\n3.0\n,\n3.0\n,\n5.0\n)))\n// a series\n// must have the same number of partitions and cardinality as seriesX\nval\nseriesY\n:\nRDD\n[\nDouble\n]\n=\nsc\n.\nparallelize\n(\nimmutable\n.\nArraySeq\n.\nunsafeWrapArray", "question": "What does the Statistics module provide methods for?", "answers": {"text": ["calculate correlations between series."], "answer_start": [146]}}
{"context": "have the same number of partitions and cardinality as seriesX\nval\nseriesY\n:\nRDD\n[\nDouble\n]\n=\nsc\n.\nparallelize\n(\nimmutable\n.\nArraySeq\n.\nunsafeWrapArray\n(\nArray\n(\n11.0\n,\n22.0\n,\n33.0\n,\n33.0\n,\n555.0\n)))\n// compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method. If a\n// method is not specified, Pearson's method will be used by default.\nval\ncorrelation\n:\nDouble\n=\nStatistics\n.\ncorr\n(\nseriesX\n,\nseriesY\n,\n\"pearson\"\n)\nprintln\n(\ns\n\"Correlation is: $correlation\"\n)\nval\ndata\n:\nRDD\n[\nVector\n]\n=\nsc\n.\nparallelize\n(\nSeq\n(\nVectors\n.\ndense\n(\n1.0\n,\n10.0\n,\n100.0\n),\nVectors\n.\ndense\n(\n2.0\n,\n20.0\n,\n200.0\n),\nVectors\n.\ndense\n(\n5.0\n,\n33.0\n,\n366.0\n))\n)\n// note that each Vector is a row and not a column\n// calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearm", "question": "What method is used by default if no method is specified when computing the correlation?", "answers": {"text": ["Pearson's method will be used by default."], "answer_start": [323]}}
{"context": "\n,\n366.0\n))\n)\n// note that each Vector is a row and not a column\n// calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method\n// If a method is not specified, Pearson's method will be used by default.\nval\ncorrelMatrix\n:\nMatrix\n=\nStatistics\n.\ncorr\n(\ndata\n,\n\"pearson\"\n)\nprintln\n(\ncorrelMatrix\n.\ntoString\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\" in the Spark repo.\nStatistics\nprovides methods to\ncalculate correlations between series. Depending on the type of input, two\nJavaDoubleRDD\ns or\na\nJavaRDD<Vector>\n, the output will be a\nDouble\nor the correlation\nMatrix\nrespectively.\nRefer to the\nStatistics\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\norg.apache.spark.api.java.J", "question": "What is used by default if a method is not specified when calculating the correlation matrix?", "answers": {"text": ["Pearson's method will be used by default."], "answer_start": [195]}}
{"context": "orrelation\nMatrix\nrespectively.\nRefer to the\nStatistics\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\norg.apache.spark.api.java.JavaDoubleRDD\n;\nimport\norg.apache.spark.api.java.JavaRDD\n;\nimport\norg.apache.spark.mllib.linalg.Matrix\n;\nimport\norg.apache.spark.mllib.linalg.Vector\n;\nimport\norg.apache.spark.mllib.linalg.Vectors\n;\nimport\norg.apache.spark.mllib.stat.Statistics\n;\nJavaDoubleRDD\nseriesX\n=\njsc\n.\nparallelizeDoubles\n(\nArrays\n.\nasList\n(\n1.0\n,\n2.0\n,\n3.0\n,\n3.0\n,\n5.0\n));\n// a series\n// must have the same number of partitions and cardinality as seriesX\nJavaDoubleRDD\nseriesY\n=\njsc\n.\nparallelizeDoubles\n(\nArrays\n.\nasList\n(\n11.0\n,\n22.0\n,\n33.0\n,\n33.0\n,\n555.0\n));\n// compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method.\n// If a method is not sp", "question": "Which method can be used to compute the correlation, besides Pearson's method?", "answers": {"text": ["Enter \"spearman\" for Spearman's method."], "answer_start": [736]}}
{"context": "0\n,\n22.0\n,\n33.0\n,\n33.0\n,\n555.0\n));\n// compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method.\n// If a method is not specified, Pearson's method will be used by default.\ndouble\ncorrelation\n=\nStatistics\n.\ncorr\n(\nseriesX\n.\nsrdd\n(),\nseriesY\n.\nsrdd\n(),\n\"pearson\"\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Correlation is: \"\n+\ncorrelation\n);\n// note that each Vector is a row and not a column\nJavaRDD\n<\nVector\n>\ndata\n=\njsc\n.\nparallelize\n(\nArrays\n.\nasList\n(\nVectors\n.\ndense\n(\n1.0\n,\n10.0\n,\n100.0\n),\nVectors\n.\ndense\n(\n2.0\n,\n20.0\n,\n200.0\n),\nVectors\n.\ndense\n(\n5.0\n,\n33.0\n,\n366.0\n)\n)\n);\n// calculate the correlation matrix using Pearson's method.\n// Use \"spearman\" for Spearman's method.\n// If a method is not specified, Pearson's method will be used by default.\nMatrix\ncorrelMatrix\n=\nStatistic", "question": "What method is used by default if a correlation method is not specified?", "answers": {"text": ["Pearson's method will be used by default."], "answer_start": [159]}}
{"context": "d.\n// Use \"spearman\" for Spearman's method.\n// If a method is not specified, Pearson's method will be used by default.\nMatrix\ncorrelMatrix\n=\nStatistics\n.\ncorr\n(\ndata\n.\nrdd\n(),\n\"pearson\"\n);\nSystem\n.\nout\n.\nprintln\n(\ncorrelMatrix\n.\ntoString\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\" in the Spark repo.\nStratified sampling\nUnlike the other statistics functions, which reside in\nspark.mllib\n, stratified sampling methods,\nsampleByKey\nand\nsampleByKeyExact\n, can be performed on RDD’s of key-value pairs. For stratified\nsampling, the keys can be thought of as a label and the value as a specific attribute. For example\nthe key can be man or woman, or document ids, and the respective values can be the list of ages\nof the people in ", "question": "What method will be used by default if a correlation method is not specified?", "answers": {"text": ["Pearson's method will be used by default."], "answer_start": [77]}}
{"context": "pairs for key $k$, and $K$ is the set of\nkeys. Sampling without replacement requires one additional pass over the RDD to guarantee sample\nsize, whereas sampling with replacement requires two additional passes.\nimport\njava.util.*\n;\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.api.java.JavaPairRDD\n;\nList\n<\nTuple2\n<\nInteger\n,\nCharacter\n>>\nlist\n=\nArrays\n.\nasList\n(\nnew\nTuple2\n<>(\n1\n,\n'a'\n),\nnew\nTuple2\n<>(\n1\n,\n'b'\n),\nnew\nTuple2\n<>(\n2\n,\n'c'\n),\nnew\nTuple2\n<>(\n2\n,\n'd'\n),\nnew\nTuple2\n<>(\n2\n,\n'e'\n),\nnew\nTuple2\n<>(\n3\n,\n'f'\n)\n);\nJavaPairRDD\n<\nInteger\n,\nCharacter\n>\ndata\n=\njsc\n.\nparallelizePairs\n(\nlist\n);\n// specify the exact fraction desired from each key Map<K, Double>\nImmutableMap\n<\nInteger\n,\nDouble\n>\nfractions\n=\nImmutableMap\n.\nof\n(\n1\n,\n0.1\n,\n2\n,\n0.6\n,\n3\n,\n0.3\n);\n// Get an approximate sample from each", "question": "What is required to guarantee sample size when sampling without replacement?", "answers": {"text": ["one additional pass over the RDD"], "answer_start": [85]}}
{"context": "Map<K, Double>\nImmutableMap\n<\nInteger\n,\nDouble\n>\nfractions\n=\nImmutableMap\n.\nof\n(\n1\n,\n0.1\n,\n2\n,\n0.6\n,\n3\n,\n0.3\n);\n// Get an approximate sample from each stratum\nJavaPairRDD\n<\nInteger\n,\nCharacter\n>\napproxSample\n=\ndata\n.\nsampleByKey\n(\nfalse\n,\nfractions\n);\n// Get an exact sample from each stratum\nJavaPairRDD\n<\nInteger\n,\nCharacter\n>\nexactSample\n=\ndata\n.\nsampleByKeyExact\n(\nfalse\n,\nfractions\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\" in the Spark repo.\nHypothesis testing\nHypothesis testing is a powerful tool in statistics to determine whether a result is statistically\nsignificant, whether this result occurred by chance or not.\nspark.mllib\ncurrently supports Pearson’s\nchi-squared ( $\\chi^2$) tests for goodness of fit and ", "question": "Where can I find a full example code for stratified sampling?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\" in the Spark repo."], "answer_start": [390]}}
{"context": "ficant, whether this result occurred by chance or not.\nspark.mllib\ncurrently supports Pearson’s\nchi-squared ( $\\chi^2$) tests for goodness of fit and independence. The input data types determine\nwhether the goodness of fit or the independence test is conducted. The goodness of fit test requires\nan input type of\nVector\n, whereas the independence test requires a\nMatrix\nas input.\nspark.mllib\nalso supports the input type\nRDD[LabeledPoint]\nto enable feature selection via chi-squared\nindependence tests.\nStatistics\nprovides methods to\nrun Pearson’s chi-squared tests. The following example demonstrates how to run and interpret\nhypothesis tests.\nRefer to the\nStatistics\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.linalg\nimport\nMatrices\n,\nVectors\nfrom\npyspark.mllib.regression\nimport\nL", "question": "What input type is required for the goodness of fit test in spark.mllib?", "answers": {"text": ["Vector"], "answer_start": [313]}}
{"context": "\\n\n\"\n%\ngoodnessOfFitTestResult\n)\nmat\n=\nMatrices\n.\ndense\n(\n3\n,\n2\n,\n[\n1.0\n,\n3.0\n,\n5.0\n,\n2.0\n,\n4.0\n,\n6.0\n])\n# a contingency matrix\n# conduct Pearson's independence test on the input contingency matrix\nindependenceTestResult\n=\nStatistics\n.\nchiSqTest\n(\nmat\n)\n# summary of the test including the p-value, degrees of freedom,\n# test statistic, the method used, and the null hypothesis.\nprint\n(\n\"\n%s\n\\n\n\"\n%\nindependenceTestResult\n)\nobs\n=\nsc\n.\nparallelize\n(\n[\nLabeledPoint\n(\n1.0\n,\n[\n1.0\n,\n0.0\n,\n3.0\n]),\nLabeledPoint\n(\n1.0\n,\n[\n1.0\n,\n2.0\n,\n0.0\n]),\nLabeledPoint\n(\n1.0\n,\n[\n-\n1.0\n,\n0.0\n,\n-\n0.5\n])]\n)\n# LabeledPoint(label, feature)\n# The contingency table is constructed from an RDD of LabeledPoint and used to conduct\n# the independence test. Returns an array containing the ChiSquaredTestResult for every feature\n", "question": "What does LabeledPoint consist of?", "answers": {"text": ["LabeledPoint(label, feature)"], "answer_start": [588]}}
{"context": "ssion.LabeledPoint\nimport\norg.apache.spark.mllib.stat.Statistics\nimport\norg.apache.spark.mllib.stat.test.ChiSqTestResult\nimport\norg.apache.spark.rdd.RDD\n// a vector composed of the frequencies of events\nval\nvec\n:\nVector\n=\nVectors\n.\ndense\n(\n0.1\n,\n0.15\n,\n0.2\n,\n0.3\n,\n0.25\n)\n// compute the goodness of fit. If a second vector to test against is not supplied\n// as a parameter, the test runs against a uniform distribution.\nval\ngoodnessOfFitTestResult\n=\nStatistics\n.\nchiSqTest\n(\nvec\n)\n// summary of the test including the p-value, degrees of freedom, test statistic, the method\n// used, and the null hypothesis.\nprintln\n(\ns\n\"$goodnessOfFitTestResult\\n\"\n)\n// a contingency matrix. Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\nval\nmat\n:\nMatrix\n=\nMatrices\n.\ndense\n(\n3\n,\n2\n,\nArray\n(\n1.0\n,\n3.0\n,", "question": "What is computed using Statistics.chiSqTest(vec)?", "answers": {"text": ["the goodness of fit"], "answer_start": [283]}}
{"context": "\n// a contingency matrix. Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\nval\nmat\n:\nMatrix\n=\nMatrices\n.\ndense\n(\n3\n,\n2\n,\nArray\n(\n1.0\n,\n3.0\n,\n5.0\n,\n2.0\n,\n4.0\n,\n6.0\n))\n// conduct Pearson's independence test on the input contingency matrix\nval\nindependenceTestResult\n=\nStatistics\n.\nchiSqTest\n(\nmat\n)\n// summary of the test including the p-value, degrees of freedom\nprintln\n(\ns\n\"$independenceTestResult\\n\"\n)\nval\nobs\n:\nRDD\n[\nLabeledPoint\n]\n=\nsc\n.\nparallelize\n(\nSeq\n(\nLabeledPoint\n(\n1.0\n,\nVectors\n.\ndense\n(\n1.0\n,\n0.0\n,\n3.0\n)),\nLabeledPoint\n(\n1.0\n,\nVectors\n.\ndense\n(\n1.0\n,\n2.0\n,\n0.0\n)),\nLabeledPoint\n(-\n1.0\n,\nVectors\n.\ndense\n(-\n1.0\n,\n0.0\n,\n-\n0.5\n)\n)\n)\n)\n// (label, feature) pairs.\n// The contingency table is constructed from the raw (label, feature) pairs and used to conduct\n// the independence ", "question": "What is created using the `Matrices.dense` function?", "answers": {"text": ["dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))"], "answer_start": [35]}}
{"context": ".\ndense\n(\n0.1\n,\n0.15\n,\n0.2\n,\n0.3\n,\n0.25\n);\n// compute the goodness of fit. If a second vector to test against is not supplied\n// as a parameter, the test runs against a uniform distribution.\nChiSqTestResult\ngoodnessOfFitTestResult\n=\nStatistics\n.\nchiSqTest\n(\nvec\n);\n// summary of the test including the p-value, degrees of freedom, test statistic,\n// the method used, and the null hypothesis.\nSystem\n.\nout\n.\nprintln\n(\ngoodnessOfFitTestResult\n+\n\"\\n\"\n);\n// Create a contingency matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\nMatrix\nmat\n=\nMatrices\n.\ndense\n(\n3\n,\n2\n,\nnew\ndouble\n[]{\n1.0\n,\n3.0\n,\n5.0\n,\n2.0\n,\n4.0\n,\n6.0\n});\n// conduct Pearson's independence test on the input contingency matrix\nChiSqTestResult\nindependenceTestResult\n=\nStatistics\n.\nchiSqTest\n(\nmat\n);\n// summary of the test including the p-value", "question": "What is used to compute the goodness of fit, and what happens if a second vector for testing isn't provided?", "answers": {"text": ["compute the goodness of fit. If a second vector to test against is not supplied\n// as a parameter, the test runs against a uniform distribution."], "answer_start": [46]}}
{"context": "turns an array containing the ChiSquaredTestResult for every feature\n// against the label.\nChiSqTestResult\n[]\nfeatureTestResults\n=\nStatistics\n.\nchiSqTest\n(\nobs\n.\nrdd\n());\nint\ni\n=\n1\n;\nfor\n(\nChiSqTestResult\nresult\n:\nfeatureTestResults\n)\n{\nSystem\n.\nout\n.\nprintln\n(\n\"Column \"\n+\ni\n+\n\":\"\n);\nSystem\n.\nout\n.\nprintln\n(\nresult\n+\n\"\\n\"\n);\n// summary of the test\ni\n++;\n}\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\" in the Spark repo.\nAdditionally,\nspark.mllib\nprovides a 1-sample, 2-sided implementation of the Kolmogorov-Smirnov (KS) test\nfor equality of probability distributions. By providing the name of a theoretical distribution\n(currently solely supported for the normal distribution) and its parameters, or a function to\ncalculate t", "question": "Where can I find a full example code for hypothesis testing?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\" in the Spark repo."], "answer_start": [358]}}
{"context": "ow to run\nand interpret the hypothesis tests.\nRefer to the\nStatistics\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.stat\nimport\nStatistics\nparallelData\n=\nsc\n.\nparallelize\n([\n0.1\n,\n0.15\n,\n0.2\n,\n0.3\n,\n0.25\n])\n# run a KS test for the sample versus a standard normal distribution\ntestResult\n=\nStatistics\n.\nkolmogorovSmirnovTest\n(\nparallelData\n,\n\"\nnorm\n\"\n,\n0\n,\n1\n)\n# summary of the test including the p-value, test statistic, and null hypothesis\n# if our p-value indicates significance, we can reject the null hypothesis\n# Note that the Scala functionality of calling Statistics.kolmogorovSmirnovTest with\n# a lambda to calculate the CDF is not made available in the Python API\nprint\n(\ntestResult\n)\nFind full example code at \"examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smir", "question": "What does the code do with the `parallelData`?", "answers": {"text": ["run a KS test for the sample versus a standard normal distribution"], "answer_start": [223]}}
{"context": "not made available in the Python API\nprint\n(\ntestResult\n)\nFind full example code at \"examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\" in the Spark repo.\nStatistics\nprovides methods to\nrun a 1-sample, 2-sided Kolmogorov-Smirnov test. The following example demonstrates how to run\nand interpret the hypothesis tests.\nRefer to the\nStatistics\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.stat.Statistics\nimport\norg.apache.spark.rdd.RDD\nval\ndata\n:\nRDD\n[\nDouble\n]\n=\nsc\n.\nparallelize\n(\nSeq\n(\n0.1\n,\n0.15\n,\n0.2\n,\n0.3\n,\n0.25\n))\n// an RDD of sample data\n// run a KS test for the sample versus a standard normal distribution\nval\ntestResult\n=\nStatistics\n.\nkolmogorovSmirnovTest\n(\ndata\n,\n\"norm\"\n,\n0\n,\n1\n)\n// summary of the test including the p-value, test s", "question": "Where can I find a full example code for the Kolmogorov-Smirnov test?", "answers": {"text": ["Find full example code at \"examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\" in the Spark repo."], "answer_start": [58]}}
{"context": "lmogorovSmirnovTestExample.scala\" in the Spark repo.\nStatistics\nprovides methods to\nrun a 1-sample, 2-sided Kolmogorov-Smirnov test. The following example demonstrates how to run\nand interpret the hypothesis tests.\nRefer to the\nStatistics\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\norg.apache.spark.api.java.JavaDoubleRDD\n;\nimport\norg.apache.spark.mllib.stat.Statistics\n;\nimport\norg.apache.spark.mllib.stat.test.KolmogorovSmirnovTestResult\n;\nJavaDoubleRDD\ndata\n=\njsc\n.\nparallelizeDoubles\n(\nArrays\n.\nasList\n(\n0.1\n,\n0.15\n,\n0.2\n,\n0.3\n,\n0.25\n));\nKolmogorovSmirnovTestResult\ntestResult\n=\nStatistics\n.\nkolmogorovSmirnovTest\n(\ndata\n,\n\"norm\"\n,\n0.0\n,\n1.0\n);\n// summary of the test including the p-value, test statistic, and null hypothesis\n// if our p-value indicates significance, we ", "question": "What test is demonstrated in the example using the Statistics class?", "answers": {"text": ["a 1-sample, 2-sided Kolmogorov-Smirnov test"], "answer_start": [88]}}
{"context": "mingTest\n=\nnew\nStreamingTest\n()\n.\nsetPeacePeriod\n(\n0\n)\n.\nsetWindowSize\n(\n0\n)\n.\nsetTestMethod\n(\n\"welch\"\n)\nval\nout\n=\nstreamingTest\n.\nregisterStream\n(\ndata\n)\nout\n.\nprint\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\" in the Spark repo.\nStreamingTest\nprovides streaming hypothesis testing.\nimport\norg.apache.spark.mllib.stat.test.BinarySample\n;\nimport\norg.apache.spark.mllib.stat.test.StreamingTest\n;\nimport\norg.apache.spark.mllib.stat.test.StreamingTestResult\n;\nJavaDStream\n<\nBinarySample\n>\ndata\n=\nssc\n.\ntextFileStream\n(\ndataDir\n).\nmap\n(\nline\n->\n{\nString\n[]\nts\n=\nline\n.\nsplit\n(\n\",\"\n);\nboolean\nlabel\n=\nBoolean\n.\nparseBoolean\n(\nts\n[\n0\n]);\ndouble\nvalue\n=\nDouble\n.\nparseDouble\n(\nts\n[\n1\n]);\nreturn\nnew\nBinarySample\n(\nlabel\n,\nvalue\n);\n});\nStr", "question": "Where can I find a full example code for StreamingTest?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\" in the Spark repo."], "answer_start": [170]}}
{"context": "oolean\nlabel\n=\nBoolean\n.\nparseBoolean\n(\nts\n[\n0\n]);\ndouble\nvalue\n=\nDouble\n.\nparseDouble\n(\nts\n[\n1\n]);\nreturn\nnew\nBinarySample\n(\nlabel\n,\nvalue\n);\n});\nStreamingTest\nstreamingTest\n=\nnew\nStreamingTest\n()\n.\nsetPeacePeriod\n(\n0\n)\n.\nsetWindowSize\n(\n0\n)\n.\nsetTestMethod\n(\n\"welch\"\n);\nJavaDStream\n<\nStreamingTestResult\n>\nout\n=\nstreamingTest\n.\nregisterStream\n(\ndata\n);\nout\n.\nprint\n();\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\" in the Spark repo.\nRandom data generation\nRandom data generation is useful for randomized algorithms, prototyping, and performance testing.\nspark.mllib\nsupports generating random RDDs with i.i.d. values drawn from a given distribution:\nuniform, standard normal, or Poisson.\nRandomRDDs\nprovides factory\nmethods to gen", "question": "Where can I find a full example code for JavaStreamingTestExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\" in the Spark repo."], "answer_start": [371]}}
{"context": "normalRDD\n(\nsc\n,\n1000000L\n,\n10\n)\n# Apply a transform to get a random double RDD following `N(1, 4)`.\nv\n=\nu\n.\nmap\n(\nlambda\nx\n:\n1.0\n+\n2.0\n*\nx\n)\nRandomRDDs\nprovides factory\nmethods to generate random double RDDs or vector RDDs.\nThe following example generates a random double RDD, whose values follows the standard normal\ndistribution\nN(0, 1)\n, and then map it to\nN(1, 4)\n.\nRefer to the\nRandomRDDs\nScala docs\nfor details on the API.\nimport\norg.apache.spark.SparkContext\nimport\norg.apache.spark.mllib.random.RandomRDDs._\nval\nsc\n:\nSparkContext\n=\n...\n// Generate a random double RDD that contains 1 million i.i.d. values drawn from the\n// standard normal distribution `N(0, 1)`, evenly distributed in 10 partitions.\nval\nu\n=\nnormalRDD\n(\nsc\n,\n1000000L\n,\n10\n)\n// Apply a transform to get a random double RDD f", "question": "What distribution does the example generate a random double RDD from initially?", "answers": {"text": ["N(0, 1)"], "answer_start": [332]}}
{"context": "ensity function of a random variables, evaluated at a given set of points. It achieves\nthis estimate by expressing the PDF of the empirical distribution at a particular point as the\nmean of PDFs of normal distributions centered around each of the samples.\nKernelDensity\nprovides methods\nto compute kernel density estimates from an RDD of samples. The following example demonstrates how\nto do so.\nRefer to the\nKernelDensity\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.stat\nimport\nKernelDensity\n# an RDD of sample data\ndata\n=\nsc\n.\nparallelize\n([\n1.0\n,\n1.0\n,\n1.0\n,\n2.0\n,\n3.0\n,\n4.0\n,\n5.0\n,\n5.0\n,\n6.0\n,\n7.0\n,\n8.0\n,\n9.0\n,\n9.0\n])\n# Construct the density estimator with the sample data and a standard deviation for the Gaussian\n# kernels\nkd\n=\nKernelDensity\n()\nkd\n.\nsetSample\n(\ndata\n)\nkd\n.\nset", "question": "What does KernelDensity provide methods to compute?", "answers": {"text": ["kernel density estimates from an RDD of samples."], "answer_start": [298]}}
{"context": " the density estimator with the sample data and a standard deviation for the Gaussian\n# kernels\nkd\n=\nKernelDensity\n()\nkd\n.\nsetSample\n(\ndata\n)\nkd\n.\nsetBandwidth\n(\n3.0\n)\n# Find density estimates for the given values\ndensities\n=\nkd\n.\nestimate\n([\n-\n1.0\n,\n2.0\n,\n5.0\n])\nFind full example code at \"examples/src/main/python/mllib/kernel_density_estimation_example.py\" in the Spark repo.\nKernelDensity\nprovides methods\nto compute kernel density estimates from an RDD of samples. The following example demonstrates how\nto do so.\nRefer to the\nKernelDensity\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.stat.KernelDensity\nimport\norg.apache.spark.rdd.RDD\n// an RDD of sample data\nval\ndata\n:\nRDD\n[\nDouble\n]\n=\nsc\n.\nparallelize\n(\nSeq\n(\n1\n,\n1\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n,\n5\n,\n6\n,\n7\n,\n8\n,\n9\n,\n9\n))\n// Cons", "question": "What does KernelDensity provide methods to compute?", "answers": {"text": ["kernel density estimates from an RDD of samples."], "answer_start": [421]}}
{"context": "s\ndouble\n[]\ndensities\n=\nkd\n.\nestimate\n(\nnew\ndouble\n[]{-\n1.0\n,\n2.0\n,\n5.0\n});\nSystem\n.\nout\n.\nprintln\n(\nArrays\n.\ntoString\n(\ndensities\n));\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\" in the Spark repo.", "question": "Where can I find the full example code for JavaKernelDensityEstimationExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\" in the Spark repo."], "answer_start": [135]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nSQL Pipe Syntax\nSyntax\nOverview\nApache Spark supports SQL pipe synta", "question": "What are some of the programming guides available in Apache Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)"], "answer_start": [46]}}
{"context": "\nin a table subquery in regular Spark SQL.\nEXTEND\nadds new columns to the input table by evaluating the provided expressions.\nThis also preserves table aliases.\nThis works like\nSELECT *, new_column\nin regular Spark SQL.\nDROP\nremoves columns from the input table.\nThis is similar to\nSELECT * EXCEPT (column)\nin regular Spark SQL.\nSET\nreplaces column values from the input table.\nThis is similar to\nSELECT * REPLACE (expression AS column)\nin regular Spark SQL.\nAS\nforwards the input table and introduces a new alias for each row.\nAggregations\nIn general, aggregation takes place differently using SQL pipe syntax as opposed to regular Spark\nSQL.\nTo perform full-table aggregation, use the\nAGGREGATE\noperator with a list of aggregate\nexpressions to evaluate. This returns one single row in the output ta", "question": "How does the EXTEND operator function in Spark SQL pipe syntax?", "answers": {"text": ["adds new columns to the input table by evaluating the provided expressions."], "answer_start": [50]}}
{"context": "form full-table aggregation, use the\nAGGREGATE\noperator with a list of aggregate\nexpressions to evaluate. This returns one single row in the output table.\nTo perform aggregation with grouping, use the\nAGGREGATE\noperator with a\nGROUP BY\nclause.\nThis returns one row for each unique combination of values of the grouping expressions. The output\ntable contains the evaluated grouping expressions followed by the evaluated aggregate functions.\nGrouping expressions support assigning aliases for purposes of referring to them in future\noperators. In this way, it is not necessary to repeat entire expressions between\nGROUP BY\nand\nSELECT\n, since\nAGGREGATE\nis a single operator that performs both.\nOther Transformations\nThe remaining operators are used for other transformations, such as filtering, joining,", "question": "What does the AGGREGATE operator return when used with a GROUP BY clause?", "answers": {"text": ["This returns one row for each unique combination of values of the grouping expressions."], "answer_start": [244]}}
{"context": "y.\nTable subqueries can be written using either regular Spark SQL syntax or pipe syntax.\nThey may appear inside enclosing queries written in either syntax.\nOther Spark SQL statements such as views and DDL and DML commands may include queries written\nusing either syntax.\nSupported Operators\nOperator\nOutput rows\nFROM\nor\nTABLE\nReturns all the output rows from the source table unmodified.\nSELECT\nEvaluates the provided expressions over each of the rows of the input table.\nEXTEND\nAppends new columns to the input table by evaluating the specified expressions over each of the input rows.\nSET\nUpdates columns of the input table by replacing them with the result of evaluating the provided expressions.\nDROP\nDrops columns of the input table by name.\nAS\nRetains the same rows and column names of the inpu", "question": "What does the FROM or TABLE operator do?", "answers": {"text": ["Returns all the output rows from the source table unmodified."], "answer_start": [326]}}
{"context": "he result of evaluating the provided expressions.\nDROP\nDrops columns of the input table by name.\nAS\nRetains the same rows and column names of the input table but with a new table alias.\nWHERE\nReturns the subset of input rows passing the condition.\nLIMIT\nReturns the specified number of input rows, preserving ordering (if any).\nAGGREGATE\nPerforms aggregation with or without grouping.\nJOIN\nJoins rows from both inputs, returning a filtered cross-product of the input table and the table argument.\nORDER BY\nReturns the input rows after sorting as indicated.\nUNION ALL\nPerforms the union or other set operation over the combined rows from the input table plus other table argument(s).\nTABLESAMPLE\nReturns the subset of rows chosen by the provided sampling algorithm.\nPIVOT\nReturns a new table with the ", "question": "What does the DROP operation do?", "answers": {"text": ["Drops columns of the input table by name."], "answer_start": [55]}}
{"context": "le plus other table argument(s).\nTABLESAMPLE\nReturns the subset of rows chosen by the provided sampling algorithm.\nPIVOT\nReturns a new table with the input rows pivoted to become columns.\nUNPIVOT\nReturns a new table with the input columns pivoted to become rows.\nThis table lists each of the supported pipe operators and describes the output rows they produce.\nNote that each operator accepts an input relation comprising the rows generated by the query\npreceding the\n|>\nsymbol.\nFROM or TABLE\nFROM\n<\ntableName\n>\nTABLE\n<\ntableName\n>\nReturns all the output rows from the source table unmodified.\nFor example:\nCREATE\nTABLE\nt\nAS\nVALUES\n(\n1\n,\n2\n),\n(\n3\n,\n4\n)\nAS\nt\n(\na\n,\nb\n);\nTABLE\nt\n;\n+\n---+---+\n|\na\n|\nb\n|\n+\n---+---+\n|\n1\n|\n2\n|\n|\n3\n|\n4\n|\n+\n---+---+\nSELECT\n|>\nSELECT\n<\nexpr\n>\n[[\nAS\n]\nalias\n],\n...\nEvaluates t", "question": "What does the FROM or TABLE operator do?", "answers": {"text": ["Returns all the output rows from the source table unmodified."], "answer_start": [532]}}
{"context": "\n)\nAS\nt\n(\na\n,\nb\n);\nTABLE\nt\n;\n+\n---+---+\n|\na\n|\nb\n|\n+\n---+---+\n|\n1\n|\n2\n|\n|\n3\n|\n4\n|\n+\n---+---+\nSELECT\n|>\nSELECT\n<\nexpr\n>\n[[\nAS\n]\nalias\n],\n...\nEvaluates the provided expressions over each of the rows of the input table.\nIn general, this operator is not always required with SQL pipe syntax. It is possible to use it at\nor near the end of a query to evaluate expressions or specify a list of output columns.\nSince the final query result always comprises the columns returned from the last pipe operator,\nwhen this\nSELECT\noperator does not appear, the output includes all columns from the full row.\nThis behavior is similar to\nSELECT *\nin standard SQL syntax.\nIt is possible to use\nDISTINCT\nand\n*\nas needed.\nThis works like the outermost\nSELECT\nin a table subquery in regular Spark SQL.\nWindow functions ar", "question": "What does the SELECT operator do in the context of SQL pipe syntax?", "answers": {"text": ["Evaluates the provided expressions over each of the rows of the input table."], "answer_start": [139]}}
{"context": "------+\nWHERE\n|>\nWHERE\n<\ncondition\n>\nReturns the subset of input rows passing the condition.\nSince this operator may appear anywhere, no separate\nHAVING\nor\nQUALIFY\nsyntax is needed.\nFor example:\nVALUES\n(\n0\n),\n(\n1\n)\ntab\n(\ncol\n)\n|>\nWHERE\ncol\n=\n1\n;\n+\n---+\n|\ncol\n|\n+\n---+\n|\n1\n|\n+\n---+\nLIMIT\n|>\n[\nLIMIT\n<\nn\n>\n]\n[\nOFFSET\n<\nm\n>\n]\nReturns the specified number of input rows, preserving ordering (if any).\nLIMIT\nand\nOFFSET\nare supported together. The\nLIMIT\nclause can also be used without the\nOFFSET\nclause, and the\nOFFSET\nclause can be used without the\nLIMIT\nclause.\nFor example:\nVALUES\n(\n0\n),\n(\n0\n)\ntab\n(\ncol\n)\n|>\nLIMIT\n1\n;\n+\n---+\n|\ncol\n|\n+\n---+\n|\n0\n|\n+\n---+\nAGGREGATE\n-- Full-table aggregation\n|>\nAGGREGATE\n<\nagg_expr\n>\n[[\nAS\n]\nalias\n],\n...\n-- Aggregation with grouping\n|>\nAGGREGATE\n[\n<\nagg_expr\n>\n[[\nAS\n]\n", "question": "What does the WHERE operator do?", "answers": {"text": ["Returns the subset of input rows passing the condition."], "answer_start": [37]}}
{"context": " the\nGROUP BY\nclause can include any number of grouping expressions, and each\n<agg_expr>\nexpression will evaluate over each unique combination of values of the grouping\nexpressions. The output table contains the evaluated grouping expressions followed by the evaluated\naggregate functions. The\nGROUP BY\nexpressions may include one-based ordinals. Unlike regular SQL\nin which such ordinals refer to the expressions in the accompanying\nSELECT\nclause, in SQL pipe\nsyntax, they refer to the columns of the relation produced by the preceding operator instead. For\nexample, in\nTABLE t |> AGGREGATE COUNT(*) GROUP BY 2\n, we refer to the second column of the input\ntable\nt\n.\nThere is no need to repeat entire expressions between\nGROUP BY\nand\nSELECT\n, since the\nAGGREGATE\noperator automatically includes the e", "question": "In SQL pipe syntax, what do one-based ordinals in the GROUP BY expressions refer to?", "answers": {"text": ["they refer to the columns of the relation produced by the preceding operator instead."], "answer_start": [469]}}
{"context": " input\ntable\nt\n.\nThere is no need to repeat entire expressions between\nGROUP BY\nand\nSELECT\n, since the\nAGGREGATE\noperator automatically includes the evaluated grouping expressions in its output. By the same token,\nafter an\nAGGREGATE\noperator, it is often unnecessary to issue a following\nSELECT\noperator, since\nAGGREGATE\nreturns both the grouping columns and the aggregate columns in a single step.\nFor example:\n-- Full-table aggregation\nVALUES\n(\n0\n),\n(\n1\n)\ntab\n(\ncol\n)\n|>\nAGGREGATE\nCOUNT\n(\ncol\n)\nAS\ncount\n;\n+\n-----+\n|\ncount\n|\n+\n-----+\n|\n2\n|\n+\n-----+\n-- Aggregation with grouping\nVALUES\n(\n0\n,\n1\n),\n(\n0\n,\n2\n)\ntab\n(\ncol1\n,\ncol2\n)\n|>\nAGGREGATE\nCOUNT\n(\ncol2\n)\nAS\ncount\nGROUP\nBY\ncol1\n;\n+\n----+-----+\n|\ncol1\n|\ncount\n|\n+\n----+-----+\n|\n0\n|\n2\n|\n+\n----+-----+\nJOIN\n|>\n[\nLEFT\n|\nRIGHT\n|\nFULL\n|\nCROSS\n|\nSEMI\n|\nANT", "question": "What does the AGGREGATE operator automatically include in its output?", "answers": {"text": ["the evaluated grouping expressions in its output"], "answer_start": [145]}}
{"context": "l\n|\n+\n---+\n|\n1\n|\n|\n0\n|\n+\n---+\nUNION, INTERSECT, EXCEPT\n|>\n{\nUNION\n|\nINTERSECT\n|\nEXCEPT\n}\n{\nALL\n|\nDISTINCT\n}\n(\n<\nquery\n>\n)\nPerforms the union or other set operation over the combined rows from the input table or subquery.\nFor example:\nVALUES\n(\n0\n),\n(\n1\n)\ntab\n(\na\n,\nb\n)\n|>\nUNION\nALL\nVALUES\n(\n2\n),\n(\n3\n)\ntab\n(\nc\n,\nd\n);\n+\n---+----+\n|\na\n|\nb\n|\n+\n---+----+\n|\n0\n|\n1\n|\n|\n2\n|\n3\n|\n+\n---+----+\nTABLESAMPLE\n|>\nTABLESAMPLE\n<\nmethod\n>\n(\n<\nsize\n>\n{\nROWS\n|\nPERCENT\n})\nReturns the subset of rows chosen by the provided sampling algorithm.\nFor example:\nVALUES\n(\n0\n),\n(\n0\n),\n(\n0\n),\n(\n0\n)\ntab\n(\ncol\n)\n|>\nTABLESAMPLE\n(\n1\nROWS\n);\n+\n---+\n|\ncol\n|\n+\n---+\n|\n0\n|\n+\n---+\nVALUES\n(\n0\n),\n(\n0\n)\ntab\n(\ncol\n)\n|>\nTABLESAMPLE\n(\n100\nPERCENT\n);\n+\n---+\n|\ncol\n|\n+\n---+\n|\n0\n|\n|\n0\n|\n+\n---+\nPIVOT\n|>\nPIVOT\n(\nagg_expr\nFOR\ncol\nIN\n(\nval1\n,\n...))\nR", "question": "What does the TABLESAMPLE operator do?", "answers": {"text": ["Returns the subset of rows chosen by the provided sampling algorithm."], "answer_start": [451]}}
{"context": "\n0\n),\n(\n0\n)\ntab\n(\ncol\n)\n|>\nTABLESAMPLE\n(\n100\nPERCENT\n);\n+\n---+\n|\ncol\n|\n+\n---+\n|\n0\n|\n|\n0\n|\n+\n---+\nPIVOT\n|>\nPIVOT\n(\nagg_expr\nFOR\ncol\nIN\n(\nval1\n,\n...))\nReturns a new table with the input rows pivoted to become columns.\nFor example:\nVALUES\n(\n\"dotNET\"\n,\n2012\n,\n10000\n),\n(\n\"Java\"\n,\n2012\n,\n20000\n),\n(\n\"dotNET\"\n,\n2012\n,\n5000\n),\n(\n\"dotNET\"\n,\n2013\n,\n48000\n),\n(\n\"Java\"\n,\n2013\n,\n30000\n)\ncourseSales\n(\ncourse\n,\nyear\n,\nearnings\n)\n|>\nPIVOT\n(\nSUM\n(\nearnings\n)\nFOR\nCOURSE\nIN\n(\n'dotNET'\n,\n'Java'\n)\n)\n+\n----+------+------+\n|\nyear\n|\ndotNET\n|\nJava\n|\n+\n----+------+------+\n|\n2012\n|\n15000\n|\n20000\n|\n|\n2013\n|\n48000\n|\n30000\n|\n+\n----+------+------+\nUNPIVOT\n|>\nUNPIVOT\n(\nvalue_col\nFOR\nkey_col\nIN\n(\ncol1\n,\n...))\nReturns a new table with the input columns pivoted to become rows.\nFor example:\nVALUES\n(\n\"dotNET\"\n,\n2012\n,\n10000\n),\n", "question": "What does the PIVOT operator do?", "answers": {"text": ["Returns a new table with the input rows pivoted to become columns."], "answer_start": [149]}}
{"context": "col\nFOR\nkey_col\nIN\n(\ncol1\n,\n...))\nReturns a new table with the input columns pivoted to become rows.\nFor example:\nVALUES\n(\n\"dotNET\"\n,\n2012\n,\n10000\n),\n(\n\"Java\"\n,\n2012\n,\n20000\n),\n(\n\"dotNET\"\n,\n2012\n,\n5000\n),\n(\n\"dotNET\"\n,\n2013\n,\n48000\n),\n(\n\"Java\"\n,\n2013\n,\n30000\n)\ncourseSales\n(\ncourse\n,\nyear\n,\nearnings\n)\n|>\nUNPIVOT\n(\nearningsYear\nFOR\n`year`\nIN\n(\n`2012`\n,\n`2013`\n,\n`2014`\n)\n+\n--------+------+--------+\n|\ncourse\n|\nyear\n|\nearnings\n|\n+\n--------+------+--------+\n|\nJava\n|\n2012\n|\n20000\n|\n|\nJava\n|\n2013\n|\n30000\n|\n|\ndotNET\n|\n2012\n|\n15000\n|\n|\ndotNET\n|\n2013\n|\n48000\n|\n|\ndotNET\n|\n2014\n|\n22500\n|\n+\n--------+------+--------+", "question": "What does the function do?", "answers": {"text": ["Returns a new table with the input columns pivoted to become rows."], "answer_start": [34]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTrou", "question": "Which file formats are mentioned as data sources in the Spark SQL Guide?", "answers": {"text": ["Parquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files"], "answer_start": [650]}}
{"context": "ve deployment can still enable Hive support. When not configured\nby the\nhive-site.xml\n, the context automatically creates\nmetastore_db\nin the current directory and\ncreates a directory configured by\nspark.sql.warehouse.dir\n, which defaults to the directory\nspark-warehouse\nin the current directory that the Spark application is started. Note that\nthe\nhive.metastore.warehouse.dir\nproperty in\nhive-site.xml\nis deprecated since Spark 2.0.0.\nInstead, use\nspark.sql.warehouse.dir\nto specify the default location of database in warehouse.\nYou may need to grant write privilege to the user who starts the Spark application.\nfrom\nos.path\nimport\nabspath\nfrom\npyspark.sql\nimport\nSparkSession\nfrom\npyspark.sql\nimport\nRow\n# warehouse_location points to the default location for managed databases and tables\nwareh", "question": "Which property should be used to specify the default location of the database in the warehouse instead of hive.metastore.warehouse.dir?", "answers": {"text": ["spark.sql.warehouse.dir"], "answer_start": [198]}}
{"context": "pyspark.sql\nimport\nSparkSession\nfrom\npyspark.sql\nimport\nRow\n# warehouse_location points to the default location for managed databases and tables\nwarehouse_location\n=\nabspath\n(\n'\nspark-warehouse\n'\n)\nspark\n=\nSparkSession\n\\\n.\nbuilder\n\\\n.\nappName\n(\n\"\nPython Spark SQL Hive integration example\n\"\n)\n\\\n.\nconfig\n(\n\"\nspark.sql.warehouse.dir\n\"\n,\nwarehouse_location\n)\n\\\n.\nenableHiveSupport\n()\n\\\n.\ngetOrCreate\n()\n# spark is an existing SparkSession\nspark\n.\nsql\n(\n\"\nCREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\n\"\n)\nspark\n.\nsql\n(\n\"\nLOAD DATA LOCAL INPATH\n'\nexamples/src/main/resources/kv1.txt\n'\nINTO TABLE src\n\"\n)\n# Queries are expressed in HiveQL\nspark\n.\nsql\n(\n\"\nSELECT * FROM src\n\"\n).\nshow\n()\n# +---+-------+\n# |key|  value|\n# +---+-------+\n# |238|val_238|\n# | 86| val_86|\n# |311|val_311|\n# ", "question": "What is the purpose of the `warehouse_location` variable?", "answers": {"text": ["warehouse_location points to the default location for managed databases and tables"], "answer_start": [62]}}
{"context": "veQL\nspark\n.\nsql\n(\n\"\nSELECT * FROM src\n\"\n).\nshow\n()\n# +---+-------+\n# |key|  value|\n# +---+-------+\n# |238|val_238|\n# | 86| val_86|\n# |311|val_311|\n# ...\n# Aggregation queries are also supported.\nspark\n.\nsql\n(\n\"\nSELECT COUNT(*) FROM src\n\"\n).\nshow\n()\n# +--------+\n# |count(1)|\n# +--------+\n# |    500 |\n# +--------+\n# The results of SQL queries are themselves DataFrames and support all normal functions.\nsqlDF\n=\nspark\n.\nsql\n(\n\"\nSELECT key, value FROM src WHERE key < 10 ORDER BY key\n\"\n)\n# The items in DataFrames are of type Row, which allows you to access each column by ordinal.\nstringsDS\n=\nsqlDF\n.\nrdd\n.\nmap\n(\nlambda\nrow\n:\n\"\nKey: %d, Value: %s\n\"\n%\n(\nrow\n.\nkey\n,\nrow\n.\nvalue\n))\nfor\nrecord\nin\nstringsDS\n.\ncollect\n():\nprint\n(\nrecord\n)\n# Key: 0, Value: val_0\n# Key: 0, Value: val_0\n# Key: 0, Value: va", "question": "What type are the items in DataFrames?", "answers": {"text": ["The items in DataFrames are of type Row, which allows you to access each column by ordinal."], "answer_start": [489]}}
{"context": "---+\n# |  2| val_2|  2| val_2|\n# |  4| val_4|  4| val_4|\n# |  5| val_5|  5| val_5|\n# ...\nFind full example code at \"examples/src/main/python/sql/hive.py\" in the Spark repo.\nimport\njava.io.File\nimport\norg.apache.spark.sql.\n{\nRow\n,\nSaveMode\n,\nSparkSession\n}\ncase\nclass\nRecord\n(\nkey\n:\nInt\n,\nvalue\n:\nString\n)\n// warehouseLocation points to the default location for managed databases and tables\nval\nwarehouseLocation\n=\nnew\nFile\n(\n\"spark-warehouse\"\n).\ngetAbsolutePath\nval\nspark\n=\nSparkSession\n.\nbuilder\n()\n.\nappName\n(\n\"Spark Hive Example\"\n)\n.\nconfig\n(\n\"spark.sql.warehouse.dir\"\n,\nwarehouseLocation\n)\n.\nenableHiveSupport\n()\n.\ngetOrCreate\n()\nimport\nspark.implicits._\nimport\nspark.sql\nsql\n(\n\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\"\n)\nsql\n(\n\"LOAD DATA LOCAL INPATH 'examples/src/main", "question": "Where can the full example code be found?", "answers": {"text": ["Find full example code at \"examples/src/main/python/sql/hive.py\" in the Spark repo."], "answer_start": [89]}}
{"context": "ue FROM src WHERE key < 10 ORDER BY key\"\n)\n// The items in DataFrames are of type Row, which allows you to access each column by ordinal.\nval\nstringsDS\n=\nsqlDF\n.\nmap\n{\ncase\nRow\n(\nkey\n:\nInt\n,\nvalue\n:\nString\n)\n=>\ns\n\"Key: $key, Value: $value\"\n}\nstringsDS\n.\nshow\n()\n// +--------------------+\n// |               value|\n// +--------------------+\n// |Key: 0, Value: val_0|\n// |Key: 0, Value: val_0|\n// |Key: 0, Value: val_0|\n// ...\n// You can also use DataFrames to create temporary views within a SparkSession.\nval\nrecordsDF\n=\nspark\n.\ncreateDataFrame\n((\n1\nto\n100\n).\nmap\n(\ni\n=>\nRecord\n(\ni\n,\ns\n\"val_$i\"\n)))\nrecordsDF\n.\ncreateOrReplaceTempView\n(\n\"records\"\n)\n// Queries can then join DataFrame data with data stored in Hive.\nsql\n(\n\"SELECT * FROM records r JOIN src s ON r.key = s.key\"\n).\nshow\n()\n// +---+------", "question": "What SQL query is used to join DataFrame data with data stored in Hive?", "answers": {"text": ["SELECT * FROM records r JOIN src s ON r.key = s.key"], "answer_start": [722]}}
{"context": "/ Queries can then join DataFrame data with data stored in Hive.\nsql\n(\n\"SELECT * FROM records r JOIN src s ON r.key = s.key\"\n).\nshow\n()\n// +---+------+---+------+\n// |key| value|key| value|\n// +---+------+---+------+\n// |  2| val_2|  2| val_2|\n// |  4| val_4|  4| val_4|\n// |  5| val_5|  5| val_5|\n// ...\n// Create a Hive managed Parquet table, with HQL syntax instead of the Spark SQL native syntax\n// `USING hive`\nsql\n(\n\"CREATE TABLE hive_records(key int, value string) STORED AS PARQUET\"\n)\n// Save DataFrame to the Hive managed table\nval\ndf\n=\nspark\n.\ntable\n(\n\"src\"\n)\ndf\n.\nwrite\n.\nmode\n(\nSaveMode\n.\nOverwrite\n).\nsaveAsTable\n(\n\"hive_records\"\n)\n// After insertion, the Hive managed table has data now\nsql\n(\n\"SELECT * FROM hive_records\"\n).\nshow\n()\n// +---+-------+\n// |key|  value|\n// +---+-------+\n//", "question": "What is the command to create a Hive managed Parquet table?", "answers": {"text": ["CREATE TABLE hive_records(key int, value string) STORED AS PARQUET"], "answer_start": [423]}}
{"context": "esses the partitions in parallel.\n// Turn on flag for Hive Dynamic Partitioning\nspark\n.\nconf\n.\nset\n(\n\"hive.exec.dynamic.partition\"\n,\n\"true\"\n)\nspark\n.\nconf\n.\nset\n(\n\"hive.exec.dynamic.partition.mode\"\n,\n\"nonstrict\"\n)\n// Create a Hive partitioned table using DataFrame API\ndf\n.\nwrite\n.\npartitionBy\n(\n\"key\"\n).\nformat\n(\n\"hive\"\n).\nsaveAsTable\n(\n\"hive_part_tbl\"\n)\n// Partitioned column `key` will be moved to the end of the schema.\nsql\n(\n\"SELECT * FROM hive_part_tbl\"\n).\nshow\n()\n// +-------+---+\n// |  value|key|\n// +-------+---+\n// |val_238|238|\n// | val_86| 86|\n// |val_311|311|\n// ...\nspark\n.\nstop\n()\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\" in the Spark repo.\nimport\njava.io.File\n;\nimport\njava.io.Serializable\n;\nimport\njava.util.ArrayL", "question": "What is set to \"true\" to turn on the flag for Hive Dynamic Partitioning?", "answers": {"text": ["\"hive.exec.dynamic.partition\""], "answer_start": [101]}}
{"context": "\nvalue\n)\n{\nthis\n.\nvalue\n=\nvalue\n;\n}\n}\n// warehouseLocation points to the default location for managed databases and tables\nString\nwarehouseLocation\n=\nnew\nFile\n(\n\"spark-warehouse\"\n).\ngetAbsolutePath\n();\nSparkSession\nspark\n=\nSparkSession\n.\nbuilder\n()\n.\nappName\n(\n\"Java Spark Hive Example\"\n)\n.\nconfig\n(\n\"spark.sql.warehouse.dir\"\n,\nwarehouseLocation\n)\n.\nenableHiveSupport\n()\n.\ngetOrCreate\n();\nspark\n.\nsql\n(\n\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\"\n);\nspark\n.\nsql\n(\n\"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src\"\n);\n// Queries are expressed in HiveQL\nspark\n.\nsql\n(\n\"SELECT * FROM src\"\n).\nshow\n();\n// +---+-------+\n// |key|  value|\n// +---+-------+\n// |238|val_238|\n// | 86| val_86|\n// |311|val_311|\n// ...\n// Aggregation queries are also supported", "question": "What does the code do with the file 'examples/src/main/resources/kv1.txt'?", "answers": {"text": ["LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src"], "answer_start": [489]}}
{"context": "// +---+-------+\n// |key|  value|\n// +---+-------+\n// |238|val_238|\n// | 86| val_86|\n// |311|val_311|\n// ...\n// Aggregation queries are also supported.\nspark\n.\nsql\n(\n\"SELECT COUNT(*) FROM src\"\n).\nshow\n();\n// +--------+\n// |count(1)|\n// +--------+\n// |    500 |\n// +--------+\n// The results of SQL queries are themselves DataFrames and support all normal functions.\nDataset\n<\nRow\n>\nsqlDF\n=\nspark\n.\nsql\n(\n\"SELECT key, value FROM src WHERE key < 10 ORDER BY key\"\n);\n// The items in DataFrames are of type Row, which lets you to access each column by ordinal.\nDataset\n<\nString\n>\nstringsDS\n=\nsqlDF\n.\nmap\n(\n(\nMapFunction\n<\nRow\n,\nString\n>)\nrow\n->\n\"Key: \"\n+\nrow\n.\nget\n(\n0\n)\n+\n\", Value: \"\n+\nrow\n.\nget\n(\n1\n),\nEncoders\n.\nSTRING\n());\nstringsDS\n.\nshow\n();\n// +--------------------+\n// |               value|\n// +-", "question": "What type are the items in DataFrames?", "answers": {"text": ["Row"], "answer_start": [375]}}
{"context": "aceTempView\n(\n\"records\"\n);\n// Queries can then join DataFrames data with data stored in Hive.\nspark\n.\nsql\n(\n\"SELECT * FROM records r JOIN src s ON r.key = s.key\"\n).\nshow\n();\n// +---+------+---+------+\n// |key| value|key| value|\n// +---+------+---+------+\n// |  2| val_2|  2| val_2|\n// |  2| val_2|  2| val_2|\n// |  4| val_4|  4| val_4|\n// ...\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\" in the Spark repo.\nWhen working with Hive one must instantiate\nSparkSession\nwith Hive support. This\nadds support for finding tables in the MetaStore and writing queries using HiveQL.\n# enableHiveSupport defaults to TRUE\nsparkR.session\n(\nenableHiveSupport\n=\nTRUE\n)\nsql\n(\n\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\"\n)\nsql\n(", "question": "What is added when instantiating SparkSession with Hive support?", "answers": {"text": ["adds support for finding tables in the MetaStore and writing queries using HiveQL."], "answer_start": [555]}}
{"context": "Support defaults to TRUE\nsparkR.session\n(\nenableHiveSupport\n=\nTRUE\n)\nsql\n(\n\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\"\n)\nsql\n(\n\"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src\"\n)\n# Queries can be expressed in HiveQL.\nresults\n<-\ncollect\n(\nsql\n(\n\"FROM src SELECT key, value\"\n))\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nSpecifying storage format for Hive tables\nWhen you create a Hive table, you need to define how this table should read/write data from/to file system,\ni.e. the “input format” and “output format”. You also need to define how this table should deserialize the data\nto rows, or serialize rows to data, i.e. the “serde”. The following options can be used to specify the storage\nformat(“serde", "question": "What is needed when creating a Hive table?", "answers": {"text": ["you need to define how this table should read/write data from/to file system"], "answer_start": [486]}}
{"context": "ould deserialize the data\nto rows, or serialize rows to data, i.e. the “serde”. The following options can be used to specify the storage\nformat(“serde”, “input format”, “output format”), e.g.\nCREATE TABLE src(id int) USING hive OPTIONS(fileFormat 'parquet')\n.\nBy default, we will read the table files as plain text. Note that, Hive storage handler is not supported yet when\ncreating table, you can create a table using storage handler at Hive side, and use Spark SQL to read it.\nProperty Name\nMeaning\nfileFormat\nA fileFormat is kind of a package of storage format specifications, including \"serde\", \"input format\" and\n      \"output format\". Currently we support 6 fileFormats: 'sequencefile', 'rcfile', 'orc', 'parquet', 'textfile' and 'avro'.\ninputFormat, outputFormat\nThese 2 options specify the na", "question": "Quais formatos de arquivo são suportados atualmente?", "answers": {"text": ["'sequencefile', 'rcfile', 'orc', 'parquet', 'textfile' and 'avro'."], "answer_start": [677]}}
{"context": "d \"rcfile\"\n      don't include the serde information and you can use this option with these 3 fileFormats.\nfieldDelim, escapeDelim, collectionDelim, mapkeyDelim, lineDelim\nThese options can only be used with \"textfile\" fileFormat. They define how to read delimited files into rows.\nAll other properties defined with\nOPTIONS\nwill be regarded as Hive serde properties.\nInteracting with Different Versions of Hive Metastore\nOne of the most important pieces of Spark SQL’s Hive support is interaction with Hive metastore,\nwhich enables Spark SQL to access metadata of Hive tables. Starting from Spark 1.4.0, a single binary\nbuild of Spark SQL can be used to query different versions of Hive metastores, using the configuration described below.\nNote that independent of the version of Hive that is being u", "question": "What enables Spark SQL to access metadata of Hive tables?", "answers": {"text": ["which enables Spark SQL to access metadata of Hive tables."], "answer_start": [518]}}
{"context": "hould be loaded using the classloader that is\n        shared between Spark SQL and a specific version of Hive. An example of classes that should\n        be shared is JDBC drivers that are needed to talk to the metastore. Other classes that need\n        to be shared are those that interact with classes that are already shared. For example,\n        custom appenders that are used by log4j.\n1.4.0\nspark.sql.hive.metastore.barrierPrefixes\n(empty)\nA comma separated list of class prefixes that should explicitly be reloaded for each version\n        of Hive that Spark SQL is communicating with. For example, Hive UDFs that are declared in a\n        prefix that typically would be shared (i.e.\norg.apache.spark.*\n).\n1.4.0", "question": "What is an example of classes that should be shared between Spark SQL and Hive?", "answers": {"text": ["JDBC drivers that are needed to talk to the metastore."], "answer_start": [166]}}
{"context": "ix that typically would be shared (i.e.\norg.apache.spark.*\n).\n1.4.0", "question": "What is an example of a package that is typically shared?", "answers": {"text": ["org.apache.spark.*"], "answer_start": [40]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTrou", "question": "Which file formats are mentioned as data sources in the Spark SQL Guide?", "answers": {"text": ["Parquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files"], "answer_start": [650]}}
{"context": "Parquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTroubleshooting\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nGeneric File Source Options\nIgnore Corrupt Files\nIgnore Missing Files\nPath Glob Filter\nRecursive File Lookup\nModification Time Path Filters\nThese generic options/configurations are effective only when using file-based sources: parquet, orc, avro, json, csv, text.\nPlease note that the hierarchy of directories used in examples below are:\ndir1/\n ├── dir2/\n │    └── file2.parquet (schema: <file: string>, content: \"file2.parquet\")\n └── file1.parquet (schema: <file, string>, content: \"file1.parquet\")\n", "question": "For which file-based sources are the generic options/configurations effective?", "answers": {"text": ["parquet, orc, avro, json, csv, text."], "answer_start": [527]}}
{"context": "\n\"\n,\n\"\nexamples/src/main/resources/dir1/dir2/\n\"\n)\ntest_corrupt_df1\n.\nshow\n()\n# +-------------+\n# |         file|\n# +-------------+\n# |file1.parquet|\n# |file2.parquet|\n# +-------------+\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\n// enable ignore corrupt files via the data source option\n// dir1/file3.json is corrupt from parquet's view\nval\ntestCorruptDF0\n=\nspark\n.\nread\n.\noption\n(\n\"ignoreCorruptFiles\"\n,\n\"true\"\n).\nparquet\n(\n\"examples/src/main/resources/dir1/\"\n,\n\"examples/src/main/resources/dir1/dir2/\"\n)\ntestCorruptDF0\n.\nshow\n()\n// +-------------+\n// |         file|\n// +-------------+\n// |file1.parquet|\n// |file2.parquet|\n// +-------------+\n// enable ignore corrupt files via the configuration\nspark\n.\nsql\n(\n\"set spark.sql.files.ignoreCorruptFiles=tr", "question": "Where can I find the full example code?", "answers": {"text": ["Find full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo."], "answer_start": [185]}}
{"context": "t|\n// |file2.parquet|\n// +-------------+\n// enable ignore corrupt files via the configuration\nspark\n.\nsql\n(\n\"set spark.sql.files.ignoreCorruptFiles=true\"\n)\n// dir1/file3.json is corrupt from parquet's view\nval\ntestCorruptDF1\n=\nspark\n.\nread\n.\nparquet\n(\n\"examples/src/main/resources/dir1/\"\n,\n\"examples/src/main/resources/dir1/dir2/\"\n)\ntestCorruptDF1\n.\nshow\n()\n// +-------------+\n// |         file|\n// +-------------+\n// |file1.parquet|\n// |file2.parquet|\n// +-------------+\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\n// enable ignore corrupt files via the data source option\n// dir1/file3.json is corrupt from parquet's view\nDataset\n<\nRow\n>\ntestCorruptDF0\n=\nspark\n.\nread\n().\noption\n(\n\"ignoreCorruptFiles\"\n,\n\"true\"\n).\n", "question": "How can you enable ignoring corrupt files via the configuration?", "answers": {"text": ["set spark.sql.files.ignoreCorruptFiles=true"], "answer_start": [109]}}
{"context": "mples/src/main/resources/dir1/dir2/\"\n);\ntestCorruptDF1\n.\nshow\n();\n// +-------------+\n// |         file|\n// +-------------+\n// |file1.parquet|\n// |file2.parquet|\n// +-------------+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\n# enable ignore corrupt files via the data source option\n# dir1/file3.json is corrupt from parquet's view\ntestCorruptDF0\n<-\nread.parquet\n(\nc\n(\n\"examples/src/main/resources/dir1/\"\n,\n\"examples/src/main/resources/dir1/dir2/\"\n),\nignoreCorruptFiles\n=\n\"true\"\n)\nhead\n(\ntestCorruptDF0\n)\n#            file\n# 1 file1.parquet\n# 2 file2.parquet\n# enable ignore corrupt files via the configuration\nsql\n(\n\"set spark.sql.files.ignoreCorruptFiles=true\"\n)\n# dir1/file3.json is corrupt from parquet's view\nte", "question": "Where can I find a full example code for JavaSQLDataSourceExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo."], "answer_start": [180]}}
{"context": "e ignore corrupt files via the configuration\nsql\n(\n\"set spark.sql.files.ignoreCorruptFiles=true\"\n)\n# dir1/file3.json is corrupt from parquet's view\ntestCorruptDF1\n<-\nread.parquet\n(\nc\n(\n\"examples/src/main/resources/dir1/\"\n,\n\"examples/src/main/resources/dir1/dir2/\"\n))\nhead\n(\ntestCorruptDF1\n)\n#            file\n# 1 file1.parquet\n# 2 file2.parquet\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nIgnore Missing Files\nSpark allows you to use the configuration\nspark.sql.files.ignoreMissingFiles\nor the data source option\nignoreMissingFiles\nto ignore missing files while reading data\nfrom files. Here, missing file really means the deleted file under directory after you construct the\nDataFrame\n. When set to true, the Spark jobs will continue to run when encounterin", "question": "How can you ignore corrupt files in Spark SQL?", "answers": {"text": ["\"set spark.sql.files.ignoreCorruptFiles=true\""], "answer_start": [51]}}
{"context": "ally means the deleted file under directory after you construct the\nDataFrame\n. When set to true, the Spark jobs will continue to run when encountering missing files and\nthe contents that have been read will still be returned.\nPath Glob Filter\npathGlobFilter\nis used to only include files with file names matching the pattern. The syntax follows\norg.apache.hadoop.fs.GlobFilter\n. It does not change the behavior of partition discovery.\nTo load files with paths matching a given glob pattern while keeping the behavior of partition discovery,\nyou can use:\ndf\n=\nspark\n.\nread\n.\nload\n(\n\"\nexamples/src/main/resources/dir1\n\"\n,\nformat\n=\n\"\nparquet\n\"\n,\npathGlobFilter\n=\n\"\n*.parquet\n\"\n)\ndf\n.\nshow\n()\n# +-------------+\n# |         file|\n# +-------------+\n# |file1.parquet|\n# +-------------+\nFind full example co", "question": "What does the `pathGlobFilter` do?", "answers": {"text": ["is used to only include files with file names matching the pattern."], "answer_start": [259]}}
{"context": "obFilter\n=\n\"\n*.parquet\n\"\n)\ndf\n.\nshow\n()\n# +-------------+\n# |         file|\n# +-------------+\n# |file1.parquet|\n# +-------------+\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\nval\ntestGlobFilterDF\n=\nspark\n.\nread\n.\nformat\n(\n\"parquet\"\n)\n.\noption\n(\n\"pathGlobFilter\"\n,\n\"*.parquet\"\n)\n// json file should be filtered out\n.\nload\n(\n\"examples/src/main/resources/dir1\"\n)\ntestGlobFilterDF\n.\nshow\n()\n// +-------------+\n// |         file|\n// +-------------+\n// |file1.parquet|\n// +-------------+\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nDataset\n<\nRow\n>\ntestGlobFilterDF\n=\nspark\n.\nread\n().\nformat\n(\n\"parquet\"\n)\n.\noption\n(\n\"pathGlobFilter\"\n,\n\"*.parquet\"\n)\n// json file should be filte", "question": "What glob filter is used to read only parquet files?", "answers": {"text": ["*.parquet"], "answer_start": [13]}}
{"context": "epo.\nDataset\n<\nRow\n>\ntestGlobFilterDF\n=\nspark\n.\nread\n().\nformat\n(\n\"parquet\"\n)\n.\noption\n(\n\"pathGlobFilter\"\n,\n\"*.parquet\"\n)\n// json file should be filtered out\n.\nload\n(\n\"examples/src/main/resources/dir1\"\n);\ntestGlobFilterDF\n.\nshow\n();\n// +-------------+\n// |         file|\n// +-------------+\n// |file1.parquet|\n// +-------------+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\ndf\n<-\nread.df\n(\n\"examples/src/main/resources/dir1\"\n,\n\"parquet\"\n,\npathGlobFilter\n=\n\"*.parquet\"\n)\n#            file\n# 1 file1.parquet\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nRecursive File Lookup\nrecursiveFileLookup\nis used to recursively load files and it disables partition inferring. Its default", "question": "Where can I find the full example code for JavaSQLDataSourceExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo."], "answer_start": [328]}}
{"context": "ple.R\" in the Spark repo.\nRecursive File Lookup\nrecursiveFileLookup\nis used to recursively load files and it disables partition inferring. Its default value is\nfalse\n.\nIf data source explicitly specifies the\npartitionSpec\nwhen\nrecursiveFileLookup\nis true, exception will be thrown.\nTo load all files recursively, you can use:\nrecursive_loaded_df\n=\nspark\n.\nread\n.\nformat\n(\n\"\nparquet\n\"\n)\n\\\n.\noption\n(\n\"\nrecursiveFileLookup\n\"\n,\n\"\ntrue\n\"\n)\n\\\n.\nload\n(\n\"\nexamples/src/main/resources/dir1\n\"\n)\nrecursive_loaded_df\n.\nshow\n()\n# +-------------+\n# |         file|\n# +-------------+\n# |file1.parquet|\n# |file2.parquet|\n# +-------------+\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\nval\nrecursiveLoadedDF\n=\nspark\n.\nread\n.\nformat\n(\n\"parquet\"\n)\n.\noption\n(\n\"recursiveFileL", "question": "What happens if a data source explicitly specifies the partitionSpec when recursiveFileLookup is true?", "answers": {"text": ["exception will be thrown."], "answer_start": [256]}}
{"context": "\"examples/src/main/python/sql/datasource.py\" in the Spark repo.\nval\nrecursiveLoadedDF\n=\nspark\n.\nread\n.\nformat\n(\n\"parquet\"\n)\n.\noption\n(\n\"recursiveFileLookup\"\n,\n\"true\"\n)\n.\nload\n(\n\"examples/src/main/resources/dir1\"\n)\nrecursiveLoadedDF\n.\nshow\n()\n// +-------------+\n// |         file|\n// +-------------+\n// |file1.parquet|\n// |file2.parquet|\n// +-------------+\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nDataset\n<\nRow\n>\nrecursiveLoadedDF\n=\nspark\n.\nread\n().\nformat\n(\n\"parquet\"\n)\n.\noption\n(\n\"recursiveFileLookup\"\n,\n\"true\"\n)\n.\nload\n(\n\"examples/src/main/resources/dir1\"\n);\nrecursiveLoadedDF\n.\nshow\n();\n// +-------------+\n// |         file|\n// +-------------+\n// |file1.parquet|\n// |file2.parquet|\n// +-------------+\nFind ful", "question": "Where can I find the full example code for the SQLDataSourceExample?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo."], "answer_start": [356]}}
{"context": "ecursiveLoadedDF\n.\nshow\n();\n// +-------------+\n// |         file|\n// +-------------+\n// |file1.parquet|\n// |file2.parquet|\n// +-------------+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\nrecursiveLoadedDF\n<-\nread.df\n(\n\"examples/src/main/resources/dir1\"\n,\n\"parquet\"\n,\nrecursiveFileLookup\n=\n\"true\"\n)\nhead\n(\nrecursiveLoadedDF\n)\n#            file\n# 1 file1.parquet\n# 2 file2.parquet\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nModification Time Path Filters\nmodifiedBefore\nand\nmodifiedAfter\nare options that can be \napplied together or separately in order to achieve greater\ngranularity over which files may load during a Spark batch query.\n(Note that Structured Streaming file", "question": "Where can I find a full example code for JavaSQLDataSourceExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo."], "answer_start": [142]}}
{"context": "ether or separately in order to achieve greater\ngranularity over which files may load during a Spark batch query.\n(Note that Structured Streaming file sources don’t support these options.)\nmodifiedBefore\n: an optional timestamp to only include files with\nmodification times occurring before the specified time. The provided timestamp\nmust be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\nmodifiedAfter\n: an optional timestamp to only include files with\nmodification times occurring after the specified time. The provided timestamp\nmust be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\nWhen a timezone option is not provided, the timestamps will be interpreted according\nto the Spark session timezone (\nspark.sql.session.timeZone\n).\nTo load files wi", "question": "In what format must the timestamps for 'modifiedBefore' and 'modifiedAfter' be provided?", "answers": {"text": ["YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)"], "answer_start": [367]}}
{"context": "ne option is not provided, the timestamps will be interpreted according\nto the Spark session timezone (\nspark.sql.session.timeZone\n).\nTo load files with paths matching a given modified time range, you can use:\n# Only load files modified before 07/1/2050 @ 08:30:00\ndf\n=\nspark\n.\nread\n.\nload\n(\n\"\nexamples/src/main/resources/dir1\n\"\n,\nformat\n=\n\"\nparquet\n\"\n,\nmodifiedBefore\n=\n\"\n2050-07-01T08:30:00\n\"\n)\ndf\n.\nshow\n()\n# +-------------+\n# |         file|\n# +-------------+\n# |file1.parquet|\n# +-------------+\n# Only load files modified after 06/01/2050 @ 08:30:00\ndf\n=\nspark\n.\nread\n.\nload\n(\n\"\nexamples/src/main/resources/dir1\n\"\n,\nformat\n=\n\"\nparquet\n\"\n,\nmodifiedAfter\n=\n\"\n2050-06-01T08:30:00\n\"\n)\ndf\n.\nshow\n()\n# +-------------+\n# |         file|\n# +-------------+\n# +-------------+\nFind full example code at \"ex", "question": "How can you load files with paths matching a given modified time range?", "answers": {"text": ["To load files with paths matching a given modified time range, you can use:"], "answer_start": [134]}}
{"context": "edAfter\n=\n\"\n2050-06-01T08:30:00\n\"\n)\ndf\n.\nshow\n()\n# +-------------+\n# |         file|\n# +-------------+\n# +-------------+\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\nval\nbeforeFilterDF\n=\nspark\n.\nread\n.\nformat\n(\n\"parquet\"\n)\n// Files modified before 07/01/2020 at 05:30 are allowed\n.\noption\n(\n\"modifiedBefore\"\n,\n\"2020-07-01T05:30:00\"\n)\n.\nload\n(\n\"examples/src/main/resources/dir1\"\n);\nbeforeFilterDF\n.\nshow\n();\n// +-------------+\n// |         file|\n// +-------------+\n// |file1.parquet|\n// +-------------+\nval\nafterFilterDF\n=\nspark\n.\nread\n.\nformat\n(\n\"parquet\"\n)\n// Files modified after 06/01/2020 at 05:30 are allowed\n.\noption\n(\n\"modifiedAfter\"\n,\n\"2020-06-01T05:30:00\"\n)\n.\nload\n(\n\"examples/src/main/resources/dir1\"\n);\nafterFilterDF\n.\nshow\n();\n// +------------", "question": "Where can I find the full example code?", "answers": {"text": ["Find full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo."], "answer_start": [121]}}
{"context": " allowed\n.\noption\n(\n\"modifiedAfter\"\n,\n\"2020-06-01T05:30:00\"\n)\n.\nload\n(\n\"examples/src/main/resources/dir1\"\n);\nafterFilterDF\n.\nshow\n();\n// +-------------+\n// |         file|\n// +-------------+\n// +-------------+\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nDataset\n<\nRow\n>\nbeforeFilterDF\n=\nspark\n.\nread\n().\nformat\n(\n\"parquet\"\n)\n// Only load files modified before 7/1/2020 at 05:30\n.\noption\n(\n\"modifiedBefore\"\n,\n\"2020-07-01T05:30:00\"\n)\n// Only load files modified after 6/1/2020 at 05:30\n.\noption\n(\n\"modifiedAfter\"\n,\n\"2020-06-01T05:30:00\"\n)\n// Interpret both times above relative to CST timezone\n.\noption\n(\n\"timeZone\"\n,\n\"CST\"\n)\n.\nload\n(\n\"examples/src/main/resources/dir1\"\n);\nbeforeFilterDF\n.\nshow\n();\n// +-------------+\n", "question": "What is the value set for the \"modifiedAfter\" option when reading the parquet file?", "answers": {"text": ["\"2020-06-01T05:30:00\""], "answer_start": [38]}}
{"context": " relative to CST timezone\n.\noption\n(\n\"timeZone\"\n,\n\"CST\"\n)\n.\nload\n(\n\"examples/src/main/resources/dir1\"\n);\nbeforeFilterDF\n.\nshow\n();\n// +-------------+\n// |         file|\n// +-------------+\n// |file1.parquet|\n// +-------------+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\nbeforeDF\n<-\nread.df\n(\n\"examples/src/main/resources/dir1\"\n,\n\"parquet\"\n,\nmodifiedBefore\n=\n\"2020-07-01T05:30:00\"\n)\n#            file\n# 1 file1.parquet\nafterDF\n<-\nread.df\n(\n\"examples/src/main/resources/dir1\"\n,\n\"parquet\"\n,\nmodifiedAfter\n=\n\"2020-06-01T05:30:00\"\n)\n#            file\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.", "question": "Where can I find the full example code for JavaSQLDataSourceExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo."], "answer_start": [226]}}
{"context": "code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.", "question": "Where can I find the code example?", "answers": {"text": ["examples/src/main/r/RSparkSQLExample.R"], "answer_start": [9]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTrou", "question": "Which file formats are mentioned as data sources in the Spark SQL Guide?", "answers": {"text": ["Parquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files"], "answer_start": [650]}}
{"context": "ataFrame.\nparquetFile\n=\nspark\n.\nread\n.\nparquet\n(\n\"\npeople.parquet\n\"\n)\n# Parquet files can also be used to create a temporary view and then used in SQL statements.\nparquetFile\n.\ncreateOrReplaceTempView\n(\n\"\nparquetFile\n\"\n)\nteenagers\n=\nspark\n.\nsql\n(\n\"\nSELECT name FROM parquetFile WHERE age >= 13 AND age <= 19\n\"\n)\nteenagers\n.\nshow\n()\n# +------+\n# |  name|\n# +------+\n# |Justin|\n# +------+\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\n// Encoders for most common types are automatically provided by importing spark.implicits._\nimport\nspark.implicits._\nval\npeopleDF\n=\nspark\n.\nread\n.\njson\n(\n\"examples/src/main/resources/people.json\"\n)\n// DataFrames can be saved as Parquet files, maintaining the schema information\npeopleDF\n.\nwrite\n.\nparquet\n(\n\"people.parquet\"", "question": "What can be used to create a temporary view and then used in SQL statements?", "answers": {"text": ["Parquet files can also be used to create a temporary view and then used in SQL statements."], "answer_start": [72]}}
{"context": "resources/people.json\"\n)\n// DataFrames can be saved as Parquet files, maintaining the schema information\npeopleDF\n.\nwrite\n.\nparquet\n(\n\"people.parquet\"\n)\n// Read in the parquet file created above\n// Parquet files are self-describing so the schema is preserved\n// The result of loading a Parquet file is also a DataFrame\nval\nparquetFileDF\n=\nspark\n.\nread\n.\nparquet\n(\n\"people.parquet\"\n)\n// Parquet files can also be used to create a temporary view and then used in SQL statements\nparquetFileDF\n.\ncreateOrReplaceTempView\n(\n\"parquetFile\"\n)\nval\nnamesDF\n=\nspark\n.\nsql\n(\n\"SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19\"\n)\nnamesDF\n.\nmap\n(\nattributes\n=>\n\"Name: \"\n+\nattributes\n(\n0\n)).\nshow\n()\n// +------------+\n// |       value|\n// +------------+\n// |Name: Justin|\n// +------------+\nFind full example c", "question": "What is the result of loading a Parquet file?", "answers": {"text": ["The result of loading a Parquet file is also a DataFrame"], "answer_start": [262]}}
{"context": "=>\n\"Name: \"\n+\nattributes\n(\n0\n)).\nshow\n()\n// +------------+\n// |       value|\n// +------------+\n// |Name: Justin|\n// +------------+\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nimport\norg.apache.spark.api.java.function.MapFunction\n;\nimport\norg.apache.spark.sql.Encoders\n;\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\nDataset\n<\nRow\n>\npeopleDF\n=\nspark\n.\nread\n().\njson\n(\n\"examples/src/main/resources/people.json\"\n);\n// DataFrames can be saved as Parquet files, maintaining the schema information\npeopleDF\n.\nwrite\n().\nparquet\n(\n\"people.parquet\"\n);\n// Read in the Parquet file created above.\n// Parquet files are self-describing so the schema is preserved\n// The result of loading a parquet file ", "question": "Where can I find a full example code for the SQLDataSourceExample?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo."], "answer_start": [131]}}
{"context": "S\n.\nshow\n();\n// +------------+\n// |       value|\n// +------------+\n// |Name: Justin|\n// +------------+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\ndf\n<-\nread.df\n(\n\"examples/src/main/resources/people.json\"\n,\n\"json\"\n)\n# SparkDataFrame can be saved as Parquet files, maintaining the schema information.\nwrite.parquet\n(\ndf\n,\n\"people.parquet\"\n)\n# Read in the Parquet file created above. Parquet files are self-describing so the schema is preserved.\n# The result of loading a parquet file is also a DataFrame.\nparquetFile\n<-\nread.parquet\n(\n\"people.parquet\"\n)\n# Parquet files can also be used to create a temporary view and then used in SQL statements.\ncreateOrReplaceTempView\n(\nparquetFile\n,\n\"parquetFile\"\n)\nteenagers\n<-", "question": "Where can I find a full example code for JavaSQLDataSourceExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo."], "answer_start": [103]}}
{"context": "iles can also be used to create a temporary view and then used in SQL statements.\ncreateOrReplaceTempView\n(\nparquetFile\n,\n\"parquetFile\"\n)\nteenagers\n<-\nsql\n(\n\"SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19\"\n)\nhead\n(\nteenagers\n)\n##     name\n## 1 Justin\n# We can also run custom R-UDFs on Spark DataFrames. Here we prefix all the names with \"Name:\"\nschema\n<-\nstructType\n(\nstructField\n(\n\"name\"\n,\n\"string\"\n))\nteenNames\n<-\ndapply\n(\ndf\n,\nfunction\n(\np\n)\n{\ncbind\n(\npaste\n(\n\"Name:\"\n,\np\n$\nname\n))\n},\nschema\n)\nfor\n(\nteenName\nin\ncollect\n(\nteenNames\n)\n$\nname\n)\n{\ncat\n(\nteenName\n,\n\"\\n\"\n)\n}\n## Name: Michael\n## Name: Andy\n## Name: Justin\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nCREATE\nTEMPORARY\nVIEW\nparquetTable\nUSING\norg\n.\napache\n.\nspark\n.\nsql\n.\nparquet\nOP", "question": "Where can the full example code be found?", "answers": {"text": ["Find full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo."], "answer_start": [634]}}
{"context": "e code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nCREATE\nTEMPORARY\nVIEW\nparquetTable\nUSING\norg\n.\napache\n.\nspark\n.\nsql\n.\nparquet\nOPTIONS\n(\npath\n\"examples/src/main/resources/people.parquet\"\n)\nSELECT\n*\nFROM\nparquetTable\nPartition Discovery\nTable partitioning is a common optimization approach used in systems like Hive. In a partitioned\ntable, data are usually stored in different directories, with partitioning column values encoded in\nthe path of each partition directory. All built-in file sources (including Text/CSV/JSON/ORC/Parquet)\nare able to discover and infer partitioning information automatically.\nFor example, we can store all our previously used\npopulation data into a partitioned table using the following directory structure, with two extra\ncolumns,\ngender\nand\ncountr", "question": "What is a common optimization approach used in systems like Hive?", "answers": {"text": ["Table partitioning is a common optimization approach used in systems like Hive."], "answer_start": [257]}}
{"context": "ead.parquet\nor\nSparkSession.read.load\n, Spark SQL\nwill automatically extract the partitioning information from the paths.\nNow the schema of the returned DataFrame becomes:\nroot\n|-- name: string (nullable = true)\n|-- age: long (nullable = true)\n|-- gender: string (nullable = true)\n|-- country: string (nullable = true)\nNotice that the data types of the partitioning columns are automatically inferred. Currently,\nnumeric data types, date, timestamp and string type are supported. Sometimes users may not want\nto automatically infer the data types of the partitioning columns. For these use cases, the\nautomatic type inference can be configured by\nspark.sql.sources.partitionColumnTypeInference.enabled\n, which is default to\ntrue\n. When type\ninference is disabled, string type will be used for the par", "question": "What configuration option controls automatic type inference for partitioning columns?", "answers": {"text": ["spark.sql.sources.partitionColumnTypeInference.enabled"], "answer_start": [647]}}
{"context": "rk.sql.sources.partitionColumnTypeInference.enabled\n, which is default to\ntrue\n. When type\ninference is disabled, string type will be used for the partitioning columns.\nStarting from Spark 1.6.0, partition discovery only finds partitions under the given paths\nby default. For the above example, if users pass\npath/to/table/gender=male\nto either\nSparkSession.read.parquet\nor\nSparkSession.read.load\n,\ngender\nwill not be considered as a\npartitioning column. If users need to specify the base path that partition discovery\nshould start with, they can set\nbasePath\nin the data source options. For example,\nwhen\npath/to/table/gender=male\nis the path of the data and\nusers set\nbasePath\nto\npath/to/table/\n,\ngender\nwill be a partitioning column.\nSchema Merging\nLike Protocol Buffer, Avro, and Thrift, Parquet ", "question": "What happens when type inference is disabled?", "answers": {"text": ["string type will be used for the partitioning columns."], "answer_start": [114]}}
{"context": "et\n(\n\"data/test_table/key=2\"\n)\n// Read the partitioned table\nval\nmergedDF\n=\nspark\n.\nread\n.\noption\n(\n\"mergeSchema\"\n,\n\"true\"\n).\nparquet\n(\n\"data/test_table\"\n)\nmergedDF\n.\nprintSchema\n()\n// The final schema consists of all 3 columns in the Parquet files together\n// with the partitioning column appeared in the partition directory paths\n// root\n//  |-- value: int (nullable = true)\n//  |-- square: int (nullable = true)\n//  |-- cube: int (nullable = true)\n//  |-- key: int (nullable = true)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nimport\ncom.google.common.collect.Lists\n;\nimport\njava.io.Serializable\n;\nimport\njava.util.ArrayList\n;\nimport\njava.util.Arrays\n;\nimport\njava.util.Collections\n;\nimport\njava.util.List\n;\nimpor", "question": "What is the final schema of the merged DataFrame?", "answers": {"text": ["// The final schema consists of all 3 columns in the Parquet files together\n// with the partitioning column appeared in the partition directory paths\n// root\n//  |-- value: int (nullable = true)\n//  |-- square: int (nullable = true)\n//  |-- cube: int (nullable = true)\n//  |-- key: int (nullable = true)"], "answer_start": [182]}}
{"context": "uares\n.\nadd\n(\nsquare\n);\n}\n// Create a simple DataFrame, store into a partition directory\nDataset\n<\nRow\n>\nsquaresDF\n=\nspark\n.\ncreateDataFrame\n(\nsquares\n,\nSquare\n.\nclass\n);\nsquaresDF\n.\nwrite\n().\nparquet\n(\n\"data/test_table/key=1\"\n);\nList\n<\nCube\n>\ncubes\n=\nnew\nArrayList\n<>();\nfor\n(\nint\nvalue\n=\n6\n;\nvalue\n<=\n10\n;\nvalue\n++)\n{\nCube\ncube\n=\nnew\nCube\n();\ncube\n.\nsetValue\n(\nvalue\n);\ncube\n.\nsetCube\n(\nvalue\n*\nvalue\n*\nvalue\n);\ncubes\n.\nadd\n(\ncube\n);\n}\n// Create another DataFrame in a new partition directory,\n// adding a new column and dropping an existing column\nDataset\n<\nRow\n>\ncubesDF\n=\nspark\n.\ncreateDataFrame\n(\ncubes\n,\nCube\n.\nclass\n);\ncubesDF\n.\nwrite\n().\nparquet\n(\n\"data/test_table/key=2\"\n);\n// Read the partitioned table\nDataset\n<\nRow\n>\nmergedDF\n=\nspark\n.\nread\n().\noption\n(\n\"mergeSchema\"\n,\ntrue\n).\nparquet\n(", "question": "What is the path used to write the `squaresDF` DataFrame to a parquet file?", "answers": {"text": ["\"data/test_table/key=1\""], "answer_start": [203]}}
{"context": "quet\n(\n\"data/test_table/key=2\"\n);\n// Read the partitioned table\nDataset\n<\nRow\n>\nmergedDF\n=\nspark\n.\nread\n().\noption\n(\n\"mergeSchema\"\n,\ntrue\n).\nparquet\n(\n\"data/test_table\"\n);\nmergedDF\n.\nprintSchema\n();\n// The final schema consists of all 3 columns in the Parquet files together\n// with the partitioning column appeared in the partition directory paths\n// root\n//  |-- value: int (nullable = true)\n//  |-- square: int (nullable = true)\n//  |-- cube: int (nullable = true)\n//  |-- key: int (nullable = true)\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\ndf1\n<-\ncreateDataFrame\n(\ndata.frame\n(\nsingle\n=\nc\n(\n12\n,\n29\n),\ndouble\n=\nc\n(\n19\n,\n23\n)))\ndf2\n<-\ncreateDataFrame\n(\ndata.frame\n(\ndouble\n=\nc\n(\n19\n,\n23\n),\ntriple\n=\nc\n(\n23\n,\n", "question": "What is the final schema of the merged DataFrame?", "answers": {"text": ["// root\n//  |-- value: int (nullable = true)\n//  |-- square: int (nullable = true)\n//  |-- cube: int (nullable = true)\n//  |-- key: int (nullable = true)"], "answer_start": [349]}}
{"context": "Frame\n(\ndata.frame\n(\nsingle\n=\nc\n(\n12\n,\n29\n),\ndouble\n=\nc\n(\n19\n,\n23\n)))\ndf2\n<-\ncreateDataFrame\n(\ndata.frame\n(\ndouble\n=\nc\n(\n19\n,\n23\n),\ntriple\n=\nc\n(\n23\n,\n18\n)))\n# Create a simple DataFrame, stored into a partition directory\nwrite.df\n(\ndf1\n,\n\"data/test_table/key=1\"\n,\n\"parquet\"\n,\n\"overwrite\"\n)\n# Create another DataFrame in a new partition directory,\n# adding a new column and dropping an existing column\nwrite.df\n(\ndf2\n,\n\"data/test_table/key=2\"\n,\n\"parquet\"\n,\n\"overwrite\"\n)\n# Read the partitioned table\ndf3\n<-\nread.df\n(\n\"data/test_table\"\n,\n\"parquet\"\n,\nmergeSchema\n=\n\"true\"\n)\nprintSchema\n(\ndf3\n)\n# The final schema consists of all 3 columns in the Parquet files together\n# with the partitioning column appeared in the partition directory paths\n## root\n##  |-- single: double (nullable = true)\n##  |-- doubl", "question": "What is the final schema of df3 after reading the partitioned table?", "answers": {"text": ["## root\n##  |-- single: double (nullable = true)\n##  |-- doubl"], "answer_start": [738]}}
{"context": "files together\n# with the partitioning column appeared in the partition directory paths\n## root\n##  |-- single: double (nullable = true)\n##  |-- double: double (nullable = true)\n##  |-- triple: double (nullable = true)\n##  |-- key: integer (nullable = true)\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nHive metastore Parquet table conversion\nWhen reading from Hive metastore Parquet tables and writing to non-partitioned Hive metastore\nParquet tables, Spark SQL will try to use its own Parquet support instead of Hive SerDe for\nbetter performance. This behavior is controlled by the\nspark.sql.hive.convertMetastoreParquet\nconfiguration, and is turned on by default.\nHive/Parquet Schema Reconciliation\nThere are two key differences between Hive and Parquet fr", "question": "What controls the behavior of Spark SQL using its own Parquet support instead of Hive SerDe when reading from and writing to Hive metastore Parquet tables?", "answers": {"text": ["spark.sql.hive.convertMetastoreParquet"], "answer_start": [625]}}
{"context": "ent\nmetadata.\n# spark is an existing SparkSession\nspark\n.\ncatalog\n.\nrefreshTable\n(\n\"\nmy_table\n\"\n)\n// spark is an existing SparkSession\nspark\n.\ncatalog\n.\nrefreshTable\n(\n\"my_table\"\n)\n// spark is an existing SparkSession\nspark\n.\ncatalog\n().\nrefreshTable\n(\n\"my_table\"\n);\nrefreshTable\n(\n\"my_table\"\n)\nREFRESH\nTABLE\nmy_table\n;\nColumnar Encryption\nSince Spark 3.2, columnar encryption is supported for Parquet tables with Apache Parquet 1.12+.\nParquet uses the envelope encryption practice, where file parts are encrypted with “data encryption keys” (DEKs), and the DEKs are encrypted with “master encryption keys” (MEKs). The DEKs are randomly generated by Parquet for each encrypted file/column. The MEKs are generated, stored and managed in a Key Management Service (KMS) of user’s choice. The Parquet Mav", "question": "Since which Spark version is columnar encryption supported for Parquet tables?", "answers": {"text": ["Since Spark 3.2, columnar encryption is supported for Parquet tables with Apache Parquet 1.12+."], "answer_start": [340]}}
{"context": ".encryption.key.list=\\\n#           \"keyA:AAECAwQFBgcICQoLDA0ODw== ,  keyB:AAECAAECAAECAAECAAECAA==\"\\\n# --conf spark.hadoop.parquet.crypto.factory.class=\\\n#           \"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\"\n# Write encrypted dataframe files.\n# Column \"square\" will be protected with master key \"keyA\".\n# Parquet file footers will be protected with master key \"keyB\"\nsquaresDF\n.\nwrite\n\\\n.\noption\n(\n\"\nparquet.encryption.column.keys\n\"\n,\n\"\nkeyA:square\n\"\n)\n\\\n.\noption\n(\n\"\nparquet.encryption.footer.key\n\"\n,\n\"\nkeyB\n\"\n)\n\\\n.\nparquet\n(\n\"\n/path/to/table.parquet.encrypted\n\"\n)\n# Read encrypted dataframe files\ndf2\n=\nspark\n.\nread\n.\nparquet\n(\n\"\n/path/to/table.parquet.encrypted\n\"\n)\nsc\n.\nhadoopConfiguration\n.\nset\n(\n\"parquet.encryption.kms.client.class\"\n,\n\"org.apache.parquet.crypto.keytoo", "question": "Which master key is used to protect the Parquet file footers?", "answers": {"text": ["keyB"], "answer_start": [69]}}
{"context": "et\n(\n\"\n/path/to/table.parquet.encrypted\n\"\n)\nsc\n.\nhadoopConfiguration\n.\nset\n(\n\"parquet.encryption.kms.client.class\"\n,\n\"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\"\n)\n// Explicit master keys (base64 encoded) - required only for mock InMemoryKMS\nsc\n.\nhadoopConfiguration\n.\nset\n(\n\"parquet.encryption.key.list\"\n,\n\"keyA:AAECAwQFBgcICQoLDA0ODw== ,  keyB:AAECAAECAAECAAECAAECAA==\"\n)\n// Activate Parquet encryption, driven by Hadoop properties\nsc\n.\nhadoopConfiguration\n.\nset\n(\n\"parquet.crypto.factory.class\"\n,\n\"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\"\n)\n// Write encrypted dataframe files.\n// Column \"square\" will be protected with master key \"keyA\".\n// Parquet file footers will be protected with master key \"keyB\"\nsquaresDF\n.\nwrite\n.\noption\n(\n\"parquet.encryption.column.key", "question": "Which class is set for parquet encryption KMS client?", "answers": {"text": ["org.apache.parquet.crypto.keytools.mocks.InMemoryKMS"], "answer_start": [118]}}
{"context": "d with master key \"keyA\".\n// Parquet file footers will be protected with master key \"keyB\"\nsquaresDF\n.\nwrite\n.\noption\n(\n\"parquet.encryption.column.keys\"\n,\n\"keyA:square\"\n).\noption\n(\n\"parquet.encryption.footer.key\"\n,\n\"keyB\"\n).\nparquet\n(\n\"/path/to/table.parquet.encrypted\"\n)\n// Read encrypted dataframe files\nval\ndf2\n=\nspark\n.\nread\n.\nparquet\n(\n\"/path/to/table.parquet.encrypted\"\n)\nsc\n.\nhadoopConfiguration\n().\nset\n(\n\"parquet.encryption.kms.client.class\"\n,\n\"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\"\n);\n// Explicit master keys (base64 encoded) - required only for mock InMemoryKMS\nsc\n.\nhadoopConfiguration\n().\nset\n(\n\"parquet.encryption.key.list\"\n,\n\"keyA:AAECAwQFBgcICQoLDA0ODw== ,  keyB:AAECAAECAAECAAECAAECAA==\"\n);\n// Activate Parquet encryption, driven by Hadoop properties\nsc\n.\nhadoopConfi", "question": "With which master key will the Parquet file footers be protected?", "answers": {"text": ["keyB"], "answer_start": [85]}}
{"context": "st\"\n,\n\"keyA:AAECAwQFBgcICQoLDA0ODw== ,  keyB:AAECAAECAAECAAECAAECAA==\"\n);\n// Activate Parquet encryption, driven by Hadoop properties\nsc\n.\nhadoopConfiguration\n().\nset\n(\n\"parquet.crypto.factory.class\"\n,\n\"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\"\n);\n// Write encrypted dataframe files.\n// Column \"square\" will be protected with master key \"keyA\".\n// Parquet file footers will be protected with master key \"keyB\"\nsquaresDF\n.\nwrite\n().\noption\n(\n\"parquet.encryption.column.keys\"\n,\n\"keyA:square\"\n).\noption\n(\n\"parquet.encryption.footer.key\"\n,\n\"keyB\"\n).\nparquet\n(\n\"/path/to/table.parquet.encrypted\"\n);\n// Read encrypted dataframe files\nDataset\n<\nRow\n>\ndf2\n=\nspark\n.\nread\n().\nparquet\n(\n\"/path/to/table.parquet.encrypted\"\n);\nKMS Client\nThe InMemoryKMS class is provided only for illustr", "question": "With which master key will the column \"square\" be protected?", "answers": {"text": ["keyA:square"], "answer_start": [501]}}
{"context": "apKey\n(\nbyte\n[]\nkeyBytes\n,\nString\nmasterKeyIdentifier\n);\n// Decrypts (unwraps) a key with the master key.\npublic\nbyte\n[]\nunwrapKey\n(\nString\nwrappedKey\n,\nString\nmasterKeyIdentifier\n);\n// Use of initialization parameters is optional.\npublic\nvoid\ninitialize\n(\nConfiguration\nconfiguration\n,\nString\nkmsInstanceID\n,\nString\nkmsInstanceURL\n,\nString\naccessToken\n);\n}\nAn\nexample\nof such class for an open source\nKMS\ncan be found in the parquet-java repository. The production KMS client should be designed in cooperation with organization’s security administrators, and built by developers with an experience in access control management. Once such class is created, it can be passed to applications via the\nparquet.encryption.kms.client.class\nparameter and leveraged by general Spark users as shown in the enc", "question": "What is the purpose of the `unwrapKey` method?", "answers": {"text": ["// Decrypts (unwraps) a key with the master key."], "answer_start": [57]}}
{"context": "eated, it can be passed to applications via the\nparquet.encryption.kms.client.class\nparameter and leveraged by general Spark users as shown in the encrypted dataframe write/read sample above.\nNote: By default, Parquet implements a “double envelope encryption” mode, that minimizes the interaction of Spark executors with a KMS server. In this mode, the DEKs are encrypted with “key encryption keys” (KEKs, randomly generated by Parquet). The KEKs are encrypted with MEKs in KMS; the result and the KEK itself are cached in Spark executor memory. Users interested in regular envelope encryption, can switch to it by setting the\nparquet.encryption.double.wrapping\nparameter to\nfalse\n. For more details on Parquet encryption parameters, visit the parquet-hadoop configuration\npage\n.\nData Source Option\nD", "question": "How can users switch to regular envelope encryption in Parquet?", "answers": {"text": ["Users interested in regular envelope encryption, can switch to it by setting the\nparquet.encryption.double.wrapping\nparameter to\nfalse"], "answer_start": [546]}}
{"context": ".\nCORRECTED\n: loads INT96 timestamps without rebasing.\nLEGACY\n: performs rebasing of ancient timestamps from the Julian to Proleptic Gregorian calendar.\nread\nmergeSchema\n(value of\nspark.sql.parquet.mergeSchema\nconfiguration)\nSets whether we should merge schemas collected from all Parquet part-files. This will override\nspark.sql.parquet.mergeSchema\n.\nread\ncompression\nsnappy\nCompression codec to use when saving to file. This can be one of the known case-insensitive shorten names (none, uncompressed, snappy, gzip, lzo, brotli, lz4, lz4_raw, and zstd). This will override\nspark.sql.parquet.compression.codec\n.\nwrite\nOther generic options can be found in\nGeneric Files Source Options\nConfiguration\nConfiguration of Parquet can be done via\nspark.conf.set\nor by running\nSET key=value\ncommands using SQ", "question": "What does the 'read mergeSchema' option do?", "answers": {"text": ["Sets whether we should merge schemas collected from all Parquet part-files. This will override\nspark.sql.parquet.mergeSchema"], "answer_start": [225]}}
{"context": "IMESTAMP_MICROS\n    is a standard timestamp type in Parquet, which stores number of microseconds from the\n    Unix epoch. TIMESTAMP_MILLIS is also standard, but with millisecond precision, which\n    means Spark has to truncate the microsecond portion of its timestamp value.\n2.3.0\nspark.sql.parquet.compression.codec\nsnappy\nSets the compression codec used when writing Parquet files. If either\ncompression\nor\nparquet.compression\nis specified in the table-specific options/properties, the precedence would be\ncompression\n,\nparquet.compression\n,\nspark.sql.parquet.compression.codec\n. Acceptable values include:\n    none, uncompressed, snappy, gzip, lzo, brotli, lz4, lz4_raw, zstd.\n    Note that\nbrotli\nrequires\nBrotliCodec\nto be installed.\n1.1.1\nspark.sql.parquet.filterPushdown\ntrue\nEnables Parquet f", "question": "What are the acceptable values for the compression codec when writing Parquet files?", "answers": {"text": ["none, uncompressed, snappy, gzip, lzo, brotli, lz4, lz4_raw, zstd."], "answer_start": [613]}}
{"context": "enabled before knowing what it means exactly.\n1.5.0\nspark.sql.parquet.writeLegacyFormat\nfalse\nIf true, data will be written in a way of Spark 1.4 and earlier. For example, decimal values\n    will be written in Apache Parquet's fixed-length byte array format, which other systems such as\n    Apache Hive and Apache Impala use. If false, the newer format in Parquet will be used. For\n    example, decimals will be written in int-based format. If Parquet output is intended for use\n    with systems that do not support this newer format, set to true.\n1.6.0\nspark.sql.parquet.enableVectorizedReader\ntrue\nEnables vectorized parquet decoding.\n2.0.0\nspark.sql.parquet.enableNestedColumnVectorizedReader\ntrue\nEnables vectorized Parquet decoding for nested columns (e.g., struct, list, map).\n    Requires\nspar", "question": "What happens when spark.sql.parquet.writeLegacyFormat is set to true?", "answers": {"text": ["If true, data will be written in a way of Spark 1.4 and earlier."], "answer_start": [94]}}
{"context": "ql.parquet.enableNestedColumnVectorizedReader\ntrue\nEnables vectorized Parquet decoding for nested columns (e.g., struct, list, map).\n    Requires\nspark.sql.parquet.enableVectorizedReader\nto be enabled.\n3.3.0\nspark.sql.parquet.recordLevelFilter.enabled\nfalse\nIf true, enables Parquet's native record-level filtering using the pushed down filters.\n    This configuration only has an effect when\nspark.sql.parquet.filterPushdown\nis enabled and the vectorized reader is not used. You can ensure the vectorized reader\n    is not used by setting\nspark.sql.parquet.enableVectorizedReader\nto false.\n2.3.0\nspark.sql.parquet.columnarReaderBatchSize\n4096\nThe number of rows to include in a parquet vectorized reader batch. The number should\n    be carefully chosen to minimize overhead and avoid OOMs in reading", "question": "What does 'spark.sql.parquet.recordLevelFilter.enabled' do?", "answers": {"text": ["If true, enables Parquet's native record-level filtering using the pushed down filters."], "answer_start": [258]}}
{"context": ".\n3.3.0\nspark.sql.parquet.fieldId.read.ignoreMissing\nfalse\nWhen the Parquet file doesn't have any field IDs but the\n    Spark read schema is using field IDs to read, we will silently return nulls\n    when this flag is enabled, or error otherwise.\n3.3.0\nspark.sql.parquet.inferTimestampNTZ.enabled\ntrue\nWhen enabled, Parquet timestamp columns with annotation\nisAdjustedToUTC = false\nare inferred as TIMESTAMP_NTZ type during schema inference. Otherwise, all the Parquet\n    timestamp columns are inferred as TIMESTAMP_LTZ types. Note that Spark writes the\n    output schema into Parquet's footer metadata on file writing and leverages it on file\n    reading. Thus this configuration only affects the schema inference on Parquet files\n    which are not written by Spark.\n3.4.0\nspark.sql.parquet.datetim", "question": "What happens when a Parquet file doesn't have field IDs but the Spark read schema is using them, and the flag `spark.sql.parquet.fieldId.read.ignoreMissing` is enabled?", "answers": {"text": ["we will silently return nulls"], "answer_start": [166]}}
{"context": "r when reading Parquet files.\nThis config is only effective if the writer info (like Spark, Hive) of the Parquet files is unknown.\n3.0.0\nspark.sql.parquet.datetimeRebaseModeInWrite\nEXCEPTION\nThe rebasing mode for the values of the\nDATE\n,\nTIMESTAMP_MILLIS\n,\nTIMESTAMP_MICROS\nlogical types from the Proleptic Gregorian to Julian calendar:\nEXCEPTION\n: Spark will fail the writing if it sees ancient dates/timestamps that are ambiguous between the two calendars.\nCORRECTED\n: Spark will not do rebase and write the dates/timestamps as it is.\nLEGACY\n: Spark will rebase dates/timestamps from Proleptic Gregorian calendar to the legacy hybrid (Julian + Gregorian) calendar when writing Parquet files.\n3.0.0\nspark.sql.parquet.int96RebaseModeInRead\nEXCEPTION\nThe rebasing mode for the values of the\nINT96\ntime", "question": "What does 'EXCEPTION' mean for spark.sql.parquet.datetimeRebaseModeInWrite?", "answers": {"text": ["Spark will fail the writing if it sees ancient dates/timestamps that are ambiguous between the two calendars."], "answer_start": [349]}}
{"context": "orian) calendar when writing Parquet files.\n3.0.0\nspark.sql.parquet.int96RebaseModeInRead\nEXCEPTION\nThe rebasing mode for the values of the\nINT96\ntimestamp type from the Julian to Proleptic Gregorian calendar:\nEXCEPTION\n: Spark will fail the reading if it sees ancient INT96 timestamps that are ambiguous between the two calendars.\nCORRECTED\n: Spark will not do rebase and read the dates/timestamps as it is.\nLEGACY\n: Spark will rebase INT96 timestamps from the legacy hybrid (Julian + Gregorian) calendar to Proleptic Gregorian calendar when reading Parquet files.\nThis config is only effective if the writer info (like Spark, Hive) of the Parquet files is unknown.\n3.1.0\nspark.sql.parquet.int96RebaseModeInWrite\nEXCEPTION\nThe rebasing mode for the values of the\nINT96\ntimestamp type from the Prolep", "question": "What happens when Spark encounters ancient INT96 timestamps that are ambiguous between the Julian and Proleptic Gregorian calendars with the EXCEPTION rebasing mode?", "answers": {"text": ["Spark will fail the reading if it sees ancient INT96 timestamps that are ambiguous between the two calendars."], "answer_start": [222]}}
{"context": "iles is unknown.\n3.1.0\nspark.sql.parquet.int96RebaseModeInWrite\nEXCEPTION\nThe rebasing mode for the values of the\nINT96\ntimestamp type from the Proleptic Gregorian to Julian calendar:\nEXCEPTION\n: Spark will fail the writing if it sees ancient timestamps that are ambiguous between the two calendars.\nCORRECTED\n: Spark will not do rebase and write the dates/timestamps as it is.\nLEGACY\n: Spark will rebase INT96 timestamps from Proleptic Gregorian calendar to the legacy hybrid (Julian + Gregorian) calendar when writing Parquet files.\n3.1.0", "question": "What happens if Spark encounters ancient timestamps ambiguous between the Proleptic Gregorian and Julian calendars when writing?", "answers": {"text": ["Spark will fail the writing if it sees ancient timestamps that are ambiguous between the two calendars."], "answer_start": [196]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTrou", "question": "Which file formats are mentioned as data sources in the Spark SQL Guide?", "answers": {"text": ["Parquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files"], "answer_start": [650]}}
{"context": "Parquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTroubleshooting\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nORC Files\nORC Implementation\nVectorized Reader\nSchema Merging\nZstandard\nBloom Filters\nColumnar Encryption\nHive metastore ORC table conversion\nConfiguration\nData Source Option\nApache ORC\nis a columnar format which has more advanced features like native zstd compression, bloom filter and columnar encryption.\nORC Implementation\nSpark supports two ORC implementations (\nnative\nand\nhive\n) which is controlled by\nspark.sql.orc.impl\n.\nTwo implementations share most functionalities with different design g", "question": "What controls which ORC implementation Spark uses?", "answers": {"text": ["spark.sql.orc.impl"], "answer_start": [709]}}
{"context": "a simple schema, and gradually add more columns to the schema as needed. In this way, users may end\nup with multiple ORC files with different but mutually compatible schemas. The ORC data\nsource is now able to automatically detect this case and merge schemas of all these files.\nSince schema merging is a relatively expensive operation, and is not a necessity in most cases, we\nturned it off by default . You may enable it by\nsetting data source option\nmergeSchema\nto\ntrue\nwhen reading ORC files, or\nsetting the global SQL option\nspark.sql.orc.mergeSchema\nto\ntrue\n.\nZstandard\nSince Spark 3.2, you can take advantage of Zstandard compression in ORC files.\nPlease see\nZstandard\nfor the benefits.\nCREATE\nTABLE\ncompressed\n(\nkey\nSTRING\n,\nvalue\nSTRING\n)\nUSING\nORC\nOPTIONS\n(\ncompression\n'zstd'\n)\nBloom Filte", "question": "How can schema merging be enabled when reading ORC files?", "answers": {"text": ["You may enable it by\nsetting data source option\nmergeSchema\nto\ntrue\nwhen reading ORC files, or\nsetting the global SQL option\nspark.sql.orc.mergeSchema\nto\ntrue"], "answer_start": [405]}}
{"context": "d\n'1.0'\n,\norc\n.\ncolumn\n.\nencoding\n.\ndirect\n'name'\n)\nColumnar Encryption\nSince Spark 3.2, columnar encryption is supported for ORC tables with Apache ORC 1.6.\nThe following example is using Hadoop KMS as a key provider with the given location.\nPlease visit\nApache Hadoop KMS\nfor the detail.\nCREATE\nTABLE\nencrypted\n(\nssn\nSTRING\n,\nemail\nSTRING\n,\nname\nSTRING\n)\nUSING\nORC\nOPTIONS\n(\nhadoop\n.\nsecurity\n.\nkey\n.\nprovider\n.\npath\n\"kms://http@localhost:9600/kms\"\n,\norc\n.\nkey\n.\nprovider\n\"hadoop\"\n,\norc\n.\nencrypt\n\"pii:ssn,email\"\n,\norc\n.\nmask\n\"nullify:ssn;sha256:email\"\n)\nHive metastore ORC table conversion\nWhen reading from Hive metastore ORC tables and inserting to Hive metastore ORC tables, Spark SQL will try to use its own ORC support instead of Hive SerDe for better performance. For CTAS statement, only no", "question": "Since which Spark version is columnar encryption supported for ORC tables?", "answers": {"text": ["Since Spark 3.2, columnar encryption is supported for ORC tables with Apache ORC 1.6."], "answer_start": [72]}}
{"context": "ion. If\nfalse\n,\n      a new non-vectorized ORC reader is used in\nnative\nimplementation.\n      For\nhive\nimplementation, this is ignored.\n2.3.0\nspark.sql.orc.columnarReaderBatchSize\n4096\nThe number of rows to include in an orc vectorized reader batch. The number should\n      be carefully chosen to minimize overhead and avoid OOMs in reading data.\n2.4.0\nspark.sql.orc.columnarWriterBatchSize\n1024\nThe number of rows to include in an orc vectorized writer batch. The number should\n      be carefully chosen to minimize overhead and avoid OOMs in writing data.\n3.4.0\nspark.sql.orc.enableNestedColumnVectorizedReader\ntrue\nEnables vectorized orc decoding in\nnative\nimplementation for nested data types\n      (array, map and struct). If\nspark.sql.orc.enableVectorizedReader\nis set to\nfalse\n, this is ignore", "question": "What happens if spark.sql.orc.enableVectorizedReader is set to false in the native implementation?", "answers": {"text": ["a new non-vectorized ORC reader is used in\nnative\nimplementation."], "answer_start": [22]}}
{"context": "rgeSchema\n. The default value is specified in\nspark.sql.orc.mergeSchema\n.\nread\ncompression\nzstd\ncompression codec to use when saving to file. This can be one of the known case-insensitive shorten names (none, snappy, zlib, lzo, zstd, lz4 and brotli). This will override\norc.compress\nand\nspark.sql.orc.compression.codec\n. Note that\nbrotli\nrequires\nbrotli4j\nto be installed.\nwrite\nOther generic options can be found in\nGeneric File Source Options\n.", "question": "What compression codecs are known and case-insensitive shorten names?", "answers": {"text": ["none, snappy, zlib, lzo, zstd, lz4 and brotli"], "answer_start": [203]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTrou", "question": "Which file formats are mentioned as data sources in the Spark SQL Guide?", "answers": {"text": ["Parquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files"], "answer_start": [650]}}
{"context": "Parquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTroubleshooting\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nJSON Files\nSpark SQL can automatically infer the schema of a JSON dataset and load it as a DataFrame.\nThis conversion can be done using\nSparkSession.read.json\non a JSON file.\nNote that the file that is offered as\na json file\nis not a typical JSON file. Each\nline must contain a separate, self-contained valid JSON object. For more information, please see\nJSON Lines text format, also called newline-delimited JSON\n.\nFor a regular multi-line JSON file, set the\nmultiLine\nparameter to\nTrue\n.\n# spark is", "question": "How can Spark SQL load a JSON dataset?", "answers": {"text": ["Spark SQL can automatically infer the schema of a JSON dataset and load it as a DataFrame."], "answer_start": [311]}}
{"context": "For a regular multi-line JSON file, set the\nmultiLine\noption to\ntrue\n.\n// Primitive types (Int, String, etc) and Product types (case classes) encoders are\n// supported by importing this when creating a Dataset.\nimport\nspark.implicits._\n// A JSON dataset is pointed to by path.\n// The path can be either a single text file or a directory storing text files\nval\npath\n=\n\"examples/src/main/resources/people.json\"\nval\npeopleDF\n=\nspark\n.\nread\n.\njson\n(\npath\n)\n// The inferred schema can be visualized using the printSchema() method\npeopleDF\n.\nprintSchema\n()\n// root\n//  |-- age: long (nullable = true)\n//  |-- name: string (nullable = true)\n// Creates a temporary view using the DataFrame\npeopleDF\n.\ncreateOrReplaceTempView\n(\n\"people\"\n)\n// SQL statements can be run by using the sql methods provided by spar", "question": "How is a regular multi-line JSON file configured?", "answers": {"text": ["set the\nmultiLine\noption to\ntrue"], "answer_start": [36]}}
{"context": "otherPeople\n.\nshow\n()\n// +---------------+----+\n// |        address|name|\n// +---------------+----+\n// |[Columbus,Ohio]| Yin|\n// +---------------+----+\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nSpark SQL can automatically infer the schema of a JSON dataset and load it as a\nDataset<Row>\n.\nThis conversion can be done using\nSparkSession.read().json()\non either a\nDataset<String>\n,\nor a JSON file.\nNote that the file that is offered as\na json file\nis not a typical JSON file. Each\nline must contain a separate, self-contained valid JSON object. For more information, please see\nJSON Lines text format, also called newline-delimited JSON\n.\nFor a regular multi-line JSON file, set the\nmultiLine\noption to\ntrue\n.\nimport", "question": "How can Spark SQL load a JSON dataset?", "answers": {"text": ["SparkSession.read().json()"], "answer_start": [409]}}
{"context": " please see\nJSON Lines text format, also called newline-delimited JSON\n.\nFor a regular multi-line JSON file, set the\nmultiLine\noption to\ntrue\n.\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\n// A JSON dataset is pointed to by path.\n// The path can be either a single text file or a directory storing text files\nDataset\n<\nRow\n>\npeople\n=\nspark\n.\nread\n().\njson\n(\n\"examples/src/main/resources/people.json\"\n);\n// The inferred schema can be visualized using the printSchema() method\npeople\n.\nprintSchema\n();\n// root\n//  |-- age: long (nullable = true)\n//  |-- name: string (nullable = true)\n// Creates a temporary view using the DataFrame\npeople\n.\ncreateOrReplaceTempView\n(\n\"people\"\n);\n// SQL statements can be run by using the sql methods provided by spark\nDataset\n<\nRow\n>\nnamesDF", "question": "What is the purpose of the `printSchema()` method?", "answers": {"text": ["The inferred schema can be visualized using the printSchema() method"], "answer_start": [433]}}
{"context": " = true)\n##  |-- name: string (nullable = true)\n# Register this DataFrame as a table.\ncreateOrReplaceTempView\n(\npeople\n,\n\"people\"\n)\n# SQL statements can be run by using the sql methods.\nteenagers\n<-\nsql\n(\n\"SELECT name FROM people WHERE age >= 13 AND age <= 19\"\n)\nhead\n(\nteenagers\n)\n##     name\n## 1 Justin\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nCREATE\nTEMPORARY\nVIEW\njsonTable\nUSING\norg\n.\napache\n.\nspark\n.\nsql\n.\njson\nOPTIONS\n(\npath\n\"examples/src/main/resources/people.json\"\n)\nSELECT\n*\nFROM\njsonTable\nData Source Option\nData source options of JSON can be set via:\nthe\n.option\n/\n.options\nmethods of\nDataFrameReader\nDataFrameWriter\nDataStreamReader\nDataStreamWriter\nthe built-in functions below\nfrom_json\nto_json\nschema_of_json\nOPTIONS\nclause at\nCREATE TAB", "question": "How can SQL statements be run?", "answers": {"text": ["SQL statements can be run by using the sql methods."], "answer_start": [134]}}
{"context": "ameReader\nDataFrameWriter\nDataStreamReader\nDataStreamWriter\nthe built-in functions below\nfrom_json\nto_json\nschema_of_json\nOPTIONS\nclause at\nCREATE TABLE USING DATA_SOURCE\nProperty Name\nDefault\nMeaning\nScope\ntimeZone\n(value of\nspark.sql.session.timeZone\nconfiguration)\nSets the string that indicates a time zone ID to be used to format timestamps in the JSON datasources or partition values. The following formats of\ntimeZone\nare supported:\nRegion-based zone ID: It should have the form 'area/city', such as 'America/Los_Angeles'.\nZone offset: It should be in the format '(+|-)HH:mm', for example '-08:00' or '+01:00'. Also 'UTC' and 'Z' are supported as aliases of '+00:00'.\nOther short names like 'CST' are not recommended to use because they can be ambiguous.\nread/write\nprimitivesAsString\nfalse\nIn", "question": "Quais formatos de timeZone são suportados?", "answers": {"text": ["Region-based zone ID: It should have the form 'area/city', such as 'America/Los_Angeles'.\nZone offset: It should be in the format '(+|-)HH:mm', for example '-08:00' or '+01:00'. Also 'UTC' and 'Z' are supported as aliases of '+00:00'."], "answer_start": [440]}}
{"context": " as aliases of '+00:00'.\nOther short names like 'CST' are not recommended to use because they can be ambiguous.\nread/write\nprimitivesAsString\nfalse\nInfers all primitive values as a string type.\nread\nprefersDecimal\nfalse\nInfers all floating-point values as a decimal type. If the values do not fit in decimal, then it infers them as doubles.\nread\nallowComments\nfalse\nIgnores Java/C++ style comment in JSON records.\nread\nallowUnquotedFieldNames\nfalse\nAllows unquoted JSON field names.\nread\nallowSingleQuotes\ntrue\nAllows single quotes in addition to double quotes.\nread\nallowNumericLeadingZeros\nfalse\nAllows leading zeros in numbers (e.g. 00012).\nread\nallowBackslashEscapingAnyCharacter\nfalse\nAllows accepting quoting of all character using backslash quoting mechanism.\nread\nmode\nPERMISSIVE\nAllows a mod", "question": "What does setting 'allowSingleQuotes' to true allow?", "answers": {"text": ["Allows single quotes in addition to double quotes."], "answer_start": [511]}}
{"context": "put schema.\nDROPMALFORMED\n: ignores the whole corrupted records. This mode is unsupported in the JSON built-in functions.\nFAILFAST\n: throws an exception when it meets corrupted records.\nread\ncolumnNameOfCorruptRecord\n(value of\nspark.sql.columnNameOfCorruptRecord\nconfiguration)\nAllows renaming the new field having malformed string created by\nPERMISSIVE\nmode. This overrides spark.sql.columnNameOfCorruptRecord.\nread\ndateFormat\nyyyy-MM-dd\nSets the string that indicates a date format. Custom date formats follow the formats at\ndatetime pattern\n. This applies to date type.\nread/write\ntimestampFormat\nyyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]\nSets the string that indicates a timestamp format. Custom date formats follow the formats at\ndatetime pattern\n. This applies to timestamp type.\nread/write\ntimestampNTZ", "question": "What does the FAILFAST mode do when encountering corrupted records?", "answers": {"text": ["throws an exception when it meets corrupted records."], "answer_start": [133]}}
{"context": "hat indicates a timestamp format. Custom date formats follow the formats at\ndatetime pattern\n. This applies to timestamp type.\nread/write\ntimestampNTZFormat\nyyyy-MM-dd'T'HH:mm:ss[.SSS]\nSets the string that indicates a timestamp without timezone format. Custom date formats follow the formats at\nDatetime Patterns\n. This applies to timestamp without timezone type, note that zone-offset and time-zone components are not supported when writing or reading this data type.\nread/write\nenableDateTimeParsingFallback\nEnabled if the time parser policy has legacy settings or if no custom date or timestamp pattern was provided.\nAllows falling back to the backward compatible (Spark 1.x and 2.0) behavior of parsing dates and timestamps if values do not match the set patterns.\nread\nmultiLine\nfalse\nParse one ", "question": "What format does timestampNTZFormat use to indicate a timestamp without timezone?", "answers": {"text": ["yyyy-MM-dd'T'HH:mm:ss[.SSS]"], "answer_start": [157]}}
{"context": "E, UTF-32LE. For writing, Specifies encoding (charset) of saved json files. JSON built-in functions ignore this option.\nread/write\nlineSep\n\\r\n,\n\\r\\n\n,\n\\n\n(for reading),\n\\n\n(for writing)\nDefines the line separator that should be used for parsing. JSON built-in functions ignore this option.\nread/write\nsamplingRatio\n1.0\nDefines fraction of input JSON objects used for schema inferring.\nread\ndropFieldIfAllNull\nfalse\nWhether to ignore column of all null values or empty array during schema inference.\nread\nlocale\nen-US\nSets a locale as language tag in IETF BCP 47 format. For instance,\nlocale\nis used while parsing dates and timestamps.\nread\nallowNonNumericNumbers\ntrue\nAllows JSON parser to recognize set of “Not-a-Number” (NaN) tokens as legal floating number values.\n+INF\n: for positive infinity, as", "question": "What does the 'locale' option do?", "answers": {"text": ["Sets a locale as language tag in IETF BCP 47 format. For instance,\nlocale\nis used while parsing dates and timestamps."], "answer_start": [517]}}
{"context": "mericNumbers\ntrue\nAllows JSON parser to recognize set of “Not-a-Number” (NaN) tokens as legal floating number values.\n+INF\n: for positive infinity, as well as alias of\n+Infinity\nand\nInfinity\n.\n-INF\n: for negative infinity, alias\n-Infinity\n.\nNaN\n: for other not-a-numbers, like result of division by zero.\nread\ncompression\n(none)\nCompression codec to use when saving to file. This can be one of the known case-insensitive shorten names (none, bzip2, gzip, lz4, snappy and deflate). JSON built-in functions ignore this option.\nwrite\nignoreNullFields\n(value of\nspark.sql.jsonGenerator.ignoreNullFields\nconfiguration)\nWhether to ignore null fields when generating JSON objects.\nwrite\nuseUnsafeRow\n(value of\nspark.sql.json.useUnsafeRow\nconfiguration)\nWhether to use UnsafeRow to represent struct result in", "question": "What compression codecs are available when saving to a file?", "answers": {"text": ["(none, bzip2, gzip, lz4, snappy and deflate)"], "answer_start": [435]}}
{"context": "enerating JSON objects.\nwrite\nuseUnsafeRow\n(value of\nspark.sql.json.useUnsafeRow\nconfiguration)\nWhether to use UnsafeRow to represent struct result in the JSON parser.\nread\nOther generic options can be found in\nGeneric File Source Options\n.", "question": "What does the configuration spark.sql.json.useUnsafeRow control?", "answers": {"text": ["Whether to use UnsafeRow to represent struct result in the JSON parser."], "answer_start": [96]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTrou", "question": "Which file formats are mentioned as data sources in the Spark SQL Guide?", "answers": {"text": ["Parquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files"], "answer_start": [650]}}
{"context": "Parquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTroubleshooting\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nCSV Files\nSpark SQL provides\nspark.read().csv(\"file_name\")\nto read a file or directory of files in CSV format into Spark DataFrame, and\ndataframe.write().csv(\"path\")\nto write to a CSV file. Function\noption()\ncan be used to customize the behavior of reading or writing, such as controlling behavior of the header, delimiter character, character set, and so on.\n# spark is from the previous example\nsc\n=\nspark\n.\nsparkContext\n# A CSV dataset is pointed to by path.\n# The path can be either a single CSV ", "question": "How can you read a file or directory of files in CSV format into a Spark DataFrame?", "answers": {"text": ["spark.read().csv(\"file_name\")"], "answer_start": [329]}}
{"context": " files in a folder, please make sure only CSV files should present in the folder.\nfolderPath\n=\n\"\nexamples/src/main/resources\n\"\ndf5\n=\nspark\n.\nread\n.\ncsv\n(\nfolderPath\n)\ndf5\n.\nshow\n()\n# Wrong schema because non-CSV files are read\n# +-----------+\n# |        _c0|\n# +-----------+\n# |238val_238|\n# |  86val_86|\n# |311val_311|\n# |  27val_27|\n# |165val_165|\n# +-----------+\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\n// A CSV dataset is pointed to by path.\n// The path can be either a single CSV file or a directory of CSV files\nval\npath\n=\n\"examples/src/main/resources/people.csv\"\nval\ndf\n=\nspark\n.\nread\n.\ncsv\n(\npath\n)\ndf\n.\nshow\n()\n// +------------------+\n// |               _c0|\n// +------------------+\n// |      name;age;job|\n// |Jorge;30;Developer|\n// |  Bob;", "question": "What should be the only file type present in the folder when reading CSV files?", "answers": {"text": ["only CSV files should present in the folder."], "answer_start": [37]}}
{"context": "CSV files are read\n// +-----------+\n// |        _c0|\n// +-----------+\n// |238val_238|\n// |  86val_86|\n// |311val_311|\n// |  27val_27|\n// |165val_165|\n// +-----------+\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\n// A CSV dataset is pointed to by path.\n// The path can be either a single CSV file or a directory of CSV files\nString\npath\n=\n\"examples/src/main/resources/people.csv\"\n;\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\nread\n().\ncsv\n(\npath\n);\ndf\n.\nshow\n();\n// +------------------+\n// |               _c0|\n// +------------------+\n// |      name;age;job|\n// |Jorge;30;Developer|\n// |  Bob;32;Developer|\n// +------------------+\n// Read a csv with delimiter, ", "question": "Where can I find a full example code for reading CSV files?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo."], "answer_start": [167]}}
{"context": "| name|age|      job|\n// +-----+---+---------+\n// |Jorge| 30|Developer|\n// |  Bob| 32|Developer|\n// +-----+---+---------+\n// You can also use options() to use multiple options\njava\n.\nutil\n.\nMap\n<\nString\n,\nString\n>\noptionsMap\n=\nnew\njava\n.\nutil\n.\nHashMap\n<\nString\n,\nString\n>();\noptionsMap\n.\nput\n(\n\"delimiter\"\n,\n\";\"\n);\noptionsMap\n.\nput\n(\n\"header\"\n,\n\"true\"\n);\nDataset\n<\nRow\n>\ndf4\n=\nspark\n.\nread\n().\noptions\n(\noptionsMap\n).\ncsv\n(\npath\n);\n// \"output\" is a folder which contains multiple csv files and a _SUCCESS file.\ndf3\n.\nwrite\n().\ncsv\n(\n\"output\"\n);\n// Read all files in a folder, please make sure only CSV files should present in the folder.\nString\nfolderPath\n=\n\"examples/src/main/resources\"\n;\nDataset\n<\nRow\n>\ndf5\n=\nspark\n.\nread\n().\ncsv\n(\nfolderPath\n);\ndf5\n.\nshow\n();\n// Wrong schema because non-CSV fil", "question": "What is used to use multiple options?", "answers": {"text": ["You can also use options()"], "answer_start": [125]}}
{"context": "erPath\n=\n\"examples/src/main/resources\"\n;\nDataset\n<\nRow\n>\ndf5\n=\nspark\n.\nread\n().\ncsv\n(\nfolderPath\n);\ndf5\n.\nshow\n();\n// Wrong schema because non-CSV files are read\n// +-----------+\n// |        _c0|\n// +-----------+\n// |238val_238|\n// |  86val_86|\n// |311val_311|\n// |  27val_27|\n// |165val_165|\n// +-----------+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\nData Source Option\nData source options of CSV can be set via:\nthe\n.option\n/\n.options\nmethods of\nDataFrameReader\nDataFrameWriter\nDataStreamReader\nDataStreamWriter\nthe built-in functions below\nfrom_csv\nto_csv\nschema_of_csv\nOPTIONS\nclause at\nCREATE TABLE USING DATA_SOURCE\nProperty Name\nDefault\nMeaning\nScope\nsep\ndelimiter\n,\nSets a separator for each field and va", "question": "Where can I find the full example code for JavaSQLDataSourceExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo."], "answer_start": [310]}}
{"context": "ema_of_csv\nOPTIONS\nclause at\nCREATE TABLE USING DATA_SOURCE\nProperty Name\nDefault\nMeaning\nScope\nsep\ndelimiter\n,\nSets a separator for each field and value. This separator can be one or more characters.\nread/write\nextension\ncsv\nSets the file extension for the output files. Limited to letters. Length must equal 3.\nwrite\nencoding\ncharset\nUTF-8\nFor reading, decodes the CSV files by the given encoding type. For writing, specifies encoding (charset) of saved CSV files. CSV built-in functions ignore this option.\nread/write\nquote\n\"\nSets a single character used for escaping quoted values where the separator can be part of the value. For reading, if you would like to turn off quotations, you need to set not\nnull\nbut an empty string. For writing, if an empty string is set, it uses\nu0000\n(null characte", "question": "What does the 'quote' option set?", "answers": {"text": ["Sets a single character used for escaping quoted values where the separator can be part of the value."], "answer_start": [529]}}
{"context": " would like to turn off quotations, you need to set not\nnull\nbut an empty string. For writing, if an empty string is set, it uses\nu0000\n(null character).\nread/write\nquoteAll\nfalse\nA flag indicating whether all values should always be enclosed in quotes. Default is to only escape values containing a quote character.\nwrite\nescape\n\\\nSets a single character used for escaping quotes inside an already quoted value.\nread/write\nescapeQuotes\ntrue\nA flag indicating whether values containing quotes should always be enclosed in quotes. Default is to escape all values containing a quote character.\nwrite\ncomment\nSets a single character used for skipping lines beginning with this character. By default, it is disabled.\nread\nheader\nfalse\nFor reading, uses the first line as names of columns. For writing, wr", "question": "What does the 'quoteAll' flag indicate?", "answers": {"text": ["A flag indicating whether all values should always be enclosed in quotes. Default is to only escape values containing a quote character."], "answer_start": [180]}}
{"context": "nes beginning with this character. By default, it is disabled.\nread\nheader\nfalse\nFor reading, uses the first line as names of columns. For writing, writes the names of columns as the first line. Note that if the given path is a RDD of Strings, this header option will remove all lines same with the header if exists. CSV built-in functions ignore this option.\nread/write\ninferSchema\nfalse\nInfers the input schema automatically from data. It requires one extra pass over the data. CSV built-in functions ignore this option.\nread\npreferDate\ntrue\nDuring schema inference (\ninferSchema\n), attempts to infer string columns that contain dates as\nDate\nif the values satisfy the\ndateFormat\noption or default date format. For columns that contain a mixture of dates and timestamps, try inferring them as\nTimes", "question": "What does the 'inferSchema' option do?", "answers": {"text": ["Infers the input schema automatically from data. It requires one extra pass over the data. CSV built-in functions ignore this option."], "answer_start": [389]}}
{"context": "unt\nspark.sql.caseSensitive\n. Though the default value is true, it is recommended to disable the\nenforceSchema\noption to avoid incorrect results. CSV built-in functions ignore this option.\nread\nignoreLeadingWhiteSpace\nfalse\n(for reading),\ntrue\n(for writing)\nA flag indicating whether or not leading whitespaces from values being read/written should be skipped.\nread/write\nignoreTrailingWhiteSpace\nfalse\n(for reading),\ntrue\n(for writing)\nA flag indicating whether or not trailing whitespaces from values being read/written should be skipped.\nread/write\nnullValue\nSets the string representation of a null value. Since 2.0.1, this\nnullValue\nparam applies to all supported types including the string type.\nread/write\nnanValue\nNaN\nSets the string representation of a non-number value.\nread\npositiveInf\nInf", "question": "What does the 'ignoreLeadingWhiteSpace' flag control?", "answers": {"text": ["A flag indicating whether or not leading whitespaces from values being read/written should be skipped."], "answer_start": [258]}}
{"context": "s to all supported types including the string type.\nread/write\nnanValue\nNaN\nSets the string representation of a non-number value.\nread\npositiveInf\nInf\nSets the string representation of a positive infinity value.\nread\nnegativeInf\n-Inf\nSets the string representation of a negative infinity value.\nread\ndateFormat\nyyyy-MM-dd\nSets the string that indicates a date format. Custom date formats follow the formats at\nDatetime Patterns\n. This applies to date type.\nread/write\ntimestampFormat\nyyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]\nSets the string that indicates a timestamp format. Custom date formats follow the formats at\nDatetime Patterns\n. This applies to timestamp type.\nread/write\ntimestampNTZFormat\nyyyy-MM-dd'T'HH:mm:ss[.SSS]\nSets the string that indicates a timestamp without timezone format. Custom date ", "question": "What string representation does 'dateFormat' set?", "answers": {"text": ["yyyy-MM-dd"], "answer_start": [311]}}
{"context": "stamp type.\nread/write\ntimestampNTZFormat\nyyyy-MM-dd'T'HH:mm:ss[.SSS]\nSets the string that indicates a timestamp without timezone format. Custom date formats follow the formats at\nDatetime Patterns\n. This applies to timestamp without timezone type, note that zone-offset and time-zone components are not supported when writing or reading this data type.\nread/write\nenableDateTimeParsingFallback\nEnabled if the time parser policy has legacy settings or if no custom date or timestamp pattern was provided.\nAllows falling back to the backward compatible (Spark 1.x and 2.0) behavior of parsing dates and timestamps if values do not match the set patterns.\nread\nmaxColumns\n20480\nDefines a hard limit of how many columns a record can have.\nread\nmaxCharsPerColumn\n-1\nDefines the maximum number of characte", "question": "What does 'enableDateTimeParsingFallback' do?", "answers": {"text": ["Allows falling back to the backward compatible (Spark 1.x and 2.0) behavior of parsing dates and timestamps if values do not match the set patterns."], "answer_start": [505]}}
{"context": "it meets a corrupted record, puts the malformed string into a field configured by\ncolumnNameOfCorruptRecord\n, and sets malformed fields to\nnull\n. To keep corrupt records, an user can set a string type field named\ncolumnNameOfCorruptRecord\nin an user-defined schema. If a schema does not have the field, it drops corrupt records during parsing. A record with less/more tokens than schema is not a corrupted record to CSV. When it meets a record having fewer tokens than the length of the schema, sets\nnull\nto extra fields. When the record has more tokens than the length of the schema, it drops extra tokens.\nDROPMALFORMED\n: ignores the whole corrupted records. This mode is unsupported in the CSV built-in functions.\nFAILFAST\n: throws an exception when it meets corrupted records.\nread\ncolumnNameOfCo", "question": "What happens when a schema does not have a field named columnNameOfCorruptRecord?", "answers": {"text": ["If a schema does not have the field, it drops corrupt records during parsing."], "answer_start": [266]}}
{"context": "er. The default value is escape character when escape and quote characters are different,\n\\0\notherwise.\nread/write\nsamplingRatio\n1.0\nDefines fraction of rows used for schema inferring. CSV built-in functions ignore this option.\nread\nemptyValue\n(for reading),\n\"\"\n(for writing)\nSets the string representation of an empty value.\nread/write\nlocale\nen-US\nSets a locale as language tag in IETF BCP 47 format. For instance, this is used while parsing dates and timestamps.\nread\nlineSep\n\\r\n,\n\\r\\n\nand\n\\n\n(for reading),\n\\n\n(for writing)\nDefines the line separator that should be used for parsing/writing. Maximum length is 1 character. CSV built-in functions ignore this option.\nread/write\nunescapedQuoteHandling\nSTOP_AT_DELIMITER\nDefines how the CsvParser will handle values with unescaped quotes.\nSTOP_AT_CL", "question": "What does the 'locale' option set?", "answers": {"text": ["Sets a locale as language tag in IETF BCP 47 format."], "answer_start": [350]}}
{"context": "ignore this option.\nread/write\nunescapedQuoteHandling\nSTOP_AT_DELIMITER\nDefines how the CsvParser will handle values with unescaped quotes.\nSTOP_AT_CLOSING_QUOTE\n: If unescaped quotes are found in the input, accumulate the quote character and proceed parsing the value as a quoted value, until a closing quote is found.\nBACK_TO_DELIMITER\n: If unescaped quotes are found in the input, consider the value as an unquoted value. This will make the parser accumulate all characters of the current parsed value until the delimiter is found. If no delimiter is found in the value, the parser will continue accumulating characters from the input until a delimiter or line ending is found.\nSTOP_AT_DELIMITER\n: If unescaped quotes are found in the input, consider the value as an unquoted value. This will make", "question": "What happens when unescaped quotes are found in the input according to the STOP_AT_CLOSING_QUOTE option?", "answers": {"text": ["If unescaped quotes are found in the input, accumulate the quote character and proceed parsing the value as a quoted value, until a closing quote is found."], "answer_start": [164]}}
{"context": "miter or line ending is found.\nSTOP_AT_DELIMITER\n: If unescaped quotes are found in the input, consider the value as an unquoted value. This will make the parser accumulate all characters until the delimiter or a line ending is found in the input.\nSKIP_VALUE\n: If unescaped quotes are found in the input, the content parsed for the given value will be skipped and the value set in nullValue will be produced instead.\nRAISE_ERROR\n: If unescaped quotes are found in the input, a TextParsingException will be thrown.\nread\ncompression\ncodec\n(none)\nCompression codec to use when saving to file. This can be one of the known case-insensitive shorten names (\nnone\n,\nbzip2\n,\ngzip\n,\nlz4\n,\nsnappy\nand\ndeflate\n). CSV built-in functions ignore this option.\nwrite\ntimeZone\n(value of\nspark.sql.session.timeZone\ncon", "question": "What happens if unescaped quotes are found in the input when using the STOP_AT_DELIMITER option?", "answers": {"text": ["If unescaped quotes are found in the input, consider the value as an unquoted value. This will make the parser accumulate all characters until the delimiter or a line ending is found in the input."], "answer_start": [51]}}
{"context": "(\nnone\n,\nbzip2\n,\ngzip\n,\nlz4\n,\nsnappy\nand\ndeflate\n). CSV built-in functions ignore this option.\nwrite\ntimeZone\n(value of\nspark.sql.session.timeZone\nconfiguration)\nSets the string that indicates a time zone ID to be used to format timestamps in the JSON datasources or partition values. The following formats of\ntimeZone\nare supported:\nRegion-based zone ID: It should have the form 'area/city', such as 'America/Los_Angeles'.\nZone offset: It should be in the format '(+|-)HH:mm', for example '-08:00' or '+01:00'. Also 'UTC' and 'Z' are supported as aliases of '+00:00'.\nOther short names like 'CST' are not recommended to use because they can be ambiguous.\nread/write\nOther generic options can be found in\nGeneric File Source Options\n.", "question": "What formats are supported for the timeZone option?", "answers": {"text": ["Region-based zone ID: It should have the form 'area/city', such as 'America/Los_Angeles'.\nZone offset: It should be in the format '(+|-)HH:mm', for example '-08:00' or '+01:00'. Also 'UTC' and 'Z' are supported as aliases of '+00:00'."], "answer_start": [334]}}
{"context": "uous.\nread/write\nOther generic options can be found in\nGeneric File Source Options\n.", "question": "Where can other generic options be found?", "answers": {"text": ["Generic File Source Options"], "answer_start": [55]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTrou", "question": "Which file formats are mentioned as data sources in the Spark SQL Guide?", "answers": {"text": ["Parquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files"], "answer_start": [650]}}
{"context": "Parquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTroubleshooting\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nText Files\nSpark SQL provides\nspark.read().text(\"file_name\")\nto read a file or directory of text files into a Spark DataFrame, and\ndataframe.write().text(\"path\")\nto write to a text file. When reading a text file, each line becomes each row that has string “value” column by default. The line separator can be changed as shown in the example below. The\noption()\nfunction can be used to customize the behavior of reading or writing, such as controlling behavior of the line separator, compression, and ", "question": "How can you read a file or directory of text files into a Spark DataFrame?", "answers": {"text": ["spark.read().text(\"file_name\")"], "answer_start": [330]}}
{"context": "e\noption()\nfunction can be used to customize the behavior of reading or writing, such as controlling behavior of the line separator, compression, and so on.\n# spark is from the previous example\nsc\n=\nspark\n.\nsparkContext\n# A text dataset is pointed to by path.\n# The path can be either a single text file or a directory of text files\npath\n=\n\"\nexamples/src/main/resources/people.txt\n\"\ndf1\n=\nspark\n.\nread\n.\ntext\n(\npath\n)\ndf1\n.\nshow\n()\n# +-----------+\n# |      value|\n# +-----------+\n# |Michael, 29|\n# |   Andy, 30|\n# | Justin, 19|\n# +-----------+\n# You can use 'lineSep' option to define the line separator.\n# The line separator handles all `\\r`, `\\r\\n` and `\\n` by default.\ndf2\n=\nspark\n.\nread\n.\ntext\n(\npath\n,\nlineSep\n=\n\"\n,\n\"\n)\ndf2\n.\nshow\n()\n# +-----------+\n# |      value|\n# +-----------+\n# |    Michae", "question": "What is the purpose of the `lineSep` option when reading a text file?", "answers": {"text": ["You can use 'lineSep' option to define the line separator."], "answer_start": [546]}}
{"context": " and `\\n` by default.\ndf2\n=\nspark\n.\nread\n.\ntext\n(\npath\n,\nlineSep\n=\n\"\n,\n\"\n)\ndf2\n.\nshow\n()\n# +-----------+\n# |      value|\n# +-----------+\n# |    Michael|\n# |   29\\nAndy|\n# | 30\\nJustin|\n# |       19\\n|\n# +-----------+\n# You can also use 'wholetext' option to read each input file as a single row.\ndf3\n=\nspark\n.\nread\n.\ntext\n(\npath\n,\nwholetext\n=\nTrue\n)\ndf3\n.\nshow\n()\n# +--------------------+\n# |               value|\n# +--------------------+\n# |Michael, 29\\nAndy...|\n# +--------------------+\n# \"output\" is a folder which contains multiple text files and a _SUCCESS file.\ndf1\n.\nwrite\n.\ncsv\n(\n\"\noutput\n\"\n)\n# You can specify the compression format using the 'compression' option.\ndf1\n.\nwrite\n.\ntext\n(\n\"\noutput_compressed\n\"\n,\ncompression\n=\n\"\ngzip\n\"\n)\nFind full example code at \"examples/src/main/python/sql/", "question": "What option can be used to read each input file as a single row?", "answers": {"text": ["You can also use 'wholetext' option to read each input file as a single row."], "answer_start": [219]}}
{"context": "e 'compression' option.\ndf1\n.\nwrite\n.\ntext\n(\n\"\noutput_compressed\n\"\n,\ncompression\n=\n\"\ngzip\n\"\n)\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\n// A text dataset is pointed to by path.\n// The path can be either a single text file or a directory of text files\nval\npath\n=\n\"examples/src/main/resources/people.txt\"\nval\ndf1\n=\nspark\n.\nread\n.\ntext\n(\npath\n)\ndf1\n.\nshow\n()\n// +-----------+\n// |      value|\n// +-----------+\n// |Michael, 29|\n// |   Andy, 30|\n// | Justin, 19|\n// +-----------+\n// You can use 'lineSep' option to define the line separator.\n// The line separator handles all `\\r`, `\\r\\n` and `\\n` by default.\nval\ndf2\n=\nspark\n.\nread\n.\noption\n(\n\"lineSep\"\n,\n\",\"\n).\ntext\n(\npath\n)\ndf2\n.\nshow\n()\n// +-----------+\n// |      value|\n// +-----------+\n// |    Michael", "question": "Where can you find the full example code?", "answers": {"text": ["Find full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo."], "answer_start": [94]}}
{"context": "t.\nval\ndf2\n=\nspark\n.\nread\n.\noption\n(\n\"lineSep\"\n,\n\",\"\n).\ntext\n(\npath\n)\ndf2\n.\nshow\n()\n// +-----------+\n// |      value|\n// +-----------+\n// |    Michael|\n// |   29\\nAndy|\n// | 30\\nJustin|\n// |       19\\n|\n// +-----------+\n// You can also use 'wholetext' option to read each input file as a single row.\nval\ndf3\n=\nspark\n.\nread\n.\noption\n(\n\"wholetext\"\n,\ntrue\n).\ntext\n(\npath\n)\ndf3\n.\nshow\n()\n//  +--------------------+\n//  |               value|\n//  +--------------------+\n//  |Michael, 29\\nAndy...|\n//  +--------------------+\n// \"output\" is a folder which contains multiple text files and a _SUCCESS file.\ndf1\n.\nwrite\n.\ntext\n(\n\"output\"\n)\n// You can specify the compression format using the 'compression' option.\ndf1\n.\nwrite\n.\noption\n(\n\"compression\"\n,\n\"gzip\"\n).\ntext\n(\n\"output_compressed\"\n)\nFind full example", "question": "What option can be used to read each input file as a single row?", "answers": {"text": ["You can also use 'wholetext' option to read each input file as a single row."], "answer_start": [223]}}
{"context": "the compression format using the 'compression' option.\ndf1\n.\nwrite\n.\noption\n(\n\"compression\"\n,\n\"gzip\"\n).\ntext\n(\n\"output_compressed\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\nimport\norg.apache.spark.sql.Dataset\n;\nimport\norg.apache.spark.sql.Row\n;\n// A text dataset is pointed to by path.\n// The path can be either a single text file or a directory of text files\nString\npath\n=\n\"examples/src/main/resources/people.txt\"\n;\nDataset\n<\nRow\n>\ndf1\n=\nspark\n.\nread\n().\ntext\n(\npath\n);\ndf1\n.\nshow\n();\n// +-----------+\n// |      value|\n// +-----------+\n// |Michael, 29|\n// |   Andy, 30|\n// | Justin, 19|\n// +-----------+\n// You can use 'lineSep' option to define the line separator.\n// The line separator handles all `\\r`, `\\r\\n", "question": "How can you specify the compression format when writing a DataFrame?", "answers": {"text": ["the compression format using the 'compression' option."], "answer_start": [0]}}
{"context": "ndy, 30|\n// | Justin, 19|\n// +-----------+\n// You can use 'lineSep' option to define the line separator.\n// The line separator handles all `\\r`, `\\r\\n` and `\\n` by default.\nDataset\n<\nRow\n>\ndf2\n=\nspark\n.\nread\n().\noption\n(\n\"lineSep\"\n,\n\",\"\n).\ntext\n(\npath\n);\ndf2\n.\nshow\n();\n// +-----------+\n// |      value|\n// +-----------+\n// |    Michael|\n// |   29\\nAndy|\n// | 30\\nJustin|\n// |       19\\n|\n// +-----------+\n// You can also use 'wholetext' option to read each input file as a single row.\nDataset\n<\nRow\n>\ndf3\n=\nspark\n.\nread\n().\noption\n(\n\"wholetext\"\n,\n\"true\"\n).\ntext\n(\npath\n);\ndf3\n.\nshow\n();\n//  +--------------------+\n//  |               value|\n//  +--------------------+\n//  |Michael, 29\\nAndy...|\n//  +--------------------+\n// \"output\" is a folder which contains multiple text files and a _SUCCESS fil", "question": "What option can be used to define the line separator?", "answers": {"text": ["'lineSep' option to define the line separator."], "answer_start": [58]}}
{"context": "-----------------+\n//  |Michael, 29\\nAndy...|\n//  +--------------------+\n// \"output\" is a folder which contains multiple text files and a _SUCCESS file.\ndf1\n.\nwrite\n().\ntext\n(\n\"output\"\n);\n// You can specify the compression format using the 'compression' option.\ndf1\n.\nwrite\n().\noption\n(\n\"compression\"\n,\n\"gzip\"\n).\ntext\n(\n\"output_compressed\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\nData Source Option\nData source options of text can be set via:\nthe\n.option\n/\n.options\nmethods of\nDataFrameReader\nDataFrameWriter\nDataStreamReader\nDataStreamWriter\nOPTIONS\nclause at\nCREATE TABLE USING DATA_SOURCE\nProperty Name\nDefault\nMeaning\nScope\nwholetext\nfalse\nIf true, read each file from input path(s) as a single row.\nrea", "question": "How can you specify the compression format when writing a text file?", "answers": {"text": ["You can specify the compression format using the 'compression' option."], "answer_start": [191]}}
{"context": " at\nCREATE TABLE USING DATA_SOURCE\nProperty Name\nDefault\nMeaning\nScope\nwholetext\nfalse\nIf true, read each file from input path(s) as a single row.\nread\nlineSep\n\\r\n,\n\\r\\n\n,\n\\n\n(for reading),\n\\n\n(for writing)\nDefines the line separator that should be used for reading or writing.\nread/write\ncompression\n(none)\nCompression codec to use when saving to file. This can be one of the known case-insensitive shorten names (none, bzip2, gzip, lz4, snappy and deflate).\nwrite\nOther generic options can be found in\nGeneric File Source Options\n.", "question": "What does the 'lineSep' property define?", "answers": {"text": ["Defines the line separator that should be used for reading or writing."], "answer_start": [207]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTrou", "question": "Which file formats are mentioned as data sources in the Spark SQL Guide?", "answers": {"text": ["Parquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files"], "answer_start": [650]}}
{"context": "Parquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTroubleshooting\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nTroubleshooting\nThe JDBC driver class must be visible to the primordial class loader on the client session and on all executors. This is because Java’s DriverManager class does a security check that results in it ignoring all drivers not visible to the primordial class loader when one goes to open a connection. One convenient way to do this is to modify compute_classpath.sh on all worker nodes to include your driver JARs.\nSome databases, such as H2, convert all names to upper case. You’ll need t", "question": "What is a requirement for the JDBC driver class?", "answers": {"text": ["The JDBC driver class must be visible to the primordial class loader on the client session and on all executors."], "answer_start": [316]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nError Conditions\nThis is a list of error states and conditions that ", "question": "What types of programming guides are available for Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)"], "answer_start": [46]}}
{"context": "Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nError Conditions\nThis is a list of error states and conditions that may be returned by Spark SQL.\nError State / SQLSTATE\nError Condition & Sub-Condition\nMessage\n07001\n#\nALL\n_PARAMETERS\n_MUST\n_BE\n_NAMED\nUsing name parameterized queries requires all parameters to be named. Parameters missing names:\n<exprs>\n.\n07501\n#\nINVALID\n_STATEMENT\n_FOR\n_EXECUTE\n_INTO\nThe INTO clause of EXECUTE IMMEDIATE is only valid for queries but the given statement is not a query:\n<sqlString>\n.\n07501\n#\nNESTED\n_EXECUTE\n_IMMEDIATE\nNested EXECUTE IMMEDIATE commands are not allowed. Please ensure that the SQL query provided (\n<sqlString>\n) does not contain another EXECUTE IMMEDIATE command.\n07501\n#\nSQL\n_SCRIPT\n_IN\n_EXECUTE\n_IMMEDIATE\nSQL Sc", "question": "What is required when using name parameterized queries?", "answers": {"text": ["Using name parameterized queries requires all parameters to be named."], "answer_start": [284]}}
{"context": "ure that the SQL query provided (\n<sqlString>\n) does not contain another EXECUTE IMMEDIATE command.\n07501\n#\nSQL\n_SCRIPT\n_IN\n_EXECUTE\n_IMMEDIATE\nSQL Scripts in EXECUTE IMMEDIATE commands are not allowed. Please ensure that the SQL query provided (\n<sqlString>\n) is not SQL Script. Make sure the sql_string is a well-formed SQL statement and does not contain BEGIN and END.\n0A000\n#\nCANNOT\n_INVOKE\n_IN\n_TRANSFORMATIONS\nDataset transformations and actions can only be invoked by the driver, not inside of other Dataset transformations; for example, dataset1.map(x => dataset2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the dataset1.map transformation. For more information, see SPARK-28702.\n0A000\n#\nCANNOT\n_UPDATE\n_FIELD\nCannot update\n", "question": "What is not allowed within other Dataset transformations?", "answers": {"text": ["dataset1.map(x => dataset2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the dataset1.map transformation."], "answer_start": [545]}}
{"context": "ion cannot be performed inside of the dataset1.map transformation. For more information, see SPARK-28702.\n0A000\n#\nCANNOT\n_UPDATE\n_FIELD\nCannot update\n<table>\nfield\n<fieldName>\ntype:\n#\nARRAY\n_TYPE\nUpdate the element by updating\n<fieldName>\n.element.\n#\nINTERVAL\n_TYPE\nUpdate an interval by updating its fields.\n#\nMAP\n_TYPE\nUpdate a map by updating\n<fieldName>\n.key or\n<fieldName>\n.value.\n#\nSTRUCT\n_TYPE\nUpdate a struct by updating its fields.\n#\nUSER\n_DEFINED\n_TYPE\nUpdate a UserDefinedType[\n<udtSql>\n] by updating its fields.\n0A000\n#\nCLASS\n_UNSUPPORTED\n_BY\n_MAP\n_OBJECTS\nMapObjects\ndoes not support the class\n<cls>\nas resulting collection.\n0A000\n#\nCOLUMN\n_ARRAY\n_ELEMENT\n_TYPE\n_MISMATCH\nSome values in field\n<pos>\nare incompatible with the column array type. Expected type\n<type>\n.\n0A000\n#\nCONCURRENT\n_", "question": "What happens if you try to perform an ion operation inside of the dataset1.map transformation?", "answers": {"text": ["ion cannot be performed inside of the dataset1.map transformation."], "answer_start": [0]}}
{"context": "MN\n_ARRAY\n_ELEMENT\n_TYPE\n_MISMATCH\nSome values in field\n<pos>\nare incompatible with the column array type. Expected type\n<type>\n.\n0A000\n#\nCONCURRENT\n_QUERY\nAnother instance of this query was just started by a concurrent session.\n0A000\n#\nCORRUPTED\n_CATALOG\n_FUNCTION\nCannot convert the catalog function '\n<identifier>\n' into a SQL function due to corrupted function information in catalog. If the function is not a SQL function, please make sure the class name '\n<className>\n' is loadable.\n0A000\n#\nCREATE\n_PERMANENT\n_VIEW\n_WITHOUT\n_ALIAS\nNot allowed to create the permanent view\n<name>\nwithout explicitly assigning an alias for the expression\n<attr>\n.\n0A000\n#\nDESCRIBE\n_JSON\n_NOT\n_EXTENDED\nDESCRIBE TABLE ... AS JSON only supported when [EXTENDED|FORMATTED] is specified. For example: DESCRIBE EXTENDE", "question": "What is required when using DESCRIBE TABLE ... AS JSON?", "answers": {"text": ["DESCRIBE TABLE ... AS JSON only supported when [EXTENDED|FORMATTED] is specified."], "answer_start": [689]}}
{"context": "\n0A000\n#\nDESCRIBE\n_JSON\n_NOT\n_EXTENDED\nDESCRIBE TABLE ... AS JSON only supported when [EXTENDED|FORMATTED] is specified. For example: DESCRIBE EXTENDED\n<tableName>\nAS JSON is supported but DESCRIBE\n<tableName>\nAS JSON is not.\n0A000\n#\nDISTINCT\n_WINDOW\n_FUNCTION\n_UNSUPPORTED\nDistinct window functions are not supported:\n<windowExpr>\n.\n0A000\n#\nEMPTY\n_SCHEMA\n_NOT\n_SUPPORTED\n_FOR\n_DATASOURCE\nThe\n<format>\ndatasource does not support writing empty or nested empty schemas. Please make sure the data schema has at least one or more column(s).\n0A000\n#\nINVALID\n_PANDAS\n_UDF\n_PLACEMENT\nThe group aggregate pandas UDF\n<functionList>\ncannot be invoked together with as other, non-pandas aggregate functions.\n0A000\n#\nINVALID\n_PARTITION\n_COLUMN\n_DATA\n_TYPE\nCannot use\n<type>\nfor partition column.\n0A000\n#\nMULTI\n_", "question": "What is required to use DESCRIBE TABLE ... AS JSON?", "answers": {"text": ["DESCRIBE TABLE ... AS JSON only supported when [EXTENDED|FORMATTED] is specified."], "answer_start": [39]}}
{"context": "CALAR\n_SUBQUERY\n_IS\n_IN\n_GROUP\n_BY\n_OR\n_AGGREGATE\n_FUNCTION\nThe correlated scalar subquery '\n<sqlExpr>\n' is neither present in GROUP BY, nor in an aggregate function. Add it to GROUP BY using ordinal position or wrap it in\nfirst()\n(or\nfirst_value\n) if you don't care which value you get.\n0A000\n#\nSTAR\n_GROUP\n_BY\n_POS\nStar (*) is not allowed in a select list when GROUP BY an ordinal position is used.\n0A000\n#\nTABLE\n_VALUED\n_ARGUMENTS\n_NOT\n_YET\n_IMPLEMENTED\n_FOR\n_SQL\n_FUNCTIONS\nCannot\n<action>\nSQL user-defined function\n<functionName>\nwith TABLE arguments because this functionality is not yet implemented.\n0A000\n#\nUNSUPPORTED\n_ADD\n_FILE\nDon't support add file.\n#\nDIRECTORY\nThe file\n<path>\nis a directory, consider to set \"spark.sql.legacy.addSingleFileInAddFile\" to \"false\".\n#\nLOCAL\n_DIRECTORY\nThe l", "question": "What should you do if a correlated scalar subquery is not present in GROUP BY or an aggregate function?", "answers": {"text": ["Add it to GROUP BY using ordinal position or wrap it in\nfirst()\n(or\nfirst_value\n) if you don't care which value you get."], "answer_start": [167]}}
{"context": "t add file.\n#\nDIRECTORY\nThe file\n<path>\nis a directory, consider to set \"spark.sql.legacy.addSingleFileInAddFile\" to \"false\".\n#\nLOCAL\n_DIRECTORY\nThe local directory\n<path>\nis not supported in a non-local master mode.\n0A000\n#\nUNSUPPORTED\n_ARROWTYPE\nUnsupported arrow type\n<typeName>\n.\n0A000\n#\nUNSUPPORTED\n_CALL\nCannot call the method \"\n<methodName>\n\" of the class \"\n<className>\n\".\n#\nFIELD\n_INDEX\nThe row shall have a schema to get an index of the field\n<fieldName>\n.\n#\nWITHOUT\n_SUGGESTION\n0A000\n#\nUNSUPPORTED\n_CHAR\n_OR\n_VARCHAR\n_AS\n_STRING\nThe char/varchar type can't be used in the table schema. If you want Spark treat them as string type as same as Spark 3.0 and earlier, please set \"spark.sql.legacy.charVarcharAsString\" to \"true\".\n0A000\n#\nUNSUPPORTED\n_COLLATION\nCollation\n<collationName>\nis not s", "question": "What should be set to \"true\" if you want Spark to treat char/varchar types as string type as same as Spark 3.0 and earlier?", "answers": {"text": ["please set \"spark.sql.legacy.charVarcharAsString\" to \"true\"."], "answer_start": [674]}}
{"context": " Spark 3.0 and earlier, please set \"spark.sql.legacy.charVarcharAsString\" to \"true\".\n0A000\n#\nUNSUPPORTED\n_COLLATION\nCollation\n<collationName>\nis not supported for:\n#\nFOR\n_FUNCTION\nfunction\n<functionName>\n. Please try to use a different collation.\n0A000\n#\nUNSUPPORTED\n_CONNECT\n_FEATURE\nFeature is not supported in Spark Connect:\n#\nDATASET\n_QUERY\n_EXECUTION\nAccess to the Dataset Query Execution. This is server side developer API.\n#\nRDD\nResilient Distributed Datasets (RDDs).\n#\nSESSION\n_BASE\n_RELATION\n_TO\n_DATAFRAME\nInvoking SparkSession 'baseRelationToDataFrame'. This is server side developer API\n#\nSESSION\n_EXPERIMENTAL\n_METHODS\nAccess to SparkSession Experimental (methods). This is server side developer API\n#\nSESSION\n_LISTENER\n_MANAGER\nAccess to the SparkSession Listener Manager. This is serve", "question": "What setting should be used for Spark 3.0 and earlier?", "answers": {"text": ["please set \"spark.sql.legacy.charVarcharAsString\" to \"true\"."], "answer_start": [24]}}
{"context": "sion Experimental (methods). This is server side developer API\n#\nSESSION\n_LISTENER\n_MANAGER\nAccess to the SparkSession Listener Manager. This is server side developer API\n#\nSESSION\n_SESSION\n_STATE\nAccess to the SparkSession Session State. This is server side developer API.\n#\nSESSION\n_SHARED\n_STATE\nAccess to the SparkSession Shared State. This is server side developer API.\n#\nSESSION\n_SPARK\n_CONTEXT\nAccess to the SparkContext.\n0A000\n#\nUNSUPPORTED\n_DATASOURCE\n_FOR\n_DIRECT\n_QUERY\nUnsupported data source type for direct query on files:\n<dataSourceType>\n0A000\n#\nUNSUPPORTED\n_DATATYPE\nUnsupported data type\n<typeName>\n.\n0A000\n#\nUNSUPPORTED\n_DATA\n_SOURCE\n_SAVE\n_MODE\nThe data source \"\n<source>\n\" cannot be written in the\n<createMode>\nmode. Please use either the \"Append\" or \"Overwrite\" mode instead.\n0A", "question": "What should be used instead of the current mode when a data source cannot be written in a specific mode?", "answers": {"text": ["Please use either the \"Append\" or \"Overwrite\" mode instead."], "answer_start": [738]}}
{"context": "CE\n_SAVE\n_MODE\nThe data source \"\n<source>\n\" cannot be written in the\n<createMode>\nmode. Please use either the \"Append\" or \"Overwrite\" mode instead.\n0A000\n#\nUNSUPPORTED\n_DATA\n_TYPE\n_FOR\n_DATASOURCE\nThe\n<format>\ndatasource doesn't support the column\n<columnName>\nof the type\n<columnType>\n.\n0A000\n#\nUNSUPPORTED\n_DATA\n_TYPE\n_FOR\n_ENCODER\nCannot create encoder for\n<dataType>\n. Please use a different output data type for your UDF or DataFrame.\n0A000\n#\nUNSUPPORTED\n_DEFAULT\n_VALUE\nDEFAULT column values is not supported.\n#\nWITHOUT\n_SUGGESTION\n#\nWITH\n_SUGGESTION\nEnable it by setting \"spark.sql.defaultColumn.enabled\" to \"true\".\n0A000\n#\nUNSUPPORTED\n_DESERIALIZER\nThe deserializer is not supported:\n#\nDATA\n_TYPE\n_MISMATCH\nneed a(n)\n<desiredType>\nfield but got\n<dataType>\n.\n#\nFIELD\n_NUMBER\n_MISMATCH\ntry to m", "question": "What should you use instead of the current mode when a data source cannot be written in the specified create mode?", "answers": {"text": ["Please use either the \"Append\" or \"Overwrite\" mode instead."], "answer_start": [88]}}
{"context": "operation>\n.\n#\nCLAUSE\n_WITH\n_PIPE\n_OPERATORS\nThe SQL pipe operator syntax using |> does not support\n<clauses>\n.\n#\nCOLLATIONS\n_IN\n_MAP\n_KEYS\nCollated strings for keys of maps\n#\nCOMBINATION\n_QUERY\n_RESULT\n_CLAUSES\nCombination of ORDER BY/SORT BY/DISTRIBUTE BY/CLUSTER BY.\n#\nCOMMENT\n_NAMESPACE\nAttach a comment to the namespace\n<namespace>\n.\n#\nCONTINUE\n_EXCEPTION\n_HANDLER\nCONTINUE exception handler is not supported. Use EXIT handler.\n#\nDESC\n_TABLE\n_COLUMN\n_JSON\nDESC TABLE COLUMN AS JSON not supported for individual columns.\n#\nDESC\n_TABLE\n_COLUMN\n_PARTITION\nDESC TABLE COLUMN for a specific partition.\n#\nDROP\n_DATABASE\nDrop the default database\n<database>\n.\n#\nDROP\n_NAMESPACE\nDrop the namespace\n<namespace>\n.\n#\nHIVE\n_TABLE\n_TYPE\nThe\n<tableName>\nis hive\n<tableType>\n.\n#\nHIVE\n_WITH\n_ANSI\n_INTERVALS\nHiv", "question": "What is not supported for individual columns?", "answers": {"text": ["DESC TABLE COLUMN AS JSON not supported for individual columns."], "answer_start": [461]}}
{"context": "_TABLE\nPurge table.\n#\nPYTHON\n_UDF\n_IN\n_ON\n_CLAUSE\nPython UDF in the ON clause of a\n<joinType>\nJOIN. In case of an INNER JOIN consider rewriting to a CROSS JOIN with a WHERE clause.\n#\nQUERY\n_ONLY\n_CORRUPT\n_RECORD\n_COLUMN\nQueries from raw JSON/CSV/XML files are disallowed when the referenced columns only include the internal corrupt record column (named\n_corrupt_record\nby default). For example:\nspark.read.schema(schema).json(file).filter($\"_corrupt_record\".isNotNull).count()\nand\nspark.read.schema(schema).json(file).select(\"_corrupt_record\").show()\n. Instead, you can cache or save the parsed results and then send the same query. For example,\nval df = spark.read.schema(schema).json(file).cache()\nand then\ndf.filter($\"_corrupt_record\".isNotNull).count()\n.\n#\nREMOVE\n_NAMESPACE\n_COMMENT\nRemove a co", "question": "What should you do instead of querying raw JSON/CSV/XML files when referenced columns only include the internal corrupt record column?", "answers": {"text": ["Instead, you can cache or save the parsed results and then send the same query."], "answer_start": [554]}}
{"context": "\n_TEMPORARY\n_VARIABLE\nDROP TEMPORARY VARIABLE is not supported within SQL scripts. To bypass this, use\nEXECUTE IMMEDIATE 'DROP TEMPORARY VARIABLE ...'\n.\n#\nSQL\n_SCRIPTING\n_WITH\n_POSITIONAL\n_PARAMETERS\nPositional parameters are not supported with SQL Scripting.\n#\nSTATE\n_STORE\n_MULTIPLE\n_COLUMN\n_FAMILIES\nCreating multiple column families with\n<stateStoreProvider>\nis not supported.\n#\nSTATE\n_STORE\n_REMOVING\n_COLUMN\n_FAMILIES\nRemoving column families with\n<stateStoreProvider>\nis not supported.\n#\nSTATE\n_STORE\n_TTL\nState TTL with\n<stateStoreProvider>\nis not supported. Please use RocksDBStateStoreProvider.\n#\nTABLE\n_OPERATION\nTable\n<tableName>\ndoes not support\n<operation>\n. Please check the current catalog and namespace to make sure the qualified table name is expected, and also check the catalog im", "question": "What should be used to bypass the lack of support for 'DROP TEMPORARY VARIABLE' within SQL scripts?", "answers": {"text": ["EXECUTE IMMEDIATE 'DROP TEMPORARY VARIABLE ...'"], "answer_start": [103]}}
{"context": " support\n<operation>\n. Please check the current catalog and namespace to make sure the qualified table name is expected, and also check the catalog implementation which is configured by \"spark.sql.catalog\".\n#\nTEMPORARY\n_VIEW\n_WITH\n_SCHEMA\n_BINDING\n_MODE\nTemporary views cannot be created with the WITH SCHEMA clause. Recreate the temporary view when the underlying schema changes, or use a persisted view.\n#\nTIME\n_TRAVEL\nTime travel on the relation:\n<relationId>\n.\n#\nTOO\n_MANY\n_TYPE\n_ARGUMENTS\n_FOR\n_UDF\n_CLASS\nUDF class with\n<num>\ntype arguments.\n#\nTRANSFORM\n_DISTINCT\n_ALL\nTRANSFORM with the DISTINCT/ALL clause.\n#\nTRANSFORM\n_NON\n_HIVE\nTRANSFORM with SERDE is only supported in hive mode.\n#\nTRIM\n_COLLATION\nTRIM specifier in the collation.\n#\nUPDATE\n_COLUMN\n_NULLABILITY\nUpdate column nullability fo", "question": "What should you check to ensure the qualified table name is as expected?", "answers": {"text": ["Please check the current catalog and namespace to make sure the qualified table name is expected, and also check the catalog implementation which is configured by \"spark.sql.catalog\"."], "answer_start": [23]}}
{"context": "_PATH\nan existent path.\n#\nNON\n_EXISTENT\n_PATH\na non-existent path.\n0A000\n#\nUNSUPPORTED\n_SHOW\n_CREATE\n_TABLE\nUnsupported a SHOW CREATE TABLE command.\n#\nON\n_DATA\n_SOURCE\n_TABLE\n_WITH\n_AS\n_SERDE\nThe table\n<tableName>\nis a Spark data source table. Please use SHOW CREATE TABLE without AS SERDE instead.\n#\nON\n_TEMPORARY\n_VIEW\nThe command is not supported on a temporary view\n<tableName>\n.\n#\nON\n_TRANSACTIONAL\n_HIVE\n_TABLE\nFailed to execute the command against transactional Hive table\n<tableName>\n. Please use SHOW CREATE TABLE\n<tableName>\nAS SERDE to show Hive DDL instead.\n#\nWITH\n_UNSUPPORTED\n_FEATURE\nFailed to execute the command against table/view\n<tableName>\nwhich is created by Hive and uses the following unsupported features\n<unsupportedFeatures>\n#\nWITH\n_UNSUPPORTED\n_SERDE\n_CONFIGURATION\nFailed ", "question": "What should be used instead of SHOW CREATE TABLE for a Spark data source table?", "answers": {"text": ["Please use SHOW CREATE TABLE without AS SERDE instead."], "answer_start": [244]}}
{"context": "\n_STREAMING\n_OPERATOR\n_WITHOUT\n_WATERMARK\n<outputMode>\noutput mode not supported for\n<statefulOperator>\non streaming DataFrames/DataSets without watermark.\n0A000\n#\nUNSUPPORTED\n_SUBQUERY\n_EXPRESSION\n_CATEGORY\nUnsupported subquery expression:\n#\nACCESSING\n_OUTER\n_QUERY\n_COLUMN\n_IS\n_NOT\n_ALLOWED\nAccessing outer query column is not allowed in this location:\n<treeNode>\n#\nAGGREGATE\n_FUNCTION\n_MIXED\n_OUTER\n_LOCAL\n_REFERENCES\nFound an aggregate function in a correlated predicate that has both outer and local references, which is not supported:\n<function>\n.\n#\nCORRELATED\n_COLUMN\n_IS\n_NOT\n_ALLOWED\n_IN\n_PREDICATE\nCorrelated column is not allowed in predicate:\n<treeNode>\n#\nCORRELATED\n_COLUMN\n_NOT\n_FOUND\nA correlated outer name reference within a subquery expression body was not found in the enclosing qu", "question": "What is not supported for stateful operators on streaming DataFrames/DataSets?", "answers": {"text": ["output mode not supported for"], "answer_start": [55]}}
{"context": "ate:\n<treeNode>\n#\nCORRELATED\n_COLUMN\n_NOT\n_FOUND\nA correlated outer name reference within a subquery expression body was not found in the enclosing query:\n<value>\n.\n#\nCORRELATED\n_REFERENCE\nExpressions referencing the outer query are not supported outside of WHERE/HAVING clauses:\n<sqlExprs>\n.\n#\nHIGHER\n_ORDER\n_FUNCTION\nSubquery expressions are not supported within higher-order functions. Please remove all subquery expressions from higher-order functions and then try the query again.\n#\nLATERAL\n_JOIN\n_CONDITION\n_NON\n_DETERMINISTIC\nLateral join condition cannot be non-deterministic:\n<condition>\n.\n#\nMUST\n_AGGREGATE\n_CORRELATED\n_SCALAR\n_SUBQUERY\nCorrelated scalar subqueries must be aggregated to return at most one row.\n#\nNON\n_CORRELATED\n_COLUMNS\n_IN\n_GROUP\n_BY\nA GROUP BY clause in a scalar correl", "question": "What is indicated by the error message 'HIGHER_ORDER_FUNCTION'?", "answers": {"text": ["Subquery expressions are not supported within higher-order functions. Please remove all subquery expressions from higher-order functions and then try the query again."], "answer_start": [319]}}
{"context": "related scalar subqueries must be aggregated to return at most one row.\n#\nNON\n_CORRELATED\n_COLUMNS\n_IN\n_GROUP\n_BY\nA GROUP BY clause in a scalar correlated subquery cannot contain non-correlated columns:\n<value>\n.\n#\nNON\n_DETERMINISTIC\n_LATERAL\n_SUBQUERIES\nNon-deterministic lateral subqueries are not supported when joining with outer relations that produce more than one row:\n<treeNode>\n#\nSCALAR\n_SUBQUERY\n_IN\n_VALUES\nScalar subqueries in the VALUES clause.\n#\nUNSUPPORTED\n_CORRELATED\n_EXPRESSION\n_IN\n_JOIN\n_CONDITION\nCorrelated subqueries in the join predicate cannot reference both join inputs:\n<subqueryExpression>\n#\nUNSUPPORTED\n_CORRELATED\n_REFERENCE\n_DATA\n_TYPE\nCorrelated column reference '\n<expr>\n' cannot be\n<dataType>\ntype.\n#\nUNSUPPORTED\n_CORRELATED\n_SCALAR\n_SUBQUERY\nCorrelated scalar subque", "question": "O que acontece com subconsultas escalares relacionadas?", "answers": {"text": ["related scalar subqueries must be aggregated to return at most one row."], "answer_start": [0]}}
{"context": "NCE\n_DATA\n_TYPE\nCorrelated column reference '\n<expr>\n' cannot be\n<dataType>\ntype.\n#\nUNSUPPORTED\n_CORRELATED\n_SCALAR\n_SUBQUERY\nCorrelated scalar subqueries can only be used in filters, aggregations, projections, and UPDATE/MERGE/DELETE commands:\n<treeNode>\n#\nUNSUPPORTED\n_IN\n_EXISTS\n_SUBQUERY\nIN/EXISTS predicate subqueries can only be used in filters, joins, aggregations, window functions, projections, and UPDATE/MERGE/DELETE commands:\n<treeNode>\n#\nUNSUPPORTED\n_TABLE\n_ARGUMENT\nTable arguments are used in a function where they are not supported:\n<treeNode>\n0A000\n#\nUNSUPPORTED\n_TYPED\n_LITERAL\nLiterals of the type\n<unsupportedType>\nare not supported. Supported types are\n<supportedTypes>\n.\n0AKD0\n#\nCANNOT\n_RENAME\n_ACROSS\n_SCHEMA\nRenaming a\n<type>\nacross schemas is not allowed.\n21000\n#\nBOOLEAN\n_ST", "question": "Onde subconsultas escalares correlacionadas podem ser usadas?", "answers": {"text": ["Correlated scalar subqueries can only be used in filters, aggregations, projections, and UPDATE/MERGE/DELETE commands:"], "answer_start": [126]}}
{"context": "um value of\n<minValue>\nand maximum value of\n<maxValue>\n. Please adjust the value accordingly.\n22003\n#\nNEGATIVE\n_VALUES\n_IN\n_FREQUENCY\n_EXPRESSION\nFound the negative value in\n<frequencyExpression>\n:\n<negativeValue>\n, but expected a positive integral value.\n22003\n#\nNUMERIC\n_OUT\n_OF\n_SUPPORTED\n_RANGE\nThe value\n<value>\ncannot be interpreted as a numeric since it has more than 38 digits.\n22003\n#\nNUMERIC\n_VALUE\n_OUT\n_OF\n_RANGE\n#\nWITHOUT\n_SUGGESTION\nThe\n<roundedValue>\nrounded half up from\n<originalValue>\ncannot be represented as Decimal(\n<precision>\n,\n<scale>\n).\n#\nWITH\n_SUGGESTION\n<value>\ncannot be represented as Decimal(\n<precision>\n,\n<scale>\n). If necessary set\n<config>\nto \"false\" to bypass this error, and return NULL instead.\n22003\n#\nSUM\n_OF\n_LIMIT\n_AND\n_OFFSET\n_EXCEEDS\n_MAX\n_INT\nThe sum of th", "question": "What should be adjusted if a value is outside the supported range?", "answers": {"text": ["Please adjust the value accordingly."], "answer_start": [57]}}
{"context": " necessary set\n<config>\nto \"false\" to bypass this error, and return NULL instead.\n22003\n#\nSUM\n_OF\n_LIMIT\n_AND\n_OFFSET\n_EXCEEDS\n_MAX\n_INT\nThe sum of the LIMIT clause and the OFFSET clause must not be greater than the maximum 32-bit integer value (2,147,483,647) but found limit =\n<limit>\n, offset =\n<offset>\n.\n22004\n#\nCOMPARATOR\n_RETURNS\n_NULL\nThe comparator has returned a NULL for a comparison between\n<firstValue>\nand\n<secondValue>\n. It should return a positive integer for \"greater than\", 0 for \"equal\" and a negative integer for \"less than\". To revert to deprecated behavior where NULL is treated as 0 (equal), you must set \"spark.sql.legacy.allowNullComparisonResultInArraySort\" to \"true\".\n22004\n#\nNULL\n_QUERY\n_STRING\n_EXECUTE\n_IMMEDIATE\nExecute immediate requires a non-null variable as the que", "question": "What should you set \"spark.sql.legacy.allowNullComparisonResultInArraySort\" to in order to revert to deprecated behavior where NULL is treated as 0 (equal)?", "answers": {"text": ["To revert to deprecated behavior where NULL is treated as 0 (equal), you must set \"spark.sql.legacy.allowNullComparisonResultInArraySort\" to \"true\"."], "answer_start": [546]}}
{"context": "wNullComparisonResultInArraySort\" to \"true\".\n22004\n#\nNULL\n_QUERY\n_STRING\n_EXECUTE\n_IMMEDIATE\nExecute immediate requires a non-null variable as the query string, but the provided variable\n<varName>\nis null.\n22004\n#\nTUPLE\n_IS\n_EMPTY\nDue to Scala's limited support of tuple, empty tuple is not supported.\n22006\n#\nCANNOT\n_PARSE\n_INTERVAL\nUnable to parse\n<intervalString>\n. Please ensure that the value provided is in a valid format for defining an interval. You can reference the documentation for the correct format. If the issue persists, please double check that the input value is not null or empty and try again.\n22006\n#\nINVALID\n_INTERVAL\n_FORMAT\nError parsing '\n<input>\n' to interval. Please ensure that the value provided is in a valid format for defining an interval. You can reference the docume", "question": "What should you ensure when providing a value for defining an interval?", "answers": {"text": ["Please ensure that the value provided is in a valid format for defining an interval."], "answer_start": [369]}}
{"context": "ror parsing '\n<input>\n' to interval. Please ensure that the value provided is in a valid format for defining an interval. You can reference the documentation for the correct format.\n#\nARITHMETIC\n_EXCEPTION\nUncaught arithmetic exception while parsing '\n<input>\n'.\n#\nDAY\n_TIME\n_PARSING\nError parsing interval day-time string:\n<msg>\n.\n#\nINPUT\n_IS\n_EMPTY\nInterval string cannot be empty.\n#\nINPUT\n_IS\n_NULL\nInterval string cannot be null.\n#\nINTERVAL\n_PARSING\nError parsing interval\n<interval>\nstring.\n#\nINVALID\n_FRACTION\n<unit>\ncannot have fractional part.\n#\nINVALID\n_PRECISION\nInterval can only support nanosecond precision,\n<value>\nis out of range.\n#\nINVALID\n_PREFIX\nInvalid interval prefix\n<prefix>\n.\n#\nINVALID\n_UNIT\nInvalid unit\n<unit>\n.\n#\nINVALID\n_VALUE\nInvalid value\n<value>\n.\n#\nMISSING\n_NUMBER\nExpe", "question": "What type of exception occurs when an arithmetic operation fails during parsing?", "answers": {"text": ["Uncaught arithmetic exception"], "answer_start": [206]}}
{"context": "VALID\n_PREFIX\nInvalid interval prefix\n<prefix>\n.\n#\nINVALID\n_UNIT\nInvalid unit\n<unit>\n.\n#\nINVALID\n_VALUE\nInvalid value\n<value>\n.\n#\nMISSING\n_NUMBER\nExpect a number after\n<word>\nbut hit EOL.\n#\nMISSING\n_UNIT\nExpect a unit name after\n<word>\nbut hit EOL.\n#\nSECOND\n_NANO\n_FORMAT\nInterval string does not match second-nano format of ss.nnnnnnnnn.\n#\nTIMEZONE\n_INTERVAL\n_OUT\n_OF\n_RANGE\nThe interval value must be in the range of [-18, +18] hours with second precision.\n#\nUNKNOWN\n_PARSING\n_ERROR\nUnknown error when parsing\n<word>\n.\n#\nUNMATCHED\n_FORMAT\n_STRING\nInterval string does not match\n<intervalStr>\nformat of\n<supportedFormat>\nwhen cast to\n<typeName>\n:\n<input>\n.\n#\nUNMATCHED\n_FORMAT\n_STRING\n_WITH\n_NOTICE\nInterval string does not match\n<intervalStr>\nformat of\n<supportedFormat>\nwhen cast to\n<typeName>\n:\n<", "question": "What is the expected range for the interval value when dealing with timezones?", "answers": {"text": ["The interval value must be in the range of [-18, +18] hours with second precision."], "answer_start": [376]}}
{"context": "\n#\nILLEGAL\n_DAY\n_OF\n_WEEK\nIllegal input for day of week:\n<string>\n.\n22009\n#\nINVALID\n_TIMEZONE\nThe timezone:\n<timeZone>\nis invalid. The timezone must be either a region-based zone ID or a zone offset. Region IDs must have the form 'area/city', such as 'America/Los_Angeles'. Zone offsets must be in the format '(+|-)HH', '(+|-)HH:mm’ or '(+|-)HH:mm:ss', e.g '-08' , '+01:00' or '-13:33:33', and must be in the range from -18:00 to +18:00. 'Z' and 'UTC' are accepted as synonyms for '+00:00'.\n2200E\n#\nNULL\n_MAP\n_KEY\nCannot use null as map key.\n22012\n#\nDIVIDE\n_BY\n_ZERO\nDivision by zero. Use\ntry_divide\nto tolerate divisor being 0 and return NULL instead. If necessary set\n<config>\nto \"false\" to bypass this error.\n22012\n#\nINTERVAL\n_DIVIDED\n_BY\n_ZERO\nDivision by zero. Use\ntry_divide\nto tolerate divisor", "question": "What should be used to tolerate a divisor being 0 and return NULL instead?", "answers": {"text": ["try_divide"], "answer_start": [589]}}
{"context": "LENGTH\n_MISMATCH\nInput row doesn't have expected number of values required by the schema.\n<expected>\nfields are required while\n<actual>\nvalues are provided.\n22022\n#\nINVALID\n_CONF\n_VALUE\nThe value '\n<confValue>\n' in the config \"\n<confName>\n\" is invalid.\n#\nTIME\n_ZONE\nCannot resolve the given timezone.\n22023\n#\nDATETIME\n_FIELD\n_OUT\n_OF\n_BOUNDS\n<rangeMessage>\n. If necessary set\n<ansiConfig>\nto \"false\" to bypass this error.\n22023\n#\nINVALID\n_FRACTION\n_OF\n_SECOND\nValid range for seconds is [0, 60] (inclusive), but the provided value is\n<secAndMicros>\n. To avoid this error, use\ntry_make_timestamp\n, which returns NULL on error. If you do not want to use the session default timestamp version of this function, use\ntry_make_timestamp_ntz\nor\ntry_make_timestamp_ltz\n.\n22023\n#\nINVALID\n_JSON\n_RECORD\n_TYPE\nD", "question": "What function can be used to avoid errors related to invalid seconds in a timestamp?", "answers": {"text": ["try_make_timestamp"], "answer_start": [576]}}
{"context": "h>\nbytes.\n#\nBINARY\n_FORMAT\nexpects one of binary formats 'base64', 'hex', 'utf-8', but got\n<invalidFormat>\n.\n#\nBIT\n_POSITION\n_RANGE\nexpects an integer value in [0,\n<upper>\n), but got\n<invalidValue>\n.\n#\nBOOLEAN\nexpects a boolean literal, but got\n<invalidValue>\n.\n#\nCHARSET\nexpects one of the\n<charsets>\n, but got\n<charset>\n.\n#\nDATETIME\n_UNIT\nexpects one of the units without quotes YEAR, QUARTER, MONTH, WEEK, DAY, DAYOFYEAR, HOUR, MINUTE, SECOND, MILLISECOND, MICROSECOND, but got the string literal\n<invalidValue>\n.\n#\nDOUBLE\nexpects an double literal, but got\n<invalidValue>\n.\n#\nDTYPE\nUnsupported dtype:\n<invalidValue>\n. Valid values: float64, float32.\n#\nEXTENSION\nInvalid extension:\n<invalidValue>\n. Extension is limited to exactly 3 letters (e.g. csv, tsv, etc...)\n#\nINTEGER\nexpects an integer lit", "question": "What are the valid values for DTYPE?", "answers": {"text": ["float64, float32."], "answer_start": [636]}}
{"context": "32.\n#\nEXTENSION\nInvalid extension:\n<invalidValue>\n. Extension is limited to exactly 3 letters (e.g. csv, tsv, etc...)\n#\nINTEGER\nexpects an integer literal, but got\n<invalidValue>\n.\n#\nLENGTH\nExpects\nlength\ngreater than or equal to 0, but got\n<length>\n.\n#\nLONG\nexpects a long literal, but got\n<invalidValue>\n.\n#\nNULL\nexpects a non-NULL value.\n#\nPATTERN\n<value>\n.\n#\nREGEX\n_GROUP\n_INDEX\nExpects group index between 0 and\n<groupCount>\n, but got\n<groupIndex>\n.\n#\nSTART\nExpects a positive or a negative value for\nstart\n, but got 0.\n#\nSTRING\nexpects a string literal, but got\n<invalidValue>\n.\n#\nZERO\n_INDEX\nexpects %1$, %2$ and so on, but got %0$.\n22023\n#\nINVALID\n_REGEXP\n_REPLACE\nCould not perform regexp_replace for source = \"\n<source>\n\", pattern = \"\n<pattern>\n\", replacement = \"\n<replacement>\n\" and positi", "question": "What is the maximum allowed length for an extension?", "answers": {"text": ["Extension is limited to exactly 3 letters (e.g. csv, tsv, etc...)"], "answer_start": [52]}}
{"context": " valid path should start with\n$\nand is followed by zero or more segments like\n[123]\n,\n.name\n,\n['name']\n, or\n[\"name\"]\n.\n22023\n#\nINVALID\n_VARIANT\n_SHREDDING\n_SCHEMA\nThe schema\n<schema>\nis not a valid variant shredding schema.\n22023\n#\nMALFORMED\n_RECORD\n_IN\n_PARSING\nMalformed records are detected in record parsing:\n<badRecord>\n. Parse Mode:\n<failFastMode>\n. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n#\nCANNOT\n_PARSE\n_JSON\n_ARRAYS\n_AS\n_STRUCTS\nParsing JSON arrays as structs is forbidden.\n#\nCANNOT\n_PARSE\n_STRING\n_AS\n_DATATYPE\nCannot parse the value\n<fieldValue>\nof the field\n<fieldName>\nas target spark data type\n<targetType>\nfrom the input type\n<inputType>\n.\n#\nWITHOUT\n_SUGGESTION\n22023\n#\nMALFORMED\n_VARIANT\nVariant binary is malformed. Please check t", "question": "What should a valid path start with?", "answers": {"text": ["$"], "answer_start": [30]}}
{"context": " data type\n<targetType>\nfrom the input type\n<inputType>\n.\n#\nWITHOUT\n_SUGGESTION\n22023\n#\nMALFORMED\n_VARIANT\nVariant binary is malformed. Please check the data source is valid.\n22023\n#\nROW\n_VALUE\n_IS\n_NULL\nFound NULL in a row at the index\n<index>\n, expected a non-NULL value.\n22023\n#\nRULE\n_ID\n_NOT\n_FOUND\nNot found an id for the rule name \"\n<ruleName>\n\". Please modify RuleIdCollection.scala if you are adding a new rule.\n22023\n#\nSECOND\n_FUNCTION\n_ARGUMENT\n_NOT\n_INTEGER\nThe second argument of\n<functionName>\nfunction needs to be an integer.\n22023\n#\nTABLE\n_VALUED\n_FUNCTION\n_REQUIRED\n_METADATA\n_INCOMPATIBLE\n_WITH\n_CALL\nFailed to evaluate the table function\n<functionName>\nbecause its table metadata\n<requestedMetadata>\n, but the function call\n<invalidFunctionCallProperty>\n.\n22023\n#\nTABLE\n_VALUED\n_FUN", "question": "What error message is displayed when a variant binary is malformed?", "answers": {"text": ["Variant binary is malformed. Please check the data source is valid."], "answer_start": [107]}}
{"context": "ction\n<functionName>\nbecause its table metadata\n<requestedMetadata>\n, but the function call\n<invalidFunctionCallProperty>\n.\n22023\n#\nTABLE\n_VALUED\n_FUNCTION\n_REQUIRED\n_METADATA\n_INVALID\nFailed to evaluate the table function\n<functionName>\nbecause its table metadata was invalid;\n<reason>\n.\n22023\n#\nUNKNOWN\n_PRIMITIVE\n_TYPE\n_IN\n_VARIANT\nUnknown primitive type with id\n<id>\nwas found in a variant value.\n22023\n#\nVARIANT\n_CONSTRUCTOR\n_SIZE\n_LIMIT\nCannot construct a Variant larger than 16 MiB. The maximum allowed size of a Variant value is 16 MiB.\n22023\n#\nVARIANT\n_DUPLICATE\n_KEY\nFailed to build variant because of a duplicate object key\n<key>\n.\n22023\n#\nVARIANT\n_SIZE\n_LIMIT\nCannot build variant bigger than\n<sizeLimit>\nin\n<functionName>\n. Please avoid large input strings to this expression (for exampl", "question": "What is the maximum allowed size of a Variant value?", "answers": {"text": ["16 MiB"], "answer_start": [482]}}
{"context": "\nVARIANT\n_SIZE\n_LIMIT\nCannot build variant bigger than\n<sizeLimit>\nin\n<functionName>\n. Please avoid large input strings to this expression (for example, add function calls(s) to check the expression size and convert it to NULL first if it is too big).\n22024\n#\nNULL\n_DATA\n_SOURCE\n_OPTION\nData source read/write option\n<option>\ncannot have null value.\n22029\n#\nINVALID\n_UTF8\n_STRING\nInvalid UTF8 byte sequence found in string:\n<str>\n.\n22032\n#\nINVALID\n_JSON\n_ROOT\n_FIELD\nCannot convert JSON root field to target Spark type.\n22032\n#\nINVALID\n_JSON\n_SCHEMA\n_MAP\n_TYPE\nInput schema\n<jsonSchema>\ncan only contain STRING as a key type for a MAP.\n2203G\n#\nCANNOT\n_PARSE\n_JSON\n_FIELD\nCannot parse the field name\n<fieldName>\nand the value\n<fieldValue>\nof the JSON token type\n<jsonType>\nto target Spark data type\n<d", "question": "What should be avoided when using large input strings with this expression?", "answers": {"text": ["Please avoid large input strings to this expression (for example, add function calls(s) to check the expression size and convert it to NULL first if it is too big)."], "answer_start": [87]}}
{"context": "'re trying to read the field as\n<sqlType>\n, which would lead to an incorrect answer. To allow reading this field, enable the SQL configuration: \"spark.sql.legacy.avro.allowIncompatibleSchema\".\n22KD3\n#\nAVRO\n_NOT\n_LOADED\n_SQL\n_FUNCTIONS\n_UNUSABLE\nCannot call the\n<functionName>\nSQL function because the Avro data source is not loaded. Please restart your job or session with the 'spark-avro' package loaded, such as by using the --packages argument on the command line, and then retry your query or command again.\n22KD3\n#\nCANNOT\n_USE\n_KRYO\nCannot load Kryo serialization codec. Kryo serialization cannot be used in the Spark Connect client. Use Java serialization, provide a custom Codec, or use Spark Classic instead.\n22KD3\n#\nPROTOBUF\n_NOT\n_LOADED\n_SQL\n_FUNCTIONS\n_UNUSABLE\nCannot call the\n<functionNa", "question": "What configuration setting can be enabled to allow reading a field that is being read as an incorrect SQL type?", "answers": {"text": ["enable the SQL configuration: \"spark.sql.legacy.avro.allowIncompatibleSchema\"."], "answer_start": [114]}}
{"context": "rialization, provide a custom Codec, or use Spark Classic instead.\n22KD3\n#\nPROTOBUF\n_NOT\n_LOADED\n_SQL\n_FUNCTIONS\n_UNUSABLE\nCannot call the\n<functionName>\nSQL function because the Protobuf data source is not loaded. Please restart your job or session with the 'spark-protobuf' package loaded, such as by using the --packages argument on the command line, and then retry your query or command again.\n22P02\n#\nINVALID\n_URL\nThe url is invalid:\n<url>\n. Use\ntry_parse_url\nto tolerate invalid URL and return NULL instead.\n22P03\n#\nINVALID\n_BYTE\n_STRING\nThe expected format is ByteString, but was\n<unsupported>\n(\n<class>\n).\n23505\n#\nDUPLICATED\n_MAP\n_KEY\nDuplicate map key\n<key>\nwas found, please check the input data. If you want to remove the duplicated keys, you can set\n<mapKeyDedupPolicy>\nto \"LAST_WIN\" so t", "question": "What should you do if the Protobuf data source is not loaded when trying to call a SQL function?", "answers": {"text": ["Please restart your job or session with the 'spark-protobuf' package loaded, such as by using the --packages argument on the command line, and then retry your query or command again."], "answer_start": [215]}}
{"context": "te map key\n<key>\nwas found, please check the input data. If you want to remove the duplicated keys, you can set\n<mapKeyDedupPolicy>\nto \"LAST_WIN\" so that the key inserted at last takes precedence.\n23505\n#\nDUPLICATE\n_KEY\nFound duplicate keys\n<keyColumn>\n.\n23K01\n#\nMERGE\n_CARDINALITY\n_VIOLATION\nThe ON search condition of the MERGE statement matched a single row from the target table with multiple rows of the source table. This could result in the target row being operated on more than once with an update or delete operation and is not allowed.\n2BP01\n#\nSCHEMA\n_NOT\n_EMPTY\nCannot drop a schema\n<schemaName>\nbecause it contains objects. Use DROP SCHEMA ... CASCADE to drop the schema and all its objects.\n38000\n#\nCLASS\n_NOT\n_OVERRIDE\n_EXPECTED\n_METHOD\n<className>\nmust override either\n<method1>\nor\n<m", "question": "What policy can be set to remove duplicated keys, giving precedence to the last inserted key?", "answers": {"text": ["to \"LAST_WIN\" so that the key inserted at last takes precedence."], "answer_start": [132]}}
{"context": "MA ... CASCADE to drop the schema and all its objects.\n38000\n#\nCLASS\n_NOT\n_OVERRIDE\n_EXPECTED\n_METHOD\n<className>\nmust override either\n<method1>\nor\n<method2>\n.\n38000\n#\nFAILED\n_FUNCTION\n_CALL\nFailed preparing of the function\n<funcName>\nfor call. Please, double check function's arguments.\n38000\n#\nFAILED\n_TO\n_LOAD\n_ROUTINE\nFailed to load routine\n<routineName>\n.\n38000\n#\nINVALID\n_UDF\n_IMPLEMENTATION\nFunction\n<funcName>\ndoes not implement a ScalarFunction or AggregateFunction.\n38000\n#\nNO\n_UDF\n_INTERFACE\nUDF class\n<className>\ndoesn't implement any UDF interface.\n38000\n#\nPYTHON\n_DATA\n_SOURCE\n_ERROR\nFailed to\n<action>\nPython data source\n<type>\n:\n<msg>\n38000\n#\nPYTHON\n_STREAMING\n_DATA\n_SOURCE\n_RUNTIME\n_ERROR\nFailed when Python streaming data source perform\n<action>\n:\n<msg>\n38000\n#\nTABLE\n_VALUED\n_FUNC", "question": "What error code is associated with failures related to function calls?", "answers": {"text": ["38000"], "answer_start": [55]}}
{"context": "\n38000\n#\nPYTHON\n_STREAMING\n_DATA\n_SOURCE\n_RUNTIME\n_ERROR\nFailed when Python streaming data source perform\n<action>\n:\n<msg>\n38000\n#\nTABLE\n_VALUED\n_FUNCTION\n_FAILED\n_TO\n_ANALYZE\n_IN\n_PYTHON\nFailed to analyze the Python user defined table function:\n<msg>\n39000\n#\nFAILED\n_EXECUTE\n_UDF\nUser defined function (\n<functionName>\n: (\n<signature>\n) =>\n<result>\n) failed due to:\n<reason>\n.\n39000\n#\nFLATMAPGROUPSWITHSTATE\n_USER\n_FUNCTION\n_ERROR\nAn error occurred in the user provided function in flatMapGroupsWithState. Reason:\n<reason>\n39000\n#\nFOREACH\n_BATCH\n_USER\n_FUNCTION\n_ERROR\nAn error occurred in the user provided function in foreach batch sink. Reason:\n<reason>\n39000\n#\nFOREACH\n_USER\n_FUNCTION\n_ERROR\nAn error occurred in the user provided function in foreach sink. Reason:\n<reason>\n3F000\n#\nMISSING\n_DATA", "question": "What type of error occurs when a Python streaming data source fails?", "answers": {"text": ["Failed when Python streaming data source perform"], "answer_start": [57]}}
{"context": "reason>\n39000\n#\nFOREACH\n_USER\n_FUNCTION\n_ERROR\nAn error occurred in the user provided function in foreach sink. Reason:\n<reason>\n3F000\n#\nMISSING\n_DATABASE\n_FOR\n_V1\n_SESSION\n_CATALOG\nDatabase name is not specified in the v1 session catalog. Please ensure to provide a valid database name when interacting with the v1 catalog.\n40000\n#\nCONCURRENT\n_STREAM\n_LOG\n_UPDATE\nConcurrent update to the log. Multiple streaming jobs detected for\n<batchId>\n. Please make sure only one streaming job runs on a specific checkpoint location at a time.\n42000\n#\nAMBIGUOUS\n_REFERENCE\n_TO\n_FIELDS\nAmbiguous reference to the field\n<field>\n. It appears\n<count>\ntimes in the schema.\n42000\n#\nCANNOT\n_REMOVE\n_RESERVED\n_PROPERTY\nCannot remove reserved property:\n<property>\n.\n42000\n#\nCLUSTERING\n_NOT\n_SUPPORTED\n'\n<operation>\n' do", "question": "What happens when a database name is not specified in the v1 session catalog?", "answers": {"text": ["Database name is not specified in the v1 session catalog. Please ensure to provide a valid database name when interacting with the v1 catalog."], "answer_start": [182]}}
{"context": "schema.\n42000\n#\nCANNOT\n_REMOVE\n_RESERVED\n_PROPERTY\nCannot remove reserved property:\n<property>\n.\n42000\n#\nCLUSTERING\n_NOT\n_SUPPORTED\n'\n<operation>\n' does not support clustering.\n42000\n#\nINVALID\n_COLUMN\n_OR\n_FIELD\n_DATA\n_TYPE\nColumn or field\n<name>\nis of type\n<type>\nwhile it's required to be\n<expectedType>\n.\n42000\n#\nINVALID\n_EXTRACT\n_BASE\n_FIELD\n_TYPE\nCan't extract a value from\n<base>\n. Need a complex type [STRUCT, ARRAY, MAP] but got\n<other>\n.\n42000\n#\nINVALID\n_EXTRACT\n_FIELD\n_TYPE\nField name should be a non-null string literal, but it's\n<extraction>\n.\n42000\n#\nINVALID\n_FIELD\n_NAME\nField name\n<fieldName>\nis invalid:\n<path>\nis not a struct.\n42000\n#\nINVALID\n_INLINE\n_TABLE\nInvalid inline table.\n#\nCANNOT\n_EVALUATE\n_EXPRESSION\n_IN\n_INLINE\n_TABLE\nCannot evaluate the expression\n<expr>\nin inline tabl", "question": "What happens when attempting to remove a reserved property?", "answers": {"text": ["Cannot remove reserved property:\n<property>\n."], "answer_start": [51]}}
{"context": "\n#\nINVALID\n_INLINE\n_TABLE\nInvalid inline table.\n#\nCANNOT\n_EVALUATE\n_EXPRESSION\n_IN\n_INLINE\n_TABLE\nCannot evaluate the expression\n<expr>\nin inline table definition.\n#\nFAILED\n_SQL\n_EXPRESSION\n_EVALUATION\nFailed to evaluate the SQL expression\n<sqlExpr>\n. Please check your syntax and ensure all required tables and columns are available.\n#\nINCOMPATIBLE\n_TYPES\n_IN\n_INLINE\n_TABLE\nFound incompatible types in the column\n<colName>\nfor inline table.\n#\nNUM\n_COLUMNS\n_MISMATCH\nInline table expected\n<expectedNumCols>\ncolumns but found\n<actualNumCols>\ncolumns in row\n<rowIndex>\n.\n42000\n#\nINVALID\n_RESET\n_COMMAND\n_FORMAT\nExpected format is 'RESET' or 'RESET key'. If you want to include special characters in key, please use quotes, e.g., RESET\nkey\n.\n42000\n#\nINVALID\n_SAVE\n_MODE\nThe specified save mode\n<mode>\ni", "question": "What is the expected format for a reset command?", "answers": {"text": ["Expected format is 'RESET' or 'RESET key'."], "answer_start": [610]}}
{"context": "'. If you want to include special characters in key, please use quotes, e.g., RESET\nkey\n.\n42000\n#\nINVALID\n_SAVE\n_MODE\nThe specified save mode\n<mode>\nis invalid. Valid save modes include \"append\", \"overwrite\", \"ignore\", \"error\", \"errorifexists\", and \"default\".\n42000\n#\nINVALID\n_SET\n_SYNTAX\nExpected format is 'SET', 'SET key', or 'SET key=value'. If you want to include special characters in key, or include semicolon in value, please use backquotes, e.g., SET\nkey\n=\nvalue\n.\n42000\n#\nINVALID\n_SQL\n_SYNTAX\nInvalid SQL syntax:\n#\nANALYZE\n_TABLE\n_UNEXPECTED\n_NOSCAN\nANALYZE TABLE(S) ... COMPUTE STATISTICS ...\n<ctx>\nmust be either NOSCAN or empty.\n#\nCREATE\n_FUNC\n_WITH\n_COLUMN\n_CONSTRAINTS\nCREATE FUNCTION with constraints on parameters is not allowed.\n#\nCREATE\n_FUNC\n_WITH\n_GENERATED\n_COLUMNS\n_AS\n_PARAMET", "question": "Quais são os modos de salvamento válidos?", "answers": {"text": ["Valid save modes include \"append\", \"overwrite\", \"ignore\", \"error\", \"errorifexists\", and \"default\"."], "answer_start": [161]}}
{"context": "\n_FUNC\n_WITH\n_COLUMN\n_CONSTRAINTS\nCREATE FUNCTION with constraints on parameters is not allowed.\n#\nCREATE\n_FUNC\n_WITH\n_GENERATED\n_COLUMNS\n_AS\n_PARAMETERS\nCREATE FUNCTION with generated columns as parameters is not allowed.\n#\nCREATE\n_ROUTINE\n_WITH\n_IF\n_NOT\n_EXISTS\n_AND\n_REPLACE\nCannot create a routine with both IF NOT EXISTS and REPLACE specified.\n#\nCREATE\n_TEMP\n_FUNC\n_WITH\n_DATABASE\nCREATE TEMPORARY FUNCTION with specifying a database(\n<database>\n) is not allowed.\n#\nCREATE\n_TEMP\n_FUNC\n_WITH\n_IF\n_NOT\n_EXISTS\nCREATE TEMPORARY FUNCTION with IF NOT EXISTS is not allowed.\n#\nEMPTY\n_PARTITION\n_VALUE\nPartition key\n<partKey>\nmust set value.\n#\nFUNCTION\n_WITH\n_UNSUPPORTED\n_SYNTAX\nThe function\n<prettyName>\ndoes not support\n<syntax>\n.\n#\nINVALID\n_COLUMN\n_REFERENCE\nExpected a column reference for transfo", "question": "What is not allowed when creating a routine?", "answers": {"text": ["Cannot create a routine with both IF NOT EXISTS and REPLACE specified."], "answer_start": [278]}}
{"context": "_SUBQUERY\n_OR\n_TABLE\n_VALUED\n_FUNC\nLATERAL can only be used with subquery and table-valued functions.\n#\nMULTI\n_PART\n_NAME\n<statement>\nwith multiple part name(\n<name>\n) is not allowed.\n#\nOPTION\n_IS\n_INVALID\noption or property key\n<key>\nis invalid; only\n<supported>\nare supported\n#\nREPETITIVE\n_WINDOW\n_DEFINITION\nThe definition of window\n<windowName>\nis repetitive.\n#\nSHOW\n_FUNCTIONS\n_INVALID\n_PATTERN\nInvalid pattern in SHOW FUNCTIONS:\n<pattern>\n. It must be a \"STRING\" literal.\n#\nSHOW\n_FUNCTIONS\n_INVALID\n_SCOPE\nSHOW\n<scope>\nFUNCTIONS not supported.\n#\nTRANSFORM\n_WRONG\n_NUM\n_ARGS\nThe transform\n<transform>\nrequires\n<expectedNum>\nparameters but the actual number is\n<actualNum>\n.\n#\nUNRESOLVED\n_WINDOW\n_REFERENCE\nCannot resolve window reference\n<windowName>\n.\n#\nUNSUPPORTED\n_FUNC\n_NAME\nUnsupported func", "question": "What is LATERAL used with?", "answers": {"text": ["LATERAL can only be used with subquery and table-valued functions."], "answer_start": [35]}}
{"context": "tual number is\n<actualNum>\n.\n#\nUNRESOLVED\n_WINDOW\n_REFERENCE\nCannot resolve window reference\n<windowName>\n.\n#\nUNSUPPORTED\n_FUNC\n_NAME\nUnsupported function name\n<funcName>\n.\n#\nUNSUPPORTED\n_SQL\n_STATEMENT\nUnsupported SQL statement:\n<sqlText>\n.\n#\nVARIABLE\n_TYPE\n_OR\n_DEFAULT\n_REQUIRED\nThe definition of a SQL variable requires either a datatype or a DEFAULT clause. For example, use\nDECLARE name STRING\nor\nDECLARE name = 'SQL'\ninstead of\nDECLARE name\n.\n42000\n#\nINVALID\n_USAGE\n_OF\n_STAR\n_OR\n_REGEX\nInvalid usage of\n<elem>\nin\n<prettyName>\n.\n42000\n#\nINVALID\n_WRITE\n_DISTRIBUTION\nThe requested write distribution is invalid.\n#\nPARTITION\n_NUM\n_AND\n_SIZE\nThe partition number and advisory partition size can't be specified at the same time.\n#\nPARTITION\n_NUM\n_WITH\n_UNSPECIFIED\n_DISTRIBUTION\nThe number of part", "question": "What is required when defining a SQL variable?", "answers": {"text": ["The definition of a SQL variable requires either a datatype or a DEFAULT clause."], "answer_start": [282]}}
{"context": "partition number and advisory partition size can't be specified at the same time.\n#\nPARTITION\n_NUM\n_WITH\n_UNSPECIFIED\n_DISTRIBUTION\nThe number of partitions can't be specified with unspecified distribution.\n#\nPARTITION\n_SIZE\n_WITH\n_UNSPECIFIED\n_DISTRIBUTION\nThe advisory partition size can't be specified with unspecified distribution.\n42000\n#\nMULTIPLE\n_QUERY\n_RESULT\n_CLAUSES\n_WITH\n_PIPE\n_OPERATORS\n<clause1>\nand\n<clause2>\ncannot coexist in the same SQL pipe operator using '|>'. Please separate the multiple result clauses into separate pipe operators and then retry the query again.\n42000\n#\nNON\n_PARTITION\n_COLUMN\nPARTITION clause cannot contain the non-partition column:\n<columnName>\n.\n42000\n#\nNOT\n_NULL\n_ASSERT\n_VIOLATION\nNULL value appeared in non-nullable field:\n<walkedTypePath>\nIf the schema", "question": "What happens when multiple query result clauses are used with pipe operators?", "answers": {"text": ["cannot coexist in the same SQL pipe operator using '|>'. Please separate the multiple result clauses into separate pipe operators and then retry the query again."], "answer_start": [424]}}
{"context": "he non-partition column:\n<columnName>\n.\n42000\n#\nNOT\n_NULL\n_ASSERT\n_VIOLATION\nNULL value appeared in non-nullable field:\n<walkedTypePath>\nIf the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (such as java.lang.Integer instead of int/scala.Int).\n42000\n#\nNOT\n_NULL\n_CONSTRAINT\n_VIOLATION\nAssigning a NULL is not allowed here.\n#\nARRAY\n_ELEMENT\nThe array\n<columnPath>\nis defined to contain only elements that are NOT NULL.\n#\nMAP\n_VALUE\nThe map\n<columnPath>\nis defined to contain only values that are NOT NULL.\n42000\n#\nNO\n_HANDLER\n_FOR\n_UDAF\nNo handler for UDAF '\n<functionName>\n'. Use sparkSession.udf.register(...) instead.\n42000\n#\nNULLABLE\n_COLUMN\n_OR\n_FIELD\nColumn or field\n<name>\nis nullable while it's required to be non-n", "question": "What is suggested when the schema is inferred from a Scala tuple/case class, or a Java bean, and a NULL value appears in a non-nullable field?", "answers": {"text": ["If the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (such as java.lang.Integer instead of int/scala.Int)."], "answer_start": [137]}}
{"context": "\n'. Use sparkSession.udf.register(...) instead.\n42000\n#\nNULLABLE\n_COLUMN\n_OR\n_FIELD\nColumn or field\n<name>\nis nullable while it's required to be non-nullable.\n42000\n#\nNULLABLE\n_ROW\n_ID\n_ATTRIBUTES\nRow ID attributes cannot be nullable:\n<nullableRowIdAttrs>\n.\n42000\n#\nPARTITION\n_COLUMN\n_NOT\n_FOUND\n_IN\n_SCHEMA\nPartition column\n<column>\nnot found in schema\n<schema>\n. Please provide the existing column for partitioning.\n42000\n#\nUNSUPPORTED\n_TABLE\n_CHANGE\n_IN\n_JDBC\n_CATALOG\nThe table change\n<change>\nis not supported for the JDBC catalog on table\n<tableName>\n. Supported changes include: AddColumn, RenameColumn, DeleteColumn, UpdateColumnType, UpdateColumnNullability.\n42001\n#\nINVALID\n_AGNOSTIC\n_ENCODER\nFound an invalid agnostic encoder. Expects an instance of AgnosticEncoder but got\n<encoderType>\n.", "question": "What types of table changes are supported for the JDBC catalog?", "answers": {"text": ["Supported changes include: AddColumn, RenameColumn, DeleteColumn, UpdateColumnType, UpdateColumnNullability."], "answer_start": [559]}}
{"context": "it has more than 2 name parts.\n42601\n#\nIDENTITY\n_COLUMNS\n_DUPLICATED\n_SEQUENCE\n_GENERATOR\n_OPTION\nDuplicated IDENTITY column sequence generator option:\n<sequenceGeneratorOption>\n.\n42601\n#\nILLEGAL\n_STATE\n_STORE\n_VALUE\nIllegal value provided to the State Store\n#\nEMPTY\n_LIST\n_VALUE\nCannot write empty list values to State Store for StateName\n<stateName>\n.\n#\nNULL\n_VALUE\nCannot write null values to State Store for StateName\n<stateName>\n.\n42601\n#\nINVALID\n_ATTRIBUTE\n_NAME\n_SYNTAX\nSyntax error in the attribute name:\n<name>\n. Check that backticks appear in pairs, a quoted string is a complete name part and use a backtick only inside quoted name parts.\n42601\n#\nINVALID\n_BUCKET\n_COLUMN\n_DATA\n_TYPE\nCannot use\n<type>\nfor bucket column. Collated data types are not supported for bucketing.\n42601\n#\nINVALID\n", "question": "What is reported when an illegal value is provided to the State Store?", "answers": {"text": ["Illegal value provided to the State Store"], "answer_start": [217]}}
{"context": "42601\n#\nINVALID\n_BUCKET\n_COLUMN\n_DATA\n_TYPE\nCannot use\n<type>\nfor bucket column. Collated data types are not supported for bucketing.\n42601\n#\nINVALID\n_EXTRACT\n_FIELD\nCannot extract\n<field>\nfrom\n<expr>\n.\n42601\n#\nINVALID\n_FORMAT\nThe format is invalid:\n<format>\n.\n#\nCONT\n_THOUSANDS\n_SEPS\nThousands separators (, or G) must have digits in between them in the number format.\n#\nCUR\n_MUST\n_BEFORE\n_DEC\nCurrency characters must appear before any decimal point in the number format.\n#\nCUR\n_MUST\n_BEFORE\n_DIGIT\nCurrency characters must appear before digits in the number format.\n#\nEMPTY\nThe number format string cannot be empty.\n#\nESC\n_AT\n_THE\n_END\nThe escape character is not allowed to end with.\n#\nESC\n_IN\n_THE\n_MIDDLE\nThe escape character is not allowed to precede\n<char>\n.\n#\nMISMATCH\n_INPUT\nThe input\n<inpu", "question": "What is not allowed to end with?", "answers": {"text": ["The escape character is not allowed to end with."], "answer_start": [639]}}
{"context": "s to NULL.\n#\nWRONG\n_TYPE\nThe data type of the expression is\n<dataType>\n.\n42601\n#\nNOT\n_UNRESOLVED\n_ENCODER\nUnresolved encoder expected, but\n<attr>\nwas found.\n42601\n#\nPARSE\n_MODE\n_UNSUPPORTED\nThe function\n<funcName>\ndoesn't support the\n<mode>\nmode. Acceptable modes are PERMISSIVE and FAILFAST.\n42601\n#\nPARSE\n_SYNTAX\n_ERROR\nSyntax error at or near\n<error>``<hint>\n.\n42601\n#\nREF\n_DEFAULT\n_VALUE\n_IS\n_NOT\n_ALLOWED\n_IN\n_PARTITION\nReferences to DEFAULT column values are not allowed within the PARTITION clause.\n42601\n#\nSORT\n_BY\n_WITHOUT\n_BUCKETING\nsortBy must be used together with bucketBy.\n42601\n#\nSPECIFY\n_BUCKETING\n_IS\n_NOT\n_ALLOWED\nA CREATE TABLE without explicit column list cannot specify bucketing information. Please use the form with explicit column list and specify bucketing information. Alter", "question": "What is required when using sortBy?", "answers": {"text": ["sortBy must be used together with bucketBy."], "answer_start": [543]}}
{"context": "hout explicit column list cannot specify bucketing information. Please use the form with explicit column list and specify bucketing information. Alternatively, allow bucketing information to be inferred by omitting the clause.\n42601\n#\nSPECIFY\n_PARTITION\n_IS\n_NOT\n_ALLOWED\nA CREATE TABLE without explicit column list cannot specify PARTITIONED BY. Please use the form with explicit column list and specify PARTITIONED BY. Alternatively, allow partitioning to be inferred by omitting the PARTITION BY clause.\n42601\n#\nSTDS\n_REQUIRED\n_OPTION\n_UNSPECIFIED\n'\n<optionName>\n' must be specified.\n42601\n#\nSYNTAX\n_DISCONTINUED\nSupport of the clause or keyword:\n<clause>\nhas been discontinued in this context.\n#\nBANG\n_EQUALS\n_NOT\nThe '!' keyword is only supported as an alias for the prefix operator 'NOT'. Use t", "question": "What should you do if a CREATE TABLE statement lacks an explicit column list and attempts to specify PARTITIONED BY?", "answers": {"text": ["Please use the form with explicit column list and specify PARTITIONED BY. Alternatively, allow partitioning to be inferred by omitting the PARTITION BY clause."], "answer_start": [347]}}
{"context": "<clause>\nhas been discontinued in this context.\n#\nBANG\n_EQUALS\n_NOT\nThe '!' keyword is only supported as an alias for the prefix operator 'NOT'. Use the 'NOT' keyword instead for infix clauses such as\nNOT LIKE\n,\nNOT IN\n,\nNOT BETWEEN\n, etc. To re-enable the '!' keyword, set \"spark.sql.legacy.bangEqualsNot\" to \"true\".\n42601\n#\nTRAILING\n_COMMA\n_IN\n_SELECT\nTrailing comma detected in SELECT clause. Remove the trailing comma before the FROM clause.\n42601\n#\nUNCLOSED\n_BRACKETED\n_COMMENT\nFound an unclosed bracketed comment. Please, append */ at the end of the comment.\n42601\n#\nUSER\n_DEFINED\n_FUNCTIONS\nUser defined function is invalid:\n#\nCANNOT\n_CONTAIN\n_COMPLEX\n_FUNCTIONS\nSQL scalar function cannot contain aggregate/window/generate functions:\n<queryText>\n#\nCANNOT\n_REPLACE\n_NON\n_SQL\n_UDF\n_WITH\n_SQL\n_U", "question": "What should you do to re-enable the '!' keyword?", "answers": {"text": ["To re-enable the '!' keyword, set \"spark.sql.legacy.bangEqualsNot\" to \"true\"."], "answer_start": [240]}}
{"context": "_COMPLEX\n_FUNCTIONS\nSQL scalar function cannot contain aggregate/window/generate functions:\n<queryText>\n#\nCANNOT\n_REPLACE\n_NON\n_SQL\n_UDF\n_WITH\n_SQL\n_UDF\nCannot replace the non-SQL function\n<name>\nwith a SQL function.\n#\nNOT\n_A\n_VALID\n_DEFAULT\n_EXPRESSION\nThe DEFAULT expression of\n<functionName>\n.\n<parameterName>\nis not supported because it contains a subquery.\n#\nNOT\n_A\n_VALID\n_DEFAULT\n_PARAMETER\n_POSITION\nIn routine\n<functionName>\nparameter\n<parameterName>\nwith DEFAULT must not be followed by parameter\n<nextParameterName>\nwithout DEFAULT.\n#\nNOT\n_NULL\n_ON\n_FUNCTION\n_PARAMETERS\nCannot specify NOT NULL on function parameters:\n<input>\n#\nRETURN\n_COLUMN\n_COUNT\n_MISMATCH\nThe number of columns produced by the RETURN clause (num:\n<outputSize>\n) does not match the number of column names specified by ", "question": "O que acontece quando uma função escalar SQL tenta conter funções de agregação, janela ou geração?", "answers": {"text": ["SQL scalar function cannot contain aggregate/window/generate functions:"], "answer_start": [20]}}
{"context": "cannot be more than one character:\n<str>\n.\n#\nEMPTY\n_STRING\nDelimiter cannot be empty string.\n#\nNULL\n_VALUE\nDelimiter cannot be null.\n#\nSINGLE\n_BACKSLASH\nSingle backslash is prohibited. It has special meaning as beginning of an escape sequence. To get the backslash character, pass a string with two backslashes as the delimiter.\n#\nUNSUPPORTED\n_SPECIAL\n_CHARACTER\nUnsupported special character for delimiter:\n<str>\n.\n42602\n#\nINVALID\n_IDENTIFIER\nThe unquoted identifier\n<ident>\nis invalid and must be back quoted as:\n<ident>\n. Unquoted identifiers can only contain ASCII letters ('a' - 'z', 'A' - 'Z'), digits ('0' - '9'), and underbar ('_'). Unquoted identifiers must also not start with a digit. Different data sources and meta stores may impose additional restrictions on valid identifiers.\n42602\n#\n", "question": "What is prohibited regarding single backslashes when used as a delimiter?", "answers": {"text": ["Single backslash is prohibited. It has special meaning as beginning of an escape sequence."], "answer_start": [153]}}
{"context": "identifiers must also not start with a digit. Different data sources and meta stores may impose additional restrictions on valid identifiers.\n42602\n#\nINVALID\n_PROPERTY\n_KEY\n<key>\nis an invalid property key, please use quotes, e.g. SET\n<key>\n=\n<value>\n.\n42602\n#\nINVALID\n_PROPERTY\n_VALUE\n<value>\nis an invalid property value, please use quotes, e.g. SET\n<key>\n=\n<value>\n42602\n#\nINVALID\n_SCHEMA\n_OR\n_RELATION\n_NAME\n<name>\nis not a valid name for tables/schemas. Valid names only contain alphabet characters, numbers and _.\n42604\n#\nAS\n_OF\n_JOIN\nInvalid as-of join.\n#\nTOLERANCE\n_IS\n_NON\n_NEGATIVE\nThe input argument\ntolerance\nmust be non-negative.\n#\nTOLERANCE\n_IS\n_UNFOLDABLE\nThe input argument\ntolerance\nmust be a constant.\n#\nUNSUPPORTED\n_DIRECTION\nUnsupported as-of join direction '\n<direction>\n'. Suppo", "question": "What characters are valid in table/schema names?", "answers": {"text": ["Valid names only contain alphabet characters, numbers and _."], "answer_start": [459]}}
{"context": "ANCE\n_IS\n_UNFOLDABLE\nThe input argument\ntolerance\nmust be a constant.\n#\nUNSUPPORTED\n_DIRECTION\nUnsupported as-of join direction '\n<direction>\n'. Supported as-of join direction include:\n<supported>\n.\n42604\n#\nEMPTY\n_JSON\n_FIELD\n_VALUE\nFailed to parse an empty string for data type\n<dataType>\n.\n42604\n#\nINVALID\n_ESC\nFound an invalid escape string:\n<invalidEscape>\n. The escape string must contain only one character.\n42604\n#\nINVALID\n_ESCAPE\n_CHAR\nEscapeChar\nshould be a string literal of length one, but got\n<sqlExpr>\n.\n42604\n#\nINVALID\n_TYPED\n_LITERAL\nThe value of the typed literal\n<valueType>\nis invalid:\n<value>\n.\n42605\n#\nWRONG\n_NUM\n_ARGS\nThe\n<functionName>\nrequires\n<expectedNum>\nparameters but the actual number is\n<actualNum>\n.\n#\nWITHOUT\n_SUGGESTION\nPlease, refer to '\n<docroot>\n/sql-ref-functions", "question": "What is required for the escape string?", "answers": {"text": ["The escape string must contain only one character."], "answer_start": [363]}}
{"context": "re are more than one MATCHED clauses in a MERGE statement, only the last MATCHED clause can omit the condition.\n42613\n#\nNON\n_LAST\n_NOT\n_MATCHED\n_BY\n_SOURCE\n_CLAUSE\n_OMIT\n_CONDITION\nWhen there are more than one NOT MATCHED BY SOURCE clauses in a MERGE statement, only the last NOT MATCHED BY SOURCE clause can omit the condition.\n42613\n#\nNON\n_LAST\n_NOT\n_MATCHED\n_BY\n_TARGET\n_CLAUSE\n_OMIT\n_CONDITION\nWhen there are more than one NOT MATCHED [BY TARGET] clauses in a MERGE statement, only the last NOT MATCHED [BY TARGET] clause can omit the condition.\n42613\n#\nSTDS\n_CONFLICT\n_OPTIONS\nThe options\n<options>\ncannot be specified together. Please specify the one.\n42614\n#\nDUPLICATE\n_CLAUSES\nFound duplicate clauses:\n<clauseName>\n. Please, remove one of them.\n42614\n#\nREPEATED\n_CLAUSE\nThe\n<clause>\nclause ma", "question": "In a MERGE statement, if there are multiple MATCHED clauses, which one can omit the condition?", "answers": {"text": ["only the last MATCHED clause can omit the condition."], "answer_start": [59]}}
{"context": "he one.\n42614\n#\nDUPLICATE\n_CLAUSES\nFound duplicate clauses:\n<clauseName>\n. Please, remove one of them.\n42614\n#\nREPEATED\n_CLAUSE\nThe\n<clause>\nclause may be used at most once per\n<operation>\noperation.\n42616\n#\nINVALID\n_SPARK\n_CONFIG\nInvalid Spark config:\n#\nINVALID\n_EXECUTOR\n_HEARTBEAT\n_INTERVAL\nThe value of\n<networkTimeoutKey>\n=\n<networkTimeoutValue>\nms must be greater than the value of\n<executorHeartbeatIntervalKey>\n=\n<executorHeartbeatIntervalValue>\nms.\n#\nINVALID\n_EXECUTOR\n_MEMORY\n_OPTIONS\n<executorOptsKey>\nis not allowed to specify max heap memory settings (was '\n<javaOpts>\n'). Use spark.executor.memory instead.\n#\nINVALID\n_EXECUTOR\n_SPARK\n_OPTIONS\n<executorOptsKey>\nis not allowed to set Spark options (was '\n<javaOpts>\n'). Set them directly on a SparkConf or in a properties file when using", "question": "What should be used instead of specifying max heap memory settings with <executorOptsKey>?", "answers": {"text": ["Use spark.executor.memory instead."], "answer_start": [586]}}
{"context": "PTIONS\n<executorOptsKey>\nis not allowed to set Spark options (was '\n<javaOpts>\n'). Set them directly on a SparkConf or in a properties file when using ./bin/spark-submit.\n#\nINVALID\n_MEMORY\n_FRACTION\n<memoryFractionKey>\nshould be between 0 and 1 (was '\n<memoryFractionValue>\n').\n#\nINVALID\n_SPARK\n_SUBMIT\n_DEPLOY\n_MODE\n_KEY\n<sparkSubmitDeployModeKey>\ncan only be \"cluster\" or \"client\".\n#\nNETWORK\n_AUTH\n_MUST\n_BE\n_ENABLED\n<networkAuthEnabledConf>\nmust be enabled when enabling encryption.\n42616\n#\nSTDS\n_INVALID\n_OPTION\n_VALUE\nInvalid value for source option '\n<optionName>\n':\n#\nIS\n_EMPTY\ncannot be empty.\n#\nIS\n_NEGATIVE\ncannot be negative.\n#\nWITH\n_MESSAGE\n<message>\n42617\n#\nPARSE\n_EMPTY\n_STATEMENT\nSyntax error, unexpected empty statement.\n42621\n#\nUNSUPPORTED\n_EXPRESSION\n_GENERATED\n_COLUMN\nCannot creat", "question": "What should be done with Spark options instead of setting them with '<executorOptsKey>'?", "answers": {"text": ["Set them directly on a SparkConf or in a properties file when using ./bin/spark-submit."], "answer_start": [83]}}
{"context": "dType>\ntype, but the statement provided a value of incompatible\n<actualType>\ntype.\n#\nNOT\n_CONSTANT\nwhich is not a constant expression whose equivalent value is known at query planning time.\n#\nSUBQUERY\n_EXPRESSION\nwhich contains subquery expressions.\n#\nUNRESOLVED\n_EXPRESSION\nwhich fails to resolve as a valid expression.\n42701\n#\nDUPLICATE\n_ASSIGNMENTS\nThe columns or variables\n<nameList>\nappear more than once as assignment targets.\n42701\n#\nEXEC\n_IMMEDIATE\n_DUPLICATE\n_ARGUMENT\n_ALIASES\nThe USING clause of this EXECUTE IMMEDIATE command contained multiple arguments with same alias (\n<aliases>\n), which is invalid; please update the command to specify unique aliases and then try it again.\n42702\n#\nAMBIGUOUS\n_COLUMN\n_OR\n_FIELD\nColumn or field\n<name>\nis ambiguous and has\n<n>\nmatches.\n42702\n#\nAMBIGUO", "question": "What error code is associated with duplicate assignments?", "answers": {"text": ["42701"], "answer_start": [321]}}
{"context": "tion name. Suggested valid collation names: [\n<proposals>\n].\n42704\n#\nCOLLATION\n_INVALID\n_PROVIDER\nThe value\n<provider>\ndoes not represent a correct collation provider. Supported providers are: [\n<supportedProviders>\n].\n42704\n#\nDATA\n_SOURCE\n_NOT\n_EXIST\nData source '\n<provider>\n' not found. Please make sure the data source is registered.\n42704\n#\nDEFAULT\n_DATABASE\n_NOT\n_EXISTS\nDefault database\n<defaultDatabase>\ndoes not exist, please create it first or change default database to\n<defaultDatabase>\n.\n42704\n#\nENCODER\n_NOT\n_FOUND\nNot found an encoder of the type\n<typeName>\nto Spark SQL internal representation. Consider to change the input type to one of supported at '\n<docroot>\n/sql-ref-datatypes.html'.\n42704\n#\nFIELD\n_NOT\n_FOUND\nNo such struct field\n<fieldName>\nin\n<fields>\n.\n42704\n#\nINDEX\n_NOT\n_F", "question": "What should be checked if a data source is not found?", "answers": {"text": ["Please make sure the data source is registered."], "answer_start": [290]}}
{"context": "ngRoutineType>\nof that name already exists. Choose a different name, drop or replace the existing\n<existingRoutineType>\n, or add the IF NOT EXISTS clause to tolerate a pre-existing\n<newRoutineType>\n.\n42723\n#\nVARIABLE\n_ALREADY\n_EXISTS\nCannot create the variable\n<variableName>\nbecause it already exists. Choose a different name, or drop or replace the existing variable.\n42734\n#\nDUPLICATE\n_CONDITION\n_IN\n_SCOPE\nFound duplicate condition\n<condition>\nin the scope. Please, remove one of them.\n42734\n#\nDUPLICATE\n_EXCEPTION\n_HANDLER\nFound duplicate handlers. Please, remove one of them.\n#\nCONDITION\nFound duplicate handlers for the same condition\n<condition>\n.\n#\nSQLSTATE\nFound duplicate handlers for the same SQLSTATE\n<sqlState>\n.\n42734\n#\nDUPLICATE\n_ROUTINE\n_PARAMETER\n_NAMES\nFound duplicate name(s) in t", "question": "What should you do if you attempt to create a variable that already exists?", "answers": {"text": ["Choose a different name, or drop or replace the existing variable."], "answer_start": [303]}}
{"context": "red to the same parameter. Please assign a value only once.\n4274K\n#\nNAMED\n_PARAMETERS\n_NOT\n_SUPPORTED\nNamed parameters are not supported for function\n<functionName>\n; please retry the query with positional arguments to the function call instead.\n4274K\n#\nREQUIRED\n_PARAMETER\n_NOT\n_FOUND\nCannot invoke routine\n<routineName>\nbecause the parameter named\n<parameterName>\nis required, but the routine call did not supply a value. Please update the routine call to supply an argument value (either positionally at index\n<index>\nor by name) and retry the query again.\n4274K\n#\nUNEXPECTED\n_POSITIONAL\n_ARGUMENT\nCannot invoke routine\n<routineName>\nbecause it contains positional argument(s) following the named argument assigned to\n<parameterName>\n; please rearrange them so the positional arguments come first ", "question": "What should you do if named parameters are not supported for a function?", "answers": {"text": ["please retry the query with positional arguments to the function call instead."], "answer_start": [167]}}
{"context": "ntains positional argument(s) following the named argument assigned to\n<parameterName>\n; please rearrange them so the positional arguments come first and then retry the query again.\n4274K\n#\nUNRECOGNIZED\n_PARAMETER\n_NAME\nCannot invoke routine\n<routineName>\nbecause the routine call included a named argument reference for the argument named\n<argumentName>\n, but this routine does not include any signature containing an argument with this name. Did you mean one of the following? [\n<proposal>\n].\n42802\n#\nASSIGNMENT\n_ARITY\n_MISMATCH\nThe number of columns or variables assigned or aliased:\n<numTarget>\ndoes not match the number of source expressions:\n<numExpr>\n.\n42802\n#\nSTATEFUL\n_PROCESSOR\n_CANNOT\n_PERFORM\n_OPERATION\n_WITH\n_INVALID\n_HANDLE\n_STATE\nFailed to perform stateful processor operation=\n<opera", "question": "What error occurs when the number of columns or variables assigned does not match the number of source expressions?", "answers": {"text": ["The number of columns or variables assigned or aliased:\n<numTarget>\ndoes not match the number of source expressions:\n<numExpr>\n."], "answer_start": [531]}}
{"context": "imeMode=\n<timeMode>\n, use TimeMode.ProcessingTime() instead.\n42802\n#\nSTATEFUL\n_PROCESSOR\n_TTL\n_DURATION\n_MUST\n_BE\n_POSITIVE\nTTL duration must be greater than zero for State store operation=\n<operationType>\non state=\n<stateName>\n.\n42802\n#\nSTATEFUL\n_PROCESSOR\n_UNKNOWN\n_TIME\n_MODE\nUnknown time mode\n<timeMode>\n. Accepted timeMode modes are 'none', 'processingTime', 'eventTime'\n42802\n#\nSTATE\n_STORE\n_CANNOT\n_CREATE\n_COLUMN\n_FAMILY\n_WITH\n_RESERVED\n_CHARS\nFailed to create column family with unsupported starting character and name=\n<colFamilyName>\n.\n42802\n#\nSTATE\n_STORE\n_CANNOT\n_USE\n_COLUMN\n_FAMILY\n_WITH\n_INVALID\n_NAME\nFailed to perform column family operation=\n<operationName>\nwith invalid name=\n<colFamilyName>\n. Column family name cannot be empty or include leading/trailing spaces or use the reser", "question": "Quais são os modos timeMode aceitos?", "answers": {"text": ["Accepted timeMode modes are 'none', 'processingTime', 'eventTime'"], "answer_start": [310]}}
{"context": "operation=\n<operationName>\nwith invalid name=\n<colFamilyName>\n. Column family name cannot be empty or include leading/trailing spaces or use the reserved keyword=default\n42802\n#\nSTATE\n_STORE\n_COLUMN\n_FAMILY\n_SCHEMA\n_INCOMPATIBLE\nIncompatible schema transformation with column family=\n<colFamilyName>\n, oldSchema=\n<oldSchema>\n, newSchema=\n<newSchema>\n.\n42802\n#\nSTATE\n_STORE\n_HANDLE\n_NOT\n_INITIALIZED\nThe handle has not been initialized for this StatefulProcessor. Please only use the StatefulProcessor within the transformWithState operator.\n42802\n#\nSTATE\n_STORE\n_INCORRECT\n_NUM\n_ORDERING\n_COLS\n_FOR\n_RANGE\n_SCAN\nIncorrect number of ordering ordinals=\n<numOrderingCols>\nfor range scan encoder. The number of ordering ordinals cannot be zero or greater than number of schema columns.\n42802\n#\nSTATE\n_STO", "question": "What is not allowed for a column family name?", "answers": {"text": ["Column family name cannot be empty or include leading/trailing spaces or use the reserved keyword=default"], "answer_start": [64]}}
{"context": "\n<numOrderingCols>\nfor range scan encoder. The number of ordering ordinals cannot be zero or greater than number of schema columns.\n42802\n#\nSTATE\n_STORE\n_INCORRECT\n_NUM\n_PREFIX\n_COLS\n_FOR\n_PREFIX\n_SCAN\nIncorrect number of prefix columns=\n<numPrefixCols>\nfor prefix scan encoder. Prefix columns cannot be zero or greater than or equal to num of schema columns.\n42802\n#\nSTATE\n_STORE\n_NULL\n_TYPE\n_ORDERING\n_COLS\n_NOT\n_SUPPORTED\nNull type ordering column with name=\n<fieldName>\nat index=\n<index>\nis not supported for range scan encoder.\n42802\n#\nSTATE\n_STORE\n_UNSUPPORTED\n_OPERATION\n_ON\n_MISSING\n_COLUMN\n_FAMILY\nState store operation=\n<operationType>\nnot supported on missing column family=\n<colFamilyName>\n.\n42802\n#\nSTATE\n_STORE\n_VARIABLE\n_SIZE\n_ORDERING\n_COLS\n_NOT\n_SUPPORTED\nVariable size ordering colu", "question": "What is stated about the number of ordering ordinals?", "answers": {"text": ["The number of ordering ordinals cannot be zero or greater than number of schema columns."], "answer_start": [43]}}
{"context": "ound in grouping columns\n<groupingColumns>\n.\n42803\n#\nGROUPING\n_ID\n_COLUMN\n_MISMATCH\nColumns of grouping_id (\n<groupingIdColumn>\n) does not match grouping columns (\n<groupByColumns>\n).\n42803\n#\nMISSING\n_AGGREGATION\nThe non-aggregating expression\n<expression>\nis based on columns which are not participating in the GROUP BY clause. Add the columns or the expression to the GROUP BY, aggregate the expression, or use\n<expressionAnyValue>\nif you do not care which of the values within a group is returned.\n42803\n#\nMISSING\n_GROUP\n_BY\nThe query does not include a GROUP BY clause. Add GROUP BY or turn it into the window functions using OVER clauses.\n42803\n#\nUNRESOLVED\n_ALL\n_IN\n_GROUP\n_BY\nCannot infer grouping columns for GROUP BY ALL based on the select clause. Please explicitly specify the grouping col", "question": "What should be done if a query does not include a GROUP BY clause?", "answers": {"text": ["Add GROUP BY or turn it into the window functions using OVER clauses."], "answer_start": [574]}}
{"context": "#\nUNRESOLVED\n_ALL\n_IN\n_GROUP\n_BY\nCannot infer grouping columns for GROUP BY ALL based on the select clause. Please explicitly specify the grouping columns.\n42804\n#\nINVALID\n_CORRUPT\n_RECORD\n_TYPE\nThe column\n<columnName>\nfor corrupt records must have the nullable STRING type, but got\n<actualType>\n.\n42804\n#\nTRANSPOSE\n_INVALID\n_INDEX\n_COLUMN\nInvalid index column for TRANSPOSE because:\n<reason>\n42805\n#\nGROUP\n_BY\n_POS\n_OUT\n_OF\n_RANGE\nGROUP BY position\n<index>\nis not in select list (valid range is [1,\n<size>\n]).\n42805\n#\nORDER\n_BY\n_POS\n_OUT\n_OF\n_RANGE\nORDER BY position\n<index>\nis not in select list (valid range is [1,\n<size>\n]).\n42809\n#\nEXPECT\n_PERMANENT\n_VIEW\n_NOT\n_TEMP\n'\n<operation>\n' expects a permanent view but\n<viewName>\nis a temp view.\n42809\n#\nEXPECT\n_TABLE\n_NOT\n_VIEW\n'\n<operation>\n' expects", "question": "What type must the column for corrupt records have?", "answers": {"text": ["The column\n<columnName>\nfor corrupt records must have the nullable STRING type"], "answer_start": [195]}}
{"context": "NENT\n_VIEW\n_NOT\n_TEMP\n'\n<operation>\n' expects a permanent view but\n<viewName>\nis a temp view.\n42809\n#\nEXPECT\n_TABLE\n_NOT\n_VIEW\n'\n<operation>\n' expects a table but\n<viewName>\nis a view.\n#\nNO\n_ALTERNATIVE\n#\nUSE\n_ALTER\n_VIEW\nPlease use ALTER VIEW instead.\n42809\n#\nEXPECT\n_VIEW\n_NOT\n_TABLE\nThe table\n<tableName>\ndoes not support\n<operation>\n.\n#\nNO\n_ALTERNATIVE\n#\nUSE\n_ALTER\n_TABLE\nPlease use ALTER TABLE instead.\n42809\n#\nFORBIDDEN\n_OPERATION\nThe operation\n<statement>\nis not allowed on the\n<objectType>\n:\n<objectName>\n.\n42809\n#\nNOT\n_A\n_PARTITIONED\n_TABLE\nOperation\n<operation>\nis not allowed for\n<tableIdentWithDB>\nbecause it is not a partitioned table.\n42809\n#\nUNSUPPORTED\n_INSERT\nCan't insert into the target.\n#\nMULTI\n_PATH\nCan only write data to relations with a single path but given paths are\n<paths", "question": "What should be used instead of the current operation when it expects a permanent view but receives a temp view?", "answers": {"text": ["Please use ALTER VIEW instead."], "answer_start": [222]}}
{"context": "de emitted a row with eventTime=\n<emittedRowEventTime>\nwhich is older than current_watermark_value=\n<currentWatermark>\nThis can lead to correctness issues in the stateful operators downstream in the execution pipeline. Please correct the operator logic to emit rows after current global watermark value.\n42818\n#\nINCOMPARABLE\n_PIVOT\n_COLUMN\nInvalid pivot column\n<columnName>\n. Pivot columns must be comparable.\n42822\n#\nEXPRESSION\n_TYPE\n_IS\n_NOT\n_ORDERABLE\nColumn expression\n<expr>\ncannot be sorted because its type\n<exprType>\nis not orderable.\n42822\n#\nGROUP\n_EXPRESSION\n_TYPE\n_IS\n_NOT\n_ORDERABLE\nThe expression\n<sqlExpr>\ncannot be used as a grouping expression because its data type\n<dataType>\nis not an orderable data type.\n42822\n#\nHINT\n_UNSUPPORTED\n_FOR\n_JDBC\n_DIALECT\nThe option\nhint\nis not support", "question": "What can happen if a row is emitted with an eventTime older than the current watermark?", "answers": {"text": ["This can lead to correctness issues in the stateful operators downstream in the execution pipeline."], "answer_start": [119]}}
{"context": "\n<invalidNumColumns>\ncolumns.\n42826\n#\nNUM\n_TABLE\n_VALUE\n_ALIASES\n_MISMATCH\nNumber of given aliases does not match number of output columns. Function name:\n<funcName>\n; number of aliases:\n<aliasesNum>\n; number of output columns:\n<outColsNum>\n.\n42836\n#\nINVALID\n_RECURSIVE\n_CTE\nInvalid recursive definition found. Recursive queries must contain an UNION or an UNION ALL statement with 2 children. The first child needs to be the anchor term without any recursive references.\n42836\n#\nINVALID\n_RECURSIVE\n_REFERENCE\nInvalid recursive reference found inside WITH RECURSIVE clause.\n#\nNUMBER\nMultiple self-references to one recursive CTE are not allowed.\n#\nPLACE\nRecursive references cannot be used on the right side of left outer/semi/anti joins, on the left side of right outer joins, in full outer joins, i", "question": "What is required in recursive queries to be valid?", "answers": {"text": ["Recursive queries must contain an UNION or an UNION ALL statement with 2 children."], "answer_start": [311]}}
{"context": "ACE\nRecursive references cannot be used on the right side of left outer/semi/anti joins, on the left side of right outer joins, in full outer joins, in aggregates, and in subquery expressions.\n42836\n#\nRECURSIVE\n_CTE\n_IN\n_LEGACY\n_MODE\nRecursive definitions cannot be used in legacy CTE precedence mode (spark.sql.legacy.ctePrecedencePolicy=LEGACY).\n42836\n#\nRECURSIVE\n_CTE\n_WHEN\n_INLINING\n_IS\n_FORCED\nRecursive definitions cannot be used when CTE inlining is forced.\n42845\n#\nAGGREGATE\n_FUNCTION\n_WITH\n_NONDETERMINISTIC\n_EXPRESSION\nNon-deterministic expression\n<sqlExpr>\nshould not appear in the arguments of an aggregate function.\n42846\n#\nCANNOT\n_CAST\n_DATATYPE\nCannot cast\n<sourceType>\nto\n<targetType>\n.\n42846\n#\nCANNOT\n_CONVERT\n_PROTOBUF\n_FIELD\n_TYPE\n_TO\n_SQL\n_TYPE\nCannot convert Protobuf\n<protobufCo", "question": "Em quais tipos de junções referências recursivas não podem ser usadas?", "answers": {"text": ["Recursive references cannot be used on the right side of left outer/semi/anti joins, on the left side of right outer joins, in full outer joins, in aggregates, and in subquery expressions."], "answer_start": [4]}}
{"context": "n column.\n42846\n#\nPARQUET\n_CONVERSION\n_FAILURE\nUnable to create a Parquet converter for the data type\n<dataType>\nwhose Parquet type is\n<parquetType>\n.\n#\nDECIMAL\nParquet DECIMAL type can only be backed by INT32, INT64, FIXED_LEN_BYTE_ARRAY, or BINARY.\n#\nUNSUPPORTED\nPlease modify the conversion making sure it is supported.\n#\nWITHOUT\n_DECIMAL\n_METADATA\nPlease read this column/field as Spark BINARY type.\n42846\n#\nPARQUET\n_TYPE\n_ILLEGAL\nIllegal Parquet type:\n<parquetType>\n.\n42846\n#\nPARQUET\n_TYPE\n_NOT\n_RECOGNIZED\nUnrecognized Parquet type:\n<field>\n.\n42846\n#\nPARQUET\n_TYPE\n_NOT\n_SUPPORTED\nParquet type not yet supported:\n<parquetType>\n.\n42846\n#\nUNEXPECTED\n_SERIALIZER\n_FOR\n_CLASS\nThe class\n<className>\nhas an unexpected expression serializer. Expects \"STRUCT\" or \"IF\" which returns \"STRUCT\" but found\n<", "question": "What types can back the Parquet DECIMAL type?", "answers": {"text": ["Parquet DECIMAL type can only be backed by INT32, INT64, FIXED_LEN_BYTE_ARRAY, or BINARY."], "answer_start": [161]}}
{"context": "lued function and that all required parameters are provided correctly. If\n<name>\nis not defined, please create the table-valued function before using it. For more information about defining table-valued functions, please refer to the Apache Spark documentation.\n42883\n#\nUNRESOLVED\n_ROUTINE\nCannot resolve routine\n<routineName>\non search path\n<searchPath>\n.\n42883\n#\nUNRESOLVED\n_VARIABLE\nCannot resolve variable\n<variableName>\non search path\n<searchPath>\n.\n42883\n#\nVARIABLE\n_NOT\n_FOUND\nThe variable\n<variableName>\ncannot be found. Verify the spelling and correctness of the schema and catalog. If you did not qualify the name with a schema and catalog, verify the current_schema() output, or qualify the name with the correct schema and catalog. To tolerate the error on drop use DROP VARIABLE IF EXIST", "question": "What should you do if <name> is not defined?", "answers": {"text": ["please create the table-valued function before using it."], "answer_start": [97]}}
{"context": " verify the current_schema() output, or qualify the name with the correct schema and catalog. To tolerate the error on drop use DROP VARIABLE IF EXISTS.\n428B3\n#\nINVALID\n_SQLSTATE\nInvalid SQLSTATE value: '\n<sqlState>\n'. SQLSTATE must be exactly 5 characters long and contain only A-Z and 0-9. SQLSTATE must not start with '00', '01', or 'XX'.\n428C4\n#\nUNPIVOT\n_VALUE\n_SIZE\n_MISMATCH\nAll unpivot value columns must have the same size as there are value column names (\n<names>\n).\n428EK\n#\nTEMP\n_VIEW\n_NAME\n_TOO\n_MANY\n_NAME\n_PARTS\nCREATE TEMPORARY VIEW or the corresponding Dataset APIs only accept single-part view names, but got:\n<actualName>\n.\n428FR\n#\nCANNOT\n_ALTER\n_COLLATION\n_BUCKET\n_COLUMN\nALTER TABLE (ALTER|CHANGE) COLUMN cannot change collation of type/subtypes of bucket columns, but found the bu", "question": "What is the requirement for the length and characters of SQLSTATE?", "answers": {"text": ["SQLSTATE must be exactly 5 characters long and contain only A-Z and 0-9."], "answer_start": [219]}}
{"context": "\nGROUP\n_BY\n_AGGREGATE\nAggregate functions are not allowed in GROUP BY, but found\n<sqlExpr>\n.\n42903\n#\nGROUP\n_BY\n_POS\n_AGGREGATE\nGROUP BY\n<index>\nrefers to an expression\n<aggExpr>\nthat contains an aggregate function. Aggregate functions are not allowed in GROUP BY.\n42903\n#\nINVALID\n_AGGREGATE\n_FILTER\nThe FILTER expression\n<filterExpr>\nin an aggregate function is invalid.\n#\nCONTAINS\n_AGGREGATE\nExpected a FILTER expression without an aggregation, but found\n<aggExpr>\n.\n#\nCONTAINS\n_WINDOW\n_FUNCTION\nExpected a FILTER expression without a window function, but found\n<windowExpr>\n.\n#\nNON\n_DETERMINISTIC\nExpected a deterministic FILTER expression.\n#\nNOT\n_BOOLEAN\nExpected a FILTER expression of the BOOLEAN type.\n42903\n#\nINVALID\n_WHERE\n_CONDITION\nThe WHERE condition\n<condition>\ncontains invalid expressio", "question": "What is not allowed in GROUP BY according to the text?", "answers": {"text": ["Aggregate functions are not allowed in GROUP BY."], "answer_start": [215]}}
{"context": "BOOLEAN\nExpected a FILTER expression of the BOOLEAN type.\n42903\n#\nINVALID\n_WHERE\n_CONDITION\nThe WHERE condition\n<condition>\ncontains invalid expressions:\n<expressionList>\n. Rewrite the query to avoid window functions, aggregate functions, and generator functions in the WHERE clause.\n42908\n#\nSPECIFY\n_CLUSTER\n_BY\n_WITH\n_BUCKETING\n_IS\n_NOT\n_ALLOWED\nCannot specify both CLUSTER BY and CLUSTERED BY INTO BUCKETS.\n42908\n#\nSPECIFY\n_CLUSTER\n_BY\n_WITH\n_PARTITIONED\n_BY\n_IS\n_NOT\n_ALLOWED\nCannot specify both CLUSTER BY and PARTITIONED BY.\n429BB\n#\nCANNOT\n_RECOGNIZE\n_HIVE\n_TYPE\nCannot recognize hive type string:\n<fieldType>\n, column:\n<fieldName>\n. The specified data type for the field cannot be recognized by Spark SQL. Please check the data type of the specified field and ensure that it is a valid Spark S", "question": "What error occurs when both CLUSTER BY and CLUSTERED BY INTO BUCKETS are specified?", "answers": {"text": ["Cannot specify both CLUSTER BY and CLUSTERED BY INTO BUCKETS."], "answer_start": [348]}}
{"context": "entType>\n\".\n#\nMAP\nThe definition of \"MAP\" type is incomplete. You must provide a key type and a value type. For example: \"MAP\n<TIMESTAMP, INT>\n\".\n#\nSTRUCT\nThe definition of \"STRUCT\" type is incomplete. You must provide at least one field type. For example: \"STRUCT\n<name STRING, phone DECIMAL(10, 0)>\n\".\n42K02\n#\nDATA\n_SOURCE\n_NOT\n_FOUND\nFailed to find the data source:\n<provider>\n. Make sure the provider name is correct and the package is properly registered and compatible with your Spark version.\n42K03\n#\nBATCH\n_METADATA\n_NOT\n_FOUND\nUnable to find batch\n<batchMetadataFile>\n.\n42K03\n#\nCANNOT\n_LOAD\n_PROTOBUF\n_CLASS\nCould not load Protobuf class with name\n<protobufClassName>\n.\n<explanation>\n.\n42K03\n#\nDATA\n_SOURCE\n_TABLE\n_SCHEMA\n_MISMATCH\nThe schema of the data source table does not match the expe", "question": "What is required to complete the definition of a \"MAP\" type?", "answers": {"text": ["You must provide a key type and a value type."], "answer_start": [62]}}
{"context": "h name\n<protobufClassName>\n.\n<explanation>\n.\n42K03\n#\nDATA\n_SOURCE\n_TABLE\n_SCHEMA\n_MISMATCH\nThe schema of the data source table does not match the expected schema. If you are using the DataFrameReader.schema API or creating a table, avoid specifying the schema. Data Source schema:\n<dsSchema>\nExpected schema:\n<expectedSchema>\n42K03\n#\nLOAD\n_DATA\n_PATH\n_NOT\n_EXISTS\nLOAD DATA input path does not exist:\n<path>\n.\n42K03\n#\nPATH\n_NOT\n_FOUND\nPath does not exist:\n<path>\n.\n42K03\n#\nRENAME\n_SRC\n_PATH\n_NOT\n_FOUND\nFailed to rename as\n<sourcePath>\nwas not found.\n42K03\n#\nSTDS\n_FAILED\n_TO\n_READ\n_OPERATOR\n_METADATA\nFailed to read the operator metadata for checkpointLocation=\n<checkpointLocation>\nand batchId=\n<batchId>\n. Either the file does not exist, or the file is corrupted. Rerun the streaming query to cons", "question": "What should you avoid doing if you encounter a DATA_SOURCE_TABLE_SCHEMA_MISMATCH error?", "answers": {"text": ["avoid specifying the schema."], "answer_start": [232]}}
{"context": "intLocation=\n<checkpointLocation>\nand batchId=\n<batchId>\n. Either the file does not exist, or the file is corrupted. Rerun the streaming query to construct the operator metadata, and report to the corresponding communities or vendors if the error persists.\n42K03\n#\nSTDS\n_FAILED\n_TO\n_READ\n_STATE\n_SCHEMA\nFailed to read the state schema. Either the file does not exist, or the file is corrupted. options:\n<sourceOptions>\n. Rerun the streaming query to construct the state schema, and report to the corresponding communities or vendors if the error persists.\n42K03\n#\nSTREAMING\n_STATEFUL\n_OPERATOR\n_NOT\n_MATCH\n_IN\n_STATE\n_METADATA\nStreaming stateful operator name does not match with the operator in state metadata. This likely to happen when user adds/removes/changes stateful operator of existing strea", "question": "What should a user do if the state schema fails to read, indicating a potentially corrupted or missing file?", "answers": {"text": ["Rerun the streaming query to construct the state schema, and report to the corresponding communities or vendors if the error persists."], "answer_start": [421]}}
{"context": "ator name does not match with the operator in state metadata. This likely to happen when user adds/removes/changes stateful operator of existing streaming query. Stateful operators in the metadata: [\n<OpsInMetadataSeq>\n]; Stateful operators in current batch: [\n<OpsInCurBatchSeq>\n].\n42K04\n#\nFAILED\n_RENAME\n_PATH\nFailed to rename\n<sourcePath>\nto\n<targetPath>\nas destination already exists.\n42K04\n#\nPATH\n_ALREADY\n_EXISTS\nPath\n<outputPath>\nalready exists. Set mode as \"overwrite\" to overwrite the existing path.\n42K05\n#\nINVALID\n_EMPTY\n_LOCATION\nThe location name cannot be empty string, but\n<location>\nwas given.\n42K05\n#\nREQUIRES\n_SINGLE\n_PART\n_NAMESPACE\n<sessionCatalog>\nrequires a single-part namespace, but got\n<namespace>\n.\n42K05\n#\nSHOW\n_COLUMNS\n_WITH\n_CONFLICT\n_NAMESPACE\nSHOW COLUMNS with conflict", "question": "What happens when a user adds, removes, or changes a stateful operator of an existing streaming query?", "answers": {"text": ["ator name does not match with the operator in state metadata. This likely to happen when user adds/removes/changes stateful operator of existing streaming query."], "answer_start": [0]}}
{"context": "E\n<sessionCatalog>\nrequires a single-part namespace, but got\n<namespace>\n.\n42K05\n#\nSHOW\n_COLUMNS\n_WITH\n_CONFLICT\n_NAMESPACE\nSHOW COLUMNS with conflicting namespaces:\n<namespaceA>\n!=\n<namespaceB>\n.\n42K06\n#\nINVALID\n_OPTIONS\nInvalid options:\n#\nNON\n_MAP\n_FUNCTION\nMust use the\nmap()\nfunction for options.\n#\nNON\n_STRING\n_TYPE\nA type of keys and values in\nmap()\nmust be string, but got\n<mapType>\n.\n42K06\n#\nSTATE\n_STORE\n_INVALID\n_CONFIG\n_AFTER\n_RESTART\nCannot change\n<configName>\nfrom\n<oldConfig>\nto\n<newConfig>\nbetween restarts. Please set\n<configName>\nto\n<oldConfig>\n, or restart with a new checkpoint directory.\n42K06\n#\nSTATE\n_STORE\n_INVALID\n_PROVIDER\nThe given State Store Provider\n<inputClass>\ndoes not extend org.apache.spark.sql.execution.streaming.state.StateStoreProvider.\n42K06\n#\nSTATE\n_STORE\n_INV", "question": "What function must be used for options?", "answers": {"text": ["Must use the\nmap()\nfunction for options."], "answer_start": [260]}}
{"context": "#\nSTATE\n_STORE\n_STATE\n_SCHEMA\n_FILES\n_THRESHOLD\n_EXCEEDED\nThe number of state schema files\n<numStateSchemaFiles>\nexceeds the maximum number of state schema files for this query:\n<maxStateSchemaFiles>\n. Added:\n<addedColumnFamilies>\n, Removed:\n<removedColumnFamilies>\nPlease set 'spark.sql.streaming.stateStore.stateSchemaFilesThreshold' to a higher number, or revert state schema modifications\n42K06\n#\nSTATE\n_STORE\n_VALUE\n_SCHEMA\n_EVOLUTION\n_THRESHOLD\n_EXCEEDED\nThe number of state schema evolutions\n<numSchemaEvolutions>\nexceeds the maximum number of state schema evolutions,\n<maxSchemaEvolutions>\n, allowed for this column family. Offending column family:\n<colFamilyName>\nPlease set 'spark.sql.streaming.stateStore.valueStateSchemaEvolutionThreshold' to a higher number, or revert state schema modif", "question": "What configuration setting should be adjusted if the number of state schema files exceeds the maximum allowed?", "answers": {"text": ["Please set 'spark.sql.streaming.stateStore.stateSchemaFilesThreshold' to a higher number, or revert state schema modifications"], "answer_start": [266]}}
{"context": "amily:\n<colFamilyName>\nPlease set 'spark.sql.streaming.stateStore.valueStateSchemaEvolutionThreshold' to a higher number, or revert state schema modifications\n42K07\n#\nINVALID\n_SCHEMA\nThe input schema\n<inputSchema>\nis not a valid schema string.\n#\nNON\n_STRING\n_LITERAL\nThe input expression must be string literal and not null.\n#\nNON\n_STRUCT\n_TYPE\nThe input expression should be evaluated to struct type, but got\n<dataType>\n.\n#\nPARSE\n_ERROR\nCannot parse the schema:\n<reason>\n42K08\n#\nINVALID\n_SQL\n_ARG\nThe argument\n<name>\nof\nsql()\nis invalid. Consider to replace it either by a SQL literal or by collection constructor functions such as\nmap()\n,\narray()\n,\nstruct()\n.\n42K08\n#\nNON\n_FOLDABLE\n_ARGUMENT\nThe function\n<funcName>\nrequires the parameter\n<paramName>\nto be a foldable expression of the type\n<paramT", "question": "What should be done if 'spark.sql.streaming.stateStore.valueStateSchemaEvolutionThreshold' is too low?", "answers": {"text": ["Please set 'spark.sql.streaming.stateStore.valueStateSchemaEvolutionThreshold' to a higher number, or revert state schema modifications"], "answer_start": [23]}}
{"context": "OnMapType\" to \"true\".\n#\nHASH\n_VARIANT\n_TYPE\nInput to the function\n<functionName>\ncannot contain elements of the \"VARIANT\" type yet.\n#\nINPUT\n_SIZE\n_NOT\n_ONE\nLength of\n<exprName>\nshould be 1.\n#\nINVALID\n_ARG\n_VALUE\nThe\n<inputName>\nvalue must to be a\n<requireType>\nliteral of\n<validValues>\n, but got\n<inputValue>\n.\n#\nINVALID\n_JSON\n_MAP\n_KEY\n_TYPE\nInput schema\n<schema>\ncan only contain STRING as a key type for a MAP.\n#\nINVALID\n_JSON\n_SCHEMA\nInput schema\n<schema>\nmust be a struct, an array, a map or a variant.\n#\nINVALID\n_MAP\n_KEY\n_TYPE\nThe key of map cannot be/contain\n<keyType>\n.\n#\nINVALID\n_ORDERING\n_TYPE\nThe\n<functionName>\ndoes not support ordering on type\n<dataType>\n.\n#\nINVALID\n_ROW\n_LEVEL\n_OPERATION\n_ASSIGNMENTS\n<errors>\n#\nINVALID\n_XML\n_MAP\n_KEY\n_TYPE\nInput schema\n<schema>\ncan only contain STRI", "question": "What types can an input schema contain as a key type for a MAP?", "answers": {"text": ["STRING"], "answer_start": [382]}}
{"context": "on type\n<dataType>\n.\n#\nINVALID\n_ROW\n_LEVEL\n_OPERATION\n_ASSIGNMENTS\n<errors>\n#\nINVALID\n_XML\n_MAP\n_KEY\n_TYPE\nInput schema\n<schema>\ncan only contain STRING as a key type for a MAP.\n#\nIN\n_SUBQUERY\n_DATA\n_TYPE\n_MISMATCH\nThe data type of one or more elements in the left hand side of an IN subquery is not compatible with the data type of the output of the subquery. Mismatched columns: [\n<mismatchedColumns>\n], left side: [\n<leftType>\n], right side: [\n<rightType>\n].\n#\nIN\n_SUBQUERY\n_LENGTH\n_MISMATCH\nThe number of columns in the left hand side of an IN subquery does not match the number of columns in the output of subquery. Left hand side columns(length:\n<leftLength>\n): [\n<leftColumns>\n], right hand side columns(length:\n<rightLength>\n): [\n<rightColumns>\n].\n#\nMAP\n_CONCAT\n_DIFF\n_TYPES\nThe\n<functionName", "question": "What is the restriction regarding key types in an input schema for a MAP?", "answers": {"text": ["can only contain STRING as a key type for a MAP."], "answer_start": [129]}}
{"context": ":\n<leftLength>\n): [\n<leftColumns>\n], right hand side columns(length:\n<rightLength>\n): [\n<rightColumns>\n].\n#\nMAP\n_CONCAT\n_DIFF\n_TYPES\nThe\n<functionName>\nshould all be of type map, but it's\n<dataType>\n.\n#\nMAP\n_FUNCTION\n_DIFF\n_TYPES\nInput to\n<functionName>\nshould have been\n<dataType>\nfollowed by a value with same key type, but it's [\n<leftType>\n,\n<rightType>\n].\n#\nMAP\n_ZIP\n_WITH\n_DIFF\n_TYPES\nInput to the\n<functionName>\nshould have been two maps with compatible key types, but it's [\n<leftType>\n,\n<rightType>\n].\n#\nNON\n_FOLDABLE\n_INPUT\nthe input\n<inputName>\nshould be a foldable\n<inputType>\nexpression; however, got\n<inputExpr>\n.\n#\nNON\n_STRING\n_TYPE\nall arguments of the function\n<funcName>\nmust be strings.\n#\nNON\n_STRUCT\n_TYPE\nthe input\n<inputName>\nshould be a struct expression; however, got\n<inputTy", "question": "What type should the input to <functionName> have been according to the error message?", "answers": {"text": ["<dataType>"], "answer_start": [188]}}
{"context": "ecification with multiple order by expressions:\n<orderSpec>\n.\n#\nRANGE\n_FRAME\n_WITHOUT\n_ORDER\nA range window frame cannot be used in an unordered window specification.\n#\nSEQUENCE\n_WRONG\n_INPUT\n_TYPES\n<functionName>\nuses the wrong parameter type. The parameter type must conform to: 1. The start and stop expressions must resolve to the same type. 2. If start and stop expressions resolve to the\n<startType>\ntype, then the step expression must resolve to the\n<stepType>\ntype. 3. Otherwise, if start and stop expressions resolve to the\n<otherStartType>\ntype, then the step expression must resolve to the same type.\n#\nSPECIFIED\n_WINDOW\n_FRAME\n_DIFF\n_TYPES\nWindow frame bounds\n<lower>\nand\n<upper>\ndo not have the same type:\n<lowerType>\n<>\n<upperType>\n.\n#\nSPECIFIED\n_WINDOW\n_FRAME\n_INVALID\n_BOUND\nWindow fr", "question": "What must the start and stop expressions conform to when using a function?", "answers": {"text": ["The start and stop expressions must resolve to the same type."], "answer_start": [284]}}
{"context": "ref-datetime-pattern.html'.\n#\nDATETIME\n_WEEK\n_BASED\n_PATTERN\nSpark >= 3.0: All week-based patterns are unsupported since Spark 3.0, detected week-based character:\n<c>\n. Please use the SQL function EXTRACT instead.\n#\nPARSE\n_DATETIME\n_BY\n_NEW\n_PARSER\nSpark >= 3.0: Fail to parse\n<datetime>\nin the new parser. You can set\n<config>\nto \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n#\nREAD\n_ANCIENT\n_DATETIME\nSpark >= 3.0: reading dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z from\n<format>\nfiles can be ambiguous, as the files may be written by Spark 2.x or legacy versions of Hive, which uses a legacy hybrid calendar that is different from Spark 3.0+'s Proleptic Gregorian calendar. See more details in SPARK-314", "question": "What should be used instead of week-based patterns in Spark >= 3.0?", "answers": {"text": ["Please use the SQL function EXTRACT instead."], "answer_start": [169]}}
{"context": "egacy hybrid calendar that is different from Spark 3.0+'s Proleptic Gregorian calendar. See more details in SPARK-31404. You can set\n<config>\nto \"LEGACY\" to rebase the datetime values w.r.t. the calendar difference during writing, to get maximum interoperability. Or set the config to \"CORRECTED\" to write the datetime values as it is, if you are sure that the written files will only be read by Spark 3.0+ or other systems that use Proleptic Gregorian calendar.\n42K0D\n#\nINVALID\n_LAMBDA\n_FUNCTION\n_CALL\nInvalid lambda function call.\n#\nDUPLICATE\n_ARG\n_NAMES\nThe lambda function has duplicate arguments\n<args>\n. Please, consider to rename the argument names or set\n<caseSensitiveConfig>\nto \"true\".\n#\nNON\n_HIGHER\n_ORDER\n_FUNCTION\nA lambda function should only be used in a higher order function. However", "question": "What can you set the <config> to for maximum interoperability when writing datetime values?", "answers": {"text": ["to \"LEGACY\" to rebase the datetime values w.r.t. the calendar difference during writing, to get maximum interoperability."], "answer_start": [142]}}
{"context": "names or set\n<caseSensitiveConfig>\nto \"true\".\n#\nNON\n_HIGHER\n_ORDER\n_FUNCTION\nA lambda function should only be used in a higher order function. However, its class is\n<class>\n, which is not a higher order function.\n#\nNUM\n_ARGS\n_MISMATCH\nA higher order function expects\n<expectedNumArgs>\narguments, but got\n<actualNumArgs>\n.\n#\nPARAMETER\n_DOES\n_NOT\n_ACCEPT\n_LAMBDA\n_FUNCTION\nYou passed a lambda function to a parameter that does not accept it. Please check if lambda function argument is in the correct position.\n42K0E\n#\nINVALID\n_LIMIT\n_LIKE\n_EXPRESSION\nThe limit like expression\n<expr>\nis invalid.\n#\nDATA\n_TYPE\nThe\n<name>\nexpression must be integer type, but got\n<dataType>\n.\n#\nIS\n_NEGATIVE\nThe\n<name>\nexpression must be equal to or greater than 0, but got\n<v>\n.\n#\nIS\n_NULL\nThe evaluated\n<name>\nexpressi", "question": "What should a lambda function only be used in?", "answers": {"text": ["A lambda function should only be used in a higher order function."], "answer_start": [77]}}
{"context": "te expression with FILTER predicate are not allowed in observed metrics, but found:\n<expr>\n.\n#\nMISSING\n_NAME\nThe observed metrics should be named:\n<operator>\n.\n#\nNESTED\n_AGGREGATES\n_UNSUPPORTED\nNested aggregates are not allowed in observed metrics, but found:\n<expr>\n.\n#\nNON\n_AGGREGATE\n_FUNC\n_ARG\n_IS\n_ATTRIBUTE\nAttribute\n<expr>\ncan only be used as an argument to an aggregate function.\n#\nNON\n_AGGREGATE\n_FUNC\n_ARG\n_IS\n_NON\n_DETERMINISTIC\nNon-deterministic expression\n<expr>\ncan only be used as an argument to an aggregate function.\n#\nWINDOW\n_EXPRESSIONS\n_UNSUPPORTED\nWindow expressions are not allowed in observed metrics, but found:\n<expr>\n.\n42K0E\n#\nINVALID\n_TIME\n_TRAVEL\n_SPEC\nCannot specify both version and timestamp when time travelling the table.\n42K0E\n#\nINVALID\n_TIME\n_TRAVEL\n_TIMESTAMP\n_EXPR", "question": "What is not allowed in observed metrics according to the text?", "answers": {"text": ["te expression with FILTER predicate are not allowed in observed metrics"], "answer_start": [0]}}
{"context": "#\nINVALID\n_TIME\n_TRAVEL\n_SPEC\nCannot specify both version and timestamp when time travelling the table.\n42K0E\n#\nINVALID\n_TIME\n_TRAVEL\n_TIMESTAMP\n_EXPR\nThe time travel timestamp expression\n<expr>\nis invalid.\n#\nINPUT\nCannot be casted to the \"TIMESTAMP\" type.\n#\nNON\n_DETERMINISTIC\nMust be deterministic.\n#\nOPTION\nTimestamp string in the options must be able to cast to TIMESTAMP type.\n#\nUNEVALUABLE\nMust be evaluable.\n42K0E\n#\nJOIN\n_CONDITION\n_IS\n_NOT\n_BOOLEAN\n_TYPE\nThe join condition\n<joinCondition>\nhas the invalid type\n<conditionType>\n, expected \"BOOLEAN\".\n42K0E\n#\nMULTIPLE\n_TIME\n_TRAVEL\n_SPEC\nCannot specify time travel in both the time travel clause and options.\n42K0E\n#\nMULTI\n_ALIAS\n_WITHOUT\n_GENERATOR\nMulti part aliasing (\n<names>\n) is not supported with\n<expr>\nas it is not a generator function", "question": "What happens when both version and timestamp are specified when time travelling the table?", "answers": {"text": ["Cannot specify both version and timestamp when time travelling the table."], "answer_start": [30]}}
{"context": "e and options.\n42K0E\n#\nMULTI\n_ALIAS\n_WITHOUT\n_GENERATOR\nMulti part aliasing (\n<names>\n) is not supported with\n<expr>\nas it is not a generator function.\n42K0E\n#\nMULTI\n_SOURCES\n_UNSUPPORTED\n_FOR\n_EXPRESSION\nThe expression\n<expr>\ndoes not support more than one source.\n42K0E\n#\nNO\n_MERGE\n_ACTION\n_SPECIFIED\ndf.mergeInto needs to be followed by at least one of whenMatched/whenNotMatched/whenNotMatchedBySource.\n42K0E\n#\nUNSUPPORTED\n_EXPR\n_FOR\n_OPERATOR\nA query operator contains one or more unsupported expressions. Consider to rewrite it to avoid window functions, aggregate functions, and generator functions in the WHERE clause. Invalid expressions: [\n<invalidExprSqls>\n]\n42K0E\n#\nUNSUPPORTED\n_EXPR\n_FOR\n_PARAMETER\nA query parameter contains unsupported expression. Parameters can either be variables or", "question": "What should follow df.mergeInto?", "answers": {"text": ["df.mergeInto needs to be followed by at least one of whenMatched/whenNotMatched/whenNotMatchedBySource."], "answer_start": [303]}}
{"context": "<invalidExprSqls>\n]\n42K0E\n#\nUNSUPPORTED\n_EXPR\n_FOR\n_PARAMETER\nA query parameter contains unsupported expression. Parameters can either be variables or literals. Invalid expression: [\n<invalidExprSql>\n]\n42K0E\n#\nUNSUPPORTED\n_GENERATOR\nThe generator is not supported:\n#\nMULTI\n_GENERATOR\nonly one generator allowed per SELECT clause but found\n<num>\n:\n<generators>\n.\n#\nNESTED\n_IN\n_EXPRESSIONS\nnested in expressions\n<expression>\n.\n#\nNOT\n_GENERATOR\n<functionName>\nis expected to be a generator. However, its class is\n<classCanonicalName>\n, which is not a generator.\n#\nOUTSIDE\n_SELECT\noutside the SELECT clause, found:\n<plan>\n.\n42K0E\n#\nUNSUPPORTED\n_GROUPING\n_EXPRESSION\ngrouping()/grouping_id() can only be used with GroupingSets/Cube/Rollup.\n42K0E\n#\nUNSUPPORTED\n_MERGE\n_CONDITION\nMERGE operation contains un", "question": "What does the error message 'UNSUPPORTED_EXPR_FOR_PARAMETER' indicate?", "answers": {"text": ["A query parameter contains unsupported expression. Parameters can either be variables or literals."], "answer_start": [62]}}
{"context": "_EXPRESSION\ngrouping()/grouping_id() can only be used with GroupingSets/Cube/Rollup.\n42K0E\n#\nUNSUPPORTED\n_MERGE\n_CONDITION\nMERGE operation contains unsupported\n<condName>\ncondition.\n#\nAGGREGATE\nAggregates are not allowed:\n<cond>\n.\n#\nNON\n_DETERMINISTIC\nNon-deterministic expressions are not allowed:\n<cond>\n.\n#\nSUBQUERY\nSubqueries are not allowed:\n<cond>\n.\n42K0E\n#\nUNTYPED\n_SCALA\n_UDF\nYou're using untyped Scala UDF, which does not have the input type information. Spark may blindly pass null to the Scala closure with primitive-type argument, and the closure will see the default value of the Java type for the null argument, e.g.\nudf((x: Int) => x, IntegerType)\n, the result is 0 for null input. To get rid of this error, you could: 1. use typed Scala UDF APIs(without return type parameter), e.g.\nu", "question": "What is the issue with using untyped Scala UDFs?", "answers": {"text": ["You're using untyped Scala UDF, which does not have the input type information. Spark may blindly pass null to the Scala closure with primitive-type argument, and the closure will see the default value of the Java type for the null argument, e.g.\nudf((x: Int) => x, IntegerType)\n, the result is 0 for null input."], "answer_start": [384]}}
{"context": "IntegerType)\n, the result is 0 for null input. To get rid of this error, you could: 1. use typed Scala UDF APIs(without return type parameter), e.g.\nudf((x: Int) => x)\n. 2. use Java UDF APIs, e.g.\nudf(new UDF1[String, Integer] { override def call(s: String): Integer = s.length() }, IntegerType)\n, if input types are all non primitive. 3. set \"spark.sql.legacy.allowUntypedScalaUDF\" to \"true\" and use this API with caution.\n42K0E\n#\nWINDOW\n_FUNCTION\n_AND\n_FRAME\n_MISMATCH\n<funcName>\nfunction can only be evaluated in an ordered row-based window frame with a single offset:\n<windowExpr>\n.\n42K0F\n#\nINVALID\n_TEMP\n_OBJ\n_REFERENCE\nCannot create the persistent object\n<objName>\nof the type\n<obj>\nbecause it references to the temporary object\n<tempObjName>\nof the type\n<tempObj>\n. Please make the temporary o", "question": "What can you do to get rid of the error when the result is 0 for null input with Scala UDF APIs?", "answers": {"text": ["use typed Scala UDF APIs(without return type parameter), e.g.\nudf((x: Int) => x)"], "answer_start": [87]}}
{"context": "and\n<orderingExpr>\ndo not match. The WITHIN GROUP ordering expression must be picked from the function inputs.\n#\nWITHIN\n_GROUP\n_MISSING\nWITHIN GROUP is required for the function.\n#\nWRONG\n_NUM\n_ORDERINGS\nThe function requires\n<expectedNum>\norderings in WITHIN GROUP but got\n<actualNum>\n.\n42K0L\n#\nEND\n_LABEL\n_WITHOUT\n_BEGIN\n_LABEL\nEnd label\n<endLabel>\ncan not exist without begin label.\n42K0L\n#\nINVALID\n_LABEL\n_USAGE\nThe usage of the label\n<labelName>\nis invalid.\n#\nDOES\n_NOT\n_EXIST\nLabel was used in the\n<statementType>\nstatement, but the label does not belong to any surrounding block.\n#\nITERATE\n_IN\n_COMPOUND\nITERATE statement cannot be used with a label that belongs to a compound (BEGIN...END) body.\n#\nQUALIFIED\n_LABEL\n_NAME\nLabel cannot be qualified.\n42K0L\n#\nLABELS\n_MISMATCH\nBegin label\n<beginLa", "question": "What is required for the function?", "answers": {"text": ["WITHIN GROUP is required for the function."], "answer_start": [136]}}
{"context": "label that belongs to a compound (BEGIN...END) body.\n#\nQUALIFIED\n_LABEL\n_NAME\nLabel cannot be qualified.\n42K0L\n#\nLABELS\n_MISMATCH\nBegin label\n<beginLabel>\ndoes not match the end label\n<endLabel>\n.\n42K0L\n#\nLABEL\n_ALREADY\n_EXISTS\nThe label\n<label>\nalready exists. Choose another name or rename the existing label.\n42K0L\n#\nLABEL\n_NAME\n_FORBIDDEN\nThe label name\n<label>\nis forbidden.\n42K0M\n#\nINVALID\n_VARIABLE\n_DECLARATION\nInvalid variable declaration.\n#\nNOT\n_ALLOWED\n_IN\n_SCOPE\nDeclaration of the variable\n<varName>\nis not allowed in this scope.\n#\nONLY\n_AT\n_BEGINNING\nVariable\n<varName>\ncan only be declared at the beginning of the compound.\n#\nQUALIFIED\n_LOCAL\n_VARIABLE\nThe variable\n<varName>\nmust be declared without a qualifier, as qualifiers are not allowed for local variable declarations.\n#\nREPLAC", "question": "What happens if a begin label does not match the end label?", "answers": {"text": ["does not match the end label"], "answer_start": [155]}}
{"context": "\n_LOCAL\n_VARIABLE\nThe variable\n<varName>\nmust be declared without a qualifier, as qualifiers are not allowed for local variable declarations.\n#\nREPLACE\n_LOCAL\n_VARIABLE\nThe variable\n<varName>\ndoes not support DECLARE OR REPLACE, as local variables cannot be replaced.\n42K0N\n#\nINVALID\n_EXTERNAL\n_TYPE\nThe external type\n<externalType>\nis not valid for the type\n<type>\nat the expression\n<expr>\n.\n42K0O\n#\nSCALAR\n_FUNCTION\n_NOT\n_COMPATIBLE\nScalarFunction\n<scalarFunc>\nnot overrides method 'produceResult(InternalRow)' with custom implementation.\n42K0P\n#\nSCALAR\n_FUNCTION\n_NOT\n_FULLY\n_IMPLEMENTED\nScalarFunction\n<scalarFunc>\nnot implements or overrides method 'produceResult(InternalRow)'.\n42K0Q\n#\nINVALID\n_HANDLER\n_DECLARATION\nInvalid handler declaration.\n#\nCONDITION\n_NOT\n_FOUND\nCondition\n<condition>\nnot", "question": "What is not allowed for local variable declarations?", "answers": {"text": ["qualifiers are not allowed for local variable declarations"], "answer_start": [82]}}
{"context": "ARATION\nHandlers must be declared after variable/condition declaration, and before other statements.\n42K0R\n#\nINVALID\n_ERROR\n_CONDITION\n_DECLARATION\nInvalid condition declaration.\n#\nNOT\n_AT\n_START\n_OF\n_COMPOUND\n_STATEMENT\nCondition\n<conditionName>\ncan only be declared at the start of a BEGIN END compound statement.\n#\nQUALIFIED\n_CONDITION\n_NAME\nCondition\n<conditionName>\ncannot be qualified.\n#\nSPECIAL\n_CHARACTER\n_FOUND\nSpecial character found in condition name\n<conditionName>\n. Only alphanumeric characters and underscores are allowed.\n42KD0\n#\nAMBIGUOUS\n_ALIAS\n_IN\n_NESTED\n_CTE\nName\n<name>\nis ambiguous in nested CTE. Please set\n<config>\nto \"CORRECTED\" so that name defined in inner CTE takes precedence. If set it to \"LEGACY\", outer CTE definitions will take precedence. See '\n<docroot>\n/sql-migra", "question": "What characters are allowed in a condition name?", "answers": {"text": ["Only alphanumeric characters and underscores are allowed."], "answer_start": [480]}}
{"context": "TED\" so that name defined in inner CTE takes precedence. If set it to \"LEGACY\", outer CTE definitions will take precedence. See '\n<docroot>\n/sql-migration-guide.html#query-engine'.\n42KD9\n#\nCANNOT\n_MERGE\n_SCHEMAS\nFailed merging schemas: Initial schema:\n<left>\nSchema that cannot be merged with the initial schema:\n<right>\n.\n42KD9\n#\nUNABLE\n_TO\n_INFER\n_SCHEMA\nUnable to infer schema for\n<format>\n. It must be specified manually.\n42KDE\n#\nCALL\n_ON\n_STREAMING\n_DATASET\n_UNSUPPORTED\nThe method\n<methodName>\ncan not be called on streaming Dataset/DataFrame.\n42KDE\n#\nCANNOT\n_CREATE\n_DATA\n_SOURCE\n_TABLE\nFailed to create data source table\n<tableName>\n:\n#\nEXTERNAL\n_METADATA\n_UNSUPPORTED\nprovider '\n<provider>\n' does not support external metadata but a schema is provided. Please remove the schema when creating", "question": "What should be set to allow inner CTE definitions to take precedence?", "answers": {"text": ["TED"], "answer_start": [0]}}
{"context": "NAL\n_METADATA\n_UNSUPPORTED\nprovider '\n<provider>\n' does not support external metadata but a schema is provided. Please remove the schema when creating the table.\n42KDE\n#\nINVALID\n_WRITER\n_COMMIT\n_MESSAGE\nThe data source writer has generated an invalid number of commit messages. Expected exactly one writer commit message from each task, but received\n<detail>\n.\n42KDE\n#\nNON\n_TIME\n_WINDOW\n_NOT\n_SUPPORTED\n_IN\n_STREAMING\nWindow function is not supported in\n<windowFunc>\n(as column\n<columnName>\n) on streaming DataFrames/Datasets. Structured Streaming only supports time-window aggregation using the WINDOW function. (window specification:\n<windowSpec>\n)\n42KDE\n#\nSTREAMING\n_OUTPUT\n_MODE\nInvalid streaming output mode:\n<outputMode>\n.\n#\nINVALID\nAccepted output modes are 'Append', 'Complete', 'Update'.\n#\nU", "question": "Quais são os modos de saída aceitos?", "answers": {"text": ["Accepted output modes are 'Append', 'Complete', 'Update'."], "answer_start": [739]}}
{"context": "\n42KDE\n#\nSTREAMING\n_OUTPUT\n_MODE\nInvalid streaming output mode:\n<outputMode>\n.\n#\nINVALID\nAccepted output modes are 'Append', 'Complete', 'Update'.\n#\nUNSUPPORTED\n_DATASOURCE\nThis output mode is not supported in Data Source\n<className>\n.\n#\nUNSUPPORTED\n_OPERATION\nThis output mode is not supported for\n<operation>\non streaming DataFrames/DataSets.\n42KDF\n#\nXML\n_ROW\n_TAG\n_MISSING\n<rowTag>\noption is required for reading/writing files in XML format.\n42P01\n#\nTABLE\n_OR\n_VIEW\n_NOT\n_FOUND\nThe table or view\n<relationName>\ncannot be found. Verify the spelling and correctness of the schema and catalog. If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog. To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABL", "question": "Quais são os modos de saída aceitos?", "answers": {"text": ["Accepted output modes are 'Append', 'Complete', 'Update'."], "answer_start": [89]}}
{"context": "e current_schema() output, or qualify the name with the correct schema and catalog. To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.\n42P01\n#\nVIEW\n_NOT\n_FOUND\nThe view\n<relationName>\ncannot be found. Verify the spelling and correctness of the schema and catalog. If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog. To tolerate the error on drop use DROP VIEW IF EXISTS.\n42P02\n#\nUNBOUND\n_SQL\n_PARAMETER\nFound the unbound parameter:\n<name>\n. Please, fix\nargs\nand provide a mapping of the parameter to either a SQL literal or collection constructor functions such as\nmap()\n,\narray()\n,\nstruct()\n.\n42P06\n#\nSCHEMA\n_ALREADY\n_EXISTS\nCannot create schema\n<schemaName>\nbecause it already exist", "question": "What should you verify if a view cannot be found?", "answers": {"text": ["Verify the spelling and correctness of the schema and catalog."], "answer_start": [229]}}
{"context": " constructor functions such as\nmap()\n,\narray()\n,\nstruct()\n.\n42P06\n#\nSCHEMA\n_ALREADY\n_EXISTS\nCannot create schema\n<schemaName>\nbecause it already exists. Choose a different name, drop the existing schema, or add the IF NOT EXISTS clause to tolerate pre-existing schema.\n42P07\n#\nTABLE\n_OR\n_VIEW\n_ALREADY\n_EXISTS\nCannot create table or view\n<relationName>\nbecause it already exists. Choose a different name, drop or replace the existing object, or add the IF NOT EXISTS clause to tolerate pre-existing objects.\n42P07\n#\nTEMP\n_TABLE\n_OR\n_VIEW\n_ALREADY\n_EXISTS\nCannot create the temporary view\n<relationName>\nbecause it already exists. Choose a different name, drop or replace the existing view,  or add the IF NOT EXISTS clause to tolerate pre-existing views.\n42P07\n#\nVIEW\n_ALREADY\n_EXISTS\nCannot create v", "question": "What should you do if you encounter the error 'Cannot create schema <schemaName> because it already exists'?", "answers": {"text": ["Choose a different name, drop the existing schema, or add the IF NOT EXISTS clause to tolerate pre-existing schema."], "answer_start": [153]}}
{"context": "d due to the mismatch between implicit collations: [\n<implicitTypes>\n]. Use COLLATE function to set the collation explicitly.\n42P22\n#\nINDETERMINATE\n_COLLATION\nCould not determine which collation to use for string operation. Use COLLATE clause to set the collation explicitly.\n42P22\n#\nINDETERMINATE\n_COLLATION\n_IN\n_EXPRESSION\nData type of\n<expr>\nhas indeterminate collation. Use COLLATE clause to set the collation explicitly.\n42P22\n#\nINDETERMINATE\n_COLLATION\n_IN\n_SCHEMA\nSchema contains indeterminate collation at: [\n<columnPaths>\n]. Use COLLATE clause to set the collation explicitly.\n42S22\n#\nNO\n_SQL\n_TYPE\n_IN\n_PROTOBUF\n_SCHEMA\nCannot find\n<catalystFieldPath>\nin Protobuf schema.\n42S23\n#\nPARTITION\n_TRANSFORM\n_EXPRESSION\n_NOT\n_IN\n_PARTITIONED\n_BY\nThe expression\n<expression>\nmust be inside 'partiti", "question": "What should be used to explicitly set the collation when a mismatch between implicit collations occurs?", "answers": {"text": ["Use COLLATE function to set the collation explicitly."], "answer_start": [72]}}
{"context": "tFieldPath>\nin Protobuf schema.\n42S23\n#\nPARTITION\n_TRANSFORM\n_EXPRESSION\n_NOT\n_IN\n_PARTITIONED\n_BY\nThe expression\n<expression>\nmust be inside 'partitionedBy'.\n46103\n#\nCANNOT\n_LOAD\n_FUNCTION\n_CLASS\nCannot load class\n<className>\nwhen registering the function\n<functionName>\n, please make sure it is on the classpath.\n46110\n#\nCANNOT\n_MODIFY\n_CONFIG\nCannot modify the value of the Spark config:\n<key>\n. See also '\n<docroot>\n/sql-migration-guide.html#ddl-statements'.\n46121\n#\nINVALID\n_COLUMN\n_NAME\n_AS\n_PATH\nThe datasource\n<datasource>\ncannot save the column\n<columnName>\nbecause its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.\n46121\n#\nINVALID\n_JAVA\n_IDENTIFIER\n_AS\n_FIELD\n_NAME\n<fieldName>\nis not a valid identifier of Java and cannot be used as f", "question": "What should you ensure when registering a function if a class cannot be loaded?", "answers": {"text": ["please make sure it is on the classpath."], "answer_start": [274]}}
{"context": "d.checkpoint()\ninstead, which is slower than local checkpointing but more fault-tolerant.\n56038\n#\nCODEC\n_NOT\n_AVAILABLE\nThe codec\n<codecName>\nis not available.\n#\nWITH\n_AVAILABLE\n_CODECS\n_SUGGESTION\nAvailable codecs are\n<availableCodecs>\n.\n#\nWITH\n_CONF\n_SUGGESTION\nConsider to set the config\n<configKey>\nto\n<configVal>\n.\n56038\n#\nFEATURE\n_NOT\n_ENABLED\nThe feature\n<featureName>\nis not enabled. Consider setting the config\n<configKey>\nto\n<configValue>\nto enable this capability.\n56038\n#\nGET\n_TABLES\n_BY\n_TYPE\n_UNSUPPORTED\n_BY\n_HIVE\n_VERSION\nHive 2.2 and lower versions don't support getTablesByType. Please use Hive 2.3 or higher version.\n56038\n#\nINCOMPATIBLE\n_DATASOURCE\n_REGISTER\nDetected an incompatible DataSourceRegister. Please remove the incompatible library from classpath or upgrade it. Error:\n", "question": "What should you do if an incompatible DataSourceRegister is detected?", "answers": {"text": ["Please remove the incompatible library from classpath or upgrade it."], "answer_start": [724]}}
{"context": "st belong to the same SparkSession.\n58030\n#\nCANNOT\n_LOAD\n_STATE\n_STORE\nAn error occurred during loading state.\n#\nCANNOT\n_FIND\n_BASE\n_SNAPSHOT\n_CHECKPOINT\nCannot find a base snapshot checkpoint with lineage:\n<lineage>\n.\n#\nCANNOT\n_READ\n_CHECKPOINT\nCannot read RocksDB checkpoint metadata. Expected\n<expectedVersion>\n, but found\n<actualVersion>\n.\n#\nCANNOT\n_READ\n_DELTA\n_FILE\n_KEY\n_SIZE\nError reading delta file\n<fileToRead>\nof\n<clazz>\n: key size cannot be\n<keySize>\n.\n#\nCANNOT\n_READ\n_DELTA\n_FILE\n_NOT\n_EXISTS\nError reading delta file\n<fileToRead>\nof\n<clazz>\n:\n<fileToRead>\ndoes not exist.\n#\nCANNOT\n_READ\n_MISSING\n_SNAPSHOT\n_FILE\nError reading snapshot file\n<fileToRead>\nof\n<clazz>\n:\n<fileToRead>\ndoes not exist.\n#\nCANNOT\n_READ\n_SNAPSHOT\n_FILE\n_KEY\n_SIZE\nError reading snapshot file\n<fileToRead>\nof\n<claz", "question": "What error occurs when a base snapshot checkpoint cannot be found?", "answers": {"text": ["Cannot find a base snapshot checkpoint with lineage:\n<lineage>\n."], "answer_start": [154]}}
{"context": "MORY\nExecutor memory\n<executorMemory>\nmust be at least\n<minSystemMemory>\n. Please increase executor memory using the --executor-memory option or \"\n<config>\n\" in Spark configuration.\nF0000\n#\nINVALID\n_KRYO\n_SERIALIZER\n_BUFFER\n_SIZE\nThe value of the config \"\n<bufferSizeConfKey>\n\" must be less than 2048 MiB, but got\n<bufferSizeConfValue>\nMiB.\nHV000\n#\nFAILED\n_JDBC\nFailed JDBC\n<url>\non the operation:\n#\nALTER\n_TABLE\nAlter the table\n<tableName>\n.\n#\nCREATE\n_INDEX\nCreate the index\n<indexName>\nin the\n<tableName>\ntable.\n#\nCREATE\n_NAMESPACE\nCreate the namespace\n<namespace>\n.\n#\nCREATE\n_NAMESPACE\n_COMMENT\nCreate a comment on the namespace:\n<namespace>\n.\n#\nCREATE\n_TABLE\nCreate the table\n<tableName>\n.\n#\nDROP\n_INDEX\nDrop the index\n<indexName>\nin the\n<tableName>\ntable.\n#\nDROP\n_NAMESPACE\nDrop the namespace\n<n", "question": "What should be done to increase executor memory?", "answers": {"text": ["Please increase executor memory using the --executor-memory option or"], "answer_start": [75]}}
{"context": ">\ndo(es) not exist. Available fields:\n<fieldNames>\nHY000\n#\nINVALID\n_HANDLE\nThe handle\n<handle>\nis invalid.\n#\nFORMAT\nHandle must be an UUID string of the format '00112233-4455-6677-8899-aabbccddeeff'\n#\nOPERATION\n_ABANDONED\nOperation was considered abandoned because of inactivity and removed.\n#\nOPERATION\n_ALREADY\n_EXISTS\nOperation already exists.\n#\nOPERATION\n_NOT\n_FOUND\nOperation not found.\n#\nSESSION\n_CHANGED\nThe existing Spark server driver instance has restarted. Please reconnect.\n#\nSESSION\n_CLOSED\nSession was closed.\n#\nSESSION\n_NOT\n_FOUND\nSession not found.\nHY000\n#\nMISSING\n_TIMEOUT\n_CONFIGURATION\nThe operation has timed out, but no timeout duration is configured. To set a processing time-based timeout, use 'GroupState.setTimeoutDuration()' in your 'mapGroupsWithState' or 'flatMapGroupsWit", "question": "What format must a handle be in?", "answers": {"text": ["Handle must be an UUID string of the format '00112233-4455-6677-8899-aabbccddeeff'"], "answer_start": [116]}}
{"context": "uration is configured. To set a processing time-based timeout, use 'GroupState.setTimeoutDuration()' in your 'mapGroupsWithState' or 'flatMapGroupsWithState' operation. For event-time-based timeout, use 'GroupState.setTimeoutTimestamp()' and define a watermark using 'Dataset.withWatermark()'.\nHY008\n#\nOPERATION\n_CANCELED\nOperation has been canceled.\nHY109\n#\nINVALID\n_CURSOR\nThe cursor is invalid.\n#\nDISCONNECTED\nThe cursor has been disconnected by the server.\n#\nNOT\n_REATTACHABLE\nThe cursor is not reattachable.\n#\nPOSITION\n_NOT\n_AVAILABLE\nThe cursor position id\n<responseId>\nis no longer available at index\n<index>\n.\n#\nPOSITION\n_NOT\n_FOUND\nThe cursor position id\n<responseId>\nis not found.\nKD000\n#\nFAILED\n_REGISTER\n_CLASS\n_WITH\n_KRYO\nFailed to register classes with Kryo.\nKD000\n#\nGRAPHITE\n_SINK\n_INV", "question": "How can a processing time-based timeout be set?", "answers": {"text": ["use 'GroupState.setTimeoutDuration()' in your 'mapGroupsWithState' or 'flatMapGroupsWithState' operation."], "answer_start": [63]}}
{"context": "cord:\n<badRecord>\nKD001\n#\nFAILED\n_READ\n_FILE\nEncountered error while reading file\n<path>\n.\n#\nCANNOT\n_READ\n_FILE\n_FOOTER\nCould not read footer. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.\n#\nFILE\n_NOT\n_EXIST\nFile does not exist. It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n#\nNO\n_HINT\n#\nPARQUET\n_COLUMN\n_DATA\n_TYPE\n_MISMATCH\nData type mismatches when reading Parquet column\n<column>\n. Expected Spark type\n<expectedType>\n, actual Parquet type\n", "question": "What should you do if you encounter an error while reading a file footer?", "answers": {"text": ["Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption."], "answer_start": [143]}}
{"context": "ET\n_COLUMN\n_DATA\n_TYPE\n_MISMATCH\nData type mismatches when reading Parquet column\n<column>\n. Expected Spark type\n<expectedType>\n, actual Parquet type\n<actualType>\n.\n#\nUNSUPPORTED\n_FILE\n_SYSTEM\nThe file system\n<fileSystemClass>\nhasn't implemented\n<method>\n.\nKD002\n#\nINVALID\n_LOG\n_VERSION\nUnsupportedLogVersion.\n#\nEXACT\n_MATCH\n_VERSION\nThe only supported log version is v\n<matchVersion>\n, but encountered v\n<version>\n.\n#\nMAX\n_SUPPORTED\n_VERSION\nThe maximum supported log version is v\n<maxSupportedVersion>\n, but encountered v\n<version>\n. The log file was produced by a newer version of Spark and cannot be read by this version. You need to upgrade.\nKD002\n#\nMALFORMED\n_LOG\n_FILE\nLog file was malformed: failed to read correct log version from\n<text>\n.\nKD005\n#\nALL\n_PARTITION\n_COLUMNS\n_NOT\n_ALLOWED\nCanno", "question": "What happens when the log file was produced by a newer version of Spark?", "answers": {"text": ["The log file was produced by a newer version of Spark and cannot be read by this version. You need to upgrade."], "answer_start": [536]}}
{"context": "heckpoint location:\n<checkpointLocation>\n. Please specify the batch ID which is available for querying - you can query the available batch IDs via using state metadata data source.\nKD006\n#\nSTDS\n_OFFSET\n_METADATA\n_LOG\n_UNAVAILABLE\nMetadata is not available for offset log for\n<batchId>\n, checkpoint location:\n<checkpointLocation>\n. The checkpoint seems to be only run with older Spark version(s). Run the streaming query with the recent Spark version, so that Spark constructs the state metadata.\nKD009\n#\nCONFLICTING\n_DIRECTORY\n_STRUCTURES\nConflicting directory structures detected. Suspicious paths:\n<discoveredBasePaths>\nIf provided paths are partition directories, please set \"basePath\" in the options of the data source to specify the root directory of the table. If there are multiple root direct", "question": "What should you do if conflicting directory structures are detected?", "answers": {"text": ["If provided paths are partition directories, please set \"basePath\" in the options of the data source to specify the root directory of the table."], "answer_start": [622]}}
{"context": "ion directories, please set \"basePath\" in the options of the data source to specify the root directory of the table. If there are multiple root directories, please load them separately and then union them.\nKD009\n#\nCONFLICTING\n_PARTITION\n_COLUMN\n_NAMES\nConflicting partition column names detected:\n<distinctPartColLists>\nFor partitioned table directories, data files should only live in leaf directories. And directories at the same level should have the same partition column name. Please check the following directories for unexpected files or inconsistent partition column names:\n<suspiciousPaths>\nKD00B\n#\nERROR\n_READING\n_AVRO\n_UNKNOWN\n_FINGERPRINT\nError reading avro data -- encountered an unknown fingerprint:\n<fingerprint>\n, not sure what schema to use. This could happen if you registered addit", "question": "What should you do if you have multiple root directories for ion directories?", "answers": {"text": ["If there are multiple root directories, please load them separately and then union them."], "answer_start": [117]}}
{"context": "\nError reading avro data -- encountered an unknown fingerprint:\n<fingerprint>\n, not sure what schema to use. This could happen if you registered additional schemas after starting your spark context.\nKD010\n#\nDATA\n_SOURCE\n_EXTERNAL\n_ERROR\nEncountered error when saving to external data source.\nP0001\n#\nUSER\n_RAISED\n_EXCEPTION\n<errorMessage>\nP0001\n#\nUSER\n_RAISED\n_EXCEPTION\n_PARAMETER\n_MISMATCH\nThe\nraise_error()\nfunction was used to raise error class:\n<errorClass>\nwhich expects parameters:\n<expectedParms>\n. The provided parameters\n<providedParms>\ndo not match the expected parameters. Please make sure to provide all expected parameters.\nP0001\n#\nUSER\n_RAISED\n_EXCEPTION\n_UNKNOWN\n_ERROR\n_CLASS\nThe\nraise_error()\nfunction was used to raise an unknown error class:\n<errorClass>\nXX000\n#\nAMBIGUOUS\n_RESOLV", "question": "What could cause an error when reading avro data due to an unknown fingerprint?", "answers": {"text": ["This could happen if you registered additional schemas after starting your spark context."], "answer_start": [109]}}
{"context": "empting to resolve a query or command with both the legacy fixed-point analyzer as well as the single-pass resolver.\n#\nFIXED\n_POINT\n_FAILED\n_SINGLE\n_PASS\n_SUCCEEDED\nFixed-point resolution failed, but single-pass resolution succeeded. Single-pass analyzer output:\n<singlePassOutput>\n#\nLOGICAL\n_PLAN\n_COMPARISON\n_MISMATCH\nOutputs of fixed-point and single-pass analyzers do not match. Fixed-point analyzer output:\n<fixedPointOutput>\nSingle-pass analyzer output:\n<singlePassOutput>\n#\nOUTPUT\n_SCHEMA\n_COMPARISON\n_MISMATCH\nOutput schemas of fixed-point and single-pass analyzers do not match. Fixed-point analyzer output schema:\n<fixedPointOutputSchema>\nSingle-pass analyzer output schema:\n<singlePassOutputSchema>\nXX000\n#\nMALFORMED\n_PROTOBUF\n_MESSAGE\nMalformed Protobuf messages are detected in message d", "question": "What happens when fixed-point resolution fails but single-pass resolution succeeds?", "answers": {"text": ["Fixed-point resolution failed, but single-pass resolution succeeded."], "answer_start": [165]}}
{"context": "ingle-pass analyzer output schema:\n<singlePassOutputSchema>\nXX000\n#\nMALFORMED\n_PROTOBUF\n_MESSAGE\nMalformed Protobuf messages are detected in message deserialization. Parse Mode:\n<failFastMode>\n. To process malformed protobuf message as null result, try setting the option 'mode' as 'PERMISSIVE'.\nXX000\n#\nMISSING\n_ATTRIBUTES\nResolved attribute(s)\n<missingAttributes>\nmissing from\n<input>\nin operator\n<operator>\n.\n#\nRESOLVED\n_ATTRIBUTE\n_APPEAR\n_IN\n_OPERATION\nAttribute(s) with the same name appear in the operation:\n<operation>\n. Please check if the right attribute(s) are used.\n#\nRESOLVED\n_ATTRIBUTE\n_MISSING\n_FROM\n_INPUT\nXX000\n#\nSTATE\n_STORE\n_KEY\n_ROW\n_FORMAT\n_VALIDATION\n_FAILURE\nThe streaming query failed to validate written state for key row. The following reasons may cause this: 1. An old Spark", "question": "What should you try setting the option 'mode' as to process malformed protobuf messages as null results?", "answers": {"text": ["PERMISSIVE"], "answer_start": [283]}}
{"context": "W\n_FORMAT\n_VALIDATION\n_FAILURE\nThe streaming query failed to validate written state for key row. The following reasons may cause this: 1. An old Spark version wrote the checkpoint that is incompatible with the current one 2. Corrupt checkpoint files 3. The query changed in an incompatible way between restarts For the first case, use a new checkpoint directory or use the original Spark version to process the streaming state. Retrieved error_message=\n<errorMsg>\nXX000\n#\nSTATE\n_STORE\n_VALUE\n_ROW\n_FORMAT\n_VALIDATION\n_FAILURE\nThe streaming query failed to validate written state for value row. The following reasons may cause this: 1. An old Spark version wrote the checkpoint that is incompatible with the current one 2. Corrupt checkpoint files 3. The query changed in an incompatible way between r", "question": "What are the possible reasons for a streaming query failing to validate written state for a key row?", "answers": {"text": ["1. An old Spark version wrote the checkpoint that is incompatible with the current one 2. Corrupt checkpoint files 3. The query changed in an incompatible way between restarts"], "answer_start": [135]}}
{"context": "rsion wrote the checkpoint that is incompatible with the current one 2. Corrupt checkpoint files 3. The query changed in an incompatible way between restarts For the first case, use a new checkpoint directory or use the original Spark version to process the streaming state. Retrieved error_message=\n<errorMsg>\nXXKD0\n#\nINVALID\n_SQL\n_FUNCTION\n_PLAN\n_STRUCTURE\nInvalid SQL function plan structure\n<plan>\nXXKD0\n#\nPLAN\n_VALIDATION\n_FAILED\n_RULE\n_EXECUTOR\nThe input plan of\n<ruleExecutor>\nis invalid:\n<reason>\nXXKD0\n#\nPLAN\n_VALIDATION\n_FAILED\n_RULE\n_IN\n_BATCH\nRule\n<rule>\nin batch\n<batch>\ngenerated an invalid plan:\n<reason>\nXXKDA\n#\nSPARK\n_JOB\n_CANCELLED\nJob\n<jobId>\ncancelled\n<reason>\nXXKST\n#\nSTATE\n_STORE\n_INVALID\n_VALUE\n_SCHEMA\n_EVOLUTION\nSchema evolution is not possible new value_schema=\n<newValueSch", "question": "What is indicated by the error message 'Invalid SQL function plan structure'?", "answers": {"text": ["Invalid SQL function plan structure"], "answer_start": [359]}}
{"context": "Job\n<jobId>\ncancelled\n<reason>\nXXKST\n#\nSTATE\n_STORE\n_INVALID\n_VALUE\n_SCHEMA\n_EVOLUTION\nSchema evolution is not possible new value_schema=\n<newValueSchema>\nand old value_schema=\n<oldValueSchema>\nPlease check https://avro.apache.org/docs/1.11.1/specification/_print/#schema-resolution for valid schema evolution.\nXXKST\n#\nSTATE\n_STORE\n_KEY\n_SCHEMA\n_NOT\n_COMPATIBLE\nProvided key schema does not match existing state key schema. Please check number and type of fields. Existing key_schema=\n<storedKeySchema>\nand new key_schema=\n<newKeySchema>\n. If you want to force running the query without schema validation, please set spark.sql.streaming.stateStore.stateSchemaCheck to false. However, please note that running the query with incompatible schema could cause non-deterministic behavior.\nXXKST\n#\nSTATE\n_S", "question": "What should be checked when schema evolution is not possible?", "answers": {"text": ["Please check https://avro.apache.org/docs/1.11.1/specification/_print/#schema-resolution for valid schema evolution."], "answer_start": [194]}}
{"context": "ateSchemaCheck to false. However, please note that running the query with incompatible schema could cause non-deterministic behavior.\nXXKST\n#\nSTATE\n_STORE\n_OPERATION\n_OUT\n_OF\n_ORDER\nStreaming stateful operator attempted to access state store out of order. This is a bug, please retry. error_msg=\n<errorMsg>\nXXKST\n#\nSTATE\n_STORE\n_UNSUPPORTED\n_OPERATION\n<operationType>\noperation not supported with\n<entity>\nXXKST\n#\nSTATE\n_STORE\n_UNSUPPORTED\n_OPERATION\n_BINARY\n_INEQUALITY\nBinary inequality column is not supported with state store. Provided schema:\n<schema>\n.\nXXKST\n#\nSTATE\n_STORE\n_VALUE\n_SCHEMA\n_NOT\n_COMPATIBLE\nProvided value schema does not match existing state value schema. Please check number and type of fields. Existing value_schema=\n<storedValueSchema>\nand new value_schema=\n<newValueSchema>\n", "question": "What happens if a query is run with an incompatible schema?", "answers": {"text": ["running the query with incompatible schema could cause non-deterministic behavior."], "answer_start": [51]}}
{"context": "xisting state value schema. Please check number and type of fields. Existing value_schema=\n<storedValueSchema>\nand new value_schema=\n<newValueSchema>\n. If you want to force running the query without schema validation, please set spark.sql.streaming.stateStore.stateSchemaCheck to false. However, please note that running the query with incompatible schema could cause non-deterministic behavior.\nXXKST\n#\nSTDS\n_INTERNAL\n_ERROR\nInternal error:\n<message>\nPlease, report this bug to the corresponding communities or vendors, and provide the full stack trace.\nXXKST\n#\nSTREAMING\n_PYTHON\n_RUNNER\n_INITIALIZATION\n_FAILURE\nStreaming Runner initialization failed, returned\n<resFromPython>\n. Cause:\n<msg>\nXXKST\n#\nSTREAM\n_FAILED\nQuery [id =\n<id>\n, runId =\n<runId>\n] terminated with exception:\n<message>\nXXKST\n#\nT", "question": "What should be set to false to force running the query without schema validation?", "answers": {"text": ["spark.sql.streaming.stateStore.stateSchemaCheck to false"], "answer_start": [229]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nStructured Streaming Programming Guide\nOverview\nGetting Started\nAPIs on DataFrames and Datasets\nCreating Streaming DataFrames and Streaming Datasets\nOperations on Streaming DataFrames/Datasets\nStarting Streaming Queries\nManaging Streaming Queries\nMoni", "question": "What are some of the programming guides available for Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)"], "answer_start": [46]}}
{"context": "ing Streaming DataFrames and Streaming Datasets\nOperations on Streaming DataFrames/Datasets\nStarting Streaming Queries\nManaging Streaming Queries\nMonitoring Streaming Queries\nRecovering from Failures with Checkpointing\nRecovery Semantics after Changes in a Streaming Query\nPerformance Tips\nAdditional Information\nStructured Streaming Programming Guide\nAPI using Datasets and DataFrames\nSince Spark 2.0, DataFrames and Datasets can represent static, bounded data, as well as streaming, unbounded data. Similar to static Datasets/DataFrames, you can use the common entry point\nSparkSession\n(\nPython\n/\nScala\n/\nJava\n/\nR\ndocs)\nto create streaming DataFrames/Datasets from streaming sources, and apply the same operations on them as static DataFrames/Datasets. If you are not familiar with Datasets/DataFra", "question": "What can DataFrames and Datasets represent since Spark 2.0?", "answers": {"text": ["Since Spark 2.0, DataFrames and Datasets can represent static, bounded data, as well as streaming, unbounded data."], "answer_start": [386]}}
{"context": "s\nThere are a few built-in sources.\nFile source\n- Reads files written in a directory as a stream of data. Files will be processed in the order of file modification time. If\nlatestFirst\nis set, order will be reversed. Supported file formats are text, CSV, JSON, ORC, Parquet. See the docs of the DataStreamReader interface for a more up-to-date list, and supported options for each file format. Note that the files must be atomically placed in the given directory, which in most file systems, can be achieved by file move operations.\nKafka source\n- Reads data from Kafka. It’s compatible with Kafka broker versions 0.10.0 or higher. See the\nKafka Integration Guide\nfor more details.\nSocket source (for testing)\n- Reads UTF8 text data from a socket connection. The listening server socket is at the dri", "question": "What file formats are supported by the File source?", "answers": {"text": ["Supported file formats are text, CSV, JSON, ORC, Parquet."], "answer_start": [217]}}
{"context": "gration Guide\nfor more details.\nSocket source (for testing)\n- Reads UTF8 text data from a socket connection. The listening server socket is at the driver. Note that this should be used only for testing as this does not provide end-to-end fault-tolerance guarantees.\nRate source (for testing)\n- Generates data at the specified number of rows per second, each output row contains a\ntimestamp\nand\nvalue\n. Where\ntimestamp\nis a\nTimestamp\ntype containing the time of message dispatch, and\nvalue\nis of\nLong\ntype containing the message count, starting from 0 as the first row. This source is intended for testing and benchmarking.\nRate Per Micro-Batch source (for testing)\n- Generates data at the specified number of rows per micro-batch, each output row contains a\ntimestamp\nand\nvalue\n. Where\ntimestamp\nis a", "question": "What type is the 'timestamp' field in the Rate source?", "answers": {"text": ["Timestamp"], "answer_start": [423]}}
{"context": " (for testing)\n- Generates data at the specified number of rows per micro-batch, each output row contains a\ntimestamp\nand\nvalue\n. Where\ntimestamp\nis a\nTimestamp\ntype containing the time of message dispatch, and\nvalue\nis of\nLong\ntype containing the message count, starting from 0 as the first row. Unlike\nrate\ndata source, this data source provides a consistent set of input rows per micro-batch regardless of query execution (configuration of trigger, query being lagging, etc.), say, batch 0 will produce 0~999 and batch 1 will produce 1000~1999, and so on. Same applies to the generated time. This source is intended for testing and benchmarking.\nSome sources are not fault-tolerant because they do not guarantee that data can be replayed using\ncheckpointed offsets after a failure. See the earlier", "question": "What does this data source provide consistently per micro-batch, regardless of query execution?", "answers": {"text": ["this data source provides a consistent set of input rows per micro-batch regardless of query execution (configuration of trigger, query being lagging, etc.), say, batch 0 will produce 0~999 and batch 1 will produce 1000~1999, and so on."], "answer_start": [322]}}
{"context": " two must be chosen. Note that a stream always reads at least one file so it can make progress and not get stuck on a file larger than a given maximum.\nlatestFirst\n: whether to process the latest new files first, useful when there is a large backlog of files (default: false)\nfileNameOnly\n: whether to check new files based on only the filename instead of on the full path (default: false). With this set to `true`, the following files would be considered as the same file, because their filenames, \"dataset.txt\", are the same:\n\"file:///dataset.txt\"\n\"s3://a/dataset.txt\"\n\"s3n://a/b/dataset.txt\"\n\"s3a://a/b/c/dataset.txt\"\nmaxFileAge\n: Maximum age of a file that can be found in this directory, before it is ignored. For the first batch all files will be considered valid. If\nlatestFirst\nis set to `tru", "question": "What happens when fileNameOnly is set to `true`?", "answers": {"text": ["With this set to `true`, the following files would be considered as the same file, because their filenames, \"dataset.txt\", are the same:"], "answer_start": [391]}}
{"context": "e input source.\ndiscardCachedInputRatio\n: ratio of cached files/bytes to max files/bytes to allow for listing from input source when there is less cached input than could be available to be read (default: 0.2).  For example, if there are only 10 cached files remaining for a batch but the\nmaxFilesPerTrigger\nis set to 100, the 10 cached files would be discarded and a new listing would be performed instead. Similarly, if there are cached files that are 10 MB remaining for a batch, but the\nmaxBytesPerTrigger\nis set to 100MB, the cached files would be discarded.\ncleanSource\n: option to clean up completed files after processing.\nAvailable options are \"archive\", \"delete\", \"off\". If the option is not provided, the default value is \"off\".\nWhen \"archive\" is provided, additional option\nsourceArchiveD", "question": "What happens when there are only 10 cached files remaining for a batch but the maxFilesPerTrigger is set to 100?", "answers": {"text": ["the 10 cached files would be discarded and a new listing would be performed instead."], "answer_start": [323]}}
{"context": "re \"archive\", \"delete\", \"off\". If the option is not provided, the default value is \"off\".\nWhen \"archive\" is provided, additional option\nsourceArchiveDir\nmust be provided as well. The value of \"sourceArchiveDir\" must not match with source pattern in depth (the number of directories from the root directory), where the depth is minimum of depth on both paths. This will ensure archived files are never included as new source files.\nFor example, suppose you provide '/hello?/spark/*' as source pattern, '/hello1/spark/archive/dir' cannot be used as the value of \"sourceArchiveDir\", as '/hello?/spark/*' and '/hello1/spark/archive' will be matched. '/hello1/spark' cannot be also used as the value of \"sourceArchiveDir\", as '/hello?/spark' and '/hello1/spark' will be matched. '/archived/here' would be ", "question": "What happens if the option is not provided?", "answers": {"text": ["If the option is not provided, the default value is \"off\"."], "answer_start": [31]}}
{"context": "llo1/spark' cannot be also used as the value of \"sourceArchiveDir\", as '/hello?/spark' and '/hello1/spark' will be matched. '/archived/here' would be OK as it doesn't match.\nSpark will move source files respecting their own path. For example, if the path of source file is\n/a/b/dataset.txt\nand the path of archive directory is\n/archived/here\n, file will be moved to\n/archived/here/a/b/dataset.txt\n.\nNOTE: Both archiving (via moving) or deleting completed files will introduce overhead (slow down, even if it's happening in separate thread) in each micro-batch, so you need to understand the cost for each operation in your file system before enabling this option. On the other hand, enabling this option will reduce the cost to list source files which can be an expensive operation.\nNumber of threads", "question": "What is an example of a source file path and archive directory, and where will the file be moved to?", "answers": {"text": ["/archived/here/a/b/dataset.txt"], "answer_start": [366]}}
{"context": " this option. On the other hand, enabling this option will reduce the cost to list source files which can be an expensive operation.\nNumber of threads used in completed file cleaner can be configured with\nspark.sql.streaming.fileSource.cleaner.numThreads\n(default: 1).\nNOTE 2: The source path should not be used from multiple sources or queries when enabling this option. Similarly, you must ensure the source path doesn't match to any files in output directory of file stream sink.\nNOTE 3: Both delete and move actions are best effort. Failing to delete or move files will not fail the streaming query. Spark may not clean up some source files in some circumstances - e.g. the application doesn't shut down gracefully, too many files are queued to clean up.\nFor file-format-specific options, see the", "question": "What is the default number of threads used in the completed file cleaner?", "answers": {"text": ["(default: 1)."], "answer_start": [255]}}
{"context": "me circumstances - e.g. the application doesn't shut down gracefully, too many files are queued to clean up.\nFor file-format-specific options, see the related methods in\nDataStreamReader\n(\nPython\n/\nScala\n/\nJava\n/\nR\n).\n        E.g. for \"parquet\" format options see\nDataStreamReader.parquet()\n.\nIn addition, there are session configurations that affect certain file-formats. See the\nSQL Programming Guide\nfor more details. E.g., for \"parquet\", see\nParquet configuration\nsection.\nYes\nSupports glob paths, but does not support multiple comma-separated paths/globs.\nSocket Source\nhost\n: host to connect to, must be specified\nport\n: port to connect to, must be specified\nNo\nRate Source\nrowsPerSecond\n(e.g. 100, default: 1): How many rows should be generated per second.\nrampUpTime\n(e.g. 5s, default: 0s): H", "question": "What is the default value for rowsPerSecond in Rate Source?", "answers": {"text": ["(e.g. 100, default: 1): How many rows should be generated per second."], "answer_start": [694]}}
{"context": "t be specified\nNo\nRate Source\nrowsPerSecond\n(e.g. 100, default: 1): How many rows should be generated per second.\nrampUpTime\n(e.g. 5s, default: 0s): How long to ramp up before the generating speed becomes\nrowsPerSecond\n. Using finer granularities than seconds will be truncated to integer seconds.\nnumPartitions\n(e.g. 10, default: Spark's default parallelism): The partition number for the generated rows.\nThe source will try its best to reach\nrowsPerSecond\n, but the query may be resource constrained, and\nnumPartitions\ncan be tweaked to help reach the desired speed.\nYes\nRate Per Micro-Batch Source\n(format:\nrate-micro-batch\n)\nrowsPerBatch\n(e.g. 100): How many rows should be generated per micro-batch.\nnumPartitions\n(e.g. 10, default: Spark's default parallelism): The partition number for the gen", "question": "What does the 'rowsPerSecond' parameter control?", "answers": {"text": ["How many rows should be generated per second."], "answer_start": [68]}}
{"context": "0): How many rows should be generated per micro-batch.\nnumPartitions\n(e.g. 10, default: Spark's default parallelism): The partition number for the generated rows.\nstartTimestamp\n(e.g. 1000, default: 0): starting value of generated time.\nadvanceMillisPerBatch\n(e.g. 1000, default: 1000): the amount of time being advanced in generated time on each micro-batch.\nYes\nKafka Source\nSee the\nKafka Integration Guide\n.\nYes\nHere are some examples.\nspark\n=\nSparkSession\n.\n...\n# Read text from socket\nsocketDF\n=\nspark\n\\\n.\nreadStream\n\\\n.\nformat\n(\n\"\nsocket\n\"\n)\n\\\n.\noption\n(\n\"\nhost\n\"\n,\n\"\nlocalhost\n\"\n)\n\\\n.\noption\n(\n\"\nport\n\"\n,\n9999\n)\n\\\n.\nload\n()\nsocketDF\n.\nisStreaming\n()\n# Returns True for DataFrames that have streaming sources\nsocketDF\n.\nprintSchema\n()\n# Read all the csv files written atomically in a directory\n", "question": "What does 'advanceMillisPerBatch' represent?", "answers": {"text": ["the amount of time being advanced in generated time on each micro-batch."], "answer_start": [287]}}
{"context": "ing\n()\n# Returns True for DataFrames that have streaming sources\nsocketDF\n.\nprintSchema\n()\n# Read all the csv files written atomically in a directory\nuserSchema\n=\nStructType\n().\nadd\n(\n\"\nname\n\"\n,\n\"\nstring\n\"\n).\nadd\n(\n\"\nage\n\"\n,\n\"\ninteger\n\"\n)\ncsvDF\n=\nspark\n\\\n.\nreadStream\n\\\n.\noption\n(\n\"\nsep\n\"\n,\n\"\n;\n\"\n)\n\\\n.\nschema\n(\nuserSchema\n)\n\\\n.\ncsv\n(\n\"\n/path/to/directory\n\"\n)\n# Equivalent to format(\"csv\").load(\"/path/to/directory\")\nval\nspark\n:\nSparkSession\n=\n...\n// Read text from socket\nval\nsocketDF\n=\nspark\n.\nreadStream\n.\nformat\n(\n\"socket\"\n)\n.\noption\n(\n\"host\"\n,\n\"localhost\"\n)\n.\noption\n(\n\"port\"\n,\n9999\n)\n.\nload\n()\nsocketDF\n.\nisStreaming\n// Returns True for DataFrames that have streaming sources\nsocketDF\n.\nprintSchema\n// Read all the csv files written atomically in a directory\nval\nuserSchema\n=\nnew\nStructType\n().", "question": "What does `socketDF.isStreaming` return?", "answers": {"text": ["Returns True for DataFrames that have streaming sources"], "answer_start": [9]}}
{"context": "mes that have streaming sources\nsocketDF\n.\nprintSchema\n// Read all the csv files written atomically in a directory\nval\nuserSchema\n=\nnew\nStructType\n().\nadd\n(\n\"name\"\n,\n\"string\"\n).\nadd\n(\n\"age\"\n,\n\"integer\"\n)\nval\ncsvDF\n=\nspark\n.\nreadStream\n.\noption\n(\n\"sep\"\n,\n\";\"\n)\n.\nschema\n(\nuserSchema\n)\n// Specify schema of the csv files\n.\ncsv\n(\n\"/path/to/directory\"\n)\n// Equivalent to format(\"csv\").load(\"/path/to/directory\")\nSparkSession\nspark\n=\n...\n// Read text from socket\nDataset\n<\nRow\n>\nsocketDF\n=\nspark\n.\nreadStream\n()\n.\nformat\n(\n\"socket\"\n)\n.\noption\n(\n\"host\"\n,\n\"localhost\"\n)\n.\noption\n(\n\"port\"\n,\n9999\n)\n.\nload\n();\nsocketDF\n.\nisStreaming\n();\n// Returns True for DataFrames that have streaming sources\nsocketDF\n.\nprintSchema\n();\n// Read all the csv files written atomically in a directory\nStructType\nuserSchema\n=\nne", "question": "What does `socketDF.isStreaming()` return for DataFrames?", "answers": {"text": ["True for DataFrames that have streaming sources"], "answer_start": [639]}}
{"context": "taFrames that have streaming sources\nsocketDF\n.\nprintSchema\n();\n// Read all the csv files written atomically in a directory\nStructType\nuserSchema\n=\nnew\nStructType\n().\nadd\n(\n\"name\"\n,\n\"string\"\n).\nadd\n(\n\"age\"\n,\n\"integer\"\n);\nDataset\n<\nRow\n>\ncsvDF\n=\nspark\n.\nreadStream\n()\n.\noption\n(\n\"sep\"\n,\n\";\"\n)\n.\nschema\n(\nuserSchema\n)\n// Specify schema of the csv files\n.\ncsv\n(\n\"/path/to/directory\"\n);\n// Equivalent to format(\"csv\").load(\"/path/to/directory\")\nsparkR.session\n(\n...\n)\n# Read text from socket\nsocketDF\n<-\nread.stream\n(\n\"socket\"\n,\nhost\n=\nhostname\n,\nport\n=\nport\n)\nisStreaming\n(\nsocketDF\n)\n# Returns TRUE for SparkDataFrames that have streaming sources\nprintSchema\n(\nsocketDF\n)\n# Read all the csv files written atomically in a directory\nschema\n<-\nstructType\n(\nstructField\n(\n\"name\"\n,\n\"string\"\n),\nstructField\n(", "question": "What does `isStreaming` return for SparkDataFrames?", "answers": {"text": ["TRUE for SparkDataFrames that have streaming sources"], "answer_start": [592]}}
{"context": "s as static DataFrame. See the\nSQL Programming Guide\nfor more details. Additionally, more details on the supported streaming sources are discussed later in the document.\nSince Spark 3.1, you can also create streaming DataFrames from tables with\nDataStreamReader.table()\n. See\nStreaming Table APIs\nfor more details.\nSchema inference and partition of streaming DataFrames/Datasets\nBy default, Structured Streaming from file based sources requires you to specify the schema, rather than rely on Spark to infer it automatically. This restriction ensures a consistent schema will be used for the streaming query, even in the case of failures. For ad-hoc use cases, you can reenable schema inference by setting\nspark.sql.streaming.schemaInference\nto\ntrue\n.\nPartition discovery does occur when subdirectorie", "question": "What does Structured Streaming from file based sources require you to specify by default?", "answers": {"text": ["By default, Structured Streaming from file based sources requires you to specify the schema, rather than rely on Spark to infer it automatically."], "answer_start": [379]}}
{"context": "ith IOT device data with schema { device: string, deviceType: string, signal: double, time: DateType }\n# Select the devices which have signal more than 10\ndf\n.\nselect\n(\n\"\ndevice\n\"\n).\nwhere\n(\n\"\nsignal > 10\n\"\n)\n# Running count of the number of updates for each device type\ndf\n.\ngroupBy\n(\n\"\ndeviceType\n\"\n).\ncount\n()\ncase\nclass\nDeviceData\n(\ndevice\n:\nString\n,\ndeviceType\n:\nString\n,\nsignal\n:\nDouble\n,\ntime\n:\nDateTime\n)\nval\ndf\n:\nDataFrame\n=\n...\n// streaming DataFrame with IOT device data with schema { device: string, deviceType: string, signal: double, time: string }\nval\nds\n:\nDataset\n[\nDeviceData\n]\n=\ndf\n.\nas\n[\nDeviceData\n]\n// streaming Dataset with IOT device data\n// Select the devices which have signal more than 10\ndf\n.\nselect\n(\n\"device\"\n).\nwhere\n(\n\"signal > 10\"\n)\n// using untyped APIs\nds\n.\nfilter\n(", "question": "What is the schema of the IOT device data?", "answers": {"text": ["{ device: string, deviceType: string, signal: double, time: DateType }"], "answer_start": [32]}}
{"context": "sing untyped APIs\nds\n.\nfilter\n((\nFilterFunction\n<\nDeviceData\n>)\nvalue\n->\nvalue\n.\ngetSignal\n()\n>\n10\n)\n.\nmap\n((\nMapFunction\n<\nDeviceData\n,\nString\n>)\nvalue\n->\nvalue\n.\ngetDevice\n(),\nEncoders\n.\nSTRING\n());\n// Running count of the number of updates for each device type\ndf\n.\ngroupBy\n(\n\"deviceType\"\n).\ncount\n();\n// using untyped API\n// Running average signal for each device type\nds\n.\ngroupByKey\n((\nMapFunction\n<\nDeviceData\n,\nString\n>)\nvalue\n->\nvalue\n.\ngetDeviceType\n(),\nEncoders\n.\nSTRING\n())\n.\nagg\n(\ntyped\n.\navg\n((\nMapFunction\n<\nDeviceData\n,\nDouble\n>)\nvalue\n->\nvalue\n.\ngetSignal\n()));\ndf\n<-\n...\n# streaming DataFrame with IOT device data with schema { device: string, deviceType: string, signal: double, time: DateType }\n# Select the devices which have signal more than 10\nselect\n(\nwhere\n(\ndf\n,\n\"signal > 1", "question": "What is the schema of the streaming DataFrame with IOT device data?", "answers": {"text": ["{ device: string, deviceType: string, signal: double, time: DateType }"], "answer_start": [644]}}
{"context": "ce: string, deviceType: string, signal: double, time: DateType }\n# Select the devices which have signal more than 10\nselect\n(\nwhere\n(\ndf\n,\n\"signal > 10\"\n),\n\"device\"\n)\n# Running count of the number of updates for each device type\ncount\n(\ngroupBy\n(\ndf\n,\n\"deviceType\"\n))\nYou can also register a streaming DataFrame/Dataset as a temporary view and then apply SQL commands on it.\ndf\n.\ncreateOrReplaceTempView\n(\n\"\nupdates\n\"\n)\nspark\n.\nsql\n(\n\"\nselect count(*) from updates\n\"\n)\n# returns another streaming DF\ndf\n.\ncreateOrReplaceTempView\n(\n\"updates\"\n)\nspark\n.\nsql\n(\n\"select count(*) from updates\"\n)\n// returns another streaming DF\ndf\n.\ncreateOrReplaceTempView\n(\n\"updates\"\n);\nspark\n.\nsql\n(\n\"select count(*) from updates\"\n);\n// returns another streaming DF\ncreateOrReplaceTempView\n(\ndf\n,\n\"updates\"\n)\nsql\n(\n\"sele", "question": "How can you apply SQL commands on a streaming DataFrame/Dataset?", "answers": {"text": ["You can also register a streaming DataFrame/Dataset as a temporary view and then apply SQL commands on it."], "answer_start": [268]}}
{"context": "\n(\n\"updates\"\n);\nspark\n.\nsql\n(\n\"select count(*) from updates\"\n);\n// returns another streaming DF\ncreateOrReplaceTempView\n(\ndf\n,\n\"updates\"\n)\nsql\n(\n\"select count(*) from updates\"\n)\nNote, you can identify whether a DataFrame/Dataset has streaming data or not by using\ndf.isStreaming\n.\ndf\n.\nisStreaming\n()\ndf\n.\nisStreaming\ndf\n.\nisStreaming\n()\nisStreaming\n(\ndf\n)\nYou may want to check the query plan of the query, as Spark could inject stateful operations during interpret of SQL statement against streaming dataset. Once stateful operations are injected in the query plan, you may need to check your query with considerations in stateful operations. (e.g. output mode, watermark, state store size maintenance, etc.)\nWindow Operations on Event Time\nAggregations over a sliding event-time window are straigh", "question": "How can you determine if a DataFrame/Dataset contains streaming data?", "answers": {"text": ["df.isStreaming"], "answer_start": [264]}}
{"context": " output mode, watermark, state store size maintenance, etc.)\nWindow Operations on Event Time\nAggregations over a sliding event-time window are straightforward with Structured Streaming and are very similar to grouped aggregations. In a grouped aggregation, aggregate values (e.g. counts) are maintained for each unique value in the user-specified grouping column. In case of window-based aggregations, aggregate values are maintained for each window the event-time of a row falls into. Let’s understand this with an illustration.\nImagine our\nquick example\nis modified and the stream now contains lines along with the time when the line was generated. Instead of running word counts, we want to count words within 10 minute windows, updating every 5 minutes. That is, word counts in words received bet", "question": "What is maintained for each window in window-based aggregations?", "answers": {"text": ["aggregate values are maintained for each window the event-time of a row falls into."], "answer_start": [402]}}
{"context": " Instead of running word counts, we want to count words within 10 minute windows, updating every 5 minutes. That is, word counts in words received between 10 minute windows 12:00 - 12:10, 12:05 - 12:15, 12:10 - 12:20, etc. Note that 12:00 - 12:10 means data that arrived after 12:00 but before 12:10. Now, consider a word that was received at 12:07. This word should increment the counts corresponding to two windows 12:00 - 12:10 and 12:05 - 12:15. So the counts will be indexed by both, the grouping key (i.e. the word) and the window (can be calculated from the event-time).\nThe result tables would look something like the following.\nSince this windowing is similar to grouping, in code, you can use\ngroupBy()\nand\nwindow()\noperations to express windowed aggregations. You can see the full code for", "question": "How should a word received at 12:07 be counted in terms of windows?", "answers": {"text": ["This word should increment the counts corresponding to two windows 12:00 - 12:10 and 12:05 - 12:15."], "answer_start": [350]}}
{"context": "each group\nval\nwindowedCounts\n=\nwords\n.\ngroupBy\n(\nwindow\n(\n$\n\"timestamp\"\n,\n\"10 minutes\"\n,\n\"5 minutes\"\n),\n$\n\"word\"\n).\ncount\n()\nDataset\n<\nRow\n>\nwords\n=\n...\n// streaming DataFrame of schema { timestamp: Timestamp, word: String }\n// Group the data by window and word and compute the count of each group\nDataset\n<\nRow\n>\nwindowedCounts\n=\nwords\n.\ngroupBy\n(\nfunctions\n.\nwindow\n(\nwords\n.\ncol\n(\n\"timestamp\"\n),\n\"10 minutes\"\n,\n\"5 minutes\"\n),\nwords\n.\ncol\n(\n\"word\"\n)\n).\ncount\n();\nwords\n<-\n...\n# streaming DataFrame of schema { timestamp: Timestamp, word: String }\n# Group the data by window and word and compute the count of each group\nwindowedCounts\n<-\ncount\n(\ngroupBy\n(\nwords\n,\nwindow\n(\nwords\n$\ntimestamp\n,\n\"10 minutes\"\n,\n\"5 minutes\"\n),\nwords\n$\nword\n))\nHandling Late Data and Watermarking\nNow consider what happe", "question": "What is done to the data by grouping it by window and word?", "answers": {"text": ["count of each group"], "answer_start": [279]}}
{"context": "orrectly, as illustrated below.\nHowever, to run this query for days, it’s necessary for the system to bound the amount of\nintermediate in-memory state it accumulates. This means the system needs to know when an old\naggregate can be dropped from the in-memory state because the application is not going to receive\nlate data for that aggregate any more. To enable this, in Spark 2.1, we have introduced\nwatermarking\n, which lets the engine automatically track the current event time in the data\nand attempt to clean up old state accordingly. You can define the watermark of a query by\nspecifying the event time column and the threshold on how late the data is expected to be in terms of\nevent time. For a specific window ending at time\nT\n, the engine will maintain state and allow late\ndata to update t", "question": "What was introduced in Spark 2.1 to automatically track the current event time and clean up old state?", "answers": {"text": ["watermarking"], "answer_start": [401]}}
{"context": "data is expected to be in terms of\nevent time. For a specific window ending at time\nT\n, the engine will maintain state and allow late\ndata to update the state until\n(max event time seen by the engine - late threshold > T)\n.\nIn other words, late data within the threshold will be aggregated,\nbut data later than the threshold will start getting dropped\n(see\nlater\nin the section for the exact guarantees). Let’s understand this with an example. We can\neasily define watermarking on the previous example using\nwithWatermark()\nas shown below.\nwords\n=\n...\n# streaming DataFrame of schema { timestamp: Timestamp, word: String }\n# Group the data by window and word and compute the count of each group\nwindowedCounts\n=\nwords\n\\\n.\nwithWatermark\n(\n\"\ntimestamp\n\"\n,\n\"\n10 minutes\n\"\n)\n\\\n.\ngroupBy\n(\nwindow\n(\nwords\n", "question": "What does the engine do with late data within the threshold?", "answers": {"text": ["late data within the threshold will be aggregated"], "answer_start": [240]}}
{"context": "imestamp, word: String }\n// Group the data by window and word and compute the count of each group\nDataset\n<\nRow\n>\nwindowedCounts\n=\nwords\n.\nwithWatermark\n(\n\"timestamp\"\n,\n\"10 minutes\"\n)\n.\ngroupBy\n(\nwindow\n(\ncol\n(\n\"timestamp\"\n),\n\"10 minutes\"\n,\n\"5 minutes\"\n),\ncol\n(\n\"word\"\n))\n.\ncount\n();\nwords\n<-\n...\n# streaming DataFrame of schema { timestamp: Timestamp, word: String }\n# Group the data by window and word and compute the count of each group\nwords\n<-\nwithWatermark\n(\nwords\n,\n\"timestamp\"\n,\n\"10 minutes\"\n)\nwindowedCounts\n<-\ncount\n(\ngroupBy\n(\nwords\n,\nwindow\n(\nwords\n$\ntimestamp\n,\n\"10 minutes\"\n,\n\"5 minutes\"\n),\nwords\n$\nword\n))\nIn this example, we are defining the watermark of the query on the value of the column “timestamp”,\nand also defining “10 minutes” as the threshold of how late is the data allowed", "question": "What is defined as the threshold for how late the data is allowed?", "answers": {"text": ["“10 minutes” as the threshold of how late is the data allowed"], "answer_start": [739]}}
{"context": "0)\nis cleared, and all subsequent data (e.g.\n(12:04, donkey)\n)\nis considered “too late” and therefore ignored. Note that after every trigger,\nthe updated counts (i.e. purple rows) are written to sink as the trigger output, as dictated by\nthe Update mode.\nSome sinks (e.g. files) may not supported fine-grained updates that Update Mode requires. To work\nwith them, we have also support Append Mode, where only the\nfinal counts\nare written to sink.\nThis is illustrated below.\nNote that using\nwithWatermark\non a non-streaming Dataset is no-op. As the watermark should not affect\nany batch query in any way, we will ignore it directly.\nSimilar to the Update Mode earlier, the engine maintains intermediate counts for each window.\nHowever, the partial counts are not updated to the Result Table and not wr", "question": "What happens to data considered \"too late\"?", "answers": {"text": ["is considered “too late” and therefore ignored."], "answer_start": [63]}}
{"context": "ate Mode earlier, the engine maintains intermediate counts for each window.\nHowever, the partial counts are not updated to the Result Table and not written to sink. The engine\nwaits for “10 mins” for late date to be counted,\nthen drops intermediate state of a window < watermark, and appends the final\ncounts to the Result Table/sink. For example, the final counts of window\n12:00 - 12:10\nis\nappended to the Result Table only after the watermark is updated to\n12:11\n.\nTypes of time windows\nSpark supports three types of time windows: tumbling (fixed), sliding and session.\nTumbling windows are a series of fixed-sized, non-overlapping and contiguous time intervals. An input\ncan only be bound to a single window.\nSliding windows are similar to the tumbling windows from the point of being “fixed-size", "question": "What happens to intermediate counts before the late data is counted?", "answers": {"text": ["However, the partial counts are not updated to the Result Table and not written to sink."], "answer_start": [76]}}
{"context": "\nwhen\n(\n$\n\"userId\"\n===\n\"user2\"\n,\n\"20 seconds\"\n)\n.\notherwise\n(\n\"5 minutes\"\n))\n// Group the data by session window and userId, and compute the count of each group\nval\nsessionizedCounts\n=\nevents\n.\nwithWatermark\n(\n\"timestamp\"\n,\n\"10 minutes\"\n)\n.\ngroupBy\n(\nColumn\n(\nsessionWindow\n),\n$\n\"userId\"\n)\n.\ncount\n()\nDataset\n<\nRow\n>\nevents\n=\n...\n// streaming DataFrame of schema { timestamp: Timestamp, userId: String }\nSessionWindow\nsessionWindow\n=\nsession_window\n(\ncol\n(\n\"timestamp\"\n),\nwhen\n(\ncol\n(\n\"userId\"\n).\nequalTo\n(\n\"user1\"\n),\n\"5 seconds\"\n)\n.\nwhen\n(\ncol\n(\n\"userId\"\n).\nequalTo\n(\n\"user2\"\n),\n\"20 seconds\"\n)\n.\notherwise\n(\n\"5 minutes\"\n))\n// Group the data by session window and userId, and compute the count of each group\nDataset\n<\nRow\n>\nsessionizedCounts\n=\nevents\n.\nwithWatermark\n(\n\"timestamp\"\n,\n\"10 minutes\"\n)\n.\n", "question": "What session window is used when the userId is equal to \"user2\"?", "answers": {"text": ["\"20 seconds\""], "answer_start": [33]}}
{"context": "on window and userId, and compute the count of each group\nDataset\n<\nRow\n>\nsessionizedCounts\n=\nevents\n.\nwithWatermark\n(\n\"timestamp\"\n,\n\"10 minutes\"\n)\n.\ngroupBy\n(\nnew\nColumn\n(\nsessionWindow\n),\ncol\n(\n\"userId\"\n))\n.\ncount\n();\nNote that there are some restrictions when you use session window in streaming query, like below:\n“Update mode” as output mode is not supported.\nThere should be at least one column in addition to\nsession_window\nin grouping key.\nFor batch query, global window (only having\nsession_window\nin grouping key) is supported.\nBy default, Spark does not perform partial aggregation for session window aggregation, since it requires additional\nsort in local partitions before grouping. It works better for the case there are only few number of input rows in\nsame group key for each local pa", "question": "What is a restriction when using session window in a streaming query?", "answers": {"text": ["“Update mode” as output mode is not supported."], "answer_start": [318]}}
{"context": "uires timestamp.\nwords\n=\n...\n# streaming DataFrame of schema { timestamp: Timestamp, word: String }\n# Group the data by window and word and compute the count of each group\nwindowedCounts\n=\nwords\n.\ngroupBy\n(\nwindow\n(\nwords\n.\ntimestamp\n,\n\"\n10 minutes\n\"\n,\n\"\n5 minutes\n\"\n),\nwords\n.\nword\n).\ncount\n()\n# Group the windowed data by another window and word and compute the count of each group\nanotherWindowedCounts\n=\nwindowedCounts\n.\ngroupBy\n(\nwindow\n(\nwindow_time\n(\nwindowedCounts\n.\nwindow\n),\n\"\n1 hour\n\"\n),\nwindowedCounts\n.\nword\n).\ncount\n()\nimport\nspark.implicits._\nval\nwords\n=\n...\n// streaming DataFrame of schema { timestamp: Timestamp, word: String }\n// Group the data by window and word and compute the count of each group\nval\nwindowedCounts\n=\nwords\n.\ngroupBy\n(\nwindow\n(\n$\n\"timestamp\"\n,\n\"10 minutes\"\n,\n\"5", "question": "What is the schema of the initial streaming DataFrame?", "answers": {"text": ["streaming DataFrame of schema { timestamp: Timestamp, word: String }"], "answer_start": [31]}}
{"context": ",\n\"10 minutes\"\n,\n\"5 minutes\"\n),\n$\n\"word\"\n).\ncount\n()\n// Group the windowed data by another window and word and compute the count of each group\nval\nanotherWindowedCounts\n=\nwindowedCounts\n.\ngroupBy\n(\nwindow\n(\n$\n\"window\"\n,\n\"1 hour\"\n),\n$\n\"word\"\n).\ncount\n()\nDataset\n<\nRow\n>\nwords\n=\n...\n// streaming DataFrame of schema { timestamp: Timestamp, word: String }\n// Group the data by window and word and compute the count of each group\nDataset\n<\nRow\n>\nwindowedCounts\n=\nwords\n.\ngroupBy\n(\nfunctions\n.\nwindow\n(\nwords\n.\ncol\n(\n\"timestamp\"\n),\n\"10 minutes\"\n,\n\"5 minutes\"\n),\nwords\n.\ncol\n(\n\"word\"\n)\n).\ncount\n();\n// Group the windowed data by another window and word and compute the count of each group\nDataset\n<\nRow\n>\nanotherWindowedCounts\n=\nwindowedCounts\n.\ngroupBy\n(\nfunctions\n.\nwindow\n(\n\"window\"\n,\n\"1 hour\"\n),\nwindow", "question": "What is the duration of the first window used for grouping the data in `windowedCounts`?", "answers": {"text": ["\"10 minutes\""], "answer_start": [2]}}
{"context": " compute the count of each group\nDataset\n<\nRow\n>\nanotherWindowedCounts\n=\nwindowedCounts\n.\ngroupBy\n(\nfunctions\n.\nwindow\n(\n\"window\"\n,\n\"1 hour\"\n),\nwindowedCounts\n.\ncol\n(\n\"word\"\n)\n).\ncount\n();\nConditions for watermarking to clean aggregation state\nIt is important to note that the following conditions must be satisfied for the watermarking to\nclean the state in aggregation queries\n(as of Spark 2.1.1, subject to change in the future)\n.\nOutput mode must be Append or Update.\nComplete mode requires all aggregate data to be preserved,\nand hence cannot use watermarking to drop intermediate state. See the\nOutput Modes\nsection for detailed explanation of the semantics of each output mode.\nThe aggregation must have either the event-time column, or a\nwindow\non the event-time column.\nwithWatermark\nmust be", "question": "Quais são os modos de saída que permitem o uso de watermarking para descartar o estado intermediário?", "answers": {"text": ["Output mode must be Append or Update."], "answer_start": [434]}}
{"context": " streaming and a static DataFrame/Dataset. Here is a simple example.\nstaticDf\n=\nspark\n.\nread\n.\n...\nstreamingDf\n=\nspark\n.\nreadStream\n.\n...\nstreamingDf\n.\njoin\n(\nstaticDf\n,\n\"\ntype\n\"\n)\n# inner equi-join with a static DF\nstreamingDf\n.\njoin\n(\nstaticDf\n,\n\"\ntype\n\"\n,\n\"\nleft_outer\n\"\n)\n# left outer join with a static DF\nval\nstaticDf\n=\nspark\n.\nread\n.\n...\nval\nstreamingDf\n=\nspark\n.\nreadStream\n.\n...\nstreamingDf\n.\njoin\n(\nstaticDf\n,\n\"type\"\n)\n// inner equi-join with a static DF\nstreamingDf\n.\njoin\n(\nstaticDf\n,\n\"type\"\n,\n\"left_outer\"\n)\n// left outer join with a static DF\nDataset\n<\nRow\n>\nstaticDf\n=\nspark\n.\nread\n().\n...;\nDataset\n<\nRow\n>\nstreamingDf\n=\nspark\n.\nreadStream\n().\n...;\nstreamingDf\n.\njoin\n(\nstaticDf\n,\n\"type\"\n);\n// inner equi-join with a static DF\nstreamingDf\n.\njoin\n(\nstaticDf\n,\n\"type\"\n,\n\"left_outer\"\n);\n/", "question": "What type of join is performed when using `streamingDf.join(staticDf, \"type\")`?", "answers": {"text": ["inner equi-join with a static DF"], "answer_start": [183]}}
{"context": "ream\n().\n...;\nstreamingDf\n.\njoin\n(\nstaticDf\n,\n\"type\"\n);\n// inner equi-join with a static DF\nstreamingDf\n.\njoin\n(\nstaticDf\n,\n\"type\"\n,\n\"left_outer\"\n);\n// left outer join with a static DF\nstaticDf\n<-\nread.df\n(\n...\n)\nstreamingDf\n<-\nread.stream\n(\n...\n)\njoined\n<-\nmerge\n(\nstreamingDf\n,\nstaticDf\n,\nsort\n=\nFALSE\n)\n# inner equi-join with a static DF\njoined\n<-\njoin\n(\nstreamingDf\n,\nstaticDf\n,\nstreamingDf\n$\nvalue\n==\nstaticDf\n$\nvalue\n,\n\"left_outer\"\n)\n# left outer join with a static DF\nNote that stream-static joins are not stateful, so no state management is necessary.\nHowever, a few types of stream-static outer joins are not yet supported.\nThese are listed at the\nend of this Join section\n.\nStream-stream Joins\nIn Spark 2.3, we have added support for stream-stream joins, that is, you can join two streaming", "question": "What type of join is not stateful, requiring no state management?", "answers": {"text": ["Note that stream-static joins are not stateful, so no state management is necessary."], "answer_start": [475]}}
{"context": "clickTime <= impressionTime + interval 1 hour\n\"\"\"\n)\n)\nimport\norg.apache.spark.sql.functions.expr\nval\nimpressions\n=\nspark\n.\nreadStream\n.\n...\nval\nclicks\n=\nspark\n.\nreadStream\n.\n...\n// Apply watermarks on event-time columns\nval\nimpressionsWithWatermark\n=\nimpressions\n.\nwithWatermark\n(\n\"impressionTime\"\n,\n\"2 hours\"\n)\nval\nclicksWithWatermark\n=\nclicks\n.\nwithWatermark\n(\n\"clickTime\"\n,\n\"3 hours\"\n)\n// Join with event-time constraints\nimpressionsWithWatermark\n.\njoin\n(\nclicksWithWatermark\n,\nexpr\n(\n\"\"\"\n    clickAdId = impressionAdId AND\n    clickTime >= impressionTime AND\n    clickTime <= impressionTime + interval 1 hour\n    \"\"\"\n)\n)\nimport\nstatic\norg\n.\napache\n.\nspark\n.\nsql\n.\nfunctions\n.\nexpr\nDataset\n<\nRow\n>\nimpressions\n=\nspark\n.\nreadStream\n().\n...\nDataset\n<\nRow\n>\nclicks\n=\nspark\n.\nreadStream\n().\n...\n// App", "question": "What is the event-time constraint used in the join operation between impressions and clicks?", "answers": {"text": ["clickTime <= impressionTime + interval 1 hour"], "answer_start": [0]}}
{"context": "e\n.\nspark\n.\nsql\n.\nfunctions\n.\nexpr\nDataset\n<\nRow\n>\nimpressions\n=\nspark\n.\nreadStream\n().\n...\nDataset\n<\nRow\n>\nclicks\n=\nspark\n.\nreadStream\n().\n...\n// Apply watermarks on event-time columns\nDataset\n<\nRow\n>\nimpressionsWithWatermark\n=\nimpressions\n.\nwithWatermark\n(\n\"impressionTime\"\n,\n\"2 hours\"\n);\nDataset\n<\nRow\n>\nclicksWithWatermark\n=\nclicks\n.\nwithWatermark\n(\n\"clickTime\"\n,\n\"3 hours\"\n);\n// Join with event-time constraints\nimpressionsWithWatermark\n.\njoin\n(\nclicksWithWatermark\n,\nexpr\n(\n\"clickAdId = impressionAdId AND \"\n+\n\"clickTime >= impressionTime AND \"\n+\n\"clickTime <= impressionTime + interval 1 hour \"\n)\n);\nimpressions\n<-\nread.stream\n(\n...\n)\nclicks\n<-\nread.stream\n(\n...\n)\n# Apply watermarks on event-time columns\nimpressionsWithWatermark\n<-\nwithWatermark\n(\nimpressions\n,\n\"impressionTime\"\n,\n\"2 hours\"\n", "question": "What watermark is applied to the 'impressionTime' column?", "answers": {"text": ["\"2 hours\""], "answer_start": [278]}}
{"context": " be an additional parameter specifying it to be an outer-join.\nimpressionsWithWatermark\n.\njoin\n(\nclicksWithWatermark\n,\nexpr\n(\n\"\"\"\nclickAdId = impressionAdId AND\n    clickTime >= impressionTime AND\n    clickTime <= impressionTime + interval 1 hour\n\"\"\"\n),\n\"\nleftOuter\n\"\n# can be \"inner\", \"leftOuter\", \"rightOuter\", \"fullOuter\", \"leftSemi\"\n)\nimpressionsWithWatermark\n.\njoin\n(\nclicksWithWatermark\n,\nexpr\n(\n\"\"\"\n    clickAdId = impressionAdId AND\n    clickTime >= impressionTime AND\n    clickTime <= impressionTime + interval 1 hour\n    \"\"\"\n),\njoinType\n=\n\"leftOuter\"\n// can be \"inner\", \"leftOuter\", \"rightOuter\", \"fullOuter\", \"leftSemi\"\n)\nimpressionsWithWatermark\n.\njoin\n(\nclicksWithWatermark\n,\nexpr\n(\n\"clickAdId = impressionAdId AND \"\n+\n\"clickTime >= impressionTime AND \"\n+\n\"clickTime <= impressionTime + ", "question": "What join types are available?", "answers": {"text": ["can be \"inner\", \"leftOuter\", \"rightOuter\", \"fullOuter\", \"leftSemi\""], "answer_start": [270]}}
{"context": "termark\n.\njoin\n(\nclicksWithWatermark\n,\nexpr\n(\n\"clickAdId = impressionAdId AND \"\n+\n\"clickTime >= impressionTime AND \"\n+\n\"clickTime <= impressionTime + interval 1 hour \"\n),\n\"leftOuter\"\n// can be \"inner\", \"leftOuter\", \"rightOuter\", \"fullOuter\", \"leftSemi\"\n);\njoined\n<-\njoin\n(\nimpressionsWithWatermark\n,\nclicksWithWatermark\n,\nexpr\n(\npaste\n(\n\"clickAdId = impressionAdId AND\"\n,\n\"clickTime >= impressionTime AND\"\n,\n\"clickTime <= impressionTime + interval 1 hour\"\n),\n\"left_outer\"\n# can be \"inner\", \"left_outer\", \"right_outer\", \"full_outer\", \"left_semi\"\n))\nSemantic Guarantees of Stream-stream Outer Joins with Watermarking\nOuter joins have the same guarantees as\ninner joins\nregarding watermark delays and whether data will be dropped or not.\nCaveats\nThere are a few important characteristics to note regardi", "question": "Quais são os tipos de junções que podem ser especificadas no código?", "answers": {"text": ["can be \"inner\", \"leftOuter\", \"rightOuter\", \"fullOuter\", \"leftSemi\""], "answer_start": [186]}}
{"context": "ults. Since we trigger a micro-batch only when there is new data to be processed, the\ngeneration of the outer result may get delayed if there no new data being received in the stream.\nIn short, if any of the two input streams being joined does not receive data for a while, the\nouter (both cases, left or right) output may get delayed.\nSemi Joins with Watermarking\nA semi join returns values from the left side of the relation that has a match with the right.\nIt is also referred to as a left semi join. Similar to outer joins, watermark + event-time\nconstraints must be specified for semi join. This is to evict unmatched input rows on left side,\nthe engine must know when an input row on left side is not going to match with anything on right\nside in future.\nSemantic Guarantees of Stream-stream Se", "question": "What is a semi join also referred to as?", "answers": {"text": ["It is also referred to as a left semi join."], "answer_start": [460]}}
{"context": "e engine must know when an input row on left side is not going to match with anything on right\nside in future.\nSemantic Guarantees of Stream-stream Semi Joins with Watermarking\nSemi joins have the same guarantees as\ninner joins\nregarding watermark delays and whether data will be dropped or not.\nSupport matrix for joins in streaming queries\nLeft Input\nRight Input\nJoin Type\nStatic\nStatic\nAll types\nSupported, since its not on streaming data even though it\n        can be present in a streaming query\nStream\nStatic\nInner\nSupported, not stateful\nLeft Outer\nSupported, not stateful\nRight Outer\nNot supported\nFull Outer\nNot supported\nLeft Semi\nSupported, not stateful\nStatic\nStream\nInner\nSupported, not stateful\nLeft Outer\nNot supported\nRight Outer\nSupported, not stateful\nFull Outer\nNot supported\nLeft ", "question": "What join types are supported when the left input is a stream and the right input is static?", "answers": {"text": ["Inner\nSupported, not stateful\nLeft Outer\nSupported, not stateful\nRight Outer\nNot supported\nFull Outer\nNot supported\nLeft Semi\nSupported, not stateful"], "answer_start": [515]}}
{"context": ", not stateful\nStatic\nStream\nInner\nSupported, not stateful\nLeft Outer\nNot supported\nRight Outer\nSupported, not stateful\nFull Outer\nNot supported\nLeft Semi\nNot supported\nStream\nStream\nInner\nSupported, optionally specify watermark on both sides +\n      time constraints for state cleanup\nLeft Outer\nConditionally supported, must specify watermark on right + time constraints for correct\n      results, optionally specify watermark on left for all state cleanup\nRight Outer\nConditionally supported, must specify watermark on left + time constraints for correct\n      results, optionally specify watermark on right for all state cleanup\nFull Outer\nConditionally supported, must specify watermark on one side + time constraints for correct\n      results, optionally specify watermark on the other side for", "question": "Which join types are supported, but not stateful?", "answers": {"text": ["Supported, not stateful"], "answer_start": [35]}}
{"context": "ionally supported, must specify watermark on one side + time constraints for correct\n      results, optionally specify watermark on the other side for all state cleanup\nLeft Semi\nConditionally supported, must specify watermark on right + time constraints for correct\n      results, optionally specify watermark on left for all state cleanup\nAdditional details on supported joins:\nJoins can be cascaded, that is, you can do\ndf1.join(df2, ...).join(df3, ...).join(df4, ....)\n.\nAs of Spark 2.4, you can use joins only when the query is in Append output mode. Other output modes are not yet supported.\nYou cannot use mapGroupsWithState and flatMapGroupsWithState before and after joins.\nIn append output mode, you can construct a query having non-map-like operations e.g. aggregation, deduplication, stre", "question": "Em qual modo de saída você pode usar joins a partir do Spark 2.4?", "answers": {"text": ["As of Spark 2.4, you can use joins only when the query is in Append output mode."], "answer_start": [475]}}
{"context": "ithState before and after joins.\nIn append output mode, you can construct a query having non-map-like operations e.g. aggregation, deduplication, stream-stream join before/after join.\nFor example, here’s an example of time window aggregation in both streams followed by stream-stream join with event time window:\nclicksWindow\n=\nclicksWithWatermark\n.\ngroupBy\n(\nclicksWithWatermark\n.\nclickAdId\n,\nwindow\n(\nclicksWithWatermark\n.\nclickTime\n,\n\"\n1 hour\n\"\n)\n).\ncount\n()\nimpressionsWindow\n=\nimpressionsWithWatermark\n.\ngroupBy\n(\nimpressionsWithWatermark\n.\nimpressionAdId\n,\nwindow\n(\nimpressionsWithWatermark\n.\nimpressionTime\n,\n\"\n1 hour\n\"\n)\n).\ncount\n()\nclicksWindow\n.\njoin\n(\nimpressionsWindow\n,\n\"\nwindow\n\"\n,\n\"\ninner\n\"\n)\nval\nclicksWindow\n=\nclicksWithWatermark\n.\ngroupBy\n(\nwindow\n(\n\"clickTime\"\n,\n\"1 hour\"\n))\n.\ncoun", "question": "What type of operations can be used in append output mode before/after a join?", "answers": {"text": ["non-map-like operations e.g. aggregation, deduplication, stream-stream join"], "answer_start": [89]}}
{"context": "dow\n.\njoin\n(\nimpressionsWindow\n,\n\"\nwindow\n\"\n,\n\"\ninner\n\"\n)\nval\nclicksWindow\n=\nclicksWithWatermark\n.\ngroupBy\n(\nwindow\n(\n\"clickTime\"\n,\n\"1 hour\"\n))\n.\ncount\n()\nval\nimpressionsWindow\n=\nimpressionsWithWatermark\n.\ngroupBy\n(\nwindow\n(\n\"impressionTime\"\n,\n\"1 hour\"\n))\n.\ncount\n()\nclicksWindow\n.\njoin\n(\nimpressionsWindow\n,\n\"window\"\n,\n\"inner\"\n)\nDataset\n<\nRow\n>\nclicksWindow\n=\nclicksWithWatermark\n.\ngroupBy\n(\nfunctions\n.\nwindow\n(\nclicksWithWatermark\n.\ncol\n(\n\"clickTime\"\n),\n\"1 hour\"\n))\n.\ncount\n();\nDataset\n<\nRow\n>\nimpressionsWindow\n=\nimpressionsWithWatermark\n.\ngroupBy\n(\nfunctions\n.\nwindow\n(\nimpressionsWithWatermark\n.\ncol\n(\n\"impressionTime\"\n),\n\"1 hour\"\n))\n.\ncount\n();\nclicksWindow\n.\njoin\n(\nimpressionsWindow\n,\n\"window\"\n,\n\"inner\"\n);\nHere’s another example of stream-stream join with time range join condition followed", "question": "What type of join is performed between clicksWindow and impressionsWindow?", "answers": {"text": ["\"inner\""], "answer_start": [320]}}
{"context": ";\nclicksWindow\n.\njoin\n(\nimpressionsWindow\n,\n\"window\"\n,\n\"inner\"\n);\nHere’s another example of stream-stream join with time range join condition followed by time window aggregation:\njoined\n=\nimpressionsWithWatermark\n.\njoin\n(\nclicksWithWatermark\n,\nexpr\n(\n\"\"\"\nclickAdId = impressionAdId AND\n    clickTime >= impressionTime AND\n    clickTime <= impressionTime + interval 1 hour\n\"\"\"\n),\n\"\nleftOuter\n\"\n# can be \"inner\", \"leftOuter\", \"rightOuter\", \"fullOuter\", \"leftSemi\"\n)\njoined\n.\ngroupBy\n(\njoined\n.\nclickAdId\n,\nwindow\n(\njoined\n.\nclickTime\n,\n\"\n1 hour\n\"\n)\n).\ncount\n()\nval\njoined\n=\nimpressionsWithWatermark\n.\njoin\n(\nclicksWithWatermark\n,\nexpr\n(\n\"\"\"\n    clickAdId = impressionAdId AND\n    clickTime >= impressionTime AND\n    clickTime <= impressionTime + interval 1 hour\n  \"\"\"\n),\njoinType\n=\n\"leftOuter\"\n// can b", "question": "What are the possible join types in the stream-stream join operation?", "answers": {"text": ["can be \"inner\", \"leftOuter\", \"rightOuter\", \"fullOuter\", \"leftSemi\""], "answer_start": [395]}}
{"context": "Id = impressionAdId AND\n    clickTime >= impressionTime AND\n    clickTime <= impressionTime + interval 1 hour\n  \"\"\"\n),\njoinType\n=\n\"leftOuter\"\n// can be \"inner\", \"leftOuter\", \"rightOuter\", \"fullOuter\", \"leftSemi\"\n)\njoined\n.\ngroupBy\n(\n$\n\"clickAdId\"\n,\nwindow\n(\n$\n\"clickTime\"\n,\n\"1 hour\"\n))\n.\ncount\n()\nDataset\n<\nRow\n>\njoined\n=\nimpressionsWithWatermark\n.\njoin\n(\nclicksWithWatermark\n,\nexpr\n(\n\"clickAdId = impressionAdId AND \"\n+\n\"clickTime >= impressionTime AND \"\n+\n\"clickTime <= impressionTime + interval 1 hour \"\n),\n\"leftOuter\"\n// can be \"inner\", \"leftOuter\", \"rightOuter\", \"fullOuter\", \"leftSemi\"\n);\njoined\n.\ngroupBy\n(\njoined\n.\ncol\n(\n\"clickAdId\"\n),\nfunctions\n.\nwindow\n(\njoined\n.\ncol\n(\n\"clickTime\"\n),\n\"1 hour\"\n))\n.\ncount\n();\nStreaming Deduplication\nYou can deduplicate records in data streams using a uniqu", "question": "What join type is used in the provided code snippet?", "answers": {"text": ["leftOuter"], "answer_start": [131]}}
{"context": "ons\n.\nwindow\n(\njoined\n.\ncol\n(\n\"clickTime\"\n),\n\"1 hour\"\n))\n.\ncount\n();\nStreaming Deduplication\nYou can deduplicate records in data streams using a unique identifier in the events. This is exactly same as deduplication on static using a unique identifier column. The query will store the necessary amount of data from previous records such that it can filter duplicate records. Similar to aggregations, you can use deduplication with or without watermarking.\nWith watermark\n- If there is an upper bound on how late a duplicate record may arrive, then you can define a watermark on an event time column and deduplicate using both the guid and the event time columns. The query will use the watermark to remove old state data from past records that are not expected to get any duplicates any more. This bo", "question": "How can records be deduplicated in data streams?", "answers": {"text": ["You can deduplicate records in data streams using a unique identifier in the events."], "answer_start": [93]}}
{"context": "..\n)\n# Without watermark using guid column\nstreamingDf\n<-\ndropDuplicates\n(\nstreamingDf\n,\n\"guid\"\n)\n# With watermark using guid and eventTime columns\nstreamingDf\n<-\nwithWatermark\n(\nstreamingDf\n,\n\"eventTime\"\n,\n\"10 seconds\"\n)\nstreamingDf\n<-\ndropDuplicates\n(\nstreamingDf\n,\n\"guid\"\n,\n\"eventTime\"\n)\nSpecifically for streaming, you can deduplicate records in data streams using a unique identifier in the events, within the time range of watermark.\nFor example, if you set the delay threshold of watermark as “1 hour”, duplicated events which occurred within 1 hour can be correctly deduplicated.\n(For more details, please refer to the API doc of\ndropDuplicatesWithinWatermark\n.)\nThis can be used to deal with use case where event time column cannot be a part of unique identifier, mostly due to the case\nwher", "question": "How can records be deduplicated in data streams?", "answers": {"text": ["Specifically for streaming, you can deduplicate records in data streams using a unique identifier in the events, within the time range of watermark."], "answer_start": [291]}}
{"context": "ime\n\"\n,\n\"\n10 hours\n\"\n)\n\\\n.\ndropDuplicatesWithinWatermark\n([\n\"\nguid\n\"\n])\nval\nstreamingDf\n=\nspark\n.\nreadStream\n.\n...\n// columns: guid, eventTime, ...\n// deduplicate using guid column with watermark based on eventTime column\nstreamingDf\n.\nwithWatermark\n(\n\"eventTime\"\n,\n\"10 hours\"\n)\n.\ndropDuplicatesWithinWatermark\n(\n\"guid\"\n)\nDataset\n<\nRow\n>\nstreamingDf\n=\nspark\n.\nreadStream\n().\n...;\n// columns: guid, eventTime, ...\n// deduplicate using guid column with watermark based on eventTime column\nstreamingDf\n.\nwithWatermark\n(\n\"eventTime\"\n,\n\"10 hours\"\n)\n.\ndropDuplicatesWithinWatermark\n(\n\"guid\"\n);\nPolicy for handling multiple watermarks\nA streaming query can have multiple input streams that are unioned or joined together.\nEach of the input streams can have a different threshold of late data that needs to\nb", "question": "What watermark duration is used for deduplication?", "answers": {"text": ["10 hours"], "answer_start": [10]}}
{"context": "trary types of data as state, and perform arbitrary operations on the state using the data stream events in every trigger.\nSince Spark 2.2, this can be done using the legacy\nmapGroupsWithState\nand\nflatMapGroupsWithState\noperators. Both operators allow you to apply user-defined code on grouped Datasets to update user-defined state. For more concrete details, take a look at the API documentation (\nScala\n/\nJava\n) and the examples (\nScala\n/\nJava\n).\nSince the Spark 4.0 release, users are encouraged to use the new\ntransformWithState\noperator to build their complex stateful applications. For more details, please refer to the in-depth documentation\nhere\n.\nThough Spark cannot check and force it, the state function should be implemented with respect to the semantics of the output mode. For example, ", "question": "Which operator is recommended for building complex stateful applications since Spark 4.0?", "answers": {"text": ["transformWithState"], "answer_start": [514]}}
{"context": "ere\n.\nThough Spark cannot check and force it, the state function should be implemented with respect to the semantics of the output mode. For example, in Update mode Spark doesn’t expect that the state function will emit rows which are older than current watermark plus allowed late record delay, whereas in Append mode the state function can emit these rows.\nUnsupported Operations\nThere are a few DataFrame/Dataset operations that are not supported with streaming DataFrames/Datasets.\nSome of them are as follows.\nLimit and take the first N rows are not supported on streaming Datasets.\nDistinct operations on streaming Datasets are not supported.\nSorting operations are supported on streaming Datasets only after an aggregation and in Complete Output Mode.\nFew types of outer joins on streaming Dat", "question": "In Update mode, what does Spark not expect from the state function?", "answers": {"text": ["Spark doesn’t expect that the state function will emit rows which are older than current watermark plus allowed late record delay"], "answer_start": [165]}}
{"context": "per query. Ensuring end-to-end exactly once for the last query is optional.\nIn addition, there are some Dataset methods that will not work on streaming Datasets. They are actions that will immediately run queries and return results, which does not make sense on a streaming Dataset. Rather, those functionalities can be done by explicitly starting a streaming query (see the next section regarding that).\ncount()\n- Cannot return a single count from a streaming Dataset. Instead, use\nds.groupBy().count()\nwhich returns a streaming Dataset containing a running count.\nforeach()\n- Instead use\nds.writeStream.foreach(...)\n(see next section).\nshow()\n- Instead use the console sink (see next section).\nIf you try any of these operations, you will see an\nAnalysisException\nlike “operation XYZ is not support", "question": "What should you use instead of `foreach()` when working with a streaming Dataset?", "answers": {"text": ["ds.writeStream.foreach(...)"], "answer_start": [590]}}
{"context": "tead use the console sink (see next section).\nIf you try any of these operations, you will see an\nAnalysisException\nlike “operation XYZ is not supported with streaming DataFrames/Datasets”.\nWhile some of them may be supported in future releases of Spark,\nthere are others which are fundamentally hard to implement on streaming data efficiently.\nFor example, sorting on the input stream is not supported, as it requires keeping\ntrack of all the data received in the stream. This is therefore fundamentally hard to execute\nefficiently.\nState Store\nState store is a versioned key-value store which provides both read and write operations. In\nStructured Streaming, we use the state store provider to handle the stateful operations across\nbatches. There are two built-in state store provider implementatio", "question": "What is a state store?", "answers": {"text": ["State store is a versioned key-value store which provides both read and write operations."], "answer_start": [546]}}
{"context": " guarantees (the same as default\nstate management).\nTo enable the new build-in state store implementation, set\nspark.sql.streaming.stateStore.providerClass\nto\norg.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider\n.\nHere are the configs regarding to RocksDB instance of the state store provider:\nConfig Name\nDescription\nDefault Value\nspark.sql.streaming.stateStore.rocksdb.compactOnCommit\nWhether we perform a range compaction of RocksDB instance for commit operation\nFalse\nspark.sql.streaming.stateStore.rocksdb.changelogCheckpointing.enabled\nWhether to upload changelog instead of snapshot during RocksDB StateStore commit\nFalse\nspark.sql.streaming.stateStore.rocksdb.blockSizeKB\nApproximate size in KB of user data packed per block for a RocksDB BlockBasedTable, which is a Rocks", "question": "What should be set to enable the new build-in state store implementation?", "answers": {"text": ["org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider"], "answer_start": [159]}}
{"context": "park.sql.streaming.stateStore.rocksdb.blockSizeKB\nApproximate size in KB of user data packed per block for a RocksDB BlockBasedTable, which is a RocksDB's default SST file format.\n4\nspark.sql.streaming.stateStore.rocksdb.blockCacheSizeMB\nThe size capacity in MB for a cache of blocks.\n8\nspark.sql.streaming.stateStore.rocksdb.lockAcquireTimeoutMs\nThe waiting time in millisecond for acquiring lock in the load operation for RocksDB instance.\n60000\nspark.sql.streaming.stateStore.rocksdb.maxOpenFiles\nThe number of open files that can be used by the RocksDB instance. Value of -1 means that files opened are always kept open. If the open file limit is reached, RocksDB will evict entries from the open file cache and close those file descriptors and remove the entries from the cache.\n-1\nspark.sql.str", "question": "What does spark.sql.streaming.stateStore.rocksdb.blockSizeKB configure?", "answers": {"text": ["Approximate size in KB of user data packed per block for a RocksDB BlockBasedTable, which is a RocksDB's default SST file format."], "answer_start": [50]}}
{"context": " reached, RocksDB will evict entries from the open file cache and close those file descriptors and remove the entries from the cache.\n-1\nspark.sql.streaming.stateStore.rocksdb.resetStatsOnLoad\nWhether we resets all ticker and histogram stats for RocksDB on load.\nTrue\nspark.sql.streaming.stateStore.rocksdb.trackTotalNumberOfRows\nWhether we track the total number of rows in state store. Please refer the details in\nPerformance-aspect considerations\n.\nTrue\nspark.sql.streaming.stateStore.rocksdb.writeBufferSizeMB\nThe maximum size of MemTable in RocksDB. Value of -1 means that RocksDB internal default values will be used\n-1\nspark.sql.streaming.stateStore.rocksdb.maxWriteBufferNumber\nThe maximum number of MemTables in RocksDB, both active and immutable. Value of -1 means that RocksDB internal def", "question": "What does a value of -1 mean for the 'spark.sql.streaming.stateStore.rocksdb.writeBufferSizeMB' configuration?", "answers": {"text": ["Value of -1 means that RocksDB internal default values will be used"], "answer_start": [555]}}
{"context": "eStore.rocksdb.maxWriteBufferNumber\nThe maximum number of MemTables in RocksDB, both active and immutable. Value of -1 means that RocksDB internal default values will be used\n-1\nspark.sql.streaming.stateStore.rocksdb.boundedMemoryUsage\nWhether total memory usage for RocksDB state store instances on a single node is bounded.\nfalse\nspark.sql.streaming.stateStore.rocksdb.maxMemoryUsageMB\nTotal memory limit in MB for RocksDB state store instances on a single node.\n500\nspark.sql.streaming.stateStore.rocksdb.writeBufferCacheRatio\nTotal memory to be occupied by write buffers as a fraction of memory allocated across all RocksDB instances on a single node using maxMemoryUsageMB.\n0.5\nspark.sql.streaming.stateStore.rocksdb.highPriorityPoolRatio\nTotal memory to be occupied by blocks in high priority p", "question": "What does the property 'eStore.rocksdb.maxWriteBufferNumber' represent?", "answers": {"text": ["The maximum number of MemTables in RocksDB, both active and immutable."], "answer_start": [36]}}
{"context": "node using maxMemoryUsageMB.\n0.5\nspark.sql.streaming.stateStore.rocksdb.highPriorityPoolRatio\nTotal memory to be occupied by blocks in high priority pool as a fraction of memory allocated across all RocksDB instances on a single node using maxMemoryUsageMB.\n0.1\nspark.sql.streaming.stateStore.rocksdb.allowFAllocate\nAllow the rocksdb runtime to use fallocate to pre-allocate disk space for logs, etc...  Disable for apps that have many smaller state stores to trade off disk space for write performance.\ntrue\nspark.sql.streaming.stateStore.rocksdb.compression\nCompression type used in RocksDB. The string is converted RocksDB compression type through RocksDB Java API getCompressionType().\nlz4\nRocksDB State Store Memory Management\nRocksDB allocates memory for different objects such as memtables, bl", "question": "What is the purpose of spark.sql.streaming.stateStore.rocksdb.allowFAllocate?", "answers": {"text": ["Allow the rocksdb runtime to use fallocate to pre-allocate disk space for logs, etc...  Disable for apps that have many smaller state stores to trade off disk space for write performance."], "answer_start": [316]}}
{"context": " RocksDB Java API getCompressionType().\nlz4\nRocksDB State Store Memory Management\nRocksDB allocates memory for different objects such as memtables, block cache and filter/index blocks. If left unbounded, RocksDB memory usage across multiple instances could grow indefinitely and potentially cause OOM (out-of-memory) issues.\nRocksDB provides a way to limit the memory usage for all DB instances running on a single node by using the write buffer manager functionality.\nIf you want to cap RocksDB memory usage in your Spark Structured Streaming deployment, this feature can be enabled by setting the\nspark.sql.streaming.stateStore.rocksdb.boundedMemoryUsage\nconfig to\ntrue\n.\nYou can also determine the max allowed memory for RocksDB instances by setting the\nspark.sql.streaming.stateStore.rocksdb.maxM", "question": "How can you limit the memory usage for all RocksDB DB instances running on a single node?", "answers": {"text": ["RocksDB provides a way to limit the memory usage for all DB instances running on a single node by using the write buffer manager functionality."], "answer_start": [325]}}
{"context": "tal memory used by RocksDB can temporarily exceed this value if all blocks allocated to higher level readers are in use.\nEnabling a strict limit is not possible at this time since it will cause query failures and we do not support re-balancing of the state across additional nodes.\nRocksDB State Store Changelog Checkpointing\nIn newer version of Spark, changelog checkpointing is introduced for RocksDB state store. The traditional checkpointing mechanism for RocksDB State Store is incremental snapshot checkpointing, where the manifest files and newly generated RocksDB SST files of RocksDB instances are uploaded to a durable storage.\nInstead of uploading data files of RocksDB instances, changelog checkpointing uploads changes made to the state since the last checkpoint for durability.\nSnapshot", "question": "What does changelog checkpointing upload for durability?", "answers": {"text": ["Instead of uploading data files of RocksDB instances, changelog checkpointing uploads changes made to the state since the last checkpoint for durability."], "answer_start": [638]}}
{"context": "ard compatible with traditional checkpointing mechanism.\nRocksDB state store provider offers seamless support for transitioning between two checkpointing mechanisms in both directions. This allows you to leverage the performance benefits of changelog checkpointing without discarding the old state checkpoint.\nIn a version of spark that supports changelog checkpointing, you can migrate streaming queries from older versions of Spark to changelog checkpointing by enabling changelog checkpointing in the spark session.\nVice versa, you can disable changelog checkpointing safely in newer version of Spark, then any query that already run with changelog checkpointing will switch back to traditional checkpointing.\nYou would need to restart you streaming queries for change in checkpointing mechanism t", "question": "What does the RocksDB state store provider allow regarding checkpointing mechanisms?", "answers": {"text": ["RocksDB state store provider offers seamless support for transitioning between two checkpointing mechanisms in both directions."], "answer_start": [57]}}
{"context": "ing state from checkpoint depends\non the external storage and the size of the state, which tends to hurt the latency of micro-batch run. For some use cases such as processing very large state data,\nloading new state store providers from checkpointed states can be very time-consuming and inefficient.\nThe stateful operations in Structured Streaming queries rely on the preferred location feature of Spark’s RDD to run the state store provider on the same executor.\nIf in the next batch the corresponding state store provider is scheduled on this executor again, it could reuse the previous states and save the time of loading checkpointed states.\nHowever, generally the preferred location is not a hard requirement and it is still possible that Spark schedules tasks to the executors other than the p", "question": "What can Structured Streaming queries rely on for state store provider execution?", "answers": {"text": ["the preferred location feature of Spark’s RDD to run the state store provider on the same executor."], "answer_start": [365]}}
{"context": "ever, generally the preferred location is not a hard requirement and it is still possible that Spark schedules tasks to the executors other than the preferred ones.\nIn this case, Spark will load state store providers from checkpointed states on new executors. The state store providers run in the previous batch will not be unloaded immediately.\nSpark runs a maintenance task which checks and unloads the state store providers that are inactive on the executors.\nBy changing the Spark configurations related to task scheduling, for example\nspark.locality.wait\n, users can configure Spark how long to wait to launch a data-local task.\nFor stateful operations in Structured Streaming, it can be used to let state store providers running on the same executors across batches.\nSpecifically for built-in H", "question": "What does Spark do when it schedules tasks to executors other than the preferred ones?", "answers": {"text": ["Spark will load state store providers from checkpointed states on new executors."], "answer_start": [179]}}
{"context": "rations in Structured Streaming, it can be used to let state store providers running on the same executors across batches.\nSpecifically for built-in HDFS state store provider, users can check the state store metrics such as\nloadedMapCacheHitCount\nand\nloadedMapCacheMissCount\n. Ideally,\nit is best if cache missing count is minimized that means Spark won’t waste too much time on loading checkpointed state.\nUser can increase Spark locality waiting configurations to avoid loading state store providers in different executors across batches.\nState Data Source (Experimental)\nApache Spark provides a streaming state related data source that provides the ability to manipulate state stores in the checkpoint. Users can run the batch query with State Data Source to get the visibility of the states for e", "question": "What metrics can users check for the built-in HDFS state store provider?", "answers": {"text": ["loadedMapCacheHitCount\nand\nloadedMapCacheMissCount"], "answer_start": [224]}}
{"context": ")\nreturned through\nDataset.writeStream()\n. You will have to specify one or more of the following in this interface.\nDetails of the output sink:\nData format, location, etc.\nOutput mode:\nSpecify what gets written to the output sink.\nQuery name:\nOptionally, specify a unique name of the query for identification.\nTrigger interval:\nOptionally, specify the trigger interval. If it is not specified, the system will check for availability of new data as soon as the previous processing has been completed. If a trigger time is missed because the previous processing has not been completed, then the system will trigger processing immediately.\nCheckpoint location:\nFor some output sinks where the end-to-end fault-tolerance can be guaranteed, specify the location where the system will write all the checkpo", "question": "What happens if a trigger time is missed during stream processing?", "answers": {"text": ["If a trigger time is missed because the previous processing has not been completed, then the system will trigger processing immediately."], "answer_start": [500]}}
{"context": "\nguarantees that each row will be output only once (assuming\nfault-tolerant sink). For example, queries with only\nselect\n,\nwhere\n,\nmap\n,\nflatMap\n,\nfilter\n,\njoin\n, etc. will support Append mode.\nComplete mode\n- The whole Result Table will be outputted to the sink after every trigger.\n This is supported for aggregation queries.\nUpdate mode\n- (\nAvailable since Spark 2.1.1\n) Only the rows in the Result Table that were\nupdated since the last trigger will be outputted to the sink.\nMore information to be added in future releases.\nDifferent types of streaming queries support different output modes.\nHere is the compatibility matrix.\nQuery Type\nSupported Output Modes\nNotes\nQueries with aggregation\nAggregation on event-time with watermark\nAppend, Update, Complete\nAppend mode uses watermark to drop ol", "question": "What output modes are supported for queries with aggregation?", "answers": {"text": ["Append, Update, Complete"], "answer_start": [738]}}
{"context": "ed Output Modes\nNotes\nQueries with aggregation\nAggregation on event-time with watermark\nAppend, Update, Complete\nAppend mode uses watermark to drop old aggregation state. But the output of a\n        windowed aggregation is delayed the late threshold specified in\nwithWatermark()\nas by\n        the modes semantics, rows can be added to the Result Table only once after they are\n        finalized (i.e. after watermark is crossed). See the\nLate Data\nsection for more details.\nUpdate mode uses watermark to drop old aggregation state.\nComplete mode does not drop old aggregation state since by definition this mode\n        preserves all data in the Result Table.\nOther aggregations\nComplete, Update\nSince no watermark is defined (only defined in other category),\n        old aggregation state is not dro", "question": "What happens in Append mode regarding old aggregation state and watermark?", "answers": {"text": ["Append mode uses watermark to drop old aggregation state."], "answer_start": [113]}}
{"context": "lt Table.\nOther aggregations\nComplete, Update\nSince no watermark is defined (only defined in other category),\n        old aggregation state is not dropped.\nAppend mode is not supported as aggregates can update thus violating the semantics of\n        this mode.\nQueries with\nmapGroupsWithState\nUpdate\nAggregations not allowed in a query with\nmapGroupsWithState\n.\nQueries with\nflatMapGroupsWithState\nAppend operation mode\nAppend\nAggregations are allowed after\nflatMapGroupsWithState\n.\nUpdate operation mode\nUpdate\nAggregations not allowed in a query with\nflatMapGroupsWithState\n.\nQueries with\njoins\nAppend\nUpdate and Complete mode not supported yet. See the\nsupport matrix in the Join Operations section\nfor more details on what types of joins are supported.\nOther queries\nAppend, Update\nComplete mode ", "question": "What operation modes are not supported for queries with joins?", "answers": {"text": ["Update and Complete mode not supported yet."], "answer_start": [604]}}
{"context": "pic\"\n,\n\"updates\"\n)\n.\nstart\n()\nForeach sink\n- Runs arbitrary computation on the records in the output. See later in the section for more details.\nwriteStream\n.\nforeach\n(...)\n.\nstart\n()\nConsole sink (for debugging)\n- Prints the output to the console/stdout every time there is a trigger. Both, Append and Complete output modes, are supported. This should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver’s memory after every trigger.\nwriteStream\n.\nformat\n(\n\"console\"\n)\n.\nstart\n()\nMemory sink (for debugging)\n- The output is stored in memory as an in-memory table.\nBoth, Append and Complete output modes, are supported. This should be used for debugging purposes\non low data volumes as the entire output is collected and stored in the driver’", "question": "What happens when using a Console sink?", "answers": {"text": ["Prints the output to the console/stdout every time there is a trigger."], "answer_start": [215]}}
{"context": "e (TTL) for output files. Output files which batches were\n        committed older than TTL will be eventually excluded in metadata log. This means reader queries which read\n        the sink's output directory may not process them. You can provide the value as string format of the time. (like \"12h\", \"7d\", etc.)\n        By default it's disabled.\nFor file-format-specific options, see the related methods in DataFrameWriter\n        (\nPython\n/\nScala\n/\nJava\n/\nR\n).\n        E.g. for \"parquet\" format options see\nDataFrameWriter.parquet()\nYes (exactly-once)\nSupports writes to partitioned tables. Partitioning by time may be useful.\nKafka Sink\nAppend, Update, Complete\nSee the\nKafka Integration Guide\nYes (at-least-once)\nMore details in the\nKafka Integration Guide\nForeach Sink\nAppend, Update, Complete\nNo", "question": "What is the default state of TTL for output files?", "answers": {"text": ["By default it's disabled."], "answer_start": [320]}}
{"context": "ate, Complete\nSee the\nKafka Integration Guide\nYes (at-least-once)\nMore details in the\nKafka Integration Guide\nForeach Sink\nAppend, Update, Complete\nNone\nYes (at-least-once)\nMore details in the\nnext section\nForeachBatch Sink\nAppend, Update, Complete\nNone\nDepends on the implementation\nMore details in the\nnext section\nConsole Sink\nAppend, Update, Complete\nnumRows\n: Number of rows to print every trigger (default: 20)\ntruncate\n: Whether to truncate the output if too long (default: true)\nNo\nMemory Sink\nAppend, Complete\nNone\nNo. But in Complete Mode, restarted query will recreate the full table.\nTable name is the query name.\nNote that you have to call\nstart()\nto actually start the execution of the query. This returns a StreamingQuery object which is a handle to the continuously running execution.", "question": "What happens when a Memory Sink is in Complete Mode and the query is restarted?", "answers": {"text": ["No. But in Complete Mode, restarted query will recreate the full table."], "answer_start": [524]}}
{"context": "ll\nstart()\nto actually start the execution of the query. This returns a StreamingQuery object which is a handle to the continuously running execution. You can use this object to manage the query, which we will discuss in the next subsection. For now, let’s understand all this with a few examples.\n# ========== DF with no aggregations ==========\nnoAggDF\n=\ndeviceDataDf\n.\nselect\n(\n\"\ndevice\n\"\n).\nwhere\n(\n\"\nsignal > 10\n\"\n)\n# Print new data to console\nnoAggDF\n\\\n.\nwriteStream\n\\\n.\nformat\n(\n\"\nconsole\n\"\n)\n\\\n.\nstart\n()\n# Write new data to Parquet files\nnoAggDF\n\\\n.\nwriteStream\n\\\n.\nformat\n(\n\"\nparquet\n\"\n)\n\\\n.\noption\n(\n\"\ncheckpointLocation\n\"\n,\n\"\npath/to/checkpoint/dir\n\"\n)\n\\\n.\noption\n(\n\"\npath\n\"\n,\n\"\npath/to/destination/dir\n\"\n)\n\\\n.\nstart\n()\n# ========== DF with aggregation ==========\naggDF\n=\ndf\n.\ngroupBy\n(\n\"\n", "question": "What does calling `start()` do?", "answers": {"text": ["to actually start the execution of the query. This returns a StreamingQuery object which is a handle to the continuously running execution."], "answer_start": [11]}}
{"context": "\n();\n// ========== DF with aggregation ==========\nDataset\n<\nRow\n>\naggDF\n=\ndf\n.\ngroupBy\n(\n\"device\"\n).\ncount\n();\n// Print updated aggregations to console\naggDF\n.\nwriteStream\n()\n.\noutputMode\n(\n\"complete\"\n)\n.\nformat\n(\n\"console\"\n)\n.\nstart\n();\n// Have all the aggregates in an in-memory table\naggDF\n.\nwriteStream\n()\n.\nqueryName\n(\n\"aggregates\"\n)\n// this query name will be the table name\n.\noutputMode\n(\n\"complete\"\n)\n.\nformat\n(\n\"memory\"\n)\n.\nstart\n();\nspark\n.\nsql\n(\n\"select * from aggregates\"\n).\nshow\n();\n// interactively query in-memory table\n# ========== DF with no aggregations ==========\nnoAggDF\n<-\nselect\n(\nwhere\n(\ndeviceDataDf\n,\n\"signal > 10\"\n),\n\"device\"\n)\n# Print new data to console\nwrite.stream\n(\nnoAggDF\n,\n\"console\"\n)\n# Write new data to Parquet files\nwrite.stream\n(\nnoAggDF\n,\n\"parquet\"\n,\npath\n=\n\"pa", "question": "What is the query name used when writing the `aggDF` to an in-memory table?", "answers": {"text": ["\"aggregates\""], "answer_start": [324]}}
{"context": "m aggregates\"\n))\nUsing Foreach and ForeachBatch\nThe\nforeach\nand\nforeachBatch\noperations allow you to apply arbitrary operations and writing\nlogic on the output of a streaming query. They have slightly different use cases - while\nforeach\nallows custom write logic on every row,\nforeachBatch\nallows arbitrary operations\nand custom logic on the output of each micro-batch. Let’s understand their usages in more detail.\nForeachBatch\nforeachBatch(...)\nallows you to specify a function that is executed on\nthe output data of every micro-batch of a streaming query. Since Spark 2.4, this is supported in Scala, Java and Python.\nIt takes two parameters: a DataFrame or Dataset that has the output data of a micro-batch and the unique ID of the micro-batch.\ndef\nforeach_batch_function\n(\ndf\n,\nepoch_id\n):\n# Tra", "question": "Since which Spark version is foreachBatch supported in Scala, Java and Python?", "answers": {"text": ["Since Spark 2.4, this is supported in Scala, Java and Python."], "answer_start": [559]}}
{"context": "h\n, you can do the following.\nReuse existing batch data sources\n- For many storage systems, there may not be a streaming sink available yet,\nbut there may already exist a data writer for batch queries. Using\nforeachBatch\n, you can use the batch\ndata writers on the output of each micro-batch.\nWrite to multiple locations\n- If you want to write the output of a streaming query to multiple locations,\nthen you can simply write the output DataFrame/Dataset multiple times. However, each attempt to write can\ncause the output data to be recomputed (including possible re-reading of the input data). To avoid recomputations,\nyou should cache the output DataFrame/Dataset, write it to multiple locations, and then uncache it. Here is an outline.\nstreamingDF\n.\nwriteStream\n.\nforeachBatch\n{\n(\nbatchDF\n:\nDataF", "question": "What can you do if a streaming sink is not yet available for many storage systems?", "answers": {"text": ["Reuse existing batch data sources"], "answer_start": [30]}}
{"context": "\nforeach\nis available in Scala, Java and Python.\nIn Python, you can invoke foreach in two ways: in a function or in an object.\nThe function offers a simple way to express your processing logic but does not allow you to\ndeduplicate generated data when failures cause reprocessing of some input data.\nFor that situation you must specify the processing logic in an object.\nFirst, the function takes a row as input.\ndef\nprocess_row\n(\nrow\n):\n# Write row to storage\npass\nquery\n=\nstreamingDF\n.\nwriteStream\n.\nforeach\n(\nprocess_row\n).\nstart\n()\nSecond, the object has a process method and optional open and close methods:\nclass\nForeachWriter\n:\ndef\nopen\n(\nself\n,\npartition_id\n,\nepoch_id\n):\n# Open connection. This method is optional in Python.\npass\ndef\nprocess\n(\nself\n,\nrow\n):\n# Write row to connection. This me", "question": "In Python, how can you invoke foreach?", "answers": {"text": ["In Python, you can invoke foreach in two ways: in a function or in an object."], "answer_start": [49]}}
{"context": "tion\n}\ndef\nclose\n(\nerrorOrNull\n:\nThrowable\n)\n:\nUnit\n=\n{\n// Close the connection\n}\n}\n).\nstart\n()\nIn Java, you have to extend the class\nForeachWriter\n(\ndocs\n).\nstreamingDatasetOfString\n.\nwriteStream\n().\nforeach\n(\nnew\nForeachWriter\n<\nString\n>()\n{\n@Override\npublic\nboolean\nopen\n(\nlong\npartitionId\n,\nlong\nversion\n)\n{\n// Open connection\n}\n@Override\npublic\nvoid\nprocess\n(\nString\nrecord\n)\n{\n// Write string to connection\n}\n@Override\npublic\nvoid\nclose\n(\nThrowable\nerrorOrNull\n)\n{\n// Close the connection\n}\n}\n).\nstart\n();\nR is not yet supported.\nExecution semantics\nWhen the streaming query is started, Spark calls the function or the object’s methods in the following way:\nA single copy of this object is responsible for all the data generated by a single task in a query.\nIn other words, one instance is resp", "question": "What is a single copy of the object responsible for?", "answers": {"text": ["A single copy of this object is responsible for all the data generated by a single task in a query."], "answer_start": [664]}}
{"context": "ch partition with partition_id:\nFor each batch/epoch of streaming data with epoch_id:\nMethod open(partitionId, epochId) is called.\nIf open(…) returns true, for each row in the partition and batch/epoch, method process(row) is called.\nMethod close(error) is called with error (if any) seen while processing rows.\nThe close() method (if it exists) is called if an open() method exists and returns successfully (irrespective of the return value), except if the JVM or Python process crashes in the middle.\nNote:\nSpark does not guarantee same output for (partitionId, epochId), so deduplication\ncannot be achieved with (partitionId, epochId). e.g. source provides different number of\npartitions for some reasons, Spark optimization changes number of partitions, etc.\nSee\nSPARK-28650\nfor more details.\nIf ", "question": "What happens if the open() method exists and returns successfully?", "answers": {"text": ["The close() method (if it exists) is called if an open() method exists and returns successfully (irrespective of the return value), except if the JVM or Python process crashes in the middle."], "answer_start": [312]}}
{"context": " provides different number of\npartitions for some reasons, Spark optimization changes number of partitions, etc.\nSee\nSPARK-28650\nfor more details.\nIf you need deduplication on output, try out\nforeachBatch\ninstead.\nStreaming Table APIs\nSince Spark 3.1, you can also use\nDataStreamReader.table()\nto read tables as streaming DataFrames and use\nDataStreamWriter.toTable()\nto write streaming DataFrames as tables:\nspark\n=\n...\n# spark session\n# Create a streaming DataFrame\ndf\n=\nspark\n.\nreadStream\n\\\n.\nformat\n(\n\"\nrate\n\"\n)\n\\\n.\noption\n(\n\"\nrowsPerSecond\n\"\n,\n10\n)\n\\\n.\nload\n()\n# Write the streaming DataFrame to a table\ndf\n.\nwriteStream\n\\\n.\noption\n(\n\"\ncheckpointLocation\n\"\n,\n\"\npath/to/checkpoint/dir\n\"\n)\n\\\n.\ntoTable\n(\n\"\nmyTable\n\"\n)\n# Check the table result\nspark\n.\nread\n.\ntable\n(\n\"\nmyTable\n\"\n).\nshow\n()\n# Transf", "question": "How can you write streaming DataFrames as tables in Spark?", "answers": {"text": ["to write streaming DataFrames as tables:"], "answer_start": [368]}}
{"context": "mark advancement including no-data batch.\nAvailable-now micro-batch\nSimilar to queries one-time micro-batch trigger, the query will process all the available data and then\n        stop on its own. The difference is that, it will process the data in (possibly) multiple micro-batches\n        based on the source options (e.g.\nmaxFilesPerTrigger\nor\nmaxBytesPerTrigger\nfor file \n        source), which will result in better query scalability.\nThis trigger provides a strong guarantee of processing: regardless of how many batches were\n                left over in previous run, it ensures all available data at the time of execution gets\n                processed before termination. All uncommitted batches will be processed first.\nWatermark gets advanced per each batch, and no-data batch gets execute", "question": "What happens with watermark advancement during processing?", "answers": {"text": ["Watermark gets advanced per each batch, and no-data batch gets execute"], "answer_start": [730]}}
{"context": " processed before termination. All uncommitted batches will be processed first.\nWatermark gets advanced per each batch, and no-data batch gets executed before termination\n                if the last batch advances the watermark. This helps to maintain smaller and predictable\n                state size and smaller latency on the output of stateful operators.\nNOTE: this trigger will be deactivated when there is any source which does not support Trigger.AvailableNow.\n        Spark will perform one-time micro-batch as a fall-back. Check the above differences for a risk of fallback.\nContinuous with fixed checkpoint interval\n(experimental)\nThe query will be executed in the new low-latency, continuous processing mode. Read more\n        about this in the\nContinuous Processing section\nbelow.\nHere a", "question": "What happens to uncommitted batches before termination?", "answers": {"text": ["All uncommitted batches will be processed first."], "answer_start": [31]}}
{"context": "y will be executed in the new low-latency, continuous processing mode. Read more\n        about this in the\nContinuous Processing section\nbelow.\nHere are a few code examples.\n# Default trigger (runs micro-batch as soon as it can)\ndf\n.\nwriteStream\n\\\n.\nformat\n(\n\"\nconsole\n\"\n)\n\\\n.\nstart\n()\n# ProcessingTime trigger with two-seconds micro-batch interval\ndf\n.\nwriteStream\n\\\n.\nformat\n(\n\"\nconsole\n\"\n)\n\\\n.\ntrigger\n(\nprocessingTime\n=\n'\n2 seconds\n'\n)\n\\\n.\nstart\n()\n# One-time trigger (Deprecated, encouraged to use Available-now trigger)\ndf\n.\nwriteStream\n\\\n.\nformat\n(\n\"\nconsole\n\"\n)\n\\\n.\ntrigger\n(\nonce\n=\nTrue\n)\n\\\n.\nstart\n()\n# Available-now trigger\ndf\n.\nwriteStream\n\\\n.\nformat\n(\n\"\nconsole\n\"\n)\n\\\n.\ntrigger\n(\navailableNow\n=\nTrue\n)\n\\\n.\nstart\n()\n# Continuous trigger with one-second checkpointing interval\ndf\n.\nwriteSt", "question": "How can a micro-batch be run as soon as it can?", "answers": {"text": ["# Default trigger (runs micro-batch as soon as it can)"], "answer_start": [174]}}
{"context": "m\n\\\n.\nformat\n(\n\"\nconsole\n\"\n)\n\\\n.\ntrigger\n(\navailableNow\n=\nTrue\n)\n\\\n.\nstart\n()\n# Continuous trigger with one-second checkpointing interval\ndf\n.\nwriteStream\n.\nformat\n(\n\"\nconsole\n\"\n)\n.\ntrigger\n(\ncontinuous\n=\n'\n1 second\n'\n)\n.\nstart\n()\nimport\norg.apache.spark.sql.streaming.Trigger\n// Default trigger (runs micro-batch as soon as it can)\ndf\n.\nwriteStream\n.\nformat\n(\n\"console\"\n)\n.\nstart\n()\n// ProcessingTime trigger with two-seconds micro-batch interval\ndf\n.\nwriteStream\n.\nformat\n(\n\"console\"\n)\n.\ntrigger\n(\nTrigger\n.\nProcessingTime\n(\n\"2 seconds\"\n))\n.\nstart\n()\n// One-time trigger (Deprecated, encouraged to use Available-now trigger)\ndf\n.\nwriteStream\n.\nformat\n(\n\"console\"\n)\n.\ntrigger\n(\nTrigger\n.\nOnce\n())\n.\nstart\n()\n// Available-now trigger\ndf\n.\nwriteStream\n.\nformat\n(\n\"console\"\n)\n.\ntrigger\n(\nTrigger\n.\nAvai", "question": "What type of trigger is deprecated and encourages the use of the Available-now trigger?", "answers": {"text": ["One-time trigger (Deprecated, encouraged to use Available-now trigger)"], "answer_start": [556]}}
{"context": "tart\n();\n// One-time trigger (Deprecated, encouraged to use Available-now trigger)\ndf\n.\nwriteStream\n.\nformat\n(\n\"console\"\n)\n.\ntrigger\n(\nTrigger\n.\nOnce\n())\n.\nstart\n();\n// Available-now trigger\ndf\n.\nwriteStream\n.\nformat\n(\n\"console\"\n)\n.\ntrigger\n(\nTrigger\n.\nAvailableNow\n())\n.\nstart\n();\n// Continuous trigger with one-second checkpointing interval\ndf\n.\nwriteStream\n.\nformat\n(\n\"console\"\n)\n.\ntrigger\n(\nTrigger\n.\nContinuous\n(\n\"1 second\"\n))\n.\nstart\n();\n# Default trigger (runs micro-batch as soon as it can)\nwrite.stream\n(\ndf\n,\n\"console\"\n)\n# ProcessingTime trigger with two-seconds micro-batch interval\nwrite.stream\n(\ndf\n,\n\"console\"\n,\ntrigger.processingTime\n=\n\"2 seconds\"\n)\n# One-time trigger\nwrite.stream\n(\ndf\n,\n\"console\"\n,\ntrigger.once\n=\nTRUE\n)\n# Continuous trigger is not yet supported\nManaging Streaming Q", "question": "What trigger is deprecated and encourages the use of the Available-now trigger?", "answers": {"text": ["One-time trigger (Deprecated, encouraged to use Available-now trigger)"], "answer_start": [12]}}
{"context": "of the most recent progress updates for this query\nquery\n.\nlastProgress\n();\n// the most recent progress update of this streaming query\nquery\n<-\nwrite.stream\n(\ndf\n,\n\"console\"\n)\n# get the query object\nqueryName\n(\nquery\n)\n# get the name of the auto-generated or user-specified name\nexplain\n(\nquery\n)\n# print detailed explanations of the query\nstopQuery\n(\nquery\n)\n# stop the query\nawaitTermination\n(\nquery\n)\n# block until query is terminated, with stop() or with error\nlastProgress\n(\nquery\n)\n# the most recent progress update of this streaming query\nYou can start any number of queries in a single SparkSession. They will all be running concurrently sharing the cluster resources. You can use\nsparkSession.streams()\nto get the\nStreamingQueryManager\n(\nPython\n/\nScala\n/\nJava\ndocs)\nthat can be used to manag", "question": "What does `lastProgress(query)` return?", "answers": {"text": ["the most recent progress update of this streaming query"], "answer_start": [79]}}
{"context": "sing\nstreamingQuery.lastProgress()\nand\nstreamingQuery.status()\n.\nlastProgress()\nreturns a\nStreamingQueryProgress\nobject\nin\nScala\nand\nJava\nand a dictionary with the same fields in Python. It has all the information about\nthe progress made in the last trigger of the stream - what data was processed,\nwhat were the processing rates, latencies, etc. There is also\nstreamingQuery.recentProgress\nwhich returns an array of last few progresses.\nIn addition,\nstreamingQuery.status()\nreturns a\nStreamingQueryStatus\nobject\nin\nScala\nand\nJava\nand a dictionary with the same fields in Python. It gives information about\nwhat the query is immediately doing - is a trigger active, is data being processed, etc.\nHere are a few examples.\nquery\n=\n...\n# a StreamingQuery\nprint\n(\nquery\n.\nlastProgress\n)\n'''\nWill print so", "question": "O que retorna `streamingQuery.lastProgress()`?", "answers": {"text": ["returns a\nStreamingQueryProgress\nobject\nin\nScala\nand\nJava\nand a dictionary with the same fields in Python."], "answer_start": [80]}}
{"context": "cessedRowsPerSecond\n'\n: 200.0, u\n'\ninputRowsPerSecond\n'\n: 120.0, u\n'\nnumInputRows\n'\n: 10, u\n'\nstartOffset\n'\n: {u\n'\ntopic-0\n'\n: {u\n'\n1\n'\n: 1, u\n'\n0\n'\n: 1, u\n'\n3\n'\n: 1, u\n'\n2\n'\n: 0, u\n'\n4\n'\n: 1}}}], u\n'\ndurationMs\n'\n: {u\n'\ngetOffset\n'\n: 2, u\n'\ntriggerExecution\n'\n: 3}, u\n'\nrunId\n'\n: u\n'\n88e2ff94-ede0-45a8-b687-6316fbef529a\n'\n, u\n'\nid\n'\n: u\n'\nce011fdc-8762-4dcb-84eb-a77333e28109\n'\n, u\n'\nsink\n'\n: {u\n'\ndescription\n'\n: u\n'\nMemorySink\n'\n}}\n'''\nprint\n(\nquery\n.\nstatus\n)\n'''\nWill print something like the following.\n\n{u\n'\nmessage\n'\n: u\n'\nWaiting for data to arrive\n'\n, u\n'\nisTriggerActive\n'\n: False, u\n'\nisDataAvailable\n'\n: False}\n'''\nval\nquery\n:\nStreamingQuery\n=\n...\nprintln\n(\nquery\n.\nlastProgress\n)\n/* Will print something like the following.\n\n{\n  \"id\" : \"ce011fdc-8762-4dcb-84eb-a77333e28109\",\n  \"runId\"", "question": "What is the value of the 'runId'?", "answers": {"text": ["88e2ff94-ede0-45a8-b687-6316fbef529a"], "answer_start": [285]}}
{"context": "      \"4\" : 1,\n        \"1\" : 1,\n        \"3\" : 1,\n        \"0\" : 1\n      }\n    },\n    \"endOffset\" : {\n      \"topic-0\" : {\n        \"2\" : 0,\n        \"4\" : 115,\n        \"1\" : 134,\n        \"3\" : 21,\n        \"0\" : 534\n      }\n    },\n    \"numInputRows\" : 10,\n    \"inputRowsPerSecond\" : 120.0,\n    \"processedRowsPerSecond\" : 200.0\n  } ],\n  \"sink\" : {\n    \"description\" : \"MemorySink\"\n  }\n}\n*/\nprintln\n(\nquery\n.\nstatus\n)\n/*  Will print something like the following.\n{\n  \"message\" : \"Waiting for data to arrive\",\n  \"isDataAvailable\" : false,\n  \"isTriggerActive\" : false\n}\n*/\nStreamingQuery\nquery\n=\n...\nSystem\n.\nout\n.\nprintln\n(\nquery\n.\nlastProgress\n());\n/* Will print something like the following.\n\n{\n  \"id\" : \"ce011fdc-8762-4dcb-84eb-a77333e28109\",\n  \"runId\" : \"88e2ff94-ede0-45a8-b687-6316fbef529a\",\n  \"name\" :", "question": "What is the value associated with the key \"3\" in the \"endOffset\" topic-0?", "answers": {"text": ["21"], "answer_start": [189]}}
{"context": "print something like the following.\n\n{\n  \"id\" : \"ce011fdc-8762-4dcb-84eb-a77333e28109\",\n  \"runId\" : \"88e2ff94-ede0-45a8-b687-6316fbef529a\",\n  \"name\" : \"MyQuery\",\n  \"timestamp\" : \"2016-12-14T18:45:24.873Z\",\n  \"numInputRows\" : 10,\n  \"inputRowsPerSecond\" : 120.0,\n  \"processedRowsPerSecond\" : 200.0,\n  \"durationMs\" : {\n    \"triggerExecution\" : 3,\n    \"getOffset\" : 2\n  },\n  \"eventTime\" : {\n    \"watermark\" : \"2016-12-14T18:45:24.873Z\"\n  },\n  \"stateOperators\" : [ ],\n  \"sources\" : [ {\n    \"description\" : \"KafkaSource[Subscribe[topic-0]]\",\n    \"startOffset\" : {\n      \"topic-0\" : {\n        \"2\" : 0,\n        \"4\" : 1,\n        \"1\" : 1,\n        \"3\" : 1,\n        \"0\" : 1\n      }\n    },\n    \"endOffset\" : {\n      \"topic-0\" : {\n        \"2\" : 0,\n        \"4\" : 115,\n        \"1\" : 134,\n        \"3\" : 21,\n        \"0", "question": "What is the value of 'numInputRows' in the provided JSON?", "answers": {"text": ["10"], "answer_start": [82]}}
{"context": "-94b5-4c99-b100-f694162df0b9\",\n  \"runId\" : \"ae505c5a-a64e-4896-8c28-c7cbaf926f16\",\n  \"name\" : null,\n  \"timestamp\" : \"2017-04-26T08:27:28.835Z\",\n  \"numInputRows\" : 0,\n  \"inputRowsPerSecond\" : 0.0,\n  \"processedRowsPerSecond\" : 0.0,\n  \"durationMs\" : {\n    \"getOffset\" : 0,\n    \"triggerExecution\" : 1\n  },\n  \"stateOperators\" : [ {\n    \"numRowsTotal\" : 4,\n    \"numRowsUpdated\" : 0\n  } ],\n  \"sources\" : [ {\n    \"description\" : \"TextSocketSource[host: localhost, port: 9999]\",\n    \"startOffset\" : 1,\n    \"endOffset\" : 1,\n    \"numInputRows\" : 0,\n    \"inputRowsPerSecond\" : 0.0,\n    \"processedRowsPerSecond\" : 0.0\n  } ],\n  \"sink\" : {\n    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSink@76b37531\"\n  }\n}\n'''\nstatus\n(\nquery\n)\n'''\nWill print something like the following.\n\n{\n  \"message\" : \"W", "question": "What is the description of the sink?", "answers": {"text": ["org.apache.spark.sql.execution.streaming.ConsoleSink@76b37531"], "answer_start": [646]}}
{"context": " query, and continue where it left off. This is done using checkpointing and write-ahead logs. You can configure a query with a checkpoint location, and the query will save all the progress information (i.e. range of offsets processed in each trigger) and the running aggregates (e.g. word counts in the\nquick example\n) to the checkpoint location. This checkpoint location has to be a path in an HDFS compatible file system, and can be set as an option in the DataStreamWriter when\nstarting a query\n.\naggDF\n\\\n.\nwriteStream\n\\\n.\noutputMode\n(\n\"\ncomplete\n\"\n)\n\\\n.\noption\n(\n\"\ncheckpointLocation\n\"\n,\n\"\npath/to/HDFS/dir\n\"\n)\n\\\n.\nformat\n(\n\"\nmemory\n\"\n)\n\\\n.\nstart\n()\naggDF\n.\nwriteStream\n.\noutputMode\n(\n\"complete\"\n)\n.\noption\n(\n\"checkpointLocation\"\n,\n\"path/to/HDFS/dir\"\n)\n.\nformat\n(\n\"memory\"\n)\n.\nstart\n()\naggDF\n.\nw", "question": "Where does the query save progress information and running aggregates?", "answers": {"text": ["to the checkpoint location"], "answer_start": [320]}}
{"context": "n the source and the query. Here are a few examples.\nAddition/deletion/modification of rate limits is allowed:\nspark.readStream.format(\"kafka\").option(\"subscribe\", \"topic\")\nto\nspark.readStream.format(\"kafka\").option(\"subscribe\", \"topic\").option(\"maxOffsetsPerTrigger\", ...)\nChanges to subscribed topics/files are generally not allowed as the results are unpredictable:\nspark.readStream.format(\"kafka\").option(\"subscribe\", \"topic\")\nto\nspark.readStream.format(\"kafka\").option(\"subscribe\", \"newTopic\")\nChanges in the type of output sink\n: Changes between a few specific combinations of sinks\nare allowed. This needs to be verified on a case-by-case basis. Here are a few examples.\nFile sink to Kafka sink is allowed. Kafka will see only the new data.\nKafka sink to file sink is not allowed.\nKafka sink c", "question": "What type of change between output sinks is allowed?", "answers": {"text": ["File sink to Kafka sink is allowed."], "answer_start": [678]}}
{"context": "c\", \"someTopic\")\nto\nsdf.writeStream.format(\"kafka\").option(\"topic\", \"anotherTopic\")\nChanges to the user-defined foreach sink (that is, the\nForeachWriter\ncode) are allowed, but the semantics of the change depends on the code.\nChanges in projection / filter / map-like operations\n: Some cases are allowed. For example:\nAddition / deletion of filters is allowed:\nsdf.selectExpr(\"a\")\nto\nsdf.where(...).selectExpr(\"a\").filter(...)\n.\nChanges in projections with same output schema are allowed:\nsdf.selectExpr(\"stringColumn AS json\").writeStream\nto\nsdf.selectExpr(\"anotherStringColumn AS json\").writeStream\nChanges in projections with different output schema are conditionally allowed:\nsdf.selectExpr(\"a\").writeStream\nto\nsdf.selectExpr(\"b\").writeStream\nis allowed only if the output sink allows the schema c", "question": "Sob quais condições as alterações em projeções com esquemas de saída diferentes são permitidas?", "answers": {"text": ["is allowed only if the output sink allows the schema c"], "answer_start": [746]}}
{"context": "ns) to the stateful operations of a streaming query are not allowed between restarts\n.\nHere is the list of stateful operations whose schema should not be changed between restarts in order to ensure state recovery:\nStreaming aggregation\n: For example,\nsdf.groupBy(\"a\").agg(...)\n. Any change in number or type of grouping keys or aggregates is not allowed.\nStreaming deduplication\n: For example,\nsdf.dropDuplicates(\"a\")\n. Any change in number or type of deduplicating columns is not allowed.\nStream-stream join\n: For example,\nsdf1.join(sdf2, ...)\n(i.e. both inputs are generated with\nsparkSession.readStream\n). Changes\nin the schema or equi-joining columns are not allowed. Changes in join type (outer or inner) are not allowed. Other changes in the join condition are ill-defined.\nArbitrary stateful o", "question": "Quais operações stateful não permitem alterações de esquema entre reinicializações para garantir a recuperação do estado?", "answers": {"text": ["Streaming aggregation\n: For example,\nsdf.groupBy(\"a\").agg(...)\n. Any change in number or type of grouping keys or aggregates is not allowed.\nStreaming deduplication\n: For example,\nsdf.dropDuplicates(\"a\")\n. Any change in number or type of deduplicating columns is not allowed.\nStream-stream join\n: For example,\nsdf1.join(sdf2, ...)"], "answer_start": [214]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTrou", "question": "Which file formats are mentioned as data sources in the Spark SQL Guide?", "answers": {"text": ["Parquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files"], "answer_start": [650]}}
{"context": "the behavior of reading or writing, such as controlling behavior of the XML attributes, XSD validation, compression, and so on.\n# Primitive types (Int, String, etc) and Product types (case classes) encoders are\n# supported by importing this when creating a Dataset.\n# An XML dataset is pointed to by path.\n# The path can be either a single xml file or more xml files\npath\n=\n\"\nexamples/src/main/resources/people.xml\n\"\npeopleDF\n=\nspark\n.\nread\n.\noption\n(\n\"\nrowTag\n\"\n,\n\"\nperson\n\"\n).\nformat\n(\n\"\nxml\n\"\n).\nload\n(\npath\n)\n# The inferred schema can be visualized using the printSchema() method\npeopleDF\n.\nprintSchema\n()\n# root\n#  |-- age: long (nullable = true)\n#  |-- name: string (nullable = true)\n# Creates a temporary view using the DataFrame\npeopleDF\n.\ncreateOrReplaceTempView\n(\n\"\npeople\n\"\n)\n# SQL stateme", "question": "How is an XML dataset pointed to?", "answers": {"text": ["An XML dataset is pointed to by path."], "answer_start": [268]}}
{"context": ")\n#  |-- name: string (nullable = true)\n# Creates a temporary view using the DataFrame\npeopleDF\n.\ncreateOrReplaceTempView\n(\n\"\npeople\n\"\n)\n# SQL statements can be run by using the sql methods provided by spark\nteenagerNamesDF\n=\nspark\n.\nsql\n(\n\"\nSELECT name FROM people WHERE age BETWEEN 13 AND 19\n\"\n)\nteenagerNamesDF\n.\nshow\n()\n# +------+\n# |  name|\n# +------+\n# |Justin|\n# +------+\n# Alternatively, a DataFrame can be created for an XML dataset represented by a Dataset[String]\nxmlStrings\n=\n[\n\"\"\"\n<person>\n          <name>laglangyue</name>\n          <job>Developer</job>\n          <age>28</age>\n      </person>\n\"\"\"\n]\nxmlRDD\n=\nspark\n.\nsparkContext\n.\nparallelize\n(\nxmlStrings\n)\notherPeople\n=\nspark\n.\nread\n\\\n.\noption\n(\n\"\nrowTag\n\"\n,\n\"\nperson\n\"\n)\n\\\n.\nxml\n(\nxmlRDD\n)\notherPeople\n.\nshow\n()\n# +---+---------+---", "question": "What is used to create a temporary view using the DataFrame peopleDF?", "answers": {"text": ["Creates a temporary view using the DataFrame"], "answer_start": [42]}}
{"context": "llelize\n(\nxmlStrings\n)\notherPeople\n=\nspark\n.\nread\n\\\n.\noption\n(\n\"\nrowTag\n\"\n,\n\"\nperson\n\"\n)\n\\\n.\nxml\n(\nxmlRDD\n)\notherPeople\n.\nshow\n()\n# +---+---------+----------+\n# |age|      job|      name|\n# +---+---------+----------+\n# | 28|Developer|laglangyue|\n# +---+---------+----------+\nFind full example code at \"examples/src/main/python/sql/datasource.py\" in the Spark repo.\n// Primitive types (Int, String, etc) and Product types (case classes) encoders are\n// supported by importing this when creating a Dataset.\nimport\nspark.implicits._\n// An XML dataset is pointed to by path.\n// The path can be either a single xml file or more xml files\nval\npath\n=\n\"examples/src/main/resources/people.xml\"\nval\npeopleDF\n=\nspark\n.\nread\n.\noption\n(\n\"rowTag\"\n,\n\"person\"\n).\nxml\n(\npath\n)\n// The inferred schema can be visualized", "question": "What option is used to specify the row tag when reading XML data?", "answers": {"text": ["\"rowTag\""], "answer_start": [724]}}
{"context": "les/src/main/resources/people.xml\"\nval\npeopleDF\n=\nspark\n.\nread\n.\noption\n(\n\"rowTag\"\n,\n\"person\"\n).\nxml\n(\npath\n)\n// The inferred schema can be visualized using the printSchema() method\npeopleDF\n.\nprintSchema\n()\n// root\n//  |-- age: long (nullable = true)\n//  |-- name: string (nullable = true)\n// Creates a temporary view using the DataFrame\npeopleDF\n.\ncreateOrReplaceTempView\n(\n\"people\"\n)\n// SQL statements can be run by using the sql methods provided by spark\nval\nteenagerNamesDF\n=\nspark\n.\nsql\n(\n\"SELECT name FROM people WHERE age BETWEEN 13 AND 19\"\n)\nteenagerNamesDF\n.\nshow\n()\n// +------+\n// |  name|\n// +------+\n// |Justin|\n// +------+\n// Alternatively, a DataFrame can be created for a XML dataset represented by a Dataset[String]\nval\notherPeopleDataset\n=\nspark\n.\ncreateDataset\n(\n\"\"\"\n    |<person>\n", "question": "How can the inferred schema of the DataFrame be visualized?", "answers": {"text": ["The inferred schema can be visualized using the printSchema() method"], "answer_start": [113]}}
{"context": "ely, a DataFrame can be created for a XML dataset represented by a Dataset[String]\nval\notherPeopleDataset\n=\nspark\n.\ncreateDataset\n(\n\"\"\"\n    |<person>\n    |    <name>laglangyue</name>\n    |    <job>Developer</job>\n    |    <age>28</age>\n    |</person>\n    |\"\"\"\n.\nstripMargin\n::\nNil\n)\nval\notherPeople\n=\nspark\n.\nread\n.\noption\n(\n\"rowTag\"\n,\n\"person\"\n)\n.\nxml\n(\notherPeopleDataset\n)\notherPeople\n.\nshow\n()\n// +---+---------+----------+\n// |age|      job|      name|\n// +---+---------+----------+\n// | 28|Developer|laglangyue|\n// +---+---------+----------+\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\n// Primitive types (Int, String, etc) and Product types (case classes) encoders are\n// supported by importing this when crea", "question": "How is a DataFrame created for an XML dataset represented by a Dataset[String]?", "answers": {"text": ["ely, a DataFrame can be created for a XML dataset represented by a Dataset[String]"], "answer_start": [0]}}
{"context": "scala\" in the Spark repo.\n// Primitive types (Int, String, etc) and Product types (case classes) encoders are\n// supported by importing this when creating a Dataset.\n// An XML dataset is pointed to by path.\n// The path can be either a single xml file or more xml files\nString\npath\n=\n\"examples/src/main/resources/people.xml\"\n;\nDataset\n<\nRow\n>\npeopleDF\n=\nspark\n.\nread\n().\noption\n(\n\"rowTag\"\n,\n\"person\"\n).\nxml\n(\npath\n);\n// The inferred schema can be visualized using the printSchema() method\npeopleDF\n.\nprintSchema\n();\n// root\n//  |-- age: long (nullable = true)\n//  |-- name: string (nullable = true)\n// Creates a temporary view using the DataFrame\npeopleDF\n.\ncreateOrReplaceTempView\n(\n\"people\"\n);\n// SQL statements can be run by using the sql methods provided by spark\nDataset\n<\nRow\n>\nteenagerNamesDF\n=", "question": "What is the row tag used when reading the XML file?", "answers": {"text": ["\"rowTag\""], "answer_start": [379]}}
{"context": "leDF\n.\ncreateOrReplaceTempView\n(\n\"people\"\n);\n// SQL statements can be run by using the sql methods provided by spark\nDataset\n<\nRow\n>\nteenagerNamesDF\n=\nspark\n.\nsql\n(\n\"SELECT name FROM people WHERE age BETWEEN 13 AND 19\"\n);\nteenagerNamesDF\n.\nshow\n();\n// +------+\n// |  name|\n// +------+\n// |Justin|\n// +------+\n// Alternatively, a DataFrame can be created for an XML dataset represented by a Dataset[String]\nList\n<\nString\n>\nxmlData\n=\nCollections\n.\nsingletonList\n(\n\"<person>\"\n+\n\"<name>laglangyue</name><job>Developer</job><age>28</age>\"\n+\n\"</person>\"\n);\nDataset\n<\nString\n>\notherPeopleDataset\n=\nspark\n.\ncreateDataset\n(\nLists\n.\nnewArrayList\n(\nxmlData\n),\nEncoders\n.\nSTRING\n());\nDataset\n<\nRow\n>\notherPeople\n=\nspark\n.\nread\n()\n.\noption\n(\n\"rowTag\"\n,\n\"person\"\n)\n.\nxml\n(\notherPeopleDataset\n);\notherPeople\n.\nshow\n", "question": "How is a DataFrame created for an XML dataset represented by a Dataset[String]?", "answers": {"text": ["Alternatively, a DataFrame can be created for an XML dataset represented by a Dataset[String]"], "answer_start": [312]}}
{"context": "ncoders\n.\nSTRING\n());\nDataset\n<\nRow\n>\notherPeople\n=\nspark\n.\nread\n()\n.\noption\n(\n\"rowTag\"\n,\n\"person\"\n)\n.\nxml\n(\notherPeopleDataset\n);\notherPeople\n.\nshow\n();\n// +---+---------+----------+\n// |age|      job|      name|\n// +---+---------+----------+\n// | 28|Developer|laglangyue|\n// +---+---------+----------+\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\nData Source Option\nData source options of XML can be set via:\nthe\n.option\n/\n.options\nmethods of\nDataFrameReader\nDataFrameWriter\nDataStreamReader\nDataStreamWriter\nthe built-in functions below\nfrom_xml\nto_xml\nschema_of_xml\nOPTIONS\nclause at\nCREATE TABLE USING DATA_SOURCE\nProperty Name\nDefault\nMeaning\nScope\nrowTag\nThe row tag of your xml files to treat as a row. For ", "question": "What is the purpose of the `rowTag` option when reading XML data with Spark?", "answers": {"text": ["The row tag of your xml files to treat as a row."], "answer_start": [747]}}
{"context": "ameOfCorruptRecord, and sets malformed fields to null. To keep corrupt records, an user can set a string type field named columnNameOfCorruptRecord in an user-defined schema. If a schema does not have the field, it drops corrupt records during parsing. When inferring a schema, it implicitly adds a columnNameOfCorruptRecord field in an output schema.\nDROPMALFORMED\n: ignores the whole corrupted records. This mode is unsupported in the XML built-in functions.\nFAILFAST\n: throws an exception when it meets corrupted records.\nread\ninferSchema\ntrue\nIf true, attempts to infer an appropriate type for each resulting DataFrame column. If false, all resulting columns are of string type.\nread\ncolumnNameOfCorruptRecord\nspark.sql.columnNameOfCorruptRecord\nAllows renaming the new field having a malformed s", "question": "What happens when a schema does not have the field 'columnNameOfCorruptRecord'?", "answers": {"text": ["If a schema does not have the field, it drops corrupt records during parsing."], "answer_start": [175]}}
{"context": "ting columns are of string type.\nread\ncolumnNameOfCorruptRecord\nspark.sql.columnNameOfCorruptRecord\nAllows renaming the new field having a malformed string created by PERMISSIVE mode.\nread\nattributePrefix\n_\nThe prefix for attributes to differentiate attributes from elements. This will be the prefix for field names. Can be empty for reading XML, but not for writing.\nread/write\nvalueTag\n_VALUE\nThe tag used for the value when there are attributes in the element having no child.\nread/write\nencoding\nUTF-8\nFor reading, decodes the XML files by the given encoding type. For writing, specifies encoding (charset) of saved XML files. XML built-in functions ignore this option.\nread/write\nignoreSurroundingSpaces\ntrue\nDefines whether surrounding whitespaces from values being read should be skipped.\nread", "question": "What is the default value for ignoreSurroundingSpaces?", "answers": {"text": ["true"], "answer_start": [709]}}
{"context": "ons ignore this option.\nread/write\nignoreSurroundingSpaces\ntrue\nDefines whether surrounding whitespaces from values being read should be skipped.\nread\nrowValidationXSDPath\nnull\nPath to an optional XSD file that is used to validate the XML for each row individually. Rows that fail to validate are treated like parse errors as above. The XSD does not otherwise affect the schema provided, or inferred.\nread\nignoreNamespace\nfalse\nIf true, namespaces prefixes on XML elements and attributes are ignored. Tags <abc:author> and <def:author> would, for example, be treated as if both are just <author>. Note that, at the moment, namespaces cannot be ignored on the rowTag element, only its children. Note that XML parsing is in general not namespace-aware even if false.\nread\ntimeZone\n(value of\nspark.sql.s", "question": "What happens when row validation fails according to the XSD file?", "answers": {"text": ["Rows that fail to validate are treated like parse errors as above."], "answer_start": [266]}}
{"context": "d on the rowTag element, only its children. Note that XML parsing is in general not namespace-aware even if false.\nread\ntimeZone\n(value of\nspark.sql.session.timeZone\nconfiguration)\nSets the string that indicates a time zone ID to be used to format timestamps in the XML datasources or partition values. The following formats of timeZone are supported:\nRegion-based zone ID: It should have the form 'area/city', such as 'America/Los_Angeles'.\nZone offset: It should be in the format '(+|-)HH:mm', for example '-08:00' or '+01:00', also 'UTC' and 'Z' are supported as aliases of '+00:00'.\nOther short names like 'CST' are not recommended to use because they can be ambiguous.\nread/write\ntimestampFormat\nyyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]\nSets the string that indicates a timestamp format. Custom date for", "question": "What formats of timeZone are supported?", "answers": {"text": ["Region-based zone ID: It should have the form 'area/city', such as 'America/Los_Angeles'.\nZone offset: It should be in the format '(+|-)HH:mm', for example '-08:00' or '+01:00', also 'UTC' and 'Z' are supported as aliases of '+00:00'."], "answer_start": [352]}}
{"context": " they can be ambiguous.\nread/write\ntimestampFormat\nyyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]\nSets the string that indicates a timestamp format. Custom date formats follow the formats at\ndatetime pattern\n. This applies to timestamp type.\nread/write\ntimestampNTZFormat\nyyyy-MM-dd'T'HH:mm:ss[.SSS]\nSets the string that indicates a timestamp without timezone format. Custom date formats follow the formats at\nDatetime Patterns\n. This applies to timestamp without timezone type, note that zone-offset and time-zone components are not supported when writing or reading this data type.\nread/write\ndateFormat\nyyyy-MM-dd\nSets the string that indicates a date format. Custom date formats follow the formats at\ndatetime pattern\n. This applies to date type.\nread/write\nlocale\nen-US\nSets a locale as a language tag in IETF", "question": "What format does the 'timestampNTZFormat' property use?", "answers": {"text": ["yyyy-MM-dd'T'HH:mm:ss[.SSS]"], "answer_start": [51]}}
{"context": "ustom date formats follow the formats at\ndatetime pattern\n. This applies to date type.\nread/write\nlocale\nen-US\nSets a locale as a language tag in IETF BCP 47 format. For instance, locale is used while parsing dates and timestamps.\nread/write\nrootTag\nROWS\nRoot tag of the xml files. For example, in this xml:\n...\nthe appropriate value would be books. It can include basic attributes by specifying a value like 'books'\nwrite\ndeclaration\nversion=\"1.0\"\nencoding=\"UTF-8\"\nstandalone=\"yes\"\nContent of XML declaration to write at the start of every output XML file, before the rootTag. For example, a value of foo causes\nto be written. Set to empty string to suppress\nwrite\narrayElementName\nitem\nName of XML element that encloses each element of an array-valued column when writing.\nwrite\nnullValue\nnull\nSets", "question": "What is the default name of the XML element that encloses each element of an array-valued column when writing?", "answers": {"text": ["item"], "answer_start": [683]}}
{"context": " suppress\nwrite\narrayElementName\nitem\nName of XML element that encloses each element of an array-valued column when writing.\nwrite\nnullValue\nnull\nSets the string representation of a null value. Default is string null. When this is null, it does not write attributes and elements for fields.\nread/write\nwildcardColName\nxs_any\nName of a column existing in the provided schema which is interpreted as a 'wildcard'. It must have type string or array of strings. It will match any XML child element that is not otherwise matched by the schema. The XML of the child becomes the string value of the column. If an array, then all unmatched elements will be returned as an array of strings. As its name implies, it is meant to emulate XSD's xs:any type.\nread\ncompression\nnone\nCompression codec to use when sav", "question": "What is the purpose of the 'wildcardColName' option?", "answers": {"text": ["Name of a column existing in the provided schema which is interpreted as a 'wildcard'. It must have type string or array of strings. It will match any XML child element that is not otherwise matched by the schema."], "answer_start": [325]}}
{"context": "eturned as an array of strings. As its name implies, it is meant to emulate XSD's xs:any type.\nread\ncompression\nnone\nCompression codec to use when saving to file. This can be one of the known case-insensitive shortened names (none, bzip2, gzip, lz4, snappy and deflate). XML built-in functions ignore this option.\nwrite\nvalidateName\ntrue\nIf true, throws error on XML element name validation failure. For example, SQL field names can have spaces, but XML element names cannot.\nwrite\nOther generic options can be found in\nGeneric File Source Options\n.", "question": "What are the possible values for the compression codec option?", "answers": {"text": ["none, bzip2, gzip, lz4, snappy and deflate"], "answer_start": [226]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTrou", "question": "Which file formats are mentioned as data sources in the Spark SQL Guide?", "answers": {"text": ["Parquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files"], "answer_start": [650]}}
{"context": "Parquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTroubleshooting\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nBinary File Data Source\nSince Spark 3.0, Spark supports binary file data source,\nwhich reads binary files and converts each file into a single record that contains the raw content\nand metadata of the file.\nIt produces a DataFrame with the following columns and possibly partition columns:\npath\n: StringType\nmodificationTime\n: TimestampType\nlength\n: LongType\ncontent\n: BinaryType\nTo read whole binary files, you need to specify the data source\nformat\nas\nbinaryFile\n.\nTo load files with paths matching ", "question": "What columns are produced when using the binary file data source?", "answers": {"text": ["path\n: StringType\nmodificationTime\n: TimestampType\nlength\n: LongType\ncontent\n: BinaryType"], "answer_start": [589]}}
{"context": "ng\"\n).\nload\n(\n\"/path/to/data\"\n);\nread.df\n(\n\"/path/to/data\"\n,\nsource\n=\n\"binaryFile\"\n,\npathGlobFilter\n=\n\"*.png\"\n)\nBinary file data source does not support writing a DataFrame back to the original files.", "question": "What limitation is described regarding the binary file data source?", "answers": {"text": ["Binary file data source does not support writing a DataFrame back to the original files."], "answer_start": [112]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTrou", "question": "Which file formats are mentioned as data sources in the Spark SQL Guide?", "answers": {"text": ["Parquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files"], "answer_start": [650]}}
{"context": "built-in support for reading and writing Apache Avro data.\nDeploying\nThe\nspark-avro\nmodule is external and not included in\nspark-submit\nor\nspark-shell\nby default.\nAs with any Spark applications,\nspark-submit\nis used to launch your application.\nspark-avro_2.13\nand its dependencies can be directly added to\nspark-submit\nusing\n--packages\n, such as,\n./bin/spark-submit --packages org.apache.spark:spark-avro_2.13:4.0.0 ...\nFor experimenting on\nspark-shell\n, you can also use\n--packages\nto add\norg.apache.spark:spark-avro_2.13\nand its dependencies directly,\n./bin/spark-shell --packages org.apache.spark:spark-avro_2.13:4.0.0 ...\nSee\nApplication Submission Guide\nfor more details about submitting applications with external dependencies.\nLoad and Save Functions\nSince\nspark-avro\nmodule is external, there", "question": "How can you add spark-avro_2.13 and its dependencies to spark-submit?", "answers": {"text": ["./bin/spark-submit --packages org.apache.spark:spark-avro_2.13:4.0.0 ..."], "answer_start": [347]}}
{"context": " useful when you would like to re-encode multiple columns into a single one when writing data out to Kafka.\nfrom\npyspark.sql.avro.functions\nimport\nfrom_avro\n,\nto_avro\n# `from_avro` requires Avro schema in JSON string format.\njsonFormatSchema\n=\nopen\n(\n\"\nexamples/src/main/resources/user.avsc\n\"\n,\n\"\nr\n\"\n).\nread\n()\ndf\n=\nspark\n\\\n.\nreadStream\n\\\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\noption\n(\n\"\nsubscribe\n\"\n,\n\"\ntopic1\n\"\n)\n\\\n.\nload\n()\n# 1. Decode the Avro data into a struct;\n# 2. Filter by column `favorite_color`;\n# 3. Encode the column `name` in Avro format.\noutput\n=\ndf\n\\\n.\nselect\n(\nfrom_avro\n(\n\"\nvalue\n\"\n,\njsonFormatSchema\n).\nalias\n(\n\"\nuser\n\"\n))\n\\\n.\nwhere\n(\n'\nuser.favorite_color ==\n\"\nred\n\"'\n)\n\\\n.\nselect\n(\nto_avro\n(\n\"\nuser.name\n\"\n).\nalias\n", "question": "What is required by `from_avro`?", "answers": {"text": ["Avro schema in JSON string format."], "answer_start": [190]}}
{"context": "o\n(\n\"\nvalue\n\"\n,\njsonFormatSchema\n).\nalias\n(\n\"\nuser\n\"\n))\n\\\n.\nwhere\n(\n'\nuser.favorite_color ==\n\"\nred\n\"'\n)\n\\\n.\nselect\n(\nto_avro\n(\n\"\nuser.name\n\"\n).\nalias\n(\n\"\nvalue\n\"\n))\nquery\n=\noutput\n\\\n.\nwriteStream\n\\\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n\\\n.\noption\n(\n\"\ntopic\n\"\n,\n\"\ntopic2\n\"\n)\n\\\n.\nstart\n()\nimport\norg.apache.spark.sql.avro.functions._\n// `from_avro` requires Avro schema in JSON string format.\nval\njsonFormatSchema\n=\nnew\nString\n(\nFiles\n.\nreadAllBytes\n(\nPaths\n.\nget\n(\n\"./examples/src/main/resources/user.avsc\"\n)))\nval\ndf\n=\nspark\n.\nreadStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribe\"\n,\n\"topic1\"\n)\n.\nload\n()\n// 1. Decode the Avro data into a struct;\n// 2. Filter by column `favo", "question": "What servers are specified for the Kafka bootstrap?", "answers": {"text": ["host1:port1,host2:port2"], "answer_start": [266]}}
{"context": "rs\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribe\"\n,\n\"topic1\"\n)\n.\nload\n()\n// 1. Decode the Avro data into a struct;\n// 2. Filter by column `favorite_color`;\n// 3. Encode the column `name` in Avro format.\nval\noutput\n=\ndf\n.\nselect\n(\nfrom_avro\n(\n$\n\"value\"\n,\njsonFormatSchema\n)\nas\n$\n\"user\"\n)\n.\nwhere\n(\n\"user.favorite_color == \\\"red\\\"\"\n)\n.\nselect\n(\nto_avro\n(\n$\n\"user.name\"\n)\nas\n$\n\"value\"\n)\nval\nquery\n=\noutput\n.\nwriteStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"topic\"\n,\n\"topic2\"\n)\n.\nstart\n()\nimport\nstatic\norg\n.\napache\n.\nspark\n.\nsql\n.\nfunctions\n.\ncol\n;\nimport\nstatic\norg\n.\napache\n.\nspark\n.\nsql\n.\navro\n.\nfunctions\n.*;\n// `from_avro` requires Avro schema in JSON string format.\nString\njsonFormatSchema\n=\nnew\nString\n(\nFiles\n.\nreadAllBytes\n(", "question": "What is required by `from_avro`?", "answers": {"text": ["Avro schema in JSON string format."], "answer_start": [704]}}
{"context": "\n.\nsql\n.\navro\n.\nfunctions\n.*;\n// `from_avro` requires Avro schema in JSON string format.\nString\njsonFormatSchema\n=\nnew\nString\n(\nFiles\n.\nreadAllBytes\n(\nPaths\n.\nget\n(\n\"./examples/src/main/resources/user.avsc\"\n)));\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\nreadStream\n()\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribe\"\n,\n\"topic1\"\n)\n.\nload\n();\n// 1. Decode the Avro data into a struct;\n// 2. Filter by column `favorite_color`;\n// 3. Encode the column `name` in Avro format.\nDataset\n<\nRow\n>\noutput\n=\ndf\n.\nselect\n(\nfrom_avro\n(\ncol\n(\n\"value\"\n),\njsonFormatSchema\n).\nas\n(\n\"user\"\n))\n.\nwhere\n(\n\"user.favorite_color == \\\"red\\\"\"\n)\n.\nselect\n(\nto_avro\n(\ncol\n(\n\"user.name\"\n)).\nas\n(\n\"value\"\n));\nStreamingQuery\nquery\n=\noutput\n.\nwriteStream\n()\n.\nformat\n(\n\"kafka\"\n)\n.\nop", "question": "What is required by `from_avro`?", "answers": {"text": ["Avro schema in JSON string format."], "answer_start": [54]}}
{"context": "output\n<-\nselect\n(\nfilter\n(\nselect\n(\ndf\n,\nalias\n(\nfrom_avro\n(\n\"value\"\n,\njsonFormatSchema\n),\n\"user\"\n)),\ncolumn\n(\n\"user.favorite_color\"\n)\n==\n\"red\"\n),\nalias\n(\nto_avro\n(\n\"user.name\"\n),\n\"value\"\n)\n)\nwrite.stream\n(\noutput\n,\n\"kafka\"\n,\nkafka.bootstrap.servers\n=\n\"host1:port1,host2:port2\"\n,\ntopic\n=\n\"topic2\"\n)\nCREATE\nTABLE\nt\nAS\nSELECT\nNAMED_STRUCT\n(\n'u'\n,\nNAMED_STRUCT\n(\n'member0'\n,\nmember0\n,\n'member1'\n,\nmember1\n))\nAS\ns\nFROM\nVALUES\n(\n1\n,\nNULL\n),\n(\nNULL\n,\n'a'\n)\ntab\n(\nmember0\n,\nmember1\n);\nDECLARE\navro_schema\nSTRING\n;\nSET\nVARIABLE\navro_schema\n=\n'{ \"type\": \"record\", \"name\": \"struct\", \"fields\": [{ \"name\": \"u\", \"type\": [\"int\",\"string\"] }] }'\n;\nSELECT\nTO_AVRO\n(\ns\n,\navro_schema\n)\nAS\nRESULT\nFROM\nt\n;\nSELECT\nFROM_AVRO\n(\nresult\n,\navro_schema\n,\nMAP\n()).\nu\nFROM\n(\nSELECT\nTO_AVRO\n(\ns\n,\navro_schema\n)\nAS\nRESULT\nFROM\nt\n)", "question": "What is the schema used for converting data to Avro format?", "answers": {"text": ["{ \"type\": \"record\", \"name\": \"struct\", \"fields\": [{ \"name\": \"u\", \"type\": [\"int\",\"string\"] }] }"], "answer_start": [536]}}
{"context": " Avro schema. The deserialization schema will be consistent with the evolved schema.\n          For example, if we set an evolved schema containing one additional column with a default value,\n          the reading result in Spark will contain the new column too. Note that when using this option with\nfrom_avro\n, you still need to pass the actual Avro schema as a parameter to the function.\nWhen writing Avro, this option can be set if the expected output Avro schema doesn't match the\n          schema converted by Spark. For example, the expected schema of one column is of \"enum\" type,\n          instead of \"string\" type in the default converted schema.\nread, write and function\nfrom_avro\n2.4.0\nrecordName\ntopLevelRecord\nTop level record name in write result, which is required in Avro spec.\nwrite\n", "question": "What is required in Avro spec for the top level record name in write result?", "answers": {"text": ["Top level record name in write result, which is required in Avro spec."], "answer_start": [723]}}
{"context": "used in write.\nCurrently supported codecs are\nuncompressed\n,\nsnappy\n,\ndeflate\n,\nbzip2\n,\nxz\nand\nzstandard\n.\nIf the option is not set, the configuration\nspark.sql.avro.compression.codec\nconfig is taken into account.\nwrite\n2.4.0\nmode\nFAILFAST\nThe\nmode\noption allows to specify parse mode for function\nfrom_avro\n.\nCurrently supported modes are:\nFAILFAST\n: Throws an exception on processing corrupted record.\nPERMISSIVE\n: Corrupt records are processed as null result. Therefore, the\n        data schema is forced to be fully nullable, which might be different from the one user provided.\nfunction\nfrom_avro\n2.4.0\ndatetimeRebaseMode\n(value of\nspark.sql.avro.datetimeRebaseModeInRead\nconfiguration)\nThe\ndatetimeRebaseMode\noption allows to specify the rebasing mode for the values of the\ndate\n,\ntimestamp-mic", "question": "What happens when the mode is set to PERMISSIVE?", "answers": {"text": ["Corrupt records are processed as null result. Therefore, the\n        data schema is forced to be fully nullable, which might be different from the one user provided."], "answer_start": [417]}}
{"context": "just the behavior for matching the fields in the provided Avro schema with those in the SQL schema. By default, the matching will be performed using field names, ignoring their positions. If this option is set to \"true\", the matching will be based on the position of the fields.\nread and write\n3.2.0\nenableStableIdentifiersForUnionType\nfalse\nIf it is set to true, Avro schema is deserialized into Spark SQL schema, and the Avro Union type is transformed into a structure where the field names remain consistent with their respective types. The resulting field names are converted to lowercase, e.g. member_int or member_string. If two user-defined type names or a user-defined type name and a built-in type name are identical regardless of case, an exception will be raised. However, in other cases, ", "question": "What happens when 'enableStableIdentifiersForUnionType' is set to true?", "answers": {"text": ["If it is set to true, Avro schema is deserialized into Spark SQL schema, and the Avro Union type is transformed into a structure where the field names remain consistent with their respective types."], "answer_start": [342]}}
{"context": "pe names or a user-defined type name and a built-in type name are identical regardless of case, an exception will be raised. However, in other cases, the field names can be uniquely identified.\nread\n3.5.0\nstableIdentifierPrefixForUnionType\nmember_\nWhen `enableStableIdentifiersForUnionType` is enabled, the option allows to configure the prefix for fields of Avro Union type.\nread\n4.0.0\nrecursiveFieldMaxDepth\n-1\nIf this option is specified to negative or is set to 0, recursive fields are not permitted. Setting it to 1 drops all recursive fields, 2 allows recursive fields to be recursed once, and 3 allows it to be recursed twice and so on, up to 15. Values larger than 15 are not allowed in order to avoid inadvertently creating very large schemas. If an avro message has depth beyond this limit,", "question": "What happens if a user-defined type name and a built-in type name are identical, regardless of case?", "answers": {"text": ["an exception will be raised."], "answer_start": [96]}}
{"context": "15. Values larger than 15 are not allowed in order to avoid inadvertently creating very large schemas. If an avro message has depth beyond this limit, the Spark struct returned is truncated after the recursion limit. An example of usage can be found in section\nHandling circular references of Avro fields\nread\n4.0.0\nConfiguration\nConfiguration of Avro can be done via\nspark.conf.set\nor by running\nSET key=value\ncommands using SQL.\nProperty Name\nDefault\nMeaning\nSince Version\nspark.sql.legacy.replaceDatabricksSparkAvro.enabled\ntrue\nIf it is set to true, the data source provider\ncom.databricks.spark.avro\nis mapped\n      to the built-in but external Avro data source module for backward compatibility.\nNote:\nthe SQL config has been deprecated in Spark 3.2 and might be removed in the future.\n2.4.0\nsp", "question": "What happens if an Avro message has depth beyond 15?", "answers": {"text": ["If an avro message has depth beyond this limit, the Spark struct returned is truncated after the recursion limit."], "answer_start": [103]}}
{"context": "Avro data source module for backward compatibility.\nNote:\nthe SQL config has been deprecated in Spark 3.2 and might be removed in the future.\n2.4.0\nspark.sql.avro.compression.codec\nsnappy\nCompression codec used in writing of AVRO files. Supported codecs: uncompressed, deflate,\n      snappy, bzip2, xz and zstandard. Default codec is snappy.\n2.4.0\nspark.sql.avro.deflate.level\n-1\nCompression level for the deflate codec used in writing of AVRO files. Valid value must be in\n      the range of from 1 to 9 inclusive or -1. The default value is -1 which corresponds to 6 level\n      in the current implementation.\n2.4.0\nspark.sql.avro.xz.level\n6\nCompression level for the xz codec used in writing of AVRO files. Valid value must be in\n      the range of from 1 to 9 inclusive. The default value is 6 in", "question": "What is the default compression codec used when writing AVRO files?", "answers": {"text": ["snappy"], "answer_start": [181]}}
{"context": "ssion level for the xz codec used in writing of AVRO files. Valid value must be in\n      the range of from 1 to 9 inclusive. The default value is 6 in the current implementation.\n4.0.0\nspark.sql.avro.zstandard.level\n3\nCompression level for the zstandard codec used in writing of AVRO files.\n      The default value is 3 in the current implementation.\n4.0.0\nspark.sql.avro.zstandard.bufferPool.enabled\nfalse\nIf true, enable buffer pool of ZSTD JNI library when writing of AVRO files.\n4.0.0\nspark.sql.avro.datetimeRebaseModeInRead\nEXCEPTION\nThe rebasing mode for the values of the\ndate\n,\ntimestamp-micros\n,\ntimestamp-millis\nlogical types from the Julian to Proleptic Gregorian calendar:\nEXCEPTION\n: Spark will fail the reading if it sees ancient dates/timestamps that are ambiguous between the two cale", "question": "What is the default compression level for the zstandard codec when writing AVRO files?", "answers": {"text": ["The default value is 3 in the current implementation."], "answer_start": [297]}}
{"context": "n to Proleptic Gregorian calendar:\nEXCEPTION\n: Spark will fail the reading if it sees ancient dates/timestamps that are ambiguous between the two calendars.\nCORRECTED\n: Spark will not do rebase and read the dates/timestamps as it is.\nLEGACY\n: Spark will rebase dates/timestamps from the legacy hybrid (Julian + Gregorian) calendar to Proleptic Gregorian calendar when reading Avro files.\nThis config is only effective if the writer info (like Spark, Hive) of the Avro files is unknown.\n3.0.0\nspark.sql.avro.datetimeRebaseModeInWrite\nEXCEPTION\nThe rebasing mode for the values of the\ndate\n,\ntimestamp-micros\n,\ntimestamp-millis\nlogical types from the Proleptic Gregorian to Julian calendar:\nEXCEPTION\n: Spark will fail the writing if it sees ancient dates/timestamps that are ambiguous between the two ", "question": "What happens if Spark encounters ambiguous ancient dates/timestamps when reading Avro files?", "answers": {"text": ["Spark will fail the reading if it sees ancient dates/timestamps that are ambiguous between the two calendars."], "answer_start": [47]}}
{"context": "roleptic Gregorian to Julian calendar:\nEXCEPTION\n: Spark will fail the writing if it sees ancient dates/timestamps that are ambiguous between the two calendars.\nCORRECTED\n: Spark will not do rebase and write the dates/timestamps as it is.\nLEGACY\n: Spark will rebase dates/timestamps from Proleptic Gregorian calendar to the legacy hybrid (Julian + Gregorian) calendar when writing Avro files.\n3.0.0\nspark.sql.avro.filterPushdown.enabled\ntrue\nWhen true, enable filter pushdown to Avro datasource.\n3.1.0\nCompatibility with Databricks spark-avro\nThis Avro data source module is originally from and compatible with Databricks’s open source repository\nspark-avro\n.\nBy default with the SQL configuration\nspark.sql.legacy.replaceDatabricksSparkAvro.enabled\nenabled, the data source provider\ncom.databricks.s", "question": "What happens when Spark encounters ancient dates/timestamps ambiguous between the Proleptic Gregorian and Julian calendars?", "answers": {"text": ["Spark will fail the writing if it sees ancient dates/timestamps that are ambiguous between the two calendars."], "answer_start": [51]}}
{"context": "er\nor\nDataFrameReader\ninstead, which should be clean and good enough.\nIf you prefer using your own build of\nspark-avro\njar file, you can simply disable the configuration\nspark.sql.legacy.replaceDatabricksSparkAvro.enabled\n, and use the option\n--jars\non deploying your\napplications. Read the\nAdvanced Dependency Management\nsection in the Application\nSubmission Guide for more details.\nSupported types for Avro -> Spark SQL conversion\nCurrently Spark supports reading all\nprimitive types\nand\ncomplex types\nunder records of Avro.\nAvro type\nSpark SQL type\nboolean\nBooleanType\nint\nIntegerType\nlong\nLongType\nfloat\nFloatType\ndouble\nDoubleType\nstring\nStringType\nenum\nStringType\nfixed\nBinaryType\nbytes\nBinaryType\nrecord\nStructType\narray\nArrayType\nmap\nMapType\nunion\nSee below\nIn addition to the types listed ab", "question": "What Spark SQL type corresponds to the Avro type 'long'?", "answers": {"text": ["LongType"], "answer_start": [593]}}
{"context": "ype\nenum\nStringType\nfixed\nBinaryType\nbytes\nBinaryType\nrecord\nStructType\narray\nArrayType\nmap\nMapType\nunion\nSee below\nIn addition to the types listed above, it supports reading\nunion\ntypes. The following three types are considered basic\nunion\ntypes:\nunion(int, long)\nwill be mapped to LongType.\nunion(float, double)\nwill be mapped to DoubleType.\nunion(something, null)\n, where something is any supported Avro type. This will be mapped to the same Spark SQL type as that of something, with nullable set to true.\nAll other union types are considered complex. They will be mapped to StructType where field names are member0, member1, etc., in accordance with members of the union. This is consistent with the behavior when converting between Avro and Parquet.\nIt also supports reading the following Avro\nl", "question": "What happens when a union of a type and null is encountered during Avro type reading?", "answers": {"text": ["This will be mapped to the same Spark SQL type as that of something, with nullable set to true."], "answer_start": [413]}}
{"context": "ted to int); however, there are a few special cases which are listed below:\nSpark SQL type\nAvro type\nAvro logical type\nByteType\nint\nShortType\nint\nBinaryType\nbytes\nDateType\nint\ndate\nTimestampType\nlong\ntimestamp-micros\nDecimalType\nfixed\ndecimal\nYou can also specify the whole output Avro schema with the option\navroSchema\n, so that Spark SQL types can be converted into other Avro types. The following conversions are not applied by default and require user specified Avro schema:\nSpark SQL type\nAvro type\nAvro logical type\nBinaryType\nfixed\nStringType\nenum\nTimestampType\nlong\ntimestamp-millis\nDecimalType\nbytes\ndecimal\nHandling circular references of Avro fields\nIn Avro, a circular reference occurs when the type of a field is defined in one of the parent records. This can cause issues when parsing t", "question": "What Avro type is used for Spark SQL's BinaryType when a user-specified Avro schema is provided?", "answers": {"text": ["fixed"], "answer_start": [229]}}
{"context": "g\n// structure based on `recursiveFieldMaxDepth` value.\n\n1: struct<Id: int>\n2: struct<Id: int, Next: struct<Id: int>>\n3: struct<Id: int, Next: struct<Id: int, Next: struct<Id: int>>>\n```", "question": "What is the structure based on?", "answers": {"text": ["`recursiveFieldMaxDepth` value."], "answer_start": [24]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)"], "answer_start": [46]}}
{"context": "ures\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-based API Guide\nData types\nBasic statistics\nClassification and regression\nCollaborative filtering\nClustering\nDimensionality reduction\nFeature extraction and transformation\nFrequent pattern mining\nEvaluation metrics\nPMML model export\nOptimization (developer)\nRegression - RDD-based API\nIsotonic regression\nIsotonic regression\nbelongs to the family of regression algorithms. Formally isotonic regression is a problem where\ngiven a finite set of real numbers\n$Y = {y_1, y_2, ..., y_n}$\nrepresenting observed responses\nand\n$X = {x_1, x_2, ..., x_n}$\nthe unknown response values to be fitted\nfinding a function that minimizes\n\\begin{equation}\n  f(x) = \\sum_", "question": "What does isotonic regression belong to?", "answers": {"text": ["belongs to the family of regression algorithms."], "answer_start": [473]}}
{"context": "ed responses\nand\n$X = {x_1, x_2, ..., x_n}$\nthe unknown response values to be fitted\nfinding a function that minimizes\n\\begin{equation}\n  f(x) = \\sum_{i=1}^n w_i (y_i - x_i)^2\n\\end{equation}\nwith respect to complete order subject to\n$x_1\\le x_2\\le ...\\le x_n$\nwhere\n$w_i$\nare positive weights.\nThe resulting function is called isotonic regression and it is unique.\nIt can be viewed as least squares problem under order restriction.\nEssentially isotonic regression is a\nmonotonic function\nbest fitting the original data points.\nspark.mllib\nsupports a\npool adjacent violators algorithm\nwhich uses an approach to\nparallelizing isotonic regression\n.\nThe training input is an RDD of tuples of three double values that represent\nlabel, feature and weight in this order. In case there are multiple tuples wi", "question": "What is the resulting function called when minimizing f(x) with respect to complete order subject to x_1 ≤ x_2 ≤ ... ≤ x_n?", "answers": {"text": ["The resulting function is called isotonic regression and it is unique."], "answer_start": [294]}}
{"context": "training input is an RDD of tuples of three double values that represent\nlabel, feature and weight in this order. In case there are multiple tuples with\nthe same feature then these tuples are aggregated into a single tuple as follows:\nAggregated label is the weighted average of all labels.\nAggregated feature is the unique feature value.\nAggregated weight is the sum of all weights.\nAdditionally, IsotonicRegression algorithm has one\noptional parameter called $isotonic$ defaulting to true.\nThis argument specifies if the isotonic regression is\nisotonic (monotonically increasing) or antitonic (monotonically decreasing).\nTraining returns an IsotonicRegressionModel that can be used to predict\nlabels for both known and unknown features. The result of isotonic regression\nis treated as piecewise lin", "question": "How are multiple tuples with the same feature aggregated?", "answers": {"text": ["Aggregated label is the weighted average of all labels.\nAggregated feature is the unique feature value.\nAggregated weight is the sum of all weights."], "answer_start": [235]}}
{"context": "xamples\nData are read from a file where each line has a format label,feature\ni.e. 4710.28,500.00. The data are split to training and testing set.\nModel is created using the training set and a mean squared error is calculated from the predicted\nlabels and real labels in the test set.\nRefer to the\nIsotonicRegression\nPython docs\nand\nIsotonicRegressionModel\nPython docs\nfor more details on the API.\nimport\nmath\nfrom\npyspark.mllib.regression\nimport\nIsotonicRegression\n,\nIsotonicRegressionModel\nfrom\npyspark.mllib.util\nimport\nMLUtils\n# Load and parse the data\ndef\nparsePoint\n(\nlabeledData\n):\nreturn\n(\nlabeledData\n.\nlabel\n,\nlabeledData\n.\nfeatures\n[\n0\n],\n1.0\n)\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"\ndata/mllib/sample_isotonic_regression_libsvm_data.txt\n\"\n)\n# Create label, feature, weight tuples from in", "question": "From which file are the data loaded?", "answers": {"text": ["data/mllib/sample_isotonic_regression_libsvm_data.txt"], "answer_start": [696]}}
{"context": "]),\np\n[\n0\n]))\n# Calculate mean squared error between predicted and real labels.\nmeanSquaredError\n=\npredictionAndLabel\n.\nmap\n(\nlambda\npl\n:\nmath\n.\npow\n((\npl\n[\n0\n]\n-\npl\n[\n1\n]),\n2\n)).\nmean\n()\nprint\n(\n\"\nMean Squared Error =\n\"\n+\nstr\n(\nmeanSquaredError\n))\n# Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"\ntarget/tmp/myIsotonicRegressionModel\n\"\n)\nsameModel\n=\nIsotonicRegressionModel\n.\nload\n(\nsc\n,\n\"\ntarget/tmp/myIsotonicRegressionModel\n\"\n)\nFind full example code at \"examples/src/main/python/mllib/isotonic_regression_example.py\" in the Spark repo.\nData are read from a file where each line has a format label,feature\ni.e. 4710.28,500.00. The data are split to training and testing set.\nModel is created using the training set and a mean squared error is calculated from the predicted\nlabels and real labels in th", "question": "Where can I find the full example code for isotonic regression?", "answers": {"text": ["Find full example code at \"examples/src/main/python/mllib/isotonic_regression_example.py\" in the Spark repo."], "answer_start": [427]}}
{"context": "=>\n(\nlabeledPoint\n.\nlabel\n,\nlabeledPoint\n.\nfeatures\n(\n0\n),\n1.0\n)\n}\n// Split data into training (60%) and test (40%) sets.\nval\nsplits\n=\nparsedData\n.\nrandomSplit\n(\nArray\n(\n0.6\n,\n0.4\n),\nseed\n=\n11L\n)\nval\ntraining\n=\nsplits\n(\n0\n)\nval\ntest\n=\nsplits\n(\n1\n)\n// Create isotonic regression model from training data.\n// Isotonic parameter defaults to true so it is only shown for demonstration\nval\nmodel\n=\nnew\nIsotonicRegression\n().\nsetIsotonic\n(\ntrue\n).\nrun\n(\ntraining\n)\n// Create tuples of predicted and real labels.\nval\npredictionAndLabel\n=\ntest\n.\nmap\n{\npoint\n=>\nval\npredictedLabel\n=\nmodel\n.\npredict\n(\npoint\n.\n_2\n)\n(\npredictedLabel\n,\npoint\n.\n_1\n)\n}\n// Calculate mean squared error between predicted and real labels.\nval\nmeanSquaredError\n=\npredictionAndLabel\n.\nmap\n{\ncase\n(\np\n,\nl\n)\n=>\nmath\n.\npow\n((\np\n-\nl\n),\n2\n)", "question": "What is the purpose of the line `val splits = parsedData.randomSplit(Array(0.6, 0.4), seed = 11L)`?", "answers": {"text": ["Split data into training (60%) and test (40%) sets."], "answer_start": [70]}}
{"context": "e mean squared error between predicted and real labels.\nval\nmeanSquaredError\n=\npredictionAndLabel\n.\nmap\n{\ncase\n(\np\n,\nl\n)\n=>\nmath\n.\npow\n((\np\n-\nl\n),\n2\n)\n}.\nmean\n()\nprintln\n(\ns\n\"Mean Squared Error = $meanSquaredError\"\n)\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/myIsotonicRegressionModel\"\n)\nval\nsameModel\n=\nIsotonicRegressionModel\n.\nload\n(\nsc\n,\n\"target/tmp/myIsotonicRegressionModel\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\" in the Spark repo.\nData are read from a file where each line has a format label,feature\ni.e. 4710.28,500.00. The data are split to training and testing set.\nModel is created using the training set and a mean squared error is calculated from the predicted\nlabels and real labels in the tes", "question": "Where can I find the full example code for Isotonic Regression?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\" in the Spark repo."], "answer_start": [396]}}
{"context": "double\ndiff\n=\npl\n.\n_1\n()\n-\npl\n.\n_2\n();\nreturn\ndiff\n*\ndiff\n;\n}).\nmean\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Mean Squared Error = \"\n+\nmeanSquaredError\n);\n// Save and load model\nmodel\n.\nsave\n(\njsc\n.\nsc\n(),\n\"target/tmp/myIsotonicRegressionModel\"\n);\nIsotonicRegressionModel\nsameModel\n=\nIsotonicRegressionModel\n.\nload\n(\njsc\n.\nsc\n(),\n\"target/tmp/myIsotonicRegressionModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\" in the Spark repo.", "question": "Where can I find the full example code for JavaIsotonicRegressionExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\" in the Spark repo."], "answer_start": [361]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTrou", "question": "Which file formats are mentioned as data sources in the Spark SQL Guide?", "answers": {"text": ["Parquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files"], "answer_start": [650]}}
{"context": "ng\nThe\nspark-protobuf\nmodule is external and not included in\nspark-submit\nor\nspark-shell\nby default.\nAs with any Spark applications,\nspark-submit\nis used to launch your application.\nspark-protobuf_2.13\nand its dependencies can be directly added to\nspark-submit\nusing\n--packages\n, such as,\n./bin/spark-submit --packages org.apache.spark:spark-protobuf_2.13:4.0.0 ...\nFor experimenting on\nspark-shell\n, you can also use\n--packages\nto add\norg.apache.spark:spark-protobuf_2.13\nand its dependencies directly,\n./bin/spark-shell --packages org.apache.spark:spark-protobuf_2.13:4.0.0 ...\nSee\nApplication Submission Guide\nfor more details about submitting applications with external dependencies.\nto_protobuf() and from_protobuf()\nThe spark-protobuf package provides function\nto_protobuf\nto encode a column as", "question": "How can you add spark-protobuf_2.13 and its dependencies to spark-submit?", "answers": {"text": ["./bin/spark-submit --packages org.apache.spark:spark-protobuf_2.13:4.0.0 ..."], "answer_start": [289]}}
{"context": "ains your data is in protobuf, you could use\nfrom_protobuf()\nto extract your data, enrich it, clean it, and then push it downstream to Kafka again or write it out to a different sink.\nto_protobuf()\ncan be used to turn structs into protobuf message. This method is particularly useful when you would like to re-encode multiple columns into a single one when writing data out to Kafka.\nSpark SQL schema is generated based on the protobuf descriptor file or protobuf class passed to\nfrom_protobuf\nand\nto_protobuf\n. The specified protobuf class or protobuf descriptor file must match the data, otherwise, the behavior is undefined: it may fail or return arbitrary results.\nThis div is only used to make markdown editor/viewer happy and does not display on web\n\n```python\nfrom\npyspark.sql.protobuf.functio", "question": "What happens if the specified protobuf class or descriptor file does not match the data?", "answers": {"text": ["otherwise, the behavior is undefined: it may fail or return arbitrary results."], "answer_start": [590]}}
{"context": "arbitrary results.\nThis div is only used to make markdown editor/viewer happy and does not display on web\n\n```python\nfrom\npyspark.sql.protobuf.functions\nimport\nfrom_protobuf\n,\nto_protobuf\n# from_protobuf and to_protobuf provide two schema choices. Via Protobuf descriptor file,\n# or via shaded Java class.\n# give input .proto protobuf schema\n# syntax = \"proto3\"\n# message AppEvent {\n#   string name = 1;\n#   int64 id = 2;\n#   string context = 3;\n# }\ndf\n=\nspark\n.\nreadStream\n.\nformat\n(\n\"\nkafka\n\"\n)\n\\\n.\noption\n(\n\"\nkafka.bootstrap.servers\n\"\n,\n\"\nhost1:port1,host2:port2\n\"\n)\n.\noption\n(\n\"\nsubscribe\n\"\n,\n\"\ntopic1\n\"\n)\n.\nload\n()\n# 1. Decode the Protobuf data of schema `AppEvent` into a struct;\n# 2. Filter by column `name`;\n# 3. Encode the column `event` in Protobuf format.\n# The Protobuf protoc command can", "question": "What do `from_protobuf` and `to_protobuf` provide?", "answers": {"text": ["from_protobuf and to_protobuf provide two schema choices."], "answer_start": [190]}}
{"context": " id = 2;\n//   string context = 3;\n// }\nval\ndf\n=\nspark\n.\nreadStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"\n)\n.\noption\n(\n\"subscribe\"\n,\n\"topic1\"\n)\n.\nload\n()\n// 1. Decode the Protobuf data of schema `AppEvent` into a struct;\n// 2. Filter by column `name`;\n// 3. Encode the column `event` in Protobuf format.\n// The Protobuf protoc command can be used to generate a protobuf descriptor file for give .proto file.\nval\noutput\n=\ndf\n.\nselect\n(\nfrom_protobuf\n(\n$\n\"value\"\n,\n\"AppEvent\"\n,\ndescriptorFilePath\n)\nas\n$\n\"event\"\n)\n.\nwhere\n(\n\"event.name == \\\"alice\\\"\"\n)\n.\nselect\n(\nto_protobuf\n(\n$\n\"user\"\n,\n\"AppEvent\"\n,\ndescriptorFilePath\n)\nas\n$\n\"event\"\n)\nval\nquery\n=\noutput\n.\nwriteStream\n.\nformat\n(\n\"kafka\"\n)\n.\noption\n(\n\"kafka.bootstrap.servers\"\n,\n\"host1:port1,host2:port2\"", "question": "What Kafka bootstrap servers are being used in the provided code?", "answers": {"text": ["\"host1:port1,host2:port2\""], "answer_start": [127]}}
{"context": " 1. Decode the Protobuf data of schema `AppEvent` into a struct;\n// 2. Filter by column `name`;\n// 3. Encode the column `event` in Protobuf format.\n// The Protobuf protoc command can be used to generate a protobuf descriptor file for give .proto file.\nDataset\n<\nRow\n>\noutput\n=\ndf\n.\nselect\n(\nfrom_protobuf\n(\ncol\n(\n\"value\"\n),\n\"AppEvent\"\n,\ndescriptorFilePath\n).\nas\n(\n\"event\"\n))\n.\nwhere\n(\n\"event.name == \\\"alice\\\"\"\n)\n.\nselect\n(\nto_protobuf\n(\ncol\n(\n\"event\"\n),\n\"AppEvent\"\n,\ndescriptorFilePath\n).\nas\n(\n\"event\"\n));\n// Alternatively, you can decode and encode the SQL columns into protobuf format using protobuf\n// class name. The specified Protobuf class must match the data, otherwise the behavior is undefined:\n// it may fail or return arbitrary result. To avoid conflicts, the jar file containing the\n// '", "question": "What is used to generate a protobuf descriptor file for a given .proto file?", "answers": {"text": ["The Protobuf protoc command can be used to generate a protobuf descriptor file for give .proto file."], "answer_start": [151]}}
{"context": "t match the data, otherwise the behavior is undefined:\n// it may fail or return arbitrary result. To avoid conflicts, the jar file containing the\n// 'com.google.protobuf.*' classes should be shaded. An example of shading can be found at\n// https://github.com/rangadi/shaded-protobuf-classes.\nDataset\n<\nRow\n>\noutput\n=\ndf\n.\nselect\n(\nfrom_protobuf\n(\ncol\n(\n\"value\"\n),\n\"org.sparkproject.spark_protobuf.protobuf.AppEvent\"\n).\nas\n(\n\"event\"\n))\n.\nwhere\n(\n\"event.name == \\\"alice\\\"\"\n)\noutput\n.\nprintSchema\n()\n// root\n//  |--event: struct (nullable = true)\n//  |    |-- name : string (nullable = true)\n//  |    |-- id: long (nullable = true)\n//  |    |-- context: string (nullable = true)\noutput\n=\noutput\n.\nselect\n(\nto_protobuf\n(\ncol\n(\n\"event\"\n),\n\"org.sparkproject.spark_protobuf.protobuf.AppEvent\"\n).\nas\n(\n\"event", "question": "What should be done to avoid conflicts with 'com.google.protobuf.*' classes?", "answers": {"text": ["the jar file containing the\n// 'com.google.protobuf.*' classes should be shaded."], "answer_start": [118]}}
{"context": "s that can have multiple possible sets of fields, but only one set can be present at a time. This is useful for situations where the data you are working with is not always in the same format, and you need to be able to handle messages with different sets of fields without encountering errors.\nProtobuf type\nSpark SQL type\nboolean\nBooleanType\nint\nIntegerType\nlong\nLongType\nfloat\nFloatType\ndouble\nDoubleType\nstring\nStringType\nenum\nStringType\nbytes\nBinaryType\nMessage\nStructType\nrepeated\nArrayType\nmap\nMapType\nOneOf\nStruct\nIt also supports reading the following Protobuf types\nTimestamp\nand\nDuration\nProtobuf logical type\nProtobuf schema\nSpark SQL type\nduration\nMessageType{seconds: Long, nanos: Int}\nDayTimeIntervalType\ntimestamp\nMessageType{seconds: Long, nanos: Int}\nTimestampType\nSupported types f", "question": "What Spark SQL type corresponds to the Protobuf type 'long'?", "answers": {"text": ["LongType"], "answer_start": [365]}}
{"context": "e\nduration\nMessageType{seconds: Long, nanos: Int}\nDayTimeIntervalType\ntimestamp\nMessageType{seconds: Long, nanos: Int}\nTimestampType\nSupported types for Spark SQL -> Protobuf conversion\nSpark supports the writing of all Spark SQL types into Protobuf. For most types, the mapping from Spark types to Protobuf types is straightforward (e.g. IntegerType gets converted to int);\nSpark SQL type\nProtobuf type\nBooleanType\nboolean\nIntegerType\nint\nLongType\nlong\nFloatType\nfloat\nDoubleType\ndouble\nStringType\nstring\nStringType\nenum\nBinaryType\nbytes\nStructType\nmessage\nArrayType\nrepeated\nMapType\nmap\nHandling circular references protobuf fields\nOne common issue that can arise when working with Protobuf data is the presence of circular references. In Protobuf, a circular reference occurs when a field refers b", "question": "What type does Spark SQL's IntegerType get converted to in Protobuf?", "answers": {"text": ["int"], "answer_start": [236]}}
{"context": "n Schema. Since Spark doesn't allow writing empty\nStructType\n, the empty proto message type will be dropped by default. Setting this option to\ntrue\nwill insert a dummy column(\n__dummy_field_in_empty_struct\n) to the empty proto message so that the empty message fields will be retained.\nread", "question": "What happens when Spark encounters an empty StructType?", "answers": {"text": ["the empty proto message type will be dropped by default."], "answer_start": [63]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nGeneric Load/Save Functions\nGeneric File Source Options\nParquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTrou", "question": "Which file formats are mentioned as data sources in the Spark SQL Guide?", "answers": {"text": ["Parquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files"], "answer_start": [650]}}
{"context": "Parquet Files\nORC Files\nJSON Files\nCSV Files\nText Files\nXML Files\nHive Tables\nJDBC To Other Databases\nAvro Files\nProtobuf data\nWhole Binary Files\nTroubleshooting\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nJDBC To Other Databases\nData Source Option\nData Type Mapping\nMapping Spark SQL Data Types from MySQL\nMapping Spark SQL Data Types to MySQL\nMapping Spark SQL Data Types from PostgreSQL\nMapping Spark SQL Data Types to PostgreSQL\nMapping Spark SQL Data Types from Oracle\nMapping Spark SQL Data Types to Oracle\nMapping Spark SQL Data Types from Microsoft SQL Server\nMapping Spark SQL Data Types to Microsoft SQL Server\nMapping Spark SQL Data Types from DB2\nMapping Spark SQL Data Types to DB2\nMapping Spa", "question": "What data sources are supported for mapping Spark SQL data types?", "answers": {"text": ["Mapping Spark SQL Data Types from MySQL\nMapping Spark SQL Data Types to MySQL\nMapping Spark SQL Data Types from PostgreSQL\nMapping Spark SQL Data Types to PostgreSQL\nMapping Spark SQL Data Types from Oracle\nMapping Spark SQL Data Types to Oracle\nMapping Spark SQL Data Types from Microsoft SQL Server\nMapping Spark SQL Data Types to Microsoft SQL Server\nMapping Spark SQL Data Types from DB2\nMapping Spark SQL Data Types to DB2"], "answer_start": [361]}}
{"context": "ferent than the Spark SQL JDBC server, which allows other applications to\nrun queries using Spark SQL).\nTo get started you will need to include the JDBC driver for your particular database on the\nspark classpath. For example, to connect to postgres from the Spark Shell you would run the\nfollowing command:\n./bin/spark-shell\n--driver-class-path\npostgresql-9.4.1207.jar\n--jars\npostgresql-9.4.1207.jar\nData Source Option\nSpark supports the following case-insensitive options for JDBC. The Data source options of JDBC can be set via:\nthe\n.option\n/\n.options\nmethods of\nDataFrameReader\nDataFrameWriter\nOPTIONS\nclause at\nCREATE TABLE USING DATA_SOURCE\nFor connection properties, users can specify the JDBC connection properties in the data source options.\nuser\nand\npassword\nare normally provided as connect", "question": "How can JDBC connection properties be specified in Spark?", "answers": {"text": ["For connection properties, users can specify the JDBC connection properties in the data source options."], "answer_start": [646]}}
{"context": "connection properties, users can specify the JDBC connection properties in the data source options.\nuser\nand\npassword\nare normally provided as connection properties for\nlogging into the data sources.\nProperty Name\nDefault\nMeaning\nScope\nurl\n(none)\nThe JDBC URL of the form\njdbc:subprotocol:subname\nto connect to. The source-specific connection properties may be specified in the URL. e.g.,\njdbc:postgresql://localhost/test?user=fred&password=secret\nread/write\ndbtable\n(none)\nThe JDBC table that should be read from or written into. Note that when using it in the read\n      path anything that is valid in a\nFROM\nclause of a SQL query can be used.\n      For example, instead of a full table you could also use a subquery in parentheses. It is not\n      allowed to specify\ndbtable\nand\nquery\noptions at t", "question": "What is the format of the JDBC URL?", "answers": {"text": ["jdbc:subprotocol:subname"], "answer_start": [272]}}
{"context": "ble to split such a query to\nprepareQuery\nand\nquery\n:\nspark.read.format(\"jdbc\")\n.option(\"url\", jdbcUrl)\n.option(\"prepareQuery\", \"(SELECT * INTO #TempTable FROM (SELECT * FROM tbl) t)\")\n.option(\"query\", \"SELECT * FROM #TempTable\")\n.load()\nread/write\ndriver\n(none)\nThe class name of the JDBC driver to use to connect to this URL.\nread/write\npartitionColumn, lowerBound, upperBound\n(none)\nThese options must all be specified if any of them is specified. In addition,\nnumPartitions\nmust be specified. They describe how to partition the table when\n      reading in parallel from multiple workers.\npartitionColumn\nmust be a numeric, date, or timestamp column from the table in question.\n      Notice that\nlowerBound\nand\nupperBound\nare just used to decide the\n      partition stride, not for filtering the r", "question": "What is the prepareQuery option set to in the provided Spark code?", "answers": {"text": ["(SELECT * INTO #TempTable FROM (SELECT * FROM tbl) t)"], "answer_start": [129]}}
{"context": "mn from the table in question.\n      Notice that\nlowerBound\nand\nupperBound\nare just used to decide the\n      partition stride, not for filtering the rows in table. So all rows in the table will be\n      partitioned and returned. This option applies only to reading.\nExample:\nspark.read.format(\"jdbc\")\n.option(\"url\", jdbcUrl)\n.option(\"dbtable\", \"(select c1, c2 from t1) as subq\")\n.option(\"partitionColumn\", \"c1\")\n.option(\"lowerBound\", \"1\")\n.option(\"upperBound\", \"100\")\n.option(\"numPartitions\", \"3\")\n.load()\nread\nnumPartitions\n(none)\nThe maximum number of partitions that can be used for parallelism in table reading and\n      writing. This also determines the maximum number of concurrent JDBC connections.\n      If the number of partitions to write exceeds this limit, we decrease it to this limit by", "question": "What determines the maximum number of concurrent JDBC connections?", "answers": {"text": ["This also determines the maximum number of concurrent JDBC connections."], "answer_start": [634]}}
{"context": "ines the maximum number of concurrent JDBC connections.\n      If the number of partitions to write exceeds this limit, we decrease it to this limit by\n      calling\ncoalesce(numPartitions)\nbefore writing.\nread/write\nqueryTimeout\n0\nThe number of seconds the driver will wait for a Statement object to execute to the given\n      number of seconds. Zero means there is no limit. In the write path, this option depends on\n      how JDBC drivers implement the API\nsetQueryTimeout\n, e.g., the h2 JDBC driver\n      checks the timeout of each query instead of an entire JDBC batch.\nread/write\nfetchsize\n0\nThe JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (e.g. Oracle with 10 rows).\nread\nbatchsize\n1000\nThe ", "question": "What does a queryTimeout of zero signify?", "answers": {"text": ["Zero means there is no limit."], "answer_start": [346]}}
{"context": "o fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (e.g. Oracle with 10 rows).\nread\nbatchsize\n1000\nThe JDBC batch size, which determines how many rows to insert per round trip. This can help performance on JDBC drivers. This option applies only to writing.\nwrite\nisolationLevel\nREAD_UNCOMMITTED\nThe transaction isolation level, which applies to current connection. It can be one of\nNONE\n,\nREAD_COMMITTED\n,\nREAD_UNCOMMITTED\n,\nREPEATABLE_READ\n, or\nSERIALIZABLE\n, corresponding to standard transaction isolation levels defined by JDBC's Connection object, with default of\nREAD_UNCOMMITTED\n. Please refer the documentation in\njava.sql.Connection\n.\nwrite\nsessionInitStatement\n(none)\nAfter each database session is opened to the remote DB and before starting ", "question": "What is the default transaction isolation level?", "answers": {"text": ["READ_UNCOMMITTED"], "answer_start": [325]}}
{"context": "e documentation in\njava.sql.Connection\n.\nwrite\nsessionInitStatement\n(none)\nAfter each database session is opened to the remote DB and before starting to read data, this option executes a custom SQL statement (or a PL/SQL block). Use this to implement session initialization code. Example:\noption(\"sessionInitStatement\", \"\"\"BEGIN execute immediate 'alter session set \"_serial_direct_read\"=true'; END;\"\"\")\nread\ntruncate\nfalse\nThis is a JDBC writer related option. When\nSaveMode.Overwrite\nis enabled, this option causes Spark to truncate an existing table instead of dropping and recreating it. This can be more efficient, and prevents the table metadata (e.g., indices) from being removed. However, it will not work in some cases, such as when the new data has a different schema. In case of failures, ", "question": "What does the 'sessionInitStatement' option do?", "answers": {"text": ["After each database session is opened to the remote DB and before starting to read data, this option executes a custom SQL statement (or a PL/SQL block). Use this to implement session initialization code."], "answer_start": [75]}}
{"context": "a (e.g., indices) from being removed. However, it will not work in some cases, such as when the new data has a different schema. In case of failures, users should turn off\ntruncate\noption to use\nDROP TABLE\nagain. Also, due to the different behavior of\nTRUNCATE TABLE\namong DBMSes, it's not always safe to use this. MySQLDialect, DB2Dialect, MsSqlServerDialect, DerbyDialect, and OracleDialect supports this while PostgresDialect and default JDBCDialect doesn't. For unknown and unsupported JDBCDialect, the user option\ntruncate\nis ignored.\nwrite\ncascadeTruncate\nthe default cascading truncate behaviour of the JDBC database in question, specified in the\nisCascadeTruncate\nin each JDBCDialect\nThis is a JDBC writer related option. If enabled and supported by the JDBC database (PostgreSQL and Oracle a", "question": "Which dialects support truncate?", "answers": {"text": ["MySQLDialect, DB2Dialect, MsSqlServerDialect, DerbyDialect, and OracleDialect supports this while PostgresDialect and default JDBCDialect doesn't."], "answer_start": [315]}}
{"context": "the\nisCascadeTruncate\nin each JDBCDialect\nThis is a JDBC writer related option. If enabled and supported by the JDBC database (PostgreSQL and Oracle at the moment), this options allows execution of a\nTRUNCATE TABLE t CASCADE\n(in the case of PostgreSQL a\nTRUNCATE TABLE ONLY t CASCADE\nis executed to prevent inadvertently truncating descendant tables). This will affect other tables, and thus should be used with care.\nwrite\ncreateTableOptions\nThis is a JDBC writer related option. If specified, this option allows setting of database-specific table and partition options when creating a table (e.g.,\nCREATE TABLE t (name string) ENGINE=InnoDB.\n).\nwrite\ncreateTableColumnTypes\n(none)\nThe database column data types to use instead of the defaults, when creating the table. Data type information should ", "question": "What does the isCascadeTruncate option allow execution of when enabled and supported by the JDBC database?", "answers": {"text": ["TRUNCATE TABLE t CASCADE"], "answer_start": [200]}}
{"context": "te\ncreateTableColumnTypes\n(none)\nThe database column data types to use instead of the defaults, when creating the table. Data type information should be specified in the same format as CREATE TABLE columns syntax (e.g:\n\"name CHAR(64), comments VARCHAR(1024)\")\n. The specified types should be valid spark sql data types.\nwrite\ncustomSchema\n(none)\nThe custom schema to use for reading data from JDBC connectors. For example,\n\"id DECIMAL(38, 0), name STRING\"\n. You can also specify partial fields, and the others use the default type mapping. For example,\n\"id DECIMAL(38, 0)\"\n. The column names should be identical to the corresponding column names of JDBC table. Users can specify the corresponding data types of Spark SQL instead of using the defaults.\nread\npushDownPredicate\ntrue\nThe option to enable", "question": "What format should data type information be specified in when creating a table?", "answers": {"text": ["\"name CHAR(64), comments VARCHAR(1024)\""], "answer_start": [219]}}
{"context": "DBC table. Users can specify the corresponding data types of Spark SQL instead of using the defaults.\nread\npushDownPredicate\ntrue\nThe option to enable or disable predicate push-down into the JDBC data source. The default value is true, in which case Spark will push down filters to the JDBC data source as much as possible. Otherwise, if set to false, no filter will be pushed down to the JDBC data source and thus all filters will be handled by Spark. Predicate push-down is usually turned off when the predicate filtering is performed faster by Spark than by the JDBC data source.\nread\npushDownAggregate\ntrue\nThe option to enable or disable aggregate push-down in V2 JDBC data source. The default value is true, in which case Spark will push down aggregates to the JDBC data source. Otherwise, if s", "question": "What happens when 'pushDownPredicate' is set to false?", "answers": {"text": ["Otherwise, if set to false, no filter will be pushed down to the JDBC data source and thus all filters will be handled by Spark."], "answer_start": [324]}}
{"context": "te push-down in V2 JDBC data source. The default value is true, in which case Spark will push down aggregates to the JDBC data source. Otherwise, if sets to false, aggregates will not be pushed down to the JDBC data source. Aggregate push-down is usually turned off when the aggregate is performed faster by Spark than by the JDBC data source. Please note that aggregates can be pushed down if and only if all the aggregate functions and the related filters can be pushed down. If\nnumPartitions\nequals to 1 or the group by key is the same as\npartitionColumn\n, Spark will push down aggregate to data source completely and not apply a final aggregate over the data source output. Otherwise, Spark will apply a final aggregate over the data source output.\nread\npushDownLimit\ntrue\nThe option to enable or", "question": "What happens when aggregate push-down is turned off?", "answers": {"text": ["Otherwise, if sets to false, aggregates will not be pushed down to the JDBC data source."], "answer_start": [135]}}
{"context": "ver the data source output. Otherwise, Spark will apply a final aggregate over the data source output.\nread\npushDownLimit\ntrue\nThe option to enable or disable LIMIT push-down into V2 JDBC data source. The LIMIT push-down also includes LIMIT + SORT , a.k.a. the Top N operator. The default value is true, in which case Spark push down LIMIT or LIMIT with SORT to the JDBC data source. Otherwise, if sets to false, LIMIT or LIMIT with SORT is not pushed down to the JDBC data source. If\nnumPartitions\nis greater than 1, Spark still applies LIMIT or LIMIT with SORT on the result from data source even if LIMIT or LIMIT with SORT is pushed down. Otherwise, if LIMIT or LIMIT with SORT is pushed down and\nnumPartitions\nequals to 1, Spark will not apply LIMIT or LIMIT with SORT on the result from data so", "question": "What is the default value for the `pushDownLimit` option?", "answers": {"text": ["The default value is true, in which case Spark push down LIMIT or LIMIT with SORT to the JDBC data source."], "answer_start": [277]}}
{"context": " source.\nread\npushDownTableSample\ntrue\nThe option to enable or disable TABLESAMPLE push-down into V2 JDBC data source. The default value is true, in which case Spark push down TABLESAMPLE to the JDBC data source. Otherwise, if value sets to false, TABLESAMPLE is not pushed down to the JDBC data source.\nread\nkeytab\n(none)\nLocation of the kerberos keytab file (which must be pre-uploaded to all nodes either by\n--files\noption of spark-submit or manually) for the JDBC client. When path information found then Spark considers the keytab distributed manually, otherwise\n--files\nassumed. If both\nkeytab\nand\nprincipal\nare defined then Spark tries to do kerberos authentication.\nread/write\nprincipal\n(none)\nSpecifies kerberos principal name for the JDBC client. If both\nkeytab\nand\nprincipal\nare defined th", "question": "What is the default value for the TABLESAMPLE push-down option into V2 JDBC data source?", "answers": {"text": ["The default value is true, in which case Spark push down TABLESAMPLE to the JDBC data source."], "answer_start": [119]}}
{"context": "e provider can handle\n      the specified driver and options. The selected provider must not be disabled by\nspark.sql.sources.disabledJdbcConnProviderList\n.\nread/write\npreferTimestampNTZ\nfalse\nWhen the option is set to\ntrue\n, TIMESTAMP WITHOUT TIME ZONE type is inferred as Spark's TimestampNTZ type.\n      Otherwise, it is interpreted as Spark's Timestamp type(equivalent to TIMESTAMP WITH LOCAL TIME ZONE).\n      This setting specifically affects only the inference of TIMESTAMP WITHOUT TIME ZONE data type. Both TIMESTAMP WITH LOCAL TIME ZONE and TIMESTAMP WITH TIME ZONE data types are consistently interpreted as Spark's Timestamp type regardless of this setting.\nread\nhint\n(none)\nThis option is used to specify the hint for reading. The supported hint format is a variant of C-style comments: i", "question": "How is TIMESTAMP WITHOUT TIME ZONE type inferred when the preferTimestampNTZ option is set to true?", "answers": {"text": ["TIMESTAMP WITHOUT TIME ZONE type is inferred as Spark's TimestampNTZ type."], "answer_start": [226]}}
{"context": "s of this setting.\nread\nhint\n(none)\nThis option is used to specify the hint for reading. The supported hint format is a variant of C-style comments: it needs to start with `/*+ ` and end with ` */`. Currently, this option is only supported in MySQLDialect, OracleDialect and DatabricksDialect.\nread\nNote that kerberos authentication with keytab is not always supported by the JDBC driver.\nBefore using\nkeytab\nand\nprincipal\nconfiguration options, please make sure the following requirements are met:\nThe included JDBC driver version supports kerberos authentication with keytab.\nThere is a built-in connection provider which supports the used database.\nThere is a built-in connection providers for the following databases:\nDB2\nMariaDB\nMS Sql\nOracle\nPostgreSQL\nIf the requirements are not met, please c", "question": "In which dialects is the read hint option currently supported?", "answers": {"text": ["Currently, this option is only supported in MySQLDialect, OracleDialect and DatabricksDialect."], "answer_start": [199]}}
{"context": "gresql:dbserver\n\"\n,\n\"\nschema.tablename\n\"\n,\nproperties\n=\n{\n\"\nuser\n\"\n:\n\"\nusername\n\"\n,\n\"\npassword\n\"\n:\n\"\npassword\n\"\n})\n# Specifying dataframe column data types on read\njdbcDF3\n=\nspark\n.\nread\n\\\n.\nformat\n(\n\"\njdbc\n\"\n)\n\\\n.\noption\n(\n\"\nurl\n\"\n,\n\"\njdbc:postgresql:dbserver\n\"\n)\n\\\n.\noption\n(\n\"\ndbtable\n\"\n,\n\"\nschema.tablename\n\"\n)\n\\\n.\noption\n(\n\"\nuser\n\"\n,\n\"\nusername\n\"\n)\n\\\n.\noption\n(\n\"\npassword\n\"\n,\n\"\npassword\n\"\n)\n\\\n.\noption\n(\n\"\ncustomSchema\n\"\n,\n\"\nid DECIMAL(38, 0), name STRING\n\"\n)\n\\\n.\nload\n()\n# Saving data to a JDBC source\njdbcDF\n.\nwrite\n\\\n.\nformat\n(\n\"\njdbc\n\"\n)\n\\\n.\noption\n(\n\"\nurl\n\"\n,\n\"\njdbc:postgresql:dbserver\n\"\n)\n\\\n.\noption\n(\n\"\ndbtable\n\"\n,\n\"\nschema.tablename\n\"\n)\n\\\n.\noption\n(\n\"\nuser\n\"\n,\n\"\nusername\n\"\n)\n\\\n.\noption\n(\n\"\npassword\n\"\n,\n\"\npassword\n\"\n)\n\\\n.\nsave\n()\njdbcDF2\n.\nwrite\n\\\n.\njdbc\n(\n\"\njdbc:postgresql:dbserver\n", "question": "What URL is used to connect to the PostgreSQL database?", "answers": {"text": ["jdbc:postgresql:dbserver"], "answer_start": [236]}}
{"context": "CHAR(1024)\"\n)\n.\njdbc\n(\n\"jdbc:postgresql:dbserver\"\n,\n\"schema.tablename\"\n,\nconnectionProperties\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\" in the Spark repo.\n// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods\n// Loading data from a JDBC source\nDataset\n<\nRow\n>\njdbcDF\n=\nspark\n.\nread\n()\n.\nformat\n(\n\"jdbc\"\n)\n.\noption\n(\n\"url\"\n,\n\"jdbc:postgresql:dbserver\"\n)\n.\noption\n(\n\"dbtable\"\n,\n\"schema.tablename\"\n)\n.\noption\n(\n\"user\"\n,\n\"username\"\n)\n.\noption\n(\n\"password\"\n,\n\"password\"\n)\n.\nload\n();\nProperties\nconnectionProperties\n=\nnew\nProperties\n();\nconnectionProperties\n.\nput\n(\n\"user\"\n,\n\"username\"\n);\nconnectionProperties\n.\nput\n(\n\"password\"\n,\n\"password\"\n);\nDataset\n<\nRow\n>\njdbcDF2\n=\nspark\n.\nread\n()\n.\njdbc\n(\n\"jdbc", "question": "What is the URL used when reading data from a JDBC source?", "answers": {"text": ["jdbc:postgresql:dbserver"], "answer_start": [24]}}
{"context": "eateTableColumnTypes\"\n,\n\"name CHAR(64), comments VARCHAR(1024)\"\n)\n.\njdbc\n(\n\"jdbc:postgresql:dbserver\"\n,\n\"schema.tablename\"\n,\nconnectionProperties\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo.\n# Loading data from a JDBC source\ndf\n<-\nread.jdbc\n(\n\"jdbc:postgresql:dbserver\"\n,\n\"schema.tablename\"\n,\nuser\n=\n\"username\"\n,\npassword\n=\n\"password\"\n)\n# Saving data to a JDBC source\nwrite.jdbc\n(\ndf\n,\n\"jdbc:postgresql:dbserver\"\n,\n\"schema.tablename\"\n,\nuser\n=\n\"username\"\n,\npassword\n=\n\"password\"\n)\nFind full example code at \"examples/src/main/r/RSparkSQLExample.R\" in the Spark repo.\nCREATE\nTEMPORARY\nVIEW\njdbcTable\nUSING\norg\n.\napache\n.\nspark\n.\nsql\n.\njdbc\nOPTIONS\n(\nurl\n\"jdbc:postgresql:dbserver\"\n,\ndbtable\n\"schema.tablename\"\n,\nus", "question": "Where can I find full example code for JavaSQLDataSource?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\" in the Spark repo."], "answer_start": [149]}}
{"context": "nt mapping rules.\nMySQL Data Type\nSpark SQL Data Type\nRemarks\nBIT(1)\nBooleanType\nBIT( >1 )\nBinaryType\n(Default)\nBIT( >1 )\nLongType\nspark.sql.legacy.mysql.bitArrayMapping.enabled=true\nTINYINT(1)\nBooleanType\nTINYINT(1)\nByteType\ntinyInt1isBit=false\nBOOLEAN\nBooleanType\nBOOLEAN\nByteType\ntinyInt1isBit=false\nTINYINT( >1 )\nByteType\nTINYINT( any ) UNSIGNED\nShortType\nSMALLINT\nShortType\nSMALLINT UNSIGNED\nIntegerType\nMEDIUMINT [UNSIGNED]\nIntegerType\nINT\nIntegerType\nINT UNSIGNED\nLongType\nBIGINT\nLongType\nBIGINT UNSIGNED\nDecimalType(20,0)\nFLOAT\nFloatType\nFLOAT UNSIGNED\nDoubleType\nDOUBLE [UNSIGNED]\nDoubleType\nDECIMAL(p,s) [UNSIGNED]\nDecimalType(min(38, p),(min(18,s)))\nThe column type is bounded to DecimalType(38, 18), if 'p>38', the fraction part will be truncated if exceeded. And if any value of this col", "question": "What Spark SQL Data Type corresponds to MySQL's DECIMAL(p,s) [UNSIGNED]?", "answers": {"text": ["DecimalType(min(38, p),(min(18,s)))"], "answer_start": [625]}}
{"context": "in(18,s)))\nThe column type is bounded to DecimalType(38, 18), if 'p>38', the fraction part will be truncated if exceeded. And if any value of this column have an actual precision greater 38 will fail with NUMERIC_VALUE_OUT_OF_RANGE.WITHOUT_SUGGESTION error\nDATE\nDateType\nDATETIME\nTimestampType\n(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\nDATETIME\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTAMP_NTZ\nTIMESTAMP\nTimestampType\nTIME\nTimestampType\n(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\nTIME\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTAMP_NTZ\nYEAR\nDateType\nyearIsDateType=true\nYEAR\nIntegerType\nyearIsDateType=false\nCHAR(n)\nCharType(n)\nVARCHAR(n)\nVarcharType(n)\nBINARY(n)\nBinaryT", "question": "What happens if a value in a DecimalType(38, 18) column has an actual precision greater than 38?", "answers": {"text": ["will fail with NUMERIC_VALUE_OUT_OF_RANGE.WITHOUT_SUGGESTION error"], "answer_start": [190]}}
{"context": "=TIMESTAMP_NTZ\nYEAR\nDateType\nyearIsDateType=true\nYEAR\nIntegerType\nyearIsDateType=false\nCHAR(n)\nCharType(n)\nVARCHAR(n)\nVarcharType(n)\nBINARY(n)\nBinaryType\nVARBINARY(n)\nBinaryType\nCHAR(n) BINARY\nBinaryType\nVARCHAR(n) BINARY\nBinaryType\nBLOB\nBinaryType\nTINYBLOB\nBinaryType\nMEDIUMBLOB\nBinaryType\nLONGBLOB\nBinaryType\nTEXT\nStringType\nTINYTEXT\nStringType\nMEDIUMTEXT\nStringType\nLONGTEXT\nStringType\nJSON\nStringType\nGEOMETRY\nBinaryType\nENUM\nCharType(n)\nSET\nCharType(n)\nMapping Spark SQL Data Types to MySQL\nThe below table describes the data type conversions from Spark SQL Data Types to MySQL data types,\nwhen creating, altering, or writing data to a MySQL table using the built-in jdbc data source with\nthe MySQL Connector/J as the activated JDBC Driver.\nNote that, different JDBC drivers, such as Maria Conne", "question": "What does the table below describe?", "answers": {"text": ["The below table describes the data type conversions from Spark SQL Data Types to MySQL data types"], "answer_start": [496]}}
{"context": "le using the built-in jdbc data source with\nthe MySQL Connector/J as the activated JDBC Driver.\nNote that, different JDBC drivers, such as Maria Connector/J, which are also available to connect MySQL,\nmay have different mapping rules.\nSpark SQL Data Type\nMySQL Data Type\nRemarks\nBooleanType\nBIT(1)\nByteType\nTINYINT\nShortType\nSMALLINT\nFor Spark 3.5 and previous, it maps to INTEGER\nIntegerType\nINTEGER\nLongType\nBIGINT\nFloatType\nFLOAT\nDoubleType\nDOUBLE PRECISION\nDecimalType(p, s)\nDECIMAL(p,s)\nDateType\nDATE\nTimestampType\nTIMESTAMP\nTimestampNTZType\nDATETIME\nStringType\nLONGTEXT\nBinaryType\nBLOB\nCharType(n)\nCHAR(n)\nVarcharType(n)\nVARCHAR(n)\nThe Spark Catalyst data types below are not supported with suitable MYSQL types.\nDayTimeIntervalType\nYearMonthIntervalType\nCalendarIntervalType\nArrayType\nMapType\n", "question": "What MySQL data type does Spark SQL's DecimalType(p, s) map to?", "answers": {"text": ["DECIMAL(p,s)"], "answer_start": [479]}}
{"context": "talyst data types below are not supported with suitable MYSQL types.\nDayTimeIntervalType\nYearMonthIntervalType\nCalendarIntervalType\nArrayType\nMapType\nStructType\nUserDefinedType\nNullType\nObjectType\nVariantType\nMapping Spark SQL Data Types from PostgreSQL\nThe below table describes the data type conversions from PostgreSQL data types to Spark SQL Data Types,\nwhen reading data from a Postgres table using the built-in jdbc data source with the\nPostgreSQL JDBC Driver\nas the activated JDBC Driver. Note that, different JDBC drivers, or different versions might result slightly different.\nPostgreSQL Data Type\nSpark SQL Data Type\nRemarks\nboolean\nBooleanType\nsmallint, smallserial\nShortType\ninteger, serial\nIntegerType\nbigint, bigserial\nLongType\nfloat, float(p),  real\nFloatType\n1 ≤ p ≤ 24\nfloat(p)\nDoubl", "question": "Quais tipos de dados Talyst não são suportados com tipos MYSQL adequados?", "answers": {"text": ["DayTimeIntervalType\nYearMonthIntervalType\nCalendarIntervalType\nArrayType\nMapType\nStructType\nUserDefinedType\nNullType\nObjectType\nVariantType"], "answer_start": [69]}}
{"context": "Type\nsmallint, smallserial\nShortType\ninteger, serial\nIntegerType\nbigint, bigserial\nLongType\nfloat, float(p),  real\nFloatType\n1 ≤ p ≤ 24\nfloat(p)\nDoubleType\n25 ≤ p ≤ 53\ndouble precision\nDoubleType\nnumeric, decimal\nDecimalType\nSince PostgreSQL 15, 's' can be negative. If 's<0' it'll be adjusted to DecimalType(min(p-s, 38), 0); Otherwise, DecimalType(p, s)\nIf 'p>38', the fraction part will be truncated if exceeded. And if any value of this column have an actual precision greater 38 will fail with NUMERIC_VALUE_OUT_OF_RANGE.WITHOUT_SUGGESTION error.\nSpecial numeric values, 'NaN', 'infinity' and '-infinity' is not supported\ncharacter varying(n), varchar(n)\nVarcharType(n)\ncharacter(n), char(n), bpchar(n)\nCharType(n)\nbpchar\nStringType\ntext\nStringType\nbytea\nBinaryType\ndate\nDateType\ntimestamp [ (p)", "question": "What happens if 'p>38' when using DecimalType?", "answers": {"text": ["If 'p>38', the fraction part will be truncated if exceeded. And if any value of this column have an actual precision greater 38 will fail with NUMERIC_VALUE_OUT_OF_RANGE.WITHOUT_SUGGESTION error."], "answer_start": [356]}}
{"context": "archar(n)\nVarcharType(n)\ncharacter(n), char(n), bpchar(n)\nCharType(n)\nbpchar\nStringType\ntext\nStringType\nbytea\nBinaryType\ndate\nDateType\ntimestamp [ (p) ] [ without time zone ]\nTimestampType\n(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\ntimestamp [ (p) ] [ without time zone ]\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTAMP_NTZ\ntimestamp [ (p) ] with time zone\nTimestampType\ntime [ (p) ] [ without time zone ]\nTimestampType\n(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\ntime [ (p) ] [ without time zone ]\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTAMP_NTZ\ntime [ (p) ] with time zone\nTimestampType\ninterval [ fields ] [ (p) ]\nStringType\nENUM\nStringType\nmoney\nStringType\nMonetary Typ", "question": "Quais tipos de dados são representados por 'varchar(n)'?", "answers": {"text": ["VarcharType(n)"], "answer_start": [10]}}
{"context": "stampType=TIMESTAMP_NTZ\ntime [ (p) ] with time zone\nTimestampType\ninterval [ fields ] [ (p) ]\nStringType\nENUM\nStringType\nmoney\nStringType\nMonetary Types\ninet, cidr, macaddr, macaddr8\nStringType\nNetwork Address Types\npoint, line, lseg, box, path, polygon, circle\nStringType\nGeometric Types\npg_lsn\nStringType\nLog Sequence Number\nbit, bit(1)\nBooleanType\nbit( >1 )\nBinaryType\nbit varying( any )\nBinaryType\ntsvector, tsquery\nStringType\nText Search Types\nuuid\nStringType\nUniversally Unique Identifier Type\nxml\nStringType\nXML Type\njson, jsonb\nStringType\nJSON Types\narray\nArrayType\nComposite Types\nStringType\nTypes created by CREATE TYPE syntax.\nint4range, int8range, numrange, tsrange, tstzrange, daterange, etc\nStringType\nRange Types\nDomain Types\n(Decided by the underlying type)\noid\nDecimalType(20, 0)\nObj", "question": "What type is used for Universally Unique Identifier?", "answers": {"text": ["StringType"], "answer_start": [94]}}
{"context": "nt8range, numrange, tsrange, tstzrange, daterange, etc\nStringType\nRange Types\nDomain Types\n(Decided by the underlying type)\noid\nDecimalType(20, 0)\nObject Identifier Types\nregxxx\nStringType\nObject Identifier Types\nvoid\nNullType\nvoid is a Postgres pseudo type, other pseudo types have not yet been verified\nMapping Spark SQL Data Types to PostgreSQL\nThe below table describes the data type conversions from Spark SQL Data Types to PostgreSQL data types,\nwhen creating, altering, or writing data to a PostgreSQL table using the built-in jdbc data source with\nthe\nPostgreSQL JDBC Driver\nas the activated JDBC Driver.\nSpark SQL Data Type\nPostgreSQL Data Type\nRemarks\nBooleanType\nboolean\nByteType\nsmallint\nShortType\nsmallint\nIntegerType\ninteger\nLongType\nbigint\nFloatType\nfloat4\nDoubleType\nfloat8\nDecimalTyp", "question": "What PostgreSQL data type does Spark SQL's LongType map to?", "answers": {"text": ["bigint"], "answer_start": [748]}}
{"context": "ype\nRemarks\nBooleanType\nboolean\nByteType\nsmallint\nShortType\nsmallint\nIntegerType\ninteger\nLongType\nbigint\nFloatType\nfloat4\nDoubleType\nfloat8\nDecimalType(p, s)\nnumeric(p,s)\nDateType\ndate\nTimestampType\ntimestamp with time zone\nBefore Spark 4.0, it was mapped as timestamp. Please refer to the migration guide for more information\nTimestampNTZType\ntimestamp\nStringType\ntext\nBinaryType\nbytea\nCharType(n)\nCHAR(n)\nVarcharType(n)\nVARCHAR(n)\nArrayType\nElement type\nPG Array\nBooleanType\nboolean[]\nByteType\nsmallint[]\nShortType\nsmallint[]\nIntegerType\ninteger[]\nLongType\nbigint[]\nFloatType\nfloat4[]\nDoubleType\nfloat8[]\nDecimalType(p, s)\nnumeric(p,s)[]\nDateType\ndate[]\nTimestampType\ntimestamp[]\nTimestampNTZType\ntimestamp[]\nStringType\ntext[]\nBinaryType\nbytea[]\nCharType(n)\nchar(n)[]\nVarcharType(n)\nvarchar(n)[]\nIf", "question": "Como o TimestampType era mapeado antes do Spark 4.0?", "answers": {"text": ["Before Spark 4.0, it was mapped as timestamp."], "answer_start": [224]}}
{"context": "ate[]\nTimestampType\ntimestamp[]\nTimestampNTZType\ntimestamp[]\nStringType\ntext[]\nBinaryType\nbytea[]\nCharType(n)\nchar(n)[]\nVarcharType(n)\nvarchar(n)[]\nIf the element type is an ArrayType, it converts to Postgres multidimensional array.\nFor instance,\nArrayType(ArrayType(StringType))\nconverts to\ntext[][]\n,\nArrayType(ArrayType(ArrayType(LongType)))\nconverts to\nbigint[][][]\nThe Spark Catalyst data types below are not supported with suitable PostgreSQL types.\nDayTimeIntervalType\nYearMonthIntervalType\nCalendarIntervalType\nArrayType - if the element type is not listed above\nMapType\nStructType\nUserDefinedType\nNullType\nObjectType\nVariantType\nMapping Spark SQL Data Types from Oracle\nThe below table describes the data type conversions from Oracle data types to Spark SQL Data Types,\nwhen reading data fro", "question": "What happens when the element type is an ArrayType?", "answers": {"text": ["If the element type is an ArrayType, it converts to Postgres multidimensional array."], "answer_start": [148]}}
{"context": "k SQL Data Types from Oracle\nThe below table describes the data type conversions from Oracle data types to Spark SQL Data Types,\nwhen reading data from an Oracle table using the built-in jdbc data source with the Oracle JDBC\nas the activated JDBC Driver.\nOracle Data Type\nSpark SQL Data Type\nRemarks\nBOOLEAN\nBooleanType\nIntroduced since Oracle Release 23c\nNUMBER[(p[,s])]\nDecimalType(p,s)\n's' can be negative in Oracle. If 's<0' it'll be adjusted to DecimalType(min(p-s, 38), 0); Otherwise, DecimalType(p, s), and if 'p>38', the fraction part will be truncated if exceeded. And if any value of this column have an actual precision greater 38 will fail with NUMERIC_VALUE_OUT_OF_RANGE.WITHOUT_SUGGESTION error\nFLOAT[(p)]\nDecimalType(38, 10)\nBINARY_FLOAT\nFloatType\nBINARY_DOUBLE\nDoubleType\nLONG\nBinaryT", "question": "What Spark SQL Data Type corresponds to the Oracle Data Type LONG?", "answers": {"text": ["BinaryT"], "answer_start": [793]}}
{"context": "l with NUMERIC_VALUE_OUT_OF_RANGE.WITHOUT_SUGGESTION error\nFLOAT[(p)]\nDecimalType(38, 10)\nBINARY_FLOAT\nFloatType\nBINARY_DOUBLE\nDoubleType\nLONG\nBinaryType\nRAW(size)\nBinaryType\nLONG RAW\nBinaryType\nDATE\nTimestampType\nWhen oracle.jdbc.mapDateToTimestamp=true, it follows TIMESTAMP's behavior below\nDATE\nDateType\nWhen oracle.jdbc.mapDateToTimestamp=false, it maps to DateType\nTIMESTAMP\nTimestampType\n(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\nTIMESTAMP\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTAMP_NTZ\nTIMESTAMP WITH TIME ZONE\nTimestampType\nTIMESTAMP WITH LOCAL TIME ZONE\nTimestampType\nINTERVAL YEAR TO MONTH\nYearMonthIntervalType\nINTERVAL DAY TO SECOND\nDayTimeIntervalType\nCHAR[(size [BYTE | CHAR])]\nCharType(size)\nNCHAR[(size)]\nStringType", "question": "What type does DATE map to when oracle.jdbc.mapDateToTimestamp=false?", "answers": {"text": ["it maps to DateType"], "answer_start": [351]}}
{"context": "RVAL YEAR TO MONTH\nYearMonthIntervalType\nINTERVAL DAY TO SECOND\nDayTimeIntervalType\nCHAR[(size [BYTE | CHAR])]\nCharType(size)\nNCHAR[(size)]\nStringType\nVARCHAR2(size [BYTE | CHAR])\nVarcharType(size)\nNVARCHAR2\nStringType\nROWID/UROWID\nStringType\nCLOB\nStringType\nNCLOB\nStringType\nBLOB\nBinaryType\nBFILE\nUNRECOGNIZED_SQL_TYPE error raised\nMapping Spark SQL Data Types to Oracle\nThe below table describes the data type conversions from Spark SQL Data Types to Oracle data types,\nwhen creating, altering, or writing data to an Oracle table using the built-in jdbc data source with\nthe Oracle JDBC as the activated JDBC Driver.\nSpark SQL Data Type\nOracle Data Type\nRemarks\nBooleanType\nNUMBER(1, 0)\nBooleanType maps to NUMBER(1, 0) as BOOLEAN is introduced since Oracle Release 23c\nByteType\nNUMBER(3)\nShortType", "question": "Para qual tipo de dados Oracle BooleanType é mapeado?", "answers": {"text": ["NUMBER(1, 0)"], "answer_start": [676]}}
{"context": " Type\nRemarks\nBooleanType\nNUMBER(1, 0)\nBooleanType maps to NUMBER(1, 0) as BOOLEAN is introduced since Oracle Release 23c\nByteType\nNUMBER(3)\nShortType\nNUMBER(5)\nIntegerType\nNUMBER(10)\nLongType\nNUMBER(19)\nFloatType\nNUMBER(19, 4)\nDoubleType\nNUMBER(19, 4)\nDecimalType(p, s)\nNUMBER(p,s)\nDateType\nDATE\nTimestampType\nTIMESTAMP WITH LOCAL TIME ZONE\nTimestampNTZType\nTIMESTAMP\nStringType\nVARCHAR2(255)\nFor historical reason, a string value has maximum 255 characters\nBinaryType\nBLOB\nCharType(n)\nCHAR(n)\nVarcharType(n)\nVARCHAR2(n)\nThe Spark Catalyst data types below are not supported with suitable Oracle types.\nDayTimeIntervalType\nYearMonthIntervalType\nCalendarIntervalType\nArrayType\nMapType\nStructType\nUserDefinedType\nNullType\nObjectType\nVariantType\nMapping Spark SQL Data Types from Microsoft SQL Server\nT", "question": "To what Oracle type does BooleanType map?", "answers": {"text": ["BooleanType maps to NUMBER(1, 0)"], "answer_start": [39]}}
{"context": "ndarIntervalType\nArrayType\nMapType\nStructType\nUserDefinedType\nNullType\nObjectType\nVariantType\nMapping Spark SQL Data Types from Microsoft SQL Server\nThe below table describes the data type conversions from Microsoft SQL Server data types to Spark SQL Data Types,\nwhen reading data from a Microsoft SQL Server table using the built-in jdbc data source with the mssql-jdbc\nas the activated JDBC Driver.\nSQL Server  Data Type\nSpark SQL Data Type\nRemarks\nbit\nBooleanType\ntinyint\nShortType\nsmallint\nShortType\nint\nIntegerType\nbigint\nLongType\nfloat(p), real\nFloatType\n1 ≤ p ≤ 24\nfloat[(p)]\nDoubleType\n25 ≤ p ≤ 53\ndouble precision\nDoubleType\nsmallmoney\nDecimalType(10, 4)\nmoney\nDecimalType(19, 4)\ndecimal[(p[, s])], numeric[(p[, s])]\nDecimalType(p, s)\ndate\nDateType\ndatetime\nTimestampType\n(Default)preferTime", "question": "Qual tipo de dado Spark SQL é mapeado para o tipo de dado SQL Server 'int'?", "answers": {"text": ["IntegerType"], "answer_start": [508]}}
{"context": "alType(10, 4)\nmoney\nDecimalType(19, 4)\ndecimal[(p[, s])], numeric[(p[, s])]\nDecimalType(p, s)\ndate\nDateType\ndatetime\nTimestampType\n(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\ndatetime\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTAMP_NTZ\ndatetime2 [ (fractional seconds precision) ]\nTimestampType\n(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\ndatetime2 [ (fractional seconds precision) ]\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTAMP_NTZ\ndatetimeoffset [ (fractional seconds precision) ]\nTimestampType\nsmalldatetime\nTimestampType\n(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\nsmalldatetime\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timesta", "question": "Qual tipo de dado é mapeado para DecimalType(19, 4)?", "answers": {"text": ["money"], "answer_start": [14]}}
{"context": "(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\nsmalldatetime\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTAMP_NTZ\ntime [ (fractional second scale) ]\nTimestampType\n(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\ntime [ (fractional second scale) ]\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTAMP_NTZ\nbinary [ ( n ) ]\nBinaryType\nvarbinary [ ( n | max ) ]\nBinaryType\nchar [ ( n ) ]\nCharType(n)\nvarchar [ ( n | max ) ]\nVarcharType(n)\nnchar [ ( n ) ]\nStringType\nnvarchar [ ( n | max ) ]\nStringType\ntext\nStringType\nntext\nStringType\nimage\nStringType\ngeography\nBinaryType\ngeometry\nBinaryType\nrowversion\nBinaryType\nsql_variant\nUNRECOGNIZED_SQL_TYPE error raised\nMapping Spark SQL Data Types to Mi", "question": "What data type is mapped to 'varchar [ ( n | max ) ]'?", "answers": {"text": ["VarcharType(n)"], "answer_start": [528]}}
{"context": "gType\ngeography\nBinaryType\ngeometry\nBinaryType\nrowversion\nBinaryType\nsql_variant\nUNRECOGNIZED_SQL_TYPE error raised\nMapping Spark SQL Data Types to Microsoft SQL Server\nThe below table describes the data type conversions from Spark SQL Data Types to Microsoft SQL Server data types,\nwhen creating, altering, or writing data to a Microsoft SQL Server table using the built-in jdbc data source with\nthe mssql-jdbc as the activated JDBC Driver.\nSpark SQL Data Type\nSQL Server Data Type\nRemarks\nBooleanType\nbit\nByteType\nsmallint\nSupported since Spark 4.0.0, previous versions throw errors\nShortType\nsmallint\nIntegerType\nint\nLongType\nbigint\nFloatType\nreal\nDoubleType\ndouble precision\nDecimalType(p, s)\nnumber(p,s)\nDateType\ndate\nTimestampType\ndatetime\nTimestampNTZType\ndatetime\nStringType\nnvarchar(max)\nBin", "question": "What data type in Spark SQL maps to 'bit' in SQL Server?", "answers": {"text": ["BooleanType"], "answer_start": [491]}}
{"context": "\nDoubleType\ndouble precision\nDecimalType(p, s)\nnumber(p,s)\nDateType\ndate\nTimestampType\ndatetime\nTimestampNTZType\ndatetime\nStringType\nnvarchar(max)\nBinaryType\nvarbinary(max)\nCharType(n)\nchar(n)\nVarcharType(n)\nvarchar(n)\nThe Spark Catalyst data types below are not supported with suitable SQL Server types.\nDayTimeIntervalType\nYearMonthIntervalType\nCalendarIntervalType\nArrayType\nMapType\nStructType\nUserDefinedType\nNullType\nObjectType\nVariantType\nMapping Spark SQL Data Types from DB2\nThe below table describes the data type conversions from DB2 data types to Spark SQL Data Types,\nwhen reading data from a DB2 table using the built-in jdbc data source with the\nIBM Data Server Driver For JDBC and SQLJ\nas the activated JDBC Driver.\nDB2 Data Type\nSpark SQL Data Type\nRemarks\nBOOLEAN\nBinaryType\nSMALLINT", "question": "Which Spark SQL Data Type corresponds to the DB2 Data Type BOOLEAN?", "answers": {"text": ["BinaryType"], "answer_start": [147]}}
{"context": " with the\nIBM Data Server Driver For JDBC and SQLJ\nas the activated JDBC Driver.\nDB2 Data Type\nSpark SQL Data Type\nRemarks\nBOOLEAN\nBinaryType\nSMALLINT\nShortType\nINTEGER\nIntegerType\nBIGINT\nLongType\nREAL\nFloatType\nDOUBLE, FLOAT\nDoubleType\nFLOAT is double precision floating-point in db2\nDECIMAL, NUMERIC, DECFLOAT\nDecimalType\nDATE\nDateType\nTIMESTAMP, TIMESTAMP WITHOUT TIME ZONE\nTimestampType\n(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\nTIMESTAMP, TIMESTAMP WITHOUT TIME ZONE\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTAMP_NTZ\nTIMESTAMP WITH TIME ZONE\nTimestampType\nTIME\nTimestampType\n(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\nTIME\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTA", "question": "What Spark SQL Data Type corresponds to the DB2 Data Type BIGINT?", "answers": {"text": ["LongType"], "answer_start": [188]}}
{"context": "ult)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\nTIME\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTAMP_NTZ\nCHAR(n)\nCharType(n)\nVARCHAR(n)\nVarcharType(n)\nCHAR(n) FOR BIT DATA\nBinaryType\nVARCHAR(n) FOR BIT DATA\nBinaryType\nBINARY(n)\nBinaryType\nVARBINARY(n)\nBinaryType\nCLOB(n)\nStringType\nDBCLOB(n)\nStringType\nBLOB(n)\nBinaryType\nGRAPHIC(n)\nStringType\nVARGRAPHIC(n)\nStringType\nXML\nStringType\nROWID\nStringType\nMapping Spark SQL Data Types to DB2\nThe below table describes the data type conversions from Spark SQL Data Types to DB2 data types,\nwhen creating, altering, or writing data to a DB2 table using the built-in jdbc data source with\nthe\nIBM Data Server Driver For JDBC and SQLJ\nas the activated JDBC Driver.\nSpark SQL Data Type\nDB2 Data Type\nRemarks\n", "question": "What does the table describe?", "answers": {"text": ["The below table describes the data type conversions from Spark SQL Data Types to DB2 data types"], "answer_start": [489]}}
{"context": "e built-in jdbc data source with\nthe\nIBM Data Server Driver For JDBC and SQLJ\nas the activated JDBC Driver.\nSpark SQL Data Type\nDB2 Data Type\nRemarks\nBooleanType\nBOOLEAN\nByteType\nSMALLINT\nShortType\nSMALLINT\nIntegerType\nINTEGER\nLongType\nBIGINT\nFloatType\nREAL\nDoubleType\nDOUBLE PRECISION\nDecimalType(p, s)\nDECIMAL(p,s)\nThe maximum value for 'p' is 31 in DB2, while it is 38 in Spark. It might fail when storing DecimalType(p>=32, s) to DB2\nDateType\nDATE\nTimestampType\nTIMESTAMP\nTimestampNTZType\nTIMESTAMP\nStringType\nCLOB\nBinaryType\nBLOB\nCharType(n)\nCHAR(n)\nThe maximum value for 'n' is 255 in DB2, while it is unlimited in Spark.\nVarcharType(n)\nVARCHAR(n)\nThe maximum value for 'n' is 255 in DB2, while it is unlimited in Spark.\nThe Spark Catalyst data types below are not supported with suitable DB2 t", "question": "What is the DB2 data type corresponding to Spark SQL's DecimalType(p, s)?", "answers": {"text": ["DECIMAL(p,s)"], "answer_start": [304]}}
{"context": "(n)\nThe maximum value for 'n' is 255 in DB2, while it is unlimited in Spark.\nThe Spark Catalyst data types below are not supported with suitable DB2 types.\nDayTimeIntervalType\nYearMonthIntervalType\nCalendarIntervalType\nArrayType\nMapType\nStructType\nUserDefinedType\nNullType\nObjectType\nVariantType\nMapping Spark SQL Data Types from Teradata\nThe below table describes the data type conversions from Teradata data types to Spark SQL Data Types,\nwhen reading data from a Teradata table using the built-in jdbc data source with the\nTeradata JDBC Driver\nas the activated JDBC Driver.\nTeradata Data Type\nSpark SQL Data Type\nRemarks\nBYTEINT\nByteType\nSMALLINT\nShortType\nINTEGER, INT\nIntegerType\nBIGINT\nLongType\nREAL, DOUBLE PRECISION, FLOAT\nDoubleType\nDECIMAL, NUMERIC, NUMBER\nDecimalType\nDATE\nDateType\nTIMESTA", "question": "What is the maximum value for 'n' in DB2?", "answers": {"text": ["The maximum value for 'n' is 255 in DB2"], "answer_start": [4]}}
{"context": "ShortType\nINTEGER, INT\nIntegerType\nBIGINT\nLongType\nREAL, DOUBLE PRECISION, FLOAT\nDoubleType\nDECIMAL, NUMERIC, NUMBER\nDecimalType\nDATE\nDateType\nTIMESTAMP, TIMESTAMP WITH TIME ZONE\nTimestampType\n(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\nTIMESTAMP, TIMESTAMP WITH TIME ZONE\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTAMP_NTZ\nTIME, TIME WITH TIME ZONE\nTimestampType\n(Default)preferTimestampNTZ=false or spark.sql.timestampType=TIMESTAMP_LTZ\nTIME, TIME WITH TIME ZONE\nTimestampNTZType\npreferTimestampNTZ=true or spark.sql.timestampType=TIMESTAMP_NTZ\nCHARACTER(n), CHAR(n), GRAPHIC(n)\nCharType(n)\nVARCHAR(n), VARGRAPHIC(n)\nVarcharType(n)\nBYTE(n), VARBYTE(n)\nBinaryType\nCLOB\nStringType\nBLOB\nBinaryType\nINTERVAL Data Types\n-\nThe INTERVAL data t", "question": "Quais tipos de dados são mapeados para o tipo TimestampType?", "answers": {"text": ["TIMESTAMP, TIMESTAMP WITH TIME ZONE"], "answer_start": [143]}}
{"context": "e(n)\nVARCHAR(n), VARGRAPHIC(n)\nVarcharType(n)\nBYTE(n), VARBYTE(n)\nBinaryType\nCLOB\nStringType\nBLOB\nBinaryType\nINTERVAL Data Types\n-\nThe INTERVAL data types are unknown yet\nPeriod Data Types, ARRAY, UDT\n-\nNot Supported\nMapping Spark SQL Data Types to Teradata\nThe below table describes the data type conversions from Spark SQL Data Types to Teradata data types,\nwhen creating, altering, or writing data to a Teradata table using the built-in jdbc data source with\nthe\nTeradata JDBC Driver\nas the activated JDBC Driver.\nSpark SQL Data Type\nTeradata Data Type\nRemarks\nBooleanType\nCHAR(1)\nByteType\nBYTEINT\nShortType\nSMALLINT\nIntegerType\nINTEGER\nLongType\nBIGINT\nFloatType\nREAL\nDoubleType\nDOUBLE PRECISION\nDecimalType(p, s)\nDECIMAL(p,s)\nDateType\nDATE\nTimestampType\nTIMESTAMP\nTimestampNTZType\nTIMESTAMP\nStrin", "question": "What Teradata data type does Spark SQL's IntegerType map to?", "answers": {"text": ["INTEGER"], "answer_start": [632]}}
{"context": "IGINT\nFloatType\nREAL\nDoubleType\nDOUBLE PRECISION\nDecimalType(p, s)\nDECIMAL(p,s)\nDateType\nDATE\nTimestampType\nTIMESTAMP\nTimestampNTZType\nTIMESTAMP\nStringType\nVARCHAR(255)\nBinaryType\nBLOB\nCharType(n)\nCHAR(n)\nVarcharType(n)\nVARCHAR(n)\nThe Spark Catalyst data types below are not supported with suitable Teradata types.\nDayTimeIntervalType\nYearMonthIntervalType\nCalendarIntervalType\nArrayType\nMapType\nStructType\nUserDefinedType\nNullType\nObjectType\nVariantType", "question": "Quais tipos de dados são representados por REAL e DOUBLE PRECISION?", "answers": {"text": ["REAL\nDoubleType\nDOUBLE PRECISION"], "answer_start": [16]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)"], "answer_start": [46]}}
{"context": "e. The model type is selected with an optional parameter\n“multinomial” or “bernoulli” with “multinomial” as the default.\nAdditive smoothing\ncan be used by\nsetting the parameter $\\lambda$ (default to $1.0$). For document classification, the input feature\nvectors are usually sparse, and sparse vectors should be supplied as input to take advantage of\nsparsity. Since the training data is only used once, it is not necessary to cache it.\nExamples\nNaiveBayes\nimplements multinomial\nnaive Bayes. It takes an RDD of\nLabeledPoint\nand an optionally\nsmoothing parameter\nlambda\nas input, and output a\nNaiveBayesModel\n, which can be\nused for evaluation and prediction.\nNote that the Python API does not yet support model save/load but will in the future.\nRefer to the\nNaiveBayes\nPython docs\nand\nNaiveBayesModel", "question": "What are the optional parameters for selecting the model type?", "answers": {"text": ["“multinomial” or “bernoulli” with “multinomial” as the default."], "answer_start": [57]}}
{"context": "ndLabel\n=\ntest\n.\nmap\n(\nlambda\np\n:\n(\nmodel\n.\npredict\n(\np\n.\nfeatures\n),\np\n.\nlabel\n))\naccuracy\n=\n1.0\n*\npredictionAndLabel\n.\nfilter\n(\nlambda\npl\n:\npl\n[\n0\n]\n==\npl\n[\n1\n]).\ncount\n()\n/\ntest\n.\ncount\n()\nprint\n(\n'\nmodel accuracy {}\n'\n.\nformat\n(\naccuracy\n))\n# Save and load model\noutput_dir\n=\n'\ntarget/tmp/myNaiveBayesModel\n'\nshutil\n.\nrmtree\n(\noutput_dir\n,\nignore_errors\n=\nTrue\n)\nmodel\n.\nsave\n(\nsc\n,\noutput_dir\n)\nsameModel\n=\nNaiveBayesModel\n.\nload\n(\nsc\n,\noutput_dir\n)\npredictionAndLabel\n=\ntest\n.\nmap\n(\nlambda\np\n:\n(\nsameModel\n.\npredict\n(\np\n.\nfeatures\n),\np\n.\nlabel\n))\naccuracy\n=\n1.0\n*\npredictionAndLabel\n.\nfilter\n(\nlambda\npl\n:\npl\n[\n0\n]\n==\npl\n[\n1\n]).\ncount\n()\n/\ntest\n.\ncount\n()\nprint\n(\n'\nsameModel accuracy {}\n'\n.\nformat\n(\naccuracy\n))\nFind full example code at \"examples/src/main/python/mllib/naive_bayes_example.py\"", "question": "Where can I find the full example code for this implementation?", "answers": {"text": ["Find full example code at \"examples/src/main/python/mllib/naive_bayes_example.py\""], "answer_start": [719]}}
{"context": "Utils\n// Load and parse the data file.\nval\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_libsvm_data.txt\"\n)\n// Split data into training (60%) and test (40%).\nval\nArray\n(\ntraining\n,\ntest\n)\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.6\n,\n0.4\n))\nval\nmodel\n=\nNaiveBayes\n.\ntrain\n(\ntraining\n,\nlambda\n=\n1.0\n,\nmodelType\n=\n\"multinomial\"\n)\nval\npredictionAndLabel\n=\ntest\n.\nmap\n(\np\n=>\n(\nmodel\n.\npredict\n(\np\n.\nfeatures\n),\np\n.\nlabel\n))\nval\naccuracy\n=\n1.0\n*\npredictionAndLabel\n.\nfilter\n(\nx\n=>\nx\n.\n_1\n==\nx\n.\n_2\n).\ncount\n()\n/\ntest\n.\ncount\n()\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/myNaiveBayesModel\"\n)\nval\nsameModel\n=\nNaiveBayesModel\n.\nload\n(\nsc\n,\n\"target/tmp/myNaiveBayesModel\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\" in the Spar", "question": "Where can I find the full example code for NaiveBayesExample?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\" in the Spar"], "answer_start": [681]}}
{"context": ",\nDouble\n>\npredictionAndLabel\n=\ntest\n.\nmapToPair\n(\np\n->\nnew\nTuple2\n<>(\nmodel\n.\npredict\n(\np\n.\nfeatures\n()),\np\n.\nlabel\n()));\ndouble\naccuracy\n=\npredictionAndLabel\n.\nfilter\n(\npl\n->\npl\n.\n_1\n().\nequals\n(\npl\n.\n_2\n())).\ncount\n()\n/\n(\ndouble\n)\ntest\n.\ncount\n();\n// Save and load model\nmodel\n.\nsave\n(\njsc\n.\nsc\n(),\n\"target/tmp/myNaiveBayesModel\"\n);\nNaiveBayesModel\nsameModel\n=\nNaiveBayesModel\n.\nload\n(\njsc\n.\nsc\n(),\n\"target/tmp/myNaiveBayesModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\" in the Spark repo.", "question": "Where can I find the full example code for JavaNaiveBayesExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\" in the Spark repo."], "answer_start": [436]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nSpark SQL Guide\nGetting Started\nData Sources\nPerformance Tuning\nDistributed SQL Engine\nPySpark Usage Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nMigration Guide: SQL, Datasets and DataFrame\nUpgrading from Spark SQ", "question": "Quais linguagens de programação possuem API Docs no Spark?", "answers": {"text": ["Python\nScala\nJava\nR"], "answer_start": [266]}}
{"context": "Guide for Pandas with Apache Arrow\nMigration Guide\nSQL Reference\nError Conditions\nMigration Guide: SQL, Datasets and DataFrame\nUpgrading from Spark SQL 3.5 to 4.0\nUpgrading from Spark SQL 3.5.3 to 3.5.4\nUpgrading from Spark SQL 3.5.1 to 3.5.2\nUpgrading from Spark SQL 3.5.0 to 3.5.1\nUpgrading from Spark SQL 3.4 to 3.5\nUpgrading from Spark SQL 3.3 to 3.4\nUpgrading from Spark SQL 3.2 to 3.3\nUpgrading from Spark SQL 3.1 to 3.2\nUpgrading from Spark SQL 3.0 to 3.1\nUpgrading from Spark SQL 3.0.1 to 3.0.2\nUpgrading from Spark SQL 3.0 to 3.0.1\nUpgrading from Spark SQL 2.4 to 3.0\nDataset/DataFrame APIs\nDDL Statements\nUDFs and Built-in Functions\nQuery Engine\nData Sources\nOthers\nUpgrading from Spark SQL 2.4.7 to 2.4.8\nUpgrading from Spark SQL 2.4.5 to 2.4.6\nUpgrading from Spark SQL 2.4.4 to 2.4.5\nUpgr", "question": "Quais versões do Spark SQL são mencionadas como tendo guias de atualização?", "answers": {"text": ["Upgrading from Spark SQL 3.5 to 4.0"], "answer_start": [127]}}
{"context": ".enabled\nis on by default. To restore the previous behavior, set\nspark.sql.ansi.enabled\nto\nfalse\nor\nSPARK_ANSI_SQL_MODE\nto\nfalse\n.\nSince Spark 4.0,\nCREATE TABLE\nsyntax without\nUSING\nand\nSTORED AS\nwill use the value of\nspark.sql.sources.default\nas the table provider instead of\nHive\n. To restore the previous behavior, set\nspark.sql.legacy.createHiveTableByDefault\nto\ntrue\nor\nSPARK_SQL_LEGACY_CREATE_HIVE_TABLE\nto\ntrue\n.\nSince Spark 4.0, the default behaviour when inserting elements in a map is changed to first normalize keys -0.0 to 0.0. The affected SQL functions are\ncreate_map\n,\nmap_from_arrays\n,\nmap_from_entries\n, and\nmap_concat\n. To restore the previous behaviour, set\nspark.sql.legacy.disableMapKeyNormalization\nto\ntrue\n.\nSince Spark 4.0, the default value of\nspark.sql.maxSinglePartitionByt", "question": "How can you restore the previous behavior regarding map key normalization in Spark 4.0?", "answers": {"text": ["To restore the previous behaviour, set\nspark.sql.legacy.disableMapKeyNormalization\nto\ntrue\n."], "answer_start": [638]}}
{"context": "he previous behaviour, set\nspark.sql.legacy.disableMapKeyNormalization\nto\ntrue\n.\nSince Spark 4.0, the default value of\nspark.sql.maxSinglePartitionBytes\nis changed from\nLong.MaxValue\nto\n128m\n. To restore the previous behavior, set\nspark.sql.maxSinglePartitionBytes\nto\n9223372036854775807\n(\nLong.MaxValue\n).\nSince Spark 4.0, any read of SQL tables takes into consideration the SQL configs\nspark.sql.files.ignoreCorruptFiles\n/\nspark.sql.files.ignoreMissingFiles\ninstead of the core config\nspark.files.ignoreCorruptFiles\n/\nspark.files.ignoreMissingFiles\n.\nSince Spark 4.0, when reading SQL tables hits\norg.apache.hadoop.security.AccessControlException\nand\norg.apache.hadoop.hdfs.BlockMissingException\n, the exception will be thrown and fail the task, even if\nspark.sql.files.ignoreCorruptFiles\nis set to", "question": "What is the new default value of spark.sql.maxSinglePartitionBytes since Spark 4.0?", "answers": {"text": ["128m"], "answer_start": [186]}}
{"context": "nd\norg.apache.hadoop.hdfs.BlockMissingException\n, the exception will be thrown and fail the task, even if\nspark.sql.files.ignoreCorruptFiles\nis set to\ntrue\n.\nSince Spark 4.0,\nspark.sql.hive.metastore\ndrops the support of Hive prior to 2.0.0 as they require JDK 8 that Spark does not support anymore. Users should migrate to higher versions.\nSince Spark 4.0, Spark removes\nhive-llap-common\ndependency. To restore the previous behavior, add\nhive-llap-common\njar to the class path.\nSince Spark 4.0,\nspark.sql.parquet.compression.codec\ndrops the support of codec name\nlz4raw\n, please use\nlz4_raw\ninstead.\nSince Spark 4.0, when overflowing during casting timestamp to byte/short/int under non-ansi mode, Spark will return null instead a wrapping value.\nSince Spark 4.0, the\nencode()\nand\ndecode()\nfunctions", "question": "What happens when encountering a BlockMissingException?", "answers": {"text": ["the exception will be thrown and fail the task, even if\nspark.sql.files.ignoreCorruptFiles\nis set to\ntrue"], "answer_start": [50]}}
{"context": "vious behavior, set\nspark.sql.legacy.codingErrorAction\nto\ntrue\n. For example, if you try to\ndecode\na string value\ntést\n/ [116, -23, 115, 116] (encoded in latin1) with ‘UTF-8’, you get\nt�st\n.\nSince Spark 4.0, the legacy datetime rebasing SQL configs with the prefix\nspark.sql.legacy\nare removed. To restore the previous behavior, use the following configs:\nspark.sql.parquet.int96RebaseModeInWrite\ninstead of\nspark.sql.legacy.parquet.int96RebaseModeInWrite\nspark.sql.parquet.datetimeRebaseModeInWrite\ninstead of\nspark.sql.legacy.parquet.datetimeRebaseModeInWrite\nspark.sql.parquet.int96RebaseModeInRead\ninstead of\nspark.sql.legacy.parquet.int96RebaseModeInRead\nspark.sql.avro.datetimeRebaseModeInWrite\ninstead of\nspark.sql.legacy.avro.datetimeRebaseModeInWrite\nspark.sql.avro.datetimeRebaseModeInRead\n", "question": "What should be used instead of spark.sql.legacy.parquet.int96RebaseModeInWrite?", "answers": {"text": ["spark.sql.parquet.int96RebaseModeInWrite"], "answer_start": [356]}}
{"context": "odeInRead\nspark.sql.avro.datetimeRebaseModeInWrite\ninstead of\nspark.sql.legacy.avro.datetimeRebaseModeInWrite\nspark.sql.avro.datetimeRebaseModeInRead\ninstead of\nspark.sql.legacy.avro.datetimeRebaseModeInRead\nSince Spark 4.0, the default value of\nspark.sql.orc.compression.codec\nis changed from\nsnappy\nto\nzstd\n. To restore the previous behavior, set\nspark.sql.orc.compression.codec\nto\nsnappy\n.\nSince Spark 4.0, the SQL config\nspark.sql.legacy.allowZeroIndexInFormatString\nis deprecated. Consider to change\nstrfmt\nof the\nformat_string\nfunction to use 1-based indexes. The first argument must be referenced by\n1$\n, the second by\n2$\n, etc.\nSince Spark 4.0, Postgres JDBC datasource will read JDBC read TIMESTAMP WITH TIME ZONE as TimestampType regardless of the JDBC read option\npreferTimestampNTZ\n, whil", "question": "What is the new default value of spark.sql.orc.compression.codec since Spark 4.0?", "answers": {"text": ["zstd"], "answer_start": [304]}}
{"context": "ESTAMP as TimestampType regardless of the JDBC read option\npreferTimestampNTZ\n, while in 3.5 and previous, TimestampNTZType when\npreferTimestampNTZ=true\n. To restore the previous behavior, set\nspark.sql.legacy.mysql.timestampNTZMapping.enabled\nto\ntrue\n, MySQL DATETIME is not affected.\nSince Spark 4.0, MySQL JDBC datasource will read SMALLINT as ShortType, while in Spark 3.5 and previous, it was read as IntegerType. MEDIUMINT UNSIGNED is read as IntegerType, while in Spark 3.5 and previous, it was read as LongType. To restore the previous behavior, you can cast the column to the old type.\nSince Spark 4.0, MySQL JDBC datasource will read FLOAT as FloatType, while in Spark 3.5 and previous, it was read as DoubleType. To restore the previous behavior, you can cast the column to the old type.\nS", "question": "How does Spark 4.0 differ from Spark 3.5 and previous versions regarding the reading of MySQL FLOAT columns using the JDBC datasource?", "answers": {"text": ["Since Spark 4.0, MySQL JDBC datasource will read FLOAT as FloatType, while in Spark 3.5 and previous, it was read as DoubleType."], "answer_start": [595]}}
{"context": "as FloatType, while in Spark 3.5 and previous, it was read as DoubleType. To restore the previous behavior, you can cast the column to the old type.\nSince Spark 4.0, MySQL JDBC datasource will read BIT(n > 1) as BinaryType, while in Spark 3.5 and previous, read as LongType. To restore the previous behavior, set\nspark.sql.legacy.mysql.bitArrayMapping.enabled\nto\ntrue\n.\nSince Spark 4.0, MySQL JDBC datasource will write ShortType as SMALLINT, while in Spark 3.5 and previous, write as INTEGER. To restore the previous behavior, you can replace the column with IntegerType whenever before writing.\nSince Spark 4.0, MySQL JDBC datasource will write TimestampNTZType as MySQL DATETIME because they both represent TIMESTAMP WITHOUT TIME ZONE, while in 3.5 and previous, it wrote as MySQL TIMESTAMP. To re", "question": "How does Spark 4.0 handle BIT(n > 1) when reading from a MySQL JDBC datasource, and how did it differ in Spark 3.5 and previous versions?", "answers": {"text": ["Since Spark 4.0, MySQL JDBC datasource will read BIT(n > 1) as BinaryType, while in Spark 3.5 and previous, read as LongType."], "answer_start": [149]}}
{"context": "estampNTZType as MySQL DATETIME because they both represent TIMESTAMP WITHOUT TIME ZONE, while in 3.5 and previous, it wrote as MySQL TIMESTAMP. To restore the previous behavior, set\nspark.sql.legacy.mysql.timestampNTZMapping.enabled\nto\ntrue\n.\nSince Spark 4.0, Oracle JDBC datasource will write TimestampType as TIMESTAMP WITH LOCAL TIME ZONE, while in Spark 3.5 and previous, write as TIMESTAMP. To restore the previous behavior, set\nspark.sql.legacy.oracle.timestampMapping.enabled\nto\ntrue\n.\nSince Spark 4.0, MsSQL Server JDBC datasource will read TINYINT as ShortType, while in Spark 3.5 and previous, read as IntegerType. To restore the previous behavior, set\nspark.sql.legacy.mssqlserver.numericMapping.enabled\nto\ntrue\n.\nSince Spark 4.0, MsSQL Server JDBC datasource will read DATETIMEOFFSET as ", "question": "How can the previous behavior of writing TimestampType as TIMESTAMP in Oracle JDBC datasource be restored?", "answers": {"text": ["To restore the previous behavior, set\nspark.sql.legacy.oracle.timestampMapping.enabled\nto\ntrue\n."], "answer_start": [397]}}
{"context": "behavior, set\nspark.sql.legacy.mssqlserver.numericMapping.enabled\nto\ntrue\n.\nSince Spark 4.0, MsSQL Server JDBC datasource will read DATETIMEOFFSET as TimestampType, while in Spark 3.5 and previous, read as StringType. To restore the previous behavior, set\nspark.sql.legacy.mssqlserver.datetimeoffsetMapping.enabled\nto\ntrue\n.\nSince Spark 4.0, DB2 JDBC datasource will read SMALLINT as ShortType, while in Spark 3.5 and previous, it was read as IntegerType. To restore the previous behavior, set\nspark.sql.legacy.db2.numericMapping.enabled\nto\ntrue\n.\nSince Spark 4.0, DB2 JDBC datasource will write BooleanType as BOOLEAN, while in Spark 3.5 and previous, write as CHAR(1). To restore the previous behavior, set\nspark.sql.legacy.db2.booleanMapping.enabled\nto\ntrue\n.\nSince Spark 4.0, The default value fo", "question": "Como restaurar o comportamento anterior do MsSQL Server JDBC datasource em relação ao tipo DATETIMEOFFSET?", "answers": {"text": ["To restore the previous behavior, set\nspark.sql.legacy.mssqlserver.datetimeoffsetMapping.enabled\nto\ntrue\n."], "answer_start": [218]}}
{"context": "rmatting and Parsing\n.\nSince Spark 4.0, A bug falsely allowing\n!\ninstead of\nNOT\nwhen\n!\nis not a prefix operator has been fixed. Clauses such as\nexpr ! IN (...)\n,\nexpr ! BETWEEN ...\n, or\ncol ! NULL\nnow raise syntax errors. To restore the previous behavior, set\nspark.sql.legacy.bangEqualsNot\nto\ntrue\n.\nSince Spark 4.0, By default views tolerate column type changes in the query and compensate with casts. To restore the previous behavior, allowing up-casts only, set\nspark.sql.legacy.viewSchemaCompensation\nto\nfalse\n.\nSince Spark 4.0, Views allow control over how they react to underlying query changes. By default views tolerate column type changes in the query and compensate with casts. To disable this feature set\nspark.sql.legacy.viewSchemaBindingMode\nto\nfalse\n. This also removes the clause from", "question": "How can you restore the previous behavior where '!' was falsely allowed instead of 'NOT' in Spark?", "answers": {"text": ["To restore the previous behavior, set\nspark.sql.legacy.bangEqualsNot\nto\ntrue\n."], "answer_start": [222]}}
{"context": "n the query and compensate with casts. To disable this feature set\nspark.sql.legacy.viewSchemaBindingMode\nto\nfalse\n. This also removes the clause from\nDESCRIBE EXTENDED\nand\nSHOW CREATE TABLE\n.\nSince Spark 4.0, The Storage-Partitioned Join feature flag\nspark.sql.sources.v2.bucketing.pushPartValues.enabled\nis set to\ntrue\n. To restore the previous behavior, set\nspark.sql.sources.v2.bucketing.pushPartValues.enabled\nto\nfalse\n.\nSince Spark 4.0, the\nsentences\nfunction uses\nLocale(language)\ninstead of\nLocale.US\nwhen\nlanguage\nparameter is not\nNULL\nand\ncountry\nparameter is\nNULL\n.\nSince Spark 4.0, reading from a file source table will correctly respect query options, e.g. delimiters. Previously, the first query plan was cached and subsequent option changes ignored. To restore the previous behavior, s", "question": "What can be done to disable the feature that causes Spark to query and compensate with casts?", "answers": {"text": ["To disable this feature set\nspark.sql.legacy.viewSchemaBindingMode\nto\nfalse"], "answer_start": [39]}}
{"context": "query options, e.g. delimiters. Previously, the first query plan was cached and subsequent option changes ignored. To restore the previous behavior, set\nspark.sql.legacy.readFileSourceTableCacheIgnoreOptions\nto\ntrue\n.\nUpgrading from Spark SQL 3.5.3 to 3.5.4\nSince Spark 3.5.4, when reading SQL tables hits\norg.apache.hadoop.security.AccessControlException\nand\norg.apache.hadoop.hdfs.BlockMissingException\n, the exception will be thrown and fail the task, even if\nspark.sql.files.ignoreCorruptFiles\nis set to\ntrue\n.\nUpgrading from Spark SQL 3.5.1 to 3.5.2\nSince 3.5.2, MySQL JDBC datasource will read TINYINT UNSIGNED as ShortType, while in 3.5.1, it was wrongly read as ByteType.\nUpgrading from Spark SQL 3.5.0 to 3.5.1\nSince Spark 3.5.1, MySQL JDBC datasource will read TINYINT(n > 1) and TINYINT UN", "question": "What change occurred regarding TINYINT UNSIGNED when reading MySQL JDBC datasource from Spark SQL 3.5.1 to 3.5.2?", "answers": {"text": ["Since 3.5.2, MySQL JDBC datasource will read TINYINT UNSIGNED as ShortType, while in 3.5.1, it was wrongly read as ByteType."], "answer_start": [555]}}
{"context": "was wrongly read as ByteType.\nUpgrading from Spark SQL 3.5.0 to 3.5.1\nSince Spark 3.5.1, MySQL JDBC datasource will read TINYINT(n > 1) and TINYINT UNSIGNED as ByteType, while in Spark 3.5.0 and below, they were read as IntegerType. To restore the previous behavior, you can cast the column to the old type.\nUpgrading from Spark SQL 3.4 to 3.5\nSince Spark 3.5, the JDBC options related to DS V2 pushdown are\ntrue\nby default. These options include:\npushDownAggregate\n,\npushDownLimit\n,\npushDownOffset\nand\npushDownTableSample\n. To restore the legacy behavior, please set them to\nfalse\n. e.g. set\nspark.sql.catalog.your_catalog_name.pushDownAggregate\nto\nfalse\n.\nSince Spark 3.5, Spark thrift server will interrupt task when canceling a running statement. To restore the previous behavior, set\nspark.sql.t", "question": "What change occurred in Spark 3.5.1 regarding MySQL JDBC datasource and TINYINT types?", "answers": {"text": ["Since Spark 3.5.1, MySQL JDBC datasource will read TINYINT(n > 1) and TINYINT UNSIGNED as ByteType, while in Spark 3.5.0 and below, they were read as IntegerType."], "answer_start": [70]}}
{"context": "false\n.\nSince Spark 3.5, Spark thrift server will interrupt task when canceling a running statement. To restore the previous behavior, set\nspark.sql.thriftServer.interruptOnCancel\nto\nfalse\n.\nSince Spark 3.5, Row’s json and prettyJson methods are moved to\nToJsonUtil\n.\nSince Spark 3.5, the\nplan\nfield is moved from\nAnalysisException\nto\nEnhancedAnalysisException\n.\nSince Spark 3.5,\nspark.sql.optimizer.canChangeCachedPlanOutputPartitioning\nis enabled by default. To restore the previous behavior, set\nspark.sql.optimizer.canChangeCachedPlanOutputPartitioning\nto\nfalse\n.\nSince Spark 3.5, the\narray_insert\nfunction is 1-based for negative indexes. It inserts new element at the end of input arrays for the index -1. To restore the previous behavior, set\nspark.sql.legacy.negativeIndexInArrayInsert\nto\ntru", "question": "How can you restore the previous behavior of Spark thrift server interrupting tasks when canceling a running statement?", "answers": {"text": ["set\nspark.sql.thriftServer.interruptOnCancel\nto\nfalse"], "answer_start": [135]}}
{"context": "olumn lacking an explicitly-assigned default value). In Spark 3.3 or earlier, these commands would have failed returning errors reporting that the number of provided columns does not match the number of columns in the target table. Note that disabling\nspark.sql.defaultColumn.useNullsForMissingDefaultValues\nwill restore the previous behavior.\nSince Spark 3.4, Number or Number(*) from Teradata will be treated as Decimal(38,18). In Spark 3.3 or earlier, Number or Number(*) from Teradata will be treated as Decimal(38, 0), in which case the fractional part will be removed.\nSince Spark 3.4, v1 database, table, permanent view and function identifier will include ‘spark_catalog’ as the catalog name if database is defined, e.g. a table identifier will be:\nspark_catalog.default.t\n. To restore the le", "question": "What happens when disabling spark.sql.defaultColumn.useNullsForMissingDefaultValues?", "answers": {"text": ["will restore the previous behavior."], "answer_start": [308]}}
{"context": " will include ‘spark_catalog’ as the catalog name if database is defined, e.g. a table identifier will be:\nspark_catalog.default.t\n. To restore the legacy behavior, set\nspark.sql.legacy.v1IdentifierNoCatalog\nto\ntrue\n.\nSince Spark 3.4, when ANSI SQL mode(configuration\nspark.sql.ansi.enabled\n) is on, Spark SQL always returns NULL result on getting a map value with a non-existing key. In Spark 3.3 or earlier, there will be an error.\nSince Spark 3.4, the SQL CLI\nspark-sql\ndoes not print the prefix\nError in query:\nbefore the error message of\nAnalysisException\n.\nSince Spark 3.4,\nsplit\nfunction ignores trailing empty strings when\nregex\nparameter is empty.\nSince Spark 3.4, the\nto_binary\nfunction throws error for a malformed\nstr\ninput. Use\ntry_to_binary\nto tolerate malformed input and return NULL i", "question": "What should be set to true to restore the legacy behavior regarding table identifiers?", "answers": {"text": ["spark.sql.legacy.v1IdentifierNoCatalog"], "answer_start": [169]}}
{"context": "empty.\nSince Spark 3.4, the\nto_binary\nfunction throws error for a malformed\nstr\ninput. Use\ntry_to_binary\nto tolerate malformed input and return NULL instead.\nValid Base64 string should include symbols from in base64 alphabet (A-Za-z0-9+/), optional padding (\n=\n), and optional whitespaces. Whitespaces are skipped in conversion except when they are preceded by padding symbol(s). If padding is present it should conclude the string and follow rules described in RFC 4648 § 4.\nValid hexadecimal strings should include only allowed symbols (0-9A-Fa-f).\nValid values for\nfmt\nare case-insensitive\nhex\n,\nbase64\n,\nutf-8\n,\nutf8\n.\nSince Spark 3.4, Spark throws only\nPartitionsAlreadyExistException\nwhen it creates partitions but some of them exist already. In Spark 3.3 or earlier, Spark can throw either\nPar", "question": "What function should be used to tolerate malformed input when using Spark 3.4 and return NULL instead of throwing an error?", "answers": {"text": ["try_to_binary"], "answer_start": [91]}}
{"context": "a types (array, map and struct). To restore the legacy behavior, set\nspark.sql.orc.enableNestedColumnVectorizedReader\nand\nspark.sql.parquet.enableNestedColumnVectorizedReader\nto\nfalse\n.\nSince Spark 3.4,\nBinaryType\nis not supported in CSV datasource. In Spark 3.3 or earlier, users can write binary columns in CSV datasource, but the output content in CSV files is\nObject.toString()\nwhich is meaningless; meanwhile, if users read CSV tables with binary columns, Spark will throw an\nUnsupported type: binary\nexception.\nSince Spark 3.4, bloom filter joins are enabled by default. To restore the legacy behavior, set\nspark.sql.optimizer.runtime.bloomFilter.enabled\nto\nfalse\n.\nSince Spark 3.4, when schema inference on external Parquet files, INT64 timestamps with annotation\nisAdjustedToUTC=false\nwill be", "question": "What happens when reading CSV tables with binary columns in Spark 3.3 or earlier?", "answers": {"text": ["but the output content in CSV files is\nObject.toString()\nwhich is meaningless; meanwhile, if users read CSV tables with binary columns, Spark will throw an\nUnsupported type: binary\nexception."], "answer_start": [325]}}
{"context": "er.enabled\nto\nfalse\n.\nSince Spark 3.4, when schema inference on external Parquet files, INT64 timestamps with annotation\nisAdjustedToUTC=false\nwill be inferred as TimestampNTZ type instead of Timestamp type. To restore the legacy behavior, set\nspark.sql.parquet.inferTimestampNTZ.enabled\nto\nfalse\n.\nSince Spark 3.4, the behavior for\nCREATE TABLE AS SELECT ...\nis changed from OVERWRITE to APPEND when\nspark.sql.legacy.allowNonEmptyLocationInCTAS\nis set to\ntrue\n. Users are recommended to avoid CTAS with a non-empty table location.\nUpgrading from Spark SQL 3.2 to 3.3\nSince Spark 3.3, the\nhistogram_numeric\nfunction in Spark SQL returns an output type of an array of structs (x, y), where the type of the ‘x’ field in the return value is propagated from the input values consumed in the aggregate fun", "question": "What change occurred in Spark 3.4 regarding INT64 timestamps with annotation isAdjustedToUTC=false when inferring schema on external Parquet files?", "answers": {"text": ["will be inferred as TimestampNTZ type instead of Timestamp type."], "answer_start": [143]}}
{"context": ". After the changes, Spark still recognizes the pattern together with\nDate patterns:\n[+-]yyyy*\n[+-]yyyy*-[m]m\n[+-]yyyy*-[m]m-[d]d\n[+-]yyyy*-[m]m-[d]d\n[+-]yyyy*-[m]m-[d]d *\n[+-]yyyy*-[m]m-[d]dT*\nTimestamp patterns:\n[+-]yyyy*\n[+-]yyyy*-[m]m\n[+-]yyyy*-[m]m-[d]d\n[+-]yyyy*-[m]m-[d]d\n[+-]yyyy*-[m]m-[d]d [h]h:[m]m:[s]s.[ms][ms][ms][us][us][us][zone_id]\n[+-]yyyy*-[m]m-[d]dT[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us][zone_id]\n[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us][zone_id]\nT[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us][zone_id]\nSince Spark 3.3, the\nstrfmt\nin\nformat_string(strfmt, obj, ...)\nand\nprintf(strfmt, obj, ...)\nwill no longer support to use\n0$\nto specify the first argument, the first argument should always reference by\n1$\nwhen use argument index to indicating the position of the argument in the argum", "question": "Quais padrões de data são reconhecidos pelo Spark após as alterações?", "answers": {"text": ["[+-]yyyy*-[m]m-[d]d"], "answer_start": [110]}}
{"context": "y the first argument, the first argument should always reference by\n1$\nwhen use argument index to indicating the position of the argument in the argument list.\nSince Spark 3.3, nulls are written as empty strings in CSV data source by default. In Spark 3.2 or earlier, nulls were written as empty strings as quoted empty strings,\n\"\"\n. To restore the previous behavior, set\nnullValue\nto\n\"\"\n, or set the configuration\nspark.sql.legacy.nullValueWrittenAsQuotedEmptyStringCsv\nto\ntrue\n.\nSince Spark 3.3, DESCRIBE FUNCTION fails if the function does not exist. In Spark 3.2 or earlier, DESCRIBE FUNCTION can still run and print “Function: func_name not found”.\nSince Spark 3.3, the table property\nexternal\nbecomes reserved. Certain commands will fail if you specify the\nexternal\nproperty, such as\nCREATE TAB", "question": "How are nulls written in CSV data source by default since Spark 3.3?", "answers": {"text": ["Since Spark 3.3, nulls are written as empty strings in CSV data source by default."], "answer_start": [160]}}
{"context": "ive only if\nspark.sql.hive.convertMetastoreParquet\nor\nspark.sql.hive.convertMetastoreOrc\nis enabled respectively for Parquet and ORC formats. To restore the behavior before Spark 3.3, you can set\nspark.sql.hive.convertMetastoreInsertDir\nto\nfalse\n.\nSince Spark 3.3, the precision of the return type of round-like functions has been fixed. This may cause Spark throw\nAnalysisException\nof the\nCANNOT_UP_CAST_DATATYPE\nerror class when using views created by prior versions. In such cases, you need to recreate the views using ALTER VIEW AS or CREATE OR REPLACE VIEW AS with newer Spark versions.\nSince Spark 3.3, the\nunbase64\nfunction throws error for a malformed\nstr\ninput. Use\ntry_to_binary(<str>, 'base64')\nto tolerate malformed input and return NULL instead. In Spark 3.2 and earlier, the\nunbase64\nfu", "question": "What can you do to restore the behavior before Spark 3.3 regarding Parquet and ORC formats?", "answers": {"text": ["you can set\nspark.sql.hive.convertMetastoreInsertDir\nto\nfalse"], "answer_start": [184]}}
{"context": "malformed\nstr\ninput. Use\ntry_to_binary(<str>, 'base64')\nto tolerate malformed input and return NULL instead. In Spark 3.2 and earlier, the\nunbase64\nfunction returns a best-efforts result for a malformed\nstr\ninput.\nSince Spark 3.3, when reading Parquet files that were not produced by Spark, Parquet timestamp columns with annotation\nisAdjustedToUTC = false\nare inferred as TIMESTAMP_NTZ type during schema inference. In Spark 3.2 and earlier, these columns are inferred as TIMESTAMP type. To restore the behavior before Spark 3.3, you can set\nspark.sql.parquet.inferTimestampNTZ.enabled\nto\nfalse\n.\nSince Spark 3.3.1 and 3.2.3, for\nSELECT ... GROUP BY a GROUPING SETS (b)\n-style SQL statements,\ngrouping__id\nreturns different values from Apache Spark 3.2.0, 3.2.1, 3.2.2, and 3.3.0. It computes based ", "question": "What function can be used to tolerate malformed input and return NULL instead?", "answers": {"text": ["try_to_binary(<str>, 'base64')"], "answer_start": [25]}}
{"context": "s StringType and the other dialects use LongType.\nSince Spark 3.2, Parquet files with nanosecond precision for timestamp type (\nINT64 (TIMESTAMP(NANOS, true))\n) are not readable. To restore the behavior before Spark 3.2, you can set\nspark.sql.legacy.parquet.nanosAsLong\nto\ntrue\n.\nIn Spark 3.2, PostgreSQL JDBC dialect uses StringType for MONEY and MONEY[] is not supported due to the JDBC driver for PostgreSQL can’t handle those types properly. In Spark 3.1 or earlier, DoubleType and ArrayType of DoubleType are used respectively.\nIn Spark 3.2,\nspark.sql.adaptive.enabled\nis enabled by default. To restore the behavior before Spark 3.2, you can set\nspark.sql.adaptive.enabled\nto\nfalse\n.\nIn Spark 3.2, the following meta-characters are escaped in the\nshow()\naction. In Spark 3.1 or earlier, the foll", "question": "What can you do to restore the behavior before Spark 3.2 regarding Parquet files with nanosecond precision for timestamp type?", "answers": {"text": ["To restore the behavior before Spark 3.2, you can set\nspark.sql.legacy.parquet.nanosAsLong\nto\ntrue\n."], "answer_start": [179]}}
{"context": "\nspark.sql.adaptive.enabled\nto\nfalse\n.\nIn Spark 3.2, the following meta-characters are escaped in the\nshow()\naction. In Spark 3.1 or earlier, the following metacharacters are output as it is.\n\\n\n(new line)\n\\r\n(carriage ret)\n\\t\n(horizontal tab)\n\\f\n(form feed)\n\\b\n(backspace)\n\\u000B\n(vertical tab)\n\\u0007\n(bell)\nIn Spark 3.2,\nALTER TABLE .. RENAME TO PARTITION\nthrows\nPartitionAlreadyExistsException\ninstead of\nAnalysisException\nfor tables from Hive external when the target partition already exists.\nIn Spark 3.2, script transform default FIELD DELIMIT is\n\\u0001\nfor no serde mode, serde property\nfield.delim\nis\n\\t\nfor Hive serde mode when user specifies serde. In Spark 3.1 or earlier, the default FIELD DELIMIT is\n\\t\n, serde property\nfield.delim\nis\n\\u0001\nfor Hive serde mode when user specifies ser", "question": "In Spark 3.2, what exception is thrown by ALTER TABLE .. RENAME TO PARTITION when the target partition already exists?", "answers": {"text": ["PartitionAlreadyExistsException"], "answer_start": [366]}}
{"context": "he old schema with the builtin catalog, you can set\nspark.sql.legacy.keepCommandOutputSchema\nto\ntrue\n.\nIn Spark 3.2, the output schema of\nSHOW TABLE EXTENDED\nbecomes\nnamespace: string, tableName: string, isTemporary: boolean, information: string\n. In Spark 3.1 or earlier, the\nnamespace\nfield was named\ndatabase\nfor the builtin catalog, and no change for the v2 catalogs. To restore the old schema with the builtin catalog, you can set\nspark.sql.legacy.keepCommandOutputSchema\nto\ntrue\n.\nIn Spark 3.2, the output schema of\nSHOW TBLPROPERTIES\nbecomes\nkey: string, value: string\nwhether you specify the table property key or not. In Spark 3.1 and earlier, the output schema of\nSHOW TBLPROPERTIES\nis\nvalue: string\nwhen you specify the table property key. To restore the old schema with the builtin catalo", "question": "What is the output schema of SHOW TBLPROPERTIES in Spark 3.2?", "answers": {"text": ["key: string, value: string"], "answer_start": [549]}}
{"context": "r, the output schema of\nSHOW TBLPROPERTIES\nis\nvalue: string\nwhen you specify the table property key. To restore the old schema with the builtin catalog, you can set\nspark.sql.legacy.keepCommandOutputSchema\nto\ntrue\n.\nIn Spark 3.2, the output schema of\nDESCRIBE NAMESPACE\nbecomes\ninfo_name: string, info_value: string\n. In Spark 3.1 or earlier, the\ninfo_name\nfield was named\ndatabase_description_item\nand the\ninfo_value\nfield was named\ndatabase_description_value\nfor the builtin catalog. To restore the old schema with the builtin catalog, you can set\nspark.sql.legacy.keepCommandOutputSchema\nto\ntrue\n.\nIn Spark 3.2, table refreshing clears cached data of the table as well as of all its dependents such as views while keeping the dependents cached. The following commands perform table refreshing:\nALT", "question": "What can you set to restore the old schema with the builtin catalog?", "answers": {"text": ["true"], "answer_start": [209]}}
{"context": " column name having a dot in the name (not nested) needs to be escaped with backtick `. Now, it throws\nAnalysisException\nif the column is not found in the data frame schema. It also throws\nIllegalArgumentException\nif the input column name is a nested column. In Spark 3.1 and earlier, it used to ignore invalid input column name and nested column name.\nIn Spark 3.2, the dates subtraction expression such as\ndate1 - date2\nreturns values of\nDayTimeIntervalType\n. In Spark 3.1 and earlier, the returned type is\nCalendarIntervalType\n. To restore the behavior before Spark 3.2, you can set\nspark.sql.legacy.interval.enabled\nto\ntrue\n.\nIn Spark 3.2, the timestamps subtraction expression such as\ntimestamp '2021-03-31 23:48:00' - timestamp '2021-01-01 00:00:00'\nreturns values of\nDayTimeIntervalType\n. In S", "question": "What type does date subtraction return in Spark 3.2?", "answers": {"text": ["DayTimeIntervalType"], "answer_start": [440]}}
{"context": "ilently removed, for example:\nTBLPROPERTIES('owner'='yao')\nwill have no effect. In Spark version 3.1 and below, the reserved properties can be used in\nCREATE TABLE .. LIKE ..\ncommand but have no side effects, for example,\nTBLPROPERTIES('location'='/tmp')\ndoes not change the location of the table but only create a headless property just like\n'a'='b'\n.\nIn Spark 3.2,\nTRANSFORM\noperator can’t support alias in inputs. In Spark 3.1 and earlier, we can write script transform like\nSELECT TRANSFORM(a AS c1, b AS c2) USING 'cat' FROM TBL\n.\nIn Spark 3.2,\nTRANSFORM\noperator can support\nArrayType/MapType/StructType\nwithout Hive SerDe, in this mode, we use\nStructsToJson\nto convert\nArrayType/MapType/StructType\ncolumn to\nSTRING\nand use\nJsonToStructs\nto parse\nSTRING\nto\nArrayType/MapType/StructType\n. In Spa", "question": "In Spark 3.2, what does the TRANSFORM operator support without Hive SerDe?", "answers": {"text": ["ArrayType/MapType/StructType"], "answer_start": [581]}}
{"context": "\nStructsToJson\nto convert\nArrayType/MapType/StructType\ncolumn to\nSTRING\nand use\nJsonToStructs\nto parse\nSTRING\nto\nArrayType/MapType/StructType\n. In Spark 3.1, Spark just support case\nArrayType/MapType/StructType\ncolumn as\nSTRING\nbut can’t support parse\nSTRING\nto\nArrayType/MapType/StructType\noutput columns.\nIn Spark 3.2, the unit-to-unit interval literals like\nINTERVAL '1-1' YEAR TO MONTH\nand the unit list interval literals like\nINTERVAL '3' DAYS '1' HOUR\nare converted to ANSI interval types:\nYearMonthIntervalType\nor\nDayTimeIntervalType\n. In Spark 3.1 and earlier, such interval literals are converted to\nCalendarIntervalType\n. To restore the behavior before Spark 3.2, you can set\nspark.sql.legacy.interval.enabled\nto\ntrue\n.\nIn Spark 3.2, the unit list interval literals can not mix year-month f", "question": "In Spark 3.1 and earlier, what type are interval literals converted to?", "answers": {"text": ["CalendarIntervalType"], "answer_start": [609]}}
{"context": "rent between Hive\nSERDE\nmode and\nROW FORMAT DELIMITED\nmode when these two types are used as inputs. In Hive\nSERDE\nmode,\nDayTimeIntervalType\ncolumn is converted to\nHiveIntervalDayTime\n, its string format is\n[-]?d h:m:s.n\n, but in\nROW FORMAT DELIMITED\nmode the format is\nINTERVAL '[-]?d h:m:s.n' DAY TO TIME\n. In Hive\nSERDE\nmode,\nYearMonthIntervalType\ncolumn is converted to\nHiveIntervalYearMonth\n, its string format is\n[-]?y-m\n, but in\nROW FORMAT DELIMITED\nmode the format is\nINTERVAL '[-]?y-m' YEAR TO MONTH\n.\nIn Spark 3.2,\nhash(0) == hash(-0)\nfor floating point types. Previously, different values were generated.\nIn Spark 3.2,\nCREATE TABLE AS SELECT\nwith non-empty\nLOCATION\nwill throw\nAnalysisException\n. To restore the behavior before Spark 3.2, you can set\nspark.sql.legacy.allowNonEmptyLocationI", "question": "Qual é o formato de string para DayTimeIntervalType em ROW FORMAT DELIMITED mode?", "answers": {"text": ["INTERVAL '[-]?d h:m:s.n' DAY TO TIME"], "answer_start": [269]}}
{"context": ", cast(c as date))\n.\nIn Spark 3.2,\nFloatType\nis mapped to\nFLOAT\nin MySQL. Prior to this, it used to be mapped to\nREAL\n, which is by default a synonym to\nDOUBLE PRECISION\nin MySQL.\nIn Spark 3.2, the query executions triggered by\nDataFrameWriter\nare always named\ncommand\nwhen being sent to\nQueryExecutionListener\n. In Spark 3.1 and earlier, the name is one of\nsave\n,\ninsertInto\n,\nsaveAsTable\n.\nIn Spark 3.2,\nDataset.unionByName\nwith\nallowMissingColumns\nset to true will add missing nested fields to the end of structs. In Spark 3.1, nested struct fields are sorted alphabetically.\nIn Spark 3.2, create/alter view will fail if the input query output columns contain auto-generated alias. This is necessary to make sure the query output column names are stable across different spark versions. To restore", "question": "In Spark 3.2, what is FloatType mapped to in MySQL?", "answers": {"text": ["FLOAT"], "answer_start": [58]}}
{"context": "for the query plan explain results. To restore the behavior before Spark 3.1, you can set\nspark.sql.ui.explainMode\nto\nextended\n.\nIn Spark 3.1,\nfrom_unixtime\n,\nunix_timestamp\n,\nto_unix_timestamp\n,\nto_timestamp\nand\nto_date\nwill fail if the specified datetime pattern is invalid. In Spark 3.0 or earlier, they result\nNULL\n.\nIn Spark 3.1, the Parquet, ORC, Avro and JSON datasources throw the exception\norg.apache.spark.sql.AnalysisException: Found duplicate column(s) in the data schema\nin read if they detect duplicate names in top-level columns as well in nested structures. The datasources take into account the SQL config\nspark.sql.caseSensitive\nwhile detecting column name duplicates.\nIn Spark 3.1, structs and maps are wrapped by the\n{}\nbrackets in casting them to strings. For instance, the\nshow(", "question": "What happens in Spark 3.1 if the datetime pattern is invalid when using functions like from_unixtime?", "answers": {"text": ["will fail if the specified datetime pattern is invalid."], "answer_start": [221]}}
{"context": "lexTypesToString.enabled\nto\ntrue\n.\nIn Spark 3.1, when\nspark.sql.ansi.enabled\nis false, Spark always returns null if the sum of decimal type column overflows. In Spark 3.0 or earlier, in the case, the sum of decimal type column may return null or incorrect result, or even fails at runtime (depending on the actual query plan execution).\nIn Spark 3.1,\npath\noption cannot coexist when the following methods are called with path parameter(s):\nDataFrameReader.load()\n,\nDataFrameWriter.save()\n,\nDataStreamReader.load()\n, or\nDataStreamWriter.start()\n. In addition,\npaths\noption cannot coexist for\nDataFrameReader.load()\n. For example,\nspark.read.format(\"csv\").option(\"path\", \"/tmp\").load(\"/tmp2\")\nor\nspark.read.option(\"path\", \"/tmp\").csv(\"/tmp2\")\nwill throw\norg.apache.spark.sql.AnalysisException\n. In Spar", "question": "What happens in Spark 3.1 when spark.sql.ansi.enabled is false and the sum of a decimal type column overflows?", "answers": {"text": ["Spark always returns null if the sum of decimal type column overflows."], "answer_start": [87]}}
{"context": "ult in\nNULL\ns.\nIn Spark 3.1, we remove the built-in Hive 1.2. You need to migrate your custom SerDes to Hive 2.3. See\nHIVE-15167\nfor more details.\nIn Spark 3.1, loading and saving of timestamps from/to parquet files fails if the timestamps are before 1900-01-01 00:00:00Z, and loaded (saved) as the INT96 type. In Spark 3.0, the actions don’t fail but might lead to shifting of the input timestamps due to rebasing from/to Julian to/from Proleptic Gregorian calendar. To restore the behavior before Spark 3.1, you can set\nspark.sql.legacy.parquet.int96RebaseModeInRead\nor/and\nspark.sql.legacy.parquet.int96RebaseModeInWrite\nto\nLEGACY\n.\nIn Spark 3.1, the\nschema_of_json\nand\nschema_of_csv\nfunctions return the schema in the SQL format in which field names are quoted. In Spark 3.0, the function returns", "question": "What change was made regarding Hive 1.2 in Spark 3.1?", "answers": {"text": ["In Spark 3.1, we remove the built-in Hive 1.2."], "answer_start": [15]}}
{"context": "the\nschema_of_json\nand\nschema_of_csv\nfunctions return the schema in the SQL format in which field names are quoted. In Spark 3.0, the function returns a catalog string without field quoting and in lower case.\nIn Spark 3.1, refreshing a table will trigger an uncache operation for all other caches that reference the table, even if the table itself is not cached. In Spark 3.0 the operation will only be triggered if the table itself is cached.\nIn Spark 3.1, creating or altering a permanent view will capture runtime SQL configs and store them as view properties. These configs will be applied during the parsing and analysis phases of the view resolution. To restore the behavior before Spark 3.1, you can set\nspark.sql.legacy.useCurrentConfigsForView\nto\ntrue\n.\nIn Spark 3.1, the temporary view will", "question": "What happens when refreshing a table in Spark 3.1?", "answers": {"text": ["refreshing a table will trigger an uncache operation for all other caches that reference the table, even if the table itself is not cached."], "answer_start": [223]}}
{"context": "ution. To restore the behavior before Spark 3.1, you can set\nspark.sql.legacy.useCurrentConfigsForView\nto\ntrue\n.\nIn Spark 3.1, the temporary view will have same behaviors with the permanent view, i.e. capture and store runtime SQL configs, SQL text, catalog and namespace. The captured view properties will be applied during the parsing and analysis phases of the view resolution. To restore the behavior before Spark 3.1, you can set\nspark.sql.legacy.storeAnalyzedPlanForView\nto\ntrue\n.\nIn Spark 3.1, temporary view created via\nCACHE TABLE ... AS SELECT\nwill also have the same behavior with permanent view. In particular, when the temporary view is dropped, Spark will invalidate all its cache dependents, as well as the cache for the temporary view itself. This is different from Spark 3.0 and belo", "question": "How can you restore the behavior before Spark 3.1 regarding runtime SQL configs?", "answers": {"text": ["To restore the behavior before Spark 3.1, you can set\nspark.sql.legacy.useCurrentConfigsForView\nto\ntrue\n."], "answer_start": [7]}}
{"context": "as STRING types and ignores a length parameter, e.g.\nCHAR(4)\n, you can set\nspark.sql.legacy.charVarcharAsString\nto\ntrue\n.\nIn Spark 3.1,\nAnalysisException\nis replaced by its sub-classes that are thrown for tables from Hive external catalog in the following situations:\nALTER TABLE .. ADD PARTITION\nthrows\nPartitionsAlreadyExistException\nif new partition exists already\nALTER TABLE .. DROP PARTITION\nthrows\nNoSuchPartitionsException\nfor not existing partitions\nUpgrading from Spark SQL 3.0.1 to 3.0.2\nIn Spark 3.0.2,\nAnalysisException\nis replaced by its sub-classes that are thrown for tables from Hive external catalog in the following situations:\nALTER TABLE .. ADD PARTITION\nthrows\nPartitionsAlreadyExistException\nif new partition exists already\nALTER TABLE .. DROP PARTITION\nthrows\nNoSuchPartitions", "question": "What exception is thrown when attempting to add a partition that already exists in Spark 3.1?", "answers": {"text": ["PartitionsAlreadyExistException"], "answer_start": [304]}}
{"context": "ER TABLE .. ADD PARTITION\nthrows\nPartitionsAlreadyExistException\nif new partition exists already\nALTER TABLE .. DROP PARTITION\nthrows\nNoSuchPartitionsException\nfor not existing partitions\nIn Spark 3.0.2,\nPARTITION(col=null)\nis always parsed as a null literal in the partition spec. In Spark 3.0.1 or earlier, it is parsed as a string literal of its text representation, e.g., string “null”, if the partition column is string type. To restore the legacy behavior, you can set\nspark.sql.legacy.parseNullPartitionSpecAsStringLiteral\nas true.\nIn Spark 3.0.2, the output schema of\nSHOW DATABASES\nbecomes\nnamespace: string\n. In Spark version 3.0.1 and earlier, the schema was\ndatabaseName: string\n. Since Spark 3.0.2, you can restore the old schema by setting\nspark.sql.legacy.keepCommandOutputSchema\nto\ntr", "question": "In Spark 3.0.2, what is the schema of the output of SHOW DATABASES?", "answers": {"text": ["namespace: string"], "answer_start": [599]}}
{"context": "array, etc. This is counterintuitive and makes the schema of aggregation queries unexpected. For example, the schema of\nds.groupByKey(...).count()\nis\n(value, count)\n. Since Spark 3.0, we name the grouping attribute to “key”. The old behavior is preserved under a newly added configuration\nspark.sql.legacy.dataset.nameNonStructGroupingKeyAsValue\nwith a default value of\nfalse\n.\nIn Spark 3.0, the column metadata will always be propagated in the API\nColumn.name\nand\nColumn.as\n. In Spark version 2.4 and earlier, the metadata of\nNamedExpression\nis set as the\nexplicitMetadata\nfor the new column at the time the API is called, it won’t change even if the underlying\nNamedExpression\nchanges metadata. To restore the behavior before Spark 3.0, you can use the API\nas(alias: String, metadata: Metadata)\nwit", "question": "What is the default value of the configuration spark.sql.legacy.dataset.nameNonStructGroupingKeyAsValue?", "answers": {"text": ["false"], "answer_start": [370]}}
{"context": "e underlying\nNamedExpression\nchanges metadata. To restore the behavior before Spark 3.0, you can use the API\nas(alias: String, metadata: Metadata)\nwith explicit metadata.\nWhen turning a Dataset to another Dataset, Spark will up cast the fields in the original Dataset to the type of corresponding fields in the target DataSet. In version 2.4 and earlier, this up cast is not very strict, e.g.\nSeq(\"str\").toDS.as[Int]\nfails, but\nSeq(\"str\").toDS.as[Boolean]\nworks and throw NPE during execution. In Spark 3.0, the up cast is stricter and turning String into something else is not allowed, i.e.\nSeq(\"str\").toDS.as[Boolean]\nwill fail during analysis. To restore the behavior before Spark 3.0, set\nspark.sql.legacy.doLooseUpcast\nto\ntrue\n.\nDDL Statements\nIn Spark 3.0, when inserting a value into a table c", "question": "How can you restore the behavior of upcasting as it was before Spark 3.0?", "answers": {"text": ["To restore the behavior before Spark 3.0, set\nspark.sql.legacy.doLooseUpcast\nto\ntrue\n."], "answer_start": [647]}}
{"context": "restore the behavior before Spark 3.0, set\nspark.sql.legacy.doLooseUpcast\nto\ntrue\n.\nDDL Statements\nIn Spark 3.0, when inserting a value into a table column with a different data type, the type coercion is performed as per ANSI SQL standard. Certain unreasonable type conversions such as converting\nstring\nto\nint\nand\ndouble\nto\nboolean\nare disallowed. A runtime exception is thrown if the value is out-of-range for the data type of the column. In Spark version 2.4 and below, type conversions during table insertion are allowed as long as they are valid\nCast\n. When inserting an out-of-range value to an integral field, the low-order bits of the value is inserted(the same as Java/Scala numeric type casting). For example, if 257 is inserted to a field of byte type, the result is 1. The behavior is co", "question": "What happened when inserting an out-of-range value to an integral field in Spark version 2.4 and below?", "answers": {"text": ["When inserting an out-of-range value to an integral field, the low-order bits of the value is inserted(the same as Java/Scala numeric type casting)."], "answer_start": [559]}}
{"context": "is inserted(the same as Java/Scala numeric type casting). For example, if 257 is inserted to a field of byte type, the result is 1. The behavior is controlled by the option\nspark.sql.storeAssignmentPolicy\n, with a default value as “ANSI”. Setting the option as “Legacy” restores the previous behavior.\nThe\nADD JAR\ncommand previously returned a result set with the single value 0. It now returns an empty result set.\nSpark 2.4 and below: the\nSET\ncommand works without any warnings even if the specified key is for\nSparkConf\nentries and it has no effect because the command does not update\nSparkConf\n, but the behavior might confuse users. In 3.0, the command fails if a\nSparkConf\nkey is used. You can disable such a check by setting\nspark.sql.legacy.setCommandRejectsSparkCoreConfs\nto\nfalse\n.\nRefreshi", "question": "What happens if 257 is inserted to a field of byte type?", "answers": {"text": ["the result is 1"], "answer_start": [115]}}
{"context": "to\ntrue\n.\nIn Spark 3.0,\nSHOW TBLPROPERTIES\nthrows\nAnalysisException\nif the table does not exist. In Spark version 2.4 and below, this scenario caused\nNoSuchTableException\n.\nIn Spark 3.0,\nSHOW CREATE TABLE table_identifier\nalways returns Spark DDL, even when the given table is a Hive SerDe table. For generating Hive DDL, use\nSHOW CREATE TABLE table_identifier AS SERDE\ncommand instead.\nIn Spark 3.0, column of CHAR type is not allowed in non-Hive-Serde tables, and CREATE/ALTER TABLE commands will fail if CHAR type is detected. Please use STRING type instead. In Spark version 2.4 and below, CHAR type is treated as STRING type and the length parameter is simply ignored.\nUDFs and Built-in Functions\nIn Spark 3.0, the\ndate_add\nand\ndate_sub\nfunctions accepts only int, smallint, tinyint as the 2nd a", "question": "What exception is thrown by SHOW TBLPROPERTIES in Spark 3.0 if the table does not exist?", "answers": {"text": ["AnalysisException"], "answer_start": [50]}}
{"context": "eter is simply ignored.\nUDFs and Built-in Functions\nIn Spark 3.0, the\ndate_add\nand\ndate_sub\nfunctions accepts only int, smallint, tinyint as the 2nd argument; fractional and non-literal strings are not valid anymore, for example:\ndate_add(cast('1964-05-23' as date), '12.34')\ncauses\nAnalysisException\n. Note that, string literals are still allowed, but Spark will throw\nAnalysisException\nif the string content is not a valid integer. In Spark version 2.4 and below, if the 2nd argument is fractional or string value, it is coerced to int value, and the result is a date value of\n1964-06-04\n.\nIn Spark 3.0, the function\npercentile_approx\nand its alias\napprox_percentile\nonly accept integral value with range in\n[1, 2147483647]\nas its 3rd argument\naccuracy\n, fractional and string types are disallowed,", "question": "In Spark 3.0, what types are accepted as the 2nd argument for the date_add and date_sub functions?", "answers": {"text": ["int, smallint, tinyint"], "answer_start": [115]}}
{"context": "\napprox_percentile\nonly accept integral value with range in\n[1, 2147483647]\nas its 3rd argument\naccuracy\n, fractional and string types are disallowed, for example,\npercentile_approx(10.0, 0.2, 1.8D)\ncauses\nAnalysisException\n. In Spark version 2.4 and below, if\naccuracy\nis fractional or string value, it is coerced to an int value,\npercentile_approx(10.0, 0.2, 1.8D)\nis operated as\npercentile_approx(10.0, 0.2, 1)\nwhich results in\n10.0\n.\nIn Spark 3.0, an analysis exception is thrown when hash expressions are applied on elements of\nMapType\n. To restore the behavior before Spark 3.0, set\nspark.sql.legacy.allowHashOnMapType\nto\ntrue\n.\nIn Spark 3.0, when the\narray\n/\nmap\nfunction is called without any parameters, it returns an empty collection with\nNullType\nas element type. In Spark version 2.4 and ", "question": "What happens in Spark version 2.4 and below if the accuracy is a fractional or string value?", "answers": {"text": ["it is coerced to an int value"], "answer_start": [301]}}
{"context": " For example, the JSON string\n{\"a\" 1}\nwith the schema\na INT\nis converted to\nnull\nby previous versions but Spark 3.0 converts it to\nRow(null)\n.\nIn Spark version 2.4 and below, you can create map values with map type key via built-in function such as\nCreateMap\n,\nMapFromArrays\n, etc. In Spark 3.0, it’s not allowed to create map values with map type key with these built-in functions. Users can use\nmap_entries\nfunction to convert map to array<struct<key, value» as a workaround. In addition, users can still read map values with map type key from data source or Java/Scala collections, though it is discouraged.\nIn Spark version 2.4 and below, you can create a map with duplicated keys via built-in functions like\nCreateMap\n,\nStringToMap\n, etc. The behavior of map with duplicated keys is undefined, f", "question": "What does Spark 3.0 convert the JSON string {\"a\" 1} with the schema a INT to?", "answers": {"text": ["Row(null)"], "answer_start": [131]}}
{"context": "using\norg.apache.spark.sql.functions.udf(AnyRef, DataType)\nis not allowed by default. Remove the return type parameter to automatically switch to typed Scala udf is recommended, or set\nspark.sql.legacy.allowUntypedScalaUDF\nto true to keep using it. In Spark version 2.4 and below, if\norg.apache.spark.sql.functions.udf(AnyRef, DataType)\ngets a Scala closure with primitive-type argument, the returned UDF returns null if the input values is null. However, in Spark 3.0, the UDF returns the default value of the Java type if the input value is null. For example,\nval f = udf((x: Int) => x, IntegerType)\n,\nf($\"x\")\nreturns null in Spark 2.4 and below if column\nx\nis null, and return 0 in Spark 3.0. This behavior change is introduced because Spark 3.0 is built with Scala 2.12 by default.\nIn Spark 3.0, ", "question": "What happens when org.apache.spark.sql.functions.udf(AnyRef, DataType) gets a Scala closure with a primitive-type argument and the input value is null in Spark 2.4 and below?", "answers": {"text": ["the returned UDF returns null if the input values is null"], "answer_start": [388]}}
{"context": "example,\nlog(3.0)\n, whose value varies between\nMath.log()\nand\nStrictMath.log()\n.\nIn Spark 3.0, the\ncast\nfunction processes string literals such as ‘Infinity’, ‘+Infinity’, ‘-Infinity’, ‘NaN’, ‘Inf’, ‘+Inf’, ‘-Inf’ in a case-insensitive manner when casting the literals to\nDouble\nor\nFloat\ntype to ensure greater compatibility with other database systems. This behavior change is illustrated in the table below:\nOperation\nResult before Spark 3.0\nResult in Spark 3.0\nCAST(‘infinity’ AS DOUBLE)\nNULL\nDouble.PositiveInfinity\nCAST(‘+infinity’ AS DOUBLE)\nNULL\nDouble.PositiveInfinity\nCAST(‘inf’ AS DOUBLE)\nNULL\nDouble.PositiveInfinity\nCAST(‘inf’ AS DOUBLE)\nNULL\nDouble.PositiveInfinity\nCAST(‘-infinity’ AS DOUBLE)\nNULL\nDouble.NegativeInfinity\nCAST(‘-inf’ AS DOUBLE)\nNULL\nDouble.NegativeInfinity\nCAST(‘infini", "question": "How does Spark 3.0 handle string literals like 'infinity' when casting to DOUBLE?", "answers": {"text": ["NULL\nDouble.PositiveInfinity"], "answer_start": [491]}}
{"context": "NULL\nDouble.PositiveInfinity\nCAST(‘-infinity’ AS DOUBLE)\nNULL\nDouble.NegativeInfinity\nCAST(‘-inf’ AS DOUBLE)\nNULL\nDouble.NegativeInfinity\nCAST(‘infinity’ AS FLOAT)\nNULL\nFloat.PositiveInfinity\nCAST(‘+infinity’ AS FLOAT)\nNULL\nFloat.PositiveInfinity\nCAST(‘inf’ AS FLOAT)\nNULL\nFloat.PositiveInfinity\nCAST(‘+inf’ AS FLOAT)\nNULL\nFloat.PositiveInfinity\nCAST(‘-infinity’ AS FLOAT)\nNULL\nFloat.NegativeInfinity\nCAST(‘-inf’ AS FLOAT)\nNULL\nFloat.NegativeInfinity\nCAST(‘nan’ AS DOUBLE)\nNULL\nDouble.NaN\nCAST(‘nan’ AS FLOAT)\nNULL\nFloat.NaN\nIn Spark 3.0, when casting interval values to string type, there is no “interval” prefix, for example,\n1 days 2 hours\n. In Spark version 2.4 and below, the string contains the “interval” prefix like\ninterval 1 days 2 hours\n.\nIn Spark 3.0, when casting string value to integra", "question": "Como os valores de intervalo são convertidos em string no Spark 3.0?", "answers": {"text": ["In Spark 3.0, when casting interval values to string type, there is no “interval” prefix, for example,\n1 days 2 hours\n."], "answer_start": [525]}}
{"context": "ark version 2.4 and below, the string contains the “interval” prefix like\ninterval 1 days 2 hours\n.\nIn Spark 3.0, when casting string value to integral types(tinyint, smallint, int and bigint), datetime types(date, timestamp and interval) and boolean type, the leading and trailing whitespaces (<= ASCII 32) will be trimmed before converted to these type values, for example,\ncast(' 1\\t' as int)\nresults\n1\n,\ncast(' 1\\t' as boolean)\nresults\ntrue\n,\ncast('2019-10-10\\t as date)\nresults the date value\n2019-10-10\n. In Spark version 2.4 and below, when casting string to integrals and booleans, it does not trim the whitespaces from both ends; the foregoing results is\nnull\n, while to datetimes, only the trailing spaces (= ASCII 32) are removed.\nQuery Engine\nIn Spark version 2.4 and below, SQL queries s", "question": "In Spark 3.0, what happens to leading and trailing whitespaces when casting a string value to integral, datetime, or boolean types?", "answers": {"text": ["the leading and trailing whitespaces (<= ASCII 32) will be trimmed before converted to these type values"], "answer_start": [257]}}
{"context": "f2(\"a\"))\nreturns an empty result which is quite confusing. This is because Spark cannot resolve Dataset column references that point to tables being self joined, and\ndf1(\"a\")\nis exactly the same as\ndf2(\"a\")\nin Spark. To restore the behavior before Spark 3.0, you can set\nspark.sql.analyzer.failAmbiguousSelfJoin\nto\nfalse\n.\nIn Spark 3.0,\nspark.sql.legacy.ctePrecedencePolicy\nis introduced to control the behavior for name conflicting in the nested WITH clause. By default value\nEXCEPTION\n, Spark throws an AnalysisException, it forces users to choose the specific substitution order they wanted. If set to\nCORRECTED\n(which is recommended), inner CTE definitions take precedence over outer definitions. For example, set the config to\nfalse\n,\nWITH t AS (SELECT 1), t2 AS (WITH t AS (SELECT 2) SELECT * F", "question": "What can be done to restore the behavior before Spark 3.0 when Spark cannot resolve Dataset column references that point to tables being self joined?", "answers": {"text": ["To restore the behavior before Spark 3.0, you can set\nspark.sql.analyzer.failAmbiguousSelfJoin\nto\nfalse"], "answer_start": [217]}}
{"context": "efinitions take precedence over outer definitions. For example, set the config to\nfalse\n,\nWITH t AS (SELECT 1), t2 AS (WITH t AS (SELECT 2) SELECT * FROM t) SELECT * FROM t2\nreturns\n2\n, while setting it to\nLEGACY\n, the result is\n1\nwhich is the behavior in version 2.4 and below.\nIn Spark 3.0, configuration\nspark.sql.crossJoin.enabled\nbecome internal configuration, and is true by default, so by default spark won’t raise exception on sql with implicit cross join.\nIn Spark version 2.4 and below, float/double -0.0 is semantically equal to 0.0, but -0.0 and 0.0 are considered as different values when used in aggregate grouping keys, window partition keys, and join keys. In Spark 3.0, this bug is fixed. For example,\nSeq(-0.0, 0.0).toDF(\"d\").groupBy(\"d\").count()\nreturns\n[(0.0, 2)]\nin Spark 3.0, an", "question": "What is the result of `Seq(-0.0, 0.0).toDF(\"d\").groupBy(\"d\").count()` in Spark 3.0?", "answers": {"text": ["[(0.0, 2)]"], "answer_start": [773]}}
{"context": "I classes from the\njava.time\npackages that are based on\nISO chronology\n. In Spark version 2.4 and below, those operations are performed using the hybrid calendar (\nJulian + Gregorian\n. The changes impact on the results for dates before October 15, 1582 (Gregorian) and affect on the following Spark 3.0 API:\nParsing/formatting of timestamp/date strings. This effects on CSV/JSON datasources and on the\nunix_timestamp\n,\ndate_format\n,\nto_unix_timestamp\n,\nfrom_unixtime\n,\nto_date\n,\nto_timestamp\nfunctions when patterns specified by users is used for parsing and formatting. In Spark 3.0, we define our own pattern strings in\nDatetime Patterns for Formatting and Parsing\n,\n which is implemented via\nDateTimeFormatter\nunder the hood. New implementation performs strict checking of its input. For example, ", "question": "Which Spark API is affected by changes related to date handling before October 15, 1582?", "answers": {"text": ["Parsing/formatting of timestamp/date strings"], "answer_start": [308]}}
{"context": "arserPolicy\nto\nLEGACY\n.\nThe\nweekofyear\n,\nweekday\n,\ndayofweek\n,\ndate_trunc\n,\nfrom_utc_timestamp\n,\nto_utc_timestamp\n, and\nunix_timestamp\nfunctions use java.time API for calculation week number of year, day number of week as well for conversion from/to TimestampType values in UTC time zone.\nThe JDBC options\nlowerBound\nand\nupperBound\nare converted to TimestampType/DateType values in the same way as casting strings to TimestampType/DateType values. The conversion is based on Proleptic Gregorian calendar, and time zone defined by the SQL config\nspark.sql.session.timeZone\n. In Spark version 2.4 and below, the conversion is based on the hybrid calendar (Julian + Gregorian) and on default system time zone.\nFormatting\nTIMESTAMP\nand\nDATE\nliterals.\nCreating typed\nTIMESTAMP\nand\nDATE\nliterals from strin", "question": "Which API is used by the weekofyear, weekday, dayofweek, date_trunc, from_utc_timestamp, to_utc_timestamp, and unix_timestamp functions for calculations?", "answers": {"text": ["java.time API for calculation week number of year, day number of week as well for conversion from/to TimestampType values in UTC time zone."], "answer_start": [149]}}
{"context": "midnight yesterday\ntomorrow [zoneId]\n- midnight tomorrow\nnow\n- current query start time\nFor example\nSELECT timestamp 'tomorrow';\n.\nSince Spark 3.0, when using\nEXTRACT\nexpression to extract the second field from date/timestamp values, the result will be a\nDecimalType(8, 6)\nvalue with 2 digits for second part, and 6 digits for the fractional part with microsecond precision. e.g.\nextract(second from to_timestamp('2019-09-20 10:10:10.1'))\nresults\n10.100000\n.  In Spark version 2.4 and earlier, it returns an\nIntegerType\nvalue and the result for the former example is\n10\n.\nIn Spark 3.0, datetime pattern letter\nF\nis\naligned day of week in month\nthat represents the concept of the count of days within the period of a week where the weeks are aligned to the start of the month. In Spark version 2.4 and", "question": "What type of value does the EXTRACT expression return in Spark 3.0 when extracting the second field from date/timestamp values?", "answers": {"text": ["DecimalType(8, 6)"], "answer_start": [255]}}
{"context": "epresents the concept of the count of days within the period of a week where the weeks are aligned to the start of the month. In Spark version 2.4 and earlier, it is\nweek of month\nthat represents the concept of the count of weeks within the month where weeks start on a fixed day-of-week, e.g.\n2020-07-30\nis 30 days (4 weeks and 2 days) after the first day of the month, so\ndate_format(date '2020-07-30', 'F')\nreturns 2 in Spark 3.0, but as a week count in Spark 2.x, it returns 5 because it locates in the 5th week of July 2020, where week one is 2020-07-01 to 07-04.\nIn Spark 3.0, Spark will try to use built-in data source writer instead of Hive serde in\nCTAS\n. This behavior is effective only if\nspark.sql.hive.convertMetastoreParquet\nor\nspark.sql.hive.convertMetastoreOrc\nis enabled respectively", "question": "In Spark 3.0, what does date_format(date '2020-07-30', 'F') return?", "answers": {"text": ["returns 2 in Spark 3.0"], "answer_start": [410]}}
{"context": "before Spark 3.0, you can set\nspark.sql.hive.convertInsertingPartitionedTable\nto\nfalse\n.\nData Sources\nIn Spark version 2.4 and below, when reading a Hive SerDe table with Spark native data sources(parquet/orc), Spark infers the actual file schema and update the table schema in metastore. In Spark 3.0, Spark doesn’t infer the schema anymore. This should not cause any problems to end users, but if it does, set\nspark.sql.hive.caseSensitiveInferenceMode\nto\nINFER_AND_SAVE\n.\nIn Spark version 2.4 and below, partition column value is converted as null if it can’t be casted to corresponding user provided schema. In 3.0, partition column value is validated with user provided schema. An exception is thrown if the validation fails. You can disable such validation by setting\nspark.sql.sources.validateP", "question": "What should you set spark.sql.hive.caseSensitiveInferenceMode to if Spark 3.0 not inferring the schema causes problems?", "answers": {"text": ["INFER_AND_SAVE"], "answer_start": [457]}}
{"context": "stampFormat\n. Set JSON option\ninferTimestamp\nto\nfalse\nto disable such type inference.\nIn Spark version 2.4 and below, CSV datasource converts a malformed CSV string to a row with all\nnull\ns in the PERMISSIVE mode. In Spark 3.0, the returned row can contain non-\nnull\nfields if some of CSV column values were parsed and converted to desired types successfully.\nIn Spark 3.0, when Avro files are written with user provided schema, the fields are matched by field names between catalyst schema and Avro schema instead of positions.\nIn Spark 3.0, when Avro files are written with user provided non-nullable schema, even the catalyst schema is nullable, Spark is still able to write the files. However, Spark throws runtime NullPointerException if any of the records contains null.\nIn Spark version 2.4 an", "question": "What happens in Spark 3.0 when Avro files are written with a user-provided schema?", "answers": {"text": ["In Spark 3.0, when Avro files are written with user provided schema, the fields are matched by field names between catalyst schema and Avro schema instead of positions."], "answer_start": [360]}}
{"context": "the Hive metastore you want to connect to. For example: set\nspark.sql.hive.metastore.version\nto\n1.2.1\nand\nspark.sql.hive.metastore.jars\nto\nmaven\nif your Hive metastore version is 1.2.1.\nYou need to migrate your custom SerDes to Hive 2.3 or build your own Spark with\nhive-1.2\nprofile. See\nHIVE-15167\nfor more details.\nThe decimal string representation can be different between Hive 1.2 and Hive 2.3 when using\nTRANSFORM\noperator in SQL for script transformation, which depends on hive’s behavior. In Hive 1.2, the string representation omits trailing zeroes. But in Hive 2.3, it is always padded to 18 digits with trailing zeroes if necessary.\nUpgrading from Spark SQL 2.4.7 to 2.4.8\nIn Spark 2.4.8,\nAnalysisException\nis replaced by its sub-classes that are thrown for tables from Hive external catalo", "question": "What should you set spark.sql.hive.metastore.version to if your Hive metastore version is 1.2.1?", "answers": {"text": ["1.2.1"], "answer_start": [96]}}
{"context": "ng from Spark SQL 2.4.7 to 2.4.8\nIn Spark 2.4.8,\nAnalysisException\nis replaced by its sub-classes that are thrown for tables from Hive external catalog in the following situations:\nALTER TABLE .. ADD PARTITION\nthrows\nPartitionsAlreadyExistException\nif new partition exists already\nALTER TABLE .. DROP PARTITION\nthrows\nNoSuchPartitionsException\nfor not existing partitions\nUpgrading from Spark SQL 2.4.5 to 2.4.6\nIn Spark 2.4.6, the\nRESET\ncommand does not reset the static SQL configuration values to the default. It only clears the runtime SQL configuration values.\nUpgrading from Spark SQL 2.4.4 to 2.4.5\nSince Spark 2.4.5,\nTRUNCATE TABLE\ncommand tries to set back original permission and ACLs during re-creating the table/partition paths. To restore the behaviour of earlier versions, set\nspark.sql", "question": "What exception is thrown when attempting to add a partition that already exists in Spark 2.4.8?", "answers": {"text": ["PartitionsAlreadyExistException"], "answer_start": [217]}}
{"context": "r Guide\n, MsSQLServer JDBC Dialect uses ShortType and FloatType for SMALLINT and REAL, respectively. Previously, IntegerType and DoubleType is used.\nUpgrading from Spark SQL 2.4 to 2.4.1\nThe value of\nspark.executor.heartbeatInterval\n, when specified without units like “30” rather than “30s”, was\ninconsistently interpreted as both seconds and milliseconds in Spark 2.4.0 in different parts of the code.\nUnitless values are now consistently interpreted as milliseconds. Applications that set values like “30”\nneed to specify a value with units like “30s” now, to avoid being interpreted as milliseconds; otherwise,\nthe extremely short interval that results will likely cause applications to fail.\nUpgrading from Spark SQL 2.3 to 2.4\nIn Spark version 2.3 and earlier, the second parameter to array_con", "question": "What change was made to how spark.executor.heartbeatInterval is interpreted when no units are specified?", "answers": {"text": ["Unitless values are now consistently interpreted as milliseconds."], "answer_start": [404]}}
{"context": "esults will likely cause applications to fail.\nUpgrading from Spark SQL 2.3 to 2.4\nIn Spark version 2.3 and earlier, the second parameter to array_contains function is implicitly promoted to the element type of first array type parameter. This type promotion can be lossy and may cause\narray_contains\nfunction to return wrong result. This problem has been addressed in 2.4 by employing a safer type promotion mechanism. This can cause some change in behavior and are illustrated in the table below.\nQuery\nSpark 2.3 or Prior\nSpark 2.4\nRemarks\nSELECT array_contains(array(1), 1.34D);\ntrue\nfalse\nIn Spark 2.4, left and right parameters are promoted to array type of double type and double type respectively.\nSELECT array_contains(array(1), '1');\ntrue\nAnalysisException\nis thrown.\nExplicit cast can be us", "question": "What change was made to the array_contains function in Spark version 2.4 to address a problem present in 2.3 and earlier?", "answers": {"text": ["This problem has been addressed in 2.4 by employing a safer type promotion mechanism."], "answer_start": [334]}}
{"context": "e a subquery, the inner query must contain a struct field as well. In previous versions, instead, the fields of the struct were compared to the output of the inner query. For example, if\na\nis a\nstruct(a string, b int)\n, in Spark 2.4\na in (select (1 as a, 'a' as b) from range(1))\nis a valid query, while\na in (select 1, 'a' from range(1))\nis not. In previous version it was the opposite.\nIn versions 2.2.1+ and 2.3, if\nspark.sql.caseSensitive\nis set to true, then the\nCURRENT_DATE\nand\nCURRENT_TIMESTAMP\nfunctions incorrectly became case-sensitive and would resolve to columns (unless typed in lower case). In Spark 2.4 this has been fixed and the functions are no longer case-sensitive.\nSince Spark 2.4, Spark will evaluate the set operations referenced in a query by following a precedence rule as p", "question": "In Spark 2.4, how are CURRENT_DATE and CURRENT_TIMESTAMP functions handled regarding case sensitivity compared to versions 2.2.1+ and 2.3?", "answers": {"text": ["In Spark 2.4 this has been fixed and the functions are no longer case-sensitive."], "answer_start": [606]}}
{"context": "ctions are no longer case-sensitive.\nSince Spark 2.4, Spark will evaluate the set operations referenced in a query by following a precedence rule as per the SQL standard. If the order is not specified by parentheses, set operations are performed from left to right with the exception that all INTERSECT operations are performed before any UNION, EXCEPT or MINUS operations. The old behaviour of giving equal precedence to all the set operations are preserved under a newly added configuration\nspark.sql.legacy.setopsPrecedence.enabled\nwith a default value of\nfalse\n. When this property is set to\ntrue\n, spark will evaluate the set operators from left to right as they appear in the query given no explicit ordering is enforced by usage of parenthesis.\nSince Spark 2.4, Spark will display table descri", "question": "What is the default value of the configuration spark.sql.legacy.setopsPrecedence.enabled?", "answers": {"text": ["false"], "answer_start": [559]}}
{"context": "ataframe to a directory launches at least one write task, even if physically the dataframe has no partition. This introduces a small behavior change that for self-describing file formats like Parquet and Orc, Spark creates a metadata-only file in the target directory when writing a 0-partition dataframe, so that schema inference can still work if users read that directory later. The new behavior is more reasonable and more consistent regarding writing empty dataframe.\nSince Spark 2.4, expression IDs in UDF arguments do not appear in column names. For example, a column name in Spark 2.4 is not\nUDF:f(col0 AS colA#28)\nbut\nUDF:f(col0 AS `colA`)\n.\nSince Spark 2.4, writing a dataframe with an empty or nested empty schema using any file formats (parquet, orc, json, text, csv etc.) is not allowed.", "question": "What happens when writing a 0-partition dataframe in self-describing file formats like Parquet and Orc?", "answers": {"text": ["Spark creates a metadata-only file in the target directory when writing a 0-partition dataframe, so that schema inference can still work if users read that directory later."], "answer_start": [209]}}
{"context": "\nSince Spark 2.4, writing a dataframe with an empty or nested empty schema using any file formats (parquet, orc, json, text, csv etc.) is not allowed. An exception is thrown when attempting to write dataframes with empty schema.\nSince Spark 2.4, Spark compares a DATE type with a TIMESTAMP type after promotes both sides to TIMESTAMP. To set\nfalse\nto\nspark.sql.legacy.compareDateTimestampInTimestamp\nrestores the previous behavior. This option will be removed in Spark 3.0.\nSince Spark 2.4, creating a managed table with nonempty location is not allowed. An exception is thrown when attempting to create a managed table with nonempty location. To set\ntrue\nto\nspark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\nrestores the previous behavior. This option will be removed in Spark 3.0.\nSin", "question": "What happens when attempting to write dataframes with empty schema since Spark 2.4?", "answers": {"text": ["An exception is thrown when attempting to write dataframes with empty schema."], "answer_start": [151]}}
{"context": "\ntrue\nto\nspark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\nrestores the previous behavior. This option will be removed in Spark 3.0.\nSince Spark 2.4, renaming a managed table to existing location is not allowed. An exception is thrown when attempting to rename a managed table to existing location.\nSince Spark 2.4, the type coercion rules can automatically promote the argument types of the variadic SQL functions (e.g., IN/COALESCE) to the widest common type, no matter how the input arguments order. In prior Spark versions, the promotion could fail in some specific orders (e.g., TimestampType, IntegerType and StringType) and throw an exception.\nSince Spark 2.4, Spark has enabled non-cascading SQL cache invalidation in addition to the traditional cache invalidation mechanism. Th", "question": "What change occurred in Spark 2.4 regarding renaming a managed table?", "answers": {"text": ["Since Spark 2.4, renaming a managed table to existing location is not allowed."], "answer_start": [147]}}
{"context": " an exception.\nSince Spark 2.4, Spark has enabled non-cascading SQL cache invalidation in addition to the traditional cache invalidation mechanism. The non-cascading cache invalidation mechanism allows users to remove a cache without impacting its dependent caches. This new cache invalidation mechanism is used in scenarios where the data of the cache to be removed is still valid, e.g., calling unpersist() on a Dataset, or dropping a temporary view. This allows users to free up memory and keep the desired caches valid at the same time.\nIn version 2.3 and earlier, Spark converts Parquet Hive tables by default but ignores table properties like\nTBLPROPERTIES (parquet.compression 'NONE')\n. This happens for ORC Hive table properties like\nTBLPROPERTIES (orc.compress 'NONE')\nin case of\nspark.sql.h", "question": "What change was enabled in Spark 2.4 regarding SQL cache invalidation?", "answers": {"text": ["Since Spark 2.4, Spark has enabled non-cascading SQL cache invalidation in addition to the traditional cache invalidation mechanism."], "answer_start": [15]}}
{"context": "BLPROPERTIES (parquet.compression 'NONE')\n. This happens for ORC Hive table properties like\nTBLPROPERTIES (orc.compress 'NONE')\nin case of\nspark.sql.hive.convertMetastoreOrc=true\n, too. Since Spark 2.4, Spark respects Parquet/ORC specific table properties while converting Parquet/ORC Hive tables. As an example,\nCREATE TABLE t(id int) STORED AS PARQUET TBLPROPERTIES (parquet.compression 'NONE')\nwould generate Snappy parquet files during insertion in Spark 2.3, and in Spark 2.4, the result would be uncompressed parquet files.\nSince Spark 2.0, Spark converts Parquet Hive tables by default for better performance. Since Spark 2.4, Spark converts ORC Hive tables by default, too. It means Spark uses its own ORC support by default instead of Hive SerDe. As an example,\nCREATE TABLE t(id int) STORED", "question": "What happens when converting Parquet/ORC Hive tables in Spark 2.4 regarding table properties?", "answers": {"text": ["Since Spark 2.4, Spark respects Parquet/ORC specific table properties while converting Parquet/ORC Hive tables."], "answer_start": [186]}}
{"context": "RC Hive tables by default, too. It means Spark uses its own ORC support by default instead of Hive SerDe. As an example,\nCREATE TABLE t(id int) STORED AS ORC\nwould be handled with Hive SerDe in Spark 2.3, and in Spark 2.4, it would be converted into Spark’s ORC data source table and ORC vectorization would be applied. To set\nfalse\nto\nspark.sql.hive.convertMetastoreOrc\nrestores the previous behavior.\nIn version 2.3 and earlier, CSV rows are considered as malformed if at least one column value in the row is malformed. CSV parser dropped such rows in the DROPMALFORMED mode or outputs an error in the FAILFAST mode. Since Spark 2.4, CSV row is considered as malformed only when it contains malformed column values requested from CSV datasource, other values can be ignored. As an example, CSV file", "question": "What change occurred in Spark 2.4 regarding how CSV rows are considered malformed?", "answers": {"text": ["Since Spark 2.4, CSV row is considered as malformed only when it contains malformed column values requested from CSV datasource, other values can be ignored."], "answer_start": [619]}}
{"context": "ne character, and zero or more characters, respectively. Example:\nLOAD DATA INPATH '/tmp/folder*/'\nor\nLOAD DATA INPATH '/tmp/part-?'\n. Special Characters like\nspace\nalso now work in paths. Example:\nLOAD DATA INPATH '/tmp/folder name/'\n.\nIn Spark version 2.3 and earlier, HAVING without GROUP BY is treated as WHERE. This means,\nSELECT 1 FROM range(10) HAVING true\nis executed as\nSELECT 1 FROM range(10) WHERE true\nand returns 10 rows. This violates SQL standard, and has been fixed in Spark 2.4. Since Spark 2.4, HAVING without GROUP BY is treated as a global aggregate, which means\nSELECT 1 FROM range(10) HAVING true\nwill return only one row. To restore the previous behavior, set\nspark.sql.legacy.parser.havingWithoutGroupByAsWhere\nto\ntrue\n.\nIn version 2.3 and earlier, when reading from a Parquet", "question": "How is HAVING without GROUP BY treated in Spark version 2.3 and earlier?", "answers": {"text": ["HAVING without GROUP BY is treated as WHERE."], "answer_start": [271]}}
{"context": "xample,\nval df = spark.read.schema(schema).json(file).cache()\nand then\ndf.filter($\"_corrupt_record\".isNotNull).count()\n.\nThe\npercentile_approx\nfunction previously accepted numeric type input and output double type results. Now it supports date type, timestamp type and numeric types as input types. The result type is also changed to be the same as the input type, which is more reasonable for percentiles.\nSince Spark 2.3, the Join/Filter’s deterministic predicates that are after the first non-deterministic predicates are also pushed down/through the child operators, if possible. In prior Spark versions, these filters are not eligible for predicate pushdown.\nPartition column inference previously found incorrect common type for different inferred types, for example, previously it ended up with", "question": "What types does the percentile_approx function now support as input?", "answers": {"text": ["Now it supports date type, timestamp type and numeric types as input types."], "answer_start": [223]}}
{"context": "ate pushdown.\nPartition column inference previously found incorrect common type for different inferred types, for example, previously it ended up with double type as the common type for double type and date type. Now it finds the correct common type for such conflicts. The conflict resolution follows the table below:\nInputA \\ InputB\nNullType\nIntegerType\nLongType\nDecimalType(38,0)*\nDoubleType\nDateType\nTimestampType\nStringType\nNullType\nNullType\nIntegerType\nLongType\nDecimalType(38,0)\nDoubleType\nDateType\nTimestampType\nStringType\nIntegerType\nIntegerType\nIntegerType\nLongType\nDecimalType(38,0)\nDoubleType\nStringType\nStringType\nStringType\nLongType\nLongType\nLongType\nLongType\nDecimalType(38,0)\nStringType\nStringType\nStringType\nStringType\nDecimalType(38,0)*\nDecimalType(38,0)\nDecimalType(38,0)\nDecimalTy", "question": "What did the partition column inference previously do incorrectly when finding a common type for different inferred types?", "answers": {"text": ["previously it ended up with double type as the common type for double type and date type."], "answer_start": [123]}}
{"context": "31). This involves the following changes\nThe rules to determine the result type of an arithmetic operation have been updated. In particular, if the precision / scale needed are out of the range of available values, the scale is reduced up to 6, in order to prevent the truncation of the integer part of the decimals. All the arithmetic operations are affected by the change, i.e. addition (\n+\n), subtraction (\n-\n), multiplication (\n*\n), division (\n/\n), remainder (\n%\n) and positive modulus (\npmod\n).\nLiteral values used in SQL operations are converted to DECIMAL with the exact precision and scale needed by them.\nThe configuration\nspark.sql.decimalOperations.allowPrecisionLoss\nhas been introduced. It defaults to\ntrue\n, which means the new behavior described here; if set to\nfalse\n, Spark uses prev", "question": "What happens if the precision / scale needed are out of the range of available values during arithmetic operations?", "answers": {"text": ["the scale is reduced up to 6, in order to prevent the truncation of the integer part of the decimals."], "answer_start": [215]}}
{"context": "perations.allowPrecisionLoss\nhas been introduced. It defaults to\ntrue\n, which means the new behavior described here; if set to\nfalse\n, Spark uses previous rules, i.e. it doesn’t adjust the needed scale to represent the values and it returns NULL if an exact representation of the value is not possible.\nUn-aliased subquery’s semantic has not been well defined with confusing behaviors. Since Spark 2.3, we invalidate such confusing cases, for example:\nSELECT v.i from (SELECT i FROM v)\n, Spark will throw an analysis exception in this case because users should not be able to use the qualifier inside a subquery. See\nSPARK-20690\nand\nSPARK-21335\nfor more details.\nWhen creating a\nSparkSession\nwith\nSparkSession.builder.getOrCreate()\n, if there is an existing\nSparkContext\n, the builder was trying to u", "question": "What happens when 'operations.allowPrecisionLoss' is set to false?", "answers": {"text": ["Spark uses previous rules, i.e. it doesn’t adjust the needed scale to represent the values and it returns NULL if an exact representation of the value is not possible."], "answer_start": [135]}}
{"context": "hich kept behavior identical to 2.1.0. However, Spark 2.2.0 changes this setting’s default value to\nINFER_AND_SAVE\nto restore compatibility with reading Hive metastore tables whose underlying file schema have mixed-case column names. With the\nINFER_AND_SAVE\nconfiguration value, on first access Spark will perform schema inference on any Hive metastore table for which it has not already saved an inferred schema. Note that schema inference can be a very time-consuming operation for tables with thousands of partitions. If compatibility with mixed-case column names is not a concern, you can safely set\nspark.sql.hive.caseSensitiveInferenceMode\nto\nNEVER_INFER\nto avoid the initial overhead of schema inference. Note that with the new default\nINFER_AND_SAVE\nsetting, the results of the schema inferen", "question": "What configuration value can be set to avoid the initial overhead of schema inference if compatibility with mixed-case column names is not a concern?", "answers": {"text": ["to\nNEVER_INFER\nto avoid the initial overhead of schema inference."], "answer_start": [646]}}
{"context": "EVER_INFER\nto avoid the initial overhead of schema inference. Note that with the new default\nINFER_AND_SAVE\nsetting, the results of the schema inference are saved as a metastore key for future use. Therefore, the initial schema inference occurs only at a table’s first access.\nSince Spark 2.2.1 and 2.3.0, the schema is always inferred at runtime when the data source tables have the columns that exist in both partition schema and data schema. The inferred schema does not have the partitioned columns. When reading the table, Spark respects the partition values of these overlapping columns instead of the values stored in the data source files. In 2.2.0 and 2.1.x release, the inferred schema is partitioned but the data of the table is invisible to users (i.e., the result set is empty).\nSince Sp", "question": "What happens with the schema inference results when the INFER_AND_SAVE setting is used?", "answers": {"text": ["the results of the schema inference are saved as a metastore key for future use"], "answer_start": [117]}}
{"context": "rce API.\nLegacy datasource tables can be migrated to this format via the\nMSCK REPAIR TABLE\ncommand. Migrating legacy tables is recommended to take advantage of Hive DDL support and improved planning performance.\nTo determine if a table has been migrated, look for the\nPartitionProvider: Catalog\nattribute when issuing\nDESCRIBE FORMATTED\non the table.\nChanges to\nINSERT OVERWRITE TABLE ... PARTITION ...\nbehavior for Datasource tables.\nIn prior Spark versions\nINSERT OVERWRITE\noverwrote the entire Datasource table, even when given a partition specification. Now only partitions matching the specification are overwritten.\nNote that this still differs from the behavior of Hive tables, which is to overwrite only partitions overlapping with newly inserted data.\nUpgrading from Spark SQL 1.6 to 2.0\nSpa", "question": "How does the behavior of INSERT OVERWRITE TABLE ... PARTITION ... change for Datasource tables?", "answers": {"text": ["Now only partitions matching the specification are overwritten."], "answer_start": [558]}}
{"context": "ce\nDataFrame\nwith\nDataset<Row>\n. Both the typed\ntransformations (e.g.,\nmap\n,\nfilter\n, and\ngroupByKey\n) and untyped transformations (e.g.,\nselect\nand\ngroupBy\n) are available on the Dataset class. Since compile-time type-safety in\nPython and R is not a language feature, the concept of Dataset does not apply to these languages’\nAPIs. Instead,\nDataFrame\nremains the primary programming abstraction, which is analogous to the\nsingle-node data frame notion in these languages.\nDataset and DataFrame API\nunionAll\nhas been deprecated and replaced by\nunion\nDataset and DataFrame API\nexplode\nhas been deprecated, alternatively, use\nfunctions.explode()\nwith\nselect\nor\nflatMap\nDataset and DataFrame API\nregisterTempTable\nhas been deprecated and replaced by\ncreateOrReplaceTempView\nChanges to\nCREATE TABLE ... L", "question": "What has replaced unionAll in the Dataset and DataFrame API?", "answers": {"text": ["union"], "answer_start": [499]}}
{"context": "ver.sh\nvia\n--conf\n:\n./sbin/start-thriftserver.sh\n\\\n--conf\nspark.sql.hive.thriftServer.singleSession\n=\ntrue\n\\\n...\nFrom Spark 1.6, LongType casts to TimestampType expect seconds instead of microseconds. This\nchange was made to match the behavior of Hive 1.2 for more consistent type casting to TimestampType\nfrom numeric types. See\nSPARK-11724\nfor\ndetails.\nUpgrading from Spark SQL 1.4 to 1.5\nOptimized execution using manually managed memory (Tungsten) is now enabled by default, along with\ncode generation for expression evaluation. These features can both be disabled by setting\nspark.sql.tungsten.enabled\nto\nfalse\n.\nParquet schema merging is no longer enabled by default. It can be re-enabled by setting\nspark.sql.parquet.mergeSchema\nto\ntrue\n.\nIn-memory columnar storage partition pruning is on by ", "question": "What change was made from Spark 1.6 regarding LongType casts to TimestampType?", "answers": {"text": ["From Spark 1.6, LongType casts to TimestampType expect seconds instead of microseconds."], "answer_start": [113]}}
{"context": "ed as decimal. HiveQL parsing remains\nunchanged.\nThe canonical name of SQL/DataFrame functions are now lower case (e.g., sum vs SUM).\nJSON data source will not automatically load new files that are created by other applications\n(i.e. files that are not inserted to the dataset through Spark SQL).\nFor a JSON persistent table (i.e. the metadata of the table is stored in Hive Metastore),\nusers can use\nREFRESH TABLE\nSQL command or\nHiveContext\n’s\nrefreshTable\nmethod\nto include those new files to the table. For a DataFrame representing a JSON dataset, users need to recreate\nthe DataFrame and the new DataFrame will include new files.\nUpgrading from Spark SQL 1.3 to 1.4\nDataFrame data reader/writer interface\nBased on user feedback, we created a new, more fluid API for reading data in (\nSQLContext.r", "question": "How can users include new files to a JSON persistent table?", "answers": {"text": ["users can use\nREFRESH TABLE\nSQL command or\nHiveContext\n’s\nrefreshTable\nmethod\nto include those new files to the table."], "answer_start": [387]}}
{"context": "tment\" to show up,\n// it must be included explicitly as part of the agg function call.\ndf\n.\ngroupBy\n(\n\"department\"\n).\nagg\n(\n$\n\"department\"\n,\nmax\n(\n\"age\"\n),\nsum\n(\n\"expense\"\n))\n// In 1.4+, grouping column \"department\" is included automatically.\ndf\n.\ngroupBy\n(\n\"department\"\n).\nagg\n(\nmax\n(\n\"age\"\n),\nsum\n(\n\"expense\"\n))\n// Revert to 1.3 behavior (not retaining grouping column) by:\nsqlContext\n.\nsetConf\n(\n\"spark.sql.retainGroupColumns\"\n,\n\"false\"\n)\n// In 1.3.x, in order for the grouping column \"department\" to show up,\n// it must be included explicitly as part of the agg function call.\ndf\n.\ngroupBy\n(\n\"department\"\n).\nagg\n(\ncol\n(\n\"department\"\n),\nmax\n(\n\"age\"\n),\nsum\n(\n\"expense\"\n));\n// In 1.4+, grouping column \"department\" is included automatically.\ndf\n.\ngroupBy\n(\n\"department\"\n).\nagg\n(\nmax\n(\n\"age\"\n),\nsum\n(", "question": "How does Spark SQL 1.4+ handle grouping columns compared to 1.3.x?", "answers": {"text": ["In 1.4+, grouping column \"department\" is included automatically."], "answer_start": [178]}}
{"context": "as\nbeen renamed to\nDataFrame\n. This is primarily because DataFrames no longer inherit from RDD\ndirectly, but instead provide most of the functionality that RDDs provide though their own\nimplementation. DataFrames can still be converted to RDDs by calling the\n.rdd\nmethod.\nIn Scala, there is a type alias from\nSchemaRDD\nto\nDataFrame\nto provide source compatibility for\nsome use cases. It is still recommended that users update their code to use\nDataFrame\ninstead.\nJava and Python users will need to update their code.\nUnification of the Java and Scala APIs\nPrior to Spark 1.3 there were separate Java compatible classes (\nJavaSQLContext\nand\nJavaSchemaRDD\n)\nthat mirrored the Scala API. In Spark 1.3 the Java API and Scala API have been unified. Users\nof either language should use\nSQLContext\nand\nDataF", "question": "What should Java and Python users do regarding their code?", "answers": {"text": ["Java and Python users will need to update their code."], "answer_start": [463]}}
{"context": "RDD\n)\nthat mirrored the Scala API. In Spark 1.3 the Java API and Scala API have been unified. Users\nof either language should use\nSQLContext\nand\nDataFrame\n. In general these classes try to\nuse types that are usable from both languages (i.e.\nArray\ninstead of language-specific collections).\nIn some cases where no common type exists (e.g., for passing in closures or Maps) function overloading\nis used instead.\nAdditionally, the Java specific types API has been removed. Users of both Scala and Java should\nuse the classes present in\norg.apache.spark.sql.types\nto describe schema programmatically.\nIsolation of Implicit Conversions and Removal of dsl Package (Scala-only)\nMany of the code examples prior to Spark 1.3 started with\nimport sqlContext._\n, which brought\nall of the functions from sqlContex", "question": "What should users of both Scala and Java use to describe schema programmatically?", "answers": {"text": ["org.apache.spark.sql.types"], "answer_start": [533]}}
{"context": "ster\n(\n\"strLen\"\n,\n(\nString\ns\n)\n->\ns\n.\nlength\n(),\nDataTypes\n.\nIntegerType\n);\nPython UDF registration is unchanged.\nCompatibility with Apache Hive\nSpark SQL is designed to be compatible with the Hive Metastore, SerDes and UDFs.\nCurrently, Hive SerDes and UDFs are based on built-in Hive,\nand Spark SQL can be connected to different versions of Hive Metastore\n(from 2.0.0 to 2.3.10 and 3.0.0 to 3.1.3. Also see\nInteracting with Different Versions of Hive Metastore\n).\nDeploying in Existing Hive Warehouses\nThe Spark SQL Thrift JDBC server is designed to be “out of the box” compatible with existing Hive\ninstallations. You do not need to modify your existing Hive Metastore or change the data placement\nor partitioning of your tables.\nSupported Hive Features\nSpark SQL supports the vast majority of Hive", "question": "What versions of Hive Metastore can Spark SQL connect to?", "answers": {"text": ["(from 2.0.0 to 2.3.10 and 3.0.0 to 3.1.3"], "answer_start": [357]}}
{"context": " serialization formats (SerDes)\nWindow functions\nJoins\nJOIN\n{LEFT|RIGHT|FULL} OUTER JOIN\nLEFT SEMI JOIN\nLEFT ANTI JOIN\nCROSS JOIN\nUnions\nSub-queries\nSub-queries in the FROM Clause\nSELECT col FROM (SELECT a + b AS col FROM t1) t2\nSub-queries in WHERE Clause\nCorrelated or non-correlated IN and NOT IN statement in WHERE Clause\nSELECT col FROM t1 WHERE col IN (SELECT a FROM t2 WHERE t1.a = t2.a)\nSELECT col FROM t1 WHERE col IN (SELECT a FROM t2)\nCorrelated or non-correlated EXISTS and NOT EXISTS statement in WHERE Clause\nSELECT col FROM t1 WHERE EXISTS (SELECT t2.a FROM t2 WHERE t1.a = t2.a AND t2.a > 10)\nSELECT col FROM t1 WHERE EXISTS (SELECT t2.a FROM t2 WHERE t2.a > 10)\nNon-correlated IN and NOT IN statement in JOIN Condition\nSELECT t1.col FROM t1 JOIN t2 ON t1.a = t2.a AND t1.a IN (SELECT", "question": "What are some types of JOIN operations?", "answers": {"text": ["LEFT SEMI JOIN\nLEFT ANTI JOIN\nCROSS JOIN"], "answer_start": [89]}}
{"context": "2.a FROM t2 WHERE t2.a > 10)\nNon-correlated IN and NOT IN statement in JOIN Condition\nSELECT t1.col FROM t1 JOIN t2 ON t1.a = t2.a AND t1.a IN (SELECT a FROM t3)\nNon-correlated EXISTS and NOT EXISTS statement in JOIN Condition\nSELECT t1.col FROM t1 JOIN t2 ON t1.a = t2.a AND EXISTS (SELECT * FROM t3 WHERE t3.a > 10)\nSampling\nExplain\nPartitioned tables including dynamic partition insertion\nView\nIf column aliases are not specified in view definition queries, both Spark and Hive will\ngenerate alias names, but in different ways. In order for Spark to be able to read views created\nby Hive, users should explicitly specify column aliases in view definition queries. As an\nexample, Spark cannot read\nv1\ncreated as below by Hive.\nCREATE VIEW v1 AS SELECT * FROM (SELECT c + 1 FROM (SELECT 1 c) t1) t2;", "question": "What should users do to ensure Spark can read views created by Hive?", "answers": {"text": ["users should explicitly specify column aliases in view definition queries."], "answer_start": [592]}}
{"context": "inition queries. As an\nexample, Spark cannot read\nv1\ncreated as below by Hive.\nCREATE VIEW v1 AS SELECT * FROM (SELECT c + 1 FROM (SELECT 1 c) t1) t2;\nInstead, you should create\nv1\nas below with column aliases explicitly specified.\nCREATE VIEW v1 AS SELECT * FROM (SELECT c + 1 AS inc_c FROM (SELECT 1 c) t1) t2;\nAll Hive DDL Functions, including:\nCREATE TABLE\nCREATE TABLE AS SELECT\nCREATE TABLE LIKE\nALTER TABLE\nMost Hive Data types, including:\nTINYINT\nSMALLINT\nINT\nBIGINT\nBOOLEAN\nFLOAT\nDOUBLE\nSTRING\nBINARY\nTIMESTAMP\nDATE\nARRAY<>\nMAP<>\nSTRUCT<>\nUnsupported Hive Functionality\nBelow is a list of Hive features that we don’t support yet. Most of these features are rarely used\nin Hive deployments.\nEsoteric Hive Features\nUNION\ntype\nUnique join\nColumn statistics collecting: Spark SQL does not piggyb", "question": "What is an example of a Hive view creation that Spark cannot read?", "answers": {"text": ["CREATE VIEW v1 AS SELECT * FROM (SELECT c + 1 FROM (SELECT 1 c) t1) t2;"], "answer_start": [79]}}
{"context": "se features are rarely used\nin Hive deployments.\nEsoteric Hive Features\nUNION\ntype\nUnique join\nColumn statistics collecting: Spark SQL does not piggyback scans to collect column statistics at\nthe moment and only supports populating the sizeInBytes field of the hive metastore.\nHive Input/Output Formats\nFile format for CLI: For results showing back to the CLI, Spark SQL only supports TextOutputFormat.\nHadoop archive\nHive Optimizations\nA handful of Hive optimizations are not yet included in Spark. Some of these (such as indexes) are\nless important due to Spark SQL’s in-memory computational model. Others are slotted for future\nreleases of Spark SQL.\nBlock-level bitmap indexes and virtual columns (used to build indexes)\nAutomatically determine the number of reducers for joins and groupbys: Curr", "question": "What file format does Spark SQL support for results showing back to the CLI?", "answers": {"text": ["Spark SQL only supports TextOutputFormat."], "answer_start": [361]}}
{"context": "e returns null, Spark SQL returns NaN.\nACOS(n)\nIf n < -1 or n > 1, Hive returns null, Spark SQL returns NaN.\nASIN(n)\nIf n < -1 or n > 1, Hive returns null, Spark SQL returns NaN.\nCAST(n AS TIMESTAMP)\nIf n is integral numbers, Hive treats n as milliseconds, Spark SQL treats n as seconds.", "question": "How do Hive and Spark SQL handle values of n outside the range of -1 to 1 for ACOS(n) and ASIN(n)?", "answers": {"text": ["If n < -1 or n > 1, Hive returns null, Spark SQL returns NaN."], "answer_start": [47]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nMLlib: Main Guide\nBasic statistics\nData sources\nPipelines\nExtracting, transforming and selecting features\nClassification and Regression\nClustering\nCollaborative filtering\nFrequent Pattern Mining\nModel selection and tuning\nAdvanced topics\nMLlib: RDD-ba", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)"], "answer_start": [46]}}
{"context": "\nmetrics that are currently available in\nspark.mllib\nare detailed in this section.\nClassification model evaluation\nWhile there are many different types of classification algorithms, the evaluation of classification models all share\nsimilar principles. In a\nsupervised classification problem\n,\nthere exists a true output and a model-generated predicted output for each data point. For this reason, the results for\neach data point can be assigned to one of four categories:\nTrue Positive (TP) - label is positive and prediction is also positive\nTrue Negative (TN) - label is negative and prediction is also negative\nFalse Positive (FP) - label is negative but prediction is positive\nFalse Negative (FN) - label is positive but prediction is negative\nThese four numbers are the building blocks for most ", "question": "What are the four categories results for each data point can be assigned to in a supervised classification problem?", "answers": {"text": ["True Positive (TP) - label is positive and prediction is also positive\nTrue Negative (TN) - label is negative and prediction is also negative\nFalse Positive (FP) - label is negative but prediction is positive\nFalse Negative (FN) - label is positive but prediction is negative"], "answer_start": [472]}}
{"context": "ive but prediction is positive\nFalse Negative (FN) - label is positive but prediction is negative\nThese four numbers are the building blocks for most classifier evaluation metrics. A fundamental point when considering\nclassifier evaluation is that pure accuracy (i.e. was the prediction correct or incorrect) is not generally a good metric. The\nreason for this is because a dataset may be highly unbalanced. For example, if a model is designed to predict fraud from\na dataset where 95% of the data points are\nnot fraud\nand 5% of the data points are\nfraud\n, then a naive classifier\nthat predicts\nnot fraud\n, regardless of input, will be 95% accurate. For this reason, metrics like\nprecision and recall\nare typically used because they take into\naccount the\ntype\nof error. In most applications there is ", "question": "Por que a precisão pura não é geralmente uma boa métrica para avaliar classificadores?", "answers": {"text": ["a dataset may be highly unbalanced."], "answer_start": [372]}}
{"context": "ion models actually output a “score” (often times a probability) for\neach class, where a higher score indicates higher likelihood. In the binary case, the model may output a probability for\neach class: $P(Y=1|X)$ and $P(Y=0|X)$. Instead of simply taking the higher probability, there may be some cases where\nthe model might need to be tuned so that it only predicts a class when the probability is very high (e.g. only block a\ncredit card transaction if the model predicts fraud with >90% probability). Therefore, there is a prediction\nthreshold\nwhich determines what the predicted class will be based on the probabilities that the model outputs.\nTuning the prediction threshold will change the precision and recall of the model and is an important part of model\noptimization. In order to visualize h", "question": "What does a higher score indicate in ion models?", "answers": {"text": ["a higher score indicates higher likelihood"], "answer_start": [87]}}
{"context": "ining algorithm to build the model\nmodel\n=\nLogisticRegressionWithLBFGS\n.\ntrain\n(\ntraining\n)\n# Compute raw scores on the test set\npredictionAndLabels\n=\ntest\n.\nmap\n(\nlambda\nlp\n:\n(\nfloat\n(\nmodel\n.\npredict\n(\nlp\n.\nfeatures\n)),\nlp\n.\nlabel\n))\n# Instantiate metrics object\nmetrics\n=\nBinaryClassificationMetrics\n(\npredictionAndLabels\n)\n# Area under precision-recall curve\nprint\n(\n\"\nArea under PR = %s\n\"\n%\nmetrics\n.\nareaUnderPR\n)\n# Area under ROC curve\nprint\n(\n\"\nArea under ROC = %s\n\"\n%\nmetrics\n.\nareaUnderROC\n)\nFind full example code at \"examples/src/main/python/mllib/binary_classification_metrics_example.py\" in the Spark repo.\nRefer to the\nLogisticRegressionWithLBFGS\nScala docs\nand\nBinaryClassificationMetrics\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.classification.LogisticRegress", "question": "Where can I find a full example code for this process?", "answers": {"text": ["Find full example code at \"examples/src/main/python/mllib/binary_classification_metrics_example.py\" in the Spark repo."], "answer_start": [502]}}
{"context": "onWithLBFGS\nScala docs\nand\nBinaryClassificationMetrics\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.classification.LogisticRegressionWithLBFGS\nimport\norg.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nimport\norg.apache.spark.mllib.regression.LabeledPoint\nimport\norg.apache.spark.mllib.util.MLUtils\n// Load training data in LIBSVM format\nval\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\n\"data/mllib/sample_binary_classification_data.txt\"\n)\n// Split data into training (60%) and test (40%)\nval\nArray\n(\ntraining\n,\ntest\n)\n=\ndata\n.\nrandomSplit\n(\nArray\n(\n0.6\n,\n0.4\n),\nseed\n=\n11L\n)\ntraining\n.\ncache\n()\n// Run training algorithm to build the model\nval\nmodel\n=\nnew\nLogisticRegressionWithLBFGS\n()\n.\nsetNumClasses\n(\n2\n)\n.\nrun\n(\ntraining\n)\n// Clear the prediction threshold so the model ", "question": "Which class is used to load the training data in LIBSVM format?", "answers": {"text": ["MLUtils"], "answer_start": [321]}}
{"context": "d\n(\nsc\n,\n\"target/tmp/LogisticRegressionModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\" in the Spark repo.\nMulticlass classification\nA\nmulticlass classification\ndescribes a classification\nproblem where there are $M \\gt 2$ possible labels for each data point (the case where $M=2$ is the binary\nclassification problem). For example, classifying handwriting samples to the digits 0 to 9, having 10 possible classes.\nFor multiclass metrics, the notion of positives and negatives is slightly different. Predictions and labels can still\nbe positive or negative, but they must be considered under the context of a particular class. Each label and prediction\ntake on the value of one of the multiple classes and so they", "question": "Where can I find a full example code for JavaBinaryClassificationMetricsExample?", "answers": {"text": ["\"examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\" in the Spark repo."], "answer_start": [75]}}
{"context": "l\n,\nmetrics\n.\nprecision\n(\nlabel\n)))\nprint\n(\n\"\nClass %s recall = %s\n\"\n%\n(\nlabel\n,\nmetrics\n.\nrecall\n(\nlabel\n)))\nprint\n(\n\"\nClass %s F1 Measure = %s\n\"\n%\n(\nlabel\n,\nmetrics\n.\nfMeasure\n(\nlabel\n,\nbeta\n=\n1.0\n)))\n# Weighted stats\nprint\n(\n\"\nWeighted recall = %s\n\"\n%\nmetrics\n.\nweightedRecall\n)\nprint\n(\n\"\nWeighted precision = %s\n\"\n%\nmetrics\n.\nweightedPrecision\n)\nprint\n(\n\"\nWeighted F(1) Score = %s\n\"\n%\nmetrics\n.\nweightedFMeasure\n())\nprint\n(\n\"\nWeighted F(0.5) Score = %s\n\"\n%\nmetrics\n.\nweightedFMeasure\n(\nbeta\n=\n0.5\n))\nprint\n(\n\"\nWeighted false positive rate = %s\n\"\n%\nmetrics\n.\nweightedFalsePositiveRate\n)\nFind full example code at \"examples/src/main/python/mllib/multi_class_metrics_example.py\" in the Spark repo.\nRefer to the\nMulticlassMetrics\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.class", "question": "Where can I find a full example code for the discussed metrics?", "answers": {"text": ["Find full example code at \"examples/src/main/python/mllib/multi_class_metrics_example.py\" in the Spark repo."], "answer_start": [590]}}
{"context": "eighted false positive rate: ${metrics.weightedFalsePositiveRate}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\" in the Spark repo.\nRefer to the\nMulticlassMetrics\nJava docs\nfor details on the API.\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.api.java.*\n;\nimport\norg.apache.spark.mllib.classification.LogisticRegressionModel\n;\nimport\norg.apache.spark.mllib.classification.LogisticRegressionWithLBFGS\n;\nimport\norg.apache.spark.mllib.evaluation.MulticlassMetrics\n;\nimport\norg.apache.spark.mllib.regression.LabeledPoint\n;\nimport\norg.apache.spark.mllib.util.MLUtils\n;\nimport\norg.apache.spark.mllib.linalg.Matrix\n;\nString\npath\n=\n\"data/mllib/sample_multiclass_classification_data.txt\"\n;\nJavaRDD\n<\nLabeledPoint\n>\ndata\n=\nMLUtils\n.\nloadLi", "question": "Where can I find a full example code for MulticlassMetrics?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\" in the Spark repo."], "answer_start": [69]}}
{"context": "he.spark.mllib.linalg.Matrix\n;\nString\npath\n=\n\"data/mllib/sample_multiclass_classification_data.txt\"\n;\nJavaRDD\n<\nLabeledPoint\n>\ndata\n=\nMLUtils\n.\nloadLibSVMFile\n(\nsc\n,\npath\n).\ntoJavaRDD\n();\n// Split initial RDD into two... [60% training data, 40% testing data].\nJavaRDD\n<\nLabeledPoint\n>[]\nsplits\n=\ndata\n.\nrandomSplit\n(\nnew\ndouble\n[]{\n0.6\n,\n0.4\n},\n11L\n);\nJavaRDD\n<\nLabeledPoint\n>\ntraining\n=\nsplits\n[\n0\n].\ncache\n();\nJavaRDD\n<\nLabeledPoint\n>\ntest\n=\nsplits\n[\n1\n];\n// Run training algorithm to build the model.\nLogisticRegressionModel\nmodel\n=\nnew\nLogisticRegressionWithLBFGS\n()\n.\nsetNumClasses\n(\n3\n)\n.\nrun\n(\ntraining\n.\nrdd\n());\n// Compute raw scores on the test set.\nJavaPairRDD\n<\nObject\n,\nObject\n>\npredictionAndLabels\n=\ntest\n.\nmapToPair\n(\np\n->\nnew\nTuple2\n<>(\nmodel\n.\npredict\n(\np\n.\nfeatures\n()),\np\n.\nlabel\n(", "question": "What is the path to the data file loaded using MLUtils.loadLibSVMFile?", "answers": {"text": ["\"data/mllib/sample_multiclass_classification_data.txt\""], "answer_start": [45]}}
{"context": "alsePositiveRate\n());\n// Save and load model\nmodel\n.\nsave\n(\nsc\n,\n\"target/tmp/LogisticRegressionModel\"\n);\nLogisticRegressionModel\nsameModel\n=\nLogisticRegressionModel\n.\nload\n(\nsc\n,\n\"target/tmp/LogisticRegressionModel\"\n);\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\" in the Spark repo.\nMultilabel classification\nA\nmultilabel classification\nproblem involves mapping\neach sample in a dataset to a set of class labels. In this type of classification problem, the labels are not\nmutually exclusive. For example, when classifying a set of news articles into topics, a single article might be both\nscience and politics.\nBecause the labels are not mutually exclusive, the predictions and true labels are now vectors of label", "question": "Where can I find a full example code for multiclass classification?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\" in the Spark repo."], "answer_start": [219]}}
{"context": "\n)))\n# Micro stats\nprint\n(\n\"\nMicro precision = %s\n\"\n%\nmetrics\n.\nmicroPrecision\n)\nprint\n(\n\"\nMicro recall = %s\n\"\n%\nmetrics\n.\nmicroRecall\n)\nprint\n(\n\"\nMicro F1 measure = %s\n\"\n%\nmetrics\n.\nmicroF1Measure\n)\n# Hamming loss\nprint\n(\n\"\nHamming loss = %s\n\"\n%\nmetrics\n.\nhammingLoss\n)\n# Subset accuracy\nprint\n(\n\"\nSubset accuracy = %s\n\"\n%\nmetrics\n.\nsubsetAccuracy\n)\nFind full example code at \"examples/src/main/python/mllib/multi_label_metrics_example.py\" in the Spark repo.\nRefer to the\nMultilabelMetrics\nScala docs\nfor details on the API.\nimport\norg.apache.spark.mllib.evaluation.MultilabelMetrics\nimport\norg.apache.spark.rdd.RDD\nval\nscoreAndLabels\n:\nRDD\n[(\nArray\n[\nDouble\n]\n,\nArray\n[\nDouble\n])]\n=\nsc\n.\nparallelize\n(\nSeq\n((\nArray\n(\n0.0\n,\n1.0\n),\nArray\n(\n0.0\n,\n2.0\n)),\n(\nArray\n(\n0.0\n,\n2.0\n),\nArray\n(\n0.0\n,\n1.0\n)),\n(", "question": "Where can I find a full example code for multilabel metrics?", "answers": {"text": ["Find full example code at \"examples/src/main/python/mllib/multi_label_metrics_example.py\" in the Spark repo."], "answer_start": [351]}}
{"context": "ccuracy = ${metrics.subsetAccuracy}\"\n)\nFind full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\" in the Spark repo.\nRefer to the\nMultilabelMetrics\nJava docs\nfor details on the API.\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\nscala.Tuple2\n;\nimport\norg.apache.spark.api.java.*\n;\nimport\norg.apache.spark.mllib.evaluation.MultilabelMetrics\n;\nimport\norg.apache.spark.SparkConf\n;\nList\n<\nTuple2\n<\ndouble\n[],\ndouble\n[]>>\ndata\n=\nArrays\n.\nasList\n(\nnew\nTuple2\n<>(\nnew\ndouble\n[]{\n0.0\n,\n1.0\n},\nnew\ndouble\n[]{\n0.0\n,\n2.0\n}),\nnew\nTuple2\n<>(\nnew\ndouble\n[]{\n0.0\n,\n2.0\n},\nnew\ndouble\n[]{\n0.0\n,\n1.0\n}),\nnew\nTuple2\n<>(\nnew\ndouble\n[]{},\nnew\ndouble\n[]{\n0.0\n}),\nnew\nTuple2\n<>(\nnew\ndouble\n[]{\n2.0\n},\nnew\ndouble\n[]{\n2.0\n}),\nnew\nTuple2\n<>(\nnew\ndouble\n[]{\n2.0", "question": "Where can I find a full example code for MultiLabelMetrics?", "answers": {"text": ["Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\" in the Spark repo."], "answer_start": [39]}}
{"context": "icro stats\nSystem\n.\nout\n.\nformat\n(\n\"Micro recall = %f\\n\"\n,\nmetrics\n.\nmicroRecall\n());\nSystem\n.\nout\n.\nformat\n(\n\"Micro precision = %f\\n\"\n,\nmetrics\n.\nmicroPrecision\n());\nSystem\n.\nout\n.\nformat\n(\n\"Micro F1 measure = %f\\n\"\n,\nmetrics\n.\nmicroF1Measure\n());\n// Hamming loss\nSystem\n.\nout\n.\nformat\n(\n\"Hamming loss = %f\\n\"\n,\nmetrics\n.\nhammingLoss\n());\n// Subset accuracy\nSystem\n.\nout\n.\nformat\n(\n\"Subset accuracy = %f\\n\"\n,\nmetrics\n.\nsubsetAccuracy\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\" in the Spark repo.\nRanking systems\nThe role of a ranking algorithm (often thought of as a\nrecommender system\n)\nis to return to the user a set of relevant items or documents based on some training data. The definition of relevance", "question": "Onde posso encontrar um exemplo de código completo?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\" in the Spark repo."], "answer_start": [440]}}
{"context": "ht of as a\nrecommender system\n)\nis to return to the user a set of relevant items or documents based on some training data. The definition of relevance\nmay vary and is usually application specific. Ranking system metrics aim to quantify the effectiveness of these\nrankings or recommendations in various contexts. Some metrics compare a set of recommended documents to a ground truth\nset of relevant documents, while other metrics may incorporate numerical ratings explicitly.\nAvailable metrics\nA ranking system usually deals with a set of $M$ users\n\\[U = \\left\\{u_0, u_1, ..., u_{M-1}\\right\\}\\]\nEach user ($u_i$) having a set of $N_i$ ground truth relevant documents\n\\[D_i = \\left\\{d_0, d_1, ..., d_{N_i-1}\\right\\}\\]\nAnd a list of $Q_i$ recommended documents, in order of decreasing relevance\n\\[R_i = ", "question": "What does a ranking system typically deal with?", "answers": {"text": ["A ranking system usually deals with a set of $M$ users"], "answer_start": [493]}}
{"context": "users. In contrast to precision at k, this metric takes into account the order of the recommendations\n        (documents are assumed to be in order of decreasing relevance).\nExamples\nThe following code snippets illustrate how to load a sample dataset, train an alternating least squares recommendation\nmodel on the data, and evaluate the performance of the recommender by several ranking metrics. A brief summary of the\nmethodology is provided below.\nMovieLens ratings are on a scale of 1-5:\n5: Must see\n4: Will enjoy\n3: It’s okay\n2: Fairly bad\n1: Awful\nSo we should not recommend a movie if the predicted rating is less than 3.\nTo map ratings to confidence scores, we use:\n5 -> 2.5\n4 -> 1.5\n3 -> 0.5\n2 -> -0.5\n1 -> -1.5.\nThis mappings means unobserved entries are generally between It’s okay and Fai", "question": "How are MovieLens ratings interpreted on a scale of 1-5?", "answers": {"text": ["5: Must see\n4: Will enjoy\n3: It’s okay\n2: Fairly bad\n1: Awful"], "answer_start": [492]}}
{"context": "fidence scores, we use:\n5 -> 2.5\n4 -> 1.5\n3 -> 0.5\n2 -> -0.5\n1 -> -1.5.\nThis mappings means unobserved entries are generally between It’s okay and Fairly bad. The semantics of 0 in this\nexpanded world of non-positive weights are “the same as never having interacted at all.”\nRefer to the\nRegressionMetrics\nPython docs\nand\nRankingMetrics\nPython docs\nfor more details on the API.\nfrom\npyspark.mllib.recommendation\nimport\nALS\n,\nRating\nfrom\npyspark.mllib.evaluation\nimport\nRegressionMetrics\n# Read in the ratings data\nlines\n=\nsc\n.\ntextFile\n(\n\"\ndata/mllib/sample_movielens_data.txt\n\"\n)\ndef\nparseLine\n(\nline\n):\nfields\n=\nline\n.\nsplit\n(\n\"\n::\n\"\n)\nreturn\nRating\n(\nint\n(\nfields\n[\n0\n]),\nint\n(\nfields\n[\n1\n]),\nfloat\n(\nfields\n[\n2\n])\n-\n2.5\n)\nratings\n=\nlines\n.\nmap\n(\nlambda\nr\n:\nparseLine\n(\nr\n))\n# Train a model on to ", "question": "How are unobserved entries generally categorized based on the provided confidence score mappings?", "answers": {"text": ["This mappings means unobserved entries are generally between It’s okay and Fairly bad."], "answer_start": [72]}}
{"context": "Movies movies.\"\n)\n// Build the model\nval\nnumIterations\n=\n10\nval\nrank\n=\n10\nval\nlambda\n=\n0.01\nval\nmodel\n=\nALS\n.\ntrain\n(\nratings\n,\nrank\n,\nnumIterations\n,\nlambda\n)\n// Define a function to scale ratings from 0 to 1\ndef\nscaledRating\n(\nr\n:\nRating\n)\n:\nRating\n=\n{\nval\nscaledRating\n=\nmath\n.\nmax\n(\nmath\n.\nmin\n(\nr\n.\nrating\n,\n1.0\n),\n0.0\n)\nRating\n(\nr\n.\nuser\n,\nr\n.\nproduct\n,\nscaledRating\n)\n}\n// Get sorted top ten predictions for each user and then scale from [0, 1]\nval\nuserRecommended\n=\nmodel\n.\nrecommendProductsForUsers\n(\n10\n).\nmap\n{\ncase\n(\nuser\n,\nrecs\n)\n=>\n(\nuser\n,\nrecs\n.\nmap\n(\nscaledRating\n))\n}\n// Assume that any movie a user rated 3 or higher (which maps to a 1) is a relevant document\n// Compare with top ten most relevant documents\nval\nuserMovies\n=\nbinarizedRatings\n.\ngroupBy\n(\n_\n.\nuser\n)\nval\nrelevantDocu", "question": "What is the value of numIterations?", "answers": {"text": ["10"], "answer_start": [57]}}
{"context": "\"\n)\n// Mean average precision at k\nprintln\n(\ns\n\"Mean average precision at 2 = ${metrics.meanAveragePrecisionAt(2)}\"\n)\n// Normalized discounted cumulative gain\nArray\n(\n1\n,\n3\n,\n5\n).\nforeach\n{\nk\n=>\nprintln\n(\ns\n\"NDCG at $k = ${metrics.ndcgAt(k)}\"\n)\n}\n// Recall at K\nArray\n(\n1\n,\n3\n,\n5\n).\nforeach\n{\nk\n=>\nprintln\n(\ns\n\"Recall at $k = ${metrics.recallAt(k)}\"\n)\n}\n// Get predictions for each data point\nval\nallPredictions\n=\nmodel\n.\npredict\n(\nratings\n.\nmap\n(\nr\n=>\n(\nr\n.\nuser\n,\nr\n.\nproduct\n))).\nmap\n(\nr\n=>\n((\nr\n.\nuser\n,\nr\n.\nproduct\n),\nr\n.\nrating\n))\nval\nallRatings\n=\nratings\n.\nmap\n(\nr\n=>\n((\nr\n.\nuser\n,\nr\n.\nproduct\n),\nr\n.\nrating\n))\nval\npredictionsAndLabels\n=\nallPredictions\n.\njoin\n(\nallRatings\n).\nmap\n{\ncase\n((\nuser\n,\nproduct\n),\n(\npredicted\n,\nactual\n))\n=>\n(\npredicted\n,\nactual\n)\n}\n// Get the RMSE using regression ", "question": "What is calculated using regression?", "answers": {"text": ["Get the RMSE using regression"], "answer_start": [770]}}
{"context": "er\n(),\nt\n.\n_2\n()[\ni\n].\nproduct\n(),\nnewRating\n);\n}\nreturn\nnew\nTuple2\n<>(\nt\n.\n_1\n(),\nscaledRatings\n);\n});\nJavaPairRDD\n<\nObject\n,\nRating\n[]>\nuserRecommended\n=\nJavaPairRDD\n.\nfromJavaRDD\n(\nuserRecsScaled\n);\n// Map ratings to 1 or 0, 1 indicating a movie that should be recommended\nJavaRDD\n<\nRating\n>\nbinarizedRatings\n=\nratings\n.\nmap\n(\nr\n->\n{\ndouble\nbinaryRating\n;\nif\n(\nr\n.\nrating\n()\n>\n0.0\n)\n{\nbinaryRating\n=\n1.0\n;\n}\nelse\n{\nbinaryRating\n=\n0.0\n;\n}\nreturn\nnew\nRating\n(\nr\n.\nuser\n(),\nr\n.\nproduct\n(),\nbinaryRating\n);\n});\n// Group ratings by common user\nJavaPairRDD\n<\nObject\n,\nIterable\n<\nRating\n>>\nuserMovies\n=\nbinarizedRatings\n.\ngroupBy\n(\nRating:\n:\nuser\n);\n// Get true relevant documents from all user ratings\nJavaPairRDD\n<\nObject\n,\nList\n<\nInteger\n>>\nuserMoviesList\n=\nuserMovies\n.\nmapValues\n(\ndocs\n->\n{\nList\n<\nI", "question": "What is done with the ratings in the line `JavaRDD<Rating> binarizedRatings = ratings.map(r -> { ... });`?", "answers": {"text": ["Map ratings to 1 or 0, 1 indicating a movie that should be recommended"], "answer_start": [205]}}
{"context": "et true relevant documents from all user ratings\nJavaPairRDD\n<\nObject\n,\nList\n<\nInteger\n>>\nuserMoviesList\n=\nuserMovies\n.\nmapValues\n(\ndocs\n->\n{\nList\n<\nInteger\n>\nproducts\n=\nnew\nArrayList\n<>();\nfor\n(\nRating\nr\n:\ndocs\n)\n{\nif\n(\nr\n.\nrating\n()\n>\n0.0\n)\n{\nproducts\n.\nadd\n(\nr\n.\nproduct\n());\n}\n}\nreturn\nproducts\n;\n});\n// Extract the product id from each recommendation\nJavaPairRDD\n<\nObject\n,\nList\n<\nInteger\n>>\nuserRecommendedList\n=\nuserRecommended\n.\nmapValues\n(\ndocs\n->\n{\nList\n<\nInteger\n>\nproducts\n=\nnew\nArrayList\n<>();\nfor\n(\nRating\nr\n:\ndocs\n)\n{\nproducts\n.\nadd\n(\nr\n.\nproduct\n());\n}\nreturn\nproducts\n;\n});\nJavaRDD\n<\nTuple2\n<\nList\n<\nInteger\n>,\nList\n<\nInteger\n>>>\nrelevantDocs\n=\nuserMoviesList\n.\njoin\n(\nuserRecommendedList\n).\nvalues\n();\n// Instantiate the metrics object\nRankingMetrics\n<\nInteger\n>\nmetrics\n=\nRankingMe", "question": "What is the purpose of the code snippet involving `userMoviesList` and `mapValues`?", "answers": {"text": ["Extract the product id from each recommendation"], "answer_start": [308]}}
{"context": "ings\n.\nmap\n(\nr\n->\nnew\nTuple2\n<\nTuple2\n<\nInteger\n,\nInteger\n>,\nObject\n>(\nnew\nTuple2\n<>(\nr\n.\nuser\n(),\nr\n.\nproduct\n()),\nr\n.\nrating\n())\n)).\njoin\n(\npredictions\n).\nvalues\n();\n// Create regression metrics object\nRegressionMetrics\nregressionMetrics\n=\nnew\nRegressionMetrics\n(\nratesAndPreds\n.\nrdd\n());\n// Root mean squared error\nSystem\n.\nout\n.\nformat\n(\n\"RMSE = %f\\n\"\n,\nregressionMetrics\n.\nrootMeanSquaredError\n());\n// R-squared\nSystem\n.\nout\n.\nformat\n(\n\"R-squared = %f\\n\"\n,\nregressionMetrics\n.\nr2\n());\nFind full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\" in the Spark repo.\nRegression model evaluation\nRegression analysis\nis used when predicting a continuous output\nvariable from a number of independent variables.\nAvailable metrics\nMetric\nDefinition\n", "question": "Where can I find the full example code for JavaRankingMetricsExample?", "answers": {"text": ["Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\" in the Spark repo."], "answer_start": [490]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nStructured Streaming Programming Guide\nOverview\nGetting Started\nAPIs on DataFrames and Datasets\nPerformance Tips\nAdditional Information\nStructured Streaming Programming Guide\nOverview\nTransformWithState is the new arbitrary stateful operator in Struct", "question": "What is the new arbitrary stateful operator in Structured Streaming?", "answers": {"text": ["TransformWithState is the new arbitrary stateful operator in Struct"], "answer_start": [733]}}
{"context": "rmance Tips\nAdditional Information\nStructured Streaming Programming Guide\nOverview\nTransformWithState is the new arbitrary stateful operator in Structured Streaming since the Apache Spark 4.0 release. This operator is the next generation replacement for the old mapGroupsWithState/flatMapGroupsWithState API in Scala and the applyInPandasWithState API in Python for arbitrary stateful processing in Apache Spark.\nThis operator has support for an umbrella of features such as object-oriented stateful processor definition, composite types, automatic TTL based eviction, timers etc and can be used to build business-critical operational use-cases.\nLanguage Support\nTransformWithState\nis available in Scala, Java and Python. Note that in Python, the operator name is called\ntransformWithStateInPandas\nsi", "question": "What is the new arbitrary stateful operator in Structured Streaming since the Apache Spark 4.0 release?", "answers": {"text": ["TransformWithState is the new arbitrary stateful operator in Structured Streaming since the Apache Spark 4.0 release."], "answer_start": [83]}}
{"context": "uage Support\nTransformWithState\nis available in Scala, Java and Python. Note that in Python, the operator name is called\ntransformWithStateInPandas\nsimilar to other operators interacting with the Pandas interface in Apache Spark.\nComponents of a TransformWithState Query\nA transformWithState query typically consists of the following components:\nStateful Processor - A user-defined stateful processor that defines the stateful logic\nOutput Mode - Output mode for the query such as Append, Update etc\nTime Mode - Time mode for the query such as EventTime, ProcessingTime etc\nInitial State - An optional initial state batch dataframe used to pre-populate the state\nIn the following sections, we will go through the above components in more detail.\nDefining a Stateful Processor\nA stateful processor is ", "question": "In which programming languages is TransformWithState available?", "answers": {"text": ["is available in Scala, Java and Python."], "answer_start": [32]}}
{"context": "te the state\nIn the following sections, we will go through the above components in more detail.\nDefining a Stateful Processor\nA stateful processor is the core of the user-defined logic used to operate on the input events. A stateful processor is defined by extending the StatefulProcessor class and implementing a few methods.\nA typical stateful processor deals with the following constructs:\nInput Records - Input records received by the stream\nState Variables - Zero or more class specific members used to store user state\nOutput Records - Output records produced by the processor. Zero or more output records may be produced by the processor.\nA stateful processor uses the object-oriented paradigm to define the stateful logic. The stateful logic is defined by implementing the following methods:\n", "question": "What is a stateful processor defined by?", "answers": {"text": ["extending the StatefulProcessor class and implementing a few methods."], "answer_start": [257]}}
{"context": "ateful processor uses the object-oriented paradigm to define the stateful logic. The stateful logic is defined by implementing the following methods:\ninit\n- Initialize the stateful processor and define any state variables as needed\nhandleInputRows\n- Process input rows belonging to a grouping key and emit output if needed\nhandleExpiredTimer\n- Handle expired timers and emit output if needed\nclose\n- Perform any cleanup operations if needed\nhandleInitialState\n- Optionally handle the initial state batch dataframe\nThe methods above will be invoked by the Spark query engine when the operator is executed as part of a streaming query.\nNote also that not all types of operations are supported in each of the methods. For eg, users cannot register timers in the\ninit\nmethod. Similarly, they cannot opera", "question": "What are the methods that define the stateful logic in a stateful processor?", "answers": {"text": ["init\n- Initialize the stateful processor and define any state variables as needed\nhandleInputRows\n- Process input rows belonging to a grouping key and emit output if needed\nhandleExpiredTimer\n- Handle expired timers and emit output if needed\nclose\n- Perform any cleanup operations if needed\nhandleInitialState\n- Optionally handle the initial state batch dataframe"], "answer_start": [150]}}
{"context": "s are class specific members used to store user state. They need to be declared once and initialized within the\ninit\nmethod of the stateful processor.\nInitializing a state variable typically involves the following steps:\nProvide a unique name for the state variable (unique within the stateful processor definition)\nProvide a type for the state variable (ValueState, ListState, MapState) - depending on the type, the appropriate method on the handle needs to be invoked\nProvide a state encoder for the state variable (in Scala - this can be skipped if implicit encoders are available)\nProvide an optional TTL config for the state variable\nTypes of state variables\nState variables can be of the following types:\nValue State\nList State\nMap State\nSimilar to collections for popular programming languages", "question": "What are the typical steps involved in initializing a state variable?", "answers": {"text": ["Provide a unique name for the state variable (unique within the stateful processor definition)\nProvide a type for the state variable (ValueState, ListState, MapState) - depending on the type, the appropriate method on the handle needs to be invoked\nProvide a state encoder for the state variable (in Scala - this can be skipped if implicit encoders are available)\nProvide an optional TTL config for the state variable"], "answer_start": [221]}}
{"context": "ate variables\nState variables can be of the following types:\nValue State\nList State\nMap State\nSimilar to collections for popular programming languages, the state types could be used to model data structures optimized for various types of operations for the underlying storage layer. For example, appends are optimized for ListState and point lookups are optimized for MapState.\nProviding state encoders\nState encoders are used to serialize and deserialize the state variables. In Scala, the state encoders can be skipped if implicit encoders are available. In Java and Python, the state encoders need to be provided explicitly.\nBuilt-in encoders for primitives, case classes and Java Bean classes are provided by default via the Spark SQL encoders.\nProviding implicit encoders in Scala\nIn Scala, impl", "question": "What types of state variables are available?", "answers": {"text": ["Value State\nList State\nMap State"], "answer_start": [61]}}
{"context": "primitives, case classes and Java Bean classes are provided by default via the Spark SQL encoders.\nProviding implicit encoders in Scala\nIn Scala, implicit encoders can be provided for case classes and primitive types. The\nimplicits\nobject is provided as part of the\nStatefulProcessor\nclass. Within the StatefulProcessor definition, the user can simply import implicits as\nimport implicits._\nand then they do not require to pass the encoder type explicitly.\nProviding TTL for state variables\nState variables can be configured with an optional TTL (Time-To-Live) value. The TTL value is used to automatically evict the state variable after the specified duration. The TTL value can be provided as a Duration.\nHandling input rows\nThe\nhandleInputRows\nmethod is used to process input rows belonging to a g", "question": "How can implicit encoders be provided for case classes and primitive types in Scala?", "answers": {"text": ["In Scala, implicit encoders can be provided for case classes and primitive types."], "answer_start": [136]}}
{"context": "processor has expired. This method is invoked once for each expired timer.\nHere are a few timer properties that are supported:\nMultiple timers associated with the same grouping key can be registered\nThe engine provides the ability to list/add/remove timers as needed\nTimers are also checkpointed as part of the query checkpoint and can be triggered on query restart as well.\nHandling initial state\nThe\nhandleInitialState\nmethod is used to optionally handle the initial state batch dataframe. The initial state batch dataframe is used to pre-populate the state for the stateful processor. The method is invoked by the Spark query engine when the initial state batch dataframe is available.\nThis method is only called once in the lifetime of the query. This is invoked before any input rows are process", "question": "When is the handleInitialState method invoked?", "answers": {"text": ["The method is invoked by the Spark query engine when the initial state batch dataframe is available."], "answer_start": [588]}}
{"context": "al state batch dataframe is available.\nThis method is only called once in the lifetime of the query. This is invoked before any input rows are processed by the stateful processor.\nPutting it all together\nHere is an example of a StatefulProcessor that implements a downtime detector. Each time a new value is seen for a given key, it updates the lastSeen state value, clears any existing timers, and resets a timer for the future.\nWhen a timer expires, the application emits the elapsed time since the last observed event for the key. It then sets a new timer to emit an update 10 seconds later.\nclass\nDownTimeDetector\n(\nStatefulProcessor\n):\ndef\ninit\n(\nself\n,\nhandle\n:\nStatefulProcessorHandle\n)\n->\nNone\n:\n# Define schema for the state value (timestamp)\nstate_schema\n=\nStructType\n([\nStructField\n(\n\"\nval", "question": "What does the StatefulProcessor do each time a new value is seen for a given key?", "answers": {"text": ["it updates the lastSeen state value, clears any existing timers, and resets a timer for the future."], "answer_start": [330]}}
{"context": "Register a new timer for 10 seconds in the future\nself\n.\nhandle\n.\nregisterTimer\n(\ntimerValues\n.\ngetCurrentProcessingTimeInMs\n()\n+\n10000\n)\n# Yield a DataFrame with the key and downtime duration\nyield\npd\n.\nDataFrame\n(\n{\n\"\nid\n\"\n:\nkey\n,\n\"\ntimeValues\n\"\n:\nstr\n(\ndowntime_duration\n),\n}\n)\ndef\nhandleInputRows\n(\nself\n,\nkey\n,\nrows\n,\ntimerValues\n)\n->\nIterator\n[\npd\n.\nDataFrame\n]:\n# Find the row with the maximum timestamp\nmax_row\n=\nmax\n((\ntuple\n(\npdf\n.\niloc\n[\n0\n])\nfor\npdf\nin\nrows\n),\nkey\n=\nlambda\nrow\n:\nrow\n[\n1\n])\n# Get the latest timestamp from existing state or use epoch start if not exists\nif\nself\n.\nlast_seen\n.\nexists\n():\nlatest_from_existing\n=\nself\n.\nlast_seen\n.\nget\n()\nelse\n:\nlatest_from_existing\n=\ndatetime\n.\nfromtimestamp\n(\n0\n)\n# If new data is more recent than existing state\nif\nlatest_from_existing\n<", "question": "What is done with the DataFrame that is yielded?", "answers": {"text": ["Yield a DataFrame with the key and downtime duration"], "answer_start": [140]}}
{"context": "_seen\n.\nget\n()\nelse\n:\nlatest_from_existing\n=\ndatetime\n.\nfromtimestamp\n(\n0\n)\n# If new data is more recent than existing state\nif\nlatest_from_existing\n<\nmax_row\n[\n1\n]:\n# Delete all existing timers\nfor\ntimer\nin\nself\n.\nhandle\n.\nlistTimers\n():\nself\n.\nhandle\n.\ndeleteTimer\n(\ntimer\n)\n# Update the last seen timestamp\nself\n.\nlast_seen\n.\nupdate\n((\nmax_row\n[\n1\n],))\n# Register a new timer for 5 seconds in the future\nself\n.\nhandle\n.\nregisterTimer\n(\ntimerValues\n.\ngetCurrentProcessingTimeInMs\n()\n+\n5000\n)\n# Yield an empty DataFrame\nyield\npd\n.\nDataFrame\n()\ndef\nclose\n(\nself\n)\n->\nNone\n:\n# No cleanup needed\npass\n// The (String, Timestamp) schema represents an (id, time). We want to do downtime\n// detection on every single unique sensor, where each sensor has a sensor ID.\nclass\nDowntimeDetector\n(\nduration\n:\nDur", "question": "What happens if new data is more recent than the existing state?", "answers": {"text": ["# If new data is more recent than existing state"], "answer_start": [76]}}
{"context": "tTimers\n().\nforeach\n(\ntimer\n=>\ngetHandle\n.\ndeleteTimer\n(\ntimer\n))\n_lastSeen\n.\nupdate\n(\nlatestTimestampFromNewRows\n)\n// Use timerValues to schedule a timer using processing time.\ngetHandle\n.\nregisterTimer\n(\ntimerValues\n.\ngetCurrentProcessingTimeInMs\n()\n+\nduration\n.\ntoMillis\n)\n}\nelse\n{\n// No new latest timestamp, so no need to update state or set a timer.\n}\nIterator\n.\nempty\n}\noverride\ndef\nhandleExpiredTimer\n(\nkey\n:\nString\n,\ntimerValues\n:\nTimerValues\n,\nexpiredTimerInfo\n:\nExpiredTimerInfo\n)\n:\nIterator\n[(\nString\n,\nDuration\n)]\n=\n{\nval\nlatestTimestamp\n=\n_lastSeen\n.\nget\n()\nval\ndowntimeDuration\n=\nnew\nDuration\n(\ntimerValues\n.\ngetCurrentProcessingTimeInMs\n()\n-\nlatestTimestamp\n.\ngetTime\n)\n// Register another timer that will fire in 10 seconds.\n// Timers can be registered anywhere but init()\ngetHandle\n", "question": "O que é usado para agendar um temporizador usando o tempo de processamento?", "answers": {"text": ["// Use timerValues to schedule a timer using processing time."], "answer_start": [116]}}
{"context": "utMode\n=\n\"\nUpdate\n\"\n,\ntimeMode\n=\n\"\nNone\n\"\n,\n)\n.\nwriteStream\n...\nval\nquery\n=\ndf\n.\ngroupBy\n(\n\"key\"\n)\n.\ntransformWithState\n(\nstatefulProcessor\n=\nnew\nDownTimeDetector\n(),\noutputMode\n=\nOutputMode\n.\nUpdate\n,\ntimeMode\n=\nTimeMode\n.\nNone\n)\n.\nwriteStream\n...\nState Schema Evolution\nTransformWithState also allows for performing schema evolution of the managed state. There are 2 parts here:\nevolution across state variables\nevolution within a state variable\nNote that schema evolution is only supported on the value side. Key side state schema evolution is not supported.\nEvolution across state variables\nThis operator allows for state variables to be added and removed across different runs of the same streaming query. In order to remove a variable, we also need to inform the engine so that the underlying s", "question": "What is supported regarding schema evolution in TransformWithState?", "answers": {"text": ["schema evolution is only supported on the value side."], "answer_start": [458]}}
{"context": " only when the underlying encoding format is set to\nAvro\n. In order to enable this, please set the following Spark config as\nspark.conf.set(\"spark.sql.streaming.stateStore.encodingFormat\", \"avro\")\n.\nThe following evolution operations are supported within Avro rules:\nAdding a new field\nRemoving a field\nType widening\nReordering fields\nThe following evolution operations are not supported:\nRenaming a field\nType narrowing\nIntegration with State Data Source\nTransformWithState is a stateful operator that allows users to maintain arbitrary state across batches. In order to read this state, the user needs to provide some additional options in the state data source reader query.\nThis operator allows for multiple state variables to be used within the same query. However, because they could be of diff", "question": "How can Avro encoding be enabled in Spark?", "answers": {"text": ["spark.conf.set(\"spark.sql.streaming.stateStore.encodingFormat\", \"avro\")"], "answer_start": [125]}}
{"context": "e data source reader query.\nThis operator allows for multiple state variables to be used within the same query. However, because they could be of different composite types and encoding formats, they need to be read within a batch query one variable at a time.\nIn order to allow this, the user needs to specify the\nstateVarName\nfor the state variable they are interested in reading.\nTimers can read by setting the option\nreadRegisteredTimers\nto true. This will return all the registered timer across grouping keys.\nWe also allow for composite type variables to be read in 2 formats:\nFlattened: This is the default format where the composite types are flattened out into individual columns.\nNon-flattened: This is where the composite types are returned as a single column of Array or Map type in Spark ", "question": "What are the two formats in which composite type variables can be read?", "answers": {"text": ["Flattened: This is the default format where the composite types are flattened out into individual columns.\nNon-flattened: This is where the composite types are returned as a single column of Array or Map type in Spark"], "answer_start": [582]}}
{"context": "flattened out into individual columns.\nNon-flattened: This is where the composite types are returned as a single column of Array or Map type in Spark SQL.\nDepending on your memory requirements, you can choose the format that best suits your use case.\nMore information about source options can be found\nhere\n.", "question": "What happens when composite types are not flattened in Spark SQL?", "answers": {"text": ["This is where the composite types are returned as a single column of Array or Map type in Spark SQL."], "answer_start": [54]}}
{"context": "4.0.0\nOverview\nProgramming Guides\nQuick Start\nRDDs, Accumulators, Broadcasts Vars\nSQL, DataFrames, and Datasets\nStructured Streaming\nSpark Streaming (DStreams)\nMLlib (Machine Learning)\nGraphX (Graph Processing)\nSparkR (R on Spark)\nPySpark (Python on Spark)\nAPI Docs\nPython\nScala\nJava\nR\nSQL, Built-in Functions\nDeploying\nOverview\nSubmitting Applications\nSpark Standalone\nYARN\nKubernetes\nMore\nConfiguration\nMonitoring\nTuning Guide\nJob Scheduling\nSecurity\nHardware Provisioning\nMigration Guide\nBuilding Spark\nContributing to Spark\nThird Party Projects\nStructured Streaming Programming Guide\nOverview\nGetting Started\nAPIs on DataFrames and Datasets\nPerformance Tips\nAdditional Information\nState Data Source Integration Guide\nState data source Guide in Structured Streaming (Experimental)\nOverview\nState d", "question": "What are some of the programming guides available in Spark?", "answers": {"text": ["RDDs, Accumulators, Broadcasts Vars"], "answer_start": [46]}}
{"context": "rmance Tips\nAdditional Information\nState Data Source Integration Guide\nState data source Guide in Structured Streaming (Experimental)\nOverview\nState data source provides functionality to manipulate the state from the checkpoint.\nAs of Spark 4.0, state data source provides the read functionality with a batch query. Additional functionalities including write is on the future roadmap.\nNOTE: this data source is currently marked as experimental - source options and the behavior (output) might be subject to change.\nReading state key-values from the checkpoint\nState data source enables reading key-value pairs from the state store in the checkpoint, via running a separate batch query.\nUsers can leverage the functionality to cover two major use cases described below:\nConstruct a test checking both ", "question": "What functionality does the state data source provide as of Spark 4.0?", "answers": {"text": ["state data source provides the read functionality with a batch query."], "answer_start": [246]}}
{"context": "via running a separate batch query.\nUsers can leverage the functionality to cover two major use cases described below:\nConstruct a test checking both output and the state. It is non-trivial to deduce the key-value of the state from the output, and having visibility of the state would be a huge win on testing.\nInvestigate an incident against stateful streaming query. If users observe the incorrect output and want to track how it came up, having visibility of the state would be required.\nUsers can read an instance of state store, which is matched to a single stateful operator in most cases. This means, users can expect that they can read the entire key-value pairs in the state for a single stateful operator.\nNote that there could be an exception, e.g. stream-stream join, which leverages mult", "question": "What can users do with the functionality described in the text?", "answers": {"text": ["Users can leverage the functionality to cover two major use cases described below:"], "answer_start": [36]}}
{"context": "atestore\"\n)\n.\nload\n(\n\"<checkpointLocation>\"\n);\nEach row in the source has the following schema:\nColumn\nType\nNote\nkey\nstruct (depends on the type for state key)\nvalue\nstruct (depends on the type for state value)\npartition_id\nint\nThe nested columns for key and value heavily depend on the input schema of the stateful operator as well as the type of operator.\nUsers are encouraged to query about the schema via df.schema() / df.printSchema() first to understand the type of output.\nThe following options must be set for the source.\nOption\nValue\nMeaning\npath\nstring\nSpecify the root directory of the checkpoint location. You can either specify the path via option(\"path\", `path`) or load(`path`).\nThe following configurations are optional:\nOption\nValue\nDefault\nMeaning\nbatchId\nnumeric value\nlatest commi", "question": "How can users understand the type of output from this source?", "answers": {"text": ["Users are encouraged to query about the schema via df.schema() / df.printSchema() first to understand the type of output."], "answer_start": [358]}}
{"context": "via option(\"path\", `path`) or load(`path`).\nThe following configurations are optional:\nOption\nValue\nDefault\nMeaning\nbatchId\nnumeric value\nlatest committed batch\nRepresents the target batch to read from. This option is used when users want to perform time-travel. The batch should be committed but not yet cleaned up.\noperatorId\nnumeric value\n0\nRepresents the target operator to read from. This option is used when the query is using multiple stateful operators.\nstoreName\nstring\nDEFAULT\nRepresents the target state store name to read from. This option is used when the stateful operator uses multiple state store instances. It is not required except stream-stream join.\njoinSide\nstring (\"left\" or \"right\")\n(none)\nRepresents the target side to read from. This option is used when users want to read th", "question": "What does the 'batchId' option represent?", "answers": {"text": ["Represents the target batch to read from. This option is used when users want to perform time-travel. The batch should be committed but not yet cleaned up."], "answer_start": [161]}}
{"context": "stream-stream join.\njoinSide\nstring (\"left\" or \"right\")\n(none)\nRepresents the target side to read from. This option is used when users want to read the state from stream-stream join.\nsnapshotStartBatchId\nnumeric value\nIf specified, force to read the snapshot at this batch ID, then changelogs will be replayed until 'batchId' or its default. Note that snapshot batch ID starts with 0 and equals to snapshot version ID minus 1. This option must be used together with 'snapshotPartitionId'.\nsnapshotPartitionId\nnumeric value\nIf specified, only this specific partition will be read. Note that partition ID starts with 0. This option must be used together with 'snapshotStartBatchId'.\nreadChangeFeed\nboolean\nfalse\nIf set to true, will read the change of state over microbatches. The output table schema w", "question": "What does the 'joinSide' option represent?", "answers": {"text": ["Represents the target side to read from."], "answer_start": [63]}}
{"context": "r with 'snapshotStartBatchId'.\nreadChangeFeed\nboolean\nfalse\nIf set to true, will read the change of state over microbatches. The output table schema will also differ. Details can be found in section\n\"Reading state changes over microbatches\"\n. Option 'changeStartBatchId' must be specified with this option. Option 'batchId', 'joinSide', 'snapshotStartBatchId' and 'snapshotPartitionId' cannot be used together with this option.\nchangeStartBatchId\nnumeric value\nRepresents the first batch to read in the read change feed mode. This option requires 'readChangeFeed' to be set to true.\nchangeEndBatchId\nnumeric value\nlatest commited batchId\nRepresents the last batch to read in the read change feed mode. This option requires 'readChangeFeed' to be set to true.\nstateVarName\nstring\nThe state variable na", "question": "Quais opções não podem ser usadas em conjunto com a opção 'readChangeFeed'?", "answers": {"text": ["Option 'batchId', 'joinSide', 'snapshotStartBatchId' and 'snapshotPartitionId' cannot be used together with this option."], "answer_start": [307]}}
{"context": "he last batch to read in the read change feed mode. This option requires 'readChangeFeed' to be set to true.\nstateVarName\nstring\nThe state variable name to read as part of this batch query. This is a required option if the transformWithState operator is used. Note that currently this option only applies to the transformWithState operator.\nreadRegisteredTimers\nboolean\nfalse\nIf true, the user can read registered timers used within the transformWithState operator. Note that currently this option only applies to the transformWithState operator. This option and the stateVarName option described above are mutually exclusive and only one of them can be used at a time.\nflattenCollectionTypes\nboolean\ntrue\nIf true, the collection types for state variables such as list state, map state etc are flatte", "question": "What does the 'flattenCollectionTypes' option do?", "answers": {"text": ["If true, the collection types for state variables such as list state, map state etc are flatte"], "answer_start": [706]}}
{"context": " be used at a time.\nflattenCollectionTypes\nboolean\ntrue\nIf true, the collection types for state variables such as list state, map state etc are flattened out. If false, the values are provided as Array or Map type in Spark SQL. Note that currently this option only applies to the transformWithState operator.\nReading state for stream-stream join\nStructured Streaming implements the stream-stream join feature via leveraging multiple instances of state store internally.\nThese instances logically compose buffers to store the input rows for left and right.\nSince it is more obvious to users to reason about, the data source provides the option ‘joinSide’ to read the buffered input for specific side of the join.\nTo enable the functionality to read the internal state store instance directly, we also ", "question": "What happens if flattenCollectionTypes is set to true?", "answers": {"text": ["If true, the collection types for state variables such as list state, map state etc are flattened out."], "answer_start": [56]}}
{"context": "de’ to read the buffered input for specific side of the join.\nTo enable the functionality to read the internal state store instance directly, we also allow specifying the option ‘storeName’, with restriction that ‘storeName’ and ‘joinSide’ cannot be specified together.\nReading state for transformWithState\nTransformWithState is a stateful operator that allows users to maintain arbitrary state across batches. In order to read this state, the user needs to provide some additional options in the state data source reader query.\nThis operator allows for multiple state variables to be used within the same query. However, because they could be of different composite types and encoding formats, they need to be read within a batch query one variable at a time.\nIn order to allow this, the user needs ", "question": "What restriction applies when using the 'storeName' option?", "answers": {"text": ["‘storeName’ and ‘joinSide’ cannot be specified together."], "answer_start": [213]}}
{"context": "ferent composite types and encoding formats, they need to be read within a batch query one variable at a time.\nIn order to allow this, the user needs to specify the\nstateVarName\nfor the state variable they are interested in reading.\nTimers can be read by setting the option\nreadRegisteredTimers\nto true. This will return all the registered timer across grouping keys.\nWe also allow for composite type variables to be read in 2 formats:\nFlattened: This is the default format where the composite types are flattened out into individual columns.\nNon-flattened: This is where the composite types are returned as a single column of Array or Map type in Spark SQL.\nDepending on your memory requirements, you can choose the format that best suits your use case.\nReading state changes over microbatches\nIf we", "question": "What are the two formats in which composite type variables can be read?", "answers": {"text": ["Flattened: This is the default format where the composite types are flattened out into individual columns.\nNon-flattened: This is where the composite types are returned as a single column of Array or Map type in Spark SQL."], "answer_start": [436]}}
{"context": "ark SQL.\nDepending on your memory requirements, you can choose the format that best suits your use case.\nReading state changes over microbatches\nIf we want to understand the change of state store over microbatches instead of the whole state store at a particular microbatch, ‘readChangeFeed’ is the option to use.\nFor example, this is the code to read the change of state from batch 2 to the latest committed batch.\ndf\n=\nspark\n\\\n.\nread\n\\\n.\nformat\n(\n\"\nstatestore\n\"\n)\n\\\n.\noption\n(\n\"\nreadChangeFeed\n\"\n,\ntrue\n)\n\\\n.\noption\n(\n\"\nchangeStartBatchId\n\"\n,\n2\n)\n\\\n.\nload\n(\n\"\n<checkpointLocation>\n\"\n)\nval\ndf\n=\nspark\n.\nread\n.\nformat\n(\n\"statestore\"\n)\n.\noption\n(\n\"readChangeFeed\"\n,\ntrue\n)\n.\noption\n(\n\"changeStartBatchId\"\n,\n2\n)\n.\nload\n(\n\"<checkpointLocation>\"\n)\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\nread\n()\n.\nformat\n(\n\"statest", "question": "What option should be used to understand the change of state store over microbatches instead of the whole state store at a particular microbatch?", "answers": {"text": ["‘readChangeFeed’ is the option to use."], "answer_start": [275]}}
{"context": "dChangeFeed\"\n,\ntrue\n)\n.\noption\n(\n\"changeStartBatchId\"\n,\n2\n)\n.\nload\n(\n\"<checkpointLocation>\"\n)\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\nread\n()\n.\nformat\n(\n\"statestore\"\n)\n.\noption\n(\n\"readChangeFeed\"\n,\ntrue\n)\n.\noption\n(\n\"changeStartBatchId\"\n,\n2\n)\n.\nload\n(\n\"<checkpointLocation>\"\n);\nThe output schema will also be different from the normal output.\nColumn\nType\nNote\nbatch_id\nlong\nchange_type\nstring\nThere are two possible values: 'update' and 'delete'. Update represents either inserting a non-existing key-value pair or updating an existing key with new value. The 'value' field will be null for delete records.\nkey\nstruct (depends on the type for state key)\nvalue\nstruct (depends on the type for state value)\npartition_id\nint\nState Metadata Source\nBefore querying the state from existing checkpoint via state data s", "question": "What are the two possible values for the 'change_type' column?", "answers": {"text": ["'update' and 'delete'"], "answer_start": [413]}}
{"context": "truct (depends on the type for state value)\npartition_id\nint\nState Metadata Source\nBefore querying the state from existing checkpoint via state data source, users would like to understand the information for the checkpoint, especially about state operator. This includes which operators and state store instances are available in the checkpoint, available range of batch IDs, etc.\nStructured Streaming provides a data source named “State metadata source” to provide the state-related metadata information from the checkpoint.\nNote: The metadata is constructed when the streaming query is running with Spark 4.0+. The existing checkpoint which has been running with lower Spark version does not have the metadata and will be unable to query/use with this metadata source. It is required to run the str", "question": "What information does the \"State metadata source\" provide?", "answers": {"text": ["to provide the state-related metadata information from the checkpoint."], "answer_start": [455]}}
{"context": "n running with lower Spark version does not have the metadata and will be unable to query/use with this metadata source. It is required to run the streaming query pointing the existing checkpoint in Spark 4.0+ to construct the metadata before querying.\nUsers can optionally provide the batchId to get the operator metadata at a point in time.\nCreating a State metadata store for Batch Queries\ndf\n=\nspark\n\\\n.\nread\n\\\n.\nformat\n(\n\"\nstate-metadata\n\"\n)\n\\\n.\nload\n(\n\"\n<checkpointLocation>\n\"\n)\nval\ndf\n=\nspark\n.\nread\n.\nformat\n(\n\"state-metadata\"\n)\n.\nload\n(\n\"<checkpointLocation>\"\n)\nDataset\n<\nRow\n>\ndf\n=\nspark\n.\nread\n()\n.\nformat\n(\n\"state-metadata\"\n)\n.\nload\n(\n\"<checkpointLocation>\"\n);\nThe following options must be set for the source:\nOption\nValue\nMeaning\npath\nstring\nSpecify the root directory of the checkpoint", "question": "What must be specified as the root directory for the checkpoint when using the 'state-metadata' format?", "answers": {"text": ["Specify the root directory of the checkpoint"], "answer_start": [756]}}
{"context": "heckpointLocation>\"\n);\nThe following options must be set for the source:\nOption\nValue\nMeaning\npath\nstring\nSpecify the root directory of the checkpoint location. You can either specify the path via option(\"path\", `path`) or load(`path`).\nThe following configurations are optional:\nOption\nValue\nDefault\nMeaning\nbatchId\nnumeric value\nLast committed batch if available, else 0\nOptional batchId used to retrieve operator metadata at that batch.\nEach row in the source has the following schema:\nColumn\nType\nNote\noperatorId\nint\noperatorName\nstring\nstateStoreName\nint\nnumPartitions\nint\nminBatchId\nint\nThe minimum batch ID available for querying state. The value could be invalid if the streaming query taking the checkpoint is running, as cleanup would run.\nmaxBatchId\nint\nThe maximum batch ID available for ", "question": "What does the 'path' option specify?", "answers": {"text": ["Specify the root directory of the checkpoint location."], "answer_start": [106]}}
{"context": "lue could be invalid if the streaming query taking the checkpoint is running, as cleanup would run.\nmaxBatchId\nint\nThe maximum batch ID available for querying state. The value could be invalid if the streaming query taking the checkpoint is running, as the query will commit further batches.\noperatorProperties\nstring\nList of properties used by the operator encoded as JSON. Output generated here is operator dependent.\n_numColsPrefixKey\nint\nmetadata column (hidden unless specified with SELECT)\nOne of the major use cases of this data source is to identify the operatorId to query if the query has multiple stateful operators, e.g. stream-stream join followed by deduplication.\nThe column ‘operatorName’ helps users to identify the operatorId for given operator.\nAdditionally, if users want to query", "question": "What is one major use case of this data source?", "answers": {"text": ["One of the major use cases of this data source is to identify the operatorId to query if the query has multiple stateful operators, e.g. stream-stream join followed by deduplication."], "answer_start": [496]}}
{"context": "n followed by deduplication.\nThe column ‘operatorName’ helps users to identify the operatorId for given operator.\nAdditionally, if users want to query about an internal state store instance for a stateful operator (e.g. stream-stream join), the column ‘stateStoreName’ would be useful to determine the target.", "question": "What column helps users identify the operatorId for a given operator?", "answers": {"text": ["The column ‘operatorName’ helps users to identify the operatorId for given operator."], "answer_start": [29]}}
